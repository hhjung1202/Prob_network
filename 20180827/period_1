Files already downloaded and verified
USE 1 GPUs!
Epoch: 0 | Batch_idx: 0 |  Loss: (2.2953) |  Loss2: (0.0000) | Acc: (9.00%) (12/128)
Epoch: 0 | Batch_idx: 10 |  Loss: (2.3017) |  Loss2: (0.0000) | Acc: (11.00%) (159/1408)
Epoch: 0 | Batch_idx: 20 |  Loss: (2.2993) |  Loss2: (0.0000) | Acc: (11.00%) (322/2688)
Epoch: 0 | Batch_idx: 30 |  Loss: (2.2945) |  Loss2: (0.0000) | Acc: (12.00%) (507/3968)
Epoch: 0 | Batch_idx: 40 |  Loss: (2.2882) |  Loss2: (0.0000) | Acc: (14.00%) (753/5248)
Epoch: 0 | Batch_idx: 50 |  Loss: (2.2809) |  Loss2: (0.0000) | Acc: (15.00%) (1013/6528)
Epoch: 0 | Batch_idx: 60 |  Loss: (2.2733) |  Loss2: (0.0000) | Acc: (16.00%) (1311/7808)
Epoch: 0 | Batch_idx: 70 |  Loss: (2.2655) |  Loss2: (0.0000) | Acc: (17.00%) (1600/9088)
Epoch: 0 | Batch_idx: 80 |  Loss: (2.2572) |  Loss2: (0.0000) | Acc: (18.00%) (1888/10368)
Epoch: 0 | Batch_idx: 90 |  Loss: (2.2481) |  Loss2: (0.0000) | Acc: (18.00%) (2159/11648)
Epoch: 0 | Batch_idx: 100 |  Loss: (2.2387) |  Loss2: (0.0000) | Acc: (18.00%) (2448/12928)
Epoch: 0 | Batch_idx: 110 |  Loss: (2.2291) |  Loss2: (0.0000) | Acc: (19.00%) (2735/14208)
Epoch: 0 | Batch_idx: 120 |  Loss: (2.2211) |  Loss2: (0.0000) | Acc: (19.00%) (3011/15488)
Epoch: 0 | Batch_idx: 130 |  Loss: (2.2113) |  Loss2: (0.0000) | Acc: (19.00%) (3326/16768)
Epoch: 0 | Batch_idx: 140 |  Loss: (2.2016) |  Loss2: (0.0000) | Acc: (20.00%) (3650/18048)
Epoch: 0 | Batch_idx: 150 |  Loss: (2.1937) |  Loss2: (0.0000) | Acc: (20.00%) (3980/19328)
Epoch: 0 | Batch_idx: 160 |  Loss: (2.1861) |  Loss2: (0.0000) | Acc: (20.00%) (4270/20608)
Epoch: 0 | Batch_idx: 170 |  Loss: (2.1774) |  Loss2: (0.0000) | Acc: (21.00%) (4622/21888)
Epoch: 0 | Batch_idx: 180 |  Loss: (2.1692) |  Loss2: (0.0000) | Acc: (21.00%) (4969/23168)
Epoch: 0 | Batch_idx: 190 |  Loss: (2.1612) |  Loss2: (0.0000) | Acc: (21.00%) (5342/24448)
Epoch: 0 | Batch_idx: 200 |  Loss: (2.1534) |  Loss2: (0.0000) | Acc: (22.00%) (5703/25728)
Epoch: 0 | Batch_idx: 210 |  Loss: (2.1460) |  Loss2: (0.0000) | Acc: (22.00%) (6034/27008)
Epoch: 0 | Batch_idx: 220 |  Loss: (2.1378) |  Loss2: (0.0000) | Acc: (22.00%) (6404/28288)
Epoch: 0 | Batch_idx: 230 |  Loss: (2.1311) |  Loss2: (0.0000) | Acc: (22.00%) (6763/29568)
Epoch: 0 | Batch_idx: 240 |  Loss: (2.1237) |  Loss2: (0.0000) | Acc: (23.00%) (7105/30848)
Epoch: 0 | Batch_idx: 250 |  Loss: (2.1170) |  Loss2: (0.0000) | Acc: (23.00%) (7488/32128)
Epoch: 0 | Batch_idx: 260 |  Loss: (2.1101) |  Loss2: (0.0000) | Acc: (23.00%) (7899/33408)
Epoch: 0 | Batch_idx: 270 |  Loss: (2.1036) |  Loss2: (0.0000) | Acc: (23.00%) (8262/34688)
Epoch: 0 | Batch_idx: 280 |  Loss: (2.0967) |  Loss2: (0.0000) | Acc: (24.00%) (8658/35968)
Epoch: 0 | Batch_idx: 290 |  Loss: (2.0907) |  Loss2: (0.0000) | Acc: (24.00%) (9028/37248)
Epoch: 0 | Batch_idx: 300 |  Loss: (2.0850) |  Loss2: (0.0000) | Acc: (24.00%) (9394/38528)
Epoch: 0 | Batch_idx: 310 |  Loss: (2.0799) |  Loss2: (0.0000) | Acc: (24.00%) (9776/39808)
Epoch: 0 | Batch_idx: 320 |  Loss: (2.0742) |  Loss2: (0.0000) | Acc: (24.00%) (10148/41088)
Epoch: 0 | Batch_idx: 330 |  Loss: (2.0685) |  Loss2: (0.0000) | Acc: (24.00%) (10548/42368)
Epoch: 0 | Batch_idx: 340 |  Loss: (2.0627) |  Loss2: (0.0000) | Acc: (25.00%) (10969/43648)
Epoch: 0 | Batch_idx: 350 |  Loss: (2.0569) |  Loss2: (0.0000) | Acc: (25.00%) (11380/44928)
Epoch: 0 | Batch_idx: 360 |  Loss: (2.0510) |  Loss2: (0.0000) | Acc: (25.00%) (11817/46208)
Epoch: 0 | Batch_idx: 370 |  Loss: (2.0454) |  Loss2: (0.0000) | Acc: (25.00%) (12258/47488)
Epoch: 0 | Batch_idx: 380 |  Loss: (2.0399) |  Loss2: (0.0000) | Acc: (26.00%) (12697/48768)
Epoch: 0 | Batch_idx: 390 |  Loss: (2.0352) |  Loss2: (0.0000) | Acc: (26.00%) (13096/50000)
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type ResNet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type BasicBlock. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type _Gate. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_000.pth.tar'
# TEST : Loss: (1.8082) | Acc: (32.00%) (3227/10000)
percent tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0') torch.Size([16])
percent tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0') torch.Size([16])
percent tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0') torch.Size([16])
percent tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0') torch.Size([16])
percent tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0') torch.Size([16])
percent tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0') torch.Size([16])
percent tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0') torch.Size([16])
percent tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(164.6802, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(768.2593, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(767.3705, device='cuda:0')
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1532.9963, device='cuda:0')
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(511.7211, device='cuda:0')
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2167.8362, device='cuda:0')
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4346.1328, device='cuda:0')
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1451.4552, device='cuda:0')
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6138.6089, device='cuda:0')
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12271.7314, device='cuda:0')
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4090.8328, device='cuda:0')
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(17361.7754, device='cuda:0')
Epoch: 1 | Batch_idx: 0 |  Loss: (1.7163) |  Loss2: (0.0000) | Acc: (40.00%) (52/128)
Epoch: 1 | Batch_idx: 10 |  Loss: (1.8324) |  Loss2: (0.0000) | Acc: (33.00%) (468/1408)
Epoch: 1 | Batch_idx: 20 |  Loss: (1.8428) |  Loss2: (0.0000) | Acc: (31.00%) (845/2688)
Epoch: 1 | Batch_idx: 30 |  Loss: (1.8496) |  Loss2: (0.0000) | Acc: (31.00%) (1252/3968)
Epoch: 1 | Batch_idx: 40 |  Loss: (1.8521) |  Loss2: (0.0000) | Acc: (31.00%) (1661/5248)
Epoch: 1 | Batch_idx: 50 |  Loss: (1.8517) |  Loss2: (0.0000) | Acc: (31.00%) (2071/6528)
Epoch: 1 | Batch_idx: 60 |  Loss: (1.8526) |  Loss2: (0.0000) | Acc: (31.00%) (2470/7808)
Epoch: 1 | Batch_idx: 70 |  Loss: (1.8517) |  Loss2: (0.0000) | Acc: (31.00%) (2904/9088)
Epoch: 1 | Batch_idx: 80 |  Loss: (1.8516) |  Loss2: (0.0000) | Acc: (31.00%) (3315/10368)
Epoch: 1 | Batch_idx: 90 |  Loss: (1.8493) |  Loss2: (0.0000) | Acc: (31.00%) (3713/11648)
Epoch: 1 | Batch_idx: 100 |  Loss: (1.8495) |  Loss2: (0.0000) | Acc: (31.00%) (4117/12928)
Epoch: 1 | Batch_idx: 110 |  Loss: (1.8502) |  Loss2: (0.0000) | Acc: (31.00%) (4510/14208)
Epoch: 1 | Batch_idx: 120 |  Loss: (1.8527) |  Loss2: (0.0000) | Acc: (31.00%) (4911/15488)
Epoch: 1 | Batch_idx: 130 |  Loss: (1.8524) |  Loss2: (0.0000) | Acc: (31.00%) (5328/16768)
Epoch: 1 | Batch_idx: 140 |  Loss: (1.8510) |  Loss2: (0.0000) | Acc: (31.00%) (5738/18048)
Epoch: 1 | Batch_idx: 150 |  Loss: (1.8497) |  Loss2: (0.0000) | Acc: (31.00%) (6164/19328)
Epoch: 1 | Batch_idx: 160 |  Loss: (1.8497) |  Loss2: (0.0000) | Acc: (31.00%) (6570/20608)
Epoch: 1 | Batch_idx: 170 |  Loss: (1.8492) |  Loss2: (0.0000) | Acc: (31.00%) (6981/21888)
Epoch: 1 | Batch_idx: 180 |  Loss: (1.8486) |  Loss2: (0.0000) | Acc: (31.00%) (7403/23168)
Epoch: 1 | Batch_idx: 190 |  Loss: (1.8480) |  Loss2: (0.0000) | Acc: (31.00%) (7793/24448)
Epoch: 1 | Batch_idx: 200 |  Loss: (1.8481) |  Loss2: (0.0000) | Acc: (31.00%) (8176/25728)
Epoch: 1 | Batch_idx: 210 |  Loss: (1.8474) |  Loss2: (0.0000) | Acc: (31.00%) (8581/27008)
Epoch: 1 | Batch_idx: 220 |  Loss: (1.8451) |  Loss2: (0.0000) | Acc: (31.00%) (9032/28288)
Epoch: 1 | Batch_idx: 230 |  Loss: (1.8443) |  Loss2: (0.0000) | Acc: (31.00%) (9445/29568)
Epoch: 1 | Batch_idx: 240 |  Loss: (1.8437) |  Loss2: (0.0000) | Acc: (31.00%) (9838/30848)
Epoch: 1 | Batch_idx: 250 |  Loss: (1.8426) |  Loss2: (0.0000) | Acc: (31.00%) (10277/32128)
Epoch: 1 | Batch_idx: 260 |  Loss: (1.8419) |  Loss2: (0.0000) | Acc: (31.00%) (10670/33408)
Epoch: 1 | Batch_idx: 270 |  Loss: (1.8401) |  Loss2: (0.0000) | Acc: (32.00%) (11114/34688)
Epoch: 1 | Batch_idx: 280 |  Loss: (1.8396) |  Loss2: (0.0000) | Acc: (32.00%) (11541/35968)
Epoch: 1 | Batch_idx: 290 |  Loss: (1.8383) |  Loss2: (0.0000) | Acc: (32.00%) (11979/37248)
Epoch: 1 | Batch_idx: 300 |  Loss: (1.8365) |  Loss2: (0.0000) | Acc: (32.00%) (12414/38528)
Epoch: 1 | Batch_idx: 310 |  Loss: (1.8354) |  Loss2: (0.0000) | Acc: (32.00%) (12851/39808)
Epoch: 1 | Batch_idx: 320 |  Loss: (1.8349) |  Loss2: (0.0000) | Acc: (32.00%) (13281/41088)
Epoch: 1 | Batch_idx: 330 |  Loss: (1.8348) |  Loss2: (0.0000) | Acc: (32.00%) (13679/42368)
Epoch: 1 | Batch_idx: 340 |  Loss: (1.8336) |  Loss2: (0.0000) | Acc: (32.00%) (14109/43648)
Epoch: 1 | Batch_idx: 350 |  Loss: (1.8328) |  Loss2: (0.0000) | Acc: (32.00%) (14531/44928)
Epoch: 1 | Batch_idx: 360 |  Loss: (1.8320) |  Loss2: (0.0000) | Acc: (32.00%) (14964/46208)
Epoch: 1 | Batch_idx: 370 |  Loss: (1.8310) |  Loss2: (0.0000) | Acc: (32.00%) (15393/47488)
Epoch: 1 | Batch_idx: 380 |  Loss: (1.8297) |  Loss2: (0.0000) | Acc: (32.00%) (15811/48768)
Epoch: 1 | Batch_idx: 390 |  Loss: (1.8291) |  Loss2: (0.0000) | Acc: (32.00%) (16207/50000)
# TEST : Loss: (1.7814) | Acc: (33.00%) (3364/10000)
percent tensor([0.5051, 0.5039, 0.5113, 0.5072, 0.5113, 0.5043, 0.5070, 0.5073, 0.5054,
        0.5065, 0.5034, 0.5114, 0.5048, 0.5025, 0.5046, 0.5041],
       device='cuda:0') torch.Size([16])
percent tensor([0.4983, 0.4978, 0.4972, 0.4972, 0.4974, 0.4980, 0.4977, 0.4970, 0.4979,
        0.4979, 0.4982, 0.4972, 0.4983, 0.4977, 0.4980, 0.4980],
       device='cuda:0') torch.Size([16])
percent tensor([0.4972, 0.4968, 0.4955, 0.4956, 0.4972, 0.4966, 0.4982, 0.4953, 0.4983,
        0.4976, 0.4969, 0.4965, 0.4972, 0.4965, 0.4962, 0.4960],
       device='cuda:0') torch.Size([16])
percent tensor([0.4983, 0.4974, 0.4975, 0.4991, 0.4975, 0.4980, 0.4975, 0.4982, 0.4970,
        0.4978, 0.4970, 0.4976, 0.4980, 0.4975, 0.4980, 0.4981],
       device='cuda:0') torch.Size([16])
percent tensor([0.5015, 0.5019, 0.5053, 0.5047, 0.5049, 0.5022, 0.5030, 0.5057, 0.5023,
        0.5023, 0.5013, 0.5042, 0.5013, 0.5016, 0.5020, 0.5023],
       device='cuda:0') torch.Size([16])
percent tensor([0.5028, 0.5027, 0.5071, 0.5087, 0.5073, 0.5038, 0.5037, 0.5069, 0.5028,
        0.5032, 0.5019, 0.5043, 0.5027, 0.5023, 0.5032, 0.5038],
       device='cuda:0') torch.Size([16])
percent tensor([0.5107, 0.5077, 0.5104, 0.5125, 0.5105, 0.5151, 0.5057, 0.5114, 0.5063,
        0.5059, 0.5081, 0.5066, 0.5086, 0.5073, 0.5071, 0.5090],
       device='cuda:0') torch.Size([16])
percent tensor([0.6408, 0.6194, 0.7092, 0.7364, 0.7289, 0.6874, 0.6190, 0.7462, 0.6197,
        0.6054, 0.6203, 0.6228, 0.6309, 0.6024, 0.6003, 0.6415],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 2 | Batch_idx: 0 |  Loss: (1.7929) |  Loss2: (0.0000) | Acc: (32.00%) (41/128)
Epoch: 2 | Batch_idx: 10 |  Loss: (1.7686) |  Loss2: (0.0000) | Acc: (32.00%) (458/1408)
Epoch: 2 | Batch_idx: 20 |  Loss: (1.7675) |  Loss2: (0.0000) | Acc: (33.00%) (899/2688)
Epoch: 2 | Batch_idx: 30 |  Loss: (1.7818) |  Loss2: (0.0000) | Acc: (33.00%) (1325/3968)
Epoch: 2 | Batch_idx: 40 |  Loss: (1.7745) |  Loss2: (0.0000) | Acc: (33.00%) (1743/5248)
Epoch: 2 | Batch_idx: 50 |  Loss: (1.7727) |  Loss2: (0.0000) | Acc: (33.00%) (2190/6528)
Epoch: 2 | Batch_idx: 60 |  Loss: (1.7727) |  Loss2: (0.0000) | Acc: (33.00%) (2635/7808)
Epoch: 2 | Batch_idx: 70 |  Loss: (1.7696) |  Loss2: (0.0000) | Acc: (34.00%) (3094/9088)
Epoch: 2 | Batch_idx: 80 |  Loss: (1.7683) |  Loss2: (0.0000) | Acc: (34.00%) (3539/10368)
Epoch: 2 | Batch_idx: 90 |  Loss: (1.7661) |  Loss2: (0.0000) | Acc: (34.00%) (4001/11648)
Epoch: 2 | Batch_idx: 100 |  Loss: (1.7623) |  Loss2: (0.0000) | Acc: (34.00%) (4478/12928)
Epoch: 2 | Batch_idx: 110 |  Loss: (1.7585) |  Loss2: (0.0000) | Acc: (34.00%) (4960/14208)
Epoch: 2 | Batch_idx: 120 |  Loss: (1.7545) |  Loss2: (0.0000) | Acc: (35.00%) (5454/15488)
Epoch: 2 | Batch_idx: 130 |  Loss: (1.7517) |  Loss2: (0.0000) | Acc: (35.00%) (5934/16768)
Epoch: 2 | Batch_idx: 140 |  Loss: (1.7478) |  Loss2: (0.0000) | Acc: (35.00%) (6422/18048)
Epoch: 2 | Batch_idx: 150 |  Loss: (1.7450) |  Loss2: (0.0000) | Acc: (35.00%) (6897/19328)
Epoch: 2 | Batch_idx: 160 |  Loss: (1.7394) |  Loss2: (0.0000) | Acc: (35.00%) (7393/20608)
Epoch: 2 | Batch_idx: 170 |  Loss: (1.7364) |  Loss2: (0.0000) | Acc: (35.00%) (7869/21888)
Epoch: 2 | Batch_idx: 180 |  Loss: (1.7339) |  Loss2: (0.0000) | Acc: (35.00%) (8331/23168)
Epoch: 2 | Batch_idx: 190 |  Loss: (1.7316) |  Loss2: (0.0000) | Acc: (36.00%) (8817/24448)
Epoch: 2 | Batch_idx: 200 |  Loss: (1.7290) |  Loss2: (0.0000) | Acc: (36.00%) (9303/25728)
Epoch: 2 | Batch_idx: 210 |  Loss: (1.7269) |  Loss2: (0.0000) | Acc: (36.00%) (9795/27008)
Epoch: 2 | Batch_idx: 220 |  Loss: (1.7229) |  Loss2: (0.0000) | Acc: (36.00%) (10306/28288)
Epoch: 2 | Batch_idx: 230 |  Loss: (1.7190) |  Loss2: (0.0000) | Acc: (36.00%) (10828/29568)
Epoch: 2 | Batch_idx: 240 |  Loss: (1.7153) |  Loss2: (0.0000) | Acc: (36.00%) (11342/30848)
Epoch: 2 | Batch_idx: 250 |  Loss: (1.7117) |  Loss2: (0.0000) | Acc: (36.00%) (11870/32128)
Epoch: 2 | Batch_idx: 260 |  Loss: (1.7099) |  Loss2: (0.0000) | Acc: (37.00%) (12372/33408)
Epoch: 2 | Batch_idx: 270 |  Loss: (1.7065) |  Loss2: (0.0000) | Acc: (37.00%) (12884/34688)
Epoch: 2 | Batch_idx: 280 |  Loss: (1.7061) |  Loss2: (0.0000) | Acc: (37.00%) (13323/35968)
Epoch: 2 | Batch_idx: 290 |  Loss: (1.7034) |  Loss2: (0.0000) | Acc: (37.00%) (13831/37248)
Epoch: 2 | Batch_idx: 300 |  Loss: (1.6996) |  Loss2: (0.0000) | Acc: (37.00%) (14378/38528)
Epoch: 2 | Batch_idx: 310 |  Loss: (1.6970) |  Loss2: (0.0000) | Acc: (37.00%) (14891/39808)
Epoch: 2 | Batch_idx: 320 |  Loss: (1.6945) |  Loss2: (0.0000) | Acc: (37.00%) (15402/41088)
Epoch: 2 | Batch_idx: 330 |  Loss: (1.6913) |  Loss2: (0.0000) | Acc: (37.00%) (15949/42368)
Epoch: 2 | Batch_idx: 340 |  Loss: (1.6889) |  Loss2: (0.0000) | Acc: (37.00%) (16474/43648)
Epoch: 2 | Batch_idx: 350 |  Loss: (1.6872) |  Loss2: (0.0000) | Acc: (37.00%) (16960/44928)
Epoch: 2 | Batch_idx: 360 |  Loss: (1.6846) |  Loss2: (0.0000) | Acc: (37.00%) (17460/46208)
Epoch: 2 | Batch_idx: 370 |  Loss: (1.6825) |  Loss2: (0.0000) | Acc: (37.00%) (17959/47488)
Epoch: 2 | Batch_idx: 380 |  Loss: (1.6801) |  Loss2: (0.0000) | Acc: (37.00%) (18496/48768)
Epoch: 2 | Batch_idx: 390 |  Loss: (1.6774) |  Loss2: (0.0000) | Acc: (38.00%) (19014/50000)
# TEST : Loss: (1.6700) | Acc: (38.00%) (3895/10000)
percent tensor([0.5056, 0.5042, 0.5109, 0.5073, 0.5108, 0.5051, 0.5069, 0.5068, 0.5055,
        0.5063, 0.5038, 0.5106, 0.5049, 0.5029, 0.5052, 0.5045],
       device='cuda:0') torch.Size([16])
percent tensor([0.4984, 0.4978, 0.4973, 0.4974, 0.4974, 0.4980, 0.4977, 0.4972, 0.4983,
        0.4980, 0.4984, 0.4973, 0.4984, 0.4975, 0.4979, 0.4981],
       device='cuda:0') torch.Size([16])
percent tensor([0.4969, 0.4966, 0.4946, 0.4957, 0.4965, 0.4967, 0.4981, 0.4953, 0.4981,
        0.4975, 0.4967, 0.4960, 0.4970, 0.4959, 0.4964, 0.4960],
       device='cuda:0') torch.Size([16])
percent tensor([0.4981, 0.4968, 0.4972, 0.4994, 0.4972, 0.4976, 0.4969, 0.4980, 0.4966,
        0.4969, 0.4967, 0.4975, 0.4976, 0.4969, 0.4976, 0.4978],
       device='cuda:0') torch.Size([16])
percent tensor([0.5018, 0.5022, 0.5051, 0.5040, 0.5046, 0.5028, 0.5031, 0.5061, 0.5021,
        0.5027, 0.5014, 0.5039, 0.5014, 0.5019, 0.5026, 0.5031],
       device='cuda:0') torch.Size([16])
percent tensor([0.5035, 0.5026, 0.5055, 0.5078, 0.5063, 0.5043, 0.5043, 0.5067, 0.5025,
        0.5029, 0.5020, 0.5045, 0.5024, 0.5023, 0.5036, 0.5035],
       device='cuda:0') torch.Size([16])
percent tensor([0.5094, 0.5097, 0.5076, 0.5118, 0.5077, 0.5134, 0.5068, 0.5124, 0.5075,
        0.5092, 0.5093, 0.5067, 0.5091, 0.5086, 0.5081, 0.5081],
       device='cuda:0') torch.Size([16])
percent tensor([0.6132, 0.6554, 0.6954, 0.7404, 0.7167, 0.6648, 0.6570, 0.7853, 0.6461,
        0.6675, 0.6381, 0.6344, 0.6431, 0.6321, 0.6492, 0.6624],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 3 | Batch_idx: 0 |  Loss: (1.5344) |  Loss2: (0.0000) | Acc: (39.00%) (51/128)
Epoch: 3 | Batch_idx: 10 |  Loss: (1.6175) |  Loss2: (0.0000) | Acc: (41.00%) (579/1408)
Epoch: 3 | Batch_idx: 20 |  Loss: (1.6318) |  Loss2: (0.0000) | Acc: (39.00%) (1061/2688)
Epoch: 3 | Batch_idx: 30 |  Loss: (1.6464) |  Loss2: (0.0000) | Acc: (39.00%) (1549/3968)
Epoch: 3 | Batch_idx: 40 |  Loss: (1.6557) |  Loss2: (0.0000) | Acc: (39.00%) (2052/5248)
Epoch: 3 | Batch_idx: 50 |  Loss: (1.6509) |  Loss2: (0.0000) | Acc: (39.00%) (2551/6528)
Epoch: 3 | Batch_idx: 60 |  Loss: (1.6493) |  Loss2: (0.0000) | Acc: (39.00%) (3046/7808)
Epoch: 3 | Batch_idx: 70 |  Loss: (1.6538) |  Loss2: (0.0000) | Acc: (38.00%) (3511/9088)
Epoch: 3 | Batch_idx: 80 |  Loss: (1.6555) |  Loss2: (0.0000) | Acc: (38.00%) (3991/10368)
Epoch: 3 | Batch_idx: 90 |  Loss: (1.6500) |  Loss2: (0.0000) | Acc: (38.00%) (4521/11648)
Epoch: 3 | Batch_idx: 100 |  Loss: (1.6480) |  Loss2: (0.0000) | Acc: (38.00%) (5029/12928)
Epoch: 3 | Batch_idx: 110 |  Loss: (1.6467) |  Loss2: (0.0000) | Acc: (38.00%) (5518/14208)
Epoch: 3 | Batch_idx: 120 |  Loss: (1.6423) |  Loss2: (0.0000) | Acc: (38.00%) (6034/15488)
Epoch: 3 | Batch_idx: 130 |  Loss: (1.6433) |  Loss2: (0.0000) | Acc: (38.00%) (6507/16768)
Epoch: 3 | Batch_idx: 140 |  Loss: (1.6410) |  Loss2: (0.0000) | Acc: (38.00%) (7015/18048)
Epoch: 3 | Batch_idx: 150 |  Loss: (1.6394) |  Loss2: (0.0000) | Acc: (38.00%) (7529/19328)
Epoch: 3 | Batch_idx: 160 |  Loss: (1.6379) |  Loss2: (0.0000) | Acc: (38.00%) (8027/20608)
Epoch: 3 | Batch_idx: 170 |  Loss: (1.6379) |  Loss2: (0.0000) | Acc: (38.00%) (8496/21888)
Epoch: 3 | Batch_idx: 180 |  Loss: (1.6372) |  Loss2: (0.0000) | Acc: (38.00%) (9008/23168)
Epoch: 3 | Batch_idx: 190 |  Loss: (1.6348) |  Loss2: (0.0000) | Acc: (38.00%) (9518/24448)
Epoch: 3 | Batch_idx: 200 |  Loss: (1.6338) |  Loss2: (0.0000) | Acc: (38.00%) (9983/25728)
Epoch: 3 | Batch_idx: 210 |  Loss: (1.6318) |  Loss2: (0.0000) | Acc: (38.00%) (10515/27008)
Epoch: 3 | Batch_idx: 220 |  Loss: (1.6318) |  Loss2: (0.0000) | Acc: (38.00%) (11020/28288)
Epoch: 3 | Batch_idx: 230 |  Loss: (1.6316) |  Loss2: (0.0000) | Acc: (39.00%) (11542/29568)
Epoch: 3 | Batch_idx: 240 |  Loss: (1.6298) |  Loss2: (0.0000) | Acc: (38.00%) (12024/30848)
Epoch: 3 | Batch_idx: 250 |  Loss: (1.6294) |  Loss2: (0.0000) | Acc: (38.00%) (12525/32128)
Epoch: 3 | Batch_idx: 260 |  Loss: (1.6283) |  Loss2: (0.0000) | Acc: (38.00%) (13027/33408)
Epoch: 3 | Batch_idx: 270 |  Loss: (1.6269) |  Loss2: (0.0000) | Acc: (38.00%) (13526/34688)
Epoch: 3 | Batch_idx: 280 |  Loss: (1.6258) |  Loss2: (0.0000) | Acc: (39.00%) (14036/35968)
Epoch: 3 | Batch_idx: 290 |  Loss: (1.6241) |  Loss2: (0.0000) | Acc: (39.00%) (14560/37248)
Epoch: 3 | Batch_idx: 300 |  Loss: (1.6231) |  Loss2: (0.0000) | Acc: (39.00%) (15082/38528)
Epoch: 3 | Batch_idx: 310 |  Loss: (1.6219) |  Loss2: (0.0000) | Acc: (39.00%) (15613/39808)
Epoch: 3 | Batch_idx: 320 |  Loss: (1.6210) |  Loss2: (0.0000) | Acc: (39.00%) (16137/41088)
Epoch: 3 | Batch_idx: 330 |  Loss: (1.6209) |  Loss2: (0.0000) | Acc: (39.00%) (16640/42368)
Epoch: 3 | Batch_idx: 340 |  Loss: (1.6197) |  Loss2: (0.0000) | Acc: (39.00%) (17140/43648)
Epoch: 3 | Batch_idx: 350 |  Loss: (1.6196) |  Loss2: (0.0000) | Acc: (39.00%) (17628/44928)
Epoch: 3 | Batch_idx: 360 |  Loss: (1.6177) |  Loss2: (0.0000) | Acc: (39.00%) (18150/46208)
Epoch: 3 | Batch_idx: 370 |  Loss: (1.6169) |  Loss2: (0.0000) | Acc: (39.00%) (18660/47488)
Epoch: 3 | Batch_idx: 380 |  Loss: (1.6174) |  Loss2: (0.0000) | Acc: (39.00%) (19150/48768)
Epoch: 3 | Batch_idx: 390 |  Loss: (1.6162) |  Loss2: (0.0000) | Acc: (39.00%) (19650/50000)
# TEST : Loss: (1.5634) | Acc: (40.00%) (4061/10000)
percent tensor([0.5010, 0.5000, 0.5037, 0.5020, 0.5034, 0.5009, 0.5016, 0.5019, 0.5009,
        0.5015, 0.5001, 0.5034, 0.5007, 0.4988, 0.5011, 0.5009],
       device='cuda:0') torch.Size([16])
percent tensor([0.4944, 0.4919, 0.4891, 0.4926, 0.4901, 0.4933, 0.4911, 0.4910, 0.4926,
        0.4926, 0.4935, 0.4898, 0.4943, 0.4912, 0.4931, 0.4937],
       device='cuda:0') torch.Size([16])
percent tensor([0.4939, 0.4940, 0.5183, 0.5002, 0.5171, 0.4919, 0.5050, 0.5068, 0.5027,
        0.5042, 0.4942, 0.5181, 0.4957, 0.4897, 0.4929, 0.4917],
       device='cuda:0') torch.Size([16])
percent tensor([0.4942, 0.4898, 0.4975, 0.4991, 0.4958, 0.4912, 0.4922, 0.4967, 0.4921,
        0.4929, 0.4900, 0.4963, 0.4932, 0.4906, 0.4919, 0.4939],
       device='cuda:0') torch.Size([16])
percent tensor([0.5023, 0.5021, 0.5121, 0.5078, 0.5107, 0.5032, 0.5052, 0.5119, 0.5029,
        0.5042, 0.5009, 0.5092, 0.5018, 0.5008, 0.5031, 0.5045],
       device='cuda:0') torch.Size([16])
percent tensor([0.4976, 0.4936, 0.4980, 0.5010, 0.4978, 0.4962, 0.4956, 0.4971, 0.4945,
        0.4948, 0.4938, 0.4969, 0.4959, 0.4941, 0.4956, 0.4969],
       device='cuda:0') torch.Size([16])
percent tensor([0.5188, 0.5122, 0.5080, 0.5128, 0.5050, 0.5248, 0.5070, 0.5074, 0.5129,
        0.5119, 0.5152, 0.5086, 0.5173, 0.5146, 0.5116, 0.5135],
       device='cuda:0') torch.Size([16])
percent tensor([0.8171, 0.8298, 0.8848, 0.8969, 0.9065, 0.8878, 0.8317, 0.9375, 0.8658,
        0.8503, 0.8395, 0.8363, 0.8527, 0.8474, 0.7789, 0.8543],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 4 | Batch_idx: 0 |  Loss: (1.6520) |  Loss2: (0.0000) | Acc: (41.00%) (53/128)
Epoch: 4 | Batch_idx: 10 |  Loss: (1.5399) |  Loss2: (0.0000) | Acc: (41.00%) (580/1408)
Epoch: 4 | Batch_idx: 20 |  Loss: (1.5486) |  Loss2: (0.0000) | Acc: (41.00%) (1119/2688)
Epoch: 4 | Batch_idx: 30 |  Loss: (1.5357) |  Loss2: (0.0000) | Acc: (42.00%) (1682/3968)
Epoch: 4 | Batch_idx: 40 |  Loss: (1.5339) |  Loss2: (0.0000) | Acc: (42.00%) (2228/5248)
Epoch: 4 | Batch_idx: 50 |  Loss: (1.5302) |  Loss2: (0.0000) | Acc: (42.00%) (2763/6528)
Epoch: 4 | Batch_idx: 60 |  Loss: (1.5264) |  Loss2: (0.0000) | Acc: (42.00%) (3308/7808)
Epoch: 4 | Batch_idx: 70 |  Loss: (1.5290) |  Loss2: (0.0000) | Acc: (42.00%) (3861/9088)
Epoch: 4 | Batch_idx: 80 |  Loss: (1.5254) |  Loss2: (0.0000) | Acc: (42.00%) (4415/10368)
Epoch: 4 | Batch_idx: 90 |  Loss: (1.5229) |  Loss2: (0.0000) | Acc: (42.00%) (4983/11648)
Epoch: 4 | Batch_idx: 100 |  Loss: (1.5217) |  Loss2: (0.0000) | Acc: (42.00%) (5537/12928)
Epoch: 4 | Batch_idx: 110 |  Loss: (1.5207) |  Loss2: (0.0000) | Acc: (42.00%) (6107/14208)
Epoch: 4 | Batch_idx: 120 |  Loss: (1.5176) |  Loss2: (0.0000) | Acc: (42.00%) (6659/15488)
Epoch: 4 | Batch_idx: 130 |  Loss: (1.5158) |  Loss2: (0.0000) | Acc: (43.00%) (7230/16768)
Epoch: 4 | Batch_idx: 140 |  Loss: (1.5130) |  Loss2: (0.0000) | Acc: (43.00%) (7817/18048)
Epoch: 4 | Batch_idx: 150 |  Loss: (1.5113) |  Loss2: (0.0000) | Acc: (43.00%) (8389/19328)
Epoch: 4 | Batch_idx: 160 |  Loss: (1.5121) |  Loss2: (0.0000) | Acc: (43.00%) (8946/20608)
Epoch: 4 | Batch_idx: 170 |  Loss: (1.5090) |  Loss2: (0.0000) | Acc: (43.00%) (9529/21888)
Epoch: 4 | Batch_idx: 180 |  Loss: (1.5081) |  Loss2: (0.0000) | Acc: (43.00%) (10117/23168)
Epoch: 4 | Batch_idx: 190 |  Loss: (1.5062) |  Loss2: (0.0000) | Acc: (43.00%) (10704/24448)
Epoch: 4 | Batch_idx: 200 |  Loss: (1.5028) |  Loss2: (0.0000) | Acc: (43.00%) (11316/25728)
Epoch: 4 | Batch_idx: 210 |  Loss: (1.5014) |  Loss2: (0.0000) | Acc: (44.00%) (11884/27008)
Epoch: 4 | Batch_idx: 220 |  Loss: (1.4994) |  Loss2: (0.0000) | Acc: (44.00%) (12498/28288)
Epoch: 4 | Batch_idx: 230 |  Loss: (1.4973) |  Loss2: (0.0000) | Acc: (44.00%) (13082/29568)
Epoch: 4 | Batch_idx: 240 |  Loss: (1.4964) |  Loss2: (0.0000) | Acc: (44.00%) (13654/30848)
Epoch: 4 | Batch_idx: 250 |  Loss: (1.4948) |  Loss2: (0.0000) | Acc: (44.00%) (14235/32128)
Epoch: 4 | Batch_idx: 260 |  Loss: (1.4903) |  Loss2: (0.0000) | Acc: (44.00%) (14884/33408)
Epoch: 4 | Batch_idx: 270 |  Loss: (1.4871) |  Loss2: (0.0000) | Acc: (44.00%) (15496/34688)
Epoch: 4 | Batch_idx: 280 |  Loss: (1.4849) |  Loss2: (0.0000) | Acc: (44.00%) (16109/35968)
Epoch: 4 | Batch_idx: 290 |  Loss: (1.4824) |  Loss2: (0.0000) | Acc: (44.00%) (16714/37248)
Epoch: 4 | Batch_idx: 300 |  Loss: (1.4819) |  Loss2: (0.0000) | Acc: (44.00%) (17312/38528)
Epoch: 4 | Batch_idx: 310 |  Loss: (1.4800) |  Loss2: (0.0000) | Acc: (45.00%) (17918/39808)
Epoch: 4 | Batch_idx: 320 |  Loss: (1.4783) |  Loss2: (0.0000) | Acc: (45.00%) (18512/41088)
Epoch: 4 | Batch_idx: 330 |  Loss: (1.4769) |  Loss2: (0.0000) | Acc: (45.00%) (19130/42368)
Epoch: 4 | Batch_idx: 340 |  Loss: (1.4749) |  Loss2: (0.0000) | Acc: (45.00%) (19768/43648)
Epoch: 4 | Batch_idx: 350 |  Loss: (1.4728) |  Loss2: (0.0000) | Acc: (45.00%) (20365/44928)
Epoch: 4 | Batch_idx: 360 |  Loss: (1.4700) |  Loss2: (0.0000) | Acc: (45.00%) (21006/46208)
Epoch: 4 | Batch_idx: 370 |  Loss: (1.4676) |  Loss2: (0.0000) | Acc: (45.00%) (21635/47488)
Epoch: 4 | Batch_idx: 380 |  Loss: (1.4659) |  Loss2: (0.0000) | Acc: (45.00%) (22273/48768)
Epoch: 4 | Batch_idx: 390 |  Loss: (1.4635) |  Loss2: (0.0000) | Acc: (45.00%) (22884/50000)
# TEST : Loss: (1.4265) | Acc: (47.00%) (4792/10000)
percent tensor([0.5010, 0.5001, 0.5036, 0.5020, 0.5033, 0.5009, 0.5015, 0.5020, 0.5010,
        0.5014, 0.5000, 0.5033, 0.5007, 0.4988, 0.5011, 0.5009],
       device='cuda:0') torch.Size([16])
percent tensor([0.4947, 0.4915, 0.4899, 0.4919, 0.4909, 0.4937, 0.4912, 0.4908, 0.4933,
        0.4927, 0.4939, 0.4908, 0.4944, 0.4912, 0.4930, 0.4939],
       device='cuda:0') torch.Size([16])
percent tensor([0.4957, 0.4935, 0.5150, 0.5014, 0.5133, 0.4918, 0.5020, 0.5076, 0.5016,
        0.5043, 0.4942, 0.5144, 0.4970, 0.4878, 0.4943, 0.4939],
       device='cuda:0') torch.Size([16])
percent tensor([0.4930, 0.4899, 0.4965, 0.4984, 0.4950, 0.4926, 0.4921, 0.4957, 0.4924,
        0.4905, 0.4896, 0.4965, 0.4921, 0.4914, 0.4918, 0.4917],
       device='cuda:0') torch.Size([16])
percent tensor([0.5031, 0.5033, 0.5123, 0.5078, 0.5098, 0.5050, 0.5052, 0.5131, 0.5034,
        0.5056, 0.5018, 0.5087, 0.5015, 0.5023, 0.5039, 0.5053],
       device='cuda:0') torch.Size([16])
percent tensor([0.4953, 0.4940, 0.4977, 0.5008, 0.4967, 0.4950, 0.4957, 0.4962, 0.4950,
        0.4928, 0.4927, 0.4973, 0.4950, 0.4944, 0.4946, 0.4943],
       device='cuda:0') torch.Size([16])
percent tensor([0.5144, 0.5112, 0.5063, 0.5123, 0.5048, 0.5212, 0.5068, 0.5067, 0.5111,
        0.5116, 0.5138, 0.5061, 0.5173, 0.5132, 0.5097, 0.5081],
       device='cuda:0') torch.Size([16])
percent tensor([0.7967, 0.8293, 0.9014, 0.8891, 0.9091, 0.8818, 0.8240, 0.9483, 0.8380,
        0.9052, 0.8470, 0.8119, 0.8682, 0.8440, 0.8376, 0.8860],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 5 | Batch_idx: 0 |  Loss: (1.3001) |  Loss2: (0.0000) | Acc: (50.00%) (64/128)
Epoch: 5 | Batch_idx: 10 |  Loss: (1.4134) |  Loss2: (0.0000) | Acc: (50.00%) (714/1408)
Epoch: 5 | Batch_idx: 20 |  Loss: (1.4427) |  Loss2: (0.0000) | Acc: (47.00%) (1278/2688)
Epoch: 5 | Batch_idx: 30 |  Loss: (1.4549) |  Loss2: (0.0000) | Acc: (47.00%) (1883/3968)
Epoch: 5 | Batch_idx: 40 |  Loss: (1.4714) |  Loss2: (0.0000) | Acc: (46.00%) (2428/5248)
Epoch: 5 | Batch_idx: 50 |  Loss: (1.4750) |  Loss2: (0.0000) | Acc: (45.00%) (2998/6528)
Epoch: 5 | Batch_idx: 60 |  Loss: (1.4790) |  Loss2: (0.0000) | Acc: (45.00%) (3574/7808)
Epoch: 5 | Batch_idx: 70 |  Loss: (1.4805) |  Loss2: (0.0000) | Acc: (45.00%) (4162/9088)
Epoch: 5 | Batch_idx: 80 |  Loss: (1.4736) |  Loss2: (0.0000) | Acc: (46.00%) (4795/10368)
Epoch: 5 | Batch_idx: 90 |  Loss: (1.4738) |  Loss2: (0.0000) | Acc: (46.00%) (5379/11648)
Epoch: 5 | Batch_idx: 100 |  Loss: (1.4754) |  Loss2: (0.0000) | Acc: (45.00%) (5939/12928)
Epoch: 5 | Batch_idx: 110 |  Loss: (1.4778) |  Loss2: (0.0000) | Acc: (45.00%) (6513/14208)
Epoch: 5 | Batch_idx: 120 |  Loss: (1.4772) |  Loss2: (0.0000) | Acc: (45.00%) (7104/15488)
Epoch: 5 | Batch_idx: 130 |  Loss: (1.4753) |  Loss2: (0.0000) | Acc: (46.00%) (7719/16768)
Epoch: 5 | Batch_idx: 140 |  Loss: (1.4740) |  Loss2: (0.0000) | Acc: (46.00%) (8319/18048)
Epoch: 5 | Batch_idx: 150 |  Loss: (1.4720) |  Loss2: (0.0000) | Acc: (46.00%) (8939/19328)
Epoch: 5 | Batch_idx: 160 |  Loss: (1.4668) |  Loss2: (0.0000) | Acc: (46.00%) (9562/20608)
Epoch: 5 | Batch_idx: 170 |  Loss: (1.4623) |  Loss2: (0.0000) | Acc: (46.00%) (10181/21888)
Epoch: 5 | Batch_idx: 180 |  Loss: (1.4611) |  Loss2: (0.0000) | Acc: (46.00%) (10789/23168)
Epoch: 5 | Batch_idx: 190 |  Loss: (1.4596) |  Loss2: (0.0000) | Acc: (46.00%) (11408/24448)
Epoch: 5 | Batch_idx: 200 |  Loss: (1.4583) |  Loss2: (0.0000) | Acc: (46.00%) (12020/25728)
Epoch: 5 | Batch_idx: 210 |  Loss: (1.4562) |  Loss2: (0.0000) | Acc: (46.00%) (12625/27008)
Epoch: 5 | Batch_idx: 220 |  Loss: (1.4544) |  Loss2: (0.0000) | Acc: (46.00%) (13247/28288)
Epoch: 5 | Batch_idx: 230 |  Loss: (1.4530) |  Loss2: (0.0000) | Acc: (46.00%) (13879/29568)
Epoch: 5 | Batch_idx: 240 |  Loss: (1.4520) |  Loss2: (0.0000) | Acc: (46.00%) (14494/30848)
Epoch: 5 | Batch_idx: 250 |  Loss: (1.4524) |  Loss2: (0.0000) | Acc: (46.00%) (15085/32128)
Epoch: 5 | Batch_idx: 260 |  Loss: (1.4519) |  Loss2: (0.0000) | Acc: (46.00%) (15679/33408)
Epoch: 5 | Batch_idx: 270 |  Loss: (1.4495) |  Loss2: (0.0000) | Acc: (47.00%) (16310/34688)
Epoch: 5 | Batch_idx: 280 |  Loss: (1.4470) |  Loss2: (0.0000) | Acc: (47.00%) (16948/35968)
Epoch: 5 | Batch_idx: 290 |  Loss: (1.4455) |  Loss2: (0.0000) | Acc: (47.00%) (17556/37248)
Epoch: 5 | Batch_idx: 300 |  Loss: (1.4435) |  Loss2: (0.0000) | Acc: (47.00%) (18187/38528)
Epoch: 5 | Batch_idx: 310 |  Loss: (1.4414) |  Loss2: (0.0000) | Acc: (47.00%) (18814/39808)
Epoch: 5 | Batch_idx: 320 |  Loss: (1.4400) |  Loss2: (0.0000) | Acc: (47.00%) (19445/41088)
Epoch: 5 | Batch_idx: 330 |  Loss: (1.4389) |  Loss2: (0.0000) | Acc: (47.00%) (20064/42368)
Epoch: 5 | Batch_idx: 340 |  Loss: (1.4368) |  Loss2: (0.0000) | Acc: (47.00%) (20697/43648)
Epoch: 5 | Batch_idx: 350 |  Loss: (1.4355) |  Loss2: (0.0000) | Acc: (47.00%) (21315/44928)
Epoch: 5 | Batch_idx: 360 |  Loss: (1.4355) |  Loss2: (0.0000) | Acc: (47.00%) (21904/46208)
Epoch: 5 | Batch_idx: 370 |  Loss: (1.4336) |  Loss2: (0.0000) | Acc: (47.00%) (22545/47488)
Epoch: 5 | Batch_idx: 380 |  Loss: (1.4320) |  Loss2: (0.0000) | Acc: (47.00%) (23166/48768)
Epoch: 5 | Batch_idx: 390 |  Loss: (1.4314) |  Loss2: (0.0000) | Acc: (47.00%) (23770/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_005.pth.tar'
# TEST : Loss: (1.3440) | Acc: (50.00%) (5080/10000)
percent tensor([0.5067, 0.5041, 0.5145, 0.5086, 0.5137, 0.5056, 0.5085, 0.5084, 0.5071,
        0.5076, 0.5041, 0.5137, 0.5059, 0.5010, 0.5056, 0.5049],
       device='cuda:0') torch.Size([16])
percent tensor([0.4774, 0.4641, 0.4389, 0.4661, 0.4473, 0.4762, 0.4550, 0.4528, 0.4637,
        0.4605, 0.4709, 0.4441, 0.4735, 0.4681, 0.4702, 0.4751],
       device='cuda:0') torch.Size([16])
percent tensor([0.4862, 0.4839, 0.4626, 0.4790, 0.4708, 0.4770, 0.4811, 0.4742, 0.4839,
        0.4825, 0.4826, 0.4675, 0.4864, 0.4822, 0.4816, 0.4840],
       device='cuda:0') torch.Size([16])
percent tensor([0.4899, 0.4862, 0.4946, 0.4980, 0.4924, 0.4889, 0.4886, 0.4939, 0.4883,
        0.4873, 0.4846, 0.4944, 0.4885, 0.4883, 0.4882, 0.4886],
       device='cuda:0') torch.Size([16])
percent tensor([0.5051, 0.5055, 0.5154, 0.5099, 0.5143, 0.5075, 0.5085, 0.5172, 0.5059,
        0.5085, 0.5042, 0.5118, 0.5031, 0.5043, 0.5061, 0.5076],
       device='cuda:0') torch.Size([16])
percent tensor([0.5042, 0.5028, 0.5122, 0.5145, 0.5117, 0.5088, 0.5069, 0.5121, 0.5028,
        0.5023, 0.5000, 0.5095, 0.5023, 0.5022, 0.5050, 0.5049],
       device='cuda:0') torch.Size([16])
percent tensor([0.5283, 0.5259, 0.5309, 0.5273, 0.5337, 0.5453, 0.5210, 0.5444, 0.5227,
        0.5331, 0.5318, 0.5220, 0.5313, 0.5236, 0.5261, 0.5328],
       device='cuda:0') torch.Size([16])
percent tensor([0.9283, 0.9037, 0.9602, 0.9592, 0.9672, 0.9670, 0.9031, 0.9838, 0.9488,
        0.9617, 0.9523, 0.9098, 0.9660, 0.9334, 0.9240, 0.9523],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 6 | Batch_idx: 0 |  Loss: (1.2382) |  Loss2: (0.0000) | Acc: (52.00%) (67/128)
Epoch: 6 | Batch_idx: 10 |  Loss: (1.3235) |  Loss2: (0.0000) | Acc: (51.00%) (729/1408)
Epoch: 6 | Batch_idx: 20 |  Loss: (1.3431) |  Loss2: (0.0000) | Acc: (50.00%) (1345/2688)
Epoch: 6 | Batch_idx: 30 |  Loss: (1.3526) |  Loss2: (0.0000) | Acc: (50.00%) (1995/3968)
Epoch: 6 | Batch_idx: 40 |  Loss: (1.3512) |  Loss2: (0.0000) | Acc: (50.00%) (2631/5248)
Epoch: 6 | Batch_idx: 50 |  Loss: (1.3502) |  Loss2: (0.0000) | Acc: (50.00%) (3280/6528)
Epoch: 6 | Batch_idx: 60 |  Loss: (1.3545) |  Loss2: (0.0000) | Acc: (50.00%) (3921/7808)
Epoch: 6 | Batch_idx: 70 |  Loss: (1.3437) |  Loss2: (0.0000) | Acc: (50.00%) (4619/9088)
Epoch: 6 | Batch_idx: 80 |  Loss: (1.3430) |  Loss2: (0.0000) | Acc: (50.00%) (5263/10368)
Epoch: 6 | Batch_idx: 90 |  Loss: (1.3446) |  Loss2: (0.0000) | Acc: (50.00%) (5907/11648)
Epoch: 6 | Batch_idx: 100 |  Loss: (1.3454) |  Loss2: (0.0000) | Acc: (50.00%) (6556/12928)
Epoch: 6 | Batch_idx: 110 |  Loss: (1.3436) |  Loss2: (0.0000) | Acc: (50.00%) (7219/14208)
Epoch: 6 | Batch_idx: 120 |  Loss: (1.3417) |  Loss2: (0.0000) | Acc: (50.00%) (7888/15488)
Epoch: 6 | Batch_idx: 130 |  Loss: (1.3394) |  Loss2: (0.0000) | Acc: (51.00%) (8585/16768)
Epoch: 6 | Batch_idx: 140 |  Loss: (1.3400) |  Loss2: (0.0000) | Acc: (51.00%) (9229/18048)
Epoch: 6 | Batch_idx: 150 |  Loss: (1.3367) |  Loss2: (0.0000) | Acc: (51.00%) (9915/19328)
Epoch: 6 | Batch_idx: 160 |  Loss: (1.3347) |  Loss2: (0.0000) | Acc: (51.00%) (10614/20608)
Epoch: 6 | Batch_idx: 170 |  Loss: (1.3328) |  Loss2: (0.0000) | Acc: (51.00%) (11284/21888)
Epoch: 6 | Batch_idx: 180 |  Loss: (1.3311) |  Loss2: (0.0000) | Acc: (51.00%) (11959/23168)
Epoch: 6 | Batch_idx: 190 |  Loss: (1.3280) |  Loss2: (0.0000) | Acc: (51.00%) (12672/24448)
Epoch: 6 | Batch_idx: 200 |  Loss: (1.3258) |  Loss2: (0.0000) | Acc: (51.00%) (13363/25728)
Epoch: 6 | Batch_idx: 210 |  Loss: (1.3226) |  Loss2: (0.0000) | Acc: (52.00%) (14056/27008)
Epoch: 6 | Batch_idx: 220 |  Loss: (1.3194) |  Loss2: (0.0000) | Acc: (52.00%) (14766/28288)
Epoch: 6 | Batch_idx: 230 |  Loss: (1.3159) |  Loss2: (0.0000) | Acc: (52.00%) (15475/29568)
Epoch: 6 | Batch_idx: 240 |  Loss: (1.3132) |  Loss2: (0.0000) | Acc: (52.00%) (16180/30848)
Epoch: 6 | Batch_idx: 250 |  Loss: (1.3101) |  Loss2: (0.0000) | Acc: (52.00%) (16887/32128)
Epoch: 6 | Batch_idx: 260 |  Loss: (1.3076) |  Loss2: (0.0000) | Acc: (52.00%) (17590/33408)
Epoch: 6 | Batch_idx: 270 |  Loss: (1.3070) |  Loss2: (0.0000) | Acc: (52.00%) (18252/34688)
Epoch: 6 | Batch_idx: 280 |  Loss: (1.3051) |  Loss2: (0.0000) | Acc: (52.00%) (18956/35968)
Epoch: 6 | Batch_idx: 290 |  Loss: (1.3018) |  Loss2: (0.0000) | Acc: (52.00%) (19686/37248)
Epoch: 6 | Batch_idx: 300 |  Loss: (1.3009) |  Loss2: (0.0000) | Acc: (52.00%) (20377/38528)
Epoch: 6 | Batch_idx: 310 |  Loss: (1.2988) |  Loss2: (0.0000) | Acc: (53.00%) (21102/39808)
Epoch: 6 | Batch_idx: 320 |  Loss: (1.2969) |  Loss2: (0.0000) | Acc: (53.00%) (21818/41088)
Epoch: 6 | Batch_idx: 330 |  Loss: (1.2952) |  Loss2: (0.0000) | Acc: (53.00%) (22534/42368)
Epoch: 6 | Batch_idx: 340 |  Loss: (1.2936) |  Loss2: (0.0000) | Acc: (53.00%) (23255/43648)
Epoch: 6 | Batch_idx: 350 |  Loss: (1.2914) |  Loss2: (0.0000) | Acc: (53.00%) (23970/44928)
Epoch: 6 | Batch_idx: 360 |  Loss: (1.2891) |  Loss2: (0.0000) | Acc: (53.00%) (24702/46208)
Epoch: 6 | Batch_idx: 370 |  Loss: (1.2870) |  Loss2: (0.0000) | Acc: (53.00%) (25431/47488)
Epoch: 6 | Batch_idx: 380 |  Loss: (1.2841) |  Loss2: (0.0000) | Acc: (53.00%) (26171/48768)
Epoch: 6 | Batch_idx: 390 |  Loss: (1.2821) |  Loss2: (0.0000) | Acc: (53.00%) (26889/50000)
# TEST : Loss: (1.2810) | Acc: (54.00%) (5409/10000)
percent tensor([0.5065, 0.5045, 0.5144, 0.5089, 0.5134, 0.5057, 0.5083, 0.5089, 0.5065,
        0.5077, 0.5036, 0.5132, 0.5058, 0.5008, 0.5059, 0.5049],
       device='cuda:0') torch.Size([16])
percent tensor([0.4739, 0.4621, 0.4527, 0.4677, 0.4561, 0.4746, 0.4586, 0.4589, 0.4671,
        0.4623, 0.4694, 0.4550, 0.4710, 0.4686, 0.4679, 0.4720],
       device='cuda:0') torch.Size([16])
percent tensor([0.4845, 0.4822, 0.4741, 0.4823, 0.4790, 0.4756, 0.4844, 0.4795, 0.4856,
        0.4846, 0.4810, 0.4781, 0.4843, 0.4819, 0.4802, 0.4823],
       device='cuda:0') torch.Size([16])
percent tensor([0.4908, 0.4855, 0.4933, 0.4955, 0.4916, 0.4920, 0.4883, 0.4922, 0.4874,
        0.4857, 0.4849, 0.4927, 0.4875, 0.4874, 0.4882, 0.4890],
       device='cuda:0') torch.Size([16])
percent tensor([0.5051, 0.5057, 0.5134, 0.5090, 0.5118, 0.5074, 0.5074, 0.5155, 0.5047,
        0.5076, 0.5042, 0.5089, 0.5028, 0.5042, 0.5067, 0.5072],
       device='cuda:0') torch.Size([16])
percent tensor([0.5040, 0.5031, 0.5117, 0.5148, 0.5123, 0.5101, 0.5056, 0.5107, 0.5033,
        0.5027, 0.5004, 0.5079, 0.5023, 0.5026, 0.5059, 0.5047],
       device='cuda:0') torch.Size([16])
percent tensor([0.5266, 0.5243, 0.5258, 0.5283, 0.5269, 0.5387, 0.5205, 0.5405, 0.5240,
        0.5292, 0.5288, 0.5178, 0.5285, 0.5255, 0.5226, 0.5348],
       device='cuda:0') torch.Size([16])
percent tensor([0.9407, 0.9262, 0.9508, 0.9650, 0.9560, 0.9647, 0.9136, 0.9722, 0.9627,
        0.9631, 0.9598, 0.9393, 0.9654, 0.9533, 0.9341, 0.9673],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 7 | Batch_idx: 0 |  Loss: (1.1506) |  Loss2: (0.0000) | Acc: (60.00%) (77/128)
Epoch: 7 | Batch_idx: 10 |  Loss: (1.3269) |  Loss2: (0.0000) | Acc: (51.00%) (725/1408)
Epoch: 7 | Batch_idx: 20 |  Loss: (1.4078) |  Loss2: (0.0000) | Acc: (49.00%) (1320/2688)
Epoch: 7 | Batch_idx: 30 |  Loss: (1.4408) |  Loss2: (0.0000) | Acc: (48.00%) (1911/3968)
Epoch: 7 | Batch_idx: 40 |  Loss: (1.4717) |  Loss2: (0.0000) | Acc: (47.00%) (2468/5248)
Epoch: 7 | Batch_idx: 50 |  Loss: (1.4825) |  Loss2: (0.0000) | Acc: (46.00%) (3040/6528)
Epoch: 7 | Batch_idx: 60 |  Loss: (1.4843) |  Loss2: (0.0000) | Acc: (46.00%) (3618/7808)
Epoch: 7 | Batch_idx: 70 |  Loss: (1.4843) |  Loss2: (0.0000) | Acc: (46.00%) (4192/9088)
Epoch: 7 | Batch_idx: 80 |  Loss: (1.4842) |  Loss2: (0.0000) | Acc: (46.00%) (4801/10368)
Epoch: 7 | Batch_idx: 90 |  Loss: (1.4790) |  Loss2: (0.0000) | Acc: (46.00%) (5412/11648)
Epoch: 7 | Batch_idx: 100 |  Loss: (1.4727) |  Loss2: (0.0000) | Acc: (46.00%) (6035/12928)
Epoch: 7 | Batch_idx: 110 |  Loss: (1.4726) |  Loss2: (0.0000) | Acc: (46.00%) (6654/14208)
Epoch: 7 | Batch_idx: 120 |  Loss: (1.4654) |  Loss2: (0.0000) | Acc: (47.00%) (7296/15488)
Epoch: 7 | Batch_idx: 130 |  Loss: (1.4601) |  Loss2: (0.0000) | Acc: (47.00%) (7924/16768)
Epoch: 7 | Batch_idx: 140 |  Loss: (1.4562) |  Loss2: (0.0000) | Acc: (47.00%) (8550/18048)
Epoch: 7 | Batch_idx: 150 |  Loss: (1.4531) |  Loss2: (0.0000) | Acc: (47.00%) (9152/19328)
Epoch: 7 | Batch_idx: 160 |  Loss: (1.4455) |  Loss2: (0.0000) | Acc: (47.00%) (9810/20608)
Epoch: 7 | Batch_idx: 170 |  Loss: (1.4406) |  Loss2: (0.0000) | Acc: (47.00%) (10458/21888)
Epoch: 7 | Batch_idx: 180 |  Loss: (1.4357) |  Loss2: (0.0000) | Acc: (48.00%) (11142/23168)
Epoch: 7 | Batch_idx: 190 |  Loss: (1.4316) |  Loss2: (0.0000) | Acc: (48.00%) (11804/24448)
Epoch: 7 | Batch_idx: 200 |  Loss: (1.4245) |  Loss2: (0.0000) | Acc: (48.00%) (12474/25728)
Epoch: 7 | Batch_idx: 210 |  Loss: (1.4198) |  Loss2: (0.0000) | Acc: (48.00%) (13135/27008)
Epoch: 7 | Batch_idx: 220 |  Loss: (1.4148) |  Loss2: (0.0000) | Acc: (48.00%) (13815/28288)
Epoch: 7 | Batch_idx: 230 |  Loss: (1.4092) |  Loss2: (0.0000) | Acc: (49.00%) (14494/29568)
Epoch: 7 | Batch_idx: 240 |  Loss: (1.4057) |  Loss2: (0.0000) | Acc: (49.00%) (15156/30848)
Epoch: 7 | Batch_idx: 250 |  Loss: (1.4018) |  Loss2: (0.0000) | Acc: (49.00%) (15839/32128)
Epoch: 7 | Batch_idx: 260 |  Loss: (1.3976) |  Loss2: (0.0000) | Acc: (49.00%) (16524/33408)
Epoch: 7 | Batch_idx: 270 |  Loss: (1.3963) |  Loss2: (0.0000) | Acc: (49.00%) (17166/34688)
Epoch: 7 | Batch_idx: 280 |  Loss: (1.3927) |  Loss2: (0.0000) | Acc: (49.00%) (17844/35968)
Epoch: 7 | Batch_idx: 290 |  Loss: (1.3897) |  Loss2: (0.0000) | Acc: (49.00%) (18522/37248)
Epoch: 7 | Batch_idx: 300 |  Loss: (1.3859) |  Loss2: (0.0000) | Acc: (49.00%) (19205/38528)
Epoch: 7 | Batch_idx: 310 |  Loss: (1.3815) |  Loss2: (0.0000) | Acc: (50.00%) (19907/39808)
Epoch: 7 | Batch_idx: 320 |  Loss: (1.3774) |  Loss2: (0.0000) | Acc: (50.00%) (20621/41088)
Epoch: 7 | Batch_idx: 330 |  Loss: (1.3752) |  Loss2: (0.0000) | Acc: (50.00%) (21295/42368)
Epoch: 7 | Batch_idx: 340 |  Loss: (1.3729) |  Loss2: (0.0000) | Acc: (50.00%) (21973/43648)
Epoch: 7 | Batch_idx: 350 |  Loss: (1.3709) |  Loss2: (0.0000) | Acc: (50.00%) (22630/44928)
Epoch: 7 | Batch_idx: 360 |  Loss: (1.3678) |  Loss2: (0.0000) | Acc: (50.00%) (23311/46208)
Epoch: 7 | Batch_idx: 370 |  Loss: (1.3659) |  Loss2: (0.0000) | Acc: (50.00%) (23991/47488)
Epoch: 7 | Batch_idx: 380 |  Loss: (1.3647) |  Loss2: (0.0000) | Acc: (50.00%) (24642/48768)
Epoch: 7 | Batch_idx: 390 |  Loss: (1.3636) |  Loss2: (0.0000) | Acc: (50.00%) (25285/50000)
# TEST : Loss: (1.2557) | Acc: (54.00%) (5408/10000)
percent tensor([0.5006, 0.5001, 0.5013, 0.4993, 0.5008, 0.4994, 0.5010, 0.5001, 0.5014,
        0.5007, 0.5011, 0.5014, 0.5009, 0.4994, 0.5000, 0.4998],
       device='cuda:0') torch.Size([16])
percent tensor([0.4722, 0.4635, 0.4603, 0.4706, 0.4623, 0.4686, 0.4616, 0.4652, 0.4675,
        0.4665, 0.4671, 0.4615, 0.4711, 0.4677, 0.4676, 0.4717],
       device='cuda:0') torch.Size([16])
percent tensor([0.4884, 0.4886, 0.4732, 0.4891, 0.4779, 0.4814, 0.4871, 0.4843, 0.4886,
        0.4871, 0.4850, 0.4763, 0.4881, 0.4905, 0.4864, 0.4888],
       device='cuda:0') torch.Size([16])
percent tensor([0.5035, 0.5005, 0.5061, 0.5052, 0.5055, 0.5039, 0.5027, 0.5047, 0.5031,
        0.5015, 0.5013, 0.5052, 0.5028, 0.5006, 0.5016, 0.5015],
       device='cuda:0') torch.Size([16])
percent tensor([0.5222, 0.5217, 0.5277, 0.5215, 0.5216, 0.5239, 0.5208, 0.5334, 0.5188,
        0.5231, 0.5199, 0.5205, 0.5187, 0.5175, 0.5207, 0.5240],
       device='cuda:0') torch.Size([16])
percent tensor([0.5021, 0.5008, 0.5138, 0.5163, 0.5131, 0.5098, 0.5032, 0.5134, 0.5005,
        0.5022, 0.4956, 0.5085, 0.4987, 0.4975, 0.5053, 0.5048],
       device='cuda:0') torch.Size([16])
percent tensor([0.5346, 0.5320, 0.5307, 0.5302, 0.5269, 0.5667, 0.5263, 0.5524, 0.5266,
        0.5329, 0.5334, 0.5222, 0.5333, 0.5307, 0.5336, 0.5428],
       device='cuda:0') torch.Size([16])
percent tensor([0.9449, 0.9358, 0.9560, 0.9519, 0.9535, 0.9689, 0.9273, 0.9883, 0.9560,
        0.9603, 0.9556, 0.9270, 0.9609, 0.9512, 0.9438, 0.9747],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 8 | Batch_idx: 0 |  Loss: (1.2972) |  Loss2: (0.0000) | Acc: (53.00%) (68/128)
Epoch: 8 | Batch_idx: 10 |  Loss: (1.1835) |  Loss2: (0.0000) | Acc: (55.00%) (782/1408)
Epoch: 8 | Batch_idx: 20 |  Loss: (1.2015) |  Loss2: (0.0000) | Acc: (56.00%) (1510/2688)
Epoch: 8 | Batch_idx: 30 |  Loss: (1.1996) |  Loss2: (0.0000) | Acc: (56.00%) (2243/3968)
Epoch: 8 | Batch_idx: 40 |  Loss: (1.1923) |  Loss2: (0.0000) | Acc: (56.00%) (2982/5248)
Epoch: 8 | Batch_idx: 50 |  Loss: (1.1913) |  Loss2: (0.0000) | Acc: (57.00%) (3721/6528)
Epoch: 8 | Batch_idx: 60 |  Loss: (1.1945) |  Loss2: (0.0000) | Acc: (56.00%) (4435/7808)
Epoch: 8 | Batch_idx: 70 |  Loss: (1.1945) |  Loss2: (0.0000) | Acc: (56.00%) (5153/9088)
Epoch: 8 | Batch_idx: 80 |  Loss: (1.1943) |  Loss2: (0.0000) | Acc: (56.00%) (5884/10368)
Epoch: 8 | Batch_idx: 90 |  Loss: (1.1932) |  Loss2: (0.0000) | Acc: (56.00%) (6629/11648)
Epoch: 8 | Batch_idx: 100 |  Loss: (1.1948) |  Loss2: (0.0000) | Acc: (56.00%) (7355/12928)
Epoch: 8 | Batch_idx: 110 |  Loss: (1.1917) |  Loss2: (0.0000) | Acc: (56.00%) (8098/14208)
Epoch: 8 | Batch_idx: 120 |  Loss: (1.1878) |  Loss2: (0.0000) | Acc: (57.00%) (8864/15488)
Epoch: 8 | Batch_idx: 130 |  Loss: (1.1861) |  Loss2: (0.0000) | Acc: (57.00%) (9598/16768)
Epoch: 8 | Batch_idx: 140 |  Loss: (1.1841) |  Loss2: (0.0000) | Acc: (57.00%) (10351/18048)
Epoch: 8 | Batch_idx: 150 |  Loss: (1.1817) |  Loss2: (0.0000) | Acc: (57.00%) (11113/19328)
Epoch: 8 | Batch_idx: 160 |  Loss: (1.1778) |  Loss2: (0.0000) | Acc: (57.00%) (11880/20608)
Epoch: 8 | Batch_idx: 170 |  Loss: (1.1736) |  Loss2: (0.0000) | Acc: (57.00%) (12654/21888)
Epoch: 8 | Batch_idx: 180 |  Loss: (1.1719) |  Loss2: (0.0000) | Acc: (57.00%) (13403/23168)
Epoch: 8 | Batch_idx: 190 |  Loss: (1.1709) |  Loss2: (0.0000) | Acc: (57.00%) (14144/24448)
Epoch: 8 | Batch_idx: 200 |  Loss: (1.1686) |  Loss2: (0.0000) | Acc: (57.00%) (14908/25728)
Epoch: 8 | Batch_idx: 210 |  Loss: (1.1679) |  Loss2: (0.0000) | Acc: (57.00%) (15660/27008)
Epoch: 8 | Batch_idx: 220 |  Loss: (1.1666) |  Loss2: (0.0000) | Acc: (58.00%) (16422/28288)
Epoch: 8 | Batch_idx: 230 |  Loss: (1.1652) |  Loss2: (0.0000) | Acc: (58.00%) (17184/29568)
Epoch: 8 | Batch_idx: 240 |  Loss: (1.1637) |  Loss2: (0.0000) | Acc: (58.00%) (17938/30848)
Epoch: 8 | Batch_idx: 250 |  Loss: (1.1622) |  Loss2: (0.0000) | Acc: (58.00%) (18686/32128)
Epoch: 8 | Batch_idx: 260 |  Loss: (1.1621) |  Loss2: (0.0000) | Acc: (58.00%) (19430/33408)
Epoch: 8 | Batch_idx: 270 |  Loss: (1.1590) |  Loss2: (0.0000) | Acc: (58.00%) (20221/34688)
Epoch: 8 | Batch_idx: 280 |  Loss: (1.1583) |  Loss2: (0.0000) | Acc: (58.00%) (20978/35968)
Epoch: 8 | Batch_idx: 290 |  Loss: (1.1559) |  Loss2: (0.0000) | Acc: (58.00%) (21767/37248)
Epoch: 8 | Batch_idx: 300 |  Loss: (1.1529) |  Loss2: (0.0000) | Acc: (58.00%) (22554/38528)
Epoch: 8 | Batch_idx: 310 |  Loss: (1.1519) |  Loss2: (0.0000) | Acc: (58.00%) (23322/39808)
Epoch: 8 | Batch_idx: 320 |  Loss: (1.1516) |  Loss2: (0.0000) | Acc: (58.00%) (24090/41088)
Epoch: 8 | Batch_idx: 330 |  Loss: (1.1497) |  Loss2: (0.0000) | Acc: (58.00%) (24865/42368)
Epoch: 8 | Batch_idx: 340 |  Loss: (1.1469) |  Loss2: (0.0000) | Acc: (58.00%) (25667/43648)
Epoch: 8 | Batch_idx: 350 |  Loss: (1.1465) |  Loss2: (0.0000) | Acc: (58.00%) (26440/44928)
Epoch: 8 | Batch_idx: 360 |  Loss: (1.1451) |  Loss2: (0.0000) | Acc: (58.00%) (27230/46208)
Epoch: 8 | Batch_idx: 370 |  Loss: (1.1444) |  Loss2: (0.0000) | Acc: (58.00%) (27986/47488)
Epoch: 8 | Batch_idx: 380 |  Loss: (1.1444) |  Loss2: (0.0000) | Acc: (58.00%) (28737/48768)
Epoch: 8 | Batch_idx: 390 |  Loss: (1.1422) |  Loss2: (0.0000) | Acc: (58.00%) (29497/50000)
# TEST : Loss: (1.2150) | Acc: (55.00%) (5527/10000)
percent tensor([0.5004, 0.5003, 0.5010, 0.4994, 0.5003, 0.4992, 0.5008, 0.5000, 0.5011,
        0.5007, 0.5010, 0.5011, 0.5008, 0.4994, 0.5000, 0.4997],
       device='cuda:0') torch.Size([16])
percent tensor([0.4729, 0.4649, 0.4554, 0.4694, 0.4586, 0.4703, 0.4606, 0.4637, 0.4672,
        0.4658, 0.4684, 0.4577, 0.4715, 0.4703, 0.4678, 0.4721],
       device='cuda:0') torch.Size([16])
percent tensor([0.4896, 0.4886, 0.4715, 0.4890, 0.4765, 0.4842, 0.4862, 0.4844, 0.4893,
        0.4858, 0.4865, 0.4740, 0.4885, 0.4905, 0.4868, 0.4898],
       device='cuda:0') torch.Size([16])
percent tensor([0.5034, 0.5009, 0.5065, 0.5055, 0.5055, 0.5040, 0.5030, 0.5050, 0.5043,
        0.5017, 0.5022, 0.5057, 0.5032, 0.5014, 0.5019, 0.5019],
       device='cuda:0') torch.Size([16])
percent tensor([0.5220, 0.5184, 0.5310, 0.5242, 0.5280, 0.5225, 0.5211, 0.5324, 0.5190,
        0.5219, 0.5171, 0.5233, 0.5162, 0.5156, 0.5193, 0.5220],
       device='cuda:0') torch.Size([16])
percent tensor([0.5036, 0.5021, 0.5138, 0.5155, 0.5148, 0.5100, 0.5068, 0.5144, 0.5022,
        0.5011, 0.4978, 0.5084, 0.4988, 0.5014, 0.5064, 0.5052],
       device='cuda:0') torch.Size([16])
percent tensor([0.5300, 0.5249, 0.5327, 0.5310, 0.5324, 0.5521, 0.5244, 0.5444, 0.5233,
        0.5279, 0.5296, 0.5232, 0.5303, 0.5271, 0.5304, 0.5381],
       device='cuda:0') torch.Size([16])
percent tensor([0.9266, 0.9282, 0.9530, 0.9463, 0.9636, 0.9513, 0.9134, 0.9806, 0.9294,
        0.9546, 0.9602, 0.9172, 0.9600, 0.9335, 0.9575, 0.9598],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 9 | Batch_idx: 0 |  Loss: (0.9814) |  Loss2: (0.0000) | Acc: (64.00%) (83/128)
Epoch: 9 | Batch_idx: 10 |  Loss: (1.1008) |  Loss2: (0.0000) | Acc: (62.00%) (873/1408)
Epoch: 9 | Batch_idx: 20 |  Loss: (1.1570) |  Loss2: (0.0000) | Acc: (58.00%) (1575/2688)
Epoch: 9 | Batch_idx: 30 |  Loss: (1.1997) |  Loss2: (0.0000) | Acc: (56.00%) (2235/3968)
Epoch: 9 | Batch_idx: 40 |  Loss: (1.2217) |  Loss2: (0.0000) | Acc: (55.00%) (2937/5248)
Epoch: 9 | Batch_idx: 50 |  Loss: (1.2395) |  Loss2: (0.0000) | Acc: (55.00%) (3627/6528)
Epoch: 9 | Batch_idx: 60 |  Loss: (1.2385) |  Loss2: (0.0000) | Acc: (55.00%) (4328/7808)
Epoch: 9 | Batch_idx: 70 |  Loss: (1.2423) |  Loss2: (0.0000) | Acc: (55.00%) (5030/9088)
Epoch: 9 | Batch_idx: 80 |  Loss: (1.2400) |  Loss2: (0.0000) | Acc: (55.00%) (5765/10368)
Epoch: 9 | Batch_idx: 90 |  Loss: (1.2394) |  Loss2: (0.0000) | Acc: (55.00%) (6494/11648)
Epoch: 9 | Batch_idx: 100 |  Loss: (1.2389) |  Loss2: (0.0000) | Acc: (55.00%) (7210/12928)
Epoch: 9 | Batch_idx: 110 |  Loss: (1.2395) |  Loss2: (0.0000) | Acc: (55.00%) (7934/14208)
Epoch: 9 | Batch_idx: 120 |  Loss: (1.2417) |  Loss2: (0.0000) | Acc: (55.00%) (8637/15488)
Epoch: 9 | Batch_idx: 130 |  Loss: (1.2414) |  Loss2: (0.0000) | Acc: (55.00%) (9371/16768)
Epoch: 9 | Batch_idx: 140 |  Loss: (1.2401) |  Loss2: (0.0000) | Acc: (55.00%) (10092/18048)
Epoch: 9 | Batch_idx: 150 |  Loss: (1.2395) |  Loss2: (0.0000) | Acc: (55.00%) (10797/19328)
Epoch: 9 | Batch_idx: 160 |  Loss: (1.2364) |  Loss2: (0.0000) | Acc: (56.00%) (11544/20608)
Epoch: 9 | Batch_idx: 170 |  Loss: (1.2343) |  Loss2: (0.0000) | Acc: (55.00%) (12255/21888)
Epoch: 9 | Batch_idx: 180 |  Loss: (1.2356) |  Loss2: (0.0000) | Acc: (55.00%) (12942/23168)
Epoch: 9 | Batch_idx: 190 |  Loss: (1.2322) |  Loss2: (0.0000) | Acc: (56.00%) (13716/24448)
Epoch: 9 | Batch_idx: 200 |  Loss: (1.2302) |  Loss2: (0.0000) | Acc: (56.00%) (14460/25728)
Epoch: 9 | Batch_idx: 210 |  Loss: (1.2290) |  Loss2: (0.0000) | Acc: (56.00%) (15195/27008)
Epoch: 9 | Batch_idx: 220 |  Loss: (1.2269) |  Loss2: (0.0000) | Acc: (56.00%) (15940/28288)
Epoch: 9 | Batch_idx: 230 |  Loss: (1.2250) |  Loss2: (0.0000) | Acc: (56.00%) (16675/29568)
Epoch: 9 | Batch_idx: 240 |  Loss: (1.2230) |  Loss2: (0.0000) | Acc: (56.00%) (17428/30848)
Epoch: 9 | Batch_idx: 250 |  Loss: (1.2217) |  Loss2: (0.0000) | Acc: (56.00%) (18158/32128)
Epoch: 9 | Batch_idx: 260 |  Loss: (1.2189) |  Loss2: (0.0000) | Acc: (56.00%) (18920/33408)
Epoch: 9 | Batch_idx: 270 |  Loss: (1.2162) |  Loss2: (0.0000) | Acc: (56.00%) (19674/34688)
Epoch: 9 | Batch_idx: 280 |  Loss: (1.2139) |  Loss2: (0.0000) | Acc: (56.00%) (20438/35968)
Epoch: 9 | Batch_idx: 290 |  Loss: (1.2112) |  Loss2: (0.0000) | Acc: (56.00%) (21205/37248)
Epoch: 9 | Batch_idx: 300 |  Loss: (1.2090) |  Loss2: (0.0000) | Acc: (57.00%) (21972/38528)
Epoch: 9 | Batch_idx: 310 |  Loss: (1.2054) |  Loss2: (0.0000) | Acc: (57.00%) (22737/39808)
Epoch: 9 | Batch_idx: 320 |  Loss: (1.2023) |  Loss2: (0.0000) | Acc: (57.00%) (23528/41088)
Epoch: 9 | Batch_idx: 330 |  Loss: (1.2010) |  Loss2: (0.0000) | Acc: (57.00%) (24266/42368)
Epoch: 9 | Batch_idx: 340 |  Loss: (1.1998) |  Loss2: (0.0000) | Acc: (57.00%) (25031/43648)
Epoch: 9 | Batch_idx: 350 |  Loss: (1.1991) |  Loss2: (0.0000) | Acc: (57.00%) (25767/44928)
Epoch: 9 | Batch_idx: 360 |  Loss: (1.1961) |  Loss2: (0.0000) | Acc: (57.00%) (26577/46208)
Epoch: 9 | Batch_idx: 370 |  Loss: (1.1952) |  Loss2: (0.0000) | Acc: (57.00%) (27316/47488)
Epoch: 9 | Batch_idx: 380 |  Loss: (1.1940) |  Loss2: (0.0000) | Acc: (57.00%) (28048/48768)
Epoch: 9 | Batch_idx: 390 |  Loss: (1.1913) |  Loss2: (0.0000) | Acc: (57.00%) (28796/50000)
# TEST : Loss: (1.1114) | Acc: (60.00%) (6002/10000)
percent tensor([0.4977, 0.4968, 0.4963, 0.4961, 0.4956, 0.4971, 0.4972, 0.4963, 0.4977,
        0.4975, 0.4986, 0.4967, 0.4981, 0.4957, 0.4975, 0.4979],
       device='cuda:0') torch.Size([16])
percent tensor([0.4744, 0.4606, 0.4528, 0.4708, 0.4562, 0.4725, 0.4568, 0.4610, 0.4654,
        0.4635, 0.4670, 0.4567, 0.4708, 0.4684, 0.4667, 0.4726],
       device='cuda:0') torch.Size([16])
percent tensor([0.4941, 0.4952, 0.4589, 0.4858, 0.4704, 0.4947, 0.4891, 0.4795, 0.4937,
        0.4871, 0.4942, 0.4674, 0.4927, 0.4974, 0.4943, 0.4955],
       device='cuda:0') torch.Size([16])
percent tensor([0.4873, 0.4809, 0.4888, 0.4923, 0.4876, 0.4898, 0.4823, 0.4868, 0.4859,
        0.4808, 0.4817, 0.4877, 0.4848, 0.4845, 0.4829, 0.4867],
       device='cuda:0') torch.Size([16])
percent tensor([0.5197, 0.5155, 0.5285, 0.5207, 0.5240, 0.5190, 0.5175, 0.5268, 0.5164,
        0.5187, 0.5140, 0.5195, 0.5141, 0.5135, 0.5155, 0.5197],
       device='cuda:0') torch.Size([16])
percent tensor([0.5083, 0.5063, 0.5193, 0.5211, 0.5193, 0.5183, 0.5117, 0.5183, 0.5065,
        0.5044, 0.5018, 0.5122, 0.5038, 0.5083, 0.5100, 0.5096],
       device='cuda:0') torch.Size([16])
percent tensor([0.5291, 0.5200, 0.5245, 0.5165, 0.5135, 0.5540, 0.5183, 0.5205, 0.5222,
        0.5198, 0.5286, 0.5198, 0.5303, 0.5276, 0.5190, 0.5398],
       device='cuda:0') torch.Size([16])
percent tensor([0.9652, 0.9629, 0.9682, 0.9658, 0.9845, 0.9862, 0.9596, 0.9897, 0.9607,
        0.9825, 0.9807, 0.9530, 0.9790, 0.9682, 0.9780, 0.9916],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 10 | Batch_idx: 0 |  Loss: (1.1025) |  Loss2: (0.0000) | Acc: (60.00%) (78/128)
Epoch: 10 | Batch_idx: 10 |  Loss: (1.0393) |  Loss2: (0.0000) | Acc: (62.00%) (887/1408)
Epoch: 10 | Batch_idx: 20 |  Loss: (1.0530) |  Loss2: (0.0000) | Acc: (62.00%) (1677/2688)
Epoch: 10 | Batch_idx: 30 |  Loss: (1.0462) |  Loss2: (0.0000) | Acc: (62.00%) (2479/3968)
Epoch: 10 | Batch_idx: 40 |  Loss: (1.0512) |  Loss2: (0.0000) | Acc: (62.00%) (3277/5248)
Epoch: 10 | Batch_idx: 50 |  Loss: (1.0488) |  Loss2: (0.0000) | Acc: (62.00%) (4078/6528)
Epoch: 10 | Batch_idx: 60 |  Loss: (1.0535) |  Loss2: (0.0000) | Acc: (62.00%) (4854/7808)
Epoch: 10 | Batch_idx: 70 |  Loss: (1.0585) |  Loss2: (0.0000) | Acc: (61.00%) (5625/9088)
Epoch: 10 | Batch_idx: 80 |  Loss: (1.0572) |  Loss2: (0.0000) | Acc: (62.00%) (6441/10368)
Epoch: 10 | Batch_idx: 90 |  Loss: (1.0598) |  Loss2: (0.0000) | Acc: (61.00%) (7220/11648)
Epoch: 10 | Batch_idx: 100 |  Loss: (1.0558) |  Loss2: (0.0000) | Acc: (61.00%) (8013/12928)
Epoch: 10 | Batch_idx: 110 |  Loss: (1.0514) |  Loss2: (0.0000) | Acc: (62.00%) (8814/14208)
Epoch: 10 | Batch_idx: 120 |  Loss: (1.0483) |  Loss2: (0.0000) | Acc: (62.00%) (9636/15488)
Epoch: 10 | Batch_idx: 130 |  Loss: (1.0489) |  Loss2: (0.0000) | Acc: (62.00%) (10449/16768)
Epoch: 10 | Batch_idx: 140 |  Loss: (1.0494) |  Loss2: (0.0000) | Acc: (62.00%) (11249/18048)
Epoch: 10 | Batch_idx: 150 |  Loss: (1.0483) |  Loss2: (0.0000) | Acc: (62.00%) (12064/19328)
Epoch: 10 | Batch_idx: 160 |  Loss: (1.0493) |  Loss2: (0.0000) | Acc: (62.00%) (12868/20608)
Epoch: 10 | Batch_idx: 170 |  Loss: (1.0509) |  Loss2: (0.0000) | Acc: (62.00%) (13662/21888)
Epoch: 10 | Batch_idx: 180 |  Loss: (1.0507) |  Loss2: (0.0000) | Acc: (62.00%) (14451/23168)
Epoch: 10 | Batch_idx: 190 |  Loss: (1.0502) |  Loss2: (0.0000) | Acc: (62.00%) (15250/24448)
Epoch: 10 | Batch_idx: 200 |  Loss: (1.0509) |  Loss2: (0.0000) | Acc: (62.00%) (16036/25728)
Epoch: 10 | Batch_idx: 210 |  Loss: (1.0529) |  Loss2: (0.0000) | Acc: (62.00%) (16813/27008)
Epoch: 10 | Batch_idx: 220 |  Loss: (1.0514) |  Loss2: (0.0000) | Acc: (62.00%) (17628/28288)
Epoch: 10 | Batch_idx: 230 |  Loss: (1.0492) |  Loss2: (0.0000) | Acc: (62.00%) (18453/29568)
Epoch: 10 | Batch_idx: 240 |  Loss: (1.0488) |  Loss2: (0.0000) | Acc: (62.00%) (19250/30848)
Epoch: 10 | Batch_idx: 250 |  Loss: (1.0465) |  Loss2: (0.0000) | Acc: (62.00%) (20079/32128)
Epoch: 10 | Batch_idx: 260 |  Loss: (1.0482) |  Loss2: (0.0000) | Acc: (62.00%) (20873/33408)
Epoch: 10 | Batch_idx: 270 |  Loss: (1.0473) |  Loss2: (0.0000) | Acc: (62.00%) (21670/34688)
Epoch: 10 | Batch_idx: 280 |  Loss: (1.0454) |  Loss2: (0.0000) | Acc: (62.00%) (22483/35968)
Epoch: 10 | Batch_idx: 290 |  Loss: (1.0439) |  Loss2: (0.0000) | Acc: (62.00%) (23308/37248)
Epoch: 10 | Batch_idx: 300 |  Loss: (1.0419) |  Loss2: (0.0000) | Acc: (62.00%) (24155/38528)
Epoch: 10 | Batch_idx: 310 |  Loss: (1.0407) |  Loss2: (0.0000) | Acc: (62.00%) (24964/39808)
Epoch: 10 | Batch_idx: 320 |  Loss: (1.0403) |  Loss2: (0.0000) | Acc: (62.00%) (25774/41088)
Epoch: 10 | Batch_idx: 330 |  Loss: (1.0383) |  Loss2: (0.0000) | Acc: (62.00%) (26616/42368)
Epoch: 10 | Batch_idx: 340 |  Loss: (1.0391) |  Loss2: (0.0000) | Acc: (62.00%) (27420/43648)
Epoch: 10 | Batch_idx: 350 |  Loss: (1.0372) |  Loss2: (0.0000) | Acc: (62.00%) (28246/44928)
Epoch: 10 | Batch_idx: 360 |  Loss: (1.0356) |  Loss2: (0.0000) | Acc: (62.00%) (29068/46208)
Epoch: 10 | Batch_idx: 370 |  Loss: (1.0339) |  Loss2: (0.0000) | Acc: (62.00%) (29893/47488)
Epoch: 10 | Batch_idx: 380 |  Loss: (1.0348) |  Loss2: (0.0000) | Acc: (62.00%) (30687/48768)
Epoch: 10 | Batch_idx: 390 |  Loss: (1.0335) |  Loss2: (0.0000) | Acc: (62.00%) (31489/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_010.pth.tar'
# TEST : Loss: (1.0758) | Acc: (61.00%) (6166/10000)
percent tensor([0.4978, 0.4974, 0.4970, 0.4969, 0.4961, 0.4973, 0.4975, 0.4967, 0.4980,
        0.4978, 0.4987, 0.4969, 0.4984, 0.4964, 0.4977, 0.4981],
       device='cuda:0') torch.Size([16])
percent tensor([0.4727, 0.4619, 0.4538, 0.4699, 0.4556, 0.4714, 0.4572, 0.4613, 0.4651,
        0.4636, 0.4671, 0.4560, 0.4694, 0.4703, 0.4656, 0.4725],
       device='cuda:0') torch.Size([16])
percent tensor([0.4932, 0.4953, 0.4609, 0.4841, 0.4729, 0.4939, 0.4910, 0.4806, 0.4939,
        0.4863, 0.4942, 0.4690, 0.4916, 0.4980, 0.4937, 0.4947],
       device='cuda:0') torch.Size([16])
percent tensor([0.4866, 0.4809, 0.4856, 0.4901, 0.4849, 0.4897, 0.4805, 0.4863, 0.4845,
        0.4787, 0.4813, 0.4842, 0.4824, 0.4840, 0.4815, 0.4859],
       device='cuda:0') torch.Size([16])
percent tensor([0.5194, 0.5155, 0.5276, 0.5234, 0.5223, 0.5190, 0.5170, 0.5276, 0.5157,
        0.5200, 0.5142, 0.5204, 0.5135, 0.5137, 0.5159, 0.5189],
       device='cuda:0') torch.Size([16])
percent tensor([0.5078, 0.5053, 0.5198, 0.5218, 0.5189, 0.5190, 0.5115, 0.5192, 0.5065,
        0.5052, 0.5034, 0.5165, 0.5040, 0.5079, 0.5114, 0.5110],
       device='cuda:0') torch.Size([16])
percent tensor([0.5329, 0.5194, 0.5207, 0.5218, 0.5158, 0.5506, 0.5155, 0.5202, 0.5225,
        0.5103, 0.5267, 0.5167, 0.5284, 0.5272, 0.5168, 0.5371],
       device='cuda:0') torch.Size([16])
percent tensor([0.9773, 0.9728, 0.9784, 0.9698, 0.9713, 0.9818, 0.9648, 0.9890, 0.9811,
        0.9905, 0.9862, 0.9783, 0.9879, 0.9669, 0.9839, 0.9896],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(165.8512, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(776.4591, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(771.5581, device='cuda:0')
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1530.0859, device='cuda:0')
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(510.0977, device='cuda:0')
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2165.4614, device='cuda:0')
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4330.0273, device='cuda:0')
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1445.7096, device='cuda:0')
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6113.4019, device='cuda:0')
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12220.0557, device='cuda:0')
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4074.6914, device='cuda:0')
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(17271.0684, device='cuda:0')
Epoch: 11 | Batch_idx: 0 |  Loss: (1.0188) |  Loss2: (0.0000) | Acc: (64.00%) (83/128)
Epoch: 11 | Batch_idx: 10 |  Loss: (1.0508) |  Loss2: (0.0000) | Acc: (62.00%) (877/1408)
Epoch: 11 | Batch_idx: 20 |  Loss: (1.0836) |  Loss2: (0.0000) | Acc: (61.00%) (1643/2688)
Epoch: 11 | Batch_idx: 30 |  Loss: (1.1231) |  Loss2: (0.0000) | Acc: (59.00%) (2359/3968)
Epoch: 11 | Batch_idx: 40 |  Loss: (1.1295) |  Loss2: (0.0000) | Acc: (59.00%) (3098/5248)
Epoch: 11 | Batch_idx: 50 |  Loss: (1.1577) |  Loss2: (0.0000) | Acc: (57.00%) (3775/6528)
Epoch: 11 | Batch_idx: 60 |  Loss: (1.1604) |  Loss2: (0.0000) | Acc: (57.00%) (4514/7808)
Epoch: 11 | Batch_idx: 70 |  Loss: (1.1631) |  Loss2: (0.0000) | Acc: (57.00%) (5246/9088)
Epoch: 11 | Batch_idx: 80 |  Loss: (1.1682) |  Loss2: (0.0000) | Acc: (57.00%) (5974/10368)
Epoch: 11 | Batch_idx: 90 |  Loss: (1.1656) |  Loss2: (0.0000) | Acc: (57.00%) (6719/11648)
Epoch: 11 | Batch_idx: 100 |  Loss: (1.1692) |  Loss2: (0.0000) | Acc: (57.00%) (7437/12928)
Epoch: 11 | Batch_idx: 110 |  Loss: (1.1670) |  Loss2: (0.0000) | Acc: (57.00%) (8181/14208)
Epoch: 11 | Batch_idx: 120 |  Loss: (1.1674) |  Loss2: (0.0000) | Acc: (57.00%) (8914/15488)
Epoch: 11 | Batch_idx: 130 |  Loss: (1.1659) |  Loss2: (0.0000) | Acc: (57.00%) (9645/16768)
Epoch: 11 | Batch_idx: 140 |  Loss: (1.1642) |  Loss2: (0.0000) | Acc: (57.00%) (10401/18048)
Epoch: 11 | Batch_idx: 150 |  Loss: (1.1632) |  Loss2: (0.0000) | Acc: (57.00%) (11146/19328)
Epoch: 11 | Batch_idx: 160 |  Loss: (1.1639) |  Loss2: (0.0000) | Acc: (57.00%) (11882/20608)
Epoch: 11 | Batch_idx: 170 |  Loss: (1.1622) |  Loss2: (0.0000) | Acc: (57.00%) (12653/21888)
Epoch: 11 | Batch_idx: 180 |  Loss: (1.1629) |  Loss2: (0.0000) | Acc: (57.00%) (13409/23168)
Epoch: 11 | Batch_idx: 190 |  Loss: (1.1611) |  Loss2: (0.0000) | Acc: (57.00%) (14171/24448)
Epoch: 11 | Batch_idx: 200 |  Loss: (1.1605) |  Loss2: (0.0000) | Acc: (57.00%) (14914/25728)
Epoch: 11 | Batch_idx: 210 |  Loss: (1.1563) |  Loss2: (0.0000) | Acc: (58.00%) (15686/27008)
Epoch: 11 | Batch_idx: 220 |  Loss: (1.1551) |  Loss2: (0.0000) | Acc: (58.00%) (16440/28288)
Epoch: 11 | Batch_idx: 230 |  Loss: (1.1529) |  Loss2: (0.0000) | Acc: (58.00%) (17213/29568)
Epoch: 11 | Batch_idx: 240 |  Loss: (1.1494) |  Loss2: (0.0000) | Acc: (58.00%) (17998/30848)
Epoch: 11 | Batch_idx: 250 |  Loss: (1.1465) |  Loss2: (0.0000) | Acc: (58.00%) (18785/32128)
Epoch: 11 | Batch_idx: 260 |  Loss: (1.1441) |  Loss2: (0.0000) | Acc: (58.00%) (19571/33408)
Epoch: 11 | Batch_idx: 270 |  Loss: (1.1410) |  Loss2: (0.0000) | Acc: (58.00%) (20347/34688)
Epoch: 11 | Batch_idx: 280 |  Loss: (1.1389) |  Loss2: (0.0000) | Acc: (58.00%) (21114/35968)
Epoch: 11 | Batch_idx: 290 |  Loss: (1.1380) |  Loss2: (0.0000) | Acc: (58.00%) (21901/37248)
Epoch: 11 | Batch_idx: 300 |  Loss: (1.1379) |  Loss2: (0.0000) | Acc: (58.00%) (22642/38528)
Epoch: 11 | Batch_idx: 310 |  Loss: (1.1367) |  Loss2: (0.0000) | Acc: (58.00%) (23425/39808)
Epoch: 11 | Batch_idx: 320 |  Loss: (1.1341) |  Loss2: (0.0000) | Acc: (58.00%) (24207/41088)
Epoch: 11 | Batch_idx: 330 |  Loss: (1.1322) |  Loss2: (0.0000) | Acc: (58.00%) (24985/42368)
Epoch: 11 | Batch_idx: 340 |  Loss: (1.1302) |  Loss2: (0.0000) | Acc: (59.00%) (25778/43648)
Epoch: 11 | Batch_idx: 350 |  Loss: (1.1295) |  Loss2: (0.0000) | Acc: (59.00%) (26541/44928)
Epoch: 11 | Batch_idx: 360 |  Loss: (1.1280) |  Loss2: (0.0000) | Acc: (59.00%) (27325/46208)
Epoch: 11 | Batch_idx: 370 |  Loss: (1.1262) |  Loss2: (0.0000) | Acc: (59.00%) (28108/47488)
Epoch: 11 | Batch_idx: 380 |  Loss: (1.1245) |  Loss2: (0.0000) | Acc: (59.00%) (28900/48768)
Epoch: 11 | Batch_idx: 390 |  Loss: (1.1214) |  Loss2: (0.0000) | Acc: (59.00%) (29693/50000)
# TEST : Loss: (1.0495) | Acc: (62.00%) (6203/10000)
percent tensor([0.4860, 0.4861, 0.4619, 0.4757, 0.4650, 0.4864, 0.4784, 0.4723, 0.4837,
        0.4799, 0.4920, 0.4652, 0.4871, 0.4914, 0.4842, 0.4872],
       device='cuda:0') torch.Size([16])
percent tensor([0.4621, 0.4380, 0.4185, 0.4514, 0.4223, 0.4562, 0.4272, 0.4335, 0.4431,
        0.4402, 0.4491, 0.4228, 0.4559, 0.4545, 0.4444, 0.4598],
       device='cuda:0') torch.Size([16])
percent tensor([0.4961, 0.4976, 0.4657, 0.4872, 0.4776, 0.4926, 0.4938, 0.4826, 0.4964,
        0.4902, 0.4968, 0.4742, 0.4957, 0.4995, 0.4956, 0.4965],
       device='cuda:0') torch.Size([16])
percent tensor([0.4995, 0.4899, 0.4837, 0.4933, 0.4879, 0.5074, 0.4876, 0.4843, 0.4919,
        0.4852, 0.4924, 0.4840, 0.4936, 0.4924, 0.4930, 0.4969],
       device='cuda:0') torch.Size([16])
percent tensor([0.5280, 0.5189, 0.5202, 0.5245, 0.5189, 0.5369, 0.5178, 0.5219, 0.5182,
        0.5166, 0.5191, 0.5136, 0.5169, 0.5201, 0.5206, 0.5276],
       device='cuda:0') torch.Size([16])
percent tensor([0.4995, 0.4939, 0.5065, 0.5111, 0.5079, 0.5127, 0.4986, 0.5047, 0.4941,
        0.4922, 0.4917, 0.5023, 0.4921, 0.4952, 0.4994, 0.5029],
       device='cuda:0') torch.Size([16])
percent tensor([0.5240, 0.5058, 0.5215, 0.5227, 0.5140, 0.5419, 0.5050, 0.5081, 0.5057,
        0.4896, 0.5074, 0.5027, 0.5090, 0.5132, 0.4982, 0.5261],
       device='cuda:0') torch.Size([16])
percent tensor([0.9813, 0.9737, 0.9781, 0.9745, 0.9777, 0.9859, 0.9680, 0.9921, 0.9768,
        0.9926, 0.9800, 0.9753, 0.9846, 0.9677, 0.9800, 0.9907],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 12 | Batch_idx: 0 |  Loss: (1.1447) |  Loss2: (0.0000) | Acc: (57.00%) (73/128)
Epoch: 12 | Batch_idx: 10 |  Loss: (0.9966) |  Loss2: (0.0000) | Acc: (66.00%) (932/1408)
Epoch: 12 | Batch_idx: 20 |  Loss: (0.9835) |  Loss2: (0.0000) | Acc: (65.00%) (1751/2688)
Epoch: 12 | Batch_idx: 30 |  Loss: (0.9967) |  Loss2: (0.0000) | Acc: (64.00%) (2553/3968)
Epoch: 12 | Batch_idx: 40 |  Loss: (0.9940) |  Loss2: (0.0000) | Acc: (64.00%) (3387/5248)
Epoch: 12 | Batch_idx: 50 |  Loss: (0.9914) |  Loss2: (0.0000) | Acc: (64.00%) (4226/6528)
Epoch: 12 | Batch_idx: 60 |  Loss: (0.9851) |  Loss2: (0.0000) | Acc: (64.00%) (5061/7808)
Epoch: 12 | Batch_idx: 70 |  Loss: (0.9807) |  Loss2: (0.0000) | Acc: (65.00%) (5920/9088)
Epoch: 12 | Batch_idx: 80 |  Loss: (0.9743) |  Loss2: (0.0000) | Acc: (65.00%) (6775/10368)
Epoch: 12 | Batch_idx: 90 |  Loss: (0.9695) |  Loss2: (0.0000) | Acc: (65.00%) (7615/11648)
Epoch: 12 | Batch_idx: 100 |  Loss: (0.9720) |  Loss2: (0.0000) | Acc: (65.00%) (8439/12928)
Epoch: 12 | Batch_idx: 110 |  Loss: (0.9693) |  Loss2: (0.0000) | Acc: (65.00%) (9289/14208)
Epoch: 12 | Batch_idx: 120 |  Loss: (0.9704) |  Loss2: (0.0000) | Acc: (65.00%) (10118/15488)
Epoch: 12 | Batch_idx: 130 |  Loss: (0.9701) |  Loss2: (0.0000) | Acc: (65.00%) (10967/16768)
Epoch: 12 | Batch_idx: 140 |  Loss: (0.9656) |  Loss2: (0.0000) | Acc: (65.00%) (11836/18048)
Epoch: 12 | Batch_idx: 150 |  Loss: (0.9658) |  Loss2: (0.0000) | Acc: (65.00%) (12679/19328)
Epoch: 12 | Batch_idx: 160 |  Loss: (0.9661) |  Loss2: (0.0000) | Acc: (65.00%) (13515/20608)
Epoch: 12 | Batch_idx: 170 |  Loss: (0.9678) |  Loss2: (0.0000) | Acc: (65.00%) (14349/21888)
Epoch: 12 | Batch_idx: 180 |  Loss: (0.9697) |  Loss2: (0.0000) | Acc: (65.00%) (15178/23168)
Epoch: 12 | Batch_idx: 190 |  Loss: (0.9696) |  Loss2: (0.0000) | Acc: (65.00%) (15983/24448)
Epoch: 12 | Batch_idx: 200 |  Loss: (0.9684) |  Loss2: (0.0000) | Acc: (65.00%) (16832/25728)
Epoch: 12 | Batch_idx: 210 |  Loss: (0.9659) |  Loss2: (0.0000) | Acc: (65.00%) (17694/27008)
Epoch: 12 | Batch_idx: 220 |  Loss: (0.9640) |  Loss2: (0.0000) | Acc: (65.00%) (18565/28288)
Epoch: 12 | Batch_idx: 230 |  Loss: (0.9620) |  Loss2: (0.0000) | Acc: (65.00%) (19426/29568)
Epoch: 12 | Batch_idx: 240 |  Loss: (0.9619) |  Loss2: (0.0000) | Acc: (65.00%) (20271/30848)
Epoch: 12 | Batch_idx: 250 |  Loss: (0.9608) |  Loss2: (0.0000) | Acc: (65.00%) (21120/32128)
Epoch: 12 | Batch_idx: 260 |  Loss: (0.9612) |  Loss2: (0.0000) | Acc: (65.00%) (21944/33408)
Epoch: 12 | Batch_idx: 270 |  Loss: (0.9601) |  Loss2: (0.0000) | Acc: (65.00%) (22806/34688)
Epoch: 12 | Batch_idx: 280 |  Loss: (0.9590) |  Loss2: (0.0000) | Acc: (65.00%) (23665/35968)
Epoch: 12 | Batch_idx: 290 |  Loss: (0.9576) |  Loss2: (0.0000) | Acc: (65.00%) (24544/37248)
Epoch: 12 | Batch_idx: 300 |  Loss: (0.9559) |  Loss2: (0.0000) | Acc: (65.00%) (25407/38528)
Epoch: 12 | Batch_idx: 310 |  Loss: (0.9547) |  Loss2: (0.0000) | Acc: (65.00%) (26257/39808)
Epoch: 12 | Batch_idx: 320 |  Loss: (0.9534) |  Loss2: (0.0000) | Acc: (66.00%) (27121/41088)
Epoch: 12 | Batch_idx: 330 |  Loss: (0.9554) |  Loss2: (0.0000) | Acc: (65.00%) (27936/42368)
Epoch: 12 | Batch_idx: 340 |  Loss: (0.9551) |  Loss2: (0.0000) | Acc: (65.00%) (28794/43648)
Epoch: 12 | Batch_idx: 350 |  Loss: (0.9539) |  Loss2: (0.0000) | Acc: (65.00%) (29649/44928)
Epoch: 12 | Batch_idx: 360 |  Loss: (0.9531) |  Loss2: (0.0000) | Acc: (66.00%) (30500/46208)
Epoch: 12 | Batch_idx: 370 |  Loss: (0.9520) |  Loss2: (0.0000) | Acc: (66.00%) (31371/47488)
Epoch: 12 | Batch_idx: 380 |  Loss: (0.9503) |  Loss2: (0.0000) | Acc: (66.00%) (32224/48768)
Epoch: 12 | Batch_idx: 390 |  Loss: (0.9499) |  Loss2: (0.0000) | Acc: (66.00%) (33035/50000)
# TEST : Loss: (0.9754) | Acc: (65.00%) (6512/10000)
percent tensor([0.4856, 0.4859, 0.4635, 0.4755, 0.4668, 0.4857, 0.4796, 0.4715, 0.4846,
        0.4810, 0.4920, 0.4682, 0.4869, 0.4910, 0.4838, 0.4861],
       device='cuda:0') torch.Size([16])
percent tensor([0.4584, 0.4380, 0.4258, 0.4524, 0.4266, 0.4523, 0.4290, 0.4377, 0.4461,
        0.4408, 0.4496, 0.4269, 0.4539, 0.4566, 0.4427, 0.4559],
       device='cuda:0') torch.Size([16])
percent tensor([0.4959, 0.4977, 0.4672, 0.4873, 0.4777, 0.4925, 0.4937, 0.4845, 0.4968,
        0.4909, 0.4970, 0.4763, 0.4960, 0.4998, 0.4954, 0.4963],
       device='cuda:0') torch.Size([16])
percent tensor([0.4980, 0.4906, 0.4885, 0.4927, 0.4890, 0.5041, 0.4889, 0.4873, 0.4950,
        0.4883, 0.4936, 0.4889, 0.4948, 0.4932, 0.4915, 0.4958],
       device='cuda:0') torch.Size([16])
percent tensor([0.5273, 0.5185, 0.5203, 0.5251, 0.5194, 0.5347, 0.5176, 0.5231, 0.5195,
        0.5165, 0.5203, 0.5127, 0.5170, 0.5202, 0.5197, 0.5275],
       device='cuda:0') torch.Size([16])
percent tensor([0.4988, 0.4965, 0.5063, 0.5104, 0.5067, 0.5102, 0.5004, 0.5054, 0.4973,
        0.4925, 0.4936, 0.5013, 0.4920, 0.5008, 0.5000, 0.5014],
       device='cuda:0') torch.Size([16])
percent tensor([0.5222, 0.5152, 0.5138, 0.5205, 0.5103, 0.5340, 0.5128, 0.5107, 0.5080,
        0.5056, 0.5125, 0.5102, 0.5142, 0.5237, 0.5036, 0.5238],
       device='cuda:0') torch.Size([16])
percent tensor([0.9851, 0.9726, 0.9763, 0.9781, 0.9860, 0.9796, 0.9737, 0.9931, 0.9713,
        0.9888, 0.9804, 0.9826, 0.9850, 0.9754, 0.9730, 0.9921],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 13 | Batch_idx: 0 |  Loss: (0.8593) |  Loss2: (0.0000) | Acc: (64.00%) (82/128)
Epoch: 13 | Batch_idx: 10 |  Loss: (1.0026) |  Loss2: (0.0000) | Acc: (63.00%) (894/1408)
Epoch: 13 | Batch_idx: 20 |  Loss: (1.0849) |  Loss2: (0.0000) | Acc: (60.00%) (1625/2688)
Epoch: 13 | Batch_idx: 30 |  Loss: (1.1443) |  Loss2: (0.0000) | Acc: (58.00%) (2302/3968)
Epoch: 13 | Batch_idx: 40 |  Loss: (1.1696) |  Loss2: (0.0000) | Acc: (57.00%) (3024/5248)
Epoch: 13 | Batch_idx: 50 |  Loss: (1.1856) |  Loss2: (0.0000) | Acc: (57.00%) (3737/6528)
Epoch: 13 | Batch_idx: 60 |  Loss: (1.1898) |  Loss2: (0.0000) | Acc: (57.00%) (4463/7808)
Epoch: 13 | Batch_idx: 70 |  Loss: (1.1973) |  Loss2: (0.0000) | Acc: (56.00%) (5169/9088)
Epoch: 13 | Batch_idx: 80 |  Loss: (1.2029) |  Loss2: (0.0000) | Acc: (56.00%) (5880/10368)
Epoch: 13 | Batch_idx: 90 |  Loss: (1.1967) |  Loss2: (0.0000) | Acc: (56.00%) (6634/11648)
Epoch: 13 | Batch_idx: 100 |  Loss: (1.1939) |  Loss2: (0.0000) | Acc: (56.00%) (7365/12928)
Epoch: 13 | Batch_idx: 110 |  Loss: (1.1860) |  Loss2: (0.0000) | Acc: (57.00%) (8136/14208)
Epoch: 13 | Batch_idx: 120 |  Loss: (1.1788) |  Loss2: (0.0000) | Acc: (57.00%) (8928/15488)
Epoch: 13 | Batch_idx: 130 |  Loss: (1.1732) |  Loss2: (0.0000) | Acc: (57.00%) (9693/16768)
Epoch: 13 | Batch_idx: 140 |  Loss: (1.1638) |  Loss2: (0.0000) | Acc: (58.00%) (10489/18048)
Epoch: 13 | Batch_idx: 150 |  Loss: (1.1590) |  Loss2: (0.0000) | Acc: (58.00%) (11254/19328)
Epoch: 13 | Batch_idx: 160 |  Loss: (1.1534) |  Loss2: (0.0000) | Acc: (58.00%) (12039/20608)
Epoch: 13 | Batch_idx: 170 |  Loss: (1.1514) |  Loss2: (0.0000) | Acc: (58.00%) (12801/21888)
Epoch: 13 | Batch_idx: 180 |  Loss: (1.1438) |  Loss2: (0.0000) | Acc: (58.00%) (13618/23168)
Epoch: 13 | Batch_idx: 190 |  Loss: (1.1397) |  Loss2: (0.0000) | Acc: (58.00%) (14418/24448)
Epoch: 13 | Batch_idx: 200 |  Loss: (1.1323) |  Loss2: (0.0000) | Acc: (59.00%) (15248/25728)
Epoch: 13 | Batch_idx: 210 |  Loss: (1.1280) |  Loss2: (0.0000) | Acc: (59.00%) (16044/27008)
Epoch: 13 | Batch_idx: 220 |  Loss: (1.1217) |  Loss2: (0.0000) | Acc: (59.00%) (16849/28288)
Epoch: 13 | Batch_idx: 230 |  Loss: (1.1153) |  Loss2: (0.0000) | Acc: (59.00%) (17682/29568)
Epoch: 13 | Batch_idx: 240 |  Loss: (1.1122) |  Loss2: (0.0000) | Acc: (59.00%) (18463/30848)
Epoch: 13 | Batch_idx: 250 |  Loss: (1.1091) |  Loss2: (0.0000) | Acc: (59.00%) (19252/32128)
Epoch: 13 | Batch_idx: 260 |  Loss: (1.1076) |  Loss2: (0.0000) | Acc: (59.00%) (20029/33408)
Epoch: 13 | Batch_idx: 270 |  Loss: (1.1046) |  Loss2: (0.0000) | Acc: (60.00%) (20827/34688)
Epoch: 13 | Batch_idx: 280 |  Loss: (1.1018) |  Loss2: (0.0000) | Acc: (60.00%) (21638/35968)
Epoch: 13 | Batch_idx: 290 |  Loss: (1.1004) |  Loss2: (0.0000) | Acc: (60.00%) (22435/37248)
Epoch: 13 | Batch_idx: 300 |  Loss: (1.0951) |  Loss2: (0.0000) | Acc: (60.00%) (23286/38528)
Epoch: 13 | Batch_idx: 310 |  Loss: (1.0919) |  Loss2: (0.0000) | Acc: (60.00%) (24094/39808)
Epoch: 13 | Batch_idx: 320 |  Loss: (1.0885) |  Loss2: (0.0000) | Acc: (60.00%) (24930/41088)
Epoch: 13 | Batch_idx: 330 |  Loss: (1.0857) |  Loss2: (0.0000) | Acc: (60.00%) (25734/42368)
Epoch: 13 | Batch_idx: 340 |  Loss: (1.0827) |  Loss2: (0.0000) | Acc: (60.00%) (26556/43648)
Epoch: 13 | Batch_idx: 350 |  Loss: (1.0792) |  Loss2: (0.0000) | Acc: (60.00%) (27379/44928)
Epoch: 13 | Batch_idx: 360 |  Loss: (1.0767) |  Loss2: (0.0000) | Acc: (61.00%) (28199/46208)
Epoch: 13 | Batch_idx: 370 |  Loss: (1.0737) |  Loss2: (0.0000) | Acc: (61.00%) (29031/47488)
Epoch: 13 | Batch_idx: 380 |  Loss: (1.0715) |  Loss2: (0.0000) | Acc: (61.00%) (29851/48768)
Epoch: 13 | Batch_idx: 390 |  Loss: (1.0697) |  Loss2: (0.0000) | Acc: (61.00%) (30663/50000)
# TEST : Loss: (0.9856) | Acc: (64.00%) (6430/10000)
percent tensor([0.4570, 0.4538, 0.4383, 0.4479, 0.4352, 0.4482, 0.4480, 0.4419, 0.4564,
        0.4541, 0.4648, 0.4413, 0.4611, 0.4618, 0.4504, 0.4576],
       device='cuda:0') torch.Size([16])
percent tensor([0.4188, 0.3936, 0.3923, 0.4152, 0.3871, 0.3983, 0.3859, 0.4024, 0.4023,
        0.4066, 0.4065, 0.3952, 0.4174, 0.4056, 0.3989, 0.4134],
       device='cuda:0') torch.Size([16])
percent tensor([0.4566, 0.4460, 0.3365, 0.4301, 0.3611, 0.4383, 0.4128, 0.4046, 0.4478,
        0.4125, 0.4550, 0.3533, 0.4480, 0.4779, 0.4414, 0.4589],
       device='cuda:0') torch.Size([16])
percent tensor([0.5076, 0.5052, 0.5062, 0.5067, 0.5058, 0.5122, 0.5055, 0.5037, 0.5073,
        0.5063, 0.5064, 0.5084, 0.5074, 0.5040, 0.5048, 0.5079],
       device='cuda:0') torch.Size([16])
percent tensor([0.5418, 0.5321, 0.5299, 0.5343, 0.5274, 0.5502, 0.5319, 0.5343, 0.5354,
        0.5308, 0.5369, 0.5236, 0.5315, 0.5368, 0.5339, 0.5425],
       device='cuda:0') torch.Size([16])
percent tensor([0.5016, 0.4990, 0.5090, 0.5118, 0.5090, 0.5127, 0.5030, 0.5073, 0.5001,
        0.4959, 0.4958, 0.5033, 0.4943, 0.5025, 0.5021, 0.5041],
       device='cuda:0') torch.Size([16])
percent tensor([0.5603, 0.5421, 0.5357, 0.5536, 0.5327, 0.5934, 0.5460, 0.5366, 0.5271,
        0.5285, 0.5362, 0.5292, 0.5414, 0.5533, 0.5271, 0.5761],
       device='cuda:0') torch.Size([16])
percent tensor([0.9880, 0.9769, 0.9799, 0.9788, 0.9843, 0.9822, 0.9824, 0.9964, 0.9732,
        0.9900, 0.9812, 0.9846, 0.9877, 0.9821, 0.9822, 0.9956],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 14 | Batch_idx: 0 |  Loss: (0.9654) |  Loss2: (0.0000) | Acc: (64.00%) (82/128)
Epoch: 14 | Batch_idx: 10 |  Loss: (0.9488) |  Loss2: (0.0000) | Acc: (65.00%) (927/1408)
Epoch: 14 | Batch_idx: 20 |  Loss: (0.9543) |  Loss2: (0.0000) | Acc: (66.00%) (1786/2688)
Epoch: 14 | Batch_idx: 30 |  Loss: (0.9385) |  Loss2: (0.0000) | Acc: (66.00%) (2638/3968)
Epoch: 14 | Batch_idx: 40 |  Loss: (0.9360) |  Loss2: (0.0000) | Acc: (66.00%) (3490/5248)
Epoch: 14 | Batch_idx: 50 |  Loss: (0.9298) |  Loss2: (0.0000) | Acc: (66.00%) (4359/6528)
Epoch: 14 | Batch_idx: 60 |  Loss: (0.9193) |  Loss2: (0.0000) | Acc: (67.00%) (5248/7808)
Epoch: 14 | Batch_idx: 70 |  Loss: (0.9112) |  Loss2: (0.0000) | Acc: (67.00%) (6134/9088)
Epoch: 14 | Batch_idx: 80 |  Loss: (0.9158) |  Loss2: (0.0000) | Acc: (67.00%) (6991/10368)
Epoch: 14 | Batch_idx: 90 |  Loss: (0.9105) |  Loss2: (0.0000) | Acc: (67.00%) (7882/11648)
Epoch: 14 | Batch_idx: 100 |  Loss: (0.9102) |  Loss2: (0.0000) | Acc: (67.00%) (8753/12928)
Epoch: 14 | Batch_idx: 110 |  Loss: (0.9158) |  Loss2: (0.0000) | Acc: (67.00%) (9597/14208)
Epoch: 14 | Batch_idx: 120 |  Loss: (0.9167) |  Loss2: (0.0000) | Acc: (67.00%) (10464/15488)
Epoch: 14 | Batch_idx: 130 |  Loss: (0.9146) |  Loss2: (0.0000) | Acc: (67.00%) (11314/16768)
Epoch: 14 | Batch_idx: 140 |  Loss: (0.9156) |  Loss2: (0.0000) | Acc: (67.00%) (12170/18048)
Epoch: 14 | Batch_idx: 150 |  Loss: (0.9163) |  Loss2: (0.0000) | Acc: (67.00%) (13017/19328)
Epoch: 14 | Batch_idx: 160 |  Loss: (0.9176) |  Loss2: (0.0000) | Acc: (67.00%) (13890/20608)
Epoch: 14 | Batch_idx: 170 |  Loss: (0.9157) |  Loss2: (0.0000) | Acc: (67.00%) (14778/21888)
Epoch: 14 | Batch_idx: 180 |  Loss: (0.9138) |  Loss2: (0.0000) | Acc: (67.00%) (15656/23168)
Epoch: 14 | Batch_idx: 190 |  Loss: (0.9132) |  Loss2: (0.0000) | Acc: (67.00%) (16541/24448)
Epoch: 14 | Batch_idx: 200 |  Loss: (0.9133) |  Loss2: (0.0000) | Acc: (67.00%) (17406/25728)
Epoch: 14 | Batch_idx: 210 |  Loss: (0.9118) |  Loss2: (0.0000) | Acc: (67.00%) (18294/27008)
Epoch: 14 | Batch_idx: 220 |  Loss: (0.9111) |  Loss2: (0.0000) | Acc: (67.00%) (19145/28288)
Epoch: 14 | Batch_idx: 230 |  Loss: (0.9102) |  Loss2: (0.0000) | Acc: (67.00%) (20019/29568)
Epoch: 14 | Batch_idx: 240 |  Loss: (0.9078) |  Loss2: (0.0000) | Acc: (67.00%) (20917/30848)
Epoch: 14 | Batch_idx: 250 |  Loss: (0.9070) |  Loss2: (0.0000) | Acc: (67.00%) (21787/32128)
Epoch: 14 | Batch_idx: 260 |  Loss: (0.9052) |  Loss2: (0.0000) | Acc: (67.00%) (22684/33408)
Epoch: 14 | Batch_idx: 270 |  Loss: (0.9049) |  Loss2: (0.0000) | Acc: (67.00%) (23556/34688)
Epoch: 14 | Batch_idx: 280 |  Loss: (0.9042) |  Loss2: (0.0000) | Acc: (67.00%) (24436/35968)
Epoch: 14 | Batch_idx: 290 |  Loss: (0.9027) |  Loss2: (0.0000) | Acc: (67.00%) (25308/37248)
Epoch: 14 | Batch_idx: 300 |  Loss: (0.9027) |  Loss2: (0.0000) | Acc: (67.00%) (26175/38528)
Epoch: 14 | Batch_idx: 310 |  Loss: (0.9026) |  Loss2: (0.0000) | Acc: (67.00%) (27040/39808)
Epoch: 14 | Batch_idx: 320 |  Loss: (0.9011) |  Loss2: (0.0000) | Acc: (68.00%) (27952/41088)
Epoch: 14 | Batch_idx: 330 |  Loss: (0.8997) |  Loss2: (0.0000) | Acc: (68.00%) (28855/42368)
Epoch: 14 | Batch_idx: 340 |  Loss: (0.8980) |  Loss2: (0.0000) | Acc: (68.00%) (29748/43648)
Epoch: 14 | Batch_idx: 350 |  Loss: (0.8961) |  Loss2: (0.0000) | Acc: (68.00%) (30646/44928)
Epoch: 14 | Batch_idx: 360 |  Loss: (0.8942) |  Loss2: (0.0000) | Acc: (68.00%) (31548/46208)
Epoch: 14 | Batch_idx: 370 |  Loss: (0.8945) |  Loss2: (0.0000) | Acc: (68.00%) (32434/47488)
Epoch: 14 | Batch_idx: 380 |  Loss: (0.8934) |  Loss2: (0.0000) | Acc: (68.00%) (33324/48768)
Epoch: 14 | Batch_idx: 390 |  Loss: (0.8917) |  Loss2: (0.0000) | Acc: (68.00%) (34206/50000)
# TEST : Loss: (0.8960) | Acc: (67.00%) (6796/10000)
percent tensor([0.4579, 0.4555, 0.4418, 0.4494, 0.4370, 0.4482, 0.4500, 0.4455, 0.4584,
        0.4561, 0.4661, 0.4442, 0.4620, 0.4634, 0.4505, 0.4576],
       device='cuda:0') torch.Size([16])
percent tensor([0.4186, 0.3990, 0.3894, 0.4121, 0.3858, 0.4009, 0.3889, 0.4003, 0.4046,
        0.4057, 0.4087, 0.3920, 0.4169, 0.4129, 0.3998, 0.4141],
       device='cuda:0') torch.Size([16])
percent tensor([0.4481, 0.4400, 0.3651, 0.4362, 0.3861, 0.4312, 0.4191, 0.4164, 0.4492,
        0.4133, 0.4471, 0.3746, 0.4429, 0.4707, 0.4336, 0.4477],
       device='cuda:0') torch.Size([16])
percent tensor([0.5082, 0.5060, 0.5073, 0.5068, 0.5059, 0.5146, 0.5053, 0.5052, 0.5083,
        0.5063, 0.5070, 0.5078, 0.5072, 0.5055, 0.5063, 0.5088],
       device='cuda:0') torch.Size([16])
percent tensor([0.5420, 0.5351, 0.5315, 0.5351, 0.5306, 0.5493, 0.5347, 0.5359, 0.5373,
        0.5328, 0.5392, 0.5284, 0.5320, 0.5410, 0.5343, 0.5413],
       device='cuda:0') torch.Size([16])
percent tensor([0.5014, 0.4969, 0.5109, 0.5141, 0.5096, 0.5151, 0.5014, 0.5086, 0.4988,
        0.4951, 0.4946, 0.5057, 0.4947, 0.4994, 0.5026, 0.5054],
       device='cuda:0') torch.Size([16])
percent tensor([0.5624, 0.5316, 0.5634, 0.5582, 0.5448, 0.6008, 0.5420, 0.5569, 0.5329,
        0.5273, 0.5386, 0.5439, 0.5450, 0.5414, 0.5232, 0.5763],
       device='cuda:0') torch.Size([16])
percent tensor([0.9920, 0.9797, 0.9830, 0.9834, 0.9902, 0.9835, 0.9836, 0.9942, 0.9842,
        0.9930, 0.9896, 0.9819, 0.9911, 0.9820, 0.9797, 0.9930],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 15 | Batch_idx: 0 |  Loss: (0.8457) |  Loss2: (0.0000) | Acc: (71.00%) (92/128)
Epoch: 15 | Batch_idx: 10 |  Loss: (0.8337) |  Loss2: (0.0000) | Acc: (70.00%) (992/1408)
Epoch: 15 | Batch_idx: 20 |  Loss: (0.8857) |  Loss2: (0.0000) | Acc: (68.00%) (1846/2688)
Epoch: 15 | Batch_idx: 30 |  Loss: (0.9035) |  Loss2: (0.0000) | Acc: (67.00%) (2692/3968)
Epoch: 15 | Batch_idx: 40 |  Loss: (0.9166) |  Loss2: (0.0000) | Acc: (67.00%) (3525/5248)
Epoch: 15 | Batch_idx: 50 |  Loss: (0.9157) |  Loss2: (0.0000) | Acc: (67.00%) (4396/6528)
Epoch: 15 | Batch_idx: 60 |  Loss: (0.9192) |  Loss2: (0.0000) | Acc: (67.00%) (5240/7808)
Epoch: 15 | Batch_idx: 70 |  Loss: (0.9224) |  Loss2: (0.0000) | Acc: (67.00%) (6105/9088)
Epoch: 15 | Batch_idx: 80 |  Loss: (0.9226) |  Loss2: (0.0000) | Acc: (67.00%) (6989/10368)
Epoch: 15 | Batch_idx: 90 |  Loss: (0.9230) |  Loss2: (0.0000) | Acc: (67.00%) (7826/11648)
Epoch: 15 | Batch_idx: 100 |  Loss: (0.9183) |  Loss2: (0.0000) | Acc: (67.00%) (8699/12928)
Epoch: 15 | Batch_idx: 110 |  Loss: (0.9207) |  Loss2: (0.0000) | Acc: (67.00%) (9551/14208)
Epoch: 15 | Batch_idx: 120 |  Loss: (0.9185) |  Loss2: (0.0000) | Acc: (67.00%) (10425/15488)
Epoch: 15 | Batch_idx: 130 |  Loss: (0.9172) |  Loss2: (0.0000) | Acc: (67.00%) (11283/16768)
Epoch: 15 | Batch_idx: 140 |  Loss: (0.9152) |  Loss2: (0.0000) | Acc: (67.00%) (12159/18048)
Epoch: 15 | Batch_idx: 150 |  Loss: (0.9158) |  Loss2: (0.0000) | Acc: (67.00%) (13026/19328)
Epoch: 15 | Batch_idx: 160 |  Loss: (0.9174) |  Loss2: (0.0000) | Acc: (67.00%) (13881/20608)
Epoch: 15 | Batch_idx: 170 |  Loss: (0.9158) |  Loss2: (0.0000) | Acc: (67.00%) (14764/21888)
Epoch: 15 | Batch_idx: 180 |  Loss: (0.9172) |  Loss2: (0.0000) | Acc: (67.00%) (15598/23168)
Epoch: 15 | Batch_idx: 190 |  Loss: (0.9148) |  Loss2: (0.0000) | Acc: (67.00%) (16468/24448)
Epoch: 15 | Batch_idx: 200 |  Loss: (0.9128) |  Loss2: (0.0000) | Acc: (67.00%) (17345/25728)
Epoch: 15 | Batch_idx: 210 |  Loss: (0.9109) |  Loss2: (0.0000) | Acc: (67.00%) (18227/27008)
Epoch: 15 | Batch_idx: 220 |  Loss: (0.9110) |  Loss2: (0.0000) | Acc: (67.00%) (19093/28288)
Epoch: 15 | Batch_idx: 230 |  Loss: (0.9107) |  Loss2: (0.0000) | Acc: (67.00%) (19976/29568)
Epoch: 15 | Batch_idx: 240 |  Loss: (0.9104) |  Loss2: (0.0000) | Acc: (67.00%) (20837/30848)
Epoch: 15 | Batch_idx: 250 |  Loss: (0.9082) |  Loss2: (0.0000) | Acc: (67.00%) (21706/32128)
Epoch: 15 | Batch_idx: 260 |  Loss: (0.9084) |  Loss2: (0.0000) | Acc: (67.00%) (22575/33408)
Epoch: 15 | Batch_idx: 270 |  Loss: (0.9068) |  Loss2: (0.0000) | Acc: (67.00%) (23468/34688)
Epoch: 15 | Batch_idx: 280 |  Loss: (0.9055) |  Loss2: (0.0000) | Acc: (67.00%) (24363/35968)
Epoch: 15 | Batch_idx: 290 |  Loss: (0.9029) |  Loss2: (0.0000) | Acc: (67.00%) (25253/37248)
Epoch: 15 | Batch_idx: 300 |  Loss: (0.9034) |  Loss2: (0.0000) | Acc: (67.00%) (26115/38528)
Epoch: 15 | Batch_idx: 310 |  Loss: (0.9030) |  Loss2: (0.0000) | Acc: (67.00%) (26993/39808)
Epoch: 15 | Batch_idx: 320 |  Loss: (0.9026) |  Loss2: (0.0000) | Acc: (67.00%) (27880/41088)
Epoch: 15 | Batch_idx: 330 |  Loss: (0.9025) |  Loss2: (0.0000) | Acc: (67.00%) (28753/42368)
Epoch: 15 | Batch_idx: 340 |  Loss: (0.9004) |  Loss2: (0.0000) | Acc: (67.00%) (29661/43648)
Epoch: 15 | Batch_idx: 350 |  Loss: (0.9001) |  Loss2: (0.0000) | Acc: (67.00%) (30535/44928)
Epoch: 15 | Batch_idx: 360 |  Loss: (0.8996) |  Loss2: (0.0000) | Acc: (67.00%) (31410/46208)
Epoch: 15 | Batch_idx: 370 |  Loss: (0.8973) |  Loss2: (0.0000) | Acc: (68.00%) (32344/47488)
Epoch: 15 | Batch_idx: 380 |  Loss: (0.8969) |  Loss2: (0.0000) | Acc: (68.00%) (33226/48768)
Epoch: 15 | Batch_idx: 390 |  Loss: (0.8954) |  Loss2: (0.0000) | Acc: (68.00%) (34094/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_015.pth.tar'
# TEST : Loss: (0.8663) | Acc: (68.00%) (6878/10000)
percent tensor([0.4716, 0.4762, 0.4657, 0.4655, 0.4608, 0.4590, 0.4724, 0.4671, 0.4775,
        0.4753, 0.4806, 0.4680, 0.4771, 0.4833, 0.4667, 0.4707],
       device='cuda:0') torch.Size([16])
percent tensor([0.4502, 0.4385, 0.4389, 0.4480, 0.4323, 0.4288, 0.4332, 0.4437, 0.4445,
        0.4467, 0.4440, 0.4401, 0.4512, 0.4513, 0.4345, 0.4469],
       device='cuda:0') torch.Size([16])
percent tensor([0.4901, 0.4885, 0.4369, 0.4879, 0.4473, 0.4812, 0.4732, 0.4724, 0.4906,
        0.4718, 0.4896, 0.4435, 0.4907, 0.4976, 0.4843, 0.4913],
       device='cuda:0') torch.Size([16])
percent tensor([0.5125, 0.5125, 0.5137, 0.5124, 0.5128, 0.5191, 0.5127, 0.5135, 0.5149,
        0.5132, 0.5132, 0.5146, 0.5124, 0.5125, 0.5127, 0.5134],
       device='cuda:0') torch.Size([16])
percent tensor([0.5508, 0.5448, 0.5528, 0.5513, 0.5518, 0.5592, 0.5498, 0.5597, 0.5476,
        0.5475, 0.5466, 0.5472, 0.5383, 0.5462, 0.5481, 0.5516],
       device='cuda:0') torch.Size([16])
percent tensor([0.5219, 0.5159, 0.5361, 0.5438, 0.5380, 0.5427, 0.5270, 0.5390, 0.5195,
        0.5162, 0.5144, 0.5310, 0.5119, 0.5189, 0.5273, 0.5287],
       device='cuda:0') torch.Size([16])
percent tensor([0.5951, 0.5564, 0.5460, 0.5430, 0.5331, 0.6583, 0.5460, 0.5495, 0.5549,
        0.5426, 0.5707, 0.5380, 0.5809, 0.5743, 0.5407, 0.6115],
       device='cuda:0') torch.Size([16])
percent tensor([0.9940, 0.9817, 0.9887, 0.9875, 0.9947, 0.9893, 0.9825, 0.9958, 0.9880,
        0.9953, 0.9895, 0.9900, 0.9923, 0.9829, 0.9877, 0.9952],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 16 | Batch_idx: 0 |  Loss: (0.9815) |  Loss2: (0.0000) | Acc: (66.00%) (85/128)
Epoch: 16 | Batch_idx: 10 |  Loss: (0.8854) |  Loss2: (0.0000) | Acc: (67.00%) (954/1408)
Epoch: 16 | Batch_idx: 20 |  Loss: (0.8639) |  Loss2: (0.0000) | Acc: (69.00%) (1856/2688)
Epoch: 16 | Batch_idx: 30 |  Loss: (0.8534) |  Loss2: (0.0000) | Acc: (69.00%) (2757/3968)
Epoch: 16 | Batch_idx: 40 |  Loss: (0.8518) |  Loss2: (0.0000) | Acc: (69.00%) (3657/5248)
Epoch: 16 | Batch_idx: 50 |  Loss: (0.8532) |  Loss2: (0.0000) | Acc: (69.00%) (4551/6528)
Epoch: 16 | Batch_idx: 60 |  Loss: (0.8565) |  Loss2: (0.0000) | Acc: (69.00%) (5441/7808)
Epoch: 16 | Batch_idx: 70 |  Loss: (0.8584) |  Loss2: (0.0000) | Acc: (69.00%) (6325/9088)
Epoch: 16 | Batch_idx: 80 |  Loss: (0.8568) |  Loss2: (0.0000) | Acc: (69.00%) (7218/10368)
Epoch: 16 | Batch_idx: 90 |  Loss: (0.8573) |  Loss2: (0.0000) | Acc: (69.00%) (8117/11648)
Epoch: 16 | Batch_idx: 100 |  Loss: (0.8531) |  Loss2: (0.0000) | Acc: (69.00%) (9031/12928)
Epoch: 16 | Batch_idx: 110 |  Loss: (0.8513) |  Loss2: (0.0000) | Acc: (69.00%) (9938/14208)
Epoch: 16 | Batch_idx: 120 |  Loss: (0.8528) |  Loss2: (0.0000) | Acc: (69.00%) (10839/15488)
Epoch: 16 | Batch_idx: 130 |  Loss: (0.8478) |  Loss2: (0.0000) | Acc: (70.00%) (11762/16768)
Epoch: 16 | Batch_idx: 140 |  Loss: (0.8454) |  Loss2: (0.0000) | Acc: (70.00%) (12663/18048)
Epoch: 16 | Batch_idx: 150 |  Loss: (0.8420) |  Loss2: (0.0000) | Acc: (70.00%) (13593/19328)
Epoch: 16 | Batch_idx: 160 |  Loss: (0.8388) |  Loss2: (0.0000) | Acc: (70.00%) (14529/20608)
Epoch: 16 | Batch_idx: 170 |  Loss: (0.8388) |  Loss2: (0.0000) | Acc: (70.00%) (15431/21888)
Epoch: 16 | Batch_idx: 180 |  Loss: (0.8370) |  Loss2: (0.0000) | Acc: (70.00%) (16329/23168)
Epoch: 16 | Batch_idx: 190 |  Loss: (0.8360) |  Loss2: (0.0000) | Acc: (70.00%) (17253/24448)
Epoch: 16 | Batch_idx: 200 |  Loss: (0.8354) |  Loss2: (0.0000) | Acc: (70.00%) (18155/25728)
Epoch: 16 | Batch_idx: 210 |  Loss: (0.8341) |  Loss2: (0.0000) | Acc: (70.00%) (19072/27008)
Epoch: 16 | Batch_idx: 220 |  Loss: (0.8311) |  Loss2: (0.0000) | Acc: (70.00%) (20023/28288)
Epoch: 16 | Batch_idx: 230 |  Loss: (0.8312) |  Loss2: (0.0000) | Acc: (70.00%) (20925/29568)
Epoch: 16 | Batch_idx: 240 |  Loss: (0.8302) |  Loss2: (0.0000) | Acc: (70.00%) (21840/30848)
Epoch: 16 | Batch_idx: 250 |  Loss: (0.8305) |  Loss2: (0.0000) | Acc: (70.00%) (22747/32128)
Epoch: 16 | Batch_idx: 260 |  Loss: (0.8298) |  Loss2: (0.0000) | Acc: (70.00%) (23668/33408)
Epoch: 16 | Batch_idx: 270 |  Loss: (0.8305) |  Loss2: (0.0000) | Acc: (70.00%) (24550/34688)
Epoch: 16 | Batch_idx: 280 |  Loss: (0.8276) |  Loss2: (0.0000) | Acc: (70.00%) (25478/35968)
Epoch: 16 | Batch_idx: 290 |  Loss: (0.8276) |  Loss2: (0.0000) | Acc: (70.00%) (26384/37248)
Epoch: 16 | Batch_idx: 300 |  Loss: (0.8274) |  Loss2: (0.0000) | Acc: (70.00%) (27303/38528)
Epoch: 16 | Batch_idx: 310 |  Loss: (0.8263) |  Loss2: (0.0000) | Acc: (70.00%) (28225/39808)
Epoch: 16 | Batch_idx: 320 |  Loss: (0.8259) |  Loss2: (0.0000) | Acc: (70.00%) (29150/41088)
Epoch: 16 | Batch_idx: 330 |  Loss: (0.8264) |  Loss2: (0.0000) | Acc: (70.00%) (30039/42368)
Epoch: 16 | Batch_idx: 340 |  Loss: (0.8259) |  Loss2: (0.0000) | Acc: (70.00%) (30961/43648)
Epoch: 16 | Batch_idx: 350 |  Loss: (0.8255) |  Loss2: (0.0000) | Acc: (70.00%) (31888/44928)
Epoch: 16 | Batch_idx: 360 |  Loss: (0.8239) |  Loss2: (0.0000) | Acc: (71.00%) (32829/46208)
Epoch: 16 | Batch_idx: 370 |  Loss: (0.8246) |  Loss2: (0.0000) | Acc: (71.00%) (33727/47488)
Epoch: 16 | Batch_idx: 380 |  Loss: (0.8243) |  Loss2: (0.0000) | Acc: (71.00%) (34635/48768)
Epoch: 16 | Batch_idx: 390 |  Loss: (0.8238) |  Loss2: (0.0000) | Acc: (71.00%) (35510/50000)
# TEST : Loss: (0.8905) | Acc: (68.00%) (6824/10000)
percent tensor([0.4730, 0.4751, 0.4647, 0.4662, 0.4600, 0.4618, 0.4712, 0.4655, 0.4779,
        0.4753, 0.4822, 0.4668, 0.4785, 0.4809, 0.4679, 0.4723],
       device='cuda:0') torch.Size([16])
percent tensor([0.4501, 0.4363, 0.4392, 0.4490, 0.4315, 0.4309, 0.4308, 0.4432, 0.4428,
        0.4460, 0.4425, 0.4394, 0.4515, 0.4470, 0.4347, 0.4462],
       device='cuda:0') torch.Size([16])
percent tensor([0.4900, 0.4892, 0.4349, 0.4854, 0.4428, 0.4810, 0.4733, 0.4740, 0.4918,
        0.4718, 0.4902, 0.4428, 0.4909, 0.4993, 0.4844, 0.4910],
       device='cuda:0') torch.Size([16])
percent tensor([0.5122, 0.5129, 0.5159, 0.5132, 0.5143, 0.5184, 0.5128, 0.5133, 0.5149,
        0.5139, 0.5132, 0.5160, 0.5120, 0.5118, 0.5123, 0.5138],
       device='cuda:0') torch.Size([16])
percent tensor([0.5496, 0.5440, 0.5532, 0.5503, 0.5513, 0.5579, 0.5510, 0.5577, 0.5500,
        0.5486, 0.5489, 0.5468, 0.5391, 0.5467, 0.5467, 0.5529],
       device='cuda:0') torch.Size([16])
percent tensor([0.5223, 0.5193, 0.5339, 0.5414, 0.5340, 0.5412, 0.5303, 0.5378, 0.5216,
        0.5182, 0.5163, 0.5286, 0.5133, 0.5270, 0.5290, 0.5296],
       device='cuda:0') torch.Size([16])
percent tensor([0.5917, 0.5544, 0.5441, 0.5560, 0.5281, 0.6511, 0.5438, 0.5448, 0.5435,
        0.5392, 0.5638, 0.5374, 0.5719, 0.5756, 0.5418, 0.6111],
       device='cuda:0') torch.Size([16])
percent tensor([0.9932, 0.9832, 0.9900, 0.9872, 0.9924, 0.9860, 0.9839, 0.9954, 0.9903,
        0.9950, 0.9937, 0.9933, 0.9921, 0.9838, 0.9875, 0.9950],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 17 | Batch_idx: 0 |  Loss: (0.8081) |  Loss2: (0.0000) | Acc: (70.00%) (90/128)
Epoch: 17 | Batch_idx: 10 |  Loss: (0.8597) |  Loss2: (0.0000) | Acc: (69.00%) (981/1408)
Epoch: 17 | Batch_idx: 20 |  Loss: (0.9427) |  Loss2: (0.0000) | Acc: (66.00%) (1786/2688)
Epoch: 17 | Batch_idx: 30 |  Loss: (0.9692) |  Loss2: (0.0000) | Acc: (65.00%) (2605/3968)
Epoch: 17 | Batch_idx: 40 |  Loss: (0.9909) |  Loss2: (0.0000) | Acc: (64.00%) (3407/5248)
Epoch: 17 | Batch_idx: 50 |  Loss: (1.0079) |  Loss2: (0.0000) | Acc: (64.00%) (4180/6528)
Epoch: 17 | Batch_idx: 60 |  Loss: (1.0200) |  Loss2: (0.0000) | Acc: (63.00%) (4954/7808)
Epoch: 17 | Batch_idx: 70 |  Loss: (1.0143) |  Loss2: (0.0000) | Acc: (63.00%) (5796/9088)
Epoch: 17 | Batch_idx: 80 |  Loss: (1.0112) |  Loss2: (0.0000) | Acc: (63.00%) (6615/10368)
Epoch: 17 | Batch_idx: 90 |  Loss: (1.0121) |  Loss2: (0.0000) | Acc: (63.00%) (7434/11648)
Epoch: 17 | Batch_idx: 100 |  Loss: (1.0127) |  Loss2: (0.0000) | Acc: (63.00%) (8248/12928)
Epoch: 17 | Batch_idx: 110 |  Loss: (1.0073) |  Loss2: (0.0000) | Acc: (63.00%) (9082/14208)
Epoch: 17 | Batch_idx: 120 |  Loss: (1.0061) |  Loss2: (0.0000) | Acc: (63.00%) (9898/15488)
Epoch: 17 | Batch_idx: 130 |  Loss: (1.0054) |  Loss2: (0.0000) | Acc: (63.00%) (10730/16768)
Epoch: 17 | Batch_idx: 140 |  Loss: (0.9981) |  Loss2: (0.0000) | Acc: (64.00%) (11611/18048)
Epoch: 17 | Batch_idx: 150 |  Loss: (0.9947) |  Loss2: (0.0000) | Acc: (64.00%) (12470/19328)
Epoch: 17 | Batch_idx: 160 |  Loss: (0.9913) |  Loss2: (0.0000) | Acc: (64.00%) (13340/20608)
Epoch: 17 | Batch_idx: 170 |  Loss: (0.9873) |  Loss2: (0.0000) | Acc: (64.00%) (14206/21888)
Epoch: 17 | Batch_idx: 180 |  Loss: (0.9857) |  Loss2: (0.0000) | Acc: (64.00%) (15045/23168)
Epoch: 17 | Batch_idx: 190 |  Loss: (0.9834) |  Loss2: (0.0000) | Acc: (64.00%) (15880/24448)
Epoch: 17 | Batch_idx: 200 |  Loss: (0.9772) |  Loss2: (0.0000) | Acc: (65.00%) (16758/25728)
Epoch: 17 | Batch_idx: 210 |  Loss: (0.9738) |  Loss2: (0.0000) | Acc: (65.00%) (17614/27008)
Epoch: 17 | Batch_idx: 220 |  Loss: (0.9700) |  Loss2: (0.0000) | Acc: (65.00%) (18478/28288)
Epoch: 17 | Batch_idx: 230 |  Loss: (0.9662) |  Loss2: (0.0000) | Acc: (65.00%) (19356/29568)
Epoch: 17 | Batch_idx: 240 |  Loss: (0.9636) |  Loss2: (0.0000) | Acc: (65.00%) (20211/30848)
Epoch: 17 | Batch_idx: 250 |  Loss: (0.9588) |  Loss2: (0.0000) | Acc: (65.00%) (21112/32128)
Epoch: 17 | Batch_idx: 260 |  Loss: (0.9573) |  Loss2: (0.0000) | Acc: (65.00%) (21976/33408)
Epoch: 17 | Batch_idx: 270 |  Loss: (0.9544) |  Loss2: (0.0000) | Acc: (65.00%) (22851/34688)
Epoch: 17 | Batch_idx: 280 |  Loss: (0.9519) |  Loss2: (0.0000) | Acc: (65.00%) (23726/35968)
Epoch: 17 | Batch_idx: 290 |  Loss: (0.9509) |  Loss2: (0.0000) | Acc: (65.00%) (24565/37248)
Epoch: 17 | Batch_idx: 300 |  Loss: (0.9480) |  Loss2: (0.0000) | Acc: (66.00%) (25434/38528)
Epoch: 17 | Batch_idx: 310 |  Loss: (0.9461) |  Loss2: (0.0000) | Acc: (66.00%) (26323/39808)
Epoch: 17 | Batch_idx: 320 |  Loss: (0.9443) |  Loss2: (0.0000) | Acc: (66.00%) (27200/41088)
Epoch: 17 | Batch_idx: 330 |  Loss: (0.9421) |  Loss2: (0.0000) | Acc: (66.00%) (28070/42368)
Epoch: 17 | Batch_idx: 340 |  Loss: (0.9410) |  Loss2: (0.0000) | Acc: (66.00%) (28930/43648)
Epoch: 17 | Batch_idx: 350 |  Loss: (0.9384) |  Loss2: (0.0000) | Acc: (66.00%) (29834/44928)
Epoch: 17 | Batch_idx: 360 |  Loss: (0.9375) |  Loss2: (0.0000) | Acc: (66.00%) (30711/46208)
Epoch: 17 | Batch_idx: 370 |  Loss: (0.9356) |  Loss2: (0.0000) | Acc: (66.00%) (31606/47488)
Epoch: 17 | Batch_idx: 380 |  Loss: (0.9353) |  Loss2: (0.0000) | Acc: (66.00%) (32484/48768)
Epoch: 17 | Batch_idx: 390 |  Loss: (0.9349) |  Loss2: (0.0000) | Acc: (66.00%) (33315/50000)
# TEST : Loss: (0.8802) | Acc: (68.00%) (6863/10000)
percent tensor([0.4527, 0.4588, 0.4151, 0.4413, 0.4149, 0.4437, 0.4423, 0.4347, 0.4541,
        0.4487, 0.4649, 0.4209, 0.4581, 0.4749, 0.4480, 0.4544],
       device='cuda:0') torch.Size([16])
percent tensor([0.4553, 0.4411, 0.4487, 0.4583, 0.4405, 0.4384, 0.4353, 0.4528, 0.4477,
        0.4513, 0.4471, 0.4475, 0.4568, 0.4526, 0.4410, 0.4536],
       device='cuda:0') torch.Size([16])
percent tensor([0.4903, 0.4905, 0.4666, 0.4918, 0.4704, 0.4794, 0.4862, 0.4905, 0.4936,
        0.4848, 0.4898, 0.4735, 0.4914, 0.4957, 0.4868, 0.4901],
       device='cuda:0') torch.Size([16])
percent tensor([0.5031, 0.5045, 0.4906, 0.4968, 0.4894, 0.5147, 0.4940, 0.4912, 0.4996,
        0.4986, 0.5038, 0.4893, 0.5026, 0.5029, 0.5013, 0.5056],
       device='cuda:0') torch.Size([16])
percent tensor([0.5375, 0.5326, 0.5464, 0.5464, 0.5419, 0.5515, 0.5338, 0.5496, 0.5366,
        0.5312, 0.5306, 0.5289, 0.5230, 0.5340, 0.5320, 0.5435],
       device='cuda:0') torch.Size([16])
percent tensor([0.5293, 0.5248, 0.5364, 0.5463, 0.5377, 0.5508, 0.5325, 0.5427, 0.5258,
        0.5211, 0.5190, 0.5260, 0.5173, 0.5290, 0.5327, 0.5361],
       device='cuda:0') torch.Size([16])
percent tensor([0.6108, 0.5626, 0.5400, 0.5431, 0.5054, 0.6670, 0.5345, 0.5109, 0.5492,
        0.5391, 0.5742, 0.5265, 0.5870, 0.5867, 0.5376, 0.6142],
       device='cuda:0') torch.Size([16])
percent tensor([0.9960, 0.9828, 0.9915, 0.9885, 0.9931, 0.9900, 0.9852, 0.9965, 0.9910,
        0.9953, 0.9940, 0.9933, 0.9932, 0.9866, 0.9852, 0.9969],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 18 | Batch_idx: 0 |  Loss: (0.7486) |  Loss2: (0.0000) | Acc: (74.00%) (95/128)
Epoch: 18 | Batch_idx: 10 |  Loss: (0.8086) |  Loss2: (0.0000) | Acc: (71.00%) (1004/1408)
Epoch: 18 | Batch_idx: 20 |  Loss: (0.8075) |  Loss2: (0.0000) | Acc: (71.00%) (1926/2688)
Epoch: 18 | Batch_idx: 30 |  Loss: (0.8011) |  Loss2: (0.0000) | Acc: (71.00%) (2848/3968)
Epoch: 18 | Batch_idx: 40 |  Loss: (0.8006) |  Loss2: (0.0000) | Acc: (71.00%) (3771/5248)
Epoch: 18 | Batch_idx: 50 |  Loss: (0.8018) |  Loss2: (0.0000) | Acc: (71.00%) (4689/6528)
Epoch: 18 | Batch_idx: 60 |  Loss: (0.7955) |  Loss2: (0.0000) | Acc: (72.00%) (5626/7808)
Epoch: 18 | Batch_idx: 70 |  Loss: (0.7940) |  Loss2: (0.0000) | Acc: (72.00%) (6560/9088)
Epoch: 18 | Batch_idx: 80 |  Loss: (0.7930) |  Loss2: (0.0000) | Acc: (72.00%) (7492/10368)
Epoch: 18 | Batch_idx: 90 |  Loss: (0.7999) |  Loss2: (0.0000) | Acc: (71.00%) (8376/11648)
Epoch: 18 | Batch_idx: 100 |  Loss: (0.7970) |  Loss2: (0.0000) | Acc: (71.00%) (9294/12928)
Epoch: 18 | Batch_idx: 110 |  Loss: (0.7946) |  Loss2: (0.0000) | Acc: (72.00%) (10235/14208)
Epoch: 18 | Batch_idx: 120 |  Loss: (0.7953) |  Loss2: (0.0000) | Acc: (72.00%) (11163/15488)
Epoch: 18 | Batch_idx: 130 |  Loss: (0.7923) |  Loss2: (0.0000) | Acc: (72.00%) (12107/16768)
Epoch: 18 | Batch_idx: 140 |  Loss: (0.7922) |  Loss2: (0.0000) | Acc: (72.00%) (13036/18048)
Epoch: 18 | Batch_idx: 150 |  Loss: (0.7891) |  Loss2: (0.0000) | Acc: (72.00%) (13983/19328)
Epoch: 18 | Batch_idx: 160 |  Loss: (0.7880) |  Loss2: (0.0000) | Acc: (72.00%) (14906/20608)
Epoch: 18 | Batch_idx: 170 |  Loss: (0.7877) |  Loss2: (0.0000) | Acc: (72.00%) (15824/21888)
Epoch: 18 | Batch_idx: 180 |  Loss: (0.7862) |  Loss2: (0.0000) | Acc: (72.00%) (16757/23168)
Epoch: 18 | Batch_idx: 190 |  Loss: (0.7884) |  Loss2: (0.0000) | Acc: (72.00%) (17659/24448)
Epoch: 18 | Batch_idx: 200 |  Loss: (0.7867) |  Loss2: (0.0000) | Acc: (72.00%) (18607/25728)
Epoch: 18 | Batch_idx: 210 |  Loss: (0.7873) |  Loss2: (0.0000) | Acc: (72.00%) (19532/27008)
Epoch: 18 | Batch_idx: 220 |  Loss: (0.7874) |  Loss2: (0.0000) | Acc: (72.00%) (20445/28288)
Epoch: 18 | Batch_idx: 230 |  Loss: (0.7878) |  Loss2: (0.0000) | Acc: (72.00%) (21372/29568)
Epoch: 18 | Batch_idx: 240 |  Loss: (0.7881) |  Loss2: (0.0000) | Acc: (72.00%) (22299/30848)
Epoch: 18 | Batch_idx: 250 |  Loss: (0.7866) |  Loss2: (0.0000) | Acc: (72.00%) (23246/32128)
Epoch: 18 | Batch_idx: 260 |  Loss: (0.7835) |  Loss2: (0.0000) | Acc: (72.00%) (24203/33408)
Epoch: 18 | Batch_idx: 270 |  Loss: (0.7838) |  Loss2: (0.0000) | Acc: (72.00%) (25112/34688)
Epoch: 18 | Batch_idx: 280 |  Loss: (0.7818) |  Loss2: (0.0000) | Acc: (72.00%) (26067/35968)
Epoch: 18 | Batch_idx: 290 |  Loss: (0.7831) |  Loss2: (0.0000) | Acc: (72.00%) (26973/37248)
Epoch: 18 | Batch_idx: 300 |  Loss: (0.7838) |  Loss2: (0.0000) | Acc: (72.00%) (27889/38528)
Epoch: 18 | Batch_idx: 310 |  Loss: (0.7838) |  Loss2: (0.0000) | Acc: (72.00%) (28825/39808)
Epoch: 18 | Batch_idx: 320 |  Loss: (0.7818) |  Loss2: (0.0000) | Acc: (72.00%) (29782/41088)
Epoch: 18 | Batch_idx: 330 |  Loss: (0.7823) |  Loss2: (0.0000) | Acc: (72.00%) (30721/42368)
Epoch: 18 | Batch_idx: 340 |  Loss: (0.7806) |  Loss2: (0.0000) | Acc: (72.00%) (31671/43648)
Epoch: 18 | Batch_idx: 350 |  Loss: (0.7792) |  Loss2: (0.0000) | Acc: (72.00%) (32627/44928)
Epoch: 18 | Batch_idx: 360 |  Loss: (0.7795) |  Loss2: (0.0000) | Acc: (72.00%) (33537/46208)
Epoch: 18 | Batch_idx: 370 |  Loss: (0.7796) |  Loss2: (0.0000) | Acc: (72.00%) (34456/47488)
Epoch: 18 | Batch_idx: 380 |  Loss: (0.7799) |  Loss2: (0.0000) | Acc: (72.00%) (35381/48768)
Epoch: 18 | Batch_idx: 390 |  Loss: (0.7784) |  Loss2: (0.0000) | Acc: (72.00%) (36305/50000)
# TEST : Loss: (0.8187) | Acc: (70.00%) (7098/10000)
percent tensor([0.4529, 0.4602, 0.4159, 0.4417, 0.4152, 0.4446, 0.4431, 0.4358, 0.4551,
        0.4502, 0.4662, 0.4228, 0.4589, 0.4771, 0.4489, 0.4553],
       device='cuda:0') torch.Size([16])
percent tensor([0.4528, 0.4411, 0.4506, 0.4578, 0.4428, 0.4399, 0.4363, 0.4527, 0.4470,
        0.4510, 0.4457, 0.4491, 0.4542, 0.4518, 0.4405, 0.4517],
       device='cuda:0') torch.Size([16])
percent tensor([0.4905, 0.4909, 0.4611, 0.4911, 0.4655, 0.4830, 0.4842, 0.4882, 0.4924,
        0.4829, 0.4896, 0.4704, 0.4907, 0.4963, 0.4874, 0.4905],
       device='cuda:0') torch.Size([16])
percent tensor([0.4999, 0.5010, 0.4933, 0.4963, 0.4948, 0.5127, 0.4942, 0.4905, 0.5015,
        0.4993, 0.5009, 0.4953, 0.5012, 0.5006, 0.4978, 0.5027],
       device='cuda:0') torch.Size([16])
percent tensor([0.5379, 0.5302, 0.5458, 0.5469, 0.5438, 0.5508, 0.5340, 0.5488, 0.5325,
        0.5335, 0.5296, 0.5362, 0.5221, 0.5331, 0.5323, 0.5424],
       device='cuda:0') torch.Size([16])
percent tensor([0.5283, 0.5234, 0.5362, 0.5449, 0.5393, 0.5493, 0.5328, 0.5417, 0.5224,
        0.5227, 0.5182, 0.5310, 0.5159, 0.5272, 0.5316, 0.5357],
       device='cuda:0') torch.Size([16])
percent tensor([0.5922, 0.5539, 0.5466, 0.5408, 0.5309, 0.6634, 0.5471, 0.5140, 0.5557,
        0.5396, 0.5672, 0.5273, 0.5807, 0.5834, 0.5212, 0.6059],
       device='cuda:0') torch.Size([16])
percent tensor([0.9930, 0.9872, 0.9900, 0.9888, 0.9912, 0.9892, 0.9892, 0.9945, 0.9915,
        0.9943, 0.9922, 0.9903, 0.9939, 0.9883, 0.9883, 0.9938],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 19 | Batch_idx: 0 |  Loss: (0.5487) |  Loss2: (0.0000) | Acc: (84.00%) (108/128)
Epoch: 19 | Batch_idx: 10 |  Loss: (0.7033) |  Loss2: (0.0000) | Acc: (76.00%) (1078/1408)
Epoch: 19 | Batch_idx: 20 |  Loss: (0.7548) |  Loss2: (0.0000) | Acc: (73.00%) (1988/2688)
Epoch: 19 | Batch_idx: 30 |  Loss: (0.7915) |  Loss2: (0.0000) | Acc: (72.00%) (2872/3968)
Epoch: 19 | Batch_idx: 40 |  Loss: (0.8078) |  Loss2: (0.0000) | Acc: (71.00%) (3761/5248)
Epoch: 19 | Batch_idx: 50 |  Loss: (0.8282) |  Loss2: (0.0000) | Acc: (70.00%) (4626/6528)
Epoch: 19 | Batch_idx: 60 |  Loss: (0.8368) |  Loss2: (0.0000) | Acc: (70.00%) (5515/7808)
Epoch: 19 | Batch_idx: 70 |  Loss: (0.8453) |  Loss2: (0.0000) | Acc: (70.00%) (6393/9088)
Epoch: 19 | Batch_idx: 80 |  Loss: (0.8485) |  Loss2: (0.0000) | Acc: (70.00%) (7299/10368)
Epoch: 19 | Batch_idx: 90 |  Loss: (0.8510) |  Loss2: (0.0000) | Acc: (70.00%) (8182/11648)
Epoch: 19 | Batch_idx: 100 |  Loss: (0.8532) |  Loss2: (0.0000) | Acc: (70.00%) (9078/12928)
Epoch: 19 | Batch_idx: 110 |  Loss: (0.8575) |  Loss2: (0.0000) | Acc: (69.00%) (9942/14208)
Epoch: 19 | Batch_idx: 120 |  Loss: (0.8558) |  Loss2: (0.0000) | Acc: (69.00%) (10839/15488)
Epoch: 19 | Batch_idx: 130 |  Loss: (0.8556) |  Loss2: (0.0000) | Acc: (70.00%) (11745/16768)
Epoch: 19 | Batch_idx: 140 |  Loss: (0.8590) |  Loss2: (0.0000) | Acc: (69.00%) (12611/18048)
Epoch: 19 | Batch_idx: 150 |  Loss: (0.8538) |  Loss2: (0.0000) | Acc: (70.00%) (13550/19328)
Epoch: 19 | Batch_idx: 160 |  Loss: (0.8523) |  Loss2: (0.0000) | Acc: (70.00%) (14461/20608)
Epoch: 19 | Batch_idx: 170 |  Loss: (0.8526) |  Loss2: (0.0000) | Acc: (70.00%) (15354/21888)
Epoch: 19 | Batch_idx: 180 |  Loss: (0.8521) |  Loss2: (0.0000) | Acc: (70.00%) (16251/23168)
Epoch: 19 | Batch_idx: 190 |  Loss: (0.8487) |  Loss2: (0.0000) | Acc: (70.00%) (17171/24448)
Epoch: 19 | Batch_idx: 200 |  Loss: (0.8476) |  Loss2: (0.0000) | Acc: (70.00%) (18065/25728)
Epoch: 19 | Batch_idx: 210 |  Loss: (0.8486) |  Loss2: (0.0000) | Acc: (70.00%) (18949/27008)
Epoch: 19 | Batch_idx: 220 |  Loss: (0.8483) |  Loss2: (0.0000) | Acc: (70.00%) (19850/28288)
Epoch: 19 | Batch_idx: 230 |  Loss: (0.8471) |  Loss2: (0.0000) | Acc: (70.00%) (20768/29568)
Epoch: 19 | Batch_idx: 240 |  Loss: (0.8460) |  Loss2: (0.0000) | Acc: (70.00%) (21678/30848)
Epoch: 19 | Batch_idx: 250 |  Loss: (0.8427) |  Loss2: (0.0000) | Acc: (70.00%) (22604/32128)
Epoch: 19 | Batch_idx: 260 |  Loss: (0.8418) |  Loss2: (0.0000) | Acc: (70.00%) (23519/33408)
Epoch: 19 | Batch_idx: 270 |  Loss: (0.8401) |  Loss2: (0.0000) | Acc: (70.00%) (24457/34688)
Epoch: 19 | Batch_idx: 280 |  Loss: (0.8393) |  Loss2: (0.0000) | Acc: (70.00%) (25382/35968)
Epoch: 19 | Batch_idx: 290 |  Loss: (0.8389) |  Loss2: (0.0000) | Acc: (70.00%) (26290/37248)
Epoch: 19 | Batch_idx: 300 |  Loss: (0.8376) |  Loss2: (0.0000) | Acc: (70.00%) (27218/38528)
Epoch: 19 | Batch_idx: 310 |  Loss: (0.8370) |  Loss2: (0.0000) | Acc: (70.00%) (28131/39808)
Epoch: 19 | Batch_idx: 320 |  Loss: (0.8362) |  Loss2: (0.0000) | Acc: (70.00%) (29045/41088)
Epoch: 19 | Batch_idx: 330 |  Loss: (0.8353) |  Loss2: (0.0000) | Acc: (70.00%) (29953/42368)
Epoch: 19 | Batch_idx: 340 |  Loss: (0.8343) |  Loss2: (0.0000) | Acc: (70.00%) (30869/43648)
Epoch: 19 | Batch_idx: 350 |  Loss: (0.8344) |  Loss2: (0.0000) | Acc: (70.00%) (31789/44928)
Epoch: 19 | Batch_idx: 360 |  Loss: (0.8340) |  Loss2: (0.0000) | Acc: (70.00%) (32693/46208)
Epoch: 19 | Batch_idx: 370 |  Loss: (0.8323) |  Loss2: (0.0000) | Acc: (70.00%) (33627/47488)
Epoch: 19 | Batch_idx: 380 |  Loss: (0.8312) |  Loss2: (0.0000) | Acc: (70.00%) (34554/48768)
Epoch: 19 | Batch_idx: 390 |  Loss: (0.8292) |  Loss2: (0.0000) | Acc: (70.00%) (35469/50000)
# TEST : Loss: (0.8097) | Acc: (71.00%) (7163/10000)
percent tensor([0.4545, 0.4638, 0.4058, 0.4398, 0.4097, 0.4507, 0.4430, 0.4320, 0.4562,
        0.4486, 0.4699, 0.4146, 0.4595, 0.4847, 0.4516, 0.4582],
       device='cuda:0') torch.Size([16])
percent tensor([0.4664, 0.4536, 0.4558, 0.4693, 0.4513, 0.4594, 0.4473, 0.4637, 0.4590,
        0.4602, 0.4606, 0.4551, 0.4661, 0.4658, 0.4554, 0.4665],
       device='cuda:0') torch.Size([16])
percent tensor([0.4955, 0.4984, 0.4661, 0.4951, 0.4729, 0.4889, 0.4943, 0.4957, 0.4973,
        0.4930, 0.4967, 0.4787, 0.4971, 0.5026, 0.4949, 0.4958],
       device='cuda:0') torch.Size([16])
percent tensor([0.5121, 0.5087, 0.5062, 0.5138, 0.5091, 0.5282, 0.5038, 0.5034, 0.5117,
        0.5072, 0.5102, 0.5046, 0.5088, 0.5092, 0.5093, 0.5161],
       device='cuda:0') torch.Size([16])
percent tensor([0.5526, 0.5364, 0.5526, 0.5643, 0.5536, 0.5792, 0.5378, 0.5555, 0.5450,
        0.5345, 0.5386, 0.5347, 0.5293, 0.5449, 0.5405, 0.5574],
       device='cuda:0') torch.Size([16])
percent tensor([0.5196, 0.5118, 0.5270, 0.5386, 0.5310, 0.5453, 0.5203, 0.5290, 0.5143,
        0.5090, 0.5083, 0.5172, 0.5057, 0.5193, 0.5186, 0.5264],
       device='cuda:0') torch.Size([16])
percent tensor([0.6137, 0.5793, 0.5579, 0.5711, 0.5454, 0.6567, 0.5688, 0.5354, 0.5688,
        0.5669, 0.5914, 0.5391, 0.6006, 0.6032, 0.5515, 0.6237],
       device='cuda:0') torch.Size([16])
percent tensor([0.9965, 0.9881, 0.9934, 0.9924, 0.9944, 0.9918, 0.9923, 0.9965, 0.9924,
        0.9960, 0.9939, 0.9944, 0.9924, 0.9903, 0.9914, 0.9971],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 20 | Batch_idx: 0 |  Loss: (0.6832) |  Loss2: (0.0000) | Acc: (78.00%) (100/128)
Epoch: 20 | Batch_idx: 10 |  Loss: (0.7572) |  Loss2: (0.0000) | Acc: (72.00%) (1026/1408)
Epoch: 20 | Batch_idx: 20 |  Loss: (0.7490) |  Loss2: (0.0000) | Acc: (73.00%) (1986/2688)
Epoch: 20 | Batch_idx: 30 |  Loss: (0.7413) |  Loss2: (0.0000) | Acc: (74.00%) (2950/3968)
Epoch: 20 | Batch_idx: 40 |  Loss: (0.7439) |  Loss2: (0.0000) | Acc: (73.00%) (3871/5248)
Epoch: 20 | Batch_idx: 50 |  Loss: (0.7476) |  Loss2: (0.0000) | Acc: (73.00%) (4823/6528)
Epoch: 20 | Batch_idx: 60 |  Loss: (0.7387) |  Loss2: (0.0000) | Acc: (74.00%) (5802/7808)
Epoch: 20 | Batch_idx: 70 |  Loss: (0.7378) |  Loss2: (0.0000) | Acc: (74.00%) (6773/9088)
Epoch: 20 | Batch_idx: 80 |  Loss: (0.7396) |  Loss2: (0.0000) | Acc: (74.00%) (7728/10368)
Epoch: 20 | Batch_idx: 90 |  Loss: (0.7425) |  Loss2: (0.0000) | Acc: (74.00%) (8675/11648)
Epoch: 20 | Batch_idx: 100 |  Loss: (0.7434) |  Loss2: (0.0000) | Acc: (74.00%) (9624/12928)
Epoch: 20 | Batch_idx: 110 |  Loss: (0.7446) |  Loss2: (0.0000) | Acc: (74.00%) (10568/14208)
Epoch: 20 | Batch_idx: 120 |  Loss: (0.7450) |  Loss2: (0.0000) | Acc: (74.00%) (11507/15488)
Epoch: 20 | Batch_idx: 130 |  Loss: (0.7475) |  Loss2: (0.0000) | Acc: (74.00%) (12427/16768)
Epoch: 20 | Batch_idx: 140 |  Loss: (0.7472) |  Loss2: (0.0000) | Acc: (74.00%) (13381/18048)
Epoch: 20 | Batch_idx: 150 |  Loss: (0.7450) |  Loss2: (0.0000) | Acc: (74.00%) (14340/19328)
Epoch: 20 | Batch_idx: 160 |  Loss: (0.7432) |  Loss2: (0.0000) | Acc: (74.00%) (15307/20608)
Epoch: 20 | Batch_idx: 170 |  Loss: (0.7413) |  Loss2: (0.0000) | Acc: (74.00%) (16275/21888)
Epoch: 20 | Batch_idx: 180 |  Loss: (0.7419) |  Loss2: (0.0000) | Acc: (74.00%) (17221/23168)
Epoch: 20 | Batch_idx: 190 |  Loss: (0.7428) |  Loss2: (0.0000) | Acc: (74.00%) (18160/24448)
Epoch: 20 | Batch_idx: 200 |  Loss: (0.7417) |  Loss2: (0.0000) | Acc: (74.00%) (19107/25728)
Epoch: 20 | Batch_idx: 210 |  Loss: (0.7431) |  Loss2: (0.0000) | Acc: (74.00%) (20048/27008)
Epoch: 20 | Batch_idx: 220 |  Loss: (0.7416) |  Loss2: (0.0000) | Acc: (74.00%) (21015/28288)
Epoch: 20 | Batch_idx: 230 |  Loss: (0.7401) |  Loss2: (0.0000) | Acc: (74.00%) (21990/29568)
Epoch: 20 | Batch_idx: 240 |  Loss: (0.7386) |  Loss2: (0.0000) | Acc: (74.00%) (22962/30848)
Epoch: 20 | Batch_idx: 250 |  Loss: (0.7384) |  Loss2: (0.0000) | Acc: (74.00%) (23924/32128)
Epoch: 20 | Batch_idx: 260 |  Loss: (0.7357) |  Loss2: (0.0000) | Acc: (74.00%) (24897/33408)
Epoch: 20 | Batch_idx: 270 |  Loss: (0.7363) |  Loss2: (0.0000) | Acc: (74.00%) (25839/34688)
Epoch: 20 | Batch_idx: 280 |  Loss: (0.7350) |  Loss2: (0.0000) | Acc: (74.00%) (26806/35968)
Epoch: 20 | Batch_idx: 290 |  Loss: (0.7327) |  Loss2: (0.0000) | Acc: (74.00%) (27786/37248)
Epoch: 20 | Batch_idx: 300 |  Loss: (0.7327) |  Loss2: (0.0000) | Acc: (74.00%) (28750/38528)
Epoch: 20 | Batch_idx: 310 |  Loss: (0.7325) |  Loss2: (0.0000) | Acc: (74.00%) (29705/39808)
Epoch: 20 | Batch_idx: 320 |  Loss: (0.7319) |  Loss2: (0.0000) | Acc: (74.00%) (30675/41088)
Epoch: 20 | Batch_idx: 330 |  Loss: (0.7316) |  Loss2: (0.0000) | Acc: (74.00%) (31645/42368)
Epoch: 20 | Batch_idx: 340 |  Loss: (0.7300) |  Loss2: (0.0000) | Acc: (74.00%) (32622/43648)
Epoch: 20 | Batch_idx: 350 |  Loss: (0.7293) |  Loss2: (0.0000) | Acc: (74.00%) (33581/44928)
Epoch: 20 | Batch_idx: 360 |  Loss: (0.7288) |  Loss2: (0.0000) | Acc: (74.00%) (34527/46208)
Epoch: 20 | Batch_idx: 370 |  Loss: (0.7277) |  Loss2: (0.0000) | Acc: (74.00%) (35508/47488)
Epoch: 20 | Batch_idx: 380 |  Loss: (0.7270) |  Loss2: (0.0000) | Acc: (74.00%) (36471/48768)
Epoch: 20 | Batch_idx: 390 |  Loss: (0.7268) |  Loss2: (0.0000) | Acc: (74.00%) (37382/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_020.pth.tar'
# TEST : Loss: (0.7822) | Acc: (72.00%) (7291/10000)
percent tensor([0.4566, 0.4657, 0.4076, 0.4417, 0.4127, 0.4541, 0.4452, 0.4324, 0.4580,
        0.4502, 0.4713, 0.4156, 0.4608, 0.4860, 0.4542, 0.4595],
       device='cuda:0') torch.Size([16])
percent tensor([0.4654, 0.4566, 0.4527, 0.4690, 0.4496, 0.4589, 0.4493, 0.4609, 0.4602,
        0.4607, 0.4598, 0.4532, 0.4655, 0.4698, 0.4557, 0.4662],
       device='cuda:0') torch.Size([16])
percent tensor([0.4957, 0.4991, 0.4633, 0.4961, 0.4732, 0.4919, 0.4954, 0.4946, 0.4972,
        0.4931, 0.4969, 0.4772, 0.4971, 0.5028, 0.4963, 0.4966],
       device='cuda:0') torch.Size([16])
percent tensor([0.5109, 0.5100, 0.5075, 0.5100, 0.5089, 0.5246, 0.5058, 0.5047, 0.5135,
        0.5096, 0.5109, 0.5072, 0.5098, 0.5113, 0.5080, 0.5153],
       device='cuda:0') torch.Size([16])
percent tensor([0.5461, 0.5366, 0.5522, 0.5623, 0.5537, 0.5690, 0.5405, 0.5571, 0.5440,
        0.5375, 0.5390, 0.5356, 0.5266, 0.5501, 0.5367, 0.5560],
       device='cuda:0') torch.Size([16])
percent tensor([0.5162, 0.5122, 0.5275, 0.5371, 0.5320, 0.5403, 0.5207, 0.5310, 0.5139,
        0.5107, 0.5082, 0.5193, 0.5053, 0.5228, 0.5177, 0.5264],
       device='cuda:0') torch.Size([16])
percent tensor([0.6192, 0.5830, 0.5576, 0.5563, 0.5465, 0.6556, 0.5749, 0.5366, 0.5766,
        0.5747, 0.5941, 0.5421, 0.6073, 0.6011, 0.5498, 0.6202],
       device='cuda:0') torch.Size([16])
percent tensor([0.9980, 0.9900, 0.9945, 0.9928, 0.9941, 0.9942, 0.9950, 0.9968, 0.9952,
        0.9967, 0.9954, 0.9962, 0.9958, 0.9931, 0.9911, 0.9970],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(167.7789, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(783.7920, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(778.1163, device='cuda:0')
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1528.6056, device='cuda:0')
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(508.4864, device='cuda:0')
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2171.0613, device='cuda:0')
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4320.3174, device='cuda:0')
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1440.6343, device='cuda:0')
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6101.4893, device='cuda:0')
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12171.6064, device='cuda:0')
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4058.7097, device='cuda:0')
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(17188.6328, device='cuda:0')
Epoch: 21 | Batch_idx: 0 |  Loss: (0.7470) |  Loss2: (0.0000) | Acc: (74.00%) (95/128)
Epoch: 21 | Batch_idx: 10 |  Loss: (0.7523) |  Loss2: (0.0000) | Acc: (73.00%) (1036/1408)
Epoch: 21 | Batch_idx: 20 |  Loss: (0.7916) |  Loss2: (0.0000) | Acc: (72.00%) (1942/2688)
Epoch: 21 | Batch_idx: 30 |  Loss: (0.8084) |  Loss2: (0.0000) | Acc: (71.00%) (2842/3968)
Epoch: 21 | Batch_idx: 40 |  Loss: (0.8198) |  Loss2: (0.0000) | Acc: (71.00%) (3740/5248)
Epoch: 21 | Batch_idx: 50 |  Loss: (0.8228) |  Loss2: (0.0000) | Acc: (70.00%) (4622/6528)
Epoch: 21 | Batch_idx: 60 |  Loss: (0.8300) |  Loss2: (0.0000) | Acc: (70.00%) (5517/7808)
Epoch: 21 | Batch_idx: 70 |  Loss: (0.8328) |  Loss2: (0.0000) | Acc: (70.00%) (6416/9088)
Epoch: 21 | Batch_idx: 80 |  Loss: (0.8370) |  Loss2: (0.0000) | Acc: (70.00%) (7306/10368)
Epoch: 21 | Batch_idx: 90 |  Loss: (0.8388) |  Loss2: (0.0000) | Acc: (70.00%) (8209/11648)
Epoch: 21 | Batch_idx: 100 |  Loss: (0.8352) |  Loss2: (0.0000) | Acc: (70.00%) (9132/12928)
Epoch: 21 | Batch_idx: 110 |  Loss: (0.8377) |  Loss2: (0.0000) | Acc: (70.00%) (10020/14208)
Epoch: 21 | Batch_idx: 120 |  Loss: (0.8351) |  Loss2: (0.0000) | Acc: (70.00%) (10934/15488)
Epoch: 21 | Batch_idx: 130 |  Loss: (0.8327) |  Loss2: (0.0000) | Acc: (70.00%) (11866/16768)
Epoch: 21 | Batch_idx: 140 |  Loss: (0.8261) |  Loss2: (0.0000) | Acc: (71.00%) (12823/18048)
Epoch: 21 | Batch_idx: 150 |  Loss: (0.8255) |  Loss2: (0.0000) | Acc: (71.00%) (13750/19328)
Epoch: 21 | Batch_idx: 160 |  Loss: (0.8270) |  Loss2: (0.0000) | Acc: (71.00%) (14648/20608)
Epoch: 21 | Batch_idx: 170 |  Loss: (0.8265) |  Loss2: (0.0000) | Acc: (71.00%) (15551/21888)
Epoch: 21 | Batch_idx: 180 |  Loss: (0.8238) |  Loss2: (0.0000) | Acc: (71.00%) (16488/23168)
Epoch: 21 | Batch_idx: 190 |  Loss: (0.8236) |  Loss2: (0.0000) | Acc: (71.00%) (17406/24448)
Epoch: 21 | Batch_idx: 200 |  Loss: (0.8234) |  Loss2: (0.0000) | Acc: (71.00%) (18326/25728)
Epoch: 21 | Batch_idx: 210 |  Loss: (0.8230) |  Loss2: (0.0000) | Acc: (71.00%) (19246/27008)
Epoch: 21 | Batch_idx: 220 |  Loss: (0.8219) |  Loss2: (0.0000) | Acc: (71.00%) (20154/28288)
Epoch: 21 | Batch_idx: 230 |  Loss: (0.8231) |  Loss2: (0.0000) | Acc: (71.00%) (21055/29568)
Epoch: 21 | Batch_idx: 240 |  Loss: (0.8191) |  Loss2: (0.0000) | Acc: (71.00%) (21996/30848)
Epoch: 21 | Batch_idx: 250 |  Loss: (0.8187) |  Loss2: (0.0000) | Acc: (71.00%) (22917/32128)
Epoch: 21 | Batch_idx: 260 |  Loss: (0.8153) |  Loss2: (0.0000) | Acc: (71.00%) (23870/33408)
Epoch: 21 | Batch_idx: 270 |  Loss: (0.8164) |  Loss2: (0.0000) | Acc: (71.00%) (24776/34688)
Epoch: 21 | Batch_idx: 280 |  Loss: (0.8158) |  Loss2: (0.0000) | Acc: (71.00%) (25709/35968)
Epoch: 21 | Batch_idx: 290 |  Loss: (0.8146) |  Loss2: (0.0000) | Acc: (71.00%) (26639/37248)
Epoch: 21 | Batch_idx: 300 |  Loss: (0.8116) |  Loss2: (0.0000) | Acc: (71.00%) (27613/38528)
Epoch: 21 | Batch_idx: 310 |  Loss: (0.8099) |  Loss2: (0.0000) | Acc: (71.00%) (28555/39808)
Epoch: 21 | Batch_idx: 320 |  Loss: (0.8081) |  Loss2: (0.0000) | Acc: (71.00%) (29498/41088)
Epoch: 21 | Batch_idx: 330 |  Loss: (0.8063) |  Loss2: (0.0000) | Acc: (71.00%) (30449/42368)
Epoch: 21 | Batch_idx: 340 |  Loss: (0.8045) |  Loss2: (0.0000) | Acc: (71.00%) (31396/43648)
Epoch: 21 | Batch_idx: 350 |  Loss: (0.8030) |  Loss2: (0.0000) | Acc: (71.00%) (32332/44928)
Epoch: 21 | Batch_idx: 360 |  Loss: (0.8017) |  Loss2: (0.0000) | Acc: (71.00%) (33268/46208)
Epoch: 21 | Batch_idx: 370 |  Loss: (0.7996) |  Loss2: (0.0000) | Acc: (72.00%) (34218/47488)
Epoch: 21 | Batch_idx: 380 |  Loss: (0.7981) |  Loss2: (0.0000) | Acc: (72.00%) (35170/48768)
Epoch: 21 | Batch_idx: 390 |  Loss: (0.7972) |  Loss2: (0.0000) | Acc: (72.00%) (36076/50000)
# TEST : Loss: (0.7620) | Acc: (72.00%) (7295/10000)
percent tensor([0.4671, 0.4736, 0.4272, 0.4544, 0.4314, 0.4657, 0.4563, 0.4459, 0.4679,
        0.4605, 0.4781, 0.4321, 0.4694, 0.4890, 0.4638, 0.4683],
       device='cuda:0') torch.Size([16])
percent tensor([0.4849, 0.4786, 0.4779, 0.4893, 0.4735, 0.4805, 0.4729, 0.4837, 0.4846,
        0.4821, 0.4816, 0.4783, 0.4846, 0.4919, 0.4772, 0.4871],
       device='cuda:0') torch.Size([16])
percent tensor([0.5093, 0.5068, 0.5039, 0.5119, 0.5058, 0.5052, 0.5095, 0.5108, 0.5094,
        0.5093, 0.5077, 0.5121, 0.5091, 0.5107, 0.5090, 0.5071],
       device='cuda:0') torch.Size([16])
percent tensor([0.5256, 0.5290, 0.5314, 0.5297, 0.5320, 0.5364, 0.5279, 0.5289, 0.5341,
        0.5310, 0.5295, 0.5310, 0.5275, 0.5298, 0.5271, 0.5305],
       device='cuda:0') torch.Size([16])
percent tensor([0.5479, 0.5427, 0.5695, 0.5762, 0.5700, 0.5688, 0.5505, 0.5776, 0.5579,
        0.5462, 0.5435, 0.5443, 0.5296, 0.5578, 0.5420, 0.5597],
       device='cuda:0') torch.Size([16])
percent tensor([0.5074, 0.5049, 0.5182, 0.5277, 0.5234, 0.5301, 0.5117, 0.5222, 0.5091,
        0.5036, 0.5028, 0.5118, 0.4997, 0.5145, 0.5100, 0.5164],
       device='cuda:0') torch.Size([16])
percent tensor([0.5680, 0.5534, 0.5414, 0.5431, 0.5346, 0.6179, 0.5374, 0.5071, 0.5524,
        0.5465, 0.5698, 0.5253, 0.5727, 0.5740, 0.5083, 0.5732],
       device='cuda:0') torch.Size([16])
percent tensor([0.9971, 0.9913, 0.9934, 0.9933, 0.9915, 0.9928, 0.9947, 0.9976, 0.9961,
        0.9975, 0.9959, 0.9972, 0.9975, 0.9940, 0.9942, 0.9962],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 22 | Batch_idx: 0 |  Loss: (0.7700) |  Loss2: (0.0000) | Acc: (73.00%) (94/128)
Epoch: 22 | Batch_idx: 10 |  Loss: (0.7011) |  Loss2: (0.0000) | Acc: (75.00%) (1057/1408)
Epoch: 22 | Batch_idx: 20 |  Loss: (0.6842) |  Loss2: (0.0000) | Acc: (76.00%) (2052/2688)
Epoch: 22 | Batch_idx: 30 |  Loss: (0.6874) |  Loss2: (0.0000) | Acc: (76.00%) (3030/3968)
Epoch: 22 | Batch_idx: 40 |  Loss: (0.6914) |  Loss2: (0.0000) | Acc: (76.00%) (4001/5248)
Epoch: 22 | Batch_idx: 50 |  Loss: (0.6924) |  Loss2: (0.0000) | Acc: (76.00%) (4970/6528)
Epoch: 22 | Batch_idx: 60 |  Loss: (0.6965) |  Loss2: (0.0000) | Acc: (75.00%) (5932/7808)
Epoch: 22 | Batch_idx: 70 |  Loss: (0.6945) |  Loss2: (0.0000) | Acc: (76.00%) (6927/9088)
Epoch: 22 | Batch_idx: 80 |  Loss: (0.6894) |  Loss2: (0.0000) | Acc: (76.00%) (7921/10368)
Epoch: 22 | Batch_idx: 90 |  Loss: (0.6933) |  Loss2: (0.0000) | Acc: (76.00%) (8875/11648)
Epoch: 22 | Batch_idx: 100 |  Loss: (0.6913) |  Loss2: (0.0000) | Acc: (76.00%) (9860/12928)
Epoch: 22 | Batch_idx: 110 |  Loss: (0.6908) |  Loss2: (0.0000) | Acc: (76.00%) (10837/14208)
Epoch: 22 | Batch_idx: 120 |  Loss: (0.6903) |  Loss2: (0.0000) | Acc: (76.00%) (11819/15488)
Epoch: 22 | Batch_idx: 130 |  Loss: (0.6893) |  Loss2: (0.0000) | Acc: (76.00%) (12801/16768)
Epoch: 22 | Batch_idx: 140 |  Loss: (0.6877) |  Loss2: (0.0000) | Acc: (76.00%) (13789/18048)
Epoch: 22 | Batch_idx: 150 |  Loss: (0.6897) |  Loss2: (0.0000) | Acc: (76.00%) (14745/19328)
Epoch: 22 | Batch_idx: 160 |  Loss: (0.6898) |  Loss2: (0.0000) | Acc: (76.00%) (15709/20608)
Epoch: 22 | Batch_idx: 170 |  Loss: (0.6935) |  Loss2: (0.0000) | Acc: (76.00%) (16665/21888)
Epoch: 22 | Batch_idx: 180 |  Loss: (0.6904) |  Loss2: (0.0000) | Acc: (76.00%) (17663/23168)
Epoch: 22 | Batch_idx: 190 |  Loss: (0.6910) |  Loss2: (0.0000) | Acc: (76.00%) (18613/24448)
Epoch: 22 | Batch_idx: 200 |  Loss: (0.6896) |  Loss2: (0.0000) | Acc: (76.00%) (19593/25728)
Epoch: 22 | Batch_idx: 210 |  Loss: (0.6889) |  Loss2: (0.0000) | Acc: (76.00%) (20570/27008)
Epoch: 22 | Batch_idx: 220 |  Loss: (0.6883) |  Loss2: (0.0000) | Acc: (76.00%) (21566/28288)
Epoch: 22 | Batch_idx: 230 |  Loss: (0.6902) |  Loss2: (0.0000) | Acc: (76.00%) (22505/29568)
Epoch: 22 | Batch_idx: 240 |  Loss: (0.6894) |  Loss2: (0.0000) | Acc: (76.00%) (23481/30848)
Epoch: 22 | Batch_idx: 250 |  Loss: (0.6882) |  Loss2: (0.0000) | Acc: (76.00%) (24477/32128)
Epoch: 22 | Batch_idx: 260 |  Loss: (0.6881) |  Loss2: (0.0000) | Acc: (76.00%) (25465/33408)
Epoch: 22 | Batch_idx: 270 |  Loss: (0.6891) |  Loss2: (0.0000) | Acc: (76.00%) (26427/34688)
Epoch: 22 | Batch_idx: 280 |  Loss: (0.6889) |  Loss2: (0.0000) | Acc: (76.00%) (27414/35968)
Epoch: 22 | Batch_idx: 290 |  Loss: (0.6887) |  Loss2: (0.0000) | Acc: (76.00%) (28400/37248)
Epoch: 22 | Batch_idx: 300 |  Loss: (0.6880) |  Loss2: (0.0000) | Acc: (76.00%) (29364/38528)
Epoch: 22 | Batch_idx: 310 |  Loss: (0.6875) |  Loss2: (0.0000) | Acc: (76.00%) (30351/39808)
Epoch: 22 | Batch_idx: 320 |  Loss: (0.6876) |  Loss2: (0.0000) | Acc: (76.00%) (31319/41088)
Epoch: 22 | Batch_idx: 330 |  Loss: (0.6876) |  Loss2: (0.0000) | Acc: (76.00%) (32267/42368)
Epoch: 22 | Batch_idx: 340 |  Loss: (0.6873) |  Loss2: (0.0000) | Acc: (76.00%) (33243/43648)
Epoch: 22 | Batch_idx: 350 |  Loss: (0.6856) |  Loss2: (0.0000) | Acc: (76.00%) (34238/44928)
Epoch: 22 | Batch_idx: 360 |  Loss: (0.6855) |  Loss2: (0.0000) | Acc: (76.00%) (35211/46208)
Epoch: 22 | Batch_idx: 370 |  Loss: (0.6842) |  Loss2: (0.0000) | Acc: (76.00%) (36208/47488)
Epoch: 22 | Batch_idx: 380 |  Loss: (0.6823) |  Loss2: (0.0000) | Acc: (76.00%) (37216/48768)
Epoch: 22 | Batch_idx: 390 |  Loss: (0.6819) |  Loss2: (0.0000) | Acc: (76.00%) (38154/50000)
# TEST : Loss: (0.7400) | Acc: (73.00%) (7390/10000)
percent tensor([0.4672, 0.4736, 0.4291, 0.4560, 0.4310, 0.4651, 0.4573, 0.4479, 0.4669,
        0.4620, 0.4782, 0.4346, 0.4704, 0.4899, 0.4638, 0.4683],
       device='cuda:0') torch.Size([16])
percent tensor([0.4864, 0.4761, 0.4811, 0.4907, 0.4758, 0.4818, 0.4724, 0.4845, 0.4829,
        0.4830, 0.4813, 0.4809, 0.4852, 0.4900, 0.4774, 0.4873],
       device='cuda:0') torch.Size([16])
percent tensor([0.5102, 0.5076, 0.5021, 0.5123, 0.5064, 0.5053, 0.5082, 0.5114, 0.5096,
        0.5098, 0.5094, 0.5096, 0.5093, 0.5105, 0.5096, 0.5076],
       device='cuda:0') torch.Size([16])
percent tensor([0.5269, 0.5287, 0.5303, 0.5295, 0.5300, 0.5389, 0.5283, 0.5244, 0.5316,
        0.5308, 0.5287, 0.5334, 0.5287, 0.5299, 0.5274, 0.5333],
       device='cuda:0') torch.Size([16])
percent tensor([0.5528, 0.5424, 0.5720, 0.5771, 0.5707, 0.5778, 0.5511, 0.5702, 0.5537,
        0.5481, 0.5437, 0.5532, 0.5305, 0.5559, 0.5438, 0.5647],
       device='cuda:0') torch.Size([16])
percent tensor([0.5112, 0.5067, 0.5206, 0.5285, 0.5236, 0.5328, 0.5146, 0.5210, 0.5101,
        0.5041, 0.5043, 0.5142, 0.5017, 0.5189, 0.5109, 0.5202],
       device='cuda:0') torch.Size([16])
percent tensor([0.5807, 0.5544, 0.5449, 0.5409, 0.5228, 0.6167, 0.5436, 0.5106, 0.5508,
        0.5525, 0.5706, 0.5339, 0.5777, 0.5806, 0.5141, 0.5868],
       device='cuda:0') torch.Size([16])
percent tensor([0.9977, 0.9885, 0.9941, 0.9910, 0.9946, 0.9921, 0.9933, 0.9972, 0.9936,
        0.9956, 0.9944, 0.9943, 0.9948, 0.9902, 0.9923, 0.9972],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 23 | Batch_idx: 0 |  Loss: (0.8679) |  Loss2: (0.0000) | Acc: (68.00%) (88/128)
Epoch: 23 | Batch_idx: 10 |  Loss: (0.7344) |  Loss2: (0.0000) | Acc: (74.00%) (1045/1408)
Epoch: 23 | Batch_idx: 20 |  Loss: (0.7823) |  Loss2: (0.0000) | Acc: (73.00%) (1964/2688)
Epoch: 23 | Batch_idx: 30 |  Loss: (0.8063) |  Loss2: (0.0000) | Acc: (72.00%) (2862/3968)
Epoch: 23 | Batch_idx: 40 |  Loss: (0.8100) |  Loss2: (0.0000) | Acc: (72.00%) (3781/5248)
Epoch: 23 | Batch_idx: 50 |  Loss: (0.8220) |  Loss2: (0.0000) | Acc: (71.00%) (4656/6528)
Epoch: 23 | Batch_idx: 60 |  Loss: (0.8189) |  Loss2: (0.0000) | Acc: (71.00%) (5574/7808)
Epoch: 23 | Batch_idx: 70 |  Loss: (0.8191) |  Loss2: (0.0000) | Acc: (71.00%) (6487/9088)
Epoch: 23 | Batch_idx: 80 |  Loss: (0.8211) |  Loss2: (0.0000) | Acc: (71.00%) (7392/10368)
Epoch: 23 | Batch_idx: 90 |  Loss: (0.8215) |  Loss2: (0.0000) | Acc: (71.00%) (8304/11648)
Epoch: 23 | Batch_idx: 100 |  Loss: (0.8200) |  Loss2: (0.0000) | Acc: (71.00%) (9220/12928)
Epoch: 23 | Batch_idx: 110 |  Loss: (0.8172) |  Loss2: (0.0000) | Acc: (71.00%) (10142/14208)
Epoch: 23 | Batch_idx: 120 |  Loss: (0.8173) |  Loss2: (0.0000) | Acc: (71.00%) (11052/15488)
Epoch: 23 | Batch_idx: 130 |  Loss: (0.8142) |  Loss2: (0.0000) | Acc: (71.00%) (11986/16768)
Epoch: 23 | Batch_idx: 140 |  Loss: (0.8139) |  Loss2: (0.0000) | Acc: (71.00%) (12899/18048)
Epoch: 23 | Batch_idx: 150 |  Loss: (0.8111) |  Loss2: (0.0000) | Acc: (71.00%) (13839/19328)
Epoch: 23 | Batch_idx: 160 |  Loss: (0.8093) |  Loss2: (0.0000) | Acc: (71.00%) (14761/20608)
Epoch: 23 | Batch_idx: 170 |  Loss: (0.8083) |  Loss2: (0.0000) | Acc: (71.00%) (15692/21888)
Epoch: 23 | Batch_idx: 180 |  Loss: (0.8070) |  Loss2: (0.0000) | Acc: (71.00%) (16621/23168)
Epoch: 23 | Batch_idx: 190 |  Loss: (0.8073) |  Loss2: (0.0000) | Acc: (71.00%) (17541/24448)
Epoch: 23 | Batch_idx: 200 |  Loss: (0.8036) |  Loss2: (0.0000) | Acc: (71.00%) (18499/25728)
Epoch: 23 | Batch_idx: 210 |  Loss: (0.8041) |  Loss2: (0.0000) | Acc: (71.00%) (19412/27008)
Epoch: 23 | Batch_idx: 220 |  Loss: (0.8031) |  Loss2: (0.0000) | Acc: (71.00%) (20340/28288)
Epoch: 23 | Batch_idx: 230 |  Loss: (0.8020) |  Loss2: (0.0000) | Acc: (71.00%) (21259/29568)
Epoch: 23 | Batch_idx: 240 |  Loss: (0.7975) |  Loss2: (0.0000) | Acc: (72.00%) (22227/30848)
Epoch: 23 | Batch_idx: 250 |  Loss: (0.7954) |  Loss2: (0.0000) | Acc: (72.00%) (23182/32128)
Epoch: 23 | Batch_idx: 260 |  Loss: (0.7917) |  Loss2: (0.0000) | Acc: (72.00%) (24151/33408)
Epoch: 23 | Batch_idx: 270 |  Loss: (0.7898) |  Loss2: (0.0000) | Acc: (72.00%) (25109/34688)
Epoch: 23 | Batch_idx: 280 |  Loss: (0.7877) |  Loss2: (0.0000) | Acc: (72.00%) (26080/35968)
Epoch: 23 | Batch_idx: 290 |  Loss: (0.7870) |  Loss2: (0.0000) | Acc: (72.00%) (27006/37248)
Epoch: 23 | Batch_idx: 300 |  Loss: (0.7861) |  Loss2: (0.0000) | Acc: (72.00%) (27955/38528)
Epoch: 23 | Batch_idx: 310 |  Loss: (0.7846) |  Loss2: (0.0000) | Acc: (72.00%) (28906/39808)
Epoch: 23 | Batch_idx: 320 |  Loss: (0.7825) |  Loss2: (0.0000) | Acc: (72.00%) (29870/41088)
Epoch: 23 | Batch_idx: 330 |  Loss: (0.7815) |  Loss2: (0.0000) | Acc: (72.00%) (30813/42368)
Epoch: 23 | Batch_idx: 340 |  Loss: (0.7798) |  Loss2: (0.0000) | Acc: (72.00%) (31767/43648)
Epoch: 23 | Batch_idx: 350 |  Loss: (0.7779) |  Loss2: (0.0000) | Acc: (72.00%) (32736/44928)
Epoch: 23 | Batch_idx: 360 |  Loss: (0.7766) |  Loss2: (0.0000) | Acc: (72.00%) (33690/46208)
Epoch: 23 | Batch_idx: 370 |  Loss: (0.7747) |  Loss2: (0.0000) | Acc: (72.00%) (34659/47488)
Epoch: 23 | Batch_idx: 380 |  Loss: (0.7736) |  Loss2: (0.0000) | Acc: (73.00%) (35614/48768)
Epoch: 23 | Batch_idx: 390 |  Loss: (0.7724) |  Loss2: (0.0000) | Acc: (73.00%) (36547/50000)
# TEST : Loss: (0.7418) | Acc: (74.00%) (7423/10000)
percent tensor([0.4724, 0.4810, 0.4402, 0.4615, 0.4421, 0.4687, 0.4669, 0.4562, 0.4747,
        0.4705, 0.4841, 0.4463, 0.4763, 0.4946, 0.4699, 0.4730],
       device='cuda:0') torch.Size([16])
percent tensor([0.4663, 0.4550, 0.4558, 0.4719, 0.4541, 0.4606, 0.4505, 0.4638, 0.4610,
        0.4602, 0.4591, 0.4561, 0.4650, 0.4714, 0.4566, 0.4668],
       device='cuda:0') torch.Size([16])
percent tensor([0.5051, 0.5076, 0.4672, 0.5016, 0.4779, 0.5037, 0.5037, 0.4999, 0.5036,
        0.5014, 0.5071, 0.4837, 0.5063, 0.5119, 0.5070, 0.5055],
       device='cuda:0') torch.Size([16])
percent tensor([0.5202, 0.5232, 0.5167, 0.5181, 0.5176, 0.5303, 0.5201, 0.5128, 0.5232,
        0.5221, 0.5217, 0.5219, 0.5223, 0.5245, 0.5202, 0.5250],
       device='cuda:0') torch.Size([16])
percent tensor([0.5852, 0.5737, 0.5944, 0.6010, 0.5937, 0.6139, 0.5814, 0.5973, 0.5847,
        0.5748, 0.5714, 0.5755, 0.5601, 0.5918, 0.5745, 0.5951],
       device='cuda:0') torch.Size([16])
percent tensor([0.6104, 0.6023, 0.6154, 0.6280, 0.6222, 0.6360, 0.6201, 0.6293, 0.6042,
        0.6034, 0.5995, 0.6139, 0.6005, 0.6229, 0.6138, 0.6208],
       device='cuda:0') torch.Size([16])
percent tensor([0.5359, 0.5111, 0.4881, 0.4945, 0.4628, 0.5880, 0.5012, 0.4509, 0.5093,
        0.4965, 0.5305, 0.4865, 0.5222, 0.5488, 0.4749, 0.5367],
       device='cuda:0') torch.Size([16])
percent tensor([0.9970, 0.9895, 0.9951, 0.9921, 0.9971, 0.9904, 0.9950, 0.9973, 0.9956,
        0.9965, 0.9959, 0.9963, 0.9951, 0.9902, 0.9929, 0.9964],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 24 | Batch_idx: 0 |  Loss: (0.6782) |  Loss2: (0.0000) | Acc: (78.00%) (100/128)
Epoch: 24 | Batch_idx: 10 |  Loss: (0.6799) |  Loss2: (0.0000) | Acc: (75.00%) (1070/1408)
Epoch: 24 | Batch_idx: 20 |  Loss: (0.6546) |  Loss2: (0.0000) | Acc: (76.00%) (2069/2688)
Epoch: 24 | Batch_idx: 30 |  Loss: (0.6622) |  Loss2: (0.0000) | Acc: (76.00%) (3041/3968)
Epoch: 24 | Batch_idx: 40 |  Loss: (0.6595) |  Loss2: (0.0000) | Acc: (76.00%) (4018/5248)
Epoch: 24 | Batch_idx: 50 |  Loss: (0.6575) |  Loss2: (0.0000) | Acc: (76.00%) (5022/6528)
Epoch: 24 | Batch_idx: 60 |  Loss: (0.6576) |  Loss2: (0.0000) | Acc: (76.00%) (5999/7808)
Epoch: 24 | Batch_idx: 70 |  Loss: (0.6599) |  Loss2: (0.0000) | Acc: (76.00%) (6979/9088)
Epoch: 24 | Batch_idx: 80 |  Loss: (0.6564) |  Loss2: (0.0000) | Acc: (76.00%) (7956/10368)
Epoch: 24 | Batch_idx: 90 |  Loss: (0.6539) |  Loss2: (0.0000) | Acc: (76.00%) (8957/11648)
Epoch: 24 | Batch_idx: 100 |  Loss: (0.6528) |  Loss2: (0.0000) | Acc: (76.00%) (9927/12928)
Epoch: 24 | Batch_idx: 110 |  Loss: (0.6537) |  Loss2: (0.0000) | Acc: (76.00%) (10914/14208)
Epoch: 24 | Batch_idx: 120 |  Loss: (0.6527) |  Loss2: (0.0000) | Acc: (76.00%) (11916/15488)
Epoch: 24 | Batch_idx: 130 |  Loss: (0.6501) |  Loss2: (0.0000) | Acc: (77.00%) (12921/16768)
Epoch: 24 | Batch_idx: 140 |  Loss: (0.6520) |  Loss2: (0.0000) | Acc: (77.00%) (13904/18048)
Epoch: 24 | Batch_idx: 150 |  Loss: (0.6531) |  Loss2: (0.0000) | Acc: (76.00%) (14881/19328)
Epoch: 24 | Batch_idx: 160 |  Loss: (0.6515) |  Loss2: (0.0000) | Acc: (77.00%) (15882/20608)
Epoch: 24 | Batch_idx: 170 |  Loss: (0.6530) |  Loss2: (0.0000) | Acc: (77.00%) (16860/21888)
Epoch: 24 | Batch_idx: 180 |  Loss: (0.6541) |  Loss2: (0.0000) | Acc: (76.00%) (17832/23168)
Epoch: 24 | Batch_idx: 190 |  Loss: (0.6536) |  Loss2: (0.0000) | Acc: (77.00%) (18835/24448)
Epoch: 24 | Batch_idx: 200 |  Loss: (0.6531) |  Loss2: (0.0000) | Acc: (77.00%) (19830/25728)
Epoch: 24 | Batch_idx: 210 |  Loss: (0.6518) |  Loss2: (0.0000) | Acc: (77.00%) (20829/27008)
Epoch: 24 | Batch_idx: 220 |  Loss: (0.6506) |  Loss2: (0.0000) | Acc: (77.00%) (21832/28288)
Epoch: 24 | Batch_idx: 230 |  Loss: (0.6510) |  Loss2: (0.0000) | Acc: (77.00%) (22819/29568)
Epoch: 24 | Batch_idx: 240 |  Loss: (0.6494) |  Loss2: (0.0000) | Acc: (77.00%) (23830/30848)
Epoch: 24 | Batch_idx: 250 |  Loss: (0.6509) |  Loss2: (0.0000) | Acc: (77.00%) (24816/32128)
Epoch: 24 | Batch_idx: 260 |  Loss: (0.6486) |  Loss2: (0.0000) | Acc: (77.00%) (25828/33408)
Epoch: 24 | Batch_idx: 270 |  Loss: (0.6496) |  Loss2: (0.0000) | Acc: (77.00%) (26801/34688)
Epoch: 24 | Batch_idx: 280 |  Loss: (0.6496) |  Loss2: (0.0000) | Acc: (77.00%) (27804/35968)
Epoch: 24 | Batch_idx: 290 |  Loss: (0.6479) |  Loss2: (0.0000) | Acc: (77.00%) (28820/37248)
Epoch: 24 | Batch_idx: 300 |  Loss: (0.6481) |  Loss2: (0.0000) | Acc: (77.00%) (29810/38528)
Epoch: 24 | Batch_idx: 310 |  Loss: (0.6474) |  Loss2: (0.0000) | Acc: (77.00%) (30803/39808)
Epoch: 24 | Batch_idx: 320 |  Loss: (0.6470) |  Loss2: (0.0000) | Acc: (77.00%) (31804/41088)
Epoch: 24 | Batch_idx: 330 |  Loss: (0.6460) |  Loss2: (0.0000) | Acc: (77.00%) (32817/42368)
Epoch: 24 | Batch_idx: 340 |  Loss: (0.6458) |  Loss2: (0.0000) | Acc: (77.00%) (33824/43648)
Epoch: 24 | Batch_idx: 350 |  Loss: (0.6462) |  Loss2: (0.0000) | Acc: (77.00%) (34827/44928)
Epoch: 24 | Batch_idx: 360 |  Loss: (0.6447) |  Loss2: (0.0000) | Acc: (77.00%) (35842/46208)
Epoch: 24 | Batch_idx: 370 |  Loss: (0.6441) |  Loss2: (0.0000) | Acc: (77.00%) (36848/47488)
Epoch: 24 | Batch_idx: 380 |  Loss: (0.6441) |  Loss2: (0.0000) | Acc: (77.00%) (37824/48768)
Epoch: 24 | Batch_idx: 390 |  Loss: (0.6439) |  Loss2: (0.0000) | Acc: (77.00%) (38786/50000)
# TEST : Loss: (0.6770) | Acc: (75.00%) (7574/10000)
percent tensor([0.4733, 0.4823, 0.4385, 0.4611, 0.4419, 0.4706, 0.4674, 0.4549, 0.4754,
        0.4701, 0.4868, 0.4444, 0.4770, 0.4947, 0.4714, 0.4739],
       device='cuda:0') torch.Size([16])
percent tensor([0.4666, 0.4581, 0.4600, 0.4727, 0.4564, 0.4609, 0.4527, 0.4664, 0.4622,
        0.4627, 0.4602, 0.4584, 0.4655, 0.4718, 0.4585, 0.4668],
       device='cuda:0') torch.Size([16])
percent tensor([0.5041, 0.5071, 0.4742, 0.5006, 0.4849, 0.5013, 0.5037, 0.4999, 0.5035,
        0.5000, 0.5058, 0.4878, 0.5058, 0.5124, 0.5061, 0.5039],
       device='cuda:0') torch.Size([16])
percent tensor([0.5203, 0.5207, 0.5196, 0.5192, 0.5195, 0.5326, 0.5157, 0.5142, 0.5215,
        0.5222, 0.5212, 0.5215, 0.5210, 0.5209, 0.5210, 0.5256],
       device='cuda:0') torch.Size([16])
percent tensor([0.5858, 0.5724, 0.5940, 0.6051, 0.5927, 0.6156, 0.5756, 0.6000, 0.5828,
        0.5733, 0.5732, 0.5697, 0.5575, 0.5880, 0.5773, 0.6009],
       device='cuda:0') torch.Size([16])
percent tensor([0.6108, 0.5991, 0.6209, 0.6364, 0.6257, 0.6394, 0.6190, 0.6319, 0.6058,
        0.6046, 0.6009, 0.6188, 0.5981, 0.6155, 0.6184, 0.6227],
       device='cuda:0') torch.Size([16])
percent tensor([0.5394, 0.4944, 0.5137, 0.5116, 0.4835, 0.5955, 0.4857, 0.4579, 0.5095,
        0.4921, 0.5267, 0.4868, 0.5231, 0.5375, 0.4630, 0.5350],
       device='cuda:0') torch.Size([16])
percent tensor([0.9974, 0.9928, 0.9952, 0.9937, 0.9960, 0.9935, 0.9952, 0.9971, 0.9948,
        0.9977, 0.9964, 0.9967, 0.9973, 0.9965, 0.9947, 0.9967],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 25 | Batch_idx: 0 |  Loss: (0.5781) |  Loss2: (0.0000) | Acc: (82.00%) (106/128)
Epoch: 25 | Batch_idx: 10 |  Loss: (0.6430) |  Loss2: (0.0000) | Acc: (78.00%) (1108/1408)
Epoch: 25 | Batch_idx: 20 |  Loss: (0.6460) |  Loss2: (0.0000) | Acc: (78.00%) (2102/2688)
Epoch: 25 | Batch_idx: 30 |  Loss: (0.6535) |  Loss2: (0.0000) | Acc: (77.00%) (3082/3968)
Epoch: 25 | Batch_idx: 40 |  Loss: (0.6761) |  Loss2: (0.0000) | Acc: (76.00%) (4039/5248)
Epoch: 25 | Batch_idx: 50 |  Loss: (0.6715) |  Loss2: (0.0000) | Acc: (77.00%) (5032/6528)
Epoch: 25 | Batch_idx: 60 |  Loss: (0.6766) |  Loss2: (0.0000) | Acc: (76.00%) (6000/7808)
Epoch: 25 | Batch_idx: 70 |  Loss: (0.6781) |  Loss2: (0.0000) | Acc: (76.00%) (6973/9088)
Epoch: 25 | Batch_idx: 80 |  Loss: (0.6772) |  Loss2: (0.0000) | Acc: (76.00%) (7961/10368)
Epoch: 25 | Batch_idx: 90 |  Loss: (0.6749) |  Loss2: (0.0000) | Acc: (76.00%) (8948/11648)
Epoch: 25 | Batch_idx: 100 |  Loss: (0.6762) |  Loss2: (0.0000) | Acc: (76.00%) (9911/12928)
Epoch: 25 | Batch_idx: 110 |  Loss: (0.6766) |  Loss2: (0.0000) | Acc: (76.00%) (10903/14208)
Epoch: 25 | Batch_idx: 120 |  Loss: (0.6761) |  Loss2: (0.0000) | Acc: (76.00%) (11895/15488)
Epoch: 25 | Batch_idx: 130 |  Loss: (0.6776) |  Loss2: (0.0000) | Acc: (76.00%) (12873/16768)
Epoch: 25 | Batch_idx: 140 |  Loss: (0.6772) |  Loss2: (0.0000) | Acc: (76.00%) (13860/18048)
Epoch: 25 | Batch_idx: 150 |  Loss: (0.6785) |  Loss2: (0.0000) | Acc: (76.00%) (14824/19328)
Epoch: 25 | Batch_idx: 160 |  Loss: (0.6787) |  Loss2: (0.0000) | Acc: (76.00%) (15810/20608)
Epoch: 25 | Batch_idx: 170 |  Loss: (0.6755) |  Loss2: (0.0000) | Acc: (76.00%) (16814/21888)
Epoch: 25 | Batch_idx: 180 |  Loss: (0.6759) |  Loss2: (0.0000) | Acc: (76.00%) (17790/23168)
Epoch: 25 | Batch_idx: 190 |  Loss: (0.6764) |  Loss2: (0.0000) | Acc: (76.00%) (18753/24448)
Epoch: 25 | Batch_idx: 200 |  Loss: (0.6745) |  Loss2: (0.0000) | Acc: (76.00%) (19752/25728)
Epoch: 25 | Batch_idx: 210 |  Loss: (0.6734) |  Loss2: (0.0000) | Acc: (76.00%) (20742/27008)
Epoch: 25 | Batch_idx: 220 |  Loss: (0.6727) |  Loss2: (0.0000) | Acc: (76.00%) (21724/28288)
Epoch: 25 | Batch_idx: 230 |  Loss: (0.6740) |  Loss2: (0.0000) | Acc: (76.00%) (22685/29568)
Epoch: 25 | Batch_idx: 240 |  Loss: (0.6751) |  Loss2: (0.0000) | Acc: (76.00%) (23654/30848)
Epoch: 25 | Batch_idx: 250 |  Loss: (0.6756) |  Loss2: (0.0000) | Acc: (76.00%) (24619/32128)
Epoch: 25 | Batch_idx: 260 |  Loss: (0.6757) |  Loss2: (0.0000) | Acc: (76.00%) (25600/33408)
Epoch: 25 | Batch_idx: 270 |  Loss: (0.6764) |  Loss2: (0.0000) | Acc: (76.00%) (26556/34688)
Epoch: 25 | Batch_idx: 280 |  Loss: (0.6771) |  Loss2: (0.0000) | Acc: (76.00%) (27518/35968)
Epoch: 25 | Batch_idx: 290 |  Loss: (0.6760) |  Loss2: (0.0000) | Acc: (76.00%) (28509/37248)
Epoch: 25 | Batch_idx: 300 |  Loss: (0.6753) |  Loss2: (0.0000) | Acc: (76.00%) (29515/38528)
Epoch: 25 | Batch_idx: 310 |  Loss: (0.6744) |  Loss2: (0.0000) | Acc: (76.00%) (30525/39808)
Epoch: 25 | Batch_idx: 320 |  Loss: (0.6740) |  Loss2: (0.0000) | Acc: (76.00%) (31503/41088)
Epoch: 25 | Batch_idx: 330 |  Loss: (0.6732) |  Loss2: (0.0000) | Acc: (76.00%) (32480/42368)
Epoch: 25 | Batch_idx: 340 |  Loss: (0.6724) |  Loss2: (0.0000) | Acc: (76.00%) (33453/43648)
Epoch: 25 | Batch_idx: 350 |  Loss: (0.6722) |  Loss2: (0.0000) | Acc: (76.00%) (34430/44928)
Epoch: 25 | Batch_idx: 360 |  Loss: (0.6719) |  Loss2: (0.0000) | Acc: (76.00%) (35420/46208)
Epoch: 25 | Batch_idx: 370 |  Loss: (0.6724) |  Loss2: (0.0000) | Acc: (76.00%) (36391/47488)
Epoch: 25 | Batch_idx: 380 |  Loss: (0.6718) |  Loss2: (0.0000) | Acc: (76.00%) (37376/48768)
Epoch: 25 | Batch_idx: 390 |  Loss: (0.6712) |  Loss2: (0.0000) | Acc: (76.00%) (38324/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_025.pth.tar'
# TEST : Loss: (0.6777) | Acc: (76.00%) (7603/10000)
percent tensor([0.4619, 0.4712, 0.4212, 0.4489, 0.4242, 0.4598, 0.4532, 0.4396, 0.4628,
        0.4572, 0.4766, 0.4263, 0.4663, 0.4881, 0.4592, 0.4631],
       device='cuda:0') torch.Size([16])
percent tensor([0.4493, 0.4350, 0.4374, 0.4569, 0.4344, 0.4472, 0.4288, 0.4462, 0.4404,
        0.4400, 0.4398, 0.4336, 0.4456, 0.4505, 0.4392, 0.4486],
       device='cuda:0') torch.Size([16])
percent tensor([0.5045, 0.5083, 0.4662, 0.4983, 0.4764, 0.5012, 0.5037, 0.4990, 0.5036,
        0.5010, 0.5069, 0.4825, 0.5069, 0.5130, 0.5066, 0.5051],
       device='cuda:0') torch.Size([16])
percent tensor([0.5204, 0.5210, 0.5172, 0.5215, 0.5164, 0.5332, 0.5142, 0.5133, 0.5205,
        0.5213, 0.5207, 0.5170, 0.5197, 0.5220, 0.5194, 0.5265],
       device='cuda:0') torch.Size([16])
percent tensor([0.5654, 0.5509, 0.5760, 0.5917, 0.5767, 0.6108, 0.5530, 0.5810, 0.5626,
        0.5498, 0.5520, 0.5457, 0.5332, 0.5698, 0.5567, 0.5857],
       device='cuda:0') torch.Size([16])
percent tensor([0.6246, 0.6145, 0.6360, 0.6487, 0.6454, 0.6646, 0.6342, 0.6473, 0.6210,
        0.6207, 0.6150, 0.6331, 0.6103, 0.6322, 0.6358, 0.6425],
       device='cuda:0') torch.Size([16])
percent tensor([0.5674, 0.5181, 0.5271, 0.5378, 0.4796, 0.6182, 0.5004, 0.4596, 0.5375,
        0.5155, 0.5623, 0.4946, 0.5632, 0.5761, 0.4672, 0.5607],
       device='cuda:0') torch.Size([16])
percent tensor([0.9984, 0.9929, 0.9958, 0.9954, 0.9977, 0.9936, 0.9958, 0.9981, 0.9953,
        0.9979, 0.9965, 0.9985, 0.9974, 0.9962, 0.9963, 0.9981],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 26 | Batch_idx: 0 |  Loss: (0.6883) |  Loss2: (0.0000) | Acc: (75.00%) (96/128)
Epoch: 26 | Batch_idx: 10 |  Loss: (0.6159) |  Loss2: (0.0000) | Acc: (79.00%) (1118/1408)
Epoch: 26 | Batch_idx: 20 |  Loss: (0.6184) |  Loss2: (0.0000) | Acc: (78.00%) (2123/2688)
Epoch: 26 | Batch_idx: 30 |  Loss: (0.6130) |  Loss2: (0.0000) | Acc: (79.00%) (3144/3968)
Epoch: 26 | Batch_idx: 40 |  Loss: (0.6144) |  Loss2: (0.0000) | Acc: (79.00%) (4159/5248)
Epoch: 26 | Batch_idx: 50 |  Loss: (0.6101) |  Loss2: (0.0000) | Acc: (79.00%) (5161/6528)
Epoch: 26 | Batch_idx: 60 |  Loss: (0.6197) |  Loss2: (0.0000) | Acc: (78.00%) (6140/7808)
Epoch: 26 | Batch_idx: 70 |  Loss: (0.6165) |  Loss2: (0.0000) | Acc: (78.00%) (7149/9088)
Epoch: 26 | Batch_idx: 80 |  Loss: (0.6190) |  Loss2: (0.0000) | Acc: (78.00%) (8152/10368)
Epoch: 26 | Batch_idx: 90 |  Loss: (0.6182) |  Loss2: (0.0000) | Acc: (78.00%) (9155/11648)
Epoch: 26 | Batch_idx: 100 |  Loss: (0.6168) |  Loss2: (0.0000) | Acc: (78.00%) (10163/12928)
Epoch: 26 | Batch_idx: 110 |  Loss: (0.6171) |  Loss2: (0.0000) | Acc: (78.00%) (11161/14208)
Epoch: 26 | Batch_idx: 120 |  Loss: (0.6154) |  Loss2: (0.0000) | Acc: (78.00%) (12189/15488)
Epoch: 26 | Batch_idx: 130 |  Loss: (0.6140) |  Loss2: (0.0000) | Acc: (78.00%) (13205/16768)
Epoch: 26 | Batch_idx: 140 |  Loss: (0.6130) |  Loss2: (0.0000) | Acc: (78.00%) (14234/18048)
Epoch: 26 | Batch_idx: 150 |  Loss: (0.6116) |  Loss2: (0.0000) | Acc: (78.00%) (15248/19328)
Epoch: 26 | Batch_idx: 160 |  Loss: (0.6105) |  Loss2: (0.0000) | Acc: (78.00%) (16244/20608)
Epoch: 26 | Batch_idx: 170 |  Loss: (0.6116) |  Loss2: (0.0000) | Acc: (78.00%) (17256/21888)
Epoch: 26 | Batch_idx: 180 |  Loss: (0.6135) |  Loss2: (0.0000) | Acc: (78.00%) (18259/23168)
Epoch: 26 | Batch_idx: 190 |  Loss: (0.6150) |  Loss2: (0.0000) | Acc: (78.00%) (19240/24448)
Epoch: 26 | Batch_idx: 200 |  Loss: (0.6150) |  Loss2: (0.0000) | Acc: (78.00%) (20267/25728)
Epoch: 26 | Batch_idx: 210 |  Loss: (0.6136) |  Loss2: (0.0000) | Acc: (78.00%) (21285/27008)
Epoch: 26 | Batch_idx: 220 |  Loss: (0.6126) |  Loss2: (0.0000) | Acc: (78.00%) (22297/28288)
Epoch: 26 | Batch_idx: 230 |  Loss: (0.6143) |  Loss2: (0.0000) | Acc: (78.00%) (23281/29568)
Epoch: 26 | Batch_idx: 240 |  Loss: (0.6149) |  Loss2: (0.0000) | Acc: (78.00%) (24269/30848)
Epoch: 26 | Batch_idx: 250 |  Loss: (0.6152) |  Loss2: (0.0000) | Acc: (78.00%) (25287/32128)
Epoch: 26 | Batch_idx: 260 |  Loss: (0.6133) |  Loss2: (0.0000) | Acc: (78.00%) (26319/33408)
Epoch: 26 | Batch_idx: 270 |  Loss: (0.6124) |  Loss2: (0.0000) | Acc: (78.00%) (27342/34688)
Epoch: 26 | Batch_idx: 280 |  Loss: (0.6143) |  Loss2: (0.0000) | Acc: (78.00%) (28326/35968)
Epoch: 26 | Batch_idx: 290 |  Loss: (0.6140) |  Loss2: (0.0000) | Acc: (78.00%) (29316/37248)
Epoch: 26 | Batch_idx: 300 |  Loss: (0.6146) |  Loss2: (0.0000) | Acc: (78.00%) (30336/38528)
Epoch: 26 | Batch_idx: 310 |  Loss: (0.6144) |  Loss2: (0.0000) | Acc: (78.00%) (31348/39808)
Epoch: 26 | Batch_idx: 320 |  Loss: (0.6132) |  Loss2: (0.0000) | Acc: (78.00%) (32374/41088)
Epoch: 26 | Batch_idx: 330 |  Loss: (0.6130) |  Loss2: (0.0000) | Acc: (78.00%) (33395/42368)
Epoch: 26 | Batch_idx: 340 |  Loss: (0.6135) |  Loss2: (0.0000) | Acc: (78.00%) (34386/43648)
Epoch: 26 | Batch_idx: 350 |  Loss: (0.6122) |  Loss2: (0.0000) | Acc: (78.00%) (35409/44928)
Epoch: 26 | Batch_idx: 360 |  Loss: (0.6129) |  Loss2: (0.0000) | Acc: (78.00%) (36403/46208)
Epoch: 26 | Batch_idx: 370 |  Loss: (0.6134) |  Loss2: (0.0000) | Acc: (78.00%) (37390/47488)
Epoch: 26 | Batch_idx: 380 |  Loss: (0.6125) |  Loss2: (0.0000) | Acc: (78.00%) (38420/48768)
Epoch: 26 | Batch_idx: 390 |  Loss: (0.6126) |  Loss2: (0.0000) | Acc: (78.00%) (39395/50000)
# TEST : Loss: (0.6787) | Acc: (76.00%) (7659/10000)
percent tensor([0.4641, 0.4715, 0.4215, 0.4467, 0.4238, 0.4625, 0.4540, 0.4388, 0.4649,
        0.4573, 0.4789, 0.4274, 0.4675, 0.4891, 0.4599, 0.4638],
       device='cuda:0') torch.Size([16])
percent tensor([0.4489, 0.4352, 0.4367, 0.4558, 0.4333, 0.4469, 0.4283, 0.4456, 0.4403,
        0.4400, 0.4400, 0.4335, 0.4455, 0.4511, 0.4389, 0.4490],
       device='cuda:0') torch.Size([16])
percent tensor([0.5041, 0.5079, 0.4638, 0.4979, 0.4749, 0.5021, 0.5030, 0.4960, 0.5026,
        0.4997, 0.5065, 0.4786, 0.5060, 0.5121, 0.5066, 0.5047],
       device='cuda:0') torch.Size([16])
percent tensor([0.5194, 0.5217, 0.5196, 0.5191, 0.5198, 0.5324, 0.5182, 0.5124, 0.5230,
        0.5213, 0.5213, 0.5229, 0.5195, 0.5231, 0.5193, 0.5258],
       device='cuda:0') torch.Size([16])
percent tensor([0.5659, 0.5530, 0.5749, 0.5884, 0.5778, 0.6035, 0.5597, 0.5753, 0.5630,
        0.5543, 0.5555, 0.5550, 0.5351, 0.5796, 0.5528, 0.5843],
       device='cuda:0') torch.Size([16])
percent tensor([0.6223, 0.6126, 0.6307, 0.6496, 0.6400, 0.6614, 0.6346, 0.6440, 0.6141,
        0.6148, 0.6129, 0.6336, 0.6060, 0.6341, 0.6351, 0.6369],
       device='cuda:0') torch.Size([16])
percent tensor([0.5516, 0.5154, 0.5144, 0.5163, 0.4897, 0.6304, 0.4922, 0.4332, 0.5370,
        0.5024, 0.5562, 0.4970, 0.5530, 0.5675, 0.4648, 0.5515],
       device='cuda:0') torch.Size([16])
percent tensor([0.9981, 0.9955, 0.9950, 0.9939, 0.9946, 0.9961, 0.9979, 0.9979, 0.9969,
        0.9982, 0.9974, 0.9979, 0.9979, 0.9965, 0.9964, 0.9968],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 27 | Batch_idx: 0 |  Loss: (0.5458) |  Loss2: (0.0000) | Acc: (83.00%) (107/128)
Epoch: 27 | Batch_idx: 10 |  Loss: (0.5816) |  Loss2: (0.0000) | Acc: (78.00%) (1112/1408)
Epoch: 27 | Batch_idx: 20 |  Loss: (0.6312) |  Loss2: (0.0000) | Acc: (77.00%) (2081/2688)
Epoch: 27 | Batch_idx: 30 |  Loss: (0.6564) |  Loss2: (0.0000) | Acc: (76.00%) (3050/3968)
Epoch: 27 | Batch_idx: 40 |  Loss: (0.6760) |  Loss2: (0.0000) | Acc: (76.00%) (3993/5248)
Epoch: 27 | Batch_idx: 50 |  Loss: (0.6914) |  Loss2: (0.0000) | Acc: (75.00%) (4939/6528)
Epoch: 27 | Batch_idx: 60 |  Loss: (0.7055) |  Loss2: (0.0000) | Acc: (75.00%) (5859/7808)
Epoch: 27 | Batch_idx: 70 |  Loss: (0.7041) |  Loss2: (0.0000) | Acc: (75.00%) (6839/9088)
Epoch: 27 | Batch_idx: 80 |  Loss: (0.7037) |  Loss2: (0.0000) | Acc: (75.00%) (7801/10368)
Epoch: 27 | Batch_idx: 90 |  Loss: (0.7030) |  Loss2: (0.0000) | Acc: (75.00%) (8750/11648)
Epoch: 27 | Batch_idx: 100 |  Loss: (0.7035) |  Loss2: (0.0000) | Acc: (75.00%) (9713/12928)
Epoch: 27 | Batch_idx: 110 |  Loss: (0.7031) |  Loss2: (0.0000) | Acc: (75.00%) (10693/14208)
Epoch: 27 | Batch_idx: 120 |  Loss: (0.7051) |  Loss2: (0.0000) | Acc: (75.00%) (11660/15488)
Epoch: 27 | Batch_idx: 130 |  Loss: (0.7058) |  Loss2: (0.0000) | Acc: (75.00%) (12627/16768)
Epoch: 27 | Batch_idx: 140 |  Loss: (0.7063) |  Loss2: (0.0000) | Acc: (75.00%) (13582/18048)
Epoch: 27 | Batch_idx: 150 |  Loss: (0.7041) |  Loss2: (0.0000) | Acc: (75.00%) (14559/19328)
Epoch: 27 | Batch_idx: 160 |  Loss: (0.7025) |  Loss2: (0.0000) | Acc: (75.00%) (15544/20608)
Epoch: 27 | Batch_idx: 170 |  Loss: (0.7034) |  Loss2: (0.0000) | Acc: (75.00%) (16501/21888)
Epoch: 27 | Batch_idx: 180 |  Loss: (0.7034) |  Loss2: (0.0000) | Acc: (75.00%) (17469/23168)
Epoch: 27 | Batch_idx: 190 |  Loss: (0.7013) |  Loss2: (0.0000) | Acc: (75.00%) (18463/24448)
Epoch: 27 | Batch_idx: 200 |  Loss: (0.7034) |  Loss2: (0.0000) | Acc: (75.00%) (19424/25728)
Epoch: 27 | Batch_idx: 210 |  Loss: (0.7020) |  Loss2: (0.0000) | Acc: (75.00%) (20400/27008)
Epoch: 27 | Batch_idx: 220 |  Loss: (0.6999) |  Loss2: (0.0000) | Acc: (75.00%) (21387/28288)
Epoch: 27 | Batch_idx: 230 |  Loss: (0.6981) |  Loss2: (0.0000) | Acc: (75.00%) (22389/29568)
Epoch: 27 | Batch_idx: 240 |  Loss: (0.6966) |  Loss2: (0.0000) | Acc: (75.00%) (23379/30848)
Epoch: 27 | Batch_idx: 250 |  Loss: (0.6952) |  Loss2: (0.0000) | Acc: (75.00%) (24355/32128)
Epoch: 27 | Batch_idx: 260 |  Loss: (0.6956) |  Loss2: (0.0000) | Acc: (75.00%) (25322/33408)
Epoch: 27 | Batch_idx: 270 |  Loss: (0.6940) |  Loss2: (0.0000) | Acc: (75.00%) (26293/34688)
Epoch: 27 | Batch_idx: 280 |  Loss: (0.6929) |  Loss2: (0.0000) | Acc: (75.00%) (27292/35968)
Epoch: 27 | Batch_idx: 290 |  Loss: (0.6900) |  Loss2: (0.0000) | Acc: (75.00%) (28295/37248)
Epoch: 27 | Batch_idx: 300 |  Loss: (0.6885) |  Loss2: (0.0000) | Acc: (76.00%) (29286/38528)
Epoch: 27 | Batch_idx: 310 |  Loss: (0.6858) |  Loss2: (0.0000) | Acc: (76.00%) (30295/39808)
Epoch: 27 | Batch_idx: 320 |  Loss: (0.6845) |  Loss2: (0.0000) | Acc: (76.00%) (31283/41088)
Epoch: 27 | Batch_idx: 330 |  Loss: (0.6839) |  Loss2: (0.0000) | Acc: (76.00%) (32261/42368)
Epoch: 27 | Batch_idx: 340 |  Loss: (0.6829) |  Loss2: (0.0000) | Acc: (76.00%) (33239/43648)
Epoch: 27 | Batch_idx: 350 |  Loss: (0.6820) |  Loss2: (0.0000) | Acc: (76.00%) (34237/44928)
Epoch: 27 | Batch_idx: 360 |  Loss: (0.6809) |  Loss2: (0.0000) | Acc: (76.00%) (35223/46208)
Epoch: 27 | Batch_idx: 370 |  Loss: (0.6801) |  Loss2: (0.0000) | Acc: (76.00%) (36216/47488)
Epoch: 27 | Batch_idx: 380 |  Loss: (0.6780) |  Loss2: (0.0000) | Acc: (76.00%) (37244/48768)
Epoch: 27 | Batch_idx: 390 |  Loss: (0.6749) |  Loss2: (0.0000) | Acc: (76.00%) (38244/50000)
# TEST : Loss: (0.6616) | Acc: (77.00%) (7708/10000)
percent tensor([0.4646, 0.4778, 0.4051, 0.4393, 0.4122, 0.4624, 0.4540, 0.4317, 0.4664,
        0.4558, 0.4848, 0.4152, 0.4682, 0.4989, 0.4611, 0.4645],
       device='cuda:0') torch.Size([16])
percent tensor([0.4806, 0.4752, 0.4672, 0.4835, 0.4676, 0.4783, 0.4692, 0.4797, 0.4768,
        0.4749, 0.4760, 0.4670, 0.4793, 0.4875, 0.4756, 0.4818],
       device='cuda:0') torch.Size([16])
percent tensor([0.5104, 0.5116, 0.4584, 0.5027, 0.4658, 0.5066, 0.5029, 0.4960, 0.5071,
        0.5019, 0.5142, 0.4748, 0.5125, 0.5192, 0.5108, 0.5101],
       device='cuda:0') torch.Size([16])
percent tensor([0.4924, 0.4927, 0.4966, 0.4948, 0.4965, 0.5081, 0.4874, 0.4877, 0.4955,
        0.4919, 0.4908, 0.4941, 0.4908, 0.4936, 0.4902, 0.4988],
       device='cuda:0') torch.Size([16])
percent tensor([0.5866, 0.5714, 0.6099, 0.6168, 0.6124, 0.6375, 0.5835, 0.6033, 0.5826,
        0.5770, 0.5768, 0.5862, 0.5505, 0.5959, 0.5766, 0.6115],
       device='cuda:0') torch.Size([16])
percent tensor([0.6038, 0.5907, 0.6251, 0.6463, 0.6337, 0.6389, 0.6188, 0.6403, 0.5960,
        0.5960, 0.5903, 0.6213, 0.5817, 0.6109, 0.6193, 0.6157],
       device='cuda:0') torch.Size([16])
percent tensor([0.5661, 0.5360, 0.5184, 0.5019, 0.4974, 0.6375, 0.5012, 0.4379, 0.5525,
        0.5321, 0.5708, 0.5067, 0.5746, 0.5690, 0.4877, 0.5627],
       device='cuda:0') torch.Size([16])
percent tensor([0.9983, 0.9950, 0.9960, 0.9952, 0.9945, 0.9960, 0.9981, 0.9983, 0.9964,
        0.9982, 0.9981, 0.9976, 0.9979, 0.9960, 0.9967, 0.9972],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 28 | Batch_idx: 0 |  Loss: (0.6305) |  Loss2: (0.0000) | Acc: (76.00%) (98/128)
Epoch: 28 | Batch_idx: 10 |  Loss: (0.6354) |  Loss2: (0.0000) | Acc: (78.00%) (1105/1408)
Epoch: 28 | Batch_idx: 20 |  Loss: (0.6092) |  Loss2: (0.0000) | Acc: (79.00%) (2146/2688)
Epoch: 28 | Batch_idx: 30 |  Loss: (0.5987) |  Loss2: (0.0000) | Acc: (79.00%) (3166/3968)
Epoch: 28 | Batch_idx: 40 |  Loss: (0.5934) |  Loss2: (0.0000) | Acc: (79.00%) (4197/5248)
Epoch: 28 | Batch_idx: 50 |  Loss: (0.5900) |  Loss2: (0.0000) | Acc: (80.00%) (5227/6528)
Epoch: 28 | Batch_idx: 60 |  Loss: (0.5928) |  Loss2: (0.0000) | Acc: (79.00%) (6216/7808)
Epoch: 28 | Batch_idx: 70 |  Loss: (0.5885) |  Loss2: (0.0000) | Acc: (79.00%) (7248/9088)
Epoch: 28 | Batch_idx: 80 |  Loss: (0.5866) |  Loss2: (0.0000) | Acc: (79.00%) (8267/10368)
Epoch: 28 | Batch_idx: 90 |  Loss: (0.5828) |  Loss2: (0.0000) | Acc: (79.00%) (9307/11648)
Epoch: 28 | Batch_idx: 100 |  Loss: (0.5849) |  Loss2: (0.0000) | Acc: (79.00%) (10328/12928)
Epoch: 28 | Batch_idx: 110 |  Loss: (0.5849) |  Loss2: (0.0000) | Acc: (79.00%) (11352/14208)
Epoch: 28 | Batch_idx: 120 |  Loss: (0.5853) |  Loss2: (0.0000) | Acc: (79.00%) (12363/15488)
Epoch: 28 | Batch_idx: 130 |  Loss: (0.5879) |  Loss2: (0.0000) | Acc: (79.00%) (13359/16768)
Epoch: 28 | Batch_idx: 140 |  Loss: (0.5880) |  Loss2: (0.0000) | Acc: (79.00%) (14370/18048)
Epoch: 28 | Batch_idx: 150 |  Loss: (0.5875) |  Loss2: (0.0000) | Acc: (79.00%) (15387/19328)
Epoch: 28 | Batch_idx: 160 |  Loss: (0.5876) |  Loss2: (0.0000) | Acc: (79.00%) (16420/20608)
Epoch: 28 | Batch_idx: 170 |  Loss: (0.5857) |  Loss2: (0.0000) | Acc: (79.00%) (17474/21888)
Epoch: 28 | Batch_idx: 180 |  Loss: (0.5859) |  Loss2: (0.0000) | Acc: (79.00%) (18489/23168)
Epoch: 28 | Batch_idx: 190 |  Loss: (0.5876) |  Loss2: (0.0000) | Acc: (79.00%) (19482/24448)
Epoch: 28 | Batch_idx: 200 |  Loss: (0.5861) |  Loss2: (0.0000) | Acc: (79.00%) (20504/25728)
Epoch: 28 | Batch_idx: 210 |  Loss: (0.5845) |  Loss2: (0.0000) | Acc: (79.00%) (21535/27008)
Epoch: 28 | Batch_idx: 220 |  Loss: (0.5829) |  Loss2: (0.0000) | Acc: (79.00%) (22579/28288)
Epoch: 28 | Batch_idx: 230 |  Loss: (0.5816) |  Loss2: (0.0000) | Acc: (79.00%) (23616/29568)
Epoch: 28 | Batch_idx: 240 |  Loss: (0.5805) |  Loss2: (0.0000) | Acc: (79.00%) (24650/30848)
Epoch: 28 | Batch_idx: 250 |  Loss: (0.5798) |  Loss2: (0.0000) | Acc: (79.00%) (25684/32128)
Epoch: 28 | Batch_idx: 260 |  Loss: (0.5798) |  Loss2: (0.0000) | Acc: (79.00%) (26708/33408)
Epoch: 28 | Batch_idx: 270 |  Loss: (0.5802) |  Loss2: (0.0000) | Acc: (79.00%) (27725/34688)
Epoch: 28 | Batch_idx: 280 |  Loss: (0.5792) |  Loss2: (0.0000) | Acc: (79.00%) (28753/35968)
Epoch: 28 | Batch_idx: 290 |  Loss: (0.5795) |  Loss2: (0.0000) | Acc: (79.00%) (29769/37248)
Epoch: 28 | Batch_idx: 300 |  Loss: (0.5807) |  Loss2: (0.0000) | Acc: (79.00%) (30787/38528)
Epoch: 28 | Batch_idx: 310 |  Loss: (0.5827) |  Loss2: (0.0000) | Acc: (79.00%) (31772/39808)
Epoch: 28 | Batch_idx: 320 |  Loss: (0.5822) |  Loss2: (0.0000) | Acc: (79.00%) (32810/41088)
Epoch: 28 | Batch_idx: 330 |  Loss: (0.5814) |  Loss2: (0.0000) | Acc: (79.00%) (33838/42368)
Epoch: 28 | Batch_idx: 340 |  Loss: (0.5812) |  Loss2: (0.0000) | Acc: (79.00%) (34848/43648)
Epoch: 28 | Batch_idx: 350 |  Loss: (0.5795) |  Loss2: (0.0000) | Acc: (79.00%) (35899/44928)
Epoch: 28 | Batch_idx: 360 |  Loss: (0.5796) |  Loss2: (0.0000) | Acc: (79.00%) (36920/46208)
Epoch: 28 | Batch_idx: 370 |  Loss: (0.5807) |  Loss2: (0.0000) | Acc: (79.00%) (37935/47488)
Epoch: 28 | Batch_idx: 380 |  Loss: (0.5802) |  Loss2: (0.0000) | Acc: (79.00%) (38971/48768)
Epoch: 28 | Batch_idx: 390 |  Loss: (0.5806) |  Loss2: (0.0000) | Acc: (79.00%) (39932/50000)
# TEST : Loss: (0.6151) | Acc: (78.00%) (7880/10000)
percent tensor([0.4645, 0.4772, 0.4145, 0.4451, 0.4183, 0.4614, 0.4552, 0.4370, 0.4665,
        0.4590, 0.4829, 0.4218, 0.4683, 0.4959, 0.4616, 0.4644],
       device='cuda:0') torch.Size([16])
percent tensor([0.4818, 0.4737, 0.4709, 0.4839, 0.4704, 0.4798, 0.4695, 0.4806, 0.4775,
        0.4751, 0.4766, 0.4692, 0.4795, 0.4860, 0.4760, 0.4817],
       device='cuda:0') torch.Size([16])
percent tensor([0.5100, 0.5109, 0.4664, 0.5050, 0.4705, 0.5067, 0.5028, 0.4983, 0.5076,
        0.5029, 0.5123, 0.4800, 0.5124, 0.5185, 0.5092, 0.5094],
       device='cuda:0') torch.Size([16])
percent tensor([0.4961, 0.4939, 0.4981, 0.4968, 0.4983, 0.5096, 0.4899, 0.4888, 0.4974,
        0.4946, 0.4946, 0.4980, 0.4934, 0.4954, 0.4931, 0.5016],
       device='cuda:0') torch.Size([16])
percent tensor([0.5969, 0.5755, 0.6196, 0.6232, 0.6198, 0.6395, 0.5873, 0.6131, 0.5922,
        0.5820, 0.5863, 0.5961, 0.5557, 0.5978, 0.5841, 0.6168],
       device='cuda:0') torch.Size([16])
percent tensor([0.6064, 0.5953, 0.6220, 0.6424, 0.6372, 0.6402, 0.6160, 0.6398, 0.6014,
        0.5971, 0.5931, 0.6157, 0.5874, 0.6172, 0.6174, 0.6187],
       device='cuda:0') torch.Size([16])
percent tensor([0.5715, 0.5506, 0.5345, 0.5136, 0.5028, 0.6278, 0.5210, 0.4489, 0.5490,
        0.5476, 0.5749, 0.5284, 0.5828, 0.5791, 0.4891, 0.5705],
       device='cuda:0') torch.Size([16])
percent tensor([0.9988, 0.9952, 0.9952, 0.9944, 0.9952, 0.9932, 0.9969, 0.9986, 0.9953,
        0.9979, 0.9965, 0.9943, 0.9970, 0.9960, 0.9974, 0.9980],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 29 | Batch_idx: 0 |  Loss: (0.5165) |  Loss2: (0.0000) | Acc: (79.00%) (102/128)
Epoch: 29 | Batch_idx: 10 |  Loss: (0.5993) |  Loss2: (0.0000) | Acc: (78.00%) (1100/1408)
Epoch: 29 | Batch_idx: 20 |  Loss: (0.6538) |  Loss2: (0.0000) | Acc: (76.00%) (2051/2688)
Epoch: 29 | Batch_idx: 30 |  Loss: (0.7068) |  Loss2: (0.0000) | Acc: (75.00%) (2979/3968)
Epoch: 29 | Batch_idx: 40 |  Loss: (0.7251) |  Loss2: (0.0000) | Acc: (74.00%) (3907/5248)
Epoch: 29 | Batch_idx: 50 |  Loss: (0.7390) |  Loss2: (0.0000) | Acc: (74.00%) (4857/6528)
Epoch: 29 | Batch_idx: 60 |  Loss: (0.7433) |  Loss2: (0.0000) | Acc: (74.00%) (5797/7808)
Epoch: 29 | Batch_idx: 70 |  Loss: (0.7447) |  Loss2: (0.0000) | Acc: (74.00%) (6760/9088)
Epoch: 29 | Batch_idx: 80 |  Loss: (0.7455) |  Loss2: (0.0000) | Acc: (74.00%) (7700/10368)
Epoch: 29 | Batch_idx: 90 |  Loss: (0.7457) |  Loss2: (0.0000) | Acc: (74.00%) (8656/11648)
Epoch: 29 | Batch_idx: 100 |  Loss: (0.7446) |  Loss2: (0.0000) | Acc: (74.00%) (9614/12928)
Epoch: 29 | Batch_idx: 110 |  Loss: (0.7464) |  Loss2: (0.0000) | Acc: (74.00%) (10561/14208)
Epoch: 29 | Batch_idx: 120 |  Loss: (0.7435) |  Loss2: (0.0000) | Acc: (74.00%) (11516/15488)
Epoch: 29 | Batch_idx: 130 |  Loss: (0.7437) |  Loss2: (0.0000) | Acc: (74.00%) (12455/16768)
Epoch: 29 | Batch_idx: 140 |  Loss: (0.7393) |  Loss2: (0.0000) | Acc: (74.00%) (13436/18048)
Epoch: 29 | Batch_idx: 150 |  Loss: (0.7349) |  Loss2: (0.0000) | Acc: (74.00%) (14410/19328)
Epoch: 29 | Batch_idx: 160 |  Loss: (0.7297) |  Loss2: (0.0000) | Acc: (74.00%) (15393/20608)
Epoch: 29 | Batch_idx: 170 |  Loss: (0.7257) |  Loss2: (0.0000) | Acc: (74.00%) (16374/21888)
Epoch: 29 | Batch_idx: 180 |  Loss: (0.7233) |  Loss2: (0.0000) | Acc: (74.00%) (17356/23168)
Epoch: 29 | Batch_idx: 190 |  Loss: (0.7220) |  Loss2: (0.0000) | Acc: (74.00%) (18335/24448)
Epoch: 29 | Batch_idx: 200 |  Loss: (0.7206) |  Loss2: (0.0000) | Acc: (75.00%) (19309/25728)
Epoch: 29 | Batch_idx: 210 |  Loss: (0.7205) |  Loss2: (0.0000) | Acc: (75.00%) (20267/27008)
Epoch: 29 | Batch_idx: 220 |  Loss: (0.7200) |  Loss2: (0.0000) | Acc: (75.00%) (21235/28288)
Epoch: 29 | Batch_idx: 230 |  Loss: (0.7183) |  Loss2: (0.0000) | Acc: (75.00%) (22206/29568)
Epoch: 29 | Batch_idx: 240 |  Loss: (0.7156) |  Loss2: (0.0000) | Acc: (75.00%) (23187/30848)
Epoch: 29 | Batch_idx: 250 |  Loss: (0.7144) |  Loss2: (0.0000) | Acc: (75.00%) (24171/32128)
Epoch: 29 | Batch_idx: 260 |  Loss: (0.7130) |  Loss2: (0.0000) | Acc: (75.00%) (25139/33408)
Epoch: 29 | Batch_idx: 270 |  Loss: (0.7117) |  Loss2: (0.0000) | Acc: (75.00%) (26127/34688)
Epoch: 29 | Batch_idx: 280 |  Loss: (0.7091) |  Loss2: (0.0000) | Acc: (75.00%) (27129/35968)
Epoch: 29 | Batch_idx: 290 |  Loss: (0.7071) |  Loss2: (0.0000) | Acc: (75.00%) (28146/37248)
Epoch: 29 | Batch_idx: 300 |  Loss: (0.7058) |  Loss2: (0.0000) | Acc: (75.00%) (29122/38528)
Epoch: 29 | Batch_idx: 310 |  Loss: (0.7038) |  Loss2: (0.0000) | Acc: (75.00%) (30105/39808)
Epoch: 29 | Batch_idx: 320 |  Loss: (0.7020) |  Loss2: (0.0000) | Acc: (75.00%) (31093/41088)
Epoch: 29 | Batch_idx: 330 |  Loss: (0.7000) |  Loss2: (0.0000) | Acc: (75.00%) (32102/42368)
Epoch: 29 | Batch_idx: 340 |  Loss: (0.6989) |  Loss2: (0.0000) | Acc: (75.00%) (33089/43648)
Epoch: 29 | Batch_idx: 350 |  Loss: (0.6970) |  Loss2: (0.0000) | Acc: (75.00%) (34083/44928)
Epoch: 29 | Batch_idx: 360 |  Loss: (0.6961) |  Loss2: (0.0000) | Acc: (75.00%) (35067/46208)
Epoch: 29 | Batch_idx: 370 |  Loss: (0.6946) |  Loss2: (0.0000) | Acc: (75.00%) (36068/47488)
Epoch: 29 | Batch_idx: 380 |  Loss: (0.6936) |  Loss2: (0.0000) | Acc: (75.00%) (37060/48768)
Epoch: 29 | Batch_idx: 390 |  Loss: (0.6935) |  Loss2: (0.0000) | Acc: (76.00%) (38011/50000)
# TEST : Loss: (0.6667) | Acc: (77.00%) (7728/10000)
percent tensor([0.4842, 0.5035, 0.4414, 0.4681, 0.4460, 0.4823, 0.4845, 0.4652, 0.4904,
        0.4843, 0.5034, 0.4505, 0.4892, 0.5108, 0.4875, 0.4865],
       device='cuda:0') torch.Size([16])
percent tensor([0.4916, 0.4864, 0.4804, 0.4926, 0.4815, 0.4910, 0.4830, 0.4899, 0.4885,
        0.4866, 0.4878, 0.4805, 0.4899, 0.4955, 0.4882, 0.4912],
       device='cuda:0') torch.Size([16])
percent tensor([0.5063, 0.4963, 0.4686, 0.5073, 0.4705, 0.5005, 0.4878, 0.4968, 0.5066,
        0.4885, 0.5049, 0.4711, 0.5062, 0.5157, 0.5008, 0.5042],
       device='cuda:0') torch.Size([16])
percent tensor([0.5166, 0.5109, 0.5240, 0.5213, 0.5254, 0.5338, 0.5137, 0.5153, 0.5194,
        0.5153, 0.5139, 0.5247, 0.5124, 0.5143, 0.5152, 0.5214],
       device='cuda:0') torch.Size([16])
percent tensor([0.5910, 0.5434, 0.6345, 0.6423, 0.6355, 0.6665, 0.5681, 0.6169, 0.5935,
        0.5670, 0.5731, 0.5987, 0.5340, 0.5984, 0.5698, 0.6139],
       device='cuda:0') torch.Size([16])
percent tensor([0.5665, 0.5522, 0.5975, 0.6133, 0.6083, 0.6079, 0.5788, 0.6058, 0.5668,
        0.5602, 0.5584, 0.5886, 0.5493, 0.5799, 0.5775, 0.5820],
       device='cuda:0') torch.Size([16])
percent tensor([0.5685, 0.5410, 0.5269, 0.4990, 0.4968, 0.6295, 0.5175, 0.4446, 0.5499,
        0.5422, 0.5666, 0.5233, 0.5743, 0.5809, 0.4738, 0.5644],
       device='cuda:0') torch.Size([16])
percent tensor([0.9980, 0.9953, 0.9956, 0.9945, 0.9955, 0.9964, 0.9972, 0.9988, 0.9965,
        0.9976, 0.9970, 0.9952, 0.9976, 0.9967, 0.9975, 0.9977],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 30 | Batch_idx: 0 |  Loss: (0.6504) |  Loss2: (0.0000) | Acc: (78.00%) (100/128)
Epoch: 30 | Batch_idx: 10 |  Loss: (0.5857) |  Loss2: (0.0000) | Acc: (80.00%) (1128/1408)
Epoch: 30 | Batch_idx: 20 |  Loss: (0.5871) |  Loss2: (0.0000) | Acc: (80.00%) (2157/2688)
Epoch: 30 | Batch_idx: 30 |  Loss: (0.5825) |  Loss2: (0.0000) | Acc: (80.00%) (3184/3968)
Epoch: 30 | Batch_idx: 40 |  Loss: (0.5601) |  Loss2: (0.0000) | Acc: (81.00%) (4252/5248)
Epoch: 30 | Batch_idx: 50 |  Loss: (0.5594) |  Loss2: (0.0000) | Acc: (81.00%) (5290/6528)
Epoch: 30 | Batch_idx: 60 |  Loss: (0.5567) |  Loss2: (0.0000) | Acc: (81.00%) (6328/7808)
Epoch: 30 | Batch_idx: 70 |  Loss: (0.5591) |  Loss2: (0.0000) | Acc: (80.00%) (7356/9088)
Epoch: 30 | Batch_idx: 80 |  Loss: (0.5574) |  Loss2: (0.0000) | Acc: (80.00%) (8395/10368)
Epoch: 30 | Batch_idx: 90 |  Loss: (0.5605) |  Loss2: (0.0000) | Acc: (80.00%) (9411/11648)
Epoch: 30 | Batch_idx: 100 |  Loss: (0.5596) |  Loss2: (0.0000) | Acc: (80.00%) (10469/12928)
Epoch: 30 | Batch_idx: 110 |  Loss: (0.5590) |  Loss2: (0.0000) | Acc: (80.00%) (11504/14208)
Epoch: 30 | Batch_idx: 120 |  Loss: (0.5573) |  Loss2: (0.0000) | Acc: (80.00%) (12535/15488)
Epoch: 30 | Batch_idx: 130 |  Loss: (0.5551) |  Loss2: (0.0000) | Acc: (80.00%) (13573/16768)
Epoch: 30 | Batch_idx: 140 |  Loss: (0.5542) |  Loss2: (0.0000) | Acc: (80.00%) (14612/18048)
Epoch: 30 | Batch_idx: 150 |  Loss: (0.5537) |  Loss2: (0.0000) | Acc: (80.00%) (15652/19328)
Epoch: 30 | Batch_idx: 160 |  Loss: (0.5539) |  Loss2: (0.0000) | Acc: (80.00%) (16688/20608)
Epoch: 30 | Batch_idx: 170 |  Loss: (0.5540) |  Loss2: (0.0000) | Acc: (80.00%) (17719/21888)
Epoch: 30 | Batch_idx: 180 |  Loss: (0.5538) |  Loss2: (0.0000) | Acc: (80.00%) (18760/23168)
Epoch: 30 | Batch_idx: 190 |  Loss: (0.5540) |  Loss2: (0.0000) | Acc: (80.00%) (19790/24448)
Epoch: 30 | Batch_idx: 200 |  Loss: (0.5540) |  Loss2: (0.0000) | Acc: (80.00%) (20829/25728)
Epoch: 30 | Batch_idx: 210 |  Loss: (0.5534) |  Loss2: (0.0000) | Acc: (81.00%) (21881/27008)
Epoch: 30 | Batch_idx: 220 |  Loss: (0.5526) |  Loss2: (0.0000) | Acc: (81.00%) (22918/28288)
Epoch: 30 | Batch_idx: 230 |  Loss: (0.5533) |  Loss2: (0.0000) | Acc: (80.00%) (23945/29568)
Epoch: 30 | Batch_idx: 240 |  Loss: (0.5540) |  Loss2: (0.0000) | Acc: (80.00%) (24986/30848)
Epoch: 30 | Batch_idx: 250 |  Loss: (0.5534) |  Loss2: (0.0000) | Acc: (81.00%) (26030/32128)
Epoch: 30 | Batch_idx: 260 |  Loss: (0.5520) |  Loss2: (0.0000) | Acc: (81.00%) (27079/33408)
Epoch: 30 | Batch_idx: 270 |  Loss: (0.5523) |  Loss2: (0.0000) | Acc: (81.00%) (28108/34688)
Epoch: 30 | Batch_idx: 280 |  Loss: (0.5529) |  Loss2: (0.0000) | Acc: (81.00%) (29144/35968)
Epoch: 30 | Batch_idx: 290 |  Loss: (0.5521) |  Loss2: (0.0000) | Acc: (81.00%) (30196/37248)
Epoch: 30 | Batch_idx: 300 |  Loss: (0.5528) |  Loss2: (0.0000) | Acc: (81.00%) (31224/38528)
Epoch: 30 | Batch_idx: 310 |  Loss: (0.5523) |  Loss2: (0.0000) | Acc: (81.00%) (32267/39808)
Epoch: 30 | Batch_idx: 320 |  Loss: (0.5525) |  Loss2: (0.0000) | Acc: (81.00%) (33301/41088)
Epoch: 30 | Batch_idx: 330 |  Loss: (0.5543) |  Loss2: (0.0000) | Acc: (80.00%) (34318/42368)
Epoch: 30 | Batch_idx: 340 |  Loss: (0.5546) |  Loss2: (0.0000) | Acc: (80.00%) (35341/43648)
Epoch: 30 | Batch_idx: 350 |  Loss: (0.5552) |  Loss2: (0.0000) | Acc: (80.00%) (36358/44928)
Epoch: 30 | Batch_idx: 360 |  Loss: (0.5553) |  Loss2: (0.0000) | Acc: (80.00%) (37389/46208)
Epoch: 30 | Batch_idx: 370 |  Loss: (0.5546) |  Loss2: (0.0000) | Acc: (80.00%) (38447/47488)
Epoch: 30 | Batch_idx: 380 |  Loss: (0.5530) |  Loss2: (0.0000) | Acc: (81.00%) (39522/48768)
Epoch: 30 | Batch_idx: 390 |  Loss: (0.5529) |  Loss2: (0.0000) | Acc: (81.00%) (40522/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_030.pth.tar'
# TEST : Loss: (0.6218) | Acc: (78.00%) (7846/10000)
percent tensor([0.4856, 0.5011, 0.4426, 0.4684, 0.4469, 0.4816, 0.4827, 0.4652, 0.4898,
        0.4837, 0.5031, 0.4501, 0.4902, 0.5091, 0.4862, 0.4871],
       device='cuda:0') torch.Size([16])
percent tensor([0.4914, 0.4865, 0.4796, 0.4928, 0.4802, 0.4898, 0.4824, 0.4899, 0.4888,
        0.4867, 0.4874, 0.4798, 0.4902, 0.4957, 0.4875, 0.4913],
       device='cuda:0') torch.Size([16])
percent tensor([0.5065, 0.4971, 0.4637, 0.5035, 0.4630, 0.4994, 0.4919, 0.4938, 0.5066,
        0.4894, 0.5074, 0.4727, 0.5072, 0.5178, 0.4999, 0.5037],
       device='cuda:0') torch.Size([16])
percent tensor([0.5158, 0.5112, 0.5238, 0.5234, 0.5249, 0.5310, 0.5111, 0.5174, 0.5193,
        0.5127, 0.5105, 0.5203, 0.5094, 0.5149, 0.5145, 0.5207],
       device='cuda:0') torch.Size([16])
percent tensor([0.5952, 0.5561, 0.6229, 0.6357, 0.6306, 0.6647, 0.5740, 0.6180, 0.6023,
        0.5604, 0.5701, 0.5784, 0.5326, 0.6065, 0.5761, 0.6134],
       device='cuda:0') torch.Size([16])
percent tensor([0.5693, 0.5610, 0.5942, 0.6164, 0.6047, 0.6083, 0.5815, 0.6091, 0.5687,
        0.5612, 0.5550, 0.5878, 0.5455, 0.5865, 0.5834, 0.5832],
       device='cuda:0') torch.Size([16])
percent tensor([0.5586, 0.5374, 0.5018, 0.4911, 0.4817, 0.6121, 0.5197, 0.4376, 0.5407,
        0.5290, 0.5615, 0.4970, 0.5749, 0.5711, 0.4702, 0.5511],
       device='cuda:0') torch.Size([16])
percent tensor([0.9984, 0.9945, 0.9966, 0.9949, 0.9961, 0.9934, 0.9977, 0.9990, 0.9979,
        0.9981, 0.9980, 0.9981, 0.9983, 0.9980, 0.9970, 0.9974],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(169.7421, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(788.9695, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(783.3051, device='cuda:0')
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1527.7449, device='cuda:0')
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(506.8335, device='cuda:0')
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2178.1870, device='cuda:0')
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4314.8032, device='cuda:0')
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1435.4108, device='cuda:0')
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6100.6548, device='cuda:0')
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12126.1270, device='cuda:0')
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4042.9827, device='cuda:0')
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(17110.7930, device='cuda:0')
Epoch: 31 | Batch_idx: 0 |  Loss: (0.5899) |  Loss2: (0.0000) | Acc: (78.00%) (101/128)
Epoch: 31 | Batch_idx: 10 |  Loss: (0.5353) |  Loss2: (0.0000) | Acc: (81.00%) (1149/1408)
Epoch: 31 | Batch_idx: 20 |  Loss: (0.5595) |  Loss2: (0.0000) | Acc: (80.00%) (2175/2688)
Epoch: 31 | Batch_idx: 30 |  Loss: (0.5831) |  Loss2: (0.0000) | Acc: (79.00%) (3160/3968)
Epoch: 31 | Batch_idx: 40 |  Loss: (0.5824) |  Loss2: (0.0000) | Acc: (79.00%) (4157/5248)
Epoch: 31 | Batch_idx: 50 |  Loss: (0.5935) |  Loss2: (0.0000) | Acc: (78.00%) (5142/6528)
Epoch: 31 | Batch_idx: 60 |  Loss: (0.5936) |  Loss2: (0.0000) | Acc: (78.00%) (6143/7808)
Epoch: 31 | Batch_idx: 70 |  Loss: (0.5931) |  Loss2: (0.0000) | Acc: (78.00%) (7155/9088)
Epoch: 31 | Batch_idx: 80 |  Loss: (0.5994) |  Loss2: (0.0000) | Acc: (78.00%) (8149/10368)
Epoch: 31 | Batch_idx: 90 |  Loss: (0.5977) |  Loss2: (0.0000) | Acc: (78.00%) (9169/11648)
Epoch: 31 | Batch_idx: 100 |  Loss: (0.5982) |  Loss2: (0.0000) | Acc: (78.00%) (10176/12928)
Epoch: 31 | Batch_idx: 110 |  Loss: (0.5984) |  Loss2: (0.0000) | Acc: (78.00%) (11188/14208)
Epoch: 31 | Batch_idx: 120 |  Loss: (0.5988) |  Loss2: (0.0000) | Acc: (78.00%) (12190/15488)
Epoch: 31 | Batch_idx: 130 |  Loss: (0.6003) |  Loss2: (0.0000) | Acc: (78.00%) (13188/16768)
Epoch: 31 | Batch_idx: 140 |  Loss: (0.5980) |  Loss2: (0.0000) | Acc: (78.00%) (14214/18048)
Epoch: 31 | Batch_idx: 150 |  Loss: (0.5973) |  Loss2: (0.0000) | Acc: (78.00%) (15234/19328)
Epoch: 31 | Batch_idx: 160 |  Loss: (0.5979) |  Loss2: (0.0000) | Acc: (78.00%) (16235/20608)
Epoch: 31 | Batch_idx: 170 |  Loss: (0.6004) |  Loss2: (0.0000) | Acc: (78.00%) (17211/21888)
Epoch: 31 | Batch_idx: 180 |  Loss: (0.5999) |  Loss2: (0.0000) | Acc: (78.00%) (18234/23168)
Epoch: 31 | Batch_idx: 190 |  Loss: (0.5995) |  Loss2: (0.0000) | Acc: (78.00%) (19253/24448)
Epoch: 31 | Batch_idx: 200 |  Loss: (0.6009) |  Loss2: (0.0000) | Acc: (78.00%) (20263/25728)
Epoch: 31 | Batch_idx: 210 |  Loss: (0.6009) |  Loss2: (0.0000) | Acc: (78.00%) (21274/27008)
Epoch: 31 | Batch_idx: 220 |  Loss: (0.5986) |  Loss2: (0.0000) | Acc: (78.00%) (22302/28288)
Epoch: 31 | Batch_idx: 230 |  Loss: (0.5972) |  Loss2: (0.0000) | Acc: (78.00%) (23330/29568)
Epoch: 31 | Batch_idx: 240 |  Loss: (0.5952) |  Loss2: (0.0000) | Acc: (79.00%) (24376/30848)
Epoch: 31 | Batch_idx: 250 |  Loss: (0.5950) |  Loss2: (0.0000) | Acc: (79.00%) (25387/32128)
Epoch: 31 | Batch_idx: 260 |  Loss: (0.5941) |  Loss2: (0.0000) | Acc: (79.00%) (26400/33408)
Epoch: 31 | Batch_idx: 270 |  Loss: (0.5940) |  Loss2: (0.0000) | Acc: (79.00%) (27416/34688)
Epoch: 31 | Batch_idx: 280 |  Loss: (0.5924) |  Loss2: (0.0000) | Acc: (79.00%) (28458/35968)
Epoch: 31 | Batch_idx: 290 |  Loss: (0.5926) |  Loss2: (0.0000) | Acc: (79.00%) (29474/37248)
Epoch: 31 | Batch_idx: 300 |  Loss: (0.5935) |  Loss2: (0.0000) | Acc: (79.00%) (30478/38528)
Epoch: 31 | Batch_idx: 310 |  Loss: (0.5936) |  Loss2: (0.0000) | Acc: (79.00%) (31497/39808)
Epoch: 31 | Batch_idx: 320 |  Loss: (0.5933) |  Loss2: (0.0000) | Acc: (79.00%) (32508/41088)
Epoch: 31 | Batch_idx: 330 |  Loss: (0.5925) |  Loss2: (0.0000) | Acc: (79.00%) (33537/42368)
Epoch: 31 | Batch_idx: 340 |  Loss: (0.5921) |  Loss2: (0.0000) | Acc: (79.00%) (34554/43648)
Epoch: 31 | Batch_idx: 350 |  Loss: (0.5918) |  Loss2: (0.0000) | Acc: (79.00%) (35586/44928)
Epoch: 31 | Batch_idx: 360 |  Loss: (0.5918) |  Loss2: (0.0000) | Acc: (79.00%) (36593/46208)
Epoch: 31 | Batch_idx: 370 |  Loss: (0.5917) |  Loss2: (0.0000) | Acc: (79.00%) (37609/47488)
Epoch: 31 | Batch_idx: 380 |  Loss: (0.5898) |  Loss2: (0.0000) | Acc: (79.00%) (38659/48768)
Epoch: 31 | Batch_idx: 390 |  Loss: (0.5904) |  Loss2: (0.0000) | Acc: (79.00%) (39619/50000)
# TEST : Loss: (0.6009) | Acc: (79.00%) (7909/10000)
percent tensor([0.4842, 0.4985, 0.4422, 0.4656, 0.4450, 0.4797, 0.4805, 0.4630, 0.4885,
        0.4819, 0.5030, 0.4478, 0.4891, 0.5086, 0.4838, 0.4859],
       device='cuda:0') torch.Size([16])
percent tensor([0.4872, 0.4807, 0.4678, 0.4874, 0.4690, 0.4834, 0.4746, 0.4825, 0.4825,
        0.4800, 0.4823, 0.4693, 0.4865, 0.4940, 0.4813, 0.4862],
       device='cuda:0') torch.Size([16])
percent tensor([0.5082, 0.4905, 0.4532, 0.5025, 0.4465, 0.4932, 0.4820, 0.4867, 0.5080,
        0.4834, 0.5069, 0.4617, 0.5092, 0.5197, 0.4942, 0.5012],
       device='cuda:0') torch.Size([16])
percent tensor([0.5014, 0.4959, 0.5035, 0.5020, 0.5025, 0.5147, 0.4931, 0.4951, 0.5026,
        0.4966, 0.4962, 0.5028, 0.4982, 0.4982, 0.4973, 0.5034],
       device='cuda:0') torch.Size([16])
percent tensor([0.6134, 0.5727, 0.6209, 0.6300, 0.6254, 0.6712, 0.5823, 0.6088, 0.6242,
        0.5811, 0.6014, 0.5930, 0.5646, 0.6262, 0.5846, 0.6181],
       device='cuda:0') torch.Size([16])
percent tensor([0.5932, 0.5806, 0.6126, 0.6352, 0.6271, 0.6369, 0.6006, 0.6278, 0.5846,
        0.5808, 0.5766, 0.6043, 0.5693, 0.6051, 0.6034, 0.6058],
       device='cuda:0') torch.Size([16])
percent tensor([0.5482, 0.5154, 0.5201, 0.5216, 0.5133, 0.6008, 0.5277, 0.4673, 0.5266,
        0.5216, 0.5335, 0.5099, 0.5358, 0.5619, 0.4698, 0.5449],
       device='cuda:0') torch.Size([16])
percent tensor([0.9987, 0.9959, 0.9963, 0.9962, 0.9967, 0.9955, 0.9976, 0.9994, 0.9983,
        0.9985, 0.9980, 0.9982, 0.9987, 0.9982, 0.9977, 0.9981],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 32 | Batch_idx: 0 |  Loss: (0.6419) |  Loss2: (0.0000) | Acc: (71.00%) (91/128)
Epoch: 32 | Batch_idx: 10 |  Loss: (0.5712) |  Loss2: (0.0000) | Acc: (79.00%) (1114/1408)
Epoch: 32 | Batch_idx: 20 |  Loss: (0.5644) |  Loss2: (0.0000) | Acc: (79.00%) (2142/2688)
Epoch: 32 | Batch_idx: 30 |  Loss: (0.5519) |  Loss2: (0.0000) | Acc: (80.00%) (3185/3968)
Epoch: 32 | Batch_idx: 40 |  Loss: (0.5501) |  Loss2: (0.0000) | Acc: (80.00%) (4236/5248)
Epoch: 32 | Batch_idx: 50 |  Loss: (0.5470) |  Loss2: (0.0000) | Acc: (80.00%) (5282/6528)
Epoch: 32 | Batch_idx: 60 |  Loss: (0.5456) |  Loss2: (0.0000) | Acc: (80.00%) (6320/7808)
Epoch: 32 | Batch_idx: 70 |  Loss: (0.5416) |  Loss2: (0.0000) | Acc: (81.00%) (7371/9088)
Epoch: 32 | Batch_idx: 80 |  Loss: (0.5417) |  Loss2: (0.0000) | Acc: (81.00%) (8409/10368)
Epoch: 32 | Batch_idx: 90 |  Loss: (0.5397) |  Loss2: (0.0000) | Acc: (81.00%) (9457/11648)
Epoch: 32 | Batch_idx: 100 |  Loss: (0.5432) |  Loss2: (0.0000) | Acc: (81.00%) (10491/12928)
Epoch: 32 | Batch_idx: 110 |  Loss: (0.5424) |  Loss2: (0.0000) | Acc: (81.00%) (11528/14208)
Epoch: 32 | Batch_idx: 120 |  Loss: (0.5428) |  Loss2: (0.0000) | Acc: (81.00%) (12567/15488)
Epoch: 32 | Batch_idx: 130 |  Loss: (0.5442) |  Loss2: (0.0000) | Acc: (81.00%) (13593/16768)
Epoch: 32 | Batch_idx: 140 |  Loss: (0.5411) |  Loss2: (0.0000) | Acc: (81.00%) (14671/18048)
Epoch: 32 | Batch_idx: 150 |  Loss: (0.5427) |  Loss2: (0.0000) | Acc: (81.00%) (15696/19328)
Epoch: 32 | Batch_idx: 160 |  Loss: (0.5434) |  Loss2: (0.0000) | Acc: (81.00%) (16723/20608)
Epoch: 32 | Batch_idx: 170 |  Loss: (0.5410) |  Loss2: (0.0000) | Acc: (81.00%) (17791/21888)
Epoch: 32 | Batch_idx: 180 |  Loss: (0.5393) |  Loss2: (0.0000) | Acc: (81.00%) (18855/23168)
Epoch: 32 | Batch_idx: 190 |  Loss: (0.5400) |  Loss2: (0.0000) | Acc: (81.00%) (19879/24448)
Epoch: 32 | Batch_idx: 200 |  Loss: (0.5406) |  Loss2: (0.0000) | Acc: (81.00%) (20905/25728)
Epoch: 32 | Batch_idx: 210 |  Loss: (0.5387) |  Loss2: (0.0000) | Acc: (81.00%) (21973/27008)
Epoch: 32 | Batch_idx: 220 |  Loss: (0.5391) |  Loss2: (0.0000) | Acc: (81.00%) (23017/28288)
Epoch: 32 | Batch_idx: 230 |  Loss: (0.5370) |  Loss2: (0.0000) | Acc: (81.00%) (24081/29568)
Epoch: 32 | Batch_idx: 240 |  Loss: (0.5384) |  Loss2: (0.0000) | Acc: (81.00%) (25117/30848)
Epoch: 32 | Batch_idx: 250 |  Loss: (0.5394) |  Loss2: (0.0000) | Acc: (81.00%) (26155/32128)
Epoch: 32 | Batch_idx: 260 |  Loss: (0.5393) |  Loss2: (0.0000) | Acc: (81.00%) (27191/33408)
Epoch: 32 | Batch_idx: 270 |  Loss: (0.5403) |  Loss2: (0.0000) | Acc: (81.00%) (28209/34688)
Epoch: 32 | Batch_idx: 280 |  Loss: (0.5399) |  Loss2: (0.0000) | Acc: (81.00%) (29259/35968)
Epoch: 32 | Batch_idx: 290 |  Loss: (0.5394) |  Loss2: (0.0000) | Acc: (81.00%) (30316/37248)
Epoch: 32 | Batch_idx: 300 |  Loss: (0.5371) |  Loss2: (0.0000) | Acc: (81.00%) (31391/38528)
Epoch: 32 | Batch_idx: 310 |  Loss: (0.5363) |  Loss2: (0.0000) | Acc: (81.00%) (32446/39808)
Epoch: 32 | Batch_idx: 320 |  Loss: (0.5361) |  Loss2: (0.0000) | Acc: (81.00%) (33494/41088)
Epoch: 32 | Batch_idx: 330 |  Loss: (0.5362) |  Loss2: (0.0000) | Acc: (81.00%) (34529/42368)
Epoch: 32 | Batch_idx: 340 |  Loss: (0.5365) |  Loss2: (0.0000) | Acc: (81.00%) (35573/43648)
Epoch: 32 | Batch_idx: 350 |  Loss: (0.5361) |  Loss2: (0.0000) | Acc: (81.00%) (36627/44928)
Epoch: 32 | Batch_idx: 360 |  Loss: (0.5363) |  Loss2: (0.0000) | Acc: (81.00%) (37665/46208)
Epoch: 32 | Batch_idx: 370 |  Loss: (0.5373) |  Loss2: (0.0000) | Acc: (81.00%) (38674/47488)
Epoch: 32 | Batch_idx: 380 |  Loss: (0.5362) |  Loss2: (0.0000) | Acc: (81.00%) (39745/48768)
Epoch: 32 | Batch_idx: 390 |  Loss: (0.5362) |  Loss2: (0.0000) | Acc: (81.00%) (40742/50000)
# TEST : Loss: (0.6317) | Acc: (78.00%) (7825/10000)
percent tensor([0.4863, 0.4985, 0.4443, 0.4644, 0.4478, 0.4812, 0.4820, 0.4621, 0.4901,
        0.4827, 0.5037, 0.4508, 0.4903, 0.5092, 0.4834, 0.4855],
       device='cuda:0') torch.Size([16])
percent tensor([0.4870, 0.4804, 0.4723, 0.4887, 0.4733, 0.4864, 0.4754, 0.4841, 0.4822,
        0.4794, 0.4814, 0.4709, 0.4855, 0.4928, 0.4818, 0.4861],
       device='cuda:0') torch.Size([16])
percent tensor([0.5082, 0.4904, 0.4615, 0.4986, 0.4557, 0.4971, 0.4832, 0.4891, 0.5059,
        0.4822, 0.5052, 0.4644, 0.5094, 0.5184, 0.4951, 0.4984],
       device='cuda:0') torch.Size([16])
percent tensor([0.5004, 0.4983, 0.5025, 0.5026, 0.5021, 0.5151, 0.4956, 0.4955, 0.5020,
        0.4967, 0.4972, 0.5001, 0.4974, 0.5000, 0.4991, 0.5059],
       device='cuda:0') torch.Size([16])
percent tensor([0.5999, 0.5721, 0.6117, 0.6250, 0.6164, 0.6608, 0.5868, 0.6023, 0.6135,
        0.5765, 0.5945, 0.5805, 0.5526, 0.6252, 0.5724, 0.6176],
       device='cuda:0') torch.Size([16])
percent tensor([0.5876, 0.5786, 0.6051, 0.6271, 0.6211, 0.6269, 0.6010, 0.6189, 0.5822,
        0.5782, 0.5739, 0.6009, 0.5689, 0.6034, 0.5973, 0.6026],
       device='cuda:0') torch.Size([16])
percent tensor([0.5484, 0.5250, 0.5352, 0.5237, 0.5193, 0.6017, 0.5267, 0.4646, 0.5325,
        0.5213, 0.5404, 0.5066, 0.5302, 0.5660, 0.4764, 0.5554],
       device='cuda:0') torch.Size([16])
percent tensor([0.9981, 0.9960, 0.9973, 0.9948, 0.9977, 0.9957, 0.9973, 0.9981, 0.9968,
        0.9984, 0.9981, 0.9973, 0.9983, 0.9976, 0.9970, 0.9980],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 33 | Batch_idx: 0 |  Loss: (0.4625) |  Loss2: (0.0000) | Acc: (85.00%) (109/128)
Epoch: 33 | Batch_idx: 10 |  Loss: (0.5628) |  Loss2: (0.0000) | Acc: (80.00%) (1136/1408)
Epoch: 33 | Batch_idx: 20 |  Loss: (0.5627) |  Loss2: (0.0000) | Acc: (80.00%) (2165/2688)
Epoch: 33 | Batch_idx: 30 |  Loss: (0.5982) |  Loss2: (0.0000) | Acc: (79.00%) (3152/3968)
Epoch: 33 | Batch_idx: 40 |  Loss: (0.6164) |  Loss2: (0.0000) | Acc: (78.00%) (4142/5248)
Epoch: 33 | Batch_idx: 50 |  Loss: (0.6305) |  Loss2: (0.0000) | Acc: (78.00%) (5118/6528)
Epoch: 33 | Batch_idx: 60 |  Loss: (0.6339) |  Loss2: (0.0000) | Acc: (78.00%) (6101/7808)
Epoch: 33 | Batch_idx: 70 |  Loss: (0.6390) |  Loss2: (0.0000) | Acc: (77.00%) (7085/9088)
Epoch: 33 | Batch_idx: 80 |  Loss: (0.6436) |  Loss2: (0.0000) | Acc: (77.00%) (8072/10368)
Epoch: 33 | Batch_idx: 90 |  Loss: (0.6429) |  Loss2: (0.0000) | Acc: (77.00%) (9084/11648)
Epoch: 33 | Batch_idx: 100 |  Loss: (0.6423) |  Loss2: (0.0000) | Acc: (77.00%) (10071/12928)
Epoch: 33 | Batch_idx: 110 |  Loss: (0.6421) |  Loss2: (0.0000) | Acc: (77.00%) (11066/14208)
Epoch: 33 | Batch_idx: 120 |  Loss: (0.6395) |  Loss2: (0.0000) | Acc: (77.00%) (12071/15488)
Epoch: 33 | Batch_idx: 130 |  Loss: (0.6405) |  Loss2: (0.0000) | Acc: (77.00%) (13048/16768)
Epoch: 33 | Batch_idx: 140 |  Loss: (0.6387) |  Loss2: (0.0000) | Acc: (77.00%) (14051/18048)
Epoch: 33 | Batch_idx: 150 |  Loss: (0.6365) |  Loss2: (0.0000) | Acc: (77.00%) (15067/19328)
Epoch: 33 | Batch_idx: 160 |  Loss: (0.6356) |  Loss2: (0.0000) | Acc: (78.00%) (16088/20608)
Epoch: 33 | Batch_idx: 170 |  Loss: (0.6343) |  Loss2: (0.0000) | Acc: (78.00%) (17099/21888)
Epoch: 33 | Batch_idx: 180 |  Loss: (0.6307) |  Loss2: (0.0000) | Acc: (78.00%) (18125/23168)
Epoch: 33 | Batch_idx: 190 |  Loss: (0.6318) |  Loss2: (0.0000) | Acc: (78.00%) (19105/24448)
Epoch: 33 | Batch_idx: 200 |  Loss: (0.6313) |  Loss2: (0.0000) | Acc: (78.00%) (20101/25728)
Epoch: 33 | Batch_idx: 210 |  Loss: (0.6296) |  Loss2: (0.0000) | Acc: (78.00%) (21117/27008)
Epoch: 33 | Batch_idx: 220 |  Loss: (0.6254) |  Loss2: (0.0000) | Acc: (78.00%) (22160/28288)
Epoch: 33 | Batch_idx: 230 |  Loss: (0.6225) |  Loss2: (0.0000) | Acc: (78.00%) (23185/29568)
Epoch: 33 | Batch_idx: 240 |  Loss: (0.6202) |  Loss2: (0.0000) | Acc: (78.00%) (24229/30848)
Epoch: 33 | Batch_idx: 250 |  Loss: (0.6179) |  Loss2: (0.0000) | Acc: (78.00%) (25253/32128)
Epoch: 33 | Batch_idx: 260 |  Loss: (0.6168) |  Loss2: (0.0000) | Acc: (78.00%) (26280/33408)
Epoch: 33 | Batch_idx: 270 |  Loss: (0.6151) |  Loss2: (0.0000) | Acc: (78.00%) (27310/34688)
Epoch: 33 | Batch_idx: 280 |  Loss: (0.6143) |  Loss2: (0.0000) | Acc: (78.00%) (28327/35968)
Epoch: 33 | Batch_idx: 290 |  Loss: (0.6137) |  Loss2: (0.0000) | Acc: (78.00%) (29338/37248)
Epoch: 33 | Batch_idx: 300 |  Loss: (0.6124) |  Loss2: (0.0000) | Acc: (78.00%) (30363/38528)
Epoch: 33 | Batch_idx: 310 |  Loss: (0.6122) |  Loss2: (0.0000) | Acc: (78.00%) (31378/39808)
Epoch: 33 | Batch_idx: 320 |  Loss: (0.6116) |  Loss2: (0.0000) | Acc: (78.00%) (32390/41088)
Epoch: 33 | Batch_idx: 330 |  Loss: (0.6110) |  Loss2: (0.0000) | Acc: (78.00%) (33407/42368)
Epoch: 33 | Batch_idx: 340 |  Loss: (0.6099) |  Loss2: (0.0000) | Acc: (78.00%) (34431/43648)
Epoch: 33 | Batch_idx: 350 |  Loss: (0.6092) |  Loss2: (0.0000) | Acc: (78.00%) (35452/44928)
Epoch: 33 | Batch_idx: 360 |  Loss: (0.6073) |  Loss2: (0.0000) | Acc: (78.00%) (36489/46208)
Epoch: 33 | Batch_idx: 370 |  Loss: (0.6053) |  Loss2: (0.0000) | Acc: (79.00%) (37525/47488)
Epoch: 33 | Batch_idx: 380 |  Loss: (0.6035) |  Loss2: (0.0000) | Acc: (79.00%) (38559/48768)
Epoch: 33 | Batch_idx: 390 |  Loss: (0.6025) |  Loss2: (0.0000) | Acc: (79.00%) (39550/50000)
# TEST : Loss: (0.6122) | Acc: (79.00%) (7902/10000)
percent tensor([0.4943, 0.5080, 0.4550, 0.4709, 0.4592, 0.4880, 0.4943, 0.4734, 0.5008,
        0.4935, 0.5105, 0.4638, 0.4990, 0.5137, 0.4928, 0.4936],
       device='cuda:0') torch.Size([16])
percent tensor([0.4917, 0.4914, 0.4753, 0.4894, 0.4795, 0.4903, 0.4854, 0.4888, 0.4876,
        0.4863, 0.4877, 0.4756, 0.4919, 0.4954, 0.4905, 0.4914],
       device='cuda:0') torch.Size([16])
percent tensor([0.5029, 0.4917, 0.4517, 0.4965, 0.4510, 0.4956, 0.4812, 0.4855, 0.5018,
        0.4772, 0.5011, 0.4539, 0.5040, 0.5192, 0.4974, 0.4964],
       device='cuda:0') torch.Size([16])
percent tensor([0.5258, 0.5273, 0.5259, 0.5224, 0.5273, 0.5348, 0.5271, 0.5216, 0.5279,
        0.5277, 0.5256, 0.5318, 0.5287, 0.5242, 0.5275, 0.5293],
       device='cuda:0') torch.Size([16])
percent tensor([0.5905, 0.5821, 0.5761, 0.5795, 0.5820, 0.6287, 0.5877, 0.5757, 0.6055,
        0.5898, 0.6028, 0.5787, 0.5766, 0.6181, 0.5736, 0.6034],
       device='cuda:0') torch.Size([16])
percent tensor([0.5717, 0.5593, 0.5881, 0.6118, 0.6012, 0.6155, 0.5822, 0.5972, 0.5670,
        0.5612, 0.5575, 0.5846, 0.5532, 0.5869, 0.5739, 0.5867],
       device='cuda:0') torch.Size([16])
percent tensor([0.5404, 0.4957, 0.5779, 0.5861, 0.5798, 0.6213, 0.5478, 0.5390, 0.5216,
        0.5092, 0.5092, 0.5342, 0.4928, 0.5721, 0.4587, 0.5609],
       device='cuda:0') torch.Size([16])
percent tensor([0.9981, 0.9965, 0.9974, 0.9955, 0.9975, 0.9949, 0.9981, 0.9987, 0.9975,
        0.9984, 0.9980, 0.9979, 0.9982, 0.9978, 0.9978, 0.9984],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 34 | Batch_idx: 0 |  Loss: (0.4611) |  Loss2: (0.0000) | Acc: (84.00%) (108/128)
Epoch: 34 | Batch_idx: 10 |  Loss: (0.5256) |  Loss2: (0.0000) | Acc: (81.00%) (1152/1408)
Epoch: 34 | Batch_idx: 20 |  Loss: (0.5062) |  Loss2: (0.0000) | Acc: (82.00%) (2223/2688)
Epoch: 34 | Batch_idx: 30 |  Loss: (0.5107) |  Loss2: (0.0000) | Acc: (82.00%) (3275/3968)
Epoch: 34 | Batch_idx: 40 |  Loss: (0.5186) |  Loss2: (0.0000) | Acc: (82.00%) (4312/5248)
Epoch: 34 | Batch_idx: 50 |  Loss: (0.5219) |  Loss2: (0.0000) | Acc: (81.00%) (5348/6528)
Epoch: 34 | Batch_idx: 60 |  Loss: (0.5189) |  Loss2: (0.0000) | Acc: (81.00%) (6398/7808)
Epoch: 34 | Batch_idx: 70 |  Loss: (0.5208) |  Loss2: (0.0000) | Acc: (81.00%) (7443/9088)
Epoch: 34 | Batch_idx: 80 |  Loss: (0.5245) |  Loss2: (0.0000) | Acc: (81.00%) (8478/10368)
Epoch: 34 | Batch_idx: 90 |  Loss: (0.5206) |  Loss2: (0.0000) | Acc: (81.00%) (9532/11648)
Epoch: 34 | Batch_idx: 100 |  Loss: (0.5204) |  Loss2: (0.0000) | Acc: (81.00%) (10582/12928)
Epoch: 34 | Batch_idx: 110 |  Loss: (0.5200) |  Loss2: (0.0000) | Acc: (81.00%) (11645/14208)
Epoch: 34 | Batch_idx: 120 |  Loss: (0.5214) |  Loss2: (0.0000) | Acc: (81.00%) (12686/15488)
Epoch: 34 | Batch_idx: 130 |  Loss: (0.5217) |  Loss2: (0.0000) | Acc: (81.00%) (13734/16768)
Epoch: 34 | Batch_idx: 140 |  Loss: (0.5210) |  Loss2: (0.0000) | Acc: (81.00%) (14787/18048)
Epoch: 34 | Batch_idx: 150 |  Loss: (0.5219) |  Loss2: (0.0000) | Acc: (81.00%) (15834/19328)
Epoch: 34 | Batch_idx: 160 |  Loss: (0.5205) |  Loss2: (0.0000) | Acc: (81.00%) (16890/20608)
Epoch: 34 | Batch_idx: 170 |  Loss: (0.5221) |  Loss2: (0.0000) | Acc: (81.00%) (17930/21888)
Epoch: 34 | Batch_idx: 180 |  Loss: (0.5218) |  Loss2: (0.0000) | Acc: (81.00%) (18984/23168)
Epoch: 34 | Batch_idx: 190 |  Loss: (0.5203) |  Loss2: (0.0000) | Acc: (81.00%) (20047/24448)
Epoch: 34 | Batch_idx: 200 |  Loss: (0.5200) |  Loss2: (0.0000) | Acc: (81.00%) (21080/25728)
Epoch: 34 | Batch_idx: 210 |  Loss: (0.5208) |  Loss2: (0.0000) | Acc: (81.00%) (22123/27008)
Epoch: 34 | Batch_idx: 220 |  Loss: (0.5214) |  Loss2: (0.0000) | Acc: (81.00%) (23174/28288)
Epoch: 34 | Batch_idx: 230 |  Loss: (0.5201) |  Loss2: (0.0000) | Acc: (81.00%) (24233/29568)
Epoch: 34 | Batch_idx: 240 |  Loss: (0.5215) |  Loss2: (0.0000) | Acc: (81.00%) (25275/30848)
Epoch: 34 | Batch_idx: 250 |  Loss: (0.5212) |  Loss2: (0.0000) | Acc: (81.00%) (26335/32128)
Epoch: 34 | Batch_idx: 260 |  Loss: (0.5215) |  Loss2: (0.0000) | Acc: (81.00%) (27385/33408)
Epoch: 34 | Batch_idx: 270 |  Loss: (0.5204) |  Loss2: (0.0000) | Acc: (81.00%) (28438/34688)
Epoch: 34 | Batch_idx: 280 |  Loss: (0.5189) |  Loss2: (0.0000) | Acc: (82.00%) (29509/35968)
Epoch: 34 | Batch_idx: 290 |  Loss: (0.5191) |  Loss2: (0.0000) | Acc: (82.00%) (30560/37248)
Epoch: 34 | Batch_idx: 300 |  Loss: (0.5175) |  Loss2: (0.0000) | Acc: (82.00%) (31644/38528)
Epoch: 34 | Batch_idx: 310 |  Loss: (0.5185) |  Loss2: (0.0000) | Acc: (82.00%) (32684/39808)
Epoch: 34 | Batch_idx: 320 |  Loss: (0.5191) |  Loss2: (0.0000) | Acc: (82.00%) (33731/41088)
Epoch: 34 | Batch_idx: 330 |  Loss: (0.5178) |  Loss2: (0.0000) | Acc: (82.00%) (34813/42368)
Epoch: 34 | Batch_idx: 340 |  Loss: (0.5176) |  Loss2: (0.0000) | Acc: (82.00%) (35870/43648)
Epoch: 34 | Batch_idx: 350 |  Loss: (0.5175) |  Loss2: (0.0000) | Acc: (82.00%) (36922/44928)
Epoch: 34 | Batch_idx: 360 |  Loss: (0.5167) |  Loss2: (0.0000) | Acc: (82.00%) (37993/46208)
Epoch: 34 | Batch_idx: 370 |  Loss: (0.5165) |  Loss2: (0.0000) | Acc: (82.00%) (39055/47488)
Epoch: 34 | Batch_idx: 380 |  Loss: (0.5161) |  Loss2: (0.0000) | Acc: (82.00%) (40108/48768)
Epoch: 34 | Batch_idx: 390 |  Loss: (0.5157) |  Loss2: (0.0000) | Acc: (82.00%) (41130/50000)
# TEST : Loss: (0.5871) | Acc: (79.00%) (7975/10000)
percent tensor([0.4931, 0.5093, 0.4538, 0.4711, 0.4595, 0.4859, 0.4959, 0.4745, 0.5021,
        0.4948, 0.5104, 0.4644, 0.4994, 0.5141, 0.4927, 0.4936],
       device='cuda:0') torch.Size([16])
percent tensor([0.4912, 0.4933, 0.4743, 0.4899, 0.4793, 0.4887, 0.4879, 0.4893, 0.4894,
        0.4874, 0.4890, 0.4773, 0.4923, 0.4975, 0.4905, 0.4913],
       device='cuda:0') torch.Size([16])
percent tensor([0.5051, 0.4970, 0.4451, 0.4984, 0.4506, 0.4986, 0.4887, 0.4868, 0.5039,
        0.4824, 0.5063, 0.4590, 0.5073, 0.5204, 0.5017, 0.5003],
       device='cuda:0') torch.Size([16])
percent tensor([0.5270, 0.5281, 0.5274, 0.5253, 0.5261, 0.5351, 0.5273, 0.5219, 0.5279,
        0.5283, 0.5256, 0.5305, 0.5291, 0.5246, 0.5287, 0.5301],
       device='cuda:0') torch.Size([16])
percent tensor([0.5901, 0.5809, 0.5834, 0.5833, 0.5855, 0.6283, 0.5849, 0.5753, 0.6045,
        0.5885, 0.6020, 0.5815, 0.5735, 0.6144, 0.5746, 0.5980],
       device='cuda:0') torch.Size([16])
percent tensor([0.5710, 0.5639, 0.5894, 0.6071, 0.6032, 0.6190, 0.5839, 0.5922, 0.5685,
        0.5618, 0.5603, 0.5824, 0.5515, 0.5855, 0.5764, 0.5883],
       device='cuda:0') torch.Size([16])
percent tensor([0.5357, 0.5061, 0.5865, 0.5909, 0.5971, 0.6277, 0.5449, 0.5411, 0.5247,
        0.5201, 0.5122, 0.5293, 0.4911, 0.5719, 0.4615, 0.5617],
       device='cuda:0') torch.Size([16])
percent tensor([0.9984, 0.9953, 0.9980, 0.9960, 0.9986, 0.9955, 0.9970, 0.9990, 0.9978,
        0.9983, 0.9984, 0.9980, 0.9985, 0.9976, 0.9978, 0.9979],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 35 | Batch_idx: 0 |  Loss: (0.4989) |  Loss2: (0.0000) | Acc: (84.00%) (108/128)
Epoch: 35 | Batch_idx: 10 |  Loss: (0.5581) |  Loss2: (0.0000) | Acc: (81.00%) (1143/1408)
Epoch: 35 | Batch_idx: 20 |  Loss: (0.5771) |  Loss2: (0.0000) | Acc: (80.00%) (2171/2688)
Epoch: 35 | Batch_idx: 30 |  Loss: (0.5925) |  Loss2: (0.0000) | Acc: (79.00%) (3164/3968)
Epoch: 35 | Batch_idx: 40 |  Loss: (0.6064) |  Loss2: (0.0000) | Acc: (78.00%) (4145/5248)
Epoch: 35 | Batch_idx: 50 |  Loss: (0.6007) |  Loss2: (0.0000) | Acc: (78.00%) (5156/6528)
Epoch: 35 | Batch_idx: 60 |  Loss: (0.6093) |  Loss2: (0.0000) | Acc: (78.00%) (6148/7808)
Epoch: 35 | Batch_idx: 70 |  Loss: (0.6199) |  Loss2: (0.0000) | Acc: (78.00%) (7123/9088)
Epoch: 35 | Batch_idx: 80 |  Loss: (0.6199) |  Loss2: (0.0000) | Acc: (78.00%) (8122/10368)
Epoch: 35 | Batch_idx: 90 |  Loss: (0.6236) |  Loss2: (0.0000) | Acc: (78.00%) (9093/11648)
Epoch: 35 | Batch_idx: 100 |  Loss: (0.6260) |  Loss2: (0.0000) | Acc: (78.00%) (10097/12928)
Epoch: 35 | Batch_idx: 110 |  Loss: (0.6248) |  Loss2: (0.0000) | Acc: (78.00%) (11088/14208)
Epoch: 35 | Batch_idx: 120 |  Loss: (0.6239) |  Loss2: (0.0000) | Acc: (78.00%) (12087/15488)
Epoch: 35 | Batch_idx: 130 |  Loss: (0.6241) |  Loss2: (0.0000) | Acc: (78.00%) (13096/16768)
Epoch: 35 | Batch_idx: 140 |  Loss: (0.6230) |  Loss2: (0.0000) | Acc: (78.00%) (14103/18048)
Epoch: 35 | Batch_idx: 150 |  Loss: (0.6258) |  Loss2: (0.0000) | Acc: (78.00%) (15077/19328)
Epoch: 35 | Batch_idx: 160 |  Loss: (0.6263) |  Loss2: (0.0000) | Acc: (77.00%) (16071/20608)
Epoch: 35 | Batch_idx: 170 |  Loss: (0.6263) |  Loss2: (0.0000) | Acc: (77.00%) (17056/21888)
Epoch: 35 | Batch_idx: 180 |  Loss: (0.6244) |  Loss2: (0.0000) | Acc: (77.00%) (18066/23168)
Epoch: 35 | Batch_idx: 190 |  Loss: (0.6248) |  Loss2: (0.0000) | Acc: (77.00%) (19069/24448)
Epoch: 35 | Batch_idx: 200 |  Loss: (0.6244) |  Loss2: (0.0000) | Acc: (78.00%) (20071/25728)
Epoch: 35 | Batch_idx: 210 |  Loss: (0.6249) |  Loss2: (0.0000) | Acc: (78.00%) (21081/27008)
Epoch: 35 | Batch_idx: 220 |  Loss: (0.6239) |  Loss2: (0.0000) | Acc: (78.00%) (22089/28288)
Epoch: 35 | Batch_idx: 230 |  Loss: (0.6219) |  Loss2: (0.0000) | Acc: (78.00%) (23110/29568)
Epoch: 35 | Batch_idx: 240 |  Loss: (0.6216) |  Loss2: (0.0000) | Acc: (78.00%) (24123/30848)
Epoch: 35 | Batch_idx: 250 |  Loss: (0.6210) |  Loss2: (0.0000) | Acc: (78.00%) (25123/32128)
Epoch: 35 | Batch_idx: 260 |  Loss: (0.6194) |  Loss2: (0.0000) | Acc: (78.00%) (26144/33408)
Epoch: 35 | Batch_idx: 270 |  Loss: (0.6199) |  Loss2: (0.0000) | Acc: (78.00%) (27137/34688)
Epoch: 35 | Batch_idx: 280 |  Loss: (0.6199) |  Loss2: (0.0000) | Acc: (78.00%) (28142/35968)
Epoch: 35 | Batch_idx: 290 |  Loss: (0.6195) |  Loss2: (0.0000) | Acc: (78.00%) (29142/37248)
Epoch: 35 | Batch_idx: 300 |  Loss: (0.6188) |  Loss2: (0.0000) | Acc: (78.00%) (30160/38528)
Epoch: 35 | Batch_idx: 310 |  Loss: (0.6177) |  Loss2: (0.0000) | Acc: (78.00%) (31168/39808)
Epoch: 35 | Batch_idx: 320 |  Loss: (0.6163) |  Loss2: (0.0000) | Acc: (78.00%) (32183/41088)
Epoch: 35 | Batch_idx: 330 |  Loss: (0.6151) |  Loss2: (0.0000) | Acc: (78.00%) (33206/42368)
Epoch: 35 | Batch_idx: 340 |  Loss: (0.6142) |  Loss2: (0.0000) | Acc: (78.00%) (34219/43648)
Epoch: 35 | Batch_idx: 350 |  Loss: (0.6138) |  Loss2: (0.0000) | Acc: (78.00%) (35230/44928)
Epoch: 35 | Batch_idx: 360 |  Loss: (0.6123) |  Loss2: (0.0000) | Acc: (78.00%) (36257/46208)
Epoch: 35 | Batch_idx: 370 |  Loss: (0.6116) |  Loss2: (0.0000) | Acc: (78.00%) (37266/47488)
Epoch: 35 | Batch_idx: 380 |  Loss: (0.6106) |  Loss2: (0.0000) | Acc: (78.00%) (38296/48768)
Epoch: 35 | Batch_idx: 390 |  Loss: (0.6101) |  Loss2: (0.0000) | Acc: (78.00%) (39277/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_035.pth.tar'
# TEST : Loss: (0.6156) | Acc: (79.00%) (7921/10000)
percent tensor([0.4903, 0.5063, 0.4559, 0.4710, 0.4608, 0.4794, 0.4951, 0.4751, 0.5001,
        0.4945, 0.5067, 0.4667, 0.4973, 0.5130, 0.4886, 0.4898],
       device='cuda:0') torch.Size([16])
percent tensor([0.4879, 0.4907, 0.4785, 0.4876, 0.4824, 0.4818, 0.4883, 0.4915, 0.4889,
        0.4866, 0.4850, 0.4804, 0.4901, 0.4952, 0.4866, 0.4873],
       device='cuda:0') torch.Size([16])
percent tensor([0.4896, 0.4810, 0.4304, 0.4825, 0.4380, 0.4848, 0.4725, 0.4671, 0.4894,
        0.4645, 0.4907, 0.4395, 0.4850, 0.5103, 0.4836, 0.4834],
       device='cuda:0') torch.Size([16])
percent tensor([0.5244, 0.5250, 0.5346, 0.5291, 0.5336, 0.5286, 0.5306, 0.5326, 0.5262,
        0.5268, 0.5202, 0.5335, 0.5240, 0.5207, 0.5267, 0.5266],
       device='cuda:0') torch.Size([16])
percent tensor([0.6040, 0.5943, 0.6088, 0.6130, 0.6124, 0.6361, 0.6081, 0.6088, 0.6284,
        0.6083, 0.6235, 0.6115, 0.5875, 0.6302, 0.5970, 0.6141],
       device='cuda:0') torch.Size([16])
percent tensor([0.5721, 0.5644, 0.5921, 0.6156, 0.6098, 0.6264, 0.5880, 0.5947, 0.5721,
        0.5618, 0.5630, 0.5838, 0.5471, 0.5885, 0.5788, 0.5906],
       device='cuda:0') torch.Size([16])
percent tensor([0.5585, 0.5237, 0.5895, 0.5915, 0.6099, 0.6606, 0.5598, 0.5491, 0.5475,
        0.5387, 0.5304, 0.5285, 0.5008, 0.5932, 0.4751, 0.5874],
       device='cuda:0') torch.Size([16])
percent tensor([0.9986, 0.9957, 0.9979, 0.9955, 0.9983, 0.9934, 0.9976, 0.9993, 0.9979,
        0.9988, 0.9988, 0.9983, 0.9990, 0.9987, 0.9979, 0.9985],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 36 | Batch_idx: 0 |  Loss: (0.5292) |  Loss2: (0.0000) | Acc: (82.00%) (106/128)
Epoch: 36 | Batch_idx: 10 |  Loss: (0.5253) |  Loss2: (0.0000) | Acc: (82.00%) (1160/1408)
Epoch: 36 | Batch_idx: 20 |  Loss: (0.5046) |  Loss2: (0.0000) | Acc: (82.00%) (2219/2688)
Epoch: 36 | Batch_idx: 30 |  Loss: (0.5064) |  Loss2: (0.0000) | Acc: (82.00%) (3265/3968)
Epoch: 36 | Batch_idx: 40 |  Loss: (0.5107) |  Loss2: (0.0000) | Acc: (82.00%) (4318/5248)
Epoch: 36 | Batch_idx: 50 |  Loss: (0.5102) |  Loss2: (0.0000) | Acc: (82.00%) (5365/6528)
Epoch: 36 | Batch_idx: 60 |  Loss: (0.5126) |  Loss2: (0.0000) | Acc: (81.00%) (6397/7808)
Epoch: 36 | Batch_idx: 70 |  Loss: (0.5140) |  Loss2: (0.0000) | Acc: (81.00%) (7452/9088)
Epoch: 36 | Batch_idx: 80 |  Loss: (0.5126) |  Loss2: (0.0000) | Acc: (82.00%) (8515/10368)
Epoch: 36 | Batch_idx: 90 |  Loss: (0.5101) |  Loss2: (0.0000) | Acc: (82.00%) (9590/11648)
Epoch: 36 | Batch_idx: 100 |  Loss: (0.5026) |  Loss2: (0.0000) | Acc: (82.00%) (10676/12928)
Epoch: 36 | Batch_idx: 110 |  Loss: (0.4995) |  Loss2: (0.0000) | Acc: (82.00%) (11766/14208)
Epoch: 36 | Batch_idx: 120 |  Loss: (0.4998) |  Loss2: (0.0000) | Acc: (82.00%) (12816/15488)
Epoch: 36 | Batch_idx: 130 |  Loss: (0.4998) |  Loss2: (0.0000) | Acc: (82.00%) (13886/16768)
Epoch: 36 | Batch_idx: 140 |  Loss: (0.4974) |  Loss2: (0.0000) | Acc: (82.00%) (14957/18048)
Epoch: 36 | Batch_idx: 150 |  Loss: (0.4985) |  Loss2: (0.0000) | Acc: (82.00%) (16001/19328)
Epoch: 36 | Batch_idx: 160 |  Loss: (0.4987) |  Loss2: (0.0000) | Acc: (82.00%) (17062/20608)
Epoch: 36 | Batch_idx: 170 |  Loss: (0.4971) |  Loss2: (0.0000) | Acc: (82.00%) (18136/21888)
Epoch: 36 | Batch_idx: 180 |  Loss: (0.4978) |  Loss2: (0.0000) | Acc: (82.00%) (19186/23168)
Epoch: 36 | Batch_idx: 190 |  Loss: (0.4972) |  Loss2: (0.0000) | Acc: (82.00%) (20247/24448)
Epoch: 36 | Batch_idx: 200 |  Loss: (0.4982) |  Loss2: (0.0000) | Acc: (82.00%) (21304/25728)
Epoch: 36 | Batch_idx: 210 |  Loss: (0.4973) |  Loss2: (0.0000) | Acc: (82.00%) (22382/27008)
Epoch: 36 | Batch_idx: 220 |  Loss: (0.4988) |  Loss2: (0.0000) | Acc: (82.00%) (23425/28288)
Epoch: 36 | Batch_idx: 230 |  Loss: (0.4978) |  Loss2: (0.0000) | Acc: (82.00%) (24500/29568)
Epoch: 36 | Batch_idx: 240 |  Loss: (0.4982) |  Loss2: (0.0000) | Acc: (82.00%) (25555/30848)
Epoch: 36 | Batch_idx: 250 |  Loss: (0.4989) |  Loss2: (0.0000) | Acc: (82.00%) (26608/32128)
Epoch: 36 | Batch_idx: 260 |  Loss: (0.4983) |  Loss2: (0.0000) | Acc: (82.00%) (27670/33408)
Epoch: 36 | Batch_idx: 270 |  Loss: (0.4980) |  Loss2: (0.0000) | Acc: (82.00%) (28729/34688)
Epoch: 36 | Batch_idx: 280 |  Loss: (0.4985) |  Loss2: (0.0000) | Acc: (82.00%) (29787/35968)
Epoch: 36 | Batch_idx: 290 |  Loss: (0.4979) |  Loss2: (0.0000) | Acc: (82.00%) (30873/37248)
Epoch: 36 | Batch_idx: 300 |  Loss: (0.4979) |  Loss2: (0.0000) | Acc: (82.00%) (31953/38528)
Epoch: 36 | Batch_idx: 310 |  Loss: (0.4978) |  Loss2: (0.0000) | Acc: (82.00%) (33015/39808)
Epoch: 36 | Batch_idx: 320 |  Loss: (0.4969) |  Loss2: (0.0000) | Acc: (82.00%) (34085/41088)
Epoch: 36 | Batch_idx: 330 |  Loss: (0.4970) |  Loss2: (0.0000) | Acc: (82.00%) (35145/42368)
Epoch: 36 | Batch_idx: 340 |  Loss: (0.4973) |  Loss2: (0.0000) | Acc: (82.00%) (36190/43648)
Epoch: 36 | Batch_idx: 350 |  Loss: (0.4968) |  Loss2: (0.0000) | Acc: (82.00%) (37248/44928)
Epoch: 36 | Batch_idx: 360 |  Loss: (0.4965) |  Loss2: (0.0000) | Acc: (82.00%) (38315/46208)
Epoch: 36 | Batch_idx: 370 |  Loss: (0.4965) |  Loss2: (0.0000) | Acc: (82.00%) (39362/47488)
Epoch: 36 | Batch_idx: 380 |  Loss: (0.4973) |  Loss2: (0.0000) | Acc: (82.00%) (40416/48768)
Epoch: 36 | Batch_idx: 390 |  Loss: (0.4977) |  Loss2: (0.0000) | Acc: (82.00%) (41431/50000)
# TEST : Loss: (0.6094) | Acc: (78.00%) (7893/10000)
percent tensor([0.4918, 0.5057, 0.4616, 0.4722, 0.4644, 0.4798, 0.4959, 0.4777, 0.5006,
        0.4952, 0.5069, 0.4705, 0.4977, 0.5117, 0.4887, 0.4904],
       device='cuda:0') torch.Size([16])
percent tensor([0.4893, 0.4913, 0.4788, 0.4896, 0.4815, 0.4835, 0.4880, 0.4935, 0.4888,
        0.4870, 0.4861, 0.4792, 0.4917, 0.4951, 0.4882, 0.4888],
       device='cuda:0') torch.Size([16])
percent tensor([0.4877, 0.4774, 0.4395, 0.4849, 0.4392, 0.4751, 0.4722, 0.4784, 0.4889,
        0.4634, 0.4854, 0.4437, 0.4828, 0.5115, 0.4808, 0.4806],
       device='cuda:0') torch.Size([16])
percent tensor([0.5248, 0.5247, 0.5324, 0.5297, 0.5333, 0.5318, 0.5293, 0.5306, 0.5257,
        0.5269, 0.5213, 0.5324, 0.5247, 0.5209, 0.5268, 0.5279],
       device='cuda:0') torch.Size([16])
percent tensor([0.5955, 0.5902, 0.6011, 0.6129, 0.6097, 0.6322, 0.6066, 0.6039, 0.6162,
        0.6083, 0.6134, 0.6075, 0.5729, 0.6410, 0.5897, 0.6109],
       device='cuda:0') torch.Size([16])
percent tensor([0.5667, 0.5616, 0.5803, 0.6084, 0.6034, 0.6280, 0.5777, 0.5901, 0.5646,
        0.5572, 0.5594, 0.5816, 0.5492, 0.5900, 0.5726, 0.5908],
       device='cuda:0') torch.Size([16])
percent tensor([0.5578, 0.5288, 0.5788, 0.6015, 0.5968, 0.6616, 0.5591, 0.5464, 0.5372,
        0.5313, 0.5356, 0.5321, 0.4942, 0.6046, 0.4780, 0.5931],
       device='cuda:0') torch.Size([16])
percent tensor([0.9986, 0.9966, 0.9979, 0.9968, 0.9981, 0.9947, 0.9977, 0.9992, 0.9979,
        0.9984, 0.9989, 0.9981, 0.9987, 0.9980, 0.9983, 0.9987],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 37 | Batch_idx: 0 |  Loss: (0.5519) |  Loss2: (0.0000) | Acc: (77.00%) (99/128)
Epoch: 37 | Batch_idx: 10 |  Loss: (0.4776) |  Loss2: (0.0000) | Acc: (83.00%) (1173/1408)
Epoch: 37 | Batch_idx: 20 |  Loss: (0.5179) |  Loss2: (0.0000) | Acc: (81.00%) (2197/2688)
Epoch: 37 | Batch_idx: 30 |  Loss: (0.5320) |  Loss2: (0.0000) | Acc: (81.00%) (3216/3968)
Epoch: 37 | Batch_idx: 40 |  Loss: (0.5495) |  Loss2: (0.0000) | Acc: (80.00%) (4238/5248)
Epoch: 37 | Batch_idx: 50 |  Loss: (0.5609) |  Loss2: (0.0000) | Acc: (80.00%) (5235/6528)
Epoch: 37 | Batch_idx: 60 |  Loss: (0.5630) |  Loss2: (0.0000) | Acc: (80.00%) (6269/7808)
Epoch: 37 | Batch_idx: 70 |  Loss: (0.5622) |  Loss2: (0.0000) | Acc: (80.00%) (7301/9088)
Epoch: 37 | Batch_idx: 80 |  Loss: (0.5584) |  Loss2: (0.0000) | Acc: (80.00%) (8351/10368)
Epoch: 37 | Batch_idx: 90 |  Loss: (0.5593) |  Loss2: (0.0000) | Acc: (80.00%) (9385/11648)
Epoch: 37 | Batch_idx: 100 |  Loss: (0.5603) |  Loss2: (0.0000) | Acc: (80.00%) (10410/12928)
Epoch: 37 | Batch_idx: 110 |  Loss: (0.5577) |  Loss2: (0.0000) | Acc: (80.00%) (11451/14208)
Epoch: 37 | Batch_idx: 120 |  Loss: (0.5575) |  Loss2: (0.0000) | Acc: (80.00%) (12469/15488)
Epoch: 37 | Batch_idx: 130 |  Loss: (0.5581) |  Loss2: (0.0000) | Acc: (80.00%) (13503/16768)
Epoch: 37 | Batch_idx: 140 |  Loss: (0.5530) |  Loss2: (0.0000) | Acc: (80.00%) (14569/18048)
Epoch: 37 | Batch_idx: 150 |  Loss: (0.5529) |  Loss2: (0.0000) | Acc: (80.00%) (15600/19328)
Epoch: 37 | Batch_idx: 160 |  Loss: (0.5540) |  Loss2: (0.0000) | Acc: (80.00%) (16622/20608)
Epoch: 37 | Batch_idx: 170 |  Loss: (0.5532) |  Loss2: (0.0000) | Acc: (80.00%) (17653/21888)
Epoch: 37 | Batch_idx: 180 |  Loss: (0.5540) |  Loss2: (0.0000) | Acc: (80.00%) (18674/23168)
Epoch: 37 | Batch_idx: 190 |  Loss: (0.5522) |  Loss2: (0.0000) | Acc: (80.00%) (19719/24448)
Epoch: 37 | Batch_idx: 200 |  Loss: (0.5499) |  Loss2: (0.0000) | Acc: (80.00%) (20772/25728)
Epoch: 37 | Batch_idx: 210 |  Loss: (0.5514) |  Loss2: (0.0000) | Acc: (80.00%) (21785/27008)
Epoch: 37 | Batch_idx: 220 |  Loss: (0.5505) |  Loss2: (0.0000) | Acc: (80.00%) (22827/28288)
Epoch: 37 | Batch_idx: 230 |  Loss: (0.5515) |  Loss2: (0.0000) | Acc: (80.00%) (23860/29568)
Epoch: 37 | Batch_idx: 240 |  Loss: (0.5488) |  Loss2: (0.0000) | Acc: (80.00%) (24927/30848)
Epoch: 37 | Batch_idx: 250 |  Loss: (0.5494) |  Loss2: (0.0000) | Acc: (80.00%) (25964/32128)
Epoch: 37 | Batch_idx: 260 |  Loss: (0.5487) |  Loss2: (0.0000) | Acc: (80.00%) (27020/33408)
Epoch: 37 | Batch_idx: 270 |  Loss: (0.5478) |  Loss2: (0.0000) | Acc: (80.00%) (28057/34688)
Epoch: 37 | Batch_idx: 280 |  Loss: (0.5471) |  Loss2: (0.0000) | Acc: (80.00%) (29106/35968)
Epoch: 37 | Batch_idx: 290 |  Loss: (0.5470) |  Loss2: (0.0000) | Acc: (80.00%) (30135/37248)
Epoch: 37 | Batch_idx: 300 |  Loss: (0.5454) |  Loss2: (0.0000) | Acc: (80.00%) (31186/38528)
Epoch: 37 | Batch_idx: 310 |  Loss: (0.5456) |  Loss2: (0.0000) | Acc: (80.00%) (32200/39808)
Epoch: 37 | Batch_idx: 320 |  Loss: (0.5452) |  Loss2: (0.0000) | Acc: (80.00%) (33235/41088)
Epoch: 37 | Batch_idx: 330 |  Loss: (0.5453) |  Loss2: (0.0000) | Acc: (80.00%) (34270/42368)
Epoch: 37 | Batch_idx: 340 |  Loss: (0.5444) |  Loss2: (0.0000) | Acc: (80.00%) (35326/43648)
Epoch: 37 | Batch_idx: 350 |  Loss: (0.5434) |  Loss2: (0.0000) | Acc: (80.00%) (36379/44928)
Epoch: 37 | Batch_idx: 360 |  Loss: (0.5431) |  Loss2: (0.0000) | Acc: (80.00%) (37419/46208)
Epoch: 37 | Batch_idx: 370 |  Loss: (0.5432) |  Loss2: (0.0000) | Acc: (80.00%) (38460/47488)
Epoch: 37 | Batch_idx: 380 |  Loss: (0.5435) |  Loss2: (0.0000) | Acc: (80.00%) (39498/48768)
Epoch: 37 | Batch_idx: 390 |  Loss: (0.5427) |  Loss2: (0.0000) | Acc: (81.00%) (40523/50000)
# TEST : Loss: (0.5661) | Acc: (80.00%) (8097/10000)
percent tensor([0.5029, 0.5162, 0.4812, 0.4831, 0.4845, 0.4885, 0.5125, 0.4936, 0.5129,
        0.5110, 0.5163, 0.4912, 0.5099, 0.5177, 0.5009, 0.5006],
       device='cuda:0') torch.Size([16])
percent tensor([0.4819, 0.4855, 0.4707, 0.4815, 0.4736, 0.4752, 0.4812, 0.4872, 0.4816,
        0.4808, 0.4791, 0.4713, 0.4854, 0.4897, 0.4814, 0.4818],
       device='cuda:0') torch.Size([16])
percent tensor([0.4773, 0.4632, 0.4280, 0.4739, 0.4197, 0.4639, 0.4540, 0.4611, 0.4774,
        0.4500, 0.4740, 0.4292, 0.4702, 0.5034, 0.4643, 0.4685],
       device='cuda:0') torch.Size([16])
percent tensor([0.5288, 0.5293, 0.5357, 0.5306, 0.5373, 0.5352, 0.5341, 0.5343, 0.5259,
        0.5316, 0.5243, 0.5364, 0.5293, 0.5206, 0.5312, 0.5322],
       device='cuda:0') torch.Size([16])
percent tensor([0.6051, 0.6100, 0.5825, 0.5821, 0.5853, 0.6224, 0.6127, 0.5828, 0.6248,
        0.6245, 0.6351, 0.6020, 0.6103, 0.6445, 0.5990, 0.6168],
       device='cuda:0') torch.Size([16])
percent tensor([0.5903, 0.5793, 0.6089, 0.6374, 0.6319, 0.6589, 0.6048, 0.6176, 0.5910,
        0.5778, 0.5839, 0.6066, 0.5681, 0.6151, 0.5922, 0.6156],
       device='cuda:0') torch.Size([16])
percent tensor([0.5707, 0.5245, 0.5997, 0.6230, 0.6109, 0.6951, 0.5752, 0.5500, 0.5725,
        0.5438, 0.5558, 0.5533, 0.5016, 0.6250, 0.4677, 0.6026],
       device='cuda:0') torch.Size([16])
percent tensor([0.9989, 0.9963, 0.9972, 0.9962, 0.9965, 0.9944, 0.9980, 0.9990, 0.9972,
        0.9985, 0.9988, 0.9972, 0.9985, 0.9978, 0.9983, 0.9993],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 38 | Batch_idx: 0 |  Loss: (0.5536) |  Loss2: (0.0000) | Acc: (79.00%) (102/128)
Epoch: 38 | Batch_idx: 10 |  Loss: (0.4651) |  Loss2: (0.0000) | Acc: (83.00%) (1179/1408)
Epoch: 38 | Batch_idx: 20 |  Loss: (0.4624) |  Loss2: (0.0000) | Acc: (84.00%) (2260/2688)
Epoch: 38 | Batch_idx: 30 |  Loss: (0.4707) |  Loss2: (0.0000) | Acc: (83.00%) (3317/3968)
Epoch: 38 | Batch_idx: 40 |  Loss: (0.4667) |  Loss2: (0.0000) | Acc: (83.00%) (4396/5248)
Epoch: 38 | Batch_idx: 50 |  Loss: (0.4623) |  Loss2: (0.0000) | Acc: (84.00%) (5486/6528)
Epoch: 38 | Batch_idx: 60 |  Loss: (0.4651) |  Loss2: (0.0000) | Acc: (83.00%) (6549/7808)
Epoch: 38 | Batch_idx: 70 |  Loss: (0.4650) |  Loss2: (0.0000) | Acc: (83.00%) (7627/9088)
Epoch: 38 | Batch_idx: 80 |  Loss: (0.4726) |  Loss2: (0.0000) | Acc: (83.00%) (8666/10368)
Epoch: 38 | Batch_idx: 90 |  Loss: (0.4715) |  Loss2: (0.0000) | Acc: (83.00%) (9727/11648)
Epoch: 38 | Batch_idx: 100 |  Loss: (0.4697) |  Loss2: (0.0000) | Acc: (83.00%) (10790/12928)
Epoch: 38 | Batch_idx: 110 |  Loss: (0.4685) |  Loss2: (0.0000) | Acc: (83.00%) (11868/14208)
Epoch: 38 | Batch_idx: 120 |  Loss: (0.4679) |  Loss2: (0.0000) | Acc: (83.00%) (12951/15488)
Epoch: 38 | Batch_idx: 130 |  Loss: (0.4714) |  Loss2: (0.0000) | Acc: (83.00%) (14009/16768)
Epoch: 38 | Batch_idx: 140 |  Loss: (0.4727) |  Loss2: (0.0000) | Acc: (83.00%) (15070/18048)
Epoch: 38 | Batch_idx: 150 |  Loss: (0.4706) |  Loss2: (0.0000) | Acc: (83.00%) (16157/19328)
Epoch: 38 | Batch_idx: 160 |  Loss: (0.4717) |  Loss2: (0.0000) | Acc: (83.00%) (17222/20608)
Epoch: 38 | Batch_idx: 170 |  Loss: (0.4720) |  Loss2: (0.0000) | Acc: (83.00%) (18285/21888)
Epoch: 38 | Batch_idx: 180 |  Loss: (0.4738) |  Loss2: (0.0000) | Acc: (83.00%) (19341/23168)
Epoch: 38 | Batch_idx: 190 |  Loss: (0.4717) |  Loss2: (0.0000) | Acc: (83.00%) (20417/24448)
Epoch: 38 | Batch_idx: 200 |  Loss: (0.4699) |  Loss2: (0.0000) | Acc: (83.00%) (21507/25728)
Epoch: 38 | Batch_idx: 210 |  Loss: (0.4715) |  Loss2: (0.0000) | Acc: (83.00%) (22561/27008)
Epoch: 38 | Batch_idx: 220 |  Loss: (0.4723) |  Loss2: (0.0000) | Acc: (83.00%) (23637/28288)
Epoch: 38 | Batch_idx: 230 |  Loss: (0.4734) |  Loss2: (0.0000) | Acc: (83.00%) (24687/29568)
Epoch: 38 | Batch_idx: 240 |  Loss: (0.4728) |  Loss2: (0.0000) | Acc: (83.00%) (25765/30848)
Epoch: 38 | Batch_idx: 250 |  Loss: (0.4743) |  Loss2: (0.0000) | Acc: (83.00%) (26818/32128)
Epoch: 38 | Batch_idx: 260 |  Loss: (0.4740) |  Loss2: (0.0000) | Acc: (83.00%) (27881/33408)
Epoch: 38 | Batch_idx: 270 |  Loss: (0.4746) |  Loss2: (0.0000) | Acc: (83.00%) (28939/34688)
Epoch: 38 | Batch_idx: 280 |  Loss: (0.4747) |  Loss2: (0.0000) | Acc: (83.00%) (30008/35968)
Epoch: 38 | Batch_idx: 290 |  Loss: (0.4763) |  Loss2: (0.0000) | Acc: (83.00%) (31055/37248)
Epoch: 38 | Batch_idx: 300 |  Loss: (0.4770) |  Loss2: (0.0000) | Acc: (83.00%) (32122/38528)
Epoch: 38 | Batch_idx: 310 |  Loss: (0.4771) |  Loss2: (0.0000) | Acc: (83.00%) (33204/39808)
Epoch: 38 | Batch_idx: 320 |  Loss: (0.4774) |  Loss2: (0.0000) | Acc: (83.00%) (34285/41088)
Epoch: 38 | Batch_idx: 330 |  Loss: (0.4784) |  Loss2: (0.0000) | Acc: (83.00%) (35331/42368)
Epoch: 38 | Batch_idx: 340 |  Loss: (0.4795) |  Loss2: (0.0000) | Acc: (83.00%) (36388/43648)
Epoch: 38 | Batch_idx: 350 |  Loss: (0.4791) |  Loss2: (0.0000) | Acc: (83.00%) (37464/44928)
Epoch: 38 | Batch_idx: 360 |  Loss: (0.4786) |  Loss2: (0.0000) | Acc: (83.00%) (38549/46208)
Epoch: 38 | Batch_idx: 370 |  Loss: (0.4790) |  Loss2: (0.0000) | Acc: (83.00%) (39610/47488)
Epoch: 38 | Batch_idx: 380 |  Loss: (0.4787) |  Loss2: (0.0000) | Acc: (83.00%) (40704/48768)
Epoch: 38 | Batch_idx: 390 |  Loss: (0.4786) |  Loss2: (0.0000) | Acc: (83.00%) (41733/50000)
# TEST : Loss: (0.6078) | Acc: (79.00%) (7966/10000)
percent tensor([0.5018, 0.5174, 0.4721, 0.4843, 0.4761, 0.4886, 0.5112, 0.4926, 0.5111,
        0.5104, 0.5159, 0.4836, 0.5094, 0.5200, 0.5026, 0.5018],
       device='cuda:0') torch.Size([16])
percent tensor([0.4811, 0.4862, 0.4694, 0.4835, 0.4743, 0.4759, 0.4822, 0.4864, 0.4816,
        0.4804, 0.4785, 0.4713, 0.4847, 0.4901, 0.4819, 0.4826],
       device='cuda:0') torch.Size([16])
percent tensor([0.4754, 0.4614, 0.4171, 0.4737, 0.4153, 0.4683, 0.4520, 0.4566, 0.4725,
        0.4448, 0.4714, 0.4224, 0.4663, 0.5030, 0.4652, 0.4696],
       device='cuda:0') torch.Size([16])
percent tensor([0.5304, 0.5313, 0.5364, 0.5293, 0.5373, 0.5357, 0.5354, 0.5324, 0.5281,
        0.5331, 0.5256, 0.5364, 0.5307, 0.5232, 0.5332, 0.5334],
       device='cuda:0') torch.Size([16])
percent tensor([0.6104, 0.6194, 0.5930, 0.5897, 0.5903, 0.6260, 0.6169, 0.5840, 0.6345,
        0.6293, 0.6412, 0.6147, 0.6178, 0.6560, 0.5974, 0.6213],
       device='cuda:0') torch.Size([16])
percent tensor([0.5939, 0.5794, 0.6183, 0.6369, 0.6365, 0.6519, 0.6029, 0.6147, 0.6007,
        0.5786, 0.5823, 0.6141, 0.5763, 0.6112, 0.5881, 0.6123],
       device='cuda:0') torch.Size([16])
percent tensor([0.5717, 0.5210, 0.6141, 0.6284, 0.6276, 0.6869, 0.5717, 0.5359, 0.5777,
        0.5462, 0.5497, 0.5749, 0.5302, 0.6153, 0.4467, 0.5842],
       device='cuda:0') torch.Size([16])
percent tensor([0.9991, 0.9975, 0.9968, 0.9969, 0.9977, 0.9935, 0.9987, 0.9989, 0.9978,
        0.9992, 0.9991, 0.9984, 0.9991, 0.9981, 0.9982, 0.9987],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 39 | Batch_idx: 0 |  Loss: (0.6287) |  Loss2: (0.0000) | Acc: (78.00%) (100/128)
Epoch: 39 | Batch_idx: 10 |  Loss: (0.5201) |  Loss2: (0.0000) | Acc: (82.00%) (1157/1408)
Epoch: 39 | Batch_idx: 20 |  Loss: (0.5278) |  Loss2: (0.0000) | Acc: (82.00%) (2208/2688)
Epoch: 39 | Batch_idx: 30 |  Loss: (0.5312) |  Loss2: (0.0000) | Acc: (81.00%) (3247/3968)
Epoch: 39 | Batch_idx: 40 |  Loss: (0.5461) |  Loss2: (0.0000) | Acc: (81.00%) (4268/5248)
Epoch: 39 | Batch_idx: 50 |  Loss: (0.5417) |  Loss2: (0.0000) | Acc: (81.00%) (5323/6528)
Epoch: 39 | Batch_idx: 60 |  Loss: (0.5461) |  Loss2: (0.0000) | Acc: (81.00%) (6365/7808)
Epoch: 39 | Batch_idx: 70 |  Loss: (0.5447) |  Loss2: (0.0000) | Acc: (81.00%) (7411/9088)
Epoch: 39 | Batch_idx: 80 |  Loss: (0.5542) |  Loss2: (0.0000) | Acc: (81.00%) (8409/10368)
Epoch: 39 | Batch_idx: 90 |  Loss: (0.5591) |  Loss2: (0.0000) | Acc: (81.00%) (9442/11648)
Epoch: 39 | Batch_idx: 100 |  Loss: (0.5579) |  Loss2: (0.0000) | Acc: (81.00%) (10493/12928)
Epoch: 39 | Batch_idx: 110 |  Loss: (0.5589) |  Loss2: (0.0000) | Acc: (81.00%) (11524/14208)
Epoch: 39 | Batch_idx: 120 |  Loss: (0.5579) |  Loss2: (0.0000) | Acc: (81.00%) (12563/15488)
Epoch: 39 | Batch_idx: 130 |  Loss: (0.5582) |  Loss2: (0.0000) | Acc: (81.00%) (13584/16768)
Epoch: 39 | Batch_idx: 140 |  Loss: (0.5605) |  Loss2: (0.0000) | Acc: (80.00%) (14591/18048)
Epoch: 39 | Batch_idx: 150 |  Loss: (0.5603) |  Loss2: (0.0000) | Acc: (80.00%) (15618/19328)
Epoch: 39 | Batch_idx: 160 |  Loss: (0.5600) |  Loss2: (0.0000) | Acc: (80.00%) (16647/20608)
Epoch: 39 | Batch_idx: 170 |  Loss: (0.5594) |  Loss2: (0.0000) | Acc: (80.00%) (17682/21888)
Epoch: 39 | Batch_idx: 180 |  Loss: (0.5575) |  Loss2: (0.0000) | Acc: (80.00%) (18735/23168)
Epoch: 39 | Batch_idx: 190 |  Loss: (0.5574) |  Loss2: (0.0000) | Acc: (80.00%) (19760/24448)
Epoch: 39 | Batch_idx: 200 |  Loss: (0.5586) |  Loss2: (0.0000) | Acc: (80.00%) (20786/25728)
Epoch: 39 | Batch_idx: 210 |  Loss: (0.5566) |  Loss2: (0.0000) | Acc: (80.00%) (21856/27008)
Epoch: 39 | Batch_idx: 220 |  Loss: (0.5564) |  Loss2: (0.0000) | Acc: (80.00%) (22896/28288)
Epoch: 39 | Batch_idx: 230 |  Loss: (0.5562) |  Loss2: (0.0000) | Acc: (80.00%) (23933/29568)
Epoch: 39 | Batch_idx: 240 |  Loss: (0.5548) |  Loss2: (0.0000) | Acc: (80.00%) (24974/30848)
Epoch: 39 | Batch_idx: 250 |  Loss: (0.5551) |  Loss2: (0.0000) | Acc: (80.00%) (26010/32128)
Epoch: 39 | Batch_idx: 260 |  Loss: (0.5534) |  Loss2: (0.0000) | Acc: (81.00%) (27075/33408)
Epoch: 39 | Batch_idx: 270 |  Loss: (0.5525) |  Loss2: (0.0000) | Acc: (81.00%) (28136/34688)
Epoch: 39 | Batch_idx: 280 |  Loss: (0.5518) |  Loss2: (0.0000) | Acc: (81.00%) (29170/35968)
Epoch: 39 | Batch_idx: 290 |  Loss: (0.5509) |  Loss2: (0.0000) | Acc: (81.00%) (30201/37248)
Epoch: 39 | Batch_idx: 300 |  Loss: (0.5492) |  Loss2: (0.0000) | Acc: (81.00%) (31271/38528)
Epoch: 39 | Batch_idx: 310 |  Loss: (0.5487) |  Loss2: (0.0000) | Acc: (81.00%) (32313/39808)
Epoch: 39 | Batch_idx: 320 |  Loss: (0.5465) |  Loss2: (0.0000) | Acc: (81.00%) (33379/41088)
Epoch: 39 | Batch_idx: 330 |  Loss: (0.5458) |  Loss2: (0.0000) | Acc: (81.00%) (34414/42368)
Epoch: 39 | Batch_idx: 340 |  Loss: (0.5440) |  Loss2: (0.0000) | Acc: (81.00%) (35487/43648)
Epoch: 39 | Batch_idx: 350 |  Loss: (0.5429) |  Loss2: (0.0000) | Acc: (81.00%) (36540/44928)
Epoch: 39 | Batch_idx: 360 |  Loss: (0.5426) |  Loss2: (0.0000) | Acc: (81.00%) (37590/46208)
Epoch: 39 | Batch_idx: 370 |  Loss: (0.5410) |  Loss2: (0.0000) | Acc: (81.00%) (38661/47488)
Epoch: 39 | Batch_idx: 380 |  Loss: (0.5397) |  Loss2: (0.0000) | Acc: (81.00%) (39725/48768)
Epoch: 39 | Batch_idx: 390 |  Loss: (0.5396) |  Loss2: (0.0000) | Acc: (81.00%) (40731/50000)
# TEST : Loss: (0.5851) | Acc: (80.00%) (8004/10000)
percent tensor([0.5105, 0.5315, 0.4781, 0.4916, 0.4854, 0.5004, 0.5247, 0.5035, 0.5232,
        0.5223, 0.5271, 0.4930, 0.5193, 0.5340, 0.5166, 0.5127],
       device='cuda:0') torch.Size([16])
percent tensor([0.4878, 0.4936, 0.4766, 0.4898, 0.4821, 0.4831, 0.4906, 0.4935, 0.4890,
        0.4874, 0.4862, 0.4796, 0.4917, 0.4971, 0.4895, 0.4891],
       device='cuda:0') torch.Size([16])
percent tensor([0.5007, 0.4949, 0.4536, 0.5014, 0.4566, 0.5007, 0.4898, 0.4896, 0.5006,
        0.4793, 0.4994, 0.4641, 0.4974, 0.5172, 0.4992, 0.4998],
       device='cuda:0') torch.Size([16])
percent tensor([0.5315, 0.5311, 0.5393, 0.5321, 0.5390, 0.5346, 0.5372, 0.5365, 0.5283,
        0.5332, 0.5254, 0.5398, 0.5318, 0.5217, 0.5353, 0.5325],
       device='cuda:0') torch.Size([16])
percent tensor([0.5857, 0.5969, 0.5967, 0.5855, 0.5968, 0.5810, 0.6137, 0.5954, 0.6167,
        0.6135, 0.6152, 0.6226, 0.5867, 0.6325, 0.5855, 0.5867],
       device='cuda:0') torch.Size([16])
percent tensor([0.6157, 0.6008, 0.6470, 0.6752, 0.6705, 0.6747, 0.6330, 0.6536, 0.6200,
        0.6013, 0.5964, 0.6400, 0.5905, 0.6339, 0.6178, 0.6367],
       device='cuda:0') torch.Size([16])
percent tensor([0.5995, 0.5469, 0.6143, 0.6299, 0.6299, 0.7389, 0.5719, 0.5188, 0.5957,
        0.5590, 0.5806, 0.5428, 0.5627, 0.6312, 0.4746, 0.6141],
       device='cuda:0') torch.Size([16])
percent tensor([0.9988, 0.9962, 0.9967, 0.9969, 0.9972, 0.9937, 0.9980, 0.9992, 0.9978,
        0.9988, 0.9991, 0.9977, 0.9990, 0.9980, 0.9970, 0.9986],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 40 | Batch_idx: 0 |  Loss: (0.5269) |  Loss2: (0.0000) | Acc: (82.00%) (105/128)
Epoch: 40 | Batch_idx: 10 |  Loss: (0.5048) |  Loss2: (0.0000) | Acc: (82.00%) (1157/1408)
Epoch: 40 | Batch_idx: 20 |  Loss: (0.4647) |  Loss2: (0.0000) | Acc: (83.00%) (2255/2688)
Epoch: 40 | Batch_idx: 30 |  Loss: (0.4595) |  Loss2: (0.0000) | Acc: (84.00%) (3334/3968)
Epoch: 40 | Batch_idx: 40 |  Loss: (0.4551) |  Loss2: (0.0000) | Acc: (84.00%) (4429/5248)
Epoch: 40 | Batch_idx: 50 |  Loss: (0.4566) |  Loss2: (0.0000) | Acc: (84.00%) (5496/6528)
Epoch: 40 | Batch_idx: 60 |  Loss: (0.4597) |  Loss2: (0.0000) | Acc: (84.00%) (6582/7808)
Epoch: 40 | Batch_idx: 70 |  Loss: (0.4632) |  Loss2: (0.0000) | Acc: (84.00%) (7657/9088)
Epoch: 40 | Batch_idx: 80 |  Loss: (0.4659) |  Loss2: (0.0000) | Acc: (84.00%) (8724/10368)
Epoch: 40 | Batch_idx: 90 |  Loss: (0.4650) |  Loss2: (0.0000) | Acc: (84.00%) (9810/11648)
Epoch: 40 | Batch_idx: 100 |  Loss: (0.4664) |  Loss2: (0.0000) | Acc: (84.00%) (10875/12928)
Epoch: 40 | Batch_idx: 110 |  Loss: (0.4612) |  Loss2: (0.0000) | Acc: (84.00%) (11974/14208)
Epoch: 40 | Batch_idx: 120 |  Loss: (0.4618) |  Loss2: (0.0000) | Acc: (84.00%) (13040/15488)
Epoch: 40 | Batch_idx: 130 |  Loss: (0.4598) |  Loss2: (0.0000) | Acc: (84.00%) (14125/16768)
Epoch: 40 | Batch_idx: 140 |  Loss: (0.4606) |  Loss2: (0.0000) | Acc: (84.00%) (15194/18048)
Epoch: 40 | Batch_idx: 150 |  Loss: (0.4627) |  Loss2: (0.0000) | Acc: (84.00%) (16261/19328)
Epoch: 40 | Batch_idx: 160 |  Loss: (0.4625) |  Loss2: (0.0000) | Acc: (84.00%) (17340/20608)
Epoch: 40 | Batch_idx: 170 |  Loss: (0.4623) |  Loss2: (0.0000) | Acc: (84.00%) (18422/21888)
Epoch: 40 | Batch_idx: 180 |  Loss: (0.4631) |  Loss2: (0.0000) | Acc: (84.00%) (19493/23168)
Epoch: 40 | Batch_idx: 190 |  Loss: (0.4624) |  Loss2: (0.0000) | Acc: (84.00%) (20578/24448)
Epoch: 40 | Batch_idx: 200 |  Loss: (0.4603) |  Loss2: (0.0000) | Acc: (84.00%) (21685/25728)
Epoch: 40 | Batch_idx: 210 |  Loss: (0.4598) |  Loss2: (0.0000) | Acc: (84.00%) (22770/27008)
Epoch: 40 | Batch_idx: 220 |  Loss: (0.4600) |  Loss2: (0.0000) | Acc: (84.00%) (23837/28288)
Epoch: 40 | Batch_idx: 230 |  Loss: (0.4597) |  Loss2: (0.0000) | Acc: (84.00%) (24920/29568)
Epoch: 40 | Batch_idx: 240 |  Loss: (0.4604) |  Loss2: (0.0000) | Acc: (84.00%) (25987/30848)
Epoch: 40 | Batch_idx: 250 |  Loss: (0.4595) |  Loss2: (0.0000) | Acc: (84.00%) (27069/32128)
Epoch: 40 | Batch_idx: 260 |  Loss: (0.4597) |  Loss2: (0.0000) | Acc: (84.00%) (28142/33408)
Epoch: 40 | Batch_idx: 270 |  Loss: (0.4613) |  Loss2: (0.0000) | Acc: (84.00%) (29203/34688)
Epoch: 40 | Batch_idx: 280 |  Loss: (0.4625) |  Loss2: (0.0000) | Acc: (84.00%) (30269/35968)
Epoch: 40 | Batch_idx: 290 |  Loss: (0.4622) |  Loss2: (0.0000) | Acc: (84.00%) (31344/37248)
Epoch: 40 | Batch_idx: 300 |  Loss: (0.4622) |  Loss2: (0.0000) | Acc: (84.00%) (32409/38528)
Epoch: 40 | Batch_idx: 310 |  Loss: (0.4623) |  Loss2: (0.0000) | Acc: (84.00%) (33493/39808)
Epoch: 40 | Batch_idx: 320 |  Loss: (0.4629) |  Loss2: (0.0000) | Acc: (84.00%) (34571/41088)
Epoch: 40 | Batch_idx: 330 |  Loss: (0.4622) |  Loss2: (0.0000) | Acc: (84.00%) (35666/42368)
Epoch: 40 | Batch_idx: 340 |  Loss: (0.4619) |  Loss2: (0.0000) | Acc: (84.00%) (36742/43648)
Epoch: 40 | Batch_idx: 350 |  Loss: (0.4615) |  Loss2: (0.0000) | Acc: (84.00%) (37827/44928)
Epoch: 40 | Batch_idx: 360 |  Loss: (0.4613) |  Loss2: (0.0000) | Acc: (84.00%) (38917/46208)
Epoch: 40 | Batch_idx: 370 |  Loss: (0.4623) |  Loss2: (0.0000) | Acc: (84.00%) (39969/47488)
Epoch: 40 | Batch_idx: 380 |  Loss: (0.4627) |  Loss2: (0.0000) | Acc: (84.00%) (41035/48768)
Epoch: 40 | Batch_idx: 390 |  Loss: (0.4630) |  Loss2: (0.0000) | Acc: (84.00%) (42074/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_040.pth.tar'
# TEST : Loss: (0.5041) | Acc: (82.00%) (8256/10000)
percent tensor([0.5151, 0.5299, 0.4854, 0.4951, 0.4933, 0.5027, 0.5258, 0.5048, 0.5252,
        0.5229, 0.5280, 0.4982, 0.5219, 0.5319, 0.5159, 0.5138],
       device='cuda:0') torch.Size([16])
percent tensor([0.4907, 0.4935, 0.4753, 0.4884, 0.4814, 0.4849, 0.4901, 0.4918, 0.4904,
        0.4871, 0.4872, 0.4776, 0.4932, 0.4973, 0.4897, 0.4896],
       device='cuda:0') torch.Size([16])
percent tensor([0.5004, 0.4947, 0.4545, 0.4956, 0.4576, 0.4995, 0.4916, 0.4900, 0.5011,
        0.4789, 0.4992, 0.4629, 0.4968, 0.5183, 0.5001, 0.4983],
       device='cuda:0') torch.Size([16])
percent tensor([0.5306, 0.5314, 0.5388, 0.5335, 0.5385, 0.5341, 0.5369, 0.5363, 0.5294,
        0.5330, 0.5261, 0.5400, 0.5323, 0.5220, 0.5343, 0.5328],
       device='cuda:0') torch.Size([16])
percent tensor([0.5897, 0.5935, 0.5938, 0.5847, 0.5921, 0.5934, 0.6113, 0.5969, 0.6166,
        0.6096, 0.6173, 0.6200, 0.5901, 0.6276, 0.5928, 0.5891],
       device='cuda:0') torch.Size([16])
percent tensor([0.6167, 0.6093, 0.6424, 0.6650, 0.6661, 0.6777, 0.6370, 0.6580, 0.6148,
        0.6047, 0.6005, 0.6331, 0.5859, 0.6388, 0.6242, 0.6431],
       device='cuda:0') torch.Size([16])
percent tensor([0.5929, 0.5689, 0.6152, 0.6137, 0.6104, 0.7395, 0.5621, 0.5284, 0.5807,
        0.5638, 0.5724, 0.5508, 0.5404, 0.6394, 0.4724, 0.6308],
       device='cuda:0') torch.Size([16])
percent tensor([0.9986, 0.9963, 0.9975, 0.9968, 0.9986, 0.9964, 0.9984, 0.9993, 0.9978,
        0.9980, 0.9984, 0.9982, 0.9976, 0.9975, 0.9972, 0.9988],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(171.6056, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(794.1172, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(787.0171, device='cuda:0')
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1526.1285, device='cuda:0')
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(505.1029, device='cuda:0')
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2185.1685, device='cuda:0')
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4311.1733, device='cuda:0')
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1430.0802, device='cuda:0')
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6105.9160, device='cuda:0')
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12084.8018, device='cuda:0')
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4027.3115, device='cuda:0')
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(17038.4844, device='cuda:0')
Epoch: 41 | Batch_idx: 0 |  Loss: (0.3904) |  Loss2: (0.0000) | Acc: (87.00%) (112/128)
Epoch: 41 | Batch_idx: 10 |  Loss: (0.5058) |  Loss2: (0.0000) | Acc: (82.00%) (1163/1408)
Epoch: 41 | Batch_idx: 20 |  Loss: (0.4945) |  Loss2: (0.0000) | Acc: (83.00%) (2232/2688)
Epoch: 41 | Batch_idx: 30 |  Loss: (0.5221) |  Loss2: (0.0000) | Acc: (81.00%) (3240/3968)
Epoch: 41 | Batch_idx: 40 |  Loss: (0.5304) |  Loss2: (0.0000) | Acc: (81.00%) (4262/5248)
Epoch: 41 | Batch_idx: 50 |  Loss: (0.5283) |  Loss2: (0.0000) | Acc: (81.00%) (5302/6528)
Epoch: 41 | Batch_idx: 60 |  Loss: (0.5301) |  Loss2: (0.0000) | Acc: (81.00%) (6347/7808)
Epoch: 41 | Batch_idx: 70 |  Loss: (0.5339) |  Loss2: (0.0000) | Acc: (81.00%) (7378/9088)
Epoch: 41 | Batch_idx: 80 |  Loss: (0.5401) |  Loss2: (0.0000) | Acc: (81.00%) (8400/10368)
Epoch: 41 | Batch_idx: 90 |  Loss: (0.5389) |  Loss2: (0.0000) | Acc: (81.00%) (9456/11648)
Epoch: 41 | Batch_idx: 100 |  Loss: (0.5357) |  Loss2: (0.0000) | Acc: (81.00%) (10506/12928)
Epoch: 41 | Batch_idx: 110 |  Loss: (0.5333) |  Loss2: (0.0000) | Acc: (81.00%) (11554/14208)
Epoch: 41 | Batch_idx: 120 |  Loss: (0.5313) |  Loss2: (0.0000) | Acc: (81.00%) (12619/15488)
Epoch: 41 | Batch_idx: 130 |  Loss: (0.5269) |  Loss2: (0.0000) | Acc: (81.00%) (13695/16768)
Epoch: 41 | Batch_idx: 140 |  Loss: (0.5283) |  Loss2: (0.0000) | Acc: (81.00%) (14723/18048)
Epoch: 41 | Batch_idx: 150 |  Loss: (0.5294) |  Loss2: (0.0000) | Acc: (81.00%) (15755/19328)
Epoch: 41 | Batch_idx: 160 |  Loss: (0.5260) |  Loss2: (0.0000) | Acc: (81.00%) (16828/20608)
Epoch: 41 | Batch_idx: 170 |  Loss: (0.5245) |  Loss2: (0.0000) | Acc: (81.00%) (17891/21888)
Epoch: 41 | Batch_idx: 180 |  Loss: (0.5255) |  Loss2: (0.0000) | Acc: (81.00%) (18907/23168)
Epoch: 41 | Batch_idx: 190 |  Loss: (0.5220) |  Loss2: (0.0000) | Acc: (81.00%) (19987/24448)
Epoch: 41 | Batch_idx: 200 |  Loss: (0.5218) |  Loss2: (0.0000) | Acc: (81.00%) (21036/25728)
Epoch: 41 | Batch_idx: 210 |  Loss: (0.5223) |  Loss2: (0.0000) | Acc: (81.00%) (22097/27008)
Epoch: 41 | Batch_idx: 220 |  Loss: (0.5220) |  Loss2: (0.0000) | Acc: (81.00%) (23159/28288)
Epoch: 41 | Batch_idx: 230 |  Loss: (0.5215) |  Loss2: (0.0000) | Acc: (81.00%) (24213/29568)
Epoch: 41 | Batch_idx: 240 |  Loss: (0.5194) |  Loss2: (0.0000) | Acc: (81.00%) (25281/30848)
Epoch: 41 | Batch_idx: 250 |  Loss: (0.5170) |  Loss2: (0.0000) | Acc: (82.00%) (26345/32128)
Epoch: 41 | Batch_idx: 260 |  Loss: (0.5174) |  Loss2: (0.0000) | Acc: (82.00%) (27405/33408)
Epoch: 41 | Batch_idx: 270 |  Loss: (0.5178) |  Loss2: (0.0000) | Acc: (82.00%) (28459/34688)
Epoch: 41 | Batch_idx: 280 |  Loss: (0.5171) |  Loss2: (0.0000) | Acc: (82.00%) (29516/35968)
Epoch: 41 | Batch_idx: 290 |  Loss: (0.5173) |  Loss2: (0.0000) | Acc: (82.00%) (30558/37248)
Epoch: 41 | Batch_idx: 300 |  Loss: (0.5152) |  Loss2: (0.0000) | Acc: (82.00%) (31641/38528)
Epoch: 41 | Batch_idx: 310 |  Loss: (0.5149) |  Loss2: (0.0000) | Acc: (82.00%) (32700/39808)
Epoch: 41 | Batch_idx: 320 |  Loss: (0.5136) |  Loss2: (0.0000) | Acc: (82.00%) (33770/41088)
Epoch: 41 | Batch_idx: 330 |  Loss: (0.5123) |  Loss2: (0.0000) | Acc: (82.00%) (34828/42368)
Epoch: 41 | Batch_idx: 340 |  Loss: (0.5119) |  Loss2: (0.0000) | Acc: (82.00%) (35882/43648)
Epoch: 41 | Batch_idx: 350 |  Loss: (0.5123) |  Loss2: (0.0000) | Acc: (82.00%) (36944/44928)
Epoch: 41 | Batch_idx: 360 |  Loss: (0.5111) |  Loss2: (0.0000) | Acc: (82.00%) (38026/46208)
Epoch: 41 | Batch_idx: 370 |  Loss: (0.5116) |  Loss2: (0.0000) | Acc: (82.00%) (39062/47488)
Epoch: 41 | Batch_idx: 380 |  Loss: (0.5111) |  Loss2: (0.0000) | Acc: (82.00%) (40134/48768)
Epoch: 41 | Batch_idx: 390 |  Loss: (0.5109) |  Loss2: (0.0000) | Acc: (82.00%) (41163/50000)
# TEST : Loss: (0.5431) | Acc: (81.00%) (8160/10000)
percent tensor([0.5155, 0.5306, 0.4883, 0.4983, 0.4956, 0.5042, 0.5272, 0.5084, 0.5261,
        0.5239, 0.5281, 0.4997, 0.5217, 0.5336, 0.5179, 0.5158],
       device='cuda:0') torch.Size([16])
percent tensor([0.4776, 0.4800, 0.4565, 0.4756, 0.4655, 0.4716, 0.4764, 0.4768, 0.4757,
        0.4718, 0.4730, 0.4603, 0.4806, 0.4861, 0.4766, 0.4775],
       device='cuda:0') torch.Size([16])
percent tensor([0.4930, 0.4911, 0.4180, 0.4826, 0.4203, 0.4956, 0.4768, 0.4656, 0.4877,
        0.4639, 0.4916, 0.4343, 0.4900, 0.5194, 0.4927, 0.4911],
       device='cuda:0') torch.Size([16])
percent tensor([0.5598, 0.5607, 0.5713, 0.5607, 0.5700, 0.5603, 0.5704, 0.5662, 0.5588,
        0.5645, 0.5567, 0.5763, 0.5637, 0.5500, 0.5664, 0.5614],
       device='cuda:0') torch.Size([16])
percent tensor([0.5998, 0.6009, 0.6254, 0.6076, 0.6302, 0.5914, 0.6334, 0.6303, 0.6309,
        0.6237, 0.6218, 0.6605, 0.5929, 0.6341, 0.6085, 0.5930],
       device='cuda:0') torch.Size([16])
percent tensor([0.6727, 0.6625, 0.6944, 0.7168, 0.7170, 0.7238, 0.6933, 0.7076, 0.6699,
        0.6616, 0.6582, 0.6839, 0.6451, 0.6925, 0.6773, 0.6966],
       device='cuda:0') torch.Size([16])
percent tensor([0.6628, 0.6485, 0.6153, 0.6183, 0.6123, 0.7951, 0.6092, 0.5015, 0.6502,
        0.6407, 0.6595, 0.5877, 0.6565, 0.7151, 0.5020, 0.6749],
       device='cuda:0') torch.Size([16])
percent tensor([0.9991, 0.9969, 0.9970, 0.9968, 0.9987, 0.9980, 0.9983, 0.9989, 0.9980,
        0.9983, 0.9981, 0.9987, 0.9976, 0.9981, 0.9977, 0.9993],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 42 | Batch_idx: 0 |  Loss: (0.4577) |  Loss2: (0.0000) | Acc: (81.00%) (104/128)
Epoch: 42 | Batch_idx: 10 |  Loss: (0.4408) |  Loss2: (0.0000) | Acc: (84.00%) (1185/1408)
Epoch: 42 | Batch_idx: 20 |  Loss: (0.4617) |  Loss2: (0.0000) | Acc: (83.00%) (2245/2688)
Epoch: 42 | Batch_idx: 30 |  Loss: (0.4616) |  Loss2: (0.0000) | Acc: (83.00%) (3317/3968)
Epoch: 42 | Batch_idx: 40 |  Loss: (0.4478) |  Loss2: (0.0000) | Acc: (84.00%) (4420/5248)
Epoch: 42 | Batch_idx: 50 |  Loss: (0.4444) |  Loss2: (0.0000) | Acc: (84.00%) (5514/6528)
Epoch: 42 | Batch_idx: 60 |  Loss: (0.4475) |  Loss2: (0.0000) | Acc: (84.00%) (6593/7808)
Epoch: 42 | Batch_idx: 70 |  Loss: (0.4498) |  Loss2: (0.0000) | Acc: (84.00%) (7657/9088)
Epoch: 42 | Batch_idx: 80 |  Loss: (0.4462) |  Loss2: (0.0000) | Acc: (84.00%) (8755/10368)
Epoch: 42 | Batch_idx: 90 |  Loss: (0.4423) |  Loss2: (0.0000) | Acc: (84.00%) (9859/11648)
Epoch: 42 | Batch_idx: 100 |  Loss: (0.4434) |  Loss2: (0.0000) | Acc: (84.00%) (10937/12928)
Epoch: 42 | Batch_idx: 110 |  Loss: (0.4420) |  Loss2: (0.0000) | Acc: (84.00%) (12027/14208)
Epoch: 42 | Batch_idx: 120 |  Loss: (0.4418) |  Loss2: (0.0000) | Acc: (84.00%) (13105/15488)
Epoch: 42 | Batch_idx: 130 |  Loss: (0.4402) |  Loss2: (0.0000) | Acc: (84.00%) (14204/16768)
Epoch: 42 | Batch_idx: 140 |  Loss: (0.4435) |  Loss2: (0.0000) | Acc: (84.00%) (15256/18048)
Epoch: 42 | Batch_idx: 150 |  Loss: (0.4432) |  Loss2: (0.0000) | Acc: (84.00%) (16344/19328)
Epoch: 42 | Batch_idx: 160 |  Loss: (0.4437) |  Loss2: (0.0000) | Acc: (84.00%) (17418/20608)
Epoch: 42 | Batch_idx: 170 |  Loss: (0.4449) |  Loss2: (0.0000) | Acc: (84.00%) (18487/21888)
Epoch: 42 | Batch_idx: 180 |  Loss: (0.4437) |  Loss2: (0.0000) | Acc: (84.00%) (19582/23168)
Epoch: 42 | Batch_idx: 190 |  Loss: (0.4430) |  Loss2: (0.0000) | Acc: (84.00%) (20667/24448)
Epoch: 42 | Batch_idx: 200 |  Loss: (0.4423) |  Loss2: (0.0000) | Acc: (84.00%) (21760/25728)
Epoch: 42 | Batch_idx: 210 |  Loss: (0.4416) |  Loss2: (0.0000) | Acc: (84.00%) (22863/27008)
Epoch: 42 | Batch_idx: 220 |  Loss: (0.4403) |  Loss2: (0.0000) | Acc: (84.00%) (23959/28288)
Epoch: 42 | Batch_idx: 230 |  Loss: (0.4404) |  Loss2: (0.0000) | Acc: (84.00%) (25046/29568)
Epoch: 42 | Batch_idx: 240 |  Loss: (0.4407) |  Loss2: (0.0000) | Acc: (84.00%) (26129/30848)
Epoch: 42 | Batch_idx: 250 |  Loss: (0.4410) |  Loss2: (0.0000) | Acc: (84.00%) (27213/32128)
Epoch: 42 | Batch_idx: 260 |  Loss: (0.4422) |  Loss2: (0.0000) | Acc: (84.00%) (28282/33408)
Epoch: 42 | Batch_idx: 270 |  Loss: (0.4429) |  Loss2: (0.0000) | Acc: (84.00%) (29366/34688)
Epoch: 42 | Batch_idx: 280 |  Loss: (0.4432) |  Loss2: (0.0000) | Acc: (84.00%) (30450/35968)
Epoch: 42 | Batch_idx: 290 |  Loss: (0.4416) |  Loss2: (0.0000) | Acc: (84.00%) (31556/37248)
Epoch: 42 | Batch_idx: 300 |  Loss: (0.4417) |  Loss2: (0.0000) | Acc: (84.00%) (32646/38528)
Epoch: 42 | Batch_idx: 310 |  Loss: (0.4434) |  Loss2: (0.0000) | Acc: (84.00%) (33698/39808)
Epoch: 42 | Batch_idx: 320 |  Loss: (0.4428) |  Loss2: (0.0000) | Acc: (84.00%) (34785/41088)
Epoch: 42 | Batch_idx: 330 |  Loss: (0.4432) |  Loss2: (0.0000) | Acc: (84.00%) (35872/42368)
Epoch: 42 | Batch_idx: 340 |  Loss: (0.4433) |  Loss2: (0.0000) | Acc: (84.00%) (36950/43648)
Epoch: 42 | Batch_idx: 350 |  Loss: (0.4448) |  Loss2: (0.0000) | Acc: (84.00%) (38017/44928)
Epoch: 42 | Batch_idx: 360 |  Loss: (0.4449) |  Loss2: (0.0000) | Acc: (84.00%) (39098/46208)
Epoch: 42 | Batch_idx: 370 |  Loss: (0.4434) |  Loss2: (0.0000) | Acc: (84.00%) (40214/47488)
Epoch: 42 | Batch_idx: 380 |  Loss: (0.4433) |  Loss2: (0.0000) | Acc: (84.00%) (41299/48768)
Epoch: 42 | Batch_idx: 390 |  Loss: (0.4446) |  Loss2: (0.0000) | Acc: (84.00%) (42324/50000)
# TEST : Loss: (0.5357) | Acc: (81.00%) (8174/10000)
percent tensor([0.5171, 0.5300, 0.4968, 0.4986, 0.5018, 0.5052, 0.5282, 0.5097, 0.5263,
        0.5248, 0.5279, 0.5068, 0.5217, 0.5314, 0.5183, 0.5158],
       device='cuda:0') torch.Size([16])
percent tensor([0.4786, 0.4799, 0.4596, 0.4751, 0.4659, 0.4724, 0.4753, 0.4776, 0.4755,
        0.4725, 0.4729, 0.4596, 0.4812, 0.4843, 0.4768, 0.4774],
       device='cuda:0') torch.Size([16])
percent tensor([0.4957, 0.4890, 0.4270, 0.4822, 0.4280, 0.4953, 0.4787, 0.4695, 0.4922,
        0.4668, 0.4938, 0.4392, 0.4924, 0.5178, 0.4929, 0.4936],
       device='cuda:0') torch.Size([16])
percent tensor([0.5596, 0.5620, 0.5657, 0.5605, 0.5676, 0.5598, 0.5698, 0.5653, 0.5583,
        0.5656, 0.5564, 0.5734, 0.5638, 0.5538, 0.5684, 0.5610],
       device='cuda:0') torch.Size([16])
percent tensor([0.6014, 0.6095, 0.6064, 0.6089, 0.6125, 0.5859, 0.6267, 0.6252, 0.6437,
        0.6301, 0.6375, 0.6366, 0.5998, 0.6560, 0.6125, 0.5978],
       device='cuda:0') torch.Size([16])
percent tensor([0.6695, 0.6599, 0.6897, 0.7136, 0.7200, 0.7293, 0.6903, 0.6977, 0.6748,
        0.6608, 0.6651, 0.6834, 0.6432, 0.7021, 0.6815, 0.6950],
       device='cuda:0') torch.Size([16])
percent tensor([0.6584, 0.6354, 0.6535, 0.6315, 0.6458, 0.8098, 0.6142, 0.5113, 0.6668,
        0.6464, 0.6659, 0.6207, 0.6214, 0.7101, 0.5115, 0.6817],
       device='cuda:0') torch.Size([16])
percent tensor([0.9992, 0.9971, 0.9982, 0.9969, 0.9986, 0.9984, 0.9977, 0.9991, 0.9979,
        0.9990, 0.9987, 0.9987, 0.9984, 0.9979, 0.9975, 0.9985],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 43 | Batch_idx: 0 |  Loss: (0.4761) |  Loss2: (0.0000) | Acc: (84.00%) (108/128)
Epoch: 43 | Batch_idx: 10 |  Loss: (0.4372) |  Loss2: (0.0000) | Acc: (85.00%) (1197/1408)
Epoch: 43 | Batch_idx: 20 |  Loss: (0.4832) |  Loss2: (0.0000) | Acc: (83.00%) (2246/2688)
Epoch: 43 | Batch_idx: 30 |  Loss: (0.5092) |  Loss2: (0.0000) | Acc: (82.00%) (3278/3968)
Epoch: 43 | Batch_idx: 40 |  Loss: (0.5109) |  Loss2: (0.0000) | Acc: (82.00%) (4334/5248)
Epoch: 43 | Batch_idx: 50 |  Loss: (0.5092) |  Loss2: (0.0000) | Acc: (82.00%) (5396/6528)
Epoch: 43 | Batch_idx: 60 |  Loss: (0.5114) |  Loss2: (0.0000) | Acc: (82.00%) (6445/7808)
Epoch: 43 | Batch_idx: 70 |  Loss: (0.5107) |  Loss2: (0.0000) | Acc: (82.00%) (7497/9088)
Epoch: 43 | Batch_idx: 80 |  Loss: (0.5187) |  Loss2: (0.0000) | Acc: (82.00%) (8523/10368)
Epoch: 43 | Batch_idx: 90 |  Loss: (0.5217) |  Loss2: (0.0000) | Acc: (82.00%) (9565/11648)
Epoch: 43 | Batch_idx: 100 |  Loss: (0.5182) |  Loss2: (0.0000) | Acc: (82.00%) (10651/12928)
Epoch: 43 | Batch_idx: 110 |  Loss: (0.5209) |  Loss2: (0.0000) | Acc: (82.00%) (11688/14208)
Epoch: 43 | Batch_idx: 120 |  Loss: (0.5246) |  Loss2: (0.0000) | Acc: (82.00%) (12715/15488)
Epoch: 43 | Batch_idx: 130 |  Loss: (0.5254) |  Loss2: (0.0000) | Acc: (82.00%) (13765/16768)
Epoch: 43 | Batch_idx: 140 |  Loss: (0.5246) |  Loss2: (0.0000) | Acc: (82.00%) (14833/18048)
Epoch: 43 | Batch_idx: 150 |  Loss: (0.5248) |  Loss2: (0.0000) | Acc: (82.00%) (15873/19328)
Epoch: 43 | Batch_idx: 160 |  Loss: (0.5229) |  Loss2: (0.0000) | Acc: (82.00%) (16939/20608)
Epoch: 43 | Batch_idx: 170 |  Loss: (0.5221) |  Loss2: (0.0000) | Acc: (82.00%) (18004/21888)
Epoch: 43 | Batch_idx: 180 |  Loss: (0.5202) |  Loss2: (0.0000) | Acc: (82.00%) (19082/23168)
Epoch: 43 | Batch_idx: 190 |  Loss: (0.5208) |  Loss2: (0.0000) | Acc: (82.00%) (20125/24448)
Epoch: 43 | Batch_idx: 200 |  Loss: (0.5190) |  Loss2: (0.0000) | Acc: (82.00%) (21176/25728)
Epoch: 43 | Batch_idx: 210 |  Loss: (0.5163) |  Loss2: (0.0000) | Acc: (82.00%) (22248/27008)
Epoch: 43 | Batch_idx: 220 |  Loss: (0.5162) |  Loss2: (0.0000) | Acc: (82.00%) (23291/28288)
Epoch: 43 | Batch_idx: 230 |  Loss: (0.5159) |  Loss2: (0.0000) | Acc: (82.00%) (24356/29568)
Epoch: 43 | Batch_idx: 240 |  Loss: (0.5155) |  Loss2: (0.0000) | Acc: (82.00%) (25407/30848)
Epoch: 43 | Batch_idx: 250 |  Loss: (0.5165) |  Loss2: (0.0000) | Acc: (82.00%) (26441/32128)
Epoch: 43 | Batch_idx: 260 |  Loss: (0.5155) |  Loss2: (0.0000) | Acc: (82.00%) (27503/33408)
Epoch: 43 | Batch_idx: 270 |  Loss: (0.5161) |  Loss2: (0.0000) | Acc: (82.00%) (28536/34688)
Epoch: 43 | Batch_idx: 280 |  Loss: (0.5146) |  Loss2: (0.0000) | Acc: (82.00%) (29606/35968)
Epoch: 43 | Batch_idx: 290 |  Loss: (0.5159) |  Loss2: (0.0000) | Acc: (82.00%) (30639/37248)
Epoch: 43 | Batch_idx: 300 |  Loss: (0.5166) |  Loss2: (0.0000) | Acc: (82.00%) (31678/38528)
Epoch: 43 | Batch_idx: 310 |  Loss: (0.5163) |  Loss2: (0.0000) | Acc: (82.00%) (32724/39808)
Epoch: 43 | Batch_idx: 320 |  Loss: (0.5159) |  Loss2: (0.0000) | Acc: (82.00%) (33789/41088)
Epoch: 43 | Batch_idx: 330 |  Loss: (0.5171) |  Loss2: (0.0000) | Acc: (82.00%) (34809/42368)
Epoch: 43 | Batch_idx: 340 |  Loss: (0.5171) |  Loss2: (0.0000) | Acc: (82.00%) (35861/43648)
Epoch: 43 | Batch_idx: 350 |  Loss: (0.5159) |  Loss2: (0.0000) | Acc: (82.00%) (36930/44928)
Epoch: 43 | Batch_idx: 360 |  Loss: (0.5151) |  Loss2: (0.0000) | Acc: (82.00%) (37993/46208)
Epoch: 43 | Batch_idx: 370 |  Loss: (0.5148) |  Loss2: (0.0000) | Acc: (82.00%) (39042/47488)
Epoch: 43 | Batch_idx: 380 |  Loss: (0.5137) |  Loss2: (0.0000) | Acc: (82.00%) (40102/48768)
Epoch: 43 | Batch_idx: 390 |  Loss: (0.5131) |  Loss2: (0.0000) | Acc: (82.00%) (41121/50000)
# TEST : Loss: (0.5595) | Acc: (80.00%) (8069/10000)
percent tensor([0.5325, 0.5483, 0.5289, 0.5175, 0.5317, 0.5209, 0.5486, 0.5328, 0.5443,
        0.5436, 0.5444, 0.5362, 0.5377, 0.5464, 0.5354, 0.5310],
       device='cuda:0') torch.Size([16])
percent tensor([0.4683, 0.4692, 0.4416, 0.4600, 0.4486, 0.4603, 0.4621, 0.4637, 0.4628,
        0.4599, 0.4618, 0.4420, 0.4711, 0.4745, 0.4663, 0.4659],
       device='cuda:0') torch.Size([16])
percent tensor([0.5012, 0.4881, 0.4327, 0.4842, 0.4211, 0.4937, 0.4740, 0.4681, 0.4958,
        0.4724, 0.4989, 0.4465, 0.4991, 0.5203, 0.4931, 0.4958],
       device='cuda:0') torch.Size([16])
percent tensor([0.5527, 0.5584, 0.5532, 0.5498, 0.5563, 0.5540, 0.5622, 0.5548, 0.5511,
        0.5603, 0.5518, 0.5624, 0.5600, 0.5491, 0.5616, 0.5556],
       device='cuda:0') torch.Size([16])
percent tensor([0.5763, 0.5820, 0.5626, 0.5737, 0.5773, 0.5801, 0.5966, 0.5931, 0.6218,
        0.5933, 0.6068, 0.5794, 0.5762, 0.6263, 0.5867, 0.5814],
       device='cuda:0') torch.Size([16])
percent tensor([0.6227, 0.6173, 0.6389, 0.6649, 0.6699, 0.6890, 0.6443, 0.6493, 0.6392,
        0.6126, 0.6208, 0.6293, 0.5957, 0.6499, 0.6309, 0.6544],
       device='cuda:0') torch.Size([16])
percent tensor([0.5657, 0.5487, 0.5734, 0.5869, 0.5832, 0.7432, 0.5466, 0.4557, 0.6078,
        0.5528, 0.5979, 0.5309, 0.5192, 0.6300, 0.4278, 0.6001],
       device='cuda:0') torch.Size([16])
percent tensor([0.9990, 0.9971, 0.9976, 0.9974, 0.9981, 0.9981, 0.9961, 0.9989, 0.9974,
        0.9987, 0.9985, 0.9988, 0.9986, 0.9977, 0.9980, 0.9987],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 44 | Batch_idx: 0 |  Loss: (0.5457) |  Loss2: (0.0000) | Acc: (80.00%) (103/128)
Epoch: 44 | Batch_idx: 10 |  Loss: (0.4256) |  Loss2: (0.0000) | Acc: (86.00%) (1217/1408)
Epoch: 44 | Batch_idx: 20 |  Loss: (0.4173) |  Loss2: (0.0000) | Acc: (85.00%) (2308/2688)
Epoch: 44 | Batch_idx: 30 |  Loss: (0.4308) |  Loss2: (0.0000) | Acc: (85.00%) (3373/3968)
Epoch: 44 | Batch_idx: 40 |  Loss: (0.4332) |  Loss2: (0.0000) | Acc: (84.00%) (4458/5248)
Epoch: 44 | Batch_idx: 50 |  Loss: (0.4422) |  Loss2: (0.0000) | Acc: (84.00%) (5532/6528)
Epoch: 44 | Batch_idx: 60 |  Loss: (0.4416) |  Loss2: (0.0000) | Acc: (84.00%) (6620/7808)
Epoch: 44 | Batch_idx: 70 |  Loss: (0.4355) |  Loss2: (0.0000) | Acc: (84.00%) (7722/9088)
Epoch: 44 | Batch_idx: 80 |  Loss: (0.4309) |  Loss2: (0.0000) | Acc: (85.00%) (8839/10368)
Epoch: 44 | Batch_idx: 90 |  Loss: (0.4314) |  Loss2: (0.0000) | Acc: (85.00%) (9932/11648)
Epoch: 44 | Batch_idx: 100 |  Loss: (0.4336) |  Loss2: (0.0000) | Acc: (85.00%) (11016/12928)
Epoch: 44 | Batch_idx: 110 |  Loss: (0.4370) |  Loss2: (0.0000) | Acc: (85.00%) (12080/14208)
Epoch: 44 | Batch_idx: 120 |  Loss: (0.4334) |  Loss2: (0.0000) | Acc: (85.00%) (13178/15488)
Epoch: 44 | Batch_idx: 130 |  Loss: (0.4329) |  Loss2: (0.0000) | Acc: (85.00%) (14253/16768)
Epoch: 44 | Batch_idx: 140 |  Loss: (0.4338) |  Loss2: (0.0000) | Acc: (84.00%) (15337/18048)
Epoch: 44 | Batch_idx: 150 |  Loss: (0.4336) |  Loss2: (0.0000) | Acc: (85.00%) (16437/19328)
Epoch: 44 | Batch_idx: 160 |  Loss: (0.4302) |  Loss2: (0.0000) | Acc: (85.00%) (17551/20608)
Epoch: 44 | Batch_idx: 170 |  Loss: (0.4298) |  Loss2: (0.0000) | Acc: (85.00%) (18637/21888)
Epoch: 44 | Batch_idx: 180 |  Loss: (0.4305) |  Loss2: (0.0000) | Acc: (85.00%) (19731/23168)
Epoch: 44 | Batch_idx: 190 |  Loss: (0.4300) |  Loss2: (0.0000) | Acc: (85.00%) (20835/24448)
Epoch: 44 | Batch_idx: 200 |  Loss: (0.4315) |  Loss2: (0.0000) | Acc: (85.00%) (21908/25728)
Epoch: 44 | Batch_idx: 210 |  Loss: (0.4320) |  Loss2: (0.0000) | Acc: (85.00%) (23011/27008)
Epoch: 44 | Batch_idx: 220 |  Loss: (0.4321) |  Loss2: (0.0000) | Acc: (85.00%) (24101/28288)
Epoch: 44 | Batch_idx: 230 |  Loss: (0.4320) |  Loss2: (0.0000) | Acc: (85.00%) (25202/29568)
Epoch: 44 | Batch_idx: 240 |  Loss: (0.4309) |  Loss2: (0.0000) | Acc: (85.00%) (26306/30848)
Epoch: 44 | Batch_idx: 250 |  Loss: (0.4311) |  Loss2: (0.0000) | Acc: (85.00%) (27394/32128)
Epoch: 44 | Batch_idx: 260 |  Loss: (0.4310) |  Loss2: (0.0000) | Acc: (85.00%) (28487/33408)
Epoch: 44 | Batch_idx: 270 |  Loss: (0.4295) |  Loss2: (0.0000) | Acc: (85.00%) (29589/34688)
Epoch: 44 | Batch_idx: 280 |  Loss: (0.4320) |  Loss2: (0.0000) | Acc: (85.00%) (30642/35968)
Epoch: 44 | Batch_idx: 290 |  Loss: (0.4315) |  Loss2: (0.0000) | Acc: (85.00%) (31745/37248)
Epoch: 44 | Batch_idx: 300 |  Loss: (0.4302) |  Loss2: (0.0000) | Acc: (85.00%) (32849/38528)
Epoch: 44 | Batch_idx: 310 |  Loss: (0.4294) |  Loss2: (0.0000) | Acc: (85.00%) (33943/39808)
Epoch: 44 | Batch_idx: 320 |  Loss: (0.4297) |  Loss2: (0.0000) | Acc: (85.00%) (35037/41088)
Epoch: 44 | Batch_idx: 330 |  Loss: (0.4301) |  Loss2: (0.0000) | Acc: (85.00%) (36115/42368)
Epoch: 44 | Batch_idx: 340 |  Loss: (0.4297) |  Loss2: (0.0000) | Acc: (85.00%) (37208/43648)
Epoch: 44 | Batch_idx: 350 |  Loss: (0.4297) |  Loss2: (0.0000) | Acc: (85.00%) (38299/44928)
Epoch: 44 | Batch_idx: 360 |  Loss: (0.4298) |  Loss2: (0.0000) | Acc: (85.00%) (39387/46208)
Epoch: 44 | Batch_idx: 370 |  Loss: (0.4298) |  Loss2: (0.0000) | Acc: (85.00%) (40480/47488)
Epoch: 44 | Batch_idx: 380 |  Loss: (0.4298) |  Loss2: (0.0000) | Acc: (85.00%) (41575/48768)
Epoch: 44 | Batch_idx: 390 |  Loss: (0.4292) |  Loss2: (0.0000) | Acc: (85.00%) (42625/50000)
# TEST : Loss: (0.5409) | Acc: (81.00%) (8179/10000)
percent tensor([0.5312, 0.5480, 0.5184, 0.5162, 0.5237, 0.5201, 0.5462, 0.5309, 0.5434,
        0.5417, 0.5443, 0.5285, 0.5370, 0.5476, 0.5349, 0.5306],
       device='cuda:0') torch.Size([16])
percent tensor([0.4647, 0.4676, 0.4400, 0.4593, 0.4493, 0.4604, 0.4617, 0.4643, 0.4626,
        0.4566, 0.4597, 0.4400, 0.4686, 0.4757, 0.4648, 0.4650],
       device='cuda:0') torch.Size([16])
percent tensor([0.4997, 0.4906, 0.4287, 0.4844, 0.4205, 0.4984, 0.4759, 0.4714, 0.4908,
        0.4691, 0.4958, 0.4420, 0.4969, 0.5217, 0.4953, 0.4985],
       device='cuda:0') torch.Size([16])
percent tensor([0.5533, 0.5575, 0.5578, 0.5522, 0.5591, 0.5555, 0.5631, 0.5545, 0.5523,
        0.5603, 0.5512, 0.5676, 0.5592, 0.5480, 0.5611, 0.5567],
       device='cuda:0') torch.Size([16])
percent tensor([0.5846, 0.5801, 0.5885, 0.5859, 0.5964, 0.5764, 0.6004, 0.5971, 0.6120,
        0.6019, 0.6054, 0.6167, 0.5712, 0.6150, 0.5833, 0.5853],
       device='cuda:0') torch.Size([16])
percent tensor([0.6258, 0.6140, 0.6487, 0.6724, 0.6803, 0.6831, 0.6434, 0.6548, 0.6338,
        0.6145, 0.6153, 0.6391, 0.5978, 0.6549, 0.6261, 0.6515],
       device='cuda:0') torch.Size([16])
percent tensor([0.5750, 0.5525, 0.6245, 0.6229, 0.6305, 0.7320, 0.5497, 0.4838, 0.6117,
        0.5595, 0.5833, 0.5639, 0.5478, 0.6351, 0.4410, 0.5969],
       device='cuda:0') torch.Size([16])
percent tensor([0.9993, 0.9977, 0.9975, 0.9973, 0.9977, 0.9969, 0.9976, 0.9990, 0.9974,
        0.9989, 0.9992, 0.9984, 0.9988, 0.9985, 0.9977, 0.9985],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 45 | Batch_idx: 0 |  Loss: (0.2644) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 45 | Batch_idx: 10 |  Loss: (0.4335) |  Loss2: (0.0000) | Acc: (85.00%) (1201/1408)
Epoch: 45 | Batch_idx: 20 |  Loss: (0.4778) |  Loss2: (0.0000) | Acc: (83.00%) (2253/2688)
Epoch: 45 | Batch_idx: 30 |  Loss: (0.4949) |  Loss2: (0.0000) | Acc: (82.00%) (3293/3968)
Epoch: 45 | Batch_idx: 40 |  Loss: (0.4979) |  Loss2: (0.0000) | Acc: (82.00%) (4351/5248)
Epoch: 45 | Batch_idx: 50 |  Loss: (0.4973) |  Loss2: (0.0000) | Acc: (83.00%) (5420/6528)
Epoch: 45 | Batch_idx: 60 |  Loss: (0.4994) |  Loss2: (0.0000) | Acc: (82.00%) (6454/7808)
Epoch: 45 | Batch_idx: 70 |  Loss: (0.4978) |  Loss2: (0.0000) | Acc: (82.00%) (7510/9088)
Epoch: 45 | Batch_idx: 80 |  Loss: (0.5037) |  Loss2: (0.0000) | Acc: (82.00%) (8552/10368)
Epoch: 45 | Batch_idx: 90 |  Loss: (0.5062) |  Loss2: (0.0000) | Acc: (82.00%) (9599/11648)
Epoch: 45 | Batch_idx: 100 |  Loss: (0.5044) |  Loss2: (0.0000) | Acc: (82.00%) (10662/12928)
Epoch: 45 | Batch_idx: 110 |  Loss: (0.5057) |  Loss2: (0.0000) | Acc: (82.00%) (11695/14208)
Epoch: 45 | Batch_idx: 120 |  Loss: (0.5029) |  Loss2: (0.0000) | Acc: (82.00%) (12754/15488)
Epoch: 45 | Batch_idx: 130 |  Loss: (0.5020) |  Loss2: (0.0000) | Acc: (82.00%) (13825/16768)
Epoch: 45 | Batch_idx: 140 |  Loss: (0.5004) |  Loss2: (0.0000) | Acc: (82.00%) (14903/18048)
Epoch: 45 | Batch_idx: 150 |  Loss: (0.4984) |  Loss2: (0.0000) | Acc: (82.00%) (15972/19328)
Epoch: 45 | Batch_idx: 160 |  Loss: (0.4982) |  Loss2: (0.0000) | Acc: (82.00%) (17034/20608)
Epoch: 45 | Batch_idx: 170 |  Loss: (0.4969) |  Loss2: (0.0000) | Acc: (82.00%) (18109/21888)
Epoch: 45 | Batch_idx: 180 |  Loss: (0.4967) |  Loss2: (0.0000) | Acc: (82.00%) (19179/23168)
Epoch: 45 | Batch_idx: 190 |  Loss: (0.4944) |  Loss2: (0.0000) | Acc: (82.00%) (20252/24448)
Epoch: 45 | Batch_idx: 200 |  Loss: (0.4935) |  Loss2: (0.0000) | Acc: (82.00%) (21318/25728)
Epoch: 45 | Batch_idx: 210 |  Loss: (0.4940) |  Loss2: (0.0000) | Acc: (82.00%) (22381/27008)
Epoch: 45 | Batch_idx: 220 |  Loss: (0.4911) |  Loss2: (0.0000) | Acc: (82.00%) (23464/28288)
Epoch: 45 | Batch_idx: 230 |  Loss: (0.4896) |  Loss2: (0.0000) | Acc: (82.00%) (24541/29568)
Epoch: 45 | Batch_idx: 240 |  Loss: (0.4899) |  Loss2: (0.0000) | Acc: (82.00%) (25598/30848)
Epoch: 45 | Batch_idx: 250 |  Loss: (0.4899) |  Loss2: (0.0000) | Acc: (82.00%) (26657/32128)
Epoch: 45 | Batch_idx: 260 |  Loss: (0.4886) |  Loss2: (0.0000) | Acc: (83.00%) (27732/33408)
Epoch: 45 | Batch_idx: 270 |  Loss: (0.4886) |  Loss2: (0.0000) | Acc: (83.00%) (28795/34688)
Epoch: 45 | Batch_idx: 280 |  Loss: (0.4885) |  Loss2: (0.0000) | Acc: (83.00%) (29862/35968)
Epoch: 45 | Batch_idx: 290 |  Loss: (0.4872) |  Loss2: (0.0000) | Acc: (83.00%) (30949/37248)
Epoch: 45 | Batch_idx: 300 |  Loss: (0.4867) |  Loss2: (0.0000) | Acc: (83.00%) (32021/38528)
Epoch: 45 | Batch_idx: 310 |  Loss: (0.4860) |  Loss2: (0.0000) | Acc: (83.00%) (33096/39808)
Epoch: 45 | Batch_idx: 320 |  Loss: (0.4854) |  Loss2: (0.0000) | Acc: (83.00%) (34165/41088)
Epoch: 45 | Batch_idx: 330 |  Loss: (0.4859) |  Loss2: (0.0000) | Acc: (83.00%) (35223/42368)
Epoch: 45 | Batch_idx: 340 |  Loss: (0.4853) |  Loss2: (0.0000) | Acc: (83.00%) (36302/43648)
Epoch: 45 | Batch_idx: 350 |  Loss: (0.4846) |  Loss2: (0.0000) | Acc: (83.00%) (37372/44928)
Epoch: 45 | Batch_idx: 360 |  Loss: (0.4833) |  Loss2: (0.0000) | Acc: (83.00%) (38460/46208)
Epoch: 45 | Batch_idx: 370 |  Loss: (0.4829) |  Loss2: (0.0000) | Acc: (83.00%) (39540/47488)
Epoch: 45 | Batch_idx: 380 |  Loss: (0.4833) |  Loss2: (0.0000) | Acc: (83.00%) (40612/48768)
Epoch: 45 | Batch_idx: 390 |  Loss: (0.4813) |  Loss2: (0.0000) | Acc: (83.00%) (41668/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_045.pth.tar'
# TEST : Loss: (0.5364) | Acc: (82.00%) (8200/10000)
percent tensor([0.5423, 0.5622, 0.5341, 0.5300, 0.5398, 0.5340, 0.5603, 0.5466, 0.5554,
        0.5547, 0.5566, 0.5440, 0.5485, 0.5610, 0.5494, 0.5432],
       device='cuda:0') torch.Size([16])
percent tensor([0.4676, 0.4705, 0.4443, 0.4655, 0.4537, 0.4655, 0.4652, 0.4682, 0.4657,
        0.4595, 0.4616, 0.4427, 0.4700, 0.4809, 0.4681, 0.4692],
       device='cuda:0') torch.Size([16])
percent tensor([0.4775, 0.4676, 0.4202, 0.4723, 0.4156, 0.4800, 0.4559, 0.4581, 0.4722,
        0.4473, 0.4689, 0.4260, 0.4700, 0.5132, 0.4698, 0.4763],
       device='cuda:0') torch.Size([16])
percent tensor([0.5437, 0.5433, 0.5485, 0.5443, 0.5497, 0.5508, 0.5495, 0.5462, 0.5427,
        0.5446, 0.5381, 0.5534, 0.5459, 0.5374, 0.5505, 0.5462],
       device='cuda:0') torch.Size([16])
percent tensor([0.5547, 0.5435, 0.5637, 0.5633, 0.5764, 0.5647, 0.5668, 0.5731, 0.5914,
        0.5577, 0.5713, 0.5813, 0.5371, 0.5863, 0.5548, 0.5571],
       device='cuda:0') torch.Size([16])
percent tensor([0.5846, 0.5741, 0.6194, 0.6417, 0.6496, 0.6390, 0.6002, 0.6211, 0.5958,
        0.5772, 0.5736, 0.6099, 0.5537, 0.6110, 0.5887, 0.6062],
       device='cuda:0') torch.Size([16])
percent tensor([0.5280, 0.5224, 0.6032, 0.5905, 0.6044, 0.6799, 0.5109, 0.4801, 0.5701,
        0.5386, 0.5505, 0.5556, 0.5077, 0.5883, 0.4259, 0.5466],
       device='cuda:0') torch.Size([16])
percent tensor([0.9992, 0.9974, 0.9973, 0.9976, 0.9981, 0.9953, 0.9975, 0.9993, 0.9975,
        0.9986, 0.9991, 0.9979, 0.9986, 0.9983, 0.9983, 0.9989],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 46 | Batch_idx: 0 |  Loss: (0.4512) |  Loss2: (0.0000) | Acc: (86.00%) (111/128)
Epoch: 46 | Batch_idx: 10 |  Loss: (0.4137) |  Loss2: (0.0000) | Acc: (85.00%) (1209/1408)
Epoch: 46 | Batch_idx: 20 |  Loss: (0.4249) |  Loss2: (0.0000) | Acc: (85.00%) (2299/2688)
Epoch: 46 | Batch_idx: 30 |  Loss: (0.4283) |  Loss2: (0.0000) | Acc: (85.00%) (3400/3968)
Epoch: 46 | Batch_idx: 40 |  Loss: (0.4326) |  Loss2: (0.0000) | Acc: (85.00%) (4485/5248)
Epoch: 46 | Batch_idx: 50 |  Loss: (0.4267) |  Loss2: (0.0000) | Acc: (85.00%) (5608/6528)
Epoch: 46 | Batch_idx: 60 |  Loss: (0.4254) |  Loss2: (0.0000) | Acc: (85.00%) (6698/7808)
Epoch: 46 | Batch_idx: 70 |  Loss: (0.4192) |  Loss2: (0.0000) | Acc: (85.00%) (7810/9088)
Epoch: 46 | Batch_idx: 80 |  Loss: (0.4180) |  Loss2: (0.0000) | Acc: (85.00%) (8911/10368)
Epoch: 46 | Batch_idx: 90 |  Loss: (0.4164) |  Loss2: (0.0000) | Acc: (85.00%) (10010/11648)
Epoch: 46 | Batch_idx: 100 |  Loss: (0.4175) |  Loss2: (0.0000) | Acc: (85.00%) (11103/12928)
Epoch: 46 | Batch_idx: 110 |  Loss: (0.4155) |  Loss2: (0.0000) | Acc: (85.00%) (12209/14208)
Epoch: 46 | Batch_idx: 120 |  Loss: (0.4148) |  Loss2: (0.0000) | Acc: (85.00%) (13316/15488)
Epoch: 46 | Batch_idx: 130 |  Loss: (0.4160) |  Loss2: (0.0000) | Acc: (85.00%) (14408/16768)
Epoch: 46 | Batch_idx: 140 |  Loss: (0.4141) |  Loss2: (0.0000) | Acc: (85.00%) (15513/18048)
Epoch: 46 | Batch_idx: 150 |  Loss: (0.4120) |  Loss2: (0.0000) | Acc: (85.00%) (16618/19328)
Epoch: 46 | Batch_idx: 160 |  Loss: (0.4134) |  Loss2: (0.0000) | Acc: (85.00%) (17704/20608)
Epoch: 46 | Batch_idx: 170 |  Loss: (0.4134) |  Loss2: (0.0000) | Acc: (85.00%) (18814/21888)
Epoch: 46 | Batch_idx: 180 |  Loss: (0.4147) |  Loss2: (0.0000) | Acc: (85.00%) (19892/23168)
Epoch: 46 | Batch_idx: 190 |  Loss: (0.4148) |  Loss2: (0.0000) | Acc: (85.00%) (20994/24448)
Epoch: 46 | Batch_idx: 200 |  Loss: (0.4159) |  Loss2: (0.0000) | Acc: (85.00%) (22088/25728)
Epoch: 46 | Batch_idx: 210 |  Loss: (0.4167) |  Loss2: (0.0000) | Acc: (85.00%) (23174/27008)
Epoch: 46 | Batch_idx: 220 |  Loss: (0.4170) |  Loss2: (0.0000) | Acc: (85.00%) (24253/28288)
Epoch: 46 | Batch_idx: 230 |  Loss: (0.4168) |  Loss2: (0.0000) | Acc: (85.00%) (25361/29568)
Epoch: 46 | Batch_idx: 240 |  Loss: (0.4154) |  Loss2: (0.0000) | Acc: (85.00%) (26458/30848)
Epoch: 46 | Batch_idx: 250 |  Loss: (0.4161) |  Loss2: (0.0000) | Acc: (85.00%) (27536/32128)
Epoch: 46 | Batch_idx: 260 |  Loss: (0.4159) |  Loss2: (0.0000) | Acc: (85.00%) (28635/33408)
Epoch: 46 | Batch_idx: 270 |  Loss: (0.4160) |  Loss2: (0.0000) | Acc: (85.00%) (29730/34688)
Epoch: 46 | Batch_idx: 280 |  Loss: (0.4152) |  Loss2: (0.0000) | Acc: (85.00%) (30842/35968)
Epoch: 46 | Batch_idx: 290 |  Loss: (0.4159) |  Loss2: (0.0000) | Acc: (85.00%) (31925/37248)
Epoch: 46 | Batch_idx: 300 |  Loss: (0.4162) |  Loss2: (0.0000) | Acc: (85.00%) (33034/38528)
Epoch: 46 | Batch_idx: 310 |  Loss: (0.4160) |  Loss2: (0.0000) | Acc: (85.00%) (34133/39808)
Epoch: 46 | Batch_idx: 320 |  Loss: (0.4161) |  Loss2: (0.0000) | Acc: (85.00%) (35234/41088)
Epoch: 46 | Batch_idx: 330 |  Loss: (0.4145) |  Loss2: (0.0000) | Acc: (85.00%) (36359/42368)
Epoch: 46 | Batch_idx: 340 |  Loss: (0.4151) |  Loss2: (0.0000) | Acc: (85.00%) (37445/43648)
Epoch: 46 | Batch_idx: 350 |  Loss: (0.4149) |  Loss2: (0.0000) | Acc: (85.00%) (38542/44928)
Epoch: 46 | Batch_idx: 360 |  Loss: (0.4170) |  Loss2: (0.0000) | Acc: (85.00%) (39611/46208)
Epoch: 46 | Batch_idx: 370 |  Loss: (0.4168) |  Loss2: (0.0000) | Acc: (85.00%) (40714/47488)
Epoch: 46 | Batch_idx: 380 |  Loss: (0.4169) |  Loss2: (0.0000) | Acc: (85.00%) (41800/48768)
Epoch: 46 | Batch_idx: 390 |  Loss: (0.4178) |  Loss2: (0.0000) | Acc: (85.00%) (42833/50000)
# TEST : Loss: (0.5257) | Acc: (82.00%) (8231/10000)
percent tensor([0.5420, 0.5618, 0.5283, 0.5287, 0.5369, 0.5317, 0.5599, 0.5441, 0.5549,
        0.5541, 0.5558, 0.5400, 0.5482, 0.5605, 0.5489, 0.5426],
       device='cuda:0') torch.Size([16])
percent tensor([0.4693, 0.4685, 0.4445, 0.4651, 0.4539, 0.4634, 0.4642, 0.4681, 0.4645,
        0.4586, 0.4611, 0.4444, 0.4707, 0.4770, 0.4668, 0.4667],
       device='cuda:0') torch.Size([16])
percent tensor([0.4819, 0.4659, 0.4128, 0.4752, 0.4139, 0.4769, 0.4590, 0.4598, 0.4786,
        0.4445, 0.4740, 0.4228, 0.4728, 0.5140, 0.4679, 0.4753],
       device='cuda:0') torch.Size([16])
percent tensor([0.5453, 0.5438, 0.5499, 0.5446, 0.5506, 0.5537, 0.5505, 0.5457, 0.5433,
        0.5462, 0.5396, 0.5549, 0.5467, 0.5373, 0.5517, 0.5476],
       device='cuda:0') torch.Size([16])
percent tensor([0.5582, 0.5419, 0.5630, 0.5603, 0.5793, 0.5818, 0.5713, 0.5667, 0.5879,
        0.5592, 0.5760, 0.5831, 0.5327, 0.5941, 0.5592, 0.5597],
       device='cuda:0') torch.Size([16])
percent tensor([0.5860, 0.5819, 0.6141, 0.6380, 0.6456, 0.6543, 0.6048, 0.6195, 0.6012,
        0.5757, 0.5796, 0.6022, 0.5571, 0.6234, 0.5923, 0.6103],
       device='cuda:0') torch.Size([16])
percent tensor([0.5341, 0.5312, 0.5742, 0.6013, 0.5845, 0.7020, 0.5205, 0.4569, 0.5906,
        0.5296, 0.5624, 0.5217, 0.5169, 0.6266, 0.4265, 0.5548],
       device='cuda:0') torch.Size([16])
percent tensor([0.9988, 0.9977, 0.9970, 0.9972, 0.9981, 0.9955, 0.9983, 0.9992, 0.9986,
        0.9991, 0.9990, 0.9980, 0.9987, 0.9979, 0.9983, 0.9986],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 47 | Batch_idx: 0 |  Loss: (0.3197) |  Loss2: (0.0000) | Acc: (87.00%) (112/128)
Epoch: 47 | Batch_idx: 10 |  Loss: (0.3979) |  Loss2: (0.0000) | Acc: (85.00%) (1206/1408)
Epoch: 47 | Batch_idx: 20 |  Loss: (0.4502) |  Loss2: (0.0000) | Acc: (84.00%) (2264/2688)
Epoch: 47 | Batch_idx: 30 |  Loss: (0.4581) |  Loss2: (0.0000) | Acc: (83.00%) (3326/3968)
Epoch: 47 | Batch_idx: 40 |  Loss: (0.4691) |  Loss2: (0.0000) | Acc: (83.00%) (4376/5248)
Epoch: 47 | Batch_idx: 50 |  Loss: (0.4750) |  Loss2: (0.0000) | Acc: (83.00%) (5448/6528)
Epoch: 47 | Batch_idx: 60 |  Loss: (0.4753) |  Loss2: (0.0000) | Acc: (83.00%) (6525/7808)
Epoch: 47 | Batch_idx: 70 |  Loss: (0.4768) |  Loss2: (0.0000) | Acc: (83.00%) (7599/9088)
Epoch: 47 | Batch_idx: 80 |  Loss: (0.4761) |  Loss2: (0.0000) | Acc: (83.00%) (8663/10368)
Epoch: 47 | Batch_idx: 90 |  Loss: (0.4813) |  Loss2: (0.0000) | Acc: (83.00%) (9709/11648)
Epoch: 47 | Batch_idx: 100 |  Loss: (0.4843) |  Loss2: (0.0000) | Acc: (83.00%) (10744/12928)
Epoch: 47 | Batch_idx: 110 |  Loss: (0.4856) |  Loss2: (0.0000) | Acc: (83.00%) (11805/14208)
Epoch: 47 | Batch_idx: 120 |  Loss: (0.4879) |  Loss2: (0.0000) | Acc: (82.00%) (12851/15488)
Epoch: 47 | Batch_idx: 130 |  Loss: (0.4881) |  Loss2: (0.0000) | Acc: (83.00%) (13918/16768)
Epoch: 47 | Batch_idx: 140 |  Loss: (0.4891) |  Loss2: (0.0000) | Acc: (83.00%) (14984/18048)
Epoch: 47 | Batch_idx: 150 |  Loss: (0.4893) |  Loss2: (0.0000) | Acc: (83.00%) (16044/19328)
Epoch: 47 | Batch_idx: 160 |  Loss: (0.4881) |  Loss2: (0.0000) | Acc: (83.00%) (17109/20608)
Epoch: 47 | Batch_idx: 170 |  Loss: (0.4862) |  Loss2: (0.0000) | Acc: (83.00%) (18187/21888)
Epoch: 47 | Batch_idx: 180 |  Loss: (0.4849) |  Loss2: (0.0000) | Acc: (83.00%) (19257/23168)
Epoch: 47 | Batch_idx: 190 |  Loss: (0.4856) |  Loss2: (0.0000) | Acc: (83.00%) (20314/24448)
Epoch: 47 | Batch_idx: 200 |  Loss: (0.4860) |  Loss2: (0.0000) | Acc: (83.00%) (21377/25728)
Epoch: 47 | Batch_idx: 210 |  Loss: (0.4851) |  Loss2: (0.0000) | Acc: (83.00%) (22451/27008)
Epoch: 47 | Batch_idx: 220 |  Loss: (0.4841) |  Loss2: (0.0000) | Acc: (83.00%) (23530/28288)
Epoch: 47 | Batch_idx: 230 |  Loss: (0.4822) |  Loss2: (0.0000) | Acc: (83.00%) (24616/29568)
Epoch: 47 | Batch_idx: 240 |  Loss: (0.4805) |  Loss2: (0.0000) | Acc: (83.00%) (25695/30848)
Epoch: 47 | Batch_idx: 250 |  Loss: (0.4794) |  Loss2: (0.0000) | Acc: (83.00%) (26778/32128)
Epoch: 47 | Batch_idx: 260 |  Loss: (0.4785) |  Loss2: (0.0000) | Acc: (83.00%) (27844/33408)
Epoch: 47 | Batch_idx: 270 |  Loss: (0.4768) |  Loss2: (0.0000) | Acc: (83.00%) (28924/34688)
Epoch: 47 | Batch_idx: 280 |  Loss: (0.4753) |  Loss2: (0.0000) | Acc: (83.00%) (30026/35968)
Epoch: 47 | Batch_idx: 290 |  Loss: (0.4735) |  Loss2: (0.0000) | Acc: (83.00%) (31117/37248)
Epoch: 47 | Batch_idx: 300 |  Loss: (0.4716) |  Loss2: (0.0000) | Acc: (83.00%) (32211/38528)
Epoch: 47 | Batch_idx: 310 |  Loss: (0.4722) |  Loss2: (0.0000) | Acc: (83.00%) (33290/39808)
Epoch: 47 | Batch_idx: 320 |  Loss: (0.4724) |  Loss2: (0.0000) | Acc: (83.00%) (34360/41088)
Epoch: 47 | Batch_idx: 330 |  Loss: (0.4727) |  Loss2: (0.0000) | Acc: (83.00%) (35418/42368)
Epoch: 47 | Batch_idx: 340 |  Loss: (0.4728) |  Loss2: (0.0000) | Acc: (83.00%) (36496/43648)
Epoch: 47 | Batch_idx: 350 |  Loss: (0.4717) |  Loss2: (0.0000) | Acc: (83.00%) (37581/44928)
Epoch: 47 | Batch_idx: 360 |  Loss: (0.4715) |  Loss2: (0.0000) | Acc: (83.00%) (38650/46208)
Epoch: 47 | Batch_idx: 370 |  Loss: (0.4712) |  Loss2: (0.0000) | Acc: (83.00%) (39736/47488)
Epoch: 47 | Batch_idx: 380 |  Loss: (0.4699) |  Loss2: (0.0000) | Acc: (83.00%) (40823/48768)
Epoch: 47 | Batch_idx: 390 |  Loss: (0.4693) |  Loss2: (0.0000) | Acc: (83.00%) (41853/50000)
# TEST : Loss: (0.5270) | Acc: (82.00%) (8209/10000)
percent tensor([0.5355, 0.5533, 0.5162, 0.5163, 0.5244, 0.5221, 0.5523, 0.5307, 0.5491,
        0.5464, 0.5497, 0.5275, 0.5423, 0.5530, 0.5369, 0.5332],
       device='cuda:0') torch.Size([16])
percent tensor([0.4743, 0.4693, 0.4479, 0.4683, 0.4570, 0.4638, 0.4655, 0.4719, 0.4687,
        0.4594, 0.4631, 0.4461, 0.4754, 0.4785, 0.4673, 0.4687],
       device='cuda:0') torch.Size([16])
percent tensor([0.4821, 0.4450, 0.3880, 0.4681, 0.3832, 0.4935, 0.4304, 0.4376, 0.4614,
        0.4179, 0.4637, 0.3891, 0.4620, 0.5072, 0.4604, 0.4734],
       device='cuda:0') torch.Size([16])
percent tensor([0.5525, 0.5524, 0.5520, 0.5459, 0.5511, 0.5567, 0.5576, 0.5478, 0.5497,
        0.5550, 0.5497, 0.5603, 0.5575, 0.5456, 0.5580, 0.5538],
       device='cuda:0') torch.Size([16])
percent tensor([0.5811, 0.5627, 0.5666, 0.5647, 0.5733, 0.5792, 0.5862, 0.5687, 0.6075,
        0.5849, 0.6064, 0.5994, 0.5709, 0.6139, 0.5711, 0.5738],
       device='cuda:0') torch.Size([16])
percent tensor([0.6416, 0.6357, 0.6718, 0.6906, 0.7012, 0.7070, 0.6624, 0.6871, 0.6533,
        0.6340, 0.6345, 0.6527, 0.6103, 0.6786, 0.6537, 0.6691],
       device='cuda:0') torch.Size([16])
percent tensor([0.4923, 0.4882, 0.5847, 0.6000, 0.5932, 0.6896, 0.5114, 0.4995, 0.5579,
        0.4898, 0.5154, 0.5045, 0.4666, 0.5988, 0.4268, 0.5435],
       device='cuda:0') torch.Size([16])
percent tensor([0.9990, 0.9975, 0.9975, 0.9981, 0.9986, 0.9961, 0.9982, 0.9992, 0.9981,
        0.9992, 0.9991, 0.9988, 0.9987, 0.9984, 0.9982, 0.9990],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 48 | Batch_idx: 0 |  Loss: (0.7111) |  Loss2: (0.0000) | Acc: (78.00%) (100/128)
Epoch: 48 | Batch_idx: 10 |  Loss: (0.4422) |  Loss2: (0.0000) | Acc: (84.00%) (1194/1408)
Epoch: 48 | Batch_idx: 20 |  Loss: (0.4221) |  Loss2: (0.0000) | Acc: (84.00%) (2284/2688)
Epoch: 48 | Batch_idx: 30 |  Loss: (0.4031) |  Loss2: (0.0000) | Acc: (85.00%) (3401/3968)
Epoch: 48 | Batch_idx: 40 |  Loss: (0.3986) |  Loss2: (0.0000) | Acc: (86.00%) (4523/5248)
Epoch: 48 | Batch_idx: 50 |  Loss: (0.4096) |  Loss2: (0.0000) | Acc: (85.00%) (5600/6528)
Epoch: 48 | Batch_idx: 60 |  Loss: (0.4085) |  Loss2: (0.0000) | Acc: (85.00%) (6713/7808)
Epoch: 48 | Batch_idx: 70 |  Loss: (0.4055) |  Loss2: (0.0000) | Acc: (85.00%) (7814/9088)
Epoch: 48 | Batch_idx: 80 |  Loss: (0.4051) |  Loss2: (0.0000) | Acc: (86.00%) (8922/10368)
Epoch: 48 | Batch_idx: 90 |  Loss: (0.4036) |  Loss2: (0.0000) | Acc: (86.00%) (10034/11648)
Epoch: 48 | Batch_idx: 100 |  Loss: (0.3997) |  Loss2: (0.0000) | Acc: (86.00%) (11158/12928)
Epoch: 48 | Batch_idx: 110 |  Loss: (0.4017) |  Loss2: (0.0000) | Acc: (86.00%) (12251/14208)
Epoch: 48 | Batch_idx: 120 |  Loss: (0.4049) |  Loss2: (0.0000) | Acc: (86.00%) (13335/15488)
Epoch: 48 | Batch_idx: 130 |  Loss: (0.4036) |  Loss2: (0.0000) | Acc: (86.00%) (14444/16768)
Epoch: 48 | Batch_idx: 140 |  Loss: (0.4043) |  Loss2: (0.0000) | Acc: (86.00%) (15534/18048)
Epoch: 48 | Batch_idx: 150 |  Loss: (0.4026) |  Loss2: (0.0000) | Acc: (86.00%) (16651/19328)
Epoch: 48 | Batch_idx: 160 |  Loss: (0.4018) |  Loss2: (0.0000) | Acc: (86.00%) (17748/20608)
Epoch: 48 | Batch_idx: 170 |  Loss: (0.4046) |  Loss2: (0.0000) | Acc: (86.00%) (18836/21888)
Epoch: 48 | Batch_idx: 180 |  Loss: (0.4055) |  Loss2: (0.0000) | Acc: (85.00%) (19921/23168)
Epoch: 48 | Batch_idx: 190 |  Loss: (0.4043) |  Loss2: (0.0000) | Acc: (86.00%) (21034/24448)
Epoch: 48 | Batch_idx: 200 |  Loss: (0.4041) |  Loss2: (0.0000) | Acc: (86.00%) (22146/25728)
Epoch: 48 | Batch_idx: 210 |  Loss: (0.4052) |  Loss2: (0.0000) | Acc: (86.00%) (23247/27008)
Epoch: 48 | Batch_idx: 220 |  Loss: (0.4049) |  Loss2: (0.0000) | Acc: (86.00%) (24352/28288)
Epoch: 48 | Batch_idx: 230 |  Loss: (0.4038) |  Loss2: (0.0000) | Acc: (86.00%) (25458/29568)
Epoch: 48 | Batch_idx: 240 |  Loss: (0.4041) |  Loss2: (0.0000) | Acc: (86.00%) (26558/30848)
Epoch: 48 | Batch_idx: 250 |  Loss: (0.4044) |  Loss2: (0.0000) | Acc: (86.00%) (27652/32128)
Epoch: 48 | Batch_idx: 260 |  Loss: (0.4059) |  Loss2: (0.0000) | Acc: (86.00%) (28732/33408)
Epoch: 48 | Batch_idx: 270 |  Loss: (0.4054) |  Loss2: (0.0000) | Acc: (86.00%) (29849/34688)
Epoch: 48 | Batch_idx: 280 |  Loss: (0.4054) |  Loss2: (0.0000) | Acc: (86.00%) (30941/35968)
Epoch: 48 | Batch_idx: 290 |  Loss: (0.4045) |  Loss2: (0.0000) | Acc: (86.00%) (32063/37248)
Epoch: 48 | Batch_idx: 300 |  Loss: (0.4039) |  Loss2: (0.0000) | Acc: (86.00%) (33178/38528)
Epoch: 48 | Batch_idx: 310 |  Loss: (0.4045) |  Loss2: (0.0000) | Acc: (86.00%) (34284/39808)
Epoch: 48 | Batch_idx: 320 |  Loss: (0.4050) |  Loss2: (0.0000) | Acc: (86.00%) (35365/41088)
Epoch: 48 | Batch_idx: 330 |  Loss: (0.4048) |  Loss2: (0.0000) | Acc: (86.00%) (36476/42368)
Epoch: 48 | Batch_idx: 340 |  Loss: (0.4038) |  Loss2: (0.0000) | Acc: (86.00%) (37582/43648)
Epoch: 48 | Batch_idx: 350 |  Loss: (0.4045) |  Loss2: (0.0000) | Acc: (86.00%) (38667/44928)
Epoch: 48 | Batch_idx: 360 |  Loss: (0.4060) |  Loss2: (0.0000) | Acc: (85.00%) (39731/46208)
Epoch: 48 | Batch_idx: 370 |  Loss: (0.4065) |  Loss2: (0.0000) | Acc: (85.00%) (40823/47488)
Epoch: 48 | Batch_idx: 380 |  Loss: (0.4055) |  Loss2: (0.0000) | Acc: (85.00%) (41929/48768)
Epoch: 48 | Batch_idx: 390 |  Loss: (0.4055) |  Loss2: (0.0000) | Acc: (85.00%) (42993/50000)
# TEST : Loss: (0.4787) | Acc: (83.00%) (8368/10000)
percent tensor([0.5329, 0.5519, 0.5125, 0.5151, 0.5202, 0.5197, 0.5487, 0.5284, 0.5461,
        0.5443, 0.5478, 0.5244, 0.5405, 0.5515, 0.5349, 0.5313],
       device='cuda:0') torch.Size([16])
percent tensor([0.4704, 0.4680, 0.4484, 0.4647, 0.4575, 0.4618, 0.4659, 0.4707, 0.4688,
        0.4603, 0.4616, 0.4477, 0.4730, 0.4780, 0.4656, 0.4673],
       device='cuda:0') torch.Size([16])
percent tensor([0.4772, 0.4419, 0.4056, 0.4659, 0.3949, 0.4919, 0.4349, 0.4425, 0.4663,
        0.4241, 0.4626, 0.4107, 0.4542, 0.5112, 0.4550, 0.4706],
       device='cuda:0') torch.Size([16])
percent tensor([0.5533, 0.5532, 0.5527, 0.5463, 0.5520, 0.5556, 0.5559, 0.5490, 0.5507,
        0.5539, 0.5489, 0.5583, 0.5574, 0.5450, 0.5588, 0.5547],
       device='cuda:0') torch.Size([16])
percent tensor([0.5761, 0.5661, 0.5615, 0.5632, 0.5677, 0.5811, 0.5801, 0.5710, 0.6047,
        0.5753, 0.6006, 0.5812, 0.5645, 0.6184, 0.5729, 0.5707],
       device='cuda:0') torch.Size([16])
percent tensor([0.6445, 0.6382, 0.6726, 0.6921, 0.7056, 0.6941, 0.6677, 0.6880, 0.6534,
        0.6323, 0.6339, 0.6603, 0.6139, 0.6689, 0.6548, 0.6687],
       device='cuda:0') torch.Size([16])
percent tensor([0.5037, 0.4924, 0.5915, 0.5889, 0.6244, 0.6735, 0.5232, 0.5011, 0.5551,
        0.4969, 0.5075, 0.5258, 0.4703, 0.5775, 0.4090, 0.5464],
       device='cuda:0') torch.Size([16])
percent tensor([0.9990, 0.9971, 0.9967, 0.9973, 0.9983, 0.9955, 0.9984, 0.9992, 0.9977,
        0.9986, 0.9988, 0.9978, 0.9986, 0.9981, 0.9987, 0.9989],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 49 | Batch_idx: 0 |  Loss: (0.3096) |  Loss2: (0.0000) | Acc: (86.00%) (111/128)
Epoch: 49 | Batch_idx: 10 |  Loss: (0.3979) |  Loss2: (0.0000) | Acc: (85.00%) (1204/1408)
Epoch: 49 | Batch_idx: 20 |  Loss: (0.4040) |  Loss2: (0.0000) | Acc: (85.00%) (2300/2688)
Epoch: 49 | Batch_idx: 30 |  Loss: (0.4337) |  Loss2: (0.0000) | Acc: (84.00%) (3365/3968)
Epoch: 49 | Batch_idx: 40 |  Loss: (0.4390) |  Loss2: (0.0000) | Acc: (84.00%) (4440/5248)
Epoch: 49 | Batch_idx: 50 |  Loss: (0.4440) |  Loss2: (0.0000) | Acc: (84.00%) (5513/6528)
Epoch: 49 | Batch_idx: 60 |  Loss: (0.4342) |  Loss2: (0.0000) | Acc: (84.00%) (6620/7808)
Epoch: 49 | Batch_idx: 70 |  Loss: (0.4349) |  Loss2: (0.0000) | Acc: (84.00%) (7710/9088)
Epoch: 49 | Batch_idx: 80 |  Loss: (0.4393) |  Loss2: (0.0000) | Acc: (84.00%) (8785/10368)
Epoch: 49 | Batch_idx: 90 |  Loss: (0.4358) |  Loss2: (0.0000) | Acc: (84.00%) (9882/11648)
Epoch: 49 | Batch_idx: 100 |  Loss: (0.4379) |  Loss2: (0.0000) | Acc: (84.00%) (10957/12928)
Epoch: 49 | Batch_idx: 110 |  Loss: (0.4408) |  Loss2: (0.0000) | Acc: (84.00%) (12022/14208)
Epoch: 49 | Batch_idx: 120 |  Loss: (0.4394) |  Loss2: (0.0000) | Acc: (84.00%) (13117/15488)
Epoch: 49 | Batch_idx: 130 |  Loss: (0.4393) |  Loss2: (0.0000) | Acc: (84.00%) (14202/16768)
Epoch: 49 | Batch_idx: 140 |  Loss: (0.4409) |  Loss2: (0.0000) | Acc: (84.00%) (15270/18048)
Epoch: 49 | Batch_idx: 150 |  Loss: (0.4437) |  Loss2: (0.0000) | Acc: (84.00%) (16348/19328)
Epoch: 49 | Batch_idx: 160 |  Loss: (0.4427) |  Loss2: (0.0000) | Acc: (84.00%) (17440/20608)
Epoch: 49 | Batch_idx: 170 |  Loss: (0.4398) |  Loss2: (0.0000) | Acc: (84.00%) (18556/21888)
Epoch: 49 | Batch_idx: 180 |  Loss: (0.4406) |  Loss2: (0.0000) | Acc: (84.00%) (19632/23168)
Epoch: 49 | Batch_idx: 190 |  Loss: (0.4401) |  Loss2: (0.0000) | Acc: (84.00%) (20720/24448)
Epoch: 49 | Batch_idx: 200 |  Loss: (0.4409) |  Loss2: (0.0000) | Acc: (84.00%) (21800/25728)
Epoch: 49 | Batch_idx: 210 |  Loss: (0.4395) |  Loss2: (0.0000) | Acc: (84.00%) (22898/27008)
Epoch: 49 | Batch_idx: 220 |  Loss: (0.4394) |  Loss2: (0.0000) | Acc: (84.00%) (23984/28288)
Epoch: 49 | Batch_idx: 230 |  Loss: (0.4404) |  Loss2: (0.0000) | Acc: (84.00%) (25050/29568)
Epoch: 49 | Batch_idx: 240 |  Loss: (0.4391) |  Loss2: (0.0000) | Acc: (84.00%) (26153/30848)
Epoch: 49 | Batch_idx: 250 |  Loss: (0.4395) |  Loss2: (0.0000) | Acc: (84.00%) (27235/32128)
Epoch: 49 | Batch_idx: 260 |  Loss: (0.4371) |  Loss2: (0.0000) | Acc: (84.00%) (28345/33408)
Epoch: 49 | Batch_idx: 270 |  Loss: (0.4382) |  Loss2: (0.0000) | Acc: (84.00%) (29421/34688)
Epoch: 49 | Batch_idx: 280 |  Loss: (0.4376) |  Loss2: (0.0000) | Acc: (84.00%) (30520/35968)
Epoch: 49 | Batch_idx: 290 |  Loss: (0.4378) |  Loss2: (0.0000) | Acc: (84.00%) (31587/37248)
Epoch: 49 | Batch_idx: 300 |  Loss: (0.4368) |  Loss2: (0.0000) | Acc: (84.00%) (32683/38528)
Epoch: 49 | Batch_idx: 310 |  Loss: (0.4356) |  Loss2: (0.0000) | Acc: (84.00%) (33783/39808)
Epoch: 49 | Batch_idx: 320 |  Loss: (0.4347) |  Loss2: (0.0000) | Acc: (84.00%) (34881/41088)
Epoch: 49 | Batch_idx: 330 |  Loss: (0.4328) |  Loss2: (0.0000) | Acc: (84.00%) (35992/42368)
Epoch: 49 | Batch_idx: 340 |  Loss: (0.4331) |  Loss2: (0.0000) | Acc: (84.00%) (37069/43648)
Epoch: 49 | Batch_idx: 350 |  Loss: (0.4318) |  Loss2: (0.0000) | Acc: (84.00%) (38172/44928)
Epoch: 49 | Batch_idx: 360 |  Loss: (0.4313) |  Loss2: (0.0000) | Acc: (84.00%) (39272/46208)
Epoch: 49 | Batch_idx: 370 |  Loss: (0.4297) |  Loss2: (0.0000) | Acc: (85.00%) (40379/47488)
Epoch: 49 | Batch_idx: 380 |  Loss: (0.4296) |  Loss2: (0.0000) | Acc: (85.00%) (41462/48768)
Epoch: 49 | Batch_idx: 390 |  Loss: (0.4290) |  Loss2: (0.0000) | Acc: (85.00%) (42520/50000)
# TEST : Loss: (0.4949) | Acc: (83.00%) (8332/10000)
percent tensor([0.5285, 0.5439, 0.5132, 0.5137, 0.5190, 0.5159, 0.5424, 0.5258, 0.5397,
        0.5384, 0.5407, 0.5231, 0.5347, 0.5436, 0.5301, 0.5275],
       device='cuda:0') torch.Size([16])
percent tensor([0.4628, 0.4634, 0.4401, 0.4567, 0.4480, 0.4506, 0.4601, 0.4646, 0.4597,
        0.4550, 0.4541, 0.4399, 0.4686, 0.4719, 0.4586, 0.4600],
       device='cuda:0') torch.Size([16])
percent tensor([0.4695, 0.4447, 0.4009, 0.4543, 0.3985, 0.4820, 0.4399, 0.4372, 0.4617,
        0.4258, 0.4592, 0.4097, 0.4498, 0.5036, 0.4525, 0.4656],
       device='cuda:0') torch.Size([16])
percent tensor([0.5641, 0.5662, 0.5596, 0.5525, 0.5584, 0.5641, 0.5671, 0.5575, 0.5581,
        0.5667, 0.5599, 0.5676, 0.5721, 0.5541, 0.5707, 0.5646],
       device='cuda:0') torch.Size([16])
percent tensor([0.6171, 0.6209, 0.5816, 0.5756, 0.5834, 0.6080, 0.6223, 0.5996, 0.6289,
        0.6293, 0.6401, 0.6155, 0.6285, 0.6532, 0.6197, 0.6153],
       device='cuda:0') torch.Size([16])
percent tensor([0.6469, 0.6346, 0.6705, 0.6894, 0.7049, 0.6955, 0.6644, 0.6809, 0.6555,
        0.6310, 0.6315, 0.6557, 0.6148, 0.6708, 0.6456, 0.6696],
       device='cuda:0') torch.Size([16])
percent tensor([0.5154, 0.4940, 0.5970, 0.5816, 0.6250, 0.6985, 0.5225, 0.4847, 0.5741,
        0.5053, 0.5052, 0.5019, 0.4761, 0.5833, 0.3955, 0.5598],
       device='cuda:0') torch.Size([16])
percent tensor([0.9991, 0.9979, 0.9970, 0.9967, 0.9983, 0.9956, 0.9986, 0.9989, 0.9972,
        0.9991, 0.9990, 0.9980, 0.9987, 0.9985, 0.9989, 0.9989],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 50 | Batch_idx: 0 |  Loss: (0.5323) |  Loss2: (0.0000) | Acc: (80.00%) (103/128)
Epoch: 50 | Batch_idx: 10 |  Loss: (0.4084) |  Loss2: (0.0000) | Acc: (85.00%) (1199/1408)
Epoch: 50 | Batch_idx: 20 |  Loss: (0.4049) |  Loss2: (0.0000) | Acc: (86.00%) (2319/2688)
Epoch: 50 | Batch_idx: 30 |  Loss: (0.4179) |  Loss2: (0.0000) | Acc: (85.00%) (3404/3968)
Epoch: 50 | Batch_idx: 40 |  Loss: (0.4118) |  Loss2: (0.0000) | Acc: (85.00%) (4505/5248)
Epoch: 50 | Batch_idx: 50 |  Loss: (0.4012) |  Loss2: (0.0000) | Acc: (86.00%) (5636/6528)
Epoch: 50 | Batch_idx: 60 |  Loss: (0.4028) |  Loss2: (0.0000) | Acc: (86.00%) (6746/7808)
Epoch: 50 | Batch_idx: 70 |  Loss: (0.3993) |  Loss2: (0.0000) | Acc: (86.00%) (7867/9088)
Epoch: 50 | Batch_idx: 80 |  Loss: (0.3971) |  Loss2: (0.0000) | Acc: (86.00%) (8977/10368)
Epoch: 50 | Batch_idx: 90 |  Loss: (0.3994) |  Loss2: (0.0000) | Acc: (86.00%) (10071/11648)
Epoch: 50 | Batch_idx: 100 |  Loss: (0.4005) |  Loss2: (0.0000) | Acc: (86.00%) (11172/12928)
Epoch: 50 | Batch_idx: 110 |  Loss: (0.4032) |  Loss2: (0.0000) | Acc: (86.00%) (12262/14208)
Epoch: 50 | Batch_idx: 120 |  Loss: (0.4013) |  Loss2: (0.0000) | Acc: (86.00%) (13377/15488)
Epoch: 50 | Batch_idx: 130 |  Loss: (0.4039) |  Loss2: (0.0000) | Acc: (86.00%) (14444/16768)
Epoch: 50 | Batch_idx: 140 |  Loss: (0.4047) |  Loss2: (0.0000) | Acc: (86.00%) (15549/18048)
Epoch: 50 | Batch_idx: 150 |  Loss: (0.4006) |  Loss2: (0.0000) | Acc: (86.00%) (16683/19328)
Epoch: 50 | Batch_idx: 160 |  Loss: (0.4022) |  Loss2: (0.0000) | Acc: (86.00%) (17774/20608)
Epoch: 50 | Batch_idx: 170 |  Loss: (0.4010) |  Loss2: (0.0000) | Acc: (86.00%) (18901/21888)
Epoch: 50 | Batch_idx: 180 |  Loss: (0.3994) |  Loss2: (0.0000) | Acc: (86.00%) (20016/23168)
Epoch: 50 | Batch_idx: 190 |  Loss: (0.3996) |  Loss2: (0.0000) | Acc: (86.00%) (21105/24448)
Epoch: 50 | Batch_idx: 200 |  Loss: (0.3993) |  Loss2: (0.0000) | Acc: (86.00%) (22213/25728)
Epoch: 50 | Batch_idx: 210 |  Loss: (0.3977) |  Loss2: (0.0000) | Acc: (86.00%) (23327/27008)
Epoch: 50 | Batch_idx: 220 |  Loss: (0.3983) |  Loss2: (0.0000) | Acc: (86.00%) (24434/28288)
Epoch: 50 | Batch_idx: 230 |  Loss: (0.3980) |  Loss2: (0.0000) | Acc: (86.00%) (25550/29568)
Epoch: 50 | Batch_idx: 240 |  Loss: (0.3980) |  Loss2: (0.0000) | Acc: (86.00%) (26654/30848)
Epoch: 50 | Batch_idx: 250 |  Loss: (0.3970) |  Loss2: (0.0000) | Acc: (86.00%) (27764/32128)
Epoch: 50 | Batch_idx: 260 |  Loss: (0.3975) |  Loss2: (0.0000) | Acc: (86.00%) (28850/33408)
Epoch: 50 | Batch_idx: 270 |  Loss: (0.3966) |  Loss2: (0.0000) | Acc: (86.00%) (29961/34688)
Epoch: 50 | Batch_idx: 280 |  Loss: (0.3962) |  Loss2: (0.0000) | Acc: (86.00%) (31083/35968)
Epoch: 50 | Batch_idx: 290 |  Loss: (0.3952) |  Loss2: (0.0000) | Acc: (86.00%) (32190/37248)
Epoch: 50 | Batch_idx: 300 |  Loss: (0.3957) |  Loss2: (0.0000) | Acc: (86.00%) (33295/38528)
Epoch: 50 | Batch_idx: 310 |  Loss: (0.3959) |  Loss2: (0.0000) | Acc: (86.00%) (34403/39808)
Epoch: 50 | Batch_idx: 320 |  Loss: (0.3964) |  Loss2: (0.0000) | Acc: (86.00%) (35494/41088)
Epoch: 50 | Batch_idx: 330 |  Loss: (0.3966) |  Loss2: (0.0000) | Acc: (86.00%) (36591/42368)
Epoch: 50 | Batch_idx: 340 |  Loss: (0.3961) |  Loss2: (0.0000) | Acc: (86.00%) (37705/43648)
Epoch: 50 | Batch_idx: 350 |  Loss: (0.3960) |  Loss2: (0.0000) | Acc: (86.00%) (38811/44928)
Epoch: 50 | Batch_idx: 360 |  Loss: (0.3950) |  Loss2: (0.0000) | Acc: (86.00%) (39931/46208)
Epoch: 50 | Batch_idx: 370 |  Loss: (0.3949) |  Loss2: (0.0000) | Acc: (86.00%) (41039/47488)
Epoch: 50 | Batch_idx: 380 |  Loss: (0.3954) |  Loss2: (0.0000) | Acc: (86.00%) (42141/48768)
Epoch: 50 | Batch_idx: 390 |  Loss: (0.3948) |  Loss2: (0.0000) | Acc: (86.00%) (43207/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_050.pth.tar'
# TEST : Loss: (0.4767) | Acc: (84.00%) (8408/10000)
percent tensor([0.5277, 0.5436, 0.5098, 0.5118, 0.5161, 0.5158, 0.5421, 0.5250, 0.5394,
        0.5382, 0.5406, 0.5210, 0.5343, 0.5439, 0.5300, 0.5271],
       device='cuda:0') torch.Size([16])
percent tensor([0.4617, 0.4645, 0.4336, 0.4558, 0.4440, 0.4500, 0.4590, 0.4628, 0.4591,
        0.4525, 0.4539, 0.4351, 0.4676, 0.4738, 0.4595, 0.4588],
       device='cuda:0') torch.Size([16])
percent tensor([0.4648, 0.4476, 0.3930, 0.4594, 0.3952, 0.4833, 0.4379, 0.4407, 0.4592,
        0.4227, 0.4579, 0.4012, 0.4475, 0.5033, 0.4591, 0.4644],
       device='cuda:0') torch.Size([16])
percent tensor([0.5649, 0.5676, 0.5603, 0.5539, 0.5591, 0.5644, 0.5701, 0.5582, 0.5596,
        0.5675, 0.5610, 0.5691, 0.5743, 0.5551, 0.5727, 0.5648],
       device='cuda:0') torch.Size([16])
percent tensor([0.6162, 0.6224, 0.5831, 0.5737, 0.5926, 0.6081, 0.6313, 0.6001, 0.6299,
        0.6300, 0.6382, 0.6218, 0.6267, 0.6488, 0.6210, 0.6116],
       device='cuda:0') torch.Size([16])
percent tensor([0.6435, 0.6327, 0.6727, 0.6920, 0.7070, 0.6974, 0.6657, 0.6811, 0.6456,
        0.6342, 0.6344, 0.6581, 0.6119, 0.6693, 0.6485, 0.6698],
       device='cuda:0') torch.Size([16])
percent tensor([0.5130, 0.5091, 0.6111, 0.6120, 0.6312, 0.6966, 0.5332, 0.4953, 0.5575,
        0.5251, 0.5344, 0.5198, 0.4829, 0.5976, 0.4137, 0.5646],
       device='cuda:0') torch.Size([16])
percent tensor([0.9994, 0.9979, 0.9973, 0.9970, 0.9982, 0.9973, 0.9986, 0.9989, 0.9990,
        0.9990, 0.9991, 0.9981, 0.9991, 0.9985, 0.9981, 0.9989],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(172.9718, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(798.9201, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(789.5144, device='cuda:0')
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1523.9142, device='cuda:0')
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(503.4080, device='cuda:0')
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2193.2888, device='cuda:0')
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4307.1230, device='cuda:0')
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1424.9554, device='cuda:0')
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6113.7529, device='cuda:0')
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12044.8096, device='cuda:0')
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4011.7949, device='cuda:0')
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16966.9336, device='cuda:0')
Epoch: 51 | Batch_idx: 0 |  Loss: (0.4564) |  Loss2: (0.0000) | Acc: (82.00%) (106/128)
Epoch: 51 | Batch_idx: 10 |  Loss: (0.4175) |  Loss2: (0.0000) | Acc: (85.00%) (1201/1408)
Epoch: 51 | Batch_idx: 20 |  Loss: (0.4135) |  Loss2: (0.0000) | Acc: (85.00%) (2301/2688)
Epoch: 51 | Batch_idx: 30 |  Loss: (0.4209) |  Loss2: (0.0000) | Acc: (85.00%) (3390/3968)
Epoch: 51 | Batch_idx: 40 |  Loss: (0.4170) |  Loss2: (0.0000) | Acc: (85.00%) (4503/5248)
Epoch: 51 | Batch_idx: 50 |  Loss: (0.4203) |  Loss2: (0.0000) | Acc: (85.00%) (5583/6528)
Epoch: 51 | Batch_idx: 60 |  Loss: (0.4208) |  Loss2: (0.0000) | Acc: (85.00%) (6681/7808)
Epoch: 51 | Batch_idx: 70 |  Loss: (0.4205) |  Loss2: (0.0000) | Acc: (85.00%) (7771/9088)
Epoch: 51 | Batch_idx: 80 |  Loss: (0.4219) |  Loss2: (0.0000) | Acc: (85.00%) (8868/10368)
Epoch: 51 | Batch_idx: 90 |  Loss: (0.4202) |  Loss2: (0.0000) | Acc: (85.00%) (9970/11648)
Epoch: 51 | Batch_idx: 100 |  Loss: (0.4224) |  Loss2: (0.0000) | Acc: (85.00%) (11050/12928)
Epoch: 51 | Batch_idx: 110 |  Loss: (0.4201) |  Loss2: (0.0000) | Acc: (85.00%) (12149/14208)
Epoch: 51 | Batch_idx: 120 |  Loss: (0.4209) |  Loss2: (0.0000) | Acc: (85.00%) (13237/15488)
Epoch: 51 | Batch_idx: 130 |  Loss: (0.4250) |  Loss2: (0.0000) | Acc: (85.00%) (14303/16768)
Epoch: 51 | Batch_idx: 140 |  Loss: (0.4243) |  Loss2: (0.0000) | Acc: (85.00%) (15408/18048)
Epoch: 51 | Batch_idx: 150 |  Loss: (0.4268) |  Loss2: (0.0000) | Acc: (85.00%) (16481/19328)
Epoch: 51 | Batch_idx: 160 |  Loss: (0.4229) |  Loss2: (0.0000) | Acc: (85.00%) (17590/20608)
Epoch: 51 | Batch_idx: 170 |  Loss: (0.4242) |  Loss2: (0.0000) | Acc: (85.00%) (18674/21888)
Epoch: 51 | Batch_idx: 180 |  Loss: (0.4245) |  Loss2: (0.0000) | Acc: (85.00%) (19757/23168)
Epoch: 51 | Batch_idx: 190 |  Loss: (0.4236) |  Loss2: (0.0000) | Acc: (85.00%) (20843/24448)
Epoch: 51 | Batch_idx: 200 |  Loss: (0.4242) |  Loss2: (0.0000) | Acc: (85.00%) (21952/25728)
Epoch: 51 | Batch_idx: 210 |  Loss: (0.4245) |  Loss2: (0.0000) | Acc: (85.00%) (23041/27008)
Epoch: 51 | Batch_idx: 220 |  Loss: (0.4221) |  Loss2: (0.0000) | Acc: (85.00%) (24161/28288)
Epoch: 51 | Batch_idx: 230 |  Loss: (0.4218) |  Loss2: (0.0000) | Acc: (85.00%) (25256/29568)
Epoch: 51 | Batch_idx: 240 |  Loss: (0.4187) |  Loss2: (0.0000) | Acc: (85.00%) (26399/30848)
Epoch: 51 | Batch_idx: 250 |  Loss: (0.4175) |  Loss2: (0.0000) | Acc: (85.00%) (27507/32128)
Epoch: 51 | Batch_idx: 260 |  Loss: (0.4186) |  Loss2: (0.0000) | Acc: (85.00%) (28586/33408)
Epoch: 51 | Batch_idx: 270 |  Loss: (0.4177) |  Loss2: (0.0000) | Acc: (85.00%) (29693/34688)
Epoch: 51 | Batch_idx: 280 |  Loss: (0.4189) |  Loss2: (0.0000) | Acc: (85.00%) (30776/35968)
Epoch: 51 | Batch_idx: 290 |  Loss: (0.4180) |  Loss2: (0.0000) | Acc: (85.00%) (31867/37248)
Epoch: 51 | Batch_idx: 300 |  Loss: (0.4188) |  Loss2: (0.0000) | Acc: (85.00%) (32944/38528)
Epoch: 51 | Batch_idx: 310 |  Loss: (0.4183) |  Loss2: (0.0000) | Acc: (85.00%) (34048/39808)
Epoch: 51 | Batch_idx: 320 |  Loss: (0.4170) |  Loss2: (0.0000) | Acc: (85.00%) (35167/41088)
Epoch: 51 | Batch_idx: 330 |  Loss: (0.4157) |  Loss2: (0.0000) | Acc: (85.00%) (36274/42368)
Epoch: 51 | Batch_idx: 340 |  Loss: (0.4157) |  Loss2: (0.0000) | Acc: (85.00%) (37375/43648)
Epoch: 51 | Batch_idx: 350 |  Loss: (0.4153) |  Loss2: (0.0000) | Acc: (85.00%) (38476/44928)
Epoch: 51 | Batch_idx: 360 |  Loss: (0.4150) |  Loss2: (0.0000) | Acc: (85.00%) (39581/46208)
Epoch: 51 | Batch_idx: 370 |  Loss: (0.4137) |  Loss2: (0.0000) | Acc: (85.00%) (40711/47488)
Epoch: 51 | Batch_idx: 380 |  Loss: (0.4139) |  Loss2: (0.0000) | Acc: (85.00%) (41815/48768)
Epoch: 51 | Batch_idx: 390 |  Loss: (0.4137) |  Loss2: (0.0000) | Acc: (85.00%) (42875/50000)
# TEST : Loss: (0.4779) | Acc: (83.00%) (8338/10000)
percent tensor([0.5259, 0.5435, 0.5047, 0.5094, 0.5120, 0.5129, 0.5409, 0.5239, 0.5391,
        0.5376, 0.5403, 0.5170, 0.5336, 0.5445, 0.5289, 0.5258],
       device='cuda:0') torch.Size([16])
percent tensor([0.4653, 0.4675, 0.4347, 0.4553, 0.4470, 0.4508, 0.4633, 0.4637, 0.4627,
        0.4544, 0.4560, 0.4378, 0.4715, 0.4760, 0.4599, 0.4606],
       device='cuda:0') torch.Size([16])
percent tensor([0.4500, 0.4341, 0.3802, 0.4505, 0.3861, 0.4744, 0.4236, 0.4275, 0.4468,
        0.4096, 0.4412, 0.3890, 0.4313, 0.5012, 0.4417, 0.4526],
       device='cuda:0') torch.Size([16])
percent tensor([0.5795, 0.5849, 0.5721, 0.5644, 0.5697, 0.5747, 0.5860, 0.5703, 0.5764,
        0.5849, 0.5798, 0.5851, 0.5944, 0.5712, 0.5877, 0.5783],
       device='cuda:0') torch.Size([16])
percent tensor([0.6597, 0.6641, 0.6300, 0.6206, 0.6315, 0.6575, 0.6735, 0.6381, 0.6818,
        0.6798, 0.6964, 0.6832, 0.6810, 0.7031, 0.6639, 0.6560],
       device='cuda:0') torch.Size([16])
percent tensor([0.6874, 0.6735, 0.7078, 0.7255, 0.7394, 0.7342, 0.7041, 0.7163, 0.6842,
        0.6775, 0.6788, 0.6953, 0.6602, 0.7131, 0.6859, 0.7084],
       device='cuda:0') torch.Size([16])
percent tensor([0.4756, 0.4738, 0.6045, 0.5953, 0.6218, 0.7043, 0.4945, 0.4566, 0.5359,
        0.4927, 0.5021, 0.4998, 0.4440, 0.5888, 0.3741, 0.5230],
       device='cuda:0') torch.Size([16])
percent tensor([0.9993, 0.9974, 0.9974, 0.9975, 0.9988, 0.9969, 0.9985, 0.9991, 0.9991,
        0.9987, 0.9991, 0.9984, 0.9985, 0.9982, 0.9984, 0.9987],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 52 | Batch_idx: 0 |  Loss: (0.4280) |  Loss2: (0.0000) | Acc: (81.00%) (104/128)
Epoch: 52 | Batch_idx: 10 |  Loss: (0.4179) |  Loss2: (0.0000) | Acc: (84.00%) (1196/1408)
Epoch: 52 | Batch_idx: 20 |  Loss: (0.4094) |  Loss2: (0.0000) | Acc: (86.00%) (2318/2688)
Epoch: 52 | Batch_idx: 30 |  Loss: (0.3913) |  Loss2: (0.0000) | Acc: (86.00%) (3444/3968)
Epoch: 52 | Batch_idx: 40 |  Loss: (0.3893) |  Loss2: (0.0000) | Acc: (86.00%) (4559/5248)
Epoch: 52 | Batch_idx: 50 |  Loss: (0.3894) |  Loss2: (0.0000) | Acc: (86.00%) (5662/6528)
Epoch: 52 | Batch_idx: 60 |  Loss: (0.3881) |  Loss2: (0.0000) | Acc: (86.00%) (6784/7808)
Epoch: 52 | Batch_idx: 70 |  Loss: (0.3859) |  Loss2: (0.0000) | Acc: (86.00%) (7897/9088)
Epoch: 52 | Batch_idx: 80 |  Loss: (0.3866) |  Loss2: (0.0000) | Acc: (86.00%) (9005/10368)
Epoch: 52 | Batch_idx: 90 |  Loss: (0.3879) |  Loss2: (0.0000) | Acc: (86.00%) (10112/11648)
Epoch: 52 | Batch_idx: 100 |  Loss: (0.3873) |  Loss2: (0.0000) | Acc: (86.00%) (11229/12928)
Epoch: 52 | Batch_idx: 110 |  Loss: (0.3854) |  Loss2: (0.0000) | Acc: (86.00%) (12339/14208)
Epoch: 52 | Batch_idx: 120 |  Loss: (0.3847) |  Loss2: (0.0000) | Acc: (86.00%) (13454/15488)
Epoch: 52 | Batch_idx: 130 |  Loss: (0.3849) |  Loss2: (0.0000) | Acc: (86.00%) (14564/16768)
Epoch: 52 | Batch_idx: 140 |  Loss: (0.3858) |  Loss2: (0.0000) | Acc: (86.00%) (15677/18048)
Epoch: 52 | Batch_idx: 150 |  Loss: (0.3863) |  Loss2: (0.0000) | Acc: (86.00%) (16784/19328)
Epoch: 52 | Batch_idx: 160 |  Loss: (0.3848) |  Loss2: (0.0000) | Acc: (86.00%) (17905/20608)
Epoch: 52 | Batch_idx: 170 |  Loss: (0.3839) |  Loss2: (0.0000) | Acc: (86.00%) (19026/21888)
Epoch: 52 | Batch_idx: 180 |  Loss: (0.3829) |  Loss2: (0.0000) | Acc: (86.00%) (20145/23168)
Epoch: 52 | Batch_idx: 190 |  Loss: (0.3826) |  Loss2: (0.0000) | Acc: (86.00%) (21263/24448)
Epoch: 52 | Batch_idx: 200 |  Loss: (0.3814) |  Loss2: (0.0000) | Acc: (86.00%) (22376/25728)
Epoch: 52 | Batch_idx: 210 |  Loss: (0.3805) |  Loss2: (0.0000) | Acc: (87.00%) (23499/27008)
Epoch: 52 | Batch_idx: 220 |  Loss: (0.3801) |  Loss2: (0.0000) | Acc: (87.00%) (24613/28288)
Epoch: 52 | Batch_idx: 230 |  Loss: (0.3804) |  Loss2: (0.0000) | Acc: (87.00%) (25729/29568)
Epoch: 52 | Batch_idx: 240 |  Loss: (0.3806) |  Loss2: (0.0000) | Acc: (86.00%) (26831/30848)
Epoch: 52 | Batch_idx: 250 |  Loss: (0.3818) |  Loss2: (0.0000) | Acc: (86.00%) (27936/32128)
Epoch: 52 | Batch_idx: 260 |  Loss: (0.3840) |  Loss2: (0.0000) | Acc: (86.00%) (29023/33408)
Epoch: 52 | Batch_idx: 270 |  Loss: (0.3831) |  Loss2: (0.0000) | Acc: (86.00%) (30150/34688)
Epoch: 52 | Batch_idx: 280 |  Loss: (0.3831) |  Loss2: (0.0000) | Acc: (86.00%) (31252/35968)
Epoch: 52 | Batch_idx: 290 |  Loss: (0.3830) |  Loss2: (0.0000) | Acc: (86.00%) (32361/37248)
Epoch: 52 | Batch_idx: 300 |  Loss: (0.3832) |  Loss2: (0.0000) | Acc: (86.00%) (33469/38528)
Epoch: 52 | Batch_idx: 310 |  Loss: (0.3828) |  Loss2: (0.0000) | Acc: (86.00%) (34586/39808)
Epoch: 52 | Batch_idx: 320 |  Loss: (0.3842) |  Loss2: (0.0000) | Acc: (86.00%) (35671/41088)
Epoch: 52 | Batch_idx: 330 |  Loss: (0.3842) |  Loss2: (0.0000) | Acc: (86.00%) (36774/42368)
Epoch: 52 | Batch_idx: 340 |  Loss: (0.3839) |  Loss2: (0.0000) | Acc: (86.00%) (37885/43648)
Epoch: 52 | Batch_idx: 350 |  Loss: (0.3850) |  Loss2: (0.0000) | Acc: (86.00%) (38978/44928)
Epoch: 52 | Batch_idx: 360 |  Loss: (0.3859) |  Loss2: (0.0000) | Acc: (86.00%) (40080/46208)
Epoch: 52 | Batch_idx: 370 |  Loss: (0.3863) |  Loss2: (0.0000) | Acc: (86.00%) (41188/47488)
Epoch: 52 | Batch_idx: 380 |  Loss: (0.3854) |  Loss2: (0.0000) | Acc: (86.00%) (42312/48768)
Epoch: 52 | Batch_idx: 390 |  Loss: (0.3852) |  Loss2: (0.0000) | Acc: (86.00%) (43379/50000)
# TEST : Loss: (0.5113) | Acc: (82.00%) (8231/10000)
percent tensor([0.5255, 0.5419, 0.5079, 0.5100, 0.5134, 0.5118, 0.5399, 0.5225, 0.5387,
        0.5363, 0.5393, 0.5190, 0.5331, 0.5426, 0.5267, 0.5236],
       device='cuda:0') torch.Size([16])
percent tensor([0.4614, 0.4691, 0.4294, 0.4511, 0.4404, 0.4471, 0.4624, 0.4619, 0.4605,
        0.4550, 0.4555, 0.4348, 0.4698, 0.4792, 0.4607, 0.4589],
       device='cuda:0') torch.Size([16])
percent tensor([0.4556, 0.4372, 0.3950, 0.4517, 0.3864, 0.4774, 0.4321, 0.4361, 0.4593,
        0.4162, 0.4500, 0.3995, 0.4357, 0.5054, 0.4464, 0.4553],
       device='cuda:0') torch.Size([16])
percent tensor([0.5783, 0.5833, 0.5694, 0.5630, 0.5685, 0.5745, 0.5829, 0.5689, 0.5746,
        0.5824, 0.5791, 0.5850, 0.5919, 0.5703, 0.5873, 0.5782],
       device='cuda:0') torch.Size([16])
percent tensor([0.6527, 0.6601, 0.6227, 0.6203, 0.6278, 0.6535, 0.6647, 0.6401, 0.6826,
        0.6675, 0.6870, 0.6707, 0.6680, 0.7064, 0.6582, 0.6552],
       device='cuda:0') torch.Size([16])
percent tensor([0.6870, 0.6786, 0.7071, 0.7244, 0.7390, 0.7346, 0.7060, 0.7187, 0.6861,
        0.6690, 0.6834, 0.6934, 0.6581, 0.7147, 0.6916, 0.7099],
       device='cuda:0') torch.Size([16])
percent tensor([0.4730, 0.4811, 0.5922, 0.5915, 0.6192, 0.6938, 0.5080, 0.4270, 0.5404,
        0.4558, 0.5142, 0.4883, 0.4481, 0.5757, 0.3670, 0.5243],
       device='cuda:0') torch.Size([16])
percent tensor([0.9988, 0.9970, 0.9975, 0.9985, 0.9990, 0.9944, 0.9982, 0.9996, 0.9984,
        0.9988, 0.9983, 0.9985, 0.9986, 0.9983, 0.9985, 0.9988],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 53 | Batch_idx: 0 |  Loss: (0.3270) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 53 | Batch_idx: 10 |  Loss: (0.4138) |  Loss2: (0.0000) | Acc: (86.00%) (1212/1408)
Epoch: 53 | Batch_idx: 20 |  Loss: (0.4426) |  Loss2: (0.0000) | Acc: (84.00%) (2282/2688)
Epoch: 53 | Batch_idx: 30 |  Loss: (0.4437) |  Loss2: (0.0000) | Acc: (84.00%) (3361/3968)
Epoch: 53 | Batch_idx: 40 |  Loss: (0.4558) |  Loss2: (0.0000) | Acc: (84.00%) (4421/5248)
Epoch: 53 | Batch_idx: 50 |  Loss: (0.4578) |  Loss2: (0.0000) | Acc: (84.00%) (5492/6528)
Epoch: 53 | Batch_idx: 60 |  Loss: (0.4698) |  Loss2: (0.0000) | Acc: (83.00%) (6532/7808)
Epoch: 53 | Batch_idx: 70 |  Loss: (0.4679) |  Loss2: (0.0000) | Acc: (83.00%) (7600/9088)
Epoch: 53 | Batch_idx: 80 |  Loss: (0.4735) |  Loss2: (0.0000) | Acc: (83.00%) (8653/10368)
Epoch: 53 | Batch_idx: 90 |  Loss: (0.4801) |  Loss2: (0.0000) | Acc: (83.00%) (9690/11648)
Epoch: 53 | Batch_idx: 100 |  Loss: (0.4811) |  Loss2: (0.0000) | Acc: (83.00%) (10758/12928)
Epoch: 53 | Batch_idx: 110 |  Loss: (0.4812) |  Loss2: (0.0000) | Acc: (83.00%) (11824/14208)
Epoch: 53 | Batch_idx: 120 |  Loss: (0.4791) |  Loss2: (0.0000) | Acc: (83.00%) (12898/15488)
Epoch: 53 | Batch_idx: 130 |  Loss: (0.4767) |  Loss2: (0.0000) | Acc: (83.00%) (13983/16768)
Epoch: 53 | Batch_idx: 140 |  Loss: (0.4757) |  Loss2: (0.0000) | Acc: (83.00%) (15069/18048)
Epoch: 53 | Batch_idx: 150 |  Loss: (0.4740) |  Loss2: (0.0000) | Acc: (83.00%) (16145/19328)
Epoch: 53 | Batch_idx: 160 |  Loss: (0.4715) |  Loss2: (0.0000) | Acc: (83.00%) (17237/20608)
Epoch: 53 | Batch_idx: 170 |  Loss: (0.4712) |  Loss2: (0.0000) | Acc: (83.00%) (18308/21888)
Epoch: 53 | Batch_idx: 180 |  Loss: (0.4687) |  Loss2: (0.0000) | Acc: (83.00%) (19403/23168)
Epoch: 53 | Batch_idx: 190 |  Loss: (0.4687) |  Loss2: (0.0000) | Acc: (83.00%) (20480/24448)
Epoch: 53 | Batch_idx: 200 |  Loss: (0.4692) |  Loss2: (0.0000) | Acc: (83.00%) (21545/25728)
Epoch: 53 | Batch_idx: 210 |  Loss: (0.4683) |  Loss2: (0.0000) | Acc: (83.00%) (22624/27008)
Epoch: 53 | Batch_idx: 220 |  Loss: (0.4679) |  Loss2: (0.0000) | Acc: (83.00%) (23702/28288)
Epoch: 53 | Batch_idx: 230 |  Loss: (0.4669) |  Loss2: (0.0000) | Acc: (83.00%) (24774/29568)
Epoch: 53 | Batch_idx: 240 |  Loss: (0.4679) |  Loss2: (0.0000) | Acc: (83.00%) (25840/30848)
Epoch: 53 | Batch_idx: 250 |  Loss: (0.4649) |  Loss2: (0.0000) | Acc: (83.00%) (26953/32128)
Epoch: 53 | Batch_idx: 260 |  Loss: (0.4638) |  Loss2: (0.0000) | Acc: (83.00%) (28041/33408)
Epoch: 53 | Batch_idx: 270 |  Loss: (0.4639) |  Loss2: (0.0000) | Acc: (83.00%) (29108/34688)
Epoch: 53 | Batch_idx: 280 |  Loss: (0.4626) |  Loss2: (0.0000) | Acc: (83.00%) (30208/35968)
Epoch: 53 | Batch_idx: 290 |  Loss: (0.4623) |  Loss2: (0.0000) | Acc: (83.00%) (31287/37248)
Epoch: 53 | Batch_idx: 300 |  Loss: (0.4608) |  Loss2: (0.0000) | Acc: (84.00%) (32380/38528)
Epoch: 53 | Batch_idx: 310 |  Loss: (0.4589) |  Loss2: (0.0000) | Acc: (84.00%) (33502/39808)
Epoch: 53 | Batch_idx: 320 |  Loss: (0.4577) |  Loss2: (0.0000) | Acc: (84.00%) (34593/41088)
Epoch: 53 | Batch_idx: 330 |  Loss: (0.4568) |  Loss2: (0.0000) | Acc: (84.00%) (35690/42368)
Epoch: 53 | Batch_idx: 340 |  Loss: (0.4566) |  Loss2: (0.0000) | Acc: (84.00%) (36768/43648)
Epoch: 53 | Batch_idx: 350 |  Loss: (0.4564) |  Loss2: (0.0000) | Acc: (84.00%) (37860/44928)
Epoch: 53 | Batch_idx: 360 |  Loss: (0.4557) |  Loss2: (0.0000) | Acc: (84.00%) (38964/46208)
Epoch: 53 | Batch_idx: 370 |  Loss: (0.4549) |  Loss2: (0.0000) | Acc: (84.00%) (40058/47488)
Epoch: 53 | Batch_idx: 380 |  Loss: (0.4540) |  Loss2: (0.0000) | Acc: (84.00%) (41154/48768)
Epoch: 53 | Batch_idx: 390 |  Loss: (0.4535) |  Loss2: (0.0000) | Acc: (84.00%) (42191/50000)
# TEST : Loss: (0.5087) | Acc: (82.00%) (8258/10000)
percent tensor([0.5387, 0.5520, 0.5327, 0.5266, 0.5369, 0.5264, 0.5526, 0.5409, 0.5504,
        0.5481, 0.5501, 0.5413, 0.5443, 0.5506, 0.5405, 0.5378],
       device='cuda:0') torch.Size([16])
percent tensor([0.4501, 0.4544, 0.4096, 0.4370, 0.4189, 0.4349, 0.4442, 0.4453, 0.4465,
        0.4413, 0.4443, 0.4145, 0.4598, 0.4686, 0.4461, 0.4481],
       device='cuda:0') torch.Size([16])
percent tensor([0.4633, 0.4548, 0.3964, 0.4482, 0.3982, 0.4901, 0.4478, 0.4382, 0.4666,
        0.4290, 0.4643, 0.4054, 0.4481, 0.5087, 0.4639, 0.4666],
       device='cuda:0') torch.Size([16])
percent tensor([0.6001, 0.6041, 0.5864, 0.5814, 0.5841, 0.5981, 0.6002, 0.5849, 0.5974,
        0.6032, 0.6030, 0.6039, 0.6189, 0.5913, 0.6077, 0.6008],
       device='cuda:0') torch.Size([16])
percent tensor([0.6423, 0.6382, 0.6163, 0.6229, 0.6166, 0.6795, 0.6300, 0.6198, 0.6786,
        0.6372, 0.6719, 0.6411, 0.6547, 0.6914, 0.6233, 0.6554],
       device='cuda:0') torch.Size([16])
percent tensor([0.6790, 0.6734, 0.6994, 0.7189, 0.7311, 0.7271, 0.7000, 0.7105, 0.6768,
        0.6621, 0.6743, 0.6909, 0.6537, 0.7084, 0.6864, 0.7023],
       device='cuda:0') torch.Size([16])
percent tensor([0.4149, 0.4166, 0.6004, 0.6103, 0.6503, 0.6698, 0.4951, 0.4739, 0.4472,
        0.3876, 0.4182, 0.4635, 0.3537, 0.5076, 0.3366, 0.4926],
       device='cuda:0') torch.Size([16])
percent tensor([0.9988, 0.9973, 0.9979, 0.9985, 0.9993, 0.9947, 0.9977, 0.9996, 0.9983,
        0.9985, 0.9983, 0.9987, 0.9983, 0.9981, 0.9987, 0.9988],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 54 | Batch_idx: 0 |  Loss: (0.5769) |  Loss2: (0.0000) | Acc: (78.00%) (101/128)
Epoch: 54 | Batch_idx: 10 |  Loss: (0.3668) |  Loss2: (0.0000) | Acc: (87.00%) (1230/1408)
Epoch: 54 | Batch_idx: 20 |  Loss: (0.3666) |  Loss2: (0.0000) | Acc: (87.00%) (2360/2688)
Epoch: 54 | Batch_idx: 30 |  Loss: (0.3646) |  Loss2: (0.0000) | Acc: (87.00%) (3482/3968)
Epoch: 54 | Batch_idx: 40 |  Loss: (0.3665) |  Loss2: (0.0000) | Acc: (87.00%) (4603/5248)
Epoch: 54 | Batch_idx: 50 |  Loss: (0.3638) |  Loss2: (0.0000) | Acc: (87.00%) (5723/6528)
Epoch: 54 | Batch_idx: 60 |  Loss: (0.3646) |  Loss2: (0.0000) | Acc: (87.00%) (6848/7808)
Epoch: 54 | Batch_idx: 70 |  Loss: (0.3634) |  Loss2: (0.0000) | Acc: (87.00%) (7972/9088)
Epoch: 54 | Batch_idx: 80 |  Loss: (0.3613) |  Loss2: (0.0000) | Acc: (87.00%) (9103/10368)
Epoch: 54 | Batch_idx: 90 |  Loss: (0.3657) |  Loss2: (0.0000) | Acc: (87.00%) (10209/11648)
Epoch: 54 | Batch_idx: 100 |  Loss: (0.3635) |  Loss2: (0.0000) | Acc: (87.00%) (11346/12928)
Epoch: 54 | Batch_idx: 110 |  Loss: (0.3637) |  Loss2: (0.0000) | Acc: (87.00%) (12467/14208)
Epoch: 54 | Batch_idx: 120 |  Loss: (0.3642) |  Loss2: (0.0000) | Acc: (87.00%) (13577/15488)
Epoch: 54 | Batch_idx: 130 |  Loss: (0.3642) |  Loss2: (0.0000) | Acc: (87.00%) (14696/16768)
Epoch: 54 | Batch_idx: 140 |  Loss: (0.3622) |  Loss2: (0.0000) | Acc: (87.00%) (15828/18048)
Epoch: 54 | Batch_idx: 150 |  Loss: (0.3630) |  Loss2: (0.0000) | Acc: (87.00%) (16942/19328)
Epoch: 54 | Batch_idx: 160 |  Loss: (0.3641) |  Loss2: (0.0000) | Acc: (87.00%) (18064/20608)
Epoch: 54 | Batch_idx: 170 |  Loss: (0.3653) |  Loss2: (0.0000) | Acc: (87.00%) (19180/21888)
Epoch: 54 | Batch_idx: 180 |  Loss: (0.3669) |  Loss2: (0.0000) | Acc: (87.00%) (20283/23168)
Epoch: 54 | Batch_idx: 190 |  Loss: (0.3705) |  Loss2: (0.0000) | Acc: (87.00%) (21359/24448)
Epoch: 54 | Batch_idx: 200 |  Loss: (0.3695) |  Loss2: (0.0000) | Acc: (87.00%) (22475/25728)
Epoch: 54 | Batch_idx: 210 |  Loss: (0.3706) |  Loss2: (0.0000) | Acc: (87.00%) (23586/27008)
Epoch: 54 | Batch_idx: 220 |  Loss: (0.3705) |  Loss2: (0.0000) | Acc: (87.00%) (24707/28288)
Epoch: 54 | Batch_idx: 230 |  Loss: (0.3699) |  Loss2: (0.0000) | Acc: (87.00%) (25835/29568)
Epoch: 54 | Batch_idx: 240 |  Loss: (0.3709) |  Loss2: (0.0000) | Acc: (87.00%) (26950/30848)
Epoch: 54 | Batch_idx: 250 |  Loss: (0.3697) |  Loss2: (0.0000) | Acc: (87.00%) (28085/32128)
Epoch: 54 | Batch_idx: 260 |  Loss: (0.3715) |  Loss2: (0.0000) | Acc: (87.00%) (29176/33408)
Epoch: 54 | Batch_idx: 270 |  Loss: (0.3708) |  Loss2: (0.0000) | Acc: (87.00%) (30297/34688)
Epoch: 54 | Batch_idx: 280 |  Loss: (0.3698) |  Loss2: (0.0000) | Acc: (87.00%) (31436/35968)
Epoch: 54 | Batch_idx: 290 |  Loss: (0.3708) |  Loss2: (0.0000) | Acc: (87.00%) (32543/37248)
Epoch: 54 | Batch_idx: 300 |  Loss: (0.3703) |  Loss2: (0.0000) | Acc: (87.00%) (33674/38528)
Epoch: 54 | Batch_idx: 310 |  Loss: (0.3711) |  Loss2: (0.0000) | Acc: (87.00%) (34780/39808)
Epoch: 54 | Batch_idx: 320 |  Loss: (0.3708) |  Loss2: (0.0000) | Acc: (87.00%) (35894/41088)
Epoch: 54 | Batch_idx: 330 |  Loss: (0.3715) |  Loss2: (0.0000) | Acc: (87.00%) (36993/42368)
Epoch: 54 | Batch_idx: 340 |  Loss: (0.3717) |  Loss2: (0.0000) | Acc: (87.00%) (38107/43648)
Epoch: 54 | Batch_idx: 350 |  Loss: (0.3718) |  Loss2: (0.0000) | Acc: (87.00%) (39207/44928)
Epoch: 54 | Batch_idx: 360 |  Loss: (0.3724) |  Loss2: (0.0000) | Acc: (87.00%) (40319/46208)
Epoch: 54 | Batch_idx: 370 |  Loss: (0.3727) |  Loss2: (0.0000) | Acc: (87.00%) (41429/47488)
Epoch: 54 | Batch_idx: 380 |  Loss: (0.3730) |  Loss2: (0.0000) | Acc: (87.00%) (42546/48768)
Epoch: 54 | Batch_idx: 390 |  Loss: (0.3735) |  Loss2: (0.0000) | Acc: (87.00%) (43606/50000)
# TEST : Loss: (0.4937) | Acc: (82.00%) (8296/10000)
percent tensor([0.5380, 0.5530, 0.5304, 0.5254, 0.5343, 0.5252, 0.5527, 0.5405, 0.5497,
        0.5482, 0.5506, 0.5398, 0.5438, 0.5526, 0.5405, 0.5379],
       device='cuda:0') torch.Size([16])
percent tensor([0.4521, 0.4538, 0.4151, 0.4401, 0.4237, 0.4355, 0.4455, 0.4474, 0.4476,
        0.4414, 0.4451, 0.4196, 0.4608, 0.4665, 0.4453, 0.4485],
       device='cuda:0') torch.Size([16])
percent tensor([0.4650, 0.4553, 0.3967, 0.4541, 0.4010, 0.4869, 0.4427, 0.4415, 0.4640,
        0.4277, 0.4601, 0.4033, 0.4481, 0.5070, 0.4634, 0.4685],
       device='cuda:0') torch.Size([16])
percent tensor([0.6038, 0.6040, 0.5909, 0.5854, 0.5861, 0.6002, 0.6044, 0.5833, 0.5984,
        0.6065, 0.6062, 0.6102, 0.6193, 0.5949, 0.6087, 0.6020],
       device='cuda:0') torch.Size([16])
percent tensor([0.6551, 0.6449, 0.6267, 0.6258, 0.6227, 0.6848, 0.6391, 0.6140, 0.6941,
        0.6556, 0.6931, 0.6748, 0.6623, 0.7083, 0.6276, 0.6652],
       device='cuda:0') torch.Size([16])
percent tensor([0.6762, 0.6733, 0.6978, 0.7133, 0.7278, 0.7261, 0.6979, 0.7114, 0.6844,
        0.6702, 0.6765, 0.6918, 0.6563, 0.7090, 0.6782, 0.7015],
       device='cuda:0') torch.Size([16])
percent tensor([0.4177, 0.4296, 0.5835, 0.5719, 0.6306, 0.6731, 0.4799, 0.4678, 0.4725,
        0.4273, 0.4374, 0.4650, 0.3697, 0.5100, 0.3565, 0.4825],
       device='cuda:0') torch.Size([16])
percent tensor([0.9990, 0.9982, 0.9976, 0.9972, 0.9990, 0.9945, 0.9986, 0.9994, 0.9989,
        0.9989, 0.9993, 0.9991, 0.9990, 0.9990, 0.9983, 0.9987],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 55 | Batch_idx: 0 |  Loss: (0.2499) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 55 | Batch_idx: 10 |  Loss: (0.3775) |  Loss2: (0.0000) | Acc: (87.00%) (1227/1408)
Epoch: 55 | Batch_idx: 20 |  Loss: (0.3776) |  Loss2: (0.0000) | Acc: (87.00%) (2339/2688)
Epoch: 55 | Batch_idx: 30 |  Loss: (0.3988) |  Loss2: (0.0000) | Acc: (86.00%) (3425/3968)
Epoch: 55 | Batch_idx: 40 |  Loss: (0.4103) |  Loss2: (0.0000) | Acc: (85.00%) (4512/5248)
Epoch: 55 | Batch_idx: 50 |  Loss: (0.4146) |  Loss2: (0.0000) | Acc: (85.00%) (5610/6528)
Epoch: 55 | Batch_idx: 60 |  Loss: (0.4147) |  Loss2: (0.0000) | Acc: (85.00%) (6708/7808)
Epoch: 55 | Batch_idx: 70 |  Loss: (0.4194) |  Loss2: (0.0000) | Acc: (85.00%) (7799/9088)
Epoch: 55 | Batch_idx: 80 |  Loss: (0.4218) |  Loss2: (0.0000) | Acc: (85.00%) (8896/10368)
Epoch: 55 | Batch_idx: 90 |  Loss: (0.4209) |  Loss2: (0.0000) | Acc: (85.00%) (9994/11648)
Epoch: 55 | Batch_idx: 100 |  Loss: (0.4185) |  Loss2: (0.0000) | Acc: (85.00%) (11093/12928)
Epoch: 55 | Batch_idx: 110 |  Loss: (0.4194) |  Loss2: (0.0000) | Acc: (85.00%) (12182/14208)
Epoch: 55 | Batch_idx: 120 |  Loss: (0.4189) |  Loss2: (0.0000) | Acc: (85.00%) (13278/15488)
Epoch: 55 | Batch_idx: 130 |  Loss: (0.4182) |  Loss2: (0.0000) | Acc: (85.00%) (14368/16768)
Epoch: 55 | Batch_idx: 140 |  Loss: (0.4195) |  Loss2: (0.0000) | Acc: (85.00%) (15452/18048)
Epoch: 55 | Batch_idx: 150 |  Loss: (0.4195) |  Loss2: (0.0000) | Acc: (85.00%) (16543/19328)
Epoch: 55 | Batch_idx: 160 |  Loss: (0.4198) |  Loss2: (0.0000) | Acc: (85.00%) (17652/20608)
Epoch: 55 | Batch_idx: 170 |  Loss: (0.4187) |  Loss2: (0.0000) | Acc: (85.00%) (18754/21888)
Epoch: 55 | Batch_idx: 180 |  Loss: (0.4191) |  Loss2: (0.0000) | Acc: (85.00%) (19859/23168)
Epoch: 55 | Batch_idx: 190 |  Loss: (0.4191) |  Loss2: (0.0000) | Acc: (85.00%) (20951/24448)
Epoch: 55 | Batch_idx: 200 |  Loss: (0.4176) |  Loss2: (0.0000) | Acc: (85.00%) (22051/25728)
Epoch: 55 | Batch_idx: 210 |  Loss: (0.4148) |  Loss2: (0.0000) | Acc: (85.00%) (23173/27008)
Epoch: 55 | Batch_idx: 220 |  Loss: (0.4158) |  Loss2: (0.0000) | Acc: (85.00%) (24258/28288)
Epoch: 55 | Batch_idx: 230 |  Loss: (0.4149) |  Loss2: (0.0000) | Acc: (85.00%) (25362/29568)
Epoch: 55 | Batch_idx: 240 |  Loss: (0.4142) |  Loss2: (0.0000) | Acc: (85.00%) (26461/30848)
Epoch: 55 | Batch_idx: 250 |  Loss: (0.4130) |  Loss2: (0.0000) | Acc: (85.00%) (27576/32128)
Epoch: 55 | Batch_idx: 260 |  Loss: (0.4126) |  Loss2: (0.0000) | Acc: (85.00%) (28685/33408)
Epoch: 55 | Batch_idx: 270 |  Loss: (0.4124) |  Loss2: (0.0000) | Acc: (85.00%) (29787/34688)
Epoch: 55 | Batch_idx: 280 |  Loss: (0.4130) |  Loss2: (0.0000) | Acc: (85.00%) (30880/35968)
Epoch: 55 | Batch_idx: 290 |  Loss: (0.4132) |  Loss2: (0.0000) | Acc: (85.00%) (31983/37248)
Epoch: 55 | Batch_idx: 300 |  Loss: (0.4130) |  Loss2: (0.0000) | Acc: (85.00%) (33087/38528)
Epoch: 55 | Batch_idx: 310 |  Loss: (0.4136) |  Loss2: (0.0000) | Acc: (85.00%) (34173/39808)
Epoch: 55 | Batch_idx: 320 |  Loss: (0.4129) |  Loss2: (0.0000) | Acc: (85.00%) (35279/41088)
Epoch: 55 | Batch_idx: 330 |  Loss: (0.4120) |  Loss2: (0.0000) | Acc: (85.00%) (36378/42368)
Epoch: 55 | Batch_idx: 340 |  Loss: (0.4113) |  Loss2: (0.0000) | Acc: (85.00%) (37508/43648)
Epoch: 55 | Batch_idx: 350 |  Loss: (0.4107) |  Loss2: (0.0000) | Acc: (85.00%) (38615/44928)
Epoch: 55 | Batch_idx: 360 |  Loss: (0.4110) |  Loss2: (0.0000) | Acc: (85.00%) (39701/46208)
Epoch: 55 | Batch_idx: 370 |  Loss: (0.4111) |  Loss2: (0.0000) | Acc: (85.00%) (40795/47488)
Epoch: 55 | Batch_idx: 380 |  Loss: (0.4107) |  Loss2: (0.0000) | Acc: (85.00%) (41905/48768)
Epoch: 55 | Batch_idx: 390 |  Loss: (0.4100) |  Loss2: (0.0000) | Acc: (85.00%) (42974/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_055.pth.tar'
# TEST : Loss: (0.4742) | Acc: (83.00%) (8389/10000)
percent tensor([0.5301, 0.5429, 0.5190, 0.5161, 0.5228, 0.5155, 0.5423, 0.5307, 0.5406,
        0.5385, 0.5414, 0.5284, 0.5353, 0.5437, 0.5301, 0.5285],
       device='cuda:0') torch.Size([16])
percent tensor([0.4785, 0.4789, 0.4531, 0.4696, 0.4598, 0.4673, 0.4753, 0.4758, 0.4771,
        0.4699, 0.4729, 0.4585, 0.4834, 0.4875, 0.4737, 0.4753],
       device='cuda:0') torch.Size([16])
percent tensor([0.4698, 0.4567, 0.3980, 0.4601, 0.4054, 0.4906, 0.4442, 0.4473, 0.4601,
        0.4292, 0.4600, 0.3998, 0.4499, 0.4998, 0.4699, 0.4742],
       device='cuda:0') torch.Size([16])
percent tensor([0.5997, 0.5923, 0.5923, 0.5861, 0.5872, 0.6002, 0.6003, 0.5836, 0.5943,
        0.5967, 0.5946, 0.6083, 0.6054, 0.5890, 0.6041, 0.5967],
       device='cuda:0') torch.Size([16])
percent tensor([0.6098, 0.5815, 0.6128, 0.6201, 0.6216, 0.6576, 0.5981, 0.6046, 0.6668,
        0.5999, 0.6329, 0.6517, 0.5827, 0.6704, 0.5808, 0.6175],
       device='cuda:0') torch.Size([16])
percent tensor([0.6947, 0.6871, 0.7100, 0.7343, 0.7465, 0.7457, 0.7149, 0.7332, 0.7024,
        0.6848, 0.6879, 0.7019, 0.6682, 0.7187, 0.6971, 0.7217],
       device='cuda:0') torch.Size([16])
percent tensor([0.5403, 0.5507, 0.6439, 0.6356, 0.6821, 0.7670, 0.5646, 0.5128, 0.5855,
        0.5550, 0.5709, 0.5052, 0.4797, 0.6234, 0.4240, 0.6048],
       device='cuda:0') torch.Size([16])
percent tensor([0.9994, 0.9974, 0.9980, 0.9977, 0.9991, 0.9968, 0.9986, 0.9994, 0.9985,
        0.9985, 0.9991, 0.9994, 0.9988, 0.9989, 0.9976, 0.9992],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 56 | Batch_idx: 0 |  Loss: (0.3797) |  Loss2: (0.0000) | Acc: (85.00%) (110/128)
Epoch: 56 | Batch_idx: 10 |  Loss: (0.3441) |  Loss2: (0.0000) | Acc: (87.00%) (1237/1408)
Epoch: 56 | Batch_idx: 20 |  Loss: (0.3478) |  Loss2: (0.0000) | Acc: (87.00%) (2359/2688)
Epoch: 56 | Batch_idx: 30 |  Loss: (0.3486) |  Loss2: (0.0000) | Acc: (87.00%) (3485/3968)
Epoch: 56 | Batch_idx: 40 |  Loss: (0.3462) |  Loss2: (0.0000) | Acc: (87.00%) (4611/5248)
Epoch: 56 | Batch_idx: 50 |  Loss: (0.3496) |  Loss2: (0.0000) | Acc: (87.00%) (5714/6528)
Epoch: 56 | Batch_idx: 60 |  Loss: (0.3487) |  Loss2: (0.0000) | Acc: (87.00%) (6853/7808)
Epoch: 56 | Batch_idx: 70 |  Loss: (0.3452) |  Loss2: (0.0000) | Acc: (87.00%) (7991/9088)
Epoch: 56 | Batch_idx: 80 |  Loss: (0.3458) |  Loss2: (0.0000) | Acc: (87.00%) (9117/10368)
Epoch: 56 | Batch_idx: 90 |  Loss: (0.3492) |  Loss2: (0.0000) | Acc: (87.00%) (10223/11648)
Epoch: 56 | Batch_idx: 100 |  Loss: (0.3511) |  Loss2: (0.0000) | Acc: (87.00%) (11339/12928)
Epoch: 56 | Batch_idx: 110 |  Loss: (0.3521) |  Loss2: (0.0000) | Acc: (87.00%) (12459/14208)
Epoch: 56 | Batch_idx: 120 |  Loss: (0.3534) |  Loss2: (0.0000) | Acc: (87.00%) (13584/15488)
Epoch: 56 | Batch_idx: 130 |  Loss: (0.3537) |  Loss2: (0.0000) | Acc: (87.00%) (14712/16768)
Epoch: 56 | Batch_idx: 140 |  Loss: (0.3548) |  Loss2: (0.0000) | Acc: (87.00%) (15832/18048)
Epoch: 56 | Batch_idx: 150 |  Loss: (0.3567) |  Loss2: (0.0000) | Acc: (87.00%) (16940/19328)
Epoch: 56 | Batch_idx: 160 |  Loss: (0.3564) |  Loss2: (0.0000) | Acc: (87.00%) (18064/20608)
Epoch: 56 | Batch_idx: 170 |  Loss: (0.3558) |  Loss2: (0.0000) | Acc: (87.00%) (19183/21888)
Epoch: 56 | Batch_idx: 180 |  Loss: (0.3567) |  Loss2: (0.0000) | Acc: (87.00%) (20300/23168)
Epoch: 56 | Batch_idx: 190 |  Loss: (0.3572) |  Loss2: (0.0000) | Acc: (87.00%) (21416/24448)
Epoch: 56 | Batch_idx: 200 |  Loss: (0.3588) |  Loss2: (0.0000) | Acc: (87.00%) (22511/25728)
Epoch: 56 | Batch_idx: 210 |  Loss: (0.3599) |  Loss2: (0.0000) | Acc: (87.00%) (23621/27008)
Epoch: 56 | Batch_idx: 220 |  Loss: (0.3588) |  Loss2: (0.0000) | Acc: (87.00%) (24754/28288)
Epoch: 56 | Batch_idx: 230 |  Loss: (0.3590) |  Loss2: (0.0000) | Acc: (87.00%) (25880/29568)
Epoch: 56 | Batch_idx: 240 |  Loss: (0.3584) |  Loss2: (0.0000) | Acc: (87.00%) (27015/30848)
Epoch: 56 | Batch_idx: 250 |  Loss: (0.3595) |  Loss2: (0.0000) | Acc: (87.00%) (28119/32128)
Epoch: 56 | Batch_idx: 260 |  Loss: (0.3604) |  Loss2: (0.0000) | Acc: (87.00%) (29231/33408)
Epoch: 56 | Batch_idx: 270 |  Loss: (0.3614) |  Loss2: (0.0000) | Acc: (87.00%) (30340/34688)
Epoch: 56 | Batch_idx: 280 |  Loss: (0.3625) |  Loss2: (0.0000) | Acc: (87.00%) (31460/35968)
Epoch: 56 | Batch_idx: 290 |  Loss: (0.3631) |  Loss2: (0.0000) | Acc: (87.00%) (32568/37248)
Epoch: 56 | Batch_idx: 300 |  Loss: (0.3648) |  Loss2: (0.0000) | Acc: (87.00%) (33660/38528)
Epoch: 56 | Batch_idx: 310 |  Loss: (0.3644) |  Loss2: (0.0000) | Acc: (87.00%) (34783/39808)
Epoch: 56 | Batch_idx: 320 |  Loss: (0.3656) |  Loss2: (0.0000) | Acc: (87.00%) (35893/41088)
Epoch: 56 | Batch_idx: 330 |  Loss: (0.3655) |  Loss2: (0.0000) | Acc: (87.00%) (37016/42368)
Epoch: 56 | Batch_idx: 340 |  Loss: (0.3664) |  Loss2: (0.0000) | Acc: (87.00%) (38123/43648)
Epoch: 56 | Batch_idx: 350 |  Loss: (0.3665) |  Loss2: (0.0000) | Acc: (87.00%) (39245/44928)
Epoch: 56 | Batch_idx: 360 |  Loss: (0.3655) |  Loss2: (0.0000) | Acc: (87.00%) (40366/46208)
Epoch: 56 | Batch_idx: 370 |  Loss: (0.3641) |  Loss2: (0.0000) | Acc: (87.00%) (41516/47488)
Epoch: 56 | Batch_idx: 380 |  Loss: (0.3630) |  Loss2: (0.0000) | Acc: (87.00%) (42659/48768)
Epoch: 56 | Batch_idx: 390 |  Loss: (0.3623) |  Loss2: (0.0000) | Acc: (87.00%) (43742/50000)
# TEST : Loss: (0.5054) | Acc: (82.00%) (8266/10000)
percent tensor([0.5300, 0.5415, 0.5193, 0.5184, 0.5244, 0.5143, 0.5415, 0.5320, 0.5404,
        0.5386, 0.5402, 0.5282, 0.5354, 0.5415, 0.5292, 0.5284],
       device='cuda:0') torch.Size([16])
percent tensor([0.4781, 0.4777, 0.4543, 0.4721, 0.4609, 0.4661, 0.4722, 0.4780, 0.4755,
        0.4699, 0.4697, 0.4559, 0.4826, 0.4862, 0.4732, 0.4743],
       device='cuda:0') torch.Size([16])
percent tensor([0.4713, 0.4542, 0.4009, 0.4627, 0.4047, 0.4883, 0.4471, 0.4501, 0.4638,
        0.4320, 0.4589, 0.4074, 0.4495, 0.5054, 0.4636, 0.4745],
       device='cuda:0') torch.Size([16])
percent tensor([0.5996, 0.5943, 0.5915, 0.5860, 0.5868, 0.6015, 0.5967, 0.5868, 0.5955,
        0.5948, 0.5970, 0.6021, 0.6087, 0.5839, 0.6056, 0.5980],
       device='cuda:0') torch.Size([16])
percent tensor([0.6182, 0.5711, 0.6133, 0.6245, 0.6151, 0.6694, 0.5801, 0.6098, 0.6567,
        0.5847, 0.6328, 0.6212, 0.5899, 0.6399, 0.5869, 0.6210],
       device='cuda:0') torch.Size([16])
percent tensor([0.6956, 0.6860, 0.7064, 0.7280, 0.7437, 0.7515, 0.7151, 0.7287, 0.6937,
        0.6809, 0.6876, 0.6996, 0.6661, 0.7239, 0.6970, 0.7215],
       device='cuda:0') torch.Size([16])
percent tensor([0.5241, 0.5332, 0.6349, 0.6221, 0.6922, 0.7650, 0.5825, 0.5047, 0.5858,
        0.5360, 0.5490, 0.5383, 0.4706, 0.6439, 0.3856, 0.6028],
       device='cuda:0') torch.Size([16])
percent tensor([0.9993, 0.9978, 0.9972, 0.9976, 0.9992, 0.9934, 0.9984, 0.9996, 0.9991,
        0.9991, 0.9992, 0.9990, 0.9990, 0.9988, 0.9987, 0.9989],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 57 | Batch_idx: 0 |  Loss: (0.2685) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 57 | Batch_idx: 10 |  Loss: (0.4021) |  Loss2: (0.0000) | Acc: (85.00%) (1208/1408)
Epoch: 57 | Batch_idx: 20 |  Loss: (0.4181) |  Loss2: (0.0000) | Acc: (85.00%) (2294/2688)
Epoch: 57 | Batch_idx: 30 |  Loss: (0.4213) |  Loss2: (0.0000) | Acc: (85.00%) (3383/3968)
Epoch: 57 | Batch_idx: 40 |  Loss: (0.4302) |  Loss2: (0.0000) | Acc: (84.00%) (4456/5248)
Epoch: 57 | Batch_idx: 50 |  Loss: (0.4320) |  Loss2: (0.0000) | Acc: (85.00%) (5555/6528)
Epoch: 57 | Batch_idx: 60 |  Loss: (0.4243) |  Loss2: (0.0000) | Acc: (85.00%) (6654/7808)
Epoch: 57 | Batch_idx: 70 |  Loss: (0.4271) |  Loss2: (0.0000) | Acc: (85.00%) (7734/9088)
Epoch: 57 | Batch_idx: 80 |  Loss: (0.4268) |  Loss2: (0.0000) | Acc: (85.00%) (8829/10368)
Epoch: 57 | Batch_idx: 90 |  Loss: (0.4258) |  Loss2: (0.0000) | Acc: (85.00%) (9921/11648)
Epoch: 57 | Batch_idx: 100 |  Loss: (0.4257) |  Loss2: (0.0000) | Acc: (85.00%) (11002/12928)
Epoch: 57 | Batch_idx: 110 |  Loss: (0.4242) |  Loss2: (0.0000) | Acc: (85.00%) (12090/14208)
Epoch: 57 | Batch_idx: 120 |  Loss: (0.4238) |  Loss2: (0.0000) | Acc: (85.00%) (13179/15488)
Epoch: 57 | Batch_idx: 130 |  Loss: (0.4219) |  Loss2: (0.0000) | Acc: (85.00%) (14263/16768)
Epoch: 57 | Batch_idx: 140 |  Loss: (0.4214) |  Loss2: (0.0000) | Acc: (85.00%) (15356/18048)
Epoch: 57 | Batch_idx: 150 |  Loss: (0.4200) |  Loss2: (0.0000) | Acc: (85.00%) (16464/19328)
Epoch: 57 | Batch_idx: 160 |  Loss: (0.4193) |  Loss2: (0.0000) | Acc: (85.00%) (17570/20608)
Epoch: 57 | Batch_idx: 170 |  Loss: (0.4181) |  Loss2: (0.0000) | Acc: (85.00%) (18675/21888)
Epoch: 57 | Batch_idx: 180 |  Loss: (0.4187) |  Loss2: (0.0000) | Acc: (85.00%) (19754/23168)
Epoch: 57 | Batch_idx: 190 |  Loss: (0.4171) |  Loss2: (0.0000) | Acc: (85.00%) (20860/24448)
Epoch: 57 | Batch_idx: 200 |  Loss: (0.4160) |  Loss2: (0.0000) | Acc: (85.00%) (21959/25728)
Epoch: 57 | Batch_idx: 210 |  Loss: (0.4161) |  Loss2: (0.0000) | Acc: (85.00%) (23053/27008)
Epoch: 57 | Batch_idx: 220 |  Loss: (0.4150) |  Loss2: (0.0000) | Acc: (85.00%) (24161/28288)
Epoch: 57 | Batch_idx: 230 |  Loss: (0.4143) |  Loss2: (0.0000) | Acc: (85.00%) (25259/29568)
Epoch: 57 | Batch_idx: 240 |  Loss: (0.4130) |  Loss2: (0.0000) | Acc: (85.00%) (26367/30848)
Epoch: 57 | Batch_idx: 250 |  Loss: (0.4138) |  Loss2: (0.0000) | Acc: (85.00%) (27459/32128)
Epoch: 57 | Batch_idx: 260 |  Loss: (0.4133) |  Loss2: (0.0000) | Acc: (85.00%) (28566/33408)
Epoch: 57 | Batch_idx: 270 |  Loss: (0.4135) |  Loss2: (0.0000) | Acc: (85.00%) (29660/34688)
Epoch: 57 | Batch_idx: 280 |  Loss: (0.4129) |  Loss2: (0.0000) | Acc: (85.00%) (30766/35968)
Epoch: 57 | Batch_idx: 290 |  Loss: (0.4126) |  Loss2: (0.0000) | Acc: (85.00%) (31875/37248)
Epoch: 57 | Batch_idx: 300 |  Loss: (0.4117) |  Loss2: (0.0000) | Acc: (85.00%) (32988/38528)
Epoch: 57 | Batch_idx: 310 |  Loss: (0.4110) |  Loss2: (0.0000) | Acc: (85.00%) (34088/39808)
Epoch: 57 | Batch_idx: 320 |  Loss: (0.4095) |  Loss2: (0.0000) | Acc: (85.00%) (35203/41088)
Epoch: 57 | Batch_idx: 330 |  Loss: (0.4083) |  Loss2: (0.0000) | Acc: (85.00%) (36317/42368)
Epoch: 57 | Batch_idx: 340 |  Loss: (0.4088) |  Loss2: (0.0000) | Acc: (85.00%) (37407/43648)
Epoch: 57 | Batch_idx: 350 |  Loss: (0.4084) |  Loss2: (0.0000) | Acc: (85.00%) (38523/44928)
Epoch: 57 | Batch_idx: 360 |  Loss: (0.4071) |  Loss2: (0.0000) | Acc: (85.00%) (39645/46208)
Epoch: 57 | Batch_idx: 370 |  Loss: (0.4062) |  Loss2: (0.0000) | Acc: (85.00%) (40762/47488)
Epoch: 57 | Batch_idx: 380 |  Loss: (0.4058) |  Loss2: (0.0000) | Acc: (85.00%) (41877/48768)
Epoch: 57 | Batch_idx: 390 |  Loss: (0.4056) |  Loss2: (0.0000) | Acc: (85.00%) (42935/50000)
# TEST : Loss: (0.4867) | Acc: (83.00%) (8375/10000)
percent tensor([0.5205, 0.5304, 0.5048, 0.5067, 0.5109, 0.5070, 0.5295, 0.5170, 0.5295,
        0.5263, 0.5305, 0.5121, 0.5253, 0.5322, 0.5187, 0.5186],
       device='cuda:0') torch.Size([16])
percent tensor([0.4769, 0.4726, 0.4507, 0.4728, 0.4553, 0.4627, 0.4667, 0.4775, 0.4727,
        0.4656, 0.4656, 0.4502, 0.4802, 0.4871, 0.4684, 0.4730],
       device='cuda:0') torch.Size([16])
percent tensor([0.4727, 0.4577, 0.4055, 0.4646, 0.3967, 0.4834, 0.4492, 0.4487, 0.4652,
        0.4406, 0.4651, 0.4143, 0.4557, 0.5100, 0.4596, 0.4758],
       device='cuda:0') torch.Size([16])
percent tensor([0.5871, 0.5831, 0.5865, 0.5846, 0.5818, 0.5918, 0.5842, 0.5828, 0.5866,
        0.5842, 0.5842, 0.5916, 0.5941, 0.5763, 0.5944, 0.5883],
       device='cuda:0') torch.Size([16])
percent tensor([0.6663, 0.6264, 0.6840, 0.7008, 0.6778, 0.7215, 0.6345, 0.6893, 0.7178,
        0.6392, 0.6883, 0.6895, 0.6359, 0.7051, 0.6493, 0.6750],
       device='cuda:0') torch.Size([16])
percent tensor([0.7026, 0.6929, 0.7172, 0.7391, 0.7505, 0.7532, 0.7200, 0.7343, 0.7035,
        0.6885, 0.6925, 0.7118, 0.6736, 0.7293, 0.7057, 0.7251],
       device='cuda:0') torch.Size([16])
percent tensor([0.5286, 0.5681, 0.5966, 0.5714, 0.6160, 0.7429, 0.5717, 0.4499, 0.5919,
        0.5670, 0.5608, 0.5340, 0.5188, 0.6330, 0.3937, 0.5944],
       device='cuda:0') torch.Size([16])
percent tensor([0.9993, 0.9980, 0.9968, 0.9969, 0.9994, 0.9928, 0.9976, 0.9994, 0.9987,
        0.9991, 0.9992, 0.9991, 0.9990, 0.9987, 0.9989, 0.9990],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 58 | Batch_idx: 0 |  Loss: (0.3266) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 58 | Batch_idx: 10 |  Loss: (0.3998) |  Loss2: (0.0000) | Acc: (86.00%) (1220/1408)
Epoch: 58 | Batch_idx: 20 |  Loss: (0.3590) |  Loss2: (0.0000) | Acc: (88.00%) (2373/2688)
Epoch: 58 | Batch_idx: 30 |  Loss: (0.3440) |  Loss2: (0.0000) | Acc: (88.00%) (3510/3968)
Epoch: 58 | Batch_idx: 40 |  Loss: (0.3492) |  Loss2: (0.0000) | Acc: (88.00%) (4637/5248)
Epoch: 58 | Batch_idx: 50 |  Loss: (0.3480) |  Loss2: (0.0000) | Acc: (88.00%) (5777/6528)
Epoch: 58 | Batch_idx: 60 |  Loss: (0.3470) |  Loss2: (0.0000) | Acc: (88.00%) (6902/7808)
Epoch: 58 | Batch_idx: 70 |  Loss: (0.3433) |  Loss2: (0.0000) | Acc: (88.00%) (8042/9088)
Epoch: 58 | Batch_idx: 80 |  Loss: (0.3483) |  Loss2: (0.0000) | Acc: (88.00%) (9145/10368)
Epoch: 58 | Batch_idx: 90 |  Loss: (0.3451) |  Loss2: (0.0000) | Acc: (88.00%) (10293/11648)
Epoch: 58 | Batch_idx: 100 |  Loss: (0.3451) |  Loss2: (0.0000) | Acc: (88.00%) (11427/12928)
Epoch: 58 | Batch_idx: 110 |  Loss: (0.3431) |  Loss2: (0.0000) | Acc: (88.00%) (12571/14208)
Epoch: 58 | Batch_idx: 120 |  Loss: (0.3432) |  Loss2: (0.0000) | Acc: (88.00%) (13713/15488)
Epoch: 58 | Batch_idx: 130 |  Loss: (0.3440) |  Loss2: (0.0000) | Acc: (88.00%) (14837/16768)
Epoch: 58 | Batch_idx: 140 |  Loss: (0.3448) |  Loss2: (0.0000) | Acc: (88.00%) (15957/18048)
Epoch: 58 | Batch_idx: 150 |  Loss: (0.3454) |  Loss2: (0.0000) | Acc: (88.00%) (17071/19328)
Epoch: 58 | Batch_idx: 160 |  Loss: (0.3473) |  Loss2: (0.0000) | Acc: (88.00%) (18189/20608)
Epoch: 58 | Batch_idx: 170 |  Loss: (0.3458) |  Loss2: (0.0000) | Acc: (88.00%) (19326/21888)
Epoch: 58 | Batch_idx: 180 |  Loss: (0.3466) |  Loss2: (0.0000) | Acc: (88.00%) (20439/23168)
Epoch: 58 | Batch_idx: 190 |  Loss: (0.3482) |  Loss2: (0.0000) | Acc: (88.00%) (21548/24448)
Epoch: 58 | Batch_idx: 200 |  Loss: (0.3487) |  Loss2: (0.0000) | Acc: (88.00%) (22685/25728)
Epoch: 58 | Batch_idx: 210 |  Loss: (0.3495) |  Loss2: (0.0000) | Acc: (88.00%) (23794/27008)
Epoch: 58 | Batch_idx: 220 |  Loss: (0.3484) |  Loss2: (0.0000) | Acc: (88.00%) (24938/28288)
Epoch: 58 | Batch_idx: 230 |  Loss: (0.3493) |  Loss2: (0.0000) | Acc: (88.00%) (26060/29568)
Epoch: 58 | Batch_idx: 240 |  Loss: (0.3489) |  Loss2: (0.0000) | Acc: (88.00%) (27199/30848)
Epoch: 58 | Batch_idx: 250 |  Loss: (0.3499) |  Loss2: (0.0000) | Acc: (88.00%) (28323/32128)
Epoch: 58 | Batch_idx: 260 |  Loss: (0.3500) |  Loss2: (0.0000) | Acc: (88.00%) (29445/33408)
Epoch: 58 | Batch_idx: 270 |  Loss: (0.3490) |  Loss2: (0.0000) | Acc: (88.00%) (30584/34688)
Epoch: 58 | Batch_idx: 280 |  Loss: (0.3485) |  Loss2: (0.0000) | Acc: (88.00%) (31704/35968)
Epoch: 58 | Batch_idx: 290 |  Loss: (0.3482) |  Loss2: (0.0000) | Acc: (88.00%) (32836/37248)
Epoch: 58 | Batch_idx: 300 |  Loss: (0.3489) |  Loss2: (0.0000) | Acc: (88.00%) (33948/38528)
Epoch: 58 | Batch_idx: 310 |  Loss: (0.3505) |  Loss2: (0.0000) | Acc: (88.00%) (35050/39808)
Epoch: 58 | Batch_idx: 320 |  Loss: (0.3497) |  Loss2: (0.0000) | Acc: (88.00%) (36184/41088)
Epoch: 58 | Batch_idx: 330 |  Loss: (0.3494) |  Loss2: (0.0000) | Acc: (88.00%) (37305/42368)
Epoch: 58 | Batch_idx: 340 |  Loss: (0.3500) |  Loss2: (0.0000) | Acc: (88.00%) (38420/43648)
Epoch: 58 | Batch_idx: 350 |  Loss: (0.3508) |  Loss2: (0.0000) | Acc: (87.00%) (39534/44928)
Epoch: 58 | Batch_idx: 360 |  Loss: (0.3515) |  Loss2: (0.0000) | Acc: (87.00%) (40652/46208)
Epoch: 58 | Batch_idx: 370 |  Loss: (0.3511) |  Loss2: (0.0000) | Acc: (87.00%) (41783/47488)
Epoch: 58 | Batch_idx: 380 |  Loss: (0.3513) |  Loss2: (0.0000) | Acc: (87.00%) (42910/48768)
Epoch: 58 | Batch_idx: 390 |  Loss: (0.3513) |  Loss2: (0.0000) | Acc: (87.00%) (43992/50000)
# TEST : Loss: (0.4609) | Acc: (84.00%) (8474/10000)
percent tensor([0.5210, 0.5312, 0.5033, 0.5073, 0.5084, 0.5093, 0.5298, 0.5170, 0.5300,
        0.5266, 0.5312, 0.5121, 0.5256, 0.5327, 0.5196, 0.5197],
       device='cuda:0') torch.Size([16])
percent tensor([0.4751, 0.4750, 0.4518, 0.4742, 0.4575, 0.4633, 0.4690, 0.4776, 0.4725,
        0.4659, 0.4659, 0.4530, 0.4796, 0.4903, 0.4696, 0.4735],
       device='cuda:0') torch.Size([16])
percent tensor([0.4690, 0.4562, 0.4092, 0.4597, 0.4047, 0.4818, 0.4477, 0.4460, 0.4584,
        0.4350, 0.4597, 0.4139, 0.4519, 0.5061, 0.4601, 0.4721],
       device='cuda:0') torch.Size([16])
percent tensor([0.5904, 0.5860, 0.5876, 0.5855, 0.5841, 0.5989, 0.5887, 0.5804, 0.5884,
        0.5873, 0.5850, 0.5974, 0.5977, 0.5809, 0.5973, 0.5906],
       device='cuda:0') torch.Size([16])
percent tensor([0.6652, 0.6440, 0.6910, 0.7056, 0.6932, 0.7043, 0.6614, 0.6930, 0.7134,
        0.6585, 0.6747, 0.7170, 0.6479, 0.7248, 0.6526, 0.6695],
       device='cuda:0') torch.Size([16])
percent tensor([0.7032, 0.6967, 0.7208, 0.7424, 0.7544, 0.7533, 0.7260, 0.7385, 0.7056,
        0.6929, 0.6952, 0.7117, 0.6810, 0.7305, 0.7133, 0.7311],
       device='cuda:0') torch.Size([16])
percent tensor([0.5580, 0.5539, 0.6010, 0.5989, 0.6494, 0.7512, 0.5740, 0.4704, 0.5956,
        0.5564, 0.5807, 0.5090, 0.5380, 0.6258, 0.4074, 0.6115],
       device='cuda:0') torch.Size([16])
percent tensor([0.9995, 0.9981, 0.9981, 0.9973, 0.9988, 0.9960, 0.9985, 0.9996, 0.9987,
        0.9992, 0.9991, 0.9986, 0.9985, 0.9988, 0.9983, 0.9993],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 59 | Batch_idx: 0 |  Loss: (0.3647) |  Loss2: (0.0000) | Acc: (85.00%) (110/128)
Epoch: 59 | Batch_idx: 10 |  Loss: (0.3507) |  Loss2: (0.0000) | Acc: (87.00%) (1228/1408)
Epoch: 59 | Batch_idx: 20 |  Loss: (0.3541) |  Loss2: (0.0000) | Acc: (87.00%) (2358/2688)
Epoch: 59 | Batch_idx: 30 |  Loss: (0.3784) |  Loss2: (0.0000) | Acc: (86.00%) (3448/3968)
Epoch: 59 | Batch_idx: 40 |  Loss: (0.3827) |  Loss2: (0.0000) | Acc: (86.00%) (4549/5248)
Epoch: 59 | Batch_idx: 50 |  Loss: (0.3856) |  Loss2: (0.0000) | Acc: (86.00%) (5648/6528)
Epoch: 59 | Batch_idx: 60 |  Loss: (0.3911) |  Loss2: (0.0000) | Acc: (86.00%) (6742/7808)
Epoch: 59 | Batch_idx: 70 |  Loss: (0.3939) |  Loss2: (0.0000) | Acc: (86.00%) (7846/9088)
Epoch: 59 | Batch_idx: 80 |  Loss: (0.3939) |  Loss2: (0.0000) | Acc: (86.00%) (8952/10368)
Epoch: 59 | Batch_idx: 90 |  Loss: (0.3934) |  Loss2: (0.0000) | Acc: (86.00%) (10067/11648)
Epoch: 59 | Batch_idx: 100 |  Loss: (0.3970) |  Loss2: (0.0000) | Acc: (86.00%) (11159/12928)
Epoch: 59 | Batch_idx: 110 |  Loss: (0.3958) |  Loss2: (0.0000) | Acc: (86.00%) (12262/14208)
Epoch: 59 | Batch_idx: 120 |  Loss: (0.3978) |  Loss2: (0.0000) | Acc: (86.00%) (13363/15488)
Epoch: 59 | Batch_idx: 130 |  Loss: (0.3973) |  Loss2: (0.0000) | Acc: (86.00%) (14473/16768)
Epoch: 59 | Batch_idx: 140 |  Loss: (0.3973) |  Loss2: (0.0000) | Acc: (86.00%) (15568/18048)
Epoch: 59 | Batch_idx: 150 |  Loss: (0.3974) |  Loss2: (0.0000) | Acc: (86.00%) (16668/19328)
Epoch: 59 | Batch_idx: 160 |  Loss: (0.3981) |  Loss2: (0.0000) | Acc: (86.00%) (17784/20608)
Epoch: 59 | Batch_idx: 170 |  Loss: (0.3968) |  Loss2: (0.0000) | Acc: (86.00%) (18896/21888)
Epoch: 59 | Batch_idx: 180 |  Loss: (0.3948) |  Loss2: (0.0000) | Acc: (86.00%) (20015/23168)
Epoch: 59 | Batch_idx: 190 |  Loss: (0.3935) |  Loss2: (0.0000) | Acc: (86.00%) (21122/24448)
Epoch: 59 | Batch_idx: 200 |  Loss: (0.3902) |  Loss2: (0.0000) | Acc: (86.00%) (22256/25728)
Epoch: 59 | Batch_idx: 210 |  Loss: (0.3907) |  Loss2: (0.0000) | Acc: (86.00%) (23360/27008)
Epoch: 59 | Batch_idx: 220 |  Loss: (0.3908) |  Loss2: (0.0000) | Acc: (86.00%) (24456/28288)
Epoch: 59 | Batch_idx: 230 |  Loss: (0.3898) |  Loss2: (0.0000) | Acc: (86.00%) (25565/29568)
Epoch: 59 | Batch_idx: 240 |  Loss: (0.3901) |  Loss2: (0.0000) | Acc: (86.00%) (26669/30848)
Epoch: 59 | Batch_idx: 250 |  Loss: (0.3894) |  Loss2: (0.0000) | Acc: (86.00%) (27780/32128)
Epoch: 59 | Batch_idx: 260 |  Loss: (0.3888) |  Loss2: (0.0000) | Acc: (86.00%) (28887/33408)
Epoch: 59 | Batch_idx: 270 |  Loss: (0.3890) |  Loss2: (0.0000) | Acc: (86.00%) (30002/34688)
Epoch: 59 | Batch_idx: 280 |  Loss: (0.3892) |  Loss2: (0.0000) | Acc: (86.00%) (31106/35968)
Epoch: 59 | Batch_idx: 290 |  Loss: (0.3884) |  Loss2: (0.0000) | Acc: (86.00%) (32220/37248)
Epoch: 59 | Batch_idx: 300 |  Loss: (0.3886) |  Loss2: (0.0000) | Acc: (86.00%) (33317/38528)
Epoch: 59 | Batch_idx: 310 |  Loss: (0.3883) |  Loss2: (0.0000) | Acc: (86.00%) (34442/39808)
Epoch: 59 | Batch_idx: 320 |  Loss: (0.3889) |  Loss2: (0.0000) | Acc: (86.00%) (35533/41088)
Epoch: 59 | Batch_idx: 330 |  Loss: (0.3877) |  Loss2: (0.0000) | Acc: (86.00%) (36653/42368)
Epoch: 59 | Batch_idx: 340 |  Loss: (0.3875) |  Loss2: (0.0000) | Acc: (86.00%) (37772/43648)
Epoch: 59 | Batch_idx: 350 |  Loss: (0.3861) |  Loss2: (0.0000) | Acc: (86.00%) (38892/44928)
Epoch: 59 | Batch_idx: 360 |  Loss: (0.3862) |  Loss2: (0.0000) | Acc: (86.00%) (39996/46208)
Epoch: 59 | Batch_idx: 370 |  Loss: (0.3858) |  Loss2: (0.0000) | Acc: (86.00%) (41110/47488)
Epoch: 59 | Batch_idx: 380 |  Loss: (0.3859) |  Loss2: (0.0000) | Acc: (86.00%) (42241/48768)
Epoch: 59 | Batch_idx: 390 |  Loss: (0.3860) |  Loss2: (0.0000) | Acc: (86.00%) (43313/50000)
# TEST : Loss: (0.4669) | Acc: (84.00%) (8430/10000)
percent tensor([0.5183, 0.5297, 0.4979, 0.5051, 0.5026, 0.5067, 0.5270, 0.5137, 0.5277,
        0.5241, 0.5289, 0.5061, 0.5234, 0.5320, 0.5171, 0.5172],
       device='cuda:0') torch.Size([16])
percent tensor([0.4717, 0.4720, 0.4494, 0.4719, 0.4568, 0.4627, 0.4662, 0.4760, 0.4690,
        0.4615, 0.4619, 0.4498, 0.4755, 0.4864, 0.4680, 0.4709],
       device='cuda:0') torch.Size([16])
percent tensor([0.4807, 0.4657, 0.4059, 0.4655, 0.4040, 0.4969, 0.4538, 0.4468, 0.4638,
        0.4396, 0.4714, 0.4131, 0.4597, 0.5125, 0.4729, 0.4830],
       device='cuda:0') torch.Size([16])
percent tensor([0.6097, 0.6041, 0.6028, 0.6001, 0.6003, 0.6214, 0.6087, 0.5966, 0.6056,
        0.6053, 0.6041, 0.6167, 0.6184, 0.5982, 0.6192, 0.6110],
       device='cuda:0') torch.Size([16])
percent tensor([0.6232, 0.5943, 0.6410, 0.6525, 0.6413, 0.6792, 0.6057, 0.6314, 0.6739,
        0.6132, 0.6386, 0.6762, 0.6127, 0.6748, 0.5970, 0.6279],
       device='cuda:0') torch.Size([16])
percent tensor([0.6716, 0.6628, 0.6889, 0.7080, 0.7236, 0.7279, 0.6944, 0.7004, 0.6788,
        0.6592, 0.6657, 0.6737, 0.6460, 0.6982, 0.6763, 0.6997],
       device='cuda:0') torch.Size([16])
percent tensor([0.5073, 0.4991, 0.5758, 0.5802, 0.6222, 0.7392, 0.5218, 0.4433, 0.5822,
        0.5103, 0.5457, 0.4876, 0.4938, 0.5847, 0.3635, 0.5649],
       device='cuda:0') torch.Size([16])
percent tensor([0.9993, 0.9983, 0.9986, 0.9982, 0.9992, 0.9969, 0.9984, 0.9997, 0.9988,
        0.9992, 0.9991, 0.9989, 0.9989, 0.9986, 0.9987, 0.9993],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 60 | Batch_idx: 0 |  Loss: (0.3032) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 60 | Batch_idx: 10 |  Loss: (0.3366) |  Loss2: (0.0000) | Acc: (88.00%) (1250/1408)
Epoch: 60 | Batch_idx: 20 |  Loss: (0.3222) |  Loss2: (0.0000) | Acc: (89.00%) (2396/2688)
Epoch: 60 | Batch_idx: 30 |  Loss: (0.3223) |  Loss2: (0.0000) | Acc: (89.00%) (3540/3968)
Epoch: 60 | Batch_idx: 40 |  Loss: (0.3225) |  Loss2: (0.0000) | Acc: (89.00%) (4688/5248)
Epoch: 60 | Batch_idx: 50 |  Loss: (0.3191) |  Loss2: (0.0000) | Acc: (89.00%) (5832/6528)
Epoch: 60 | Batch_idx: 60 |  Loss: (0.3202) |  Loss2: (0.0000) | Acc: (89.00%) (6975/7808)
Epoch: 60 | Batch_idx: 70 |  Loss: (0.3233) |  Loss2: (0.0000) | Acc: (89.00%) (8110/9088)
Epoch: 60 | Batch_idx: 80 |  Loss: (0.3246) |  Loss2: (0.0000) | Acc: (89.00%) (9241/10368)
Epoch: 60 | Batch_idx: 90 |  Loss: (0.3259) |  Loss2: (0.0000) | Acc: (89.00%) (10368/11648)
Epoch: 60 | Batch_idx: 100 |  Loss: (0.3268) |  Loss2: (0.0000) | Acc: (88.00%) (11501/12928)
Epoch: 60 | Batch_idx: 110 |  Loss: (0.3311) |  Loss2: (0.0000) | Acc: (88.00%) (12611/14208)
Epoch: 60 | Batch_idx: 120 |  Loss: (0.3323) |  Loss2: (0.0000) | Acc: (88.00%) (13738/15488)
Epoch: 60 | Batch_idx: 130 |  Loss: (0.3313) |  Loss2: (0.0000) | Acc: (88.00%) (14878/16768)
Epoch: 60 | Batch_idx: 140 |  Loss: (0.3310) |  Loss2: (0.0000) | Acc: (88.00%) (16005/18048)
Epoch: 60 | Batch_idx: 150 |  Loss: (0.3328) |  Loss2: (0.0000) | Acc: (88.00%) (17121/19328)
Epoch: 60 | Batch_idx: 160 |  Loss: (0.3347) |  Loss2: (0.0000) | Acc: (88.00%) (18237/20608)
Epoch: 60 | Batch_idx: 170 |  Loss: (0.3363) |  Loss2: (0.0000) | Acc: (88.00%) (19364/21888)
Epoch: 60 | Batch_idx: 180 |  Loss: (0.3375) |  Loss2: (0.0000) | Acc: (88.00%) (20493/23168)
Epoch: 60 | Batch_idx: 190 |  Loss: (0.3369) |  Loss2: (0.0000) | Acc: (88.00%) (21633/24448)
Epoch: 60 | Batch_idx: 200 |  Loss: (0.3357) |  Loss2: (0.0000) | Acc: (88.00%) (22767/25728)
Epoch: 60 | Batch_idx: 210 |  Loss: (0.3355) |  Loss2: (0.0000) | Acc: (88.00%) (23907/27008)
Epoch: 60 | Batch_idx: 220 |  Loss: (0.3360) |  Loss2: (0.0000) | Acc: (88.00%) (25037/28288)
Epoch: 60 | Batch_idx: 230 |  Loss: (0.3371) |  Loss2: (0.0000) | Acc: (88.00%) (26161/29568)
Epoch: 60 | Batch_idx: 240 |  Loss: (0.3373) |  Loss2: (0.0000) | Acc: (88.00%) (27303/30848)
Epoch: 60 | Batch_idx: 250 |  Loss: (0.3381) |  Loss2: (0.0000) | Acc: (88.00%) (28426/32128)
Epoch: 60 | Batch_idx: 260 |  Loss: (0.3388) |  Loss2: (0.0000) | Acc: (88.00%) (29558/33408)
Epoch: 60 | Batch_idx: 270 |  Loss: (0.3386) |  Loss2: (0.0000) | Acc: (88.00%) (30683/34688)
Epoch: 60 | Batch_idx: 280 |  Loss: (0.3380) |  Loss2: (0.0000) | Acc: (88.00%) (31827/35968)
Epoch: 60 | Batch_idx: 290 |  Loss: (0.3383) |  Loss2: (0.0000) | Acc: (88.00%) (32947/37248)
Epoch: 60 | Batch_idx: 300 |  Loss: (0.3381) |  Loss2: (0.0000) | Acc: (88.00%) (34084/38528)
Epoch: 60 | Batch_idx: 310 |  Loss: (0.3381) |  Loss2: (0.0000) | Acc: (88.00%) (35202/39808)
Epoch: 60 | Batch_idx: 320 |  Loss: (0.3379) |  Loss2: (0.0000) | Acc: (88.00%) (36331/41088)
Epoch: 60 | Batch_idx: 330 |  Loss: (0.3377) |  Loss2: (0.0000) | Acc: (88.00%) (37465/42368)
Epoch: 60 | Batch_idx: 340 |  Loss: (0.3379) |  Loss2: (0.0000) | Acc: (88.00%) (38595/43648)
Epoch: 60 | Batch_idx: 350 |  Loss: (0.3375) |  Loss2: (0.0000) | Acc: (88.00%) (39725/44928)
Epoch: 60 | Batch_idx: 360 |  Loss: (0.3374) |  Loss2: (0.0000) | Acc: (88.00%) (40855/46208)
Epoch: 60 | Batch_idx: 370 |  Loss: (0.3368) |  Loss2: (0.0000) | Acc: (88.00%) (41994/47488)
Epoch: 60 | Batch_idx: 380 |  Loss: (0.3374) |  Loss2: (0.0000) | Acc: (88.00%) (43113/48768)
Epoch: 60 | Batch_idx: 390 |  Loss: (0.3373) |  Loss2: (0.0000) | Acc: (88.00%) (44213/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_060.pth.tar'
# TEST : Loss: (0.5008) | Acc: (83.00%) (8342/10000)
percent tensor([0.5186, 0.5301, 0.4966, 0.5046, 0.5022, 0.5066, 0.5276, 0.5137, 0.5277,
        0.5244, 0.5292, 0.5053, 0.5240, 0.5327, 0.5179, 0.5175],
       device='cuda:0') torch.Size([16])
percent tensor([0.4737, 0.4710, 0.4495, 0.4701, 0.4560, 0.4601, 0.4662, 0.4753, 0.4713,
        0.4633, 0.4640, 0.4508, 0.4782, 0.4839, 0.4667, 0.4701],
       device='cuda:0') torch.Size([16])
percent tensor([0.4854, 0.4687, 0.4145, 0.4709, 0.4107, 0.4955, 0.4583, 0.4567, 0.4720,
        0.4453, 0.4771, 0.4182, 0.4644, 0.5135, 0.4785, 0.4873],
       device='cuda:0') torch.Size([16])
percent tensor([0.6099, 0.6009, 0.6014, 0.5993, 0.5986, 0.6215, 0.6062, 0.5950, 0.6042,
        0.6030, 0.6017, 0.6126, 0.6178, 0.5941, 0.6158, 0.6099],
       device='cuda:0') torch.Size([16])
percent tensor([0.6199, 0.5855, 0.6440, 0.6410, 0.6403, 0.6708, 0.5936, 0.6247, 0.6549,
        0.6036, 0.6259, 0.6523, 0.5978, 0.6548, 0.5893, 0.6256],
       device='cuda:0') torch.Size([16])
percent tensor([0.6732, 0.6689, 0.6972, 0.7152, 0.7318, 0.7185, 0.7012, 0.7089, 0.6726,
        0.6671, 0.6678, 0.6818, 0.6431, 0.7028, 0.6765, 0.7026],
       device='cuda:0') torch.Size([16])
percent tensor([0.5059, 0.5370, 0.5969, 0.5489, 0.6384, 0.7200, 0.5348, 0.4705, 0.5576,
        0.5381, 0.5383, 0.4925, 0.4770, 0.6013, 0.3833, 0.5812],
       device='cuda:0') torch.Size([16])
percent tensor([0.9995, 0.9977, 0.9982, 0.9985, 0.9990, 0.9961, 0.9988, 0.9996, 0.9991,
        0.9988, 0.9991, 0.9989, 0.9987, 0.9980, 0.9988, 0.9993],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(174.2300, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(802.1989, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(792.1155, device='cuda:0')
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1521.5560, device='cuda:0')
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(501.7035, device='cuda:0')
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2202.0452, device='cuda:0')
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4305.1479, device='cuda:0')
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1419.7423, device='cuda:0')
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6124.3203, device='cuda:0')
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12006.4375, device='cuda:0')
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3996.3984, device='cuda:0')
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16896.9941, device='cuda:0')
Epoch: 61 | Batch_idx: 0 |  Loss: (0.3440) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 61 | Batch_idx: 10 |  Loss: (0.3745) |  Loss2: (0.0000) | Acc: (87.00%) (1232/1408)
Epoch: 61 | Batch_idx: 20 |  Loss: (0.4011) |  Loss2: (0.0000) | Acc: (86.00%) (2320/2688)
Epoch: 61 | Batch_idx: 30 |  Loss: (0.4045) |  Loss2: (0.0000) | Acc: (86.00%) (3433/3968)
Epoch: 61 | Batch_idx: 40 |  Loss: (0.4160) |  Loss2: (0.0000) | Acc: (86.00%) (4515/5248)
Epoch: 61 | Batch_idx: 50 |  Loss: (0.4190) |  Loss2: (0.0000) | Acc: (85.00%) (5594/6528)
Epoch: 61 | Batch_idx: 60 |  Loss: (0.4317) |  Loss2: (0.0000) | Acc: (85.00%) (6646/7808)
Epoch: 61 | Batch_idx: 70 |  Loss: (0.4338) |  Loss2: (0.0000) | Acc: (85.00%) (7729/9088)
Epoch: 61 | Batch_idx: 80 |  Loss: (0.4327) |  Loss2: (0.0000) | Acc: (85.00%) (8825/10368)
Epoch: 61 | Batch_idx: 90 |  Loss: (0.4383) |  Loss2: (0.0000) | Acc: (84.00%) (9893/11648)
Epoch: 61 | Batch_idx: 100 |  Loss: (0.4393) |  Loss2: (0.0000) | Acc: (84.00%) (10963/12928)
Epoch: 61 | Batch_idx: 110 |  Loss: (0.4370) |  Loss2: (0.0000) | Acc: (84.00%) (12059/14208)
Epoch: 61 | Batch_idx: 120 |  Loss: (0.4333) |  Loss2: (0.0000) | Acc: (84.00%) (13162/15488)
Epoch: 61 | Batch_idx: 130 |  Loss: (0.4318) |  Loss2: (0.0000) | Acc: (85.00%) (14268/16768)
Epoch: 61 | Batch_idx: 140 |  Loss: (0.4308) |  Loss2: (0.0000) | Acc: (85.00%) (15350/18048)
Epoch: 61 | Batch_idx: 150 |  Loss: (0.4304) |  Loss2: (0.0000) | Acc: (85.00%) (16456/19328)
Epoch: 61 | Batch_idx: 160 |  Loss: (0.4317) |  Loss2: (0.0000) | Acc: (85.00%) (17524/20608)
Epoch: 61 | Batch_idx: 170 |  Loss: (0.4300) |  Loss2: (0.0000) | Acc: (85.00%) (18620/21888)
Epoch: 61 | Batch_idx: 180 |  Loss: (0.4300) |  Loss2: (0.0000) | Acc: (85.00%) (19719/23168)
Epoch: 61 | Batch_idx: 190 |  Loss: (0.4274) |  Loss2: (0.0000) | Acc: (85.00%) (20845/24448)
Epoch: 61 | Batch_idx: 200 |  Loss: (0.4242) |  Loss2: (0.0000) | Acc: (85.00%) (21970/25728)
Epoch: 61 | Batch_idx: 210 |  Loss: (0.4231) |  Loss2: (0.0000) | Acc: (85.00%) (23062/27008)
Epoch: 61 | Batch_idx: 220 |  Loss: (0.4216) |  Loss2: (0.0000) | Acc: (85.00%) (24164/28288)
Epoch: 61 | Batch_idx: 230 |  Loss: (0.4218) |  Loss2: (0.0000) | Acc: (85.00%) (25253/29568)
Epoch: 61 | Batch_idx: 240 |  Loss: (0.4208) |  Loss2: (0.0000) | Acc: (85.00%) (26360/30848)
Epoch: 61 | Batch_idx: 250 |  Loss: (0.4206) |  Loss2: (0.0000) | Acc: (85.00%) (27460/32128)
Epoch: 61 | Batch_idx: 260 |  Loss: (0.4182) |  Loss2: (0.0000) | Acc: (85.00%) (28590/33408)
Epoch: 61 | Batch_idx: 270 |  Loss: (0.4166) |  Loss2: (0.0000) | Acc: (85.00%) (29711/34688)
Epoch: 61 | Batch_idx: 280 |  Loss: (0.4152) |  Loss2: (0.0000) | Acc: (85.00%) (30829/35968)
Epoch: 61 | Batch_idx: 290 |  Loss: (0.4142) |  Loss2: (0.0000) | Acc: (85.00%) (31938/37248)
Epoch: 61 | Batch_idx: 300 |  Loss: (0.4130) |  Loss2: (0.0000) | Acc: (85.00%) (33046/38528)
Epoch: 61 | Batch_idx: 310 |  Loss: (0.4111) |  Loss2: (0.0000) | Acc: (85.00%) (34171/39808)
Epoch: 61 | Batch_idx: 320 |  Loss: (0.4095) |  Loss2: (0.0000) | Acc: (85.00%) (35295/41088)
Epoch: 61 | Batch_idx: 330 |  Loss: (0.4078) |  Loss2: (0.0000) | Acc: (85.00%) (36416/42368)
Epoch: 61 | Batch_idx: 340 |  Loss: (0.4068) |  Loss2: (0.0000) | Acc: (86.00%) (37549/43648)
Epoch: 61 | Batch_idx: 350 |  Loss: (0.4072) |  Loss2: (0.0000) | Acc: (85.00%) (38636/44928)
Epoch: 61 | Batch_idx: 360 |  Loss: (0.4063) |  Loss2: (0.0000) | Acc: (86.00%) (39743/46208)
Epoch: 61 | Batch_idx: 370 |  Loss: (0.4057) |  Loss2: (0.0000) | Acc: (86.00%) (40850/47488)
Epoch: 61 | Batch_idx: 380 |  Loss: (0.4050) |  Loss2: (0.0000) | Acc: (86.00%) (41963/48768)
Epoch: 61 | Batch_idx: 390 |  Loss: (0.4041) |  Loss2: (0.0000) | Acc: (86.00%) (43043/50000)
# TEST : Loss: (0.5004) | Acc: (82.00%) (8298/10000)
percent tensor([0.5282, 0.5407, 0.5105, 0.5161, 0.5152, 0.5178, 0.5382, 0.5284, 0.5371,
        0.5344, 0.5383, 0.5191, 0.5326, 0.5428, 0.5306, 0.5282],
       device='cuda:0') torch.Size([16])
percent tensor([0.4824, 0.4833, 0.4599, 0.4754, 0.4683, 0.4694, 0.4787, 0.4849, 0.4800,
        0.4735, 0.4732, 0.4607, 0.4867, 0.4904, 0.4774, 0.4787],
       device='cuda:0') torch.Size([16])
percent tensor([0.4633, 0.4501, 0.3941, 0.4542, 0.3950, 0.4837, 0.4373, 0.4370, 0.4473,
        0.4217, 0.4524, 0.3940, 0.4396, 0.5042, 0.4603, 0.4662],
       device='cuda:0') torch.Size([16])
percent tensor([0.6042, 0.5985, 0.5993, 0.5935, 0.5969, 0.6065, 0.6041, 0.5944, 0.6017,
        0.6013, 0.5983, 0.6115, 0.6158, 0.5904, 0.6089, 0.6023],
       device='cuda:0') torch.Size([16])
percent tensor([0.6635, 0.6341, 0.6737, 0.6561, 0.6727, 0.7094, 0.6393, 0.6641, 0.6768,
        0.6473, 0.6532, 0.6829, 0.6552, 0.6778, 0.6370, 0.6737],
       device='cuda:0') torch.Size([16])
percent tensor([0.7225, 0.7144, 0.7458, 0.7638, 0.7805, 0.7536, 0.7521, 0.7654, 0.7206,
        0.7207, 0.7175, 0.7374, 0.6958, 0.7492, 0.7345, 0.7441],
       device='cuda:0') torch.Size([16])
percent tensor([0.4293, 0.4531, 0.5502, 0.5201, 0.5927, 0.6563, 0.4822, 0.4366, 0.4890,
        0.4830, 0.4669, 0.5009, 0.4068, 0.5379, 0.3529, 0.4886],
       device='cuda:0') torch.Size([16])
percent tensor([0.9993, 0.9980, 0.9977, 0.9979, 0.9990, 0.9965, 0.9990, 0.9994, 0.9989,
        0.9985, 0.9991, 0.9988, 0.9986, 0.9981, 0.9990, 0.9988],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 62 | Batch_idx: 0 |  Loss: (0.3410) |  Loss2: (0.0000) | Acc: (87.00%) (112/128)
Epoch: 62 | Batch_idx: 10 |  Loss: (0.3399) |  Loss2: (0.0000) | Acc: (88.00%) (1248/1408)
Epoch: 62 | Batch_idx: 20 |  Loss: (0.3217) |  Loss2: (0.0000) | Acc: (89.00%) (2414/2688)
Epoch: 62 | Batch_idx: 30 |  Loss: (0.3334) |  Loss2: (0.0000) | Acc: (88.00%) (3531/3968)
Epoch: 62 | Batch_idx: 40 |  Loss: (0.3307) |  Loss2: (0.0000) | Acc: (89.00%) (4672/5248)
Epoch: 62 | Batch_idx: 50 |  Loss: (0.3366) |  Loss2: (0.0000) | Acc: (88.00%) (5795/6528)
Epoch: 62 | Batch_idx: 60 |  Loss: (0.3369) |  Loss2: (0.0000) | Acc: (88.00%) (6925/7808)
Epoch: 62 | Batch_idx: 70 |  Loss: (0.3408) |  Loss2: (0.0000) | Acc: (88.00%) (8028/9088)
Epoch: 62 | Batch_idx: 80 |  Loss: (0.3435) |  Loss2: (0.0000) | Acc: (88.00%) (9141/10368)
Epoch: 62 | Batch_idx: 90 |  Loss: (0.3457) |  Loss2: (0.0000) | Acc: (88.00%) (10261/11648)
Epoch: 62 | Batch_idx: 100 |  Loss: (0.3453) |  Loss2: (0.0000) | Acc: (88.00%) (11396/12928)
Epoch: 62 | Batch_idx: 110 |  Loss: (0.3429) |  Loss2: (0.0000) | Acc: (88.00%) (12544/14208)
Epoch: 62 | Batch_idx: 120 |  Loss: (0.3446) |  Loss2: (0.0000) | Acc: (88.00%) (13657/15488)
Epoch: 62 | Batch_idx: 130 |  Loss: (0.3452) |  Loss2: (0.0000) | Acc: (88.00%) (14781/16768)
Epoch: 62 | Batch_idx: 140 |  Loss: (0.3455) |  Loss2: (0.0000) | Acc: (88.00%) (15914/18048)
Epoch: 62 | Batch_idx: 150 |  Loss: (0.3448) |  Loss2: (0.0000) | Acc: (88.00%) (17042/19328)
Epoch: 62 | Batch_idx: 160 |  Loss: (0.3453) |  Loss2: (0.0000) | Acc: (88.00%) (18171/20608)
Epoch: 62 | Batch_idx: 170 |  Loss: (0.3434) |  Loss2: (0.0000) | Acc: (88.00%) (19311/21888)
Epoch: 62 | Batch_idx: 180 |  Loss: (0.3437) |  Loss2: (0.0000) | Acc: (88.00%) (20433/23168)
Epoch: 62 | Batch_idx: 190 |  Loss: (0.3415) |  Loss2: (0.0000) | Acc: (88.00%) (21584/24448)
Epoch: 62 | Batch_idx: 200 |  Loss: (0.3400) |  Loss2: (0.0000) | Acc: (88.00%) (22728/25728)
Epoch: 62 | Batch_idx: 210 |  Loss: (0.3397) |  Loss2: (0.0000) | Acc: (88.00%) (23854/27008)
Epoch: 62 | Batch_idx: 220 |  Loss: (0.3395) |  Loss2: (0.0000) | Acc: (88.00%) (24982/28288)
Epoch: 62 | Batch_idx: 230 |  Loss: (0.3386) |  Loss2: (0.0000) | Acc: (88.00%) (26116/29568)
Epoch: 62 | Batch_idx: 240 |  Loss: (0.3388) |  Loss2: (0.0000) | Acc: (88.00%) (27247/30848)
Epoch: 62 | Batch_idx: 250 |  Loss: (0.3407) |  Loss2: (0.0000) | Acc: (88.00%) (28358/32128)
Epoch: 62 | Batch_idx: 260 |  Loss: (0.3401) |  Loss2: (0.0000) | Acc: (88.00%) (29496/33408)
Epoch: 62 | Batch_idx: 270 |  Loss: (0.3409) |  Loss2: (0.0000) | Acc: (88.00%) (30614/34688)
Epoch: 62 | Batch_idx: 280 |  Loss: (0.3405) |  Loss2: (0.0000) | Acc: (88.00%) (31758/35968)
Epoch: 62 | Batch_idx: 290 |  Loss: (0.3407) |  Loss2: (0.0000) | Acc: (88.00%) (32873/37248)
Epoch: 62 | Batch_idx: 300 |  Loss: (0.3398) |  Loss2: (0.0000) | Acc: (88.00%) (34014/38528)
Epoch: 62 | Batch_idx: 310 |  Loss: (0.3395) |  Loss2: (0.0000) | Acc: (88.00%) (35136/39808)
Epoch: 62 | Batch_idx: 320 |  Loss: (0.3390) |  Loss2: (0.0000) | Acc: (88.00%) (36278/41088)
Epoch: 62 | Batch_idx: 330 |  Loss: (0.3375) |  Loss2: (0.0000) | Acc: (88.00%) (37435/42368)
Epoch: 62 | Batch_idx: 340 |  Loss: (0.3376) |  Loss2: (0.0000) | Acc: (88.00%) (38569/43648)
Epoch: 62 | Batch_idx: 350 |  Loss: (0.3374) |  Loss2: (0.0000) | Acc: (88.00%) (39701/44928)
Epoch: 62 | Batch_idx: 360 |  Loss: (0.3372) |  Loss2: (0.0000) | Acc: (88.00%) (40845/46208)
Epoch: 62 | Batch_idx: 370 |  Loss: (0.3366) |  Loss2: (0.0000) | Acc: (88.00%) (41980/47488)
Epoch: 62 | Batch_idx: 380 |  Loss: (0.3358) |  Loss2: (0.0000) | Acc: (88.00%) (43122/48768)
Epoch: 62 | Batch_idx: 390 |  Loss: (0.3357) |  Loss2: (0.0000) | Acc: (88.00%) (44212/50000)
# TEST : Loss: (0.4377) | Acc: (85.00%) (8506/10000)
percent tensor([0.5299, 0.5407, 0.5121, 0.5197, 0.5188, 0.5198, 0.5384, 0.5290, 0.5372,
        0.5351, 0.5390, 0.5207, 0.5341, 0.5414, 0.5318, 0.5296],
       device='cuda:0') torch.Size([16])
percent tensor([0.4829, 0.4846, 0.4607, 0.4777, 0.4691, 0.4722, 0.4800, 0.4873, 0.4826,
        0.4751, 0.4746, 0.4626, 0.4864, 0.4939, 0.4800, 0.4801],
       device='cuda:0') torch.Size([16])
percent tensor([0.4679, 0.4551, 0.3926, 0.4638, 0.3941, 0.4870, 0.4465, 0.4437, 0.4547,
        0.4290, 0.4609, 0.3984, 0.4427, 0.5142, 0.4667, 0.4746],
       device='cuda:0') torch.Size([16])
percent tensor([0.6040, 0.6013, 0.5998, 0.5934, 0.5949, 0.6082, 0.6035, 0.5937, 0.6025,
        0.6040, 0.6027, 0.6091, 0.6155, 0.5942, 0.6101, 0.6031],
       device='cuda:0') torch.Size([16])
percent tensor([0.6432, 0.6229, 0.6424, 0.6412, 0.6448, 0.7127, 0.6206, 0.6482, 0.6783,
        0.6265, 0.6537, 0.6505, 0.6274, 0.6898, 0.6282, 0.6675],
       device='cuda:0') torch.Size([16])
percent tensor([0.7212, 0.7064, 0.7411, 0.7655, 0.7769, 0.7615, 0.7455, 0.7557, 0.7253,
        0.7115, 0.7147, 0.7360, 0.6965, 0.7425, 0.7355, 0.7451],
       device='cuda:0') torch.Size([16])
percent tensor([0.4120, 0.4379, 0.5477, 0.5208, 0.5718, 0.6517, 0.4730, 0.4298, 0.5039,
        0.4359, 0.4652, 0.4653, 0.4126, 0.4903, 0.3527, 0.4747],
       device='cuda:0') torch.Size([16])
percent tensor([0.9994, 0.9986, 0.9981, 0.9979, 0.9989, 0.9936, 0.9988, 0.9994, 0.9992,
        0.9992, 0.9994, 0.9990, 0.9991, 0.9989, 0.9986, 0.9994],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 63 | Batch_idx: 0 |  Loss: (0.3774) |  Loss2: (0.0000) | Acc: (85.00%) (110/128)
Epoch: 63 | Batch_idx: 10 |  Loss: (0.3180) |  Loss2: (0.0000) | Acc: (88.00%) (1249/1408)
Epoch: 63 | Batch_idx: 20 |  Loss: (0.3327) |  Loss2: (0.0000) | Acc: (87.00%) (2365/2688)
Epoch: 63 | Batch_idx: 30 |  Loss: (0.3396) |  Loss2: (0.0000) | Acc: (87.00%) (3485/3968)
Epoch: 63 | Batch_idx: 40 |  Loss: (0.3513) |  Loss2: (0.0000) | Acc: (87.00%) (4588/5248)
Epoch: 63 | Batch_idx: 50 |  Loss: (0.3610) |  Loss2: (0.0000) | Acc: (87.00%) (5691/6528)
Epoch: 63 | Batch_idx: 60 |  Loss: (0.3604) |  Loss2: (0.0000) | Acc: (87.00%) (6807/7808)
Epoch: 63 | Batch_idx: 70 |  Loss: (0.3617) |  Loss2: (0.0000) | Acc: (87.00%) (7920/9088)
Epoch: 63 | Batch_idx: 80 |  Loss: (0.3611) |  Loss2: (0.0000) | Acc: (87.00%) (9039/10368)
Epoch: 63 | Batch_idx: 90 |  Loss: (0.3601) |  Loss2: (0.0000) | Acc: (87.00%) (10165/11648)
Epoch: 63 | Batch_idx: 100 |  Loss: (0.3599) |  Loss2: (0.0000) | Acc: (87.00%) (11281/12928)
Epoch: 63 | Batch_idx: 110 |  Loss: (0.3634) |  Loss2: (0.0000) | Acc: (87.00%) (12385/14208)
Epoch: 63 | Batch_idx: 120 |  Loss: (0.3633) |  Loss2: (0.0000) | Acc: (87.00%) (13510/15488)
Epoch: 63 | Batch_idx: 130 |  Loss: (0.3615) |  Loss2: (0.0000) | Acc: (87.00%) (14627/16768)
Epoch: 63 | Batch_idx: 140 |  Loss: (0.3624) |  Loss2: (0.0000) | Acc: (87.00%) (15735/18048)
Epoch: 63 | Batch_idx: 150 |  Loss: (0.3629) |  Loss2: (0.0000) | Acc: (87.00%) (16850/19328)
Epoch: 63 | Batch_idx: 160 |  Loss: (0.3625) |  Loss2: (0.0000) | Acc: (87.00%) (17971/20608)
Epoch: 63 | Batch_idx: 170 |  Loss: (0.3607) |  Loss2: (0.0000) | Acc: (87.00%) (19107/21888)
Epoch: 63 | Batch_idx: 180 |  Loss: (0.3603) |  Loss2: (0.0000) | Acc: (87.00%) (20222/23168)
Epoch: 63 | Batch_idx: 190 |  Loss: (0.3606) |  Loss2: (0.0000) | Acc: (87.00%) (21343/24448)
Epoch: 63 | Batch_idx: 200 |  Loss: (0.3597) |  Loss2: (0.0000) | Acc: (87.00%) (22463/25728)
Epoch: 63 | Batch_idx: 210 |  Loss: (0.3607) |  Loss2: (0.0000) | Acc: (87.00%) (23563/27008)
Epoch: 63 | Batch_idx: 220 |  Loss: (0.3606) |  Loss2: (0.0000) | Acc: (87.00%) (24684/28288)
Epoch: 63 | Batch_idx: 230 |  Loss: (0.3597) |  Loss2: (0.0000) | Acc: (87.00%) (25821/29568)
Epoch: 63 | Batch_idx: 240 |  Loss: (0.3603) |  Loss2: (0.0000) | Acc: (87.00%) (26936/30848)
Epoch: 63 | Batch_idx: 250 |  Loss: (0.3585) |  Loss2: (0.0000) | Acc: (87.00%) (28086/32128)
Epoch: 63 | Batch_idx: 260 |  Loss: (0.3590) |  Loss2: (0.0000) | Acc: (87.00%) (29208/33408)
Epoch: 63 | Batch_idx: 270 |  Loss: (0.3598) |  Loss2: (0.0000) | Acc: (87.00%) (30303/34688)
Epoch: 63 | Batch_idx: 280 |  Loss: (0.3598) |  Loss2: (0.0000) | Acc: (87.00%) (31416/35968)
Epoch: 63 | Batch_idx: 290 |  Loss: (0.3587) |  Loss2: (0.0000) | Acc: (87.00%) (32553/37248)
Epoch: 63 | Batch_idx: 300 |  Loss: (0.3589) |  Loss2: (0.0000) | Acc: (87.00%) (33663/38528)
Epoch: 63 | Batch_idx: 310 |  Loss: (0.3582) |  Loss2: (0.0000) | Acc: (87.00%) (34792/39808)
Epoch: 63 | Batch_idx: 320 |  Loss: (0.3573) |  Loss2: (0.0000) | Acc: (87.00%) (35926/41088)
Epoch: 63 | Batch_idx: 330 |  Loss: (0.3570) |  Loss2: (0.0000) | Acc: (87.00%) (37050/42368)
Epoch: 63 | Batch_idx: 340 |  Loss: (0.3556) |  Loss2: (0.0000) | Acc: (87.00%) (38192/43648)
Epoch: 63 | Batch_idx: 350 |  Loss: (0.3563) |  Loss2: (0.0000) | Acc: (87.00%) (39297/44928)
Epoch: 63 | Batch_idx: 360 |  Loss: (0.3574) |  Loss2: (0.0000) | Acc: (87.00%) (40408/46208)
Epoch: 63 | Batch_idx: 370 |  Loss: (0.3568) |  Loss2: (0.0000) | Acc: (87.00%) (41548/47488)
Epoch: 63 | Batch_idx: 380 |  Loss: (0.3566) |  Loss2: (0.0000) | Acc: (87.00%) (42682/48768)
Epoch: 63 | Batch_idx: 390 |  Loss: (0.3561) |  Loss2: (0.0000) | Acc: (87.00%) (43767/50000)
# TEST : Loss: (0.4519) | Acc: (84.00%) (8465/10000)
percent tensor([0.5272, 0.5369, 0.5092, 0.5157, 0.5145, 0.5155, 0.5349, 0.5256, 0.5343,
        0.5322, 0.5360, 0.5182, 0.5313, 0.5377, 0.5282, 0.5267],
       device='cuda:0') torch.Size([16])
percent tensor([0.4775, 0.4791, 0.4556, 0.4733, 0.4629, 0.4656, 0.4744, 0.4823, 0.4776,
        0.4708, 0.4702, 0.4583, 0.4822, 0.4900, 0.4742, 0.4743],
       device='cuda:0') torch.Size([16])
percent tensor([0.4782, 0.4678, 0.3870, 0.4665, 0.3962, 0.5013, 0.4569, 0.4505, 0.4605,
        0.4338, 0.4706, 0.3959, 0.4536, 0.5224, 0.4833, 0.4851],
       device='cuda:0') torch.Size([16])
percent tensor([0.5693, 0.5678, 0.5690, 0.5613, 0.5629, 0.5699, 0.5693, 0.5609, 0.5667,
        0.5699, 0.5661, 0.5755, 0.5803, 0.5574, 0.5745, 0.5678],
       device='cuda:0') torch.Size([16])
percent tensor([0.6346, 0.6214, 0.6287, 0.6373, 0.6307, 0.6831, 0.6155, 0.6333, 0.6716,
        0.6272, 0.6520, 0.6522, 0.6276, 0.6905, 0.6202, 0.6533],
       device='cuda:0') torch.Size([16])
percent tensor([0.7409, 0.7311, 0.7579, 0.7794, 0.7966, 0.7823, 0.7667, 0.7788, 0.7461,
        0.7340, 0.7396, 0.7543, 0.7191, 0.7709, 0.7543, 0.7662],
       device='cuda:0') torch.Size([16])
percent tensor([0.4358, 0.4603, 0.5452, 0.5330, 0.5648, 0.6687, 0.4831, 0.4330, 0.5206,
        0.4727, 0.4935, 0.4724, 0.4343, 0.5291, 0.3593, 0.5019],
       device='cuda:0') torch.Size([16])
percent tensor([0.9993, 0.9985, 0.9981, 0.9983, 0.9989, 0.9941, 0.9988, 0.9995, 0.9990,
        0.9992, 0.9994, 0.9989, 0.9990, 0.9986, 0.9986, 0.9994],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 64 | Batch_idx: 0 |  Loss: (0.3617) |  Loss2: (0.0000) | Acc: (84.00%) (108/128)
Epoch: 64 | Batch_idx: 10 |  Loss: (0.3388) |  Loss2: (0.0000) | Acc: (87.00%) (1238/1408)
Epoch: 64 | Batch_idx: 20 |  Loss: (0.3276) |  Loss2: (0.0000) | Acc: (88.00%) (2391/2688)
Epoch: 64 | Batch_idx: 30 |  Loss: (0.3174) |  Loss2: (0.0000) | Acc: (89.00%) (3540/3968)
Epoch: 64 | Batch_idx: 40 |  Loss: (0.3170) |  Loss2: (0.0000) | Acc: (89.00%) (4672/5248)
Epoch: 64 | Batch_idx: 50 |  Loss: (0.3160) |  Loss2: (0.0000) | Acc: (89.00%) (5811/6528)
Epoch: 64 | Batch_idx: 60 |  Loss: (0.3154) |  Loss2: (0.0000) | Acc: (89.00%) (6960/7808)
Epoch: 64 | Batch_idx: 70 |  Loss: (0.3134) |  Loss2: (0.0000) | Acc: (89.00%) (8104/9088)
Epoch: 64 | Batch_idx: 80 |  Loss: (0.3119) |  Loss2: (0.0000) | Acc: (89.00%) (9265/10368)
Epoch: 64 | Batch_idx: 90 |  Loss: (0.3095) |  Loss2: (0.0000) | Acc: (89.00%) (10413/11648)
Epoch: 64 | Batch_idx: 100 |  Loss: (0.3122) |  Loss2: (0.0000) | Acc: (89.00%) (11543/12928)
Epoch: 64 | Batch_idx: 110 |  Loss: (0.3145) |  Loss2: (0.0000) | Acc: (89.00%) (12673/14208)
Epoch: 64 | Batch_idx: 120 |  Loss: (0.3140) |  Loss2: (0.0000) | Acc: (89.00%) (13825/15488)
Epoch: 64 | Batch_idx: 130 |  Loss: (0.3119) |  Loss2: (0.0000) | Acc: (89.00%) (14983/16768)
Epoch: 64 | Batch_idx: 140 |  Loss: (0.3128) |  Loss2: (0.0000) | Acc: (89.00%) (16123/18048)
Epoch: 64 | Batch_idx: 150 |  Loss: (0.3146) |  Loss2: (0.0000) | Acc: (89.00%) (17245/19328)
Epoch: 64 | Batch_idx: 160 |  Loss: (0.3176) |  Loss2: (0.0000) | Acc: (89.00%) (18363/20608)
Epoch: 64 | Batch_idx: 170 |  Loss: (0.3166) |  Loss2: (0.0000) | Acc: (89.00%) (19515/21888)
Epoch: 64 | Batch_idx: 180 |  Loss: (0.3185) |  Loss2: (0.0000) | Acc: (89.00%) (20641/23168)
Epoch: 64 | Batch_idx: 190 |  Loss: (0.3199) |  Loss2: (0.0000) | Acc: (89.00%) (21762/24448)
Epoch: 64 | Batch_idx: 200 |  Loss: (0.3171) |  Loss2: (0.0000) | Acc: (89.00%) (22923/25728)
Epoch: 64 | Batch_idx: 210 |  Loss: (0.3171) |  Loss2: (0.0000) | Acc: (89.00%) (24066/27008)
Epoch: 64 | Batch_idx: 220 |  Loss: (0.3161) |  Loss2: (0.0000) | Acc: (89.00%) (25218/28288)
Epoch: 64 | Batch_idx: 230 |  Loss: (0.3161) |  Loss2: (0.0000) | Acc: (89.00%) (26351/29568)
Epoch: 64 | Batch_idx: 240 |  Loss: (0.3172) |  Loss2: (0.0000) | Acc: (89.00%) (27473/30848)
Epoch: 64 | Batch_idx: 250 |  Loss: (0.3176) |  Loss2: (0.0000) | Acc: (89.00%) (28603/32128)
Epoch: 64 | Batch_idx: 260 |  Loss: (0.3179) |  Loss2: (0.0000) | Acc: (89.00%) (29736/33408)
Epoch: 64 | Batch_idx: 270 |  Loss: (0.3189) |  Loss2: (0.0000) | Acc: (88.00%) (30858/34688)
Epoch: 64 | Batch_idx: 280 |  Loss: (0.3190) |  Loss2: (0.0000) | Acc: (88.00%) (32004/35968)
Epoch: 64 | Batch_idx: 290 |  Loss: (0.3177) |  Loss2: (0.0000) | Acc: (89.00%) (33170/37248)
Epoch: 64 | Batch_idx: 300 |  Loss: (0.3178) |  Loss2: (0.0000) | Acc: (89.00%) (34312/38528)
Epoch: 64 | Batch_idx: 310 |  Loss: (0.3179) |  Loss2: (0.0000) | Acc: (89.00%) (35459/39808)
Epoch: 64 | Batch_idx: 320 |  Loss: (0.3175) |  Loss2: (0.0000) | Acc: (89.00%) (36612/41088)
Epoch: 64 | Batch_idx: 330 |  Loss: (0.3189) |  Loss2: (0.0000) | Acc: (89.00%) (37742/42368)
Epoch: 64 | Batch_idx: 340 |  Loss: (0.3197) |  Loss2: (0.0000) | Acc: (89.00%) (38871/43648)
Epoch: 64 | Batch_idx: 350 |  Loss: (0.3198) |  Loss2: (0.0000) | Acc: (89.00%) (40008/44928)
Epoch: 64 | Batch_idx: 360 |  Loss: (0.3199) |  Loss2: (0.0000) | Acc: (89.00%) (41145/46208)
Epoch: 64 | Batch_idx: 370 |  Loss: (0.3212) |  Loss2: (0.0000) | Acc: (89.00%) (42267/47488)
Epoch: 64 | Batch_idx: 380 |  Loss: (0.3215) |  Loss2: (0.0000) | Acc: (88.00%) (43402/48768)
Epoch: 64 | Batch_idx: 390 |  Loss: (0.3207) |  Loss2: (0.0000) | Acc: (89.00%) (44508/50000)
# TEST : Loss: (0.4660) | Acc: (84.00%) (8415/10000)
percent tensor([0.5276, 0.5361, 0.5164, 0.5176, 0.5203, 0.5165, 0.5356, 0.5281, 0.5354,
        0.5327, 0.5361, 0.5229, 0.5316, 0.5365, 0.5276, 0.5266],
       device='cuda:0') torch.Size([16])
percent tensor([0.4784, 0.4784, 0.4600, 0.4768, 0.4670, 0.4664, 0.4753, 0.4828, 0.4762,
        0.4710, 0.4680, 0.4613, 0.4827, 0.4890, 0.4745, 0.4752],
       device='cuda:0') torch.Size([16])
percent tensor([0.4778, 0.4636, 0.3955, 0.4683, 0.3986, 0.5082, 0.4543, 0.4547, 0.4633,
        0.4320, 0.4667, 0.3984, 0.4513, 0.5193, 0.4851, 0.4844],
       device='cuda:0') torch.Size([16])
percent tensor([0.5689, 0.5664, 0.5695, 0.5630, 0.5637, 0.5688, 0.5691, 0.5620, 0.5652,
        0.5677, 0.5630, 0.5767, 0.5788, 0.5546, 0.5747, 0.5678],
       device='cuda:0') torch.Size([16])
percent tensor([0.6320, 0.6116, 0.6625, 0.6566, 0.6622, 0.6846, 0.6199, 0.6493, 0.6633,
        0.6278, 0.6448, 0.6753, 0.6193, 0.6699, 0.6231, 0.6495],
       device='cuda:0') torch.Size([16])
percent tensor([0.7425, 0.7282, 0.7652, 0.7857, 0.8049, 0.7789, 0.7711, 0.7875, 0.7446,
        0.7298, 0.7317, 0.7524, 0.7093, 0.7640, 0.7552, 0.7643],
       device='cuda:0') torch.Size([16])
percent tensor([0.4348, 0.4550, 0.5583, 0.5376, 0.6118, 0.6684, 0.4963, 0.4224, 0.5078,
        0.4425, 0.4708, 0.4637, 0.4206, 0.5222, 0.3559, 0.4822],
       device='cuda:0') torch.Size([16])
percent tensor([0.9993, 0.9986, 0.9981, 0.9985, 0.9993, 0.9950, 0.9991, 0.9997, 0.9991,
        0.9992, 0.9993, 0.9990, 0.9993, 0.9989, 0.9987, 0.9995],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 65 | Batch_idx: 0 |  Loss: (0.3042) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 65 | Batch_idx: 10 |  Loss: (0.3331) |  Loss2: (0.0000) | Acc: (88.00%) (1251/1408)
Epoch: 65 | Batch_idx: 20 |  Loss: (0.3471) |  Loss2: (0.0000) | Acc: (87.00%) (2360/2688)
Epoch: 65 | Batch_idx: 30 |  Loss: (0.3500) |  Loss2: (0.0000) | Acc: (87.00%) (3472/3968)
Epoch: 65 | Batch_idx: 40 |  Loss: (0.3620) |  Loss2: (0.0000) | Acc: (87.00%) (4577/5248)
Epoch: 65 | Batch_idx: 50 |  Loss: (0.3612) |  Loss2: (0.0000) | Acc: (87.00%) (5700/6528)
Epoch: 65 | Batch_idx: 60 |  Loss: (0.3663) |  Loss2: (0.0000) | Acc: (87.00%) (6794/7808)
Epoch: 65 | Batch_idx: 70 |  Loss: (0.3634) |  Loss2: (0.0000) | Acc: (87.00%) (7915/9088)
Epoch: 65 | Batch_idx: 80 |  Loss: (0.3656) |  Loss2: (0.0000) | Acc: (86.00%) (9012/10368)
Epoch: 65 | Batch_idx: 90 |  Loss: (0.3609) |  Loss2: (0.0000) | Acc: (87.00%) (10147/11648)
Epoch: 65 | Batch_idx: 100 |  Loss: (0.3609) |  Loss2: (0.0000) | Acc: (87.00%) (11267/12928)
Epoch: 65 | Batch_idx: 110 |  Loss: (0.3593) |  Loss2: (0.0000) | Acc: (87.00%) (12392/14208)
Epoch: 65 | Batch_idx: 120 |  Loss: (0.3589) |  Loss2: (0.0000) | Acc: (87.00%) (13511/15488)
Epoch: 65 | Batch_idx: 130 |  Loss: (0.3600) |  Loss2: (0.0000) | Acc: (87.00%) (14615/16768)
Epoch: 65 | Batch_idx: 140 |  Loss: (0.3637) |  Loss2: (0.0000) | Acc: (87.00%) (15712/18048)
Epoch: 65 | Batch_idx: 150 |  Loss: (0.3628) |  Loss2: (0.0000) | Acc: (87.00%) (16846/19328)
Epoch: 65 | Batch_idx: 160 |  Loss: (0.3600) |  Loss2: (0.0000) | Acc: (87.00%) (17979/20608)
Epoch: 65 | Batch_idx: 170 |  Loss: (0.3589) |  Loss2: (0.0000) | Acc: (87.00%) (19097/21888)
Epoch: 65 | Batch_idx: 180 |  Loss: (0.3583) |  Loss2: (0.0000) | Acc: (87.00%) (20225/23168)
Epoch: 65 | Batch_idx: 190 |  Loss: (0.3564) |  Loss2: (0.0000) | Acc: (87.00%) (21366/24448)
Epoch: 65 | Batch_idx: 200 |  Loss: (0.3569) |  Loss2: (0.0000) | Acc: (87.00%) (22487/25728)
Epoch: 65 | Batch_idx: 210 |  Loss: (0.3561) |  Loss2: (0.0000) | Acc: (87.00%) (23622/27008)
Epoch: 65 | Batch_idx: 220 |  Loss: (0.3566) |  Loss2: (0.0000) | Acc: (87.00%) (24735/28288)
Epoch: 65 | Batch_idx: 230 |  Loss: (0.3547) |  Loss2: (0.0000) | Acc: (87.00%) (25866/29568)
Epoch: 65 | Batch_idx: 240 |  Loss: (0.3555) |  Loss2: (0.0000) | Acc: (87.00%) (26983/30848)
Epoch: 65 | Batch_idx: 250 |  Loss: (0.3556) |  Loss2: (0.0000) | Acc: (87.00%) (28119/32128)
Epoch: 65 | Batch_idx: 260 |  Loss: (0.3564) |  Loss2: (0.0000) | Acc: (87.00%) (29230/33408)
Epoch: 65 | Batch_idx: 270 |  Loss: (0.3565) |  Loss2: (0.0000) | Acc: (87.00%) (30352/34688)
Epoch: 65 | Batch_idx: 280 |  Loss: (0.3557) |  Loss2: (0.0000) | Acc: (87.00%) (31474/35968)
Epoch: 65 | Batch_idx: 290 |  Loss: (0.3552) |  Loss2: (0.0000) | Acc: (87.00%) (32594/37248)
Epoch: 65 | Batch_idx: 300 |  Loss: (0.3558) |  Loss2: (0.0000) | Acc: (87.00%) (33706/38528)
Epoch: 65 | Batch_idx: 310 |  Loss: (0.3568) |  Loss2: (0.0000) | Acc: (87.00%) (34810/39808)
Epoch: 65 | Batch_idx: 320 |  Loss: (0.3557) |  Loss2: (0.0000) | Acc: (87.00%) (35949/41088)
Epoch: 65 | Batch_idx: 330 |  Loss: (0.3554) |  Loss2: (0.0000) | Acc: (87.00%) (37073/42368)
Epoch: 65 | Batch_idx: 340 |  Loss: (0.3552) |  Loss2: (0.0000) | Acc: (87.00%) (38198/43648)
Epoch: 65 | Batch_idx: 350 |  Loss: (0.3544) |  Loss2: (0.0000) | Acc: (87.00%) (39332/44928)
Epoch: 65 | Batch_idx: 360 |  Loss: (0.3537) |  Loss2: (0.0000) | Acc: (87.00%) (40474/46208)
Epoch: 65 | Batch_idx: 370 |  Loss: (0.3527) |  Loss2: (0.0000) | Acc: (87.00%) (41607/47488)
Epoch: 65 | Batch_idx: 380 |  Loss: (0.3519) |  Loss2: (0.0000) | Acc: (87.00%) (42736/48768)
Epoch: 65 | Batch_idx: 390 |  Loss: (0.3517) |  Loss2: (0.0000) | Acc: (87.00%) (43811/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_065.pth.tar'
# TEST : Loss: (0.4492) | Acc: (84.00%) (8464/10000)
percent tensor([0.5304, 0.5399, 0.5213, 0.5188, 0.5231, 0.5176, 0.5392, 0.5317, 0.5389,
        0.5361, 0.5394, 0.5272, 0.5348, 0.5398, 0.5292, 0.5290],
       device='cuda:0') torch.Size([16])
percent tensor([0.4869, 0.4865, 0.4655, 0.4816, 0.4724, 0.4753, 0.4827, 0.4871, 0.4835,
        0.4782, 0.4770, 0.4686, 0.4914, 0.4970, 0.4819, 0.4829],
       device='cuda:0') torch.Size([16])
percent tensor([0.4769, 0.4535, 0.4096, 0.4748, 0.4107, 0.5062, 0.4516, 0.4614, 0.4675,
        0.4270, 0.4585, 0.4025, 0.4448, 0.5160, 0.4782, 0.4790],
       device='cuda:0') torch.Size([16])
percent tensor([0.5922, 0.5910, 0.5852, 0.5814, 0.5814, 0.5904, 0.5936, 0.5809, 0.5858,
        0.5915, 0.5887, 0.5980, 0.6035, 0.5778, 0.5985, 0.5927],
       device='cuda:0') torch.Size([16])
percent tensor([0.6363, 0.6126, 0.6267, 0.6261, 0.6261, 0.6863, 0.6183, 0.6079, 0.6547,
        0.6270, 0.6578, 0.6470, 0.6428, 0.6578, 0.6220, 0.6551],
       device='cuda:0') torch.Size([16])
percent tensor([0.7023, 0.6893, 0.7275, 0.7474, 0.7685, 0.7459, 0.7274, 0.7392, 0.7067,
        0.6915, 0.6912, 0.7134, 0.6716, 0.7234, 0.7132, 0.7260],
       device='cuda:0') torch.Size([16])
percent tensor([0.4419, 0.4525, 0.5778, 0.5502, 0.6395, 0.6935, 0.4872, 0.4352, 0.5146,
        0.4417, 0.4632, 0.4651, 0.4319, 0.5144, 0.3482, 0.4991],
       device='cuda:0') torch.Size([16])
percent tensor([0.9990, 0.9985, 0.9976, 0.9980, 0.9989, 0.9967, 0.9990, 0.9995, 0.9989,
        0.9990, 0.9990, 0.9990, 0.9991, 0.9984, 0.9987, 0.9994],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 66 | Batch_idx: 0 |  Loss: (0.3914) |  Loss2: (0.0000) | Acc: (85.00%) (109/128)
Epoch: 66 | Batch_idx: 10 |  Loss: (0.3296) |  Loss2: (0.0000) | Acc: (88.00%) (1253/1408)
Epoch: 66 | Batch_idx: 20 |  Loss: (0.3359) |  Loss2: (0.0000) | Acc: (88.00%) (2382/2688)
Epoch: 66 | Batch_idx: 30 |  Loss: (0.3356) |  Loss2: (0.0000) | Acc: (88.00%) (3515/3968)
Epoch: 66 | Batch_idx: 40 |  Loss: (0.3245) |  Loss2: (0.0000) | Acc: (89.00%) (4674/5248)
Epoch: 66 | Batch_idx: 50 |  Loss: (0.3173) |  Loss2: (0.0000) | Acc: (89.00%) (5825/6528)
Epoch: 66 | Batch_idx: 60 |  Loss: (0.3121) |  Loss2: (0.0000) | Acc: (89.00%) (6981/7808)
Epoch: 66 | Batch_idx: 70 |  Loss: (0.3159) |  Loss2: (0.0000) | Acc: (89.00%) (8107/9088)
Epoch: 66 | Batch_idx: 80 |  Loss: (0.3130) |  Loss2: (0.0000) | Acc: (89.00%) (9264/10368)
Epoch: 66 | Batch_idx: 90 |  Loss: (0.3105) |  Loss2: (0.0000) | Acc: (89.00%) (10418/11648)
Epoch: 66 | Batch_idx: 100 |  Loss: (0.3119) |  Loss2: (0.0000) | Acc: (89.00%) (11556/12928)
Epoch: 66 | Batch_idx: 110 |  Loss: (0.3127) |  Loss2: (0.0000) | Acc: (89.00%) (12682/14208)
Epoch: 66 | Batch_idx: 120 |  Loss: (0.3147) |  Loss2: (0.0000) | Acc: (89.00%) (13806/15488)
Epoch: 66 | Batch_idx: 130 |  Loss: (0.3147) |  Loss2: (0.0000) | Acc: (89.00%) (14945/16768)
Epoch: 66 | Batch_idx: 140 |  Loss: (0.3124) |  Loss2: (0.0000) | Acc: (89.00%) (16100/18048)
Epoch: 66 | Batch_idx: 150 |  Loss: (0.3121) |  Loss2: (0.0000) | Acc: (89.00%) (17257/19328)
Epoch: 66 | Batch_idx: 160 |  Loss: (0.3125) |  Loss2: (0.0000) | Acc: (89.00%) (18396/20608)
Epoch: 66 | Batch_idx: 170 |  Loss: (0.3147) |  Loss2: (0.0000) | Acc: (89.00%) (19505/21888)
Epoch: 66 | Batch_idx: 180 |  Loss: (0.3135) |  Loss2: (0.0000) | Acc: (89.00%) (20651/23168)
Epoch: 66 | Batch_idx: 190 |  Loss: (0.3113) |  Loss2: (0.0000) | Acc: (89.00%) (21812/24448)
Epoch: 66 | Batch_idx: 200 |  Loss: (0.3116) |  Loss2: (0.0000) | Acc: (89.00%) (22949/25728)
Epoch: 66 | Batch_idx: 210 |  Loss: (0.3103) |  Loss2: (0.0000) | Acc: (89.00%) (24111/27008)
Epoch: 66 | Batch_idx: 220 |  Loss: (0.3097) |  Loss2: (0.0000) | Acc: (89.00%) (25264/28288)
Epoch: 66 | Batch_idx: 230 |  Loss: (0.3102) |  Loss2: (0.0000) | Acc: (89.00%) (26406/29568)
Epoch: 66 | Batch_idx: 240 |  Loss: (0.3115) |  Loss2: (0.0000) | Acc: (89.00%) (27527/30848)
Epoch: 66 | Batch_idx: 250 |  Loss: (0.3104) |  Loss2: (0.0000) | Acc: (89.00%) (28686/32128)
Epoch: 66 | Batch_idx: 260 |  Loss: (0.3105) |  Loss2: (0.0000) | Acc: (89.00%) (29827/33408)
Epoch: 66 | Batch_idx: 270 |  Loss: (0.3109) |  Loss2: (0.0000) | Acc: (89.00%) (30962/34688)
Epoch: 66 | Batch_idx: 280 |  Loss: (0.3119) |  Loss2: (0.0000) | Acc: (89.00%) (32094/35968)
Epoch: 66 | Batch_idx: 290 |  Loss: (0.3129) |  Loss2: (0.0000) | Acc: (89.00%) (33221/37248)
Epoch: 66 | Batch_idx: 300 |  Loss: (0.3133) |  Loss2: (0.0000) | Acc: (89.00%) (34355/38528)
Epoch: 66 | Batch_idx: 310 |  Loss: (0.3129) |  Loss2: (0.0000) | Acc: (89.00%) (35516/39808)
Epoch: 66 | Batch_idx: 320 |  Loss: (0.3136) |  Loss2: (0.0000) | Acc: (89.00%) (36650/41088)
Epoch: 66 | Batch_idx: 330 |  Loss: (0.3135) |  Loss2: (0.0000) | Acc: (89.00%) (37782/42368)
Epoch: 66 | Batch_idx: 340 |  Loss: (0.3141) |  Loss2: (0.0000) | Acc: (89.00%) (38907/43648)
Epoch: 66 | Batch_idx: 350 |  Loss: (0.3146) |  Loss2: (0.0000) | Acc: (89.00%) (40038/44928)
Epoch: 66 | Batch_idx: 360 |  Loss: (0.3154) |  Loss2: (0.0000) | Acc: (89.00%) (41163/46208)
Epoch: 66 | Batch_idx: 370 |  Loss: (0.3158) |  Loss2: (0.0000) | Acc: (89.00%) (42300/47488)
Epoch: 66 | Batch_idx: 380 |  Loss: (0.3159) |  Loss2: (0.0000) | Acc: (89.00%) (43438/48768)
Epoch: 66 | Batch_idx: 390 |  Loss: (0.3163) |  Loss2: (0.0000) | Acc: (89.00%) (44533/50000)
# TEST : Loss: (0.4341) | Acc: (84.00%) (8490/10000)
percent tensor([0.5299, 0.5411, 0.5156, 0.5181, 0.5199, 0.5177, 0.5399, 0.5291, 0.5394,
        0.5364, 0.5399, 0.5234, 0.5345, 0.5426, 0.5298, 0.5290],
       device='cuda:0') torch.Size([16])
percent tensor([0.4864, 0.4857, 0.4626, 0.4776, 0.4700, 0.4736, 0.4818, 0.4870, 0.4833,
        0.4765, 0.4776, 0.4642, 0.4906, 0.4958, 0.4808, 0.4821],
       device='cuda:0') torch.Size([16])
percent tensor([0.4779, 0.4493, 0.4013, 0.4618, 0.4062, 0.5014, 0.4507, 0.4528, 0.4644,
        0.4275, 0.4623, 0.3999, 0.4439, 0.5146, 0.4726, 0.4783],
       device='cuda:0') torch.Size([16])
percent tensor([0.5924, 0.5926, 0.5854, 0.5820, 0.5809, 0.5867, 0.5951, 0.5821, 0.5872,
        0.5941, 0.5886, 0.6010, 0.6046, 0.5803, 0.5975, 0.5914],
       device='cuda:0') torch.Size([16])
percent tensor([0.6480, 0.6275, 0.6252, 0.6232, 0.6198, 0.6782, 0.6331, 0.6112, 0.6797,
        0.6457, 0.6684, 0.6748, 0.6558, 0.6855, 0.6287, 0.6624],
       device='cuda:0') torch.Size([16])
percent tensor([0.6976, 0.6886, 0.7184, 0.7334, 0.7528, 0.7428, 0.7243, 0.7447, 0.7137,
        0.6926, 0.6992, 0.7129, 0.6747, 0.7349, 0.7059, 0.7207],
       device='cuda:0') torch.Size([16])
percent tensor([0.4241, 0.4654, 0.5384, 0.5276, 0.5901, 0.6762, 0.4693, 0.4288, 0.5385,
        0.4555, 0.4926, 0.4521, 0.4293, 0.5408, 0.3479, 0.5153],
       device='cuda:0') torch.Size([16])
percent tensor([0.9993, 0.9982, 0.9983, 0.9979, 0.9992, 0.9962, 0.9986, 0.9994, 0.9989,
        0.9986, 0.9991, 0.9990, 0.9988, 0.9979, 0.9984, 0.9994],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 67 | Batch_idx: 0 |  Loss: (0.2627) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 67 | Batch_idx: 10 |  Loss: (0.3292) |  Loss2: (0.0000) | Acc: (88.00%) (1241/1408)
Epoch: 67 | Batch_idx: 20 |  Loss: (0.3513) |  Loss2: (0.0000) | Acc: (87.00%) (2350/2688)
Epoch: 67 | Batch_idx: 30 |  Loss: (0.3667) |  Loss2: (0.0000) | Acc: (86.00%) (3451/3968)
Epoch: 67 | Batch_idx: 40 |  Loss: (0.3715) |  Loss2: (0.0000) | Acc: (86.00%) (4556/5248)
Epoch: 67 | Batch_idx: 50 |  Loss: (0.3748) |  Loss2: (0.0000) | Acc: (86.00%) (5668/6528)
Epoch: 67 | Batch_idx: 60 |  Loss: (0.3766) |  Loss2: (0.0000) | Acc: (86.00%) (6780/7808)
Epoch: 67 | Batch_idx: 70 |  Loss: (0.3784) |  Loss2: (0.0000) | Acc: (86.00%) (7898/9088)
Epoch: 67 | Batch_idx: 80 |  Loss: (0.3805) |  Loss2: (0.0000) | Acc: (86.00%) (9006/10368)
Epoch: 67 | Batch_idx: 90 |  Loss: (0.3787) |  Loss2: (0.0000) | Acc: (86.00%) (10125/11648)
Epoch: 67 | Batch_idx: 100 |  Loss: (0.3803) |  Loss2: (0.0000) | Acc: (86.00%) (11221/12928)
Epoch: 67 | Batch_idx: 110 |  Loss: (0.3801) |  Loss2: (0.0000) | Acc: (86.00%) (12327/14208)
Epoch: 67 | Batch_idx: 120 |  Loss: (0.3808) |  Loss2: (0.0000) | Acc: (86.00%) (13436/15488)
Epoch: 67 | Batch_idx: 130 |  Loss: (0.3793) |  Loss2: (0.0000) | Acc: (86.00%) (14557/16768)
Epoch: 67 | Batch_idx: 140 |  Loss: (0.3779) |  Loss2: (0.0000) | Acc: (86.00%) (15679/18048)
Epoch: 67 | Batch_idx: 150 |  Loss: (0.3759) |  Loss2: (0.0000) | Acc: (86.00%) (16799/19328)
Epoch: 67 | Batch_idx: 160 |  Loss: (0.3747) |  Loss2: (0.0000) | Acc: (87.00%) (17936/20608)
Epoch: 67 | Batch_idx: 170 |  Loss: (0.3736) |  Loss2: (0.0000) | Acc: (87.00%) (19061/21888)
Epoch: 67 | Batch_idx: 180 |  Loss: (0.3719) |  Loss2: (0.0000) | Acc: (87.00%) (20199/23168)
Epoch: 67 | Batch_idx: 190 |  Loss: (0.3729) |  Loss2: (0.0000) | Acc: (87.00%) (21307/24448)
Epoch: 67 | Batch_idx: 200 |  Loss: (0.3712) |  Loss2: (0.0000) | Acc: (87.00%) (22436/25728)
Epoch: 67 | Batch_idx: 210 |  Loss: (0.3702) |  Loss2: (0.0000) | Acc: (87.00%) (23556/27008)
Epoch: 67 | Batch_idx: 220 |  Loss: (0.3697) |  Loss2: (0.0000) | Acc: (87.00%) (24672/28288)
Epoch: 67 | Batch_idx: 230 |  Loss: (0.3673) |  Loss2: (0.0000) | Acc: (87.00%) (25822/29568)
Epoch: 67 | Batch_idx: 240 |  Loss: (0.3668) |  Loss2: (0.0000) | Acc: (87.00%) (26941/30848)
Epoch: 67 | Batch_idx: 250 |  Loss: (0.3655) |  Loss2: (0.0000) | Acc: (87.00%) (28076/32128)
Epoch: 67 | Batch_idx: 260 |  Loss: (0.3644) |  Loss2: (0.0000) | Acc: (87.00%) (29213/33408)
Epoch: 67 | Batch_idx: 270 |  Loss: (0.3642) |  Loss2: (0.0000) | Acc: (87.00%) (30331/34688)
Epoch: 67 | Batch_idx: 280 |  Loss: (0.3632) |  Loss2: (0.0000) | Acc: (87.00%) (31454/35968)
Epoch: 67 | Batch_idx: 290 |  Loss: (0.3612) |  Loss2: (0.0000) | Acc: (87.00%) (32600/37248)
Epoch: 67 | Batch_idx: 300 |  Loss: (0.3599) |  Loss2: (0.0000) | Acc: (87.00%) (33752/38528)
Epoch: 67 | Batch_idx: 310 |  Loss: (0.3603) |  Loss2: (0.0000) | Acc: (87.00%) (34872/39808)
Epoch: 67 | Batch_idx: 320 |  Loss: (0.3601) |  Loss2: (0.0000) | Acc: (87.00%) (35998/41088)
Epoch: 67 | Batch_idx: 330 |  Loss: (0.3588) |  Loss2: (0.0000) | Acc: (87.00%) (37131/42368)
Epoch: 67 | Batch_idx: 340 |  Loss: (0.3582) |  Loss2: (0.0000) | Acc: (87.00%) (38257/43648)
Epoch: 67 | Batch_idx: 350 |  Loss: (0.3567) |  Loss2: (0.0000) | Acc: (87.00%) (39392/44928)
Epoch: 67 | Batch_idx: 360 |  Loss: (0.3561) |  Loss2: (0.0000) | Acc: (87.00%) (40524/46208)
Epoch: 67 | Batch_idx: 370 |  Loss: (0.3556) |  Loss2: (0.0000) | Acc: (87.00%) (41674/47488)
Epoch: 67 | Batch_idx: 380 |  Loss: (0.3545) |  Loss2: (0.0000) | Acc: (87.00%) (42812/48768)
Epoch: 67 | Batch_idx: 390 |  Loss: (0.3534) |  Loss2: (0.0000) | Acc: (87.00%) (43902/50000)
# TEST : Loss: (0.4490) | Acc: (84.00%) (8452/10000)
percent tensor([0.5547, 0.5723, 0.5478, 0.5465, 0.5550, 0.5445, 0.5715, 0.5630, 0.5668,
        0.5662, 0.5670, 0.5581, 0.5615, 0.5695, 0.5601, 0.5548],
       device='cuda:0') torch.Size([16])
percent tensor([0.4709, 0.4653, 0.4425, 0.4581, 0.4529, 0.4581, 0.4628, 0.4690, 0.4633,
        0.4546, 0.4562, 0.4408, 0.4744, 0.4748, 0.4620, 0.4644],
       device='cuda:0') torch.Size([16])
percent tensor([0.4882, 0.4567, 0.4039, 0.4657, 0.4043, 0.5122, 0.4543, 0.4501, 0.4667,
        0.4345, 0.4713, 0.4063, 0.4551, 0.5226, 0.4802, 0.4867],
       device='cuda:0') torch.Size([16])
percent tensor([0.5945, 0.5876, 0.5773, 0.5749, 0.5755, 0.5937, 0.5921, 0.5765, 0.5837,
        0.5867, 0.5855, 0.5888, 0.6011, 0.5772, 0.5981, 0.5941],
       device='cuda:0') torch.Size([16])
percent tensor([0.6726, 0.6391, 0.6316, 0.6253, 0.6310, 0.7132, 0.6540, 0.6206, 0.6872,
        0.6600, 0.6859, 0.6927, 0.6694, 0.6936, 0.6540, 0.6888],
       device='cuda:0') torch.Size([16])
percent tensor([0.7076, 0.6932, 0.7342, 0.7479, 0.7622, 0.7550, 0.7326, 0.7565, 0.7190,
        0.7000, 0.7052, 0.7199, 0.6821, 0.7394, 0.7094, 0.7310],
       device='cuda:0') torch.Size([16])
percent tensor([0.4085, 0.4638, 0.5252, 0.5080, 0.5530, 0.6575, 0.4336, 0.3674, 0.5551,
        0.4710, 0.5157, 0.4475, 0.4475, 0.5364, 0.3112, 0.4724],
       device='cuda:0') torch.Size([16])
percent tensor([0.9993, 0.9984, 0.9979, 0.9979, 0.9989, 0.9956, 0.9986, 0.9994, 0.9993,
        0.9988, 0.9994, 0.9990, 0.9991, 0.9978, 0.9987, 0.9993],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 68 | Batch_idx: 0 |  Loss: (0.2756) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 68 | Batch_idx: 10 |  Loss: (0.3250) |  Loss2: (0.0000) | Acc: (88.00%) (1247/1408)
Epoch: 68 | Batch_idx: 20 |  Loss: (0.3056) |  Loss2: (0.0000) | Acc: (89.00%) (2398/2688)
Epoch: 68 | Batch_idx: 30 |  Loss: (0.2900) |  Loss2: (0.0000) | Acc: (89.00%) (3570/3968)
Epoch: 68 | Batch_idx: 40 |  Loss: (0.2973) |  Loss2: (0.0000) | Acc: (89.00%) (4706/5248)
Epoch: 68 | Batch_idx: 50 |  Loss: (0.2929) |  Loss2: (0.0000) | Acc: (89.00%) (5874/6528)
Epoch: 68 | Batch_idx: 60 |  Loss: (0.2966) |  Loss2: (0.0000) | Acc: (89.00%) (7027/7808)
Epoch: 68 | Batch_idx: 70 |  Loss: (0.2950) |  Loss2: (0.0000) | Acc: (90.00%) (8188/9088)
Epoch: 68 | Batch_idx: 80 |  Loss: (0.2937) |  Loss2: (0.0000) | Acc: (90.00%) (9341/10368)
Epoch: 68 | Batch_idx: 90 |  Loss: (0.2934) |  Loss2: (0.0000) | Acc: (90.00%) (10502/11648)
Epoch: 68 | Batch_idx: 100 |  Loss: (0.2942) |  Loss2: (0.0000) | Acc: (90.00%) (11646/12928)
Epoch: 68 | Batch_idx: 110 |  Loss: (0.2962) |  Loss2: (0.0000) | Acc: (89.00%) (12783/14208)
Epoch: 68 | Batch_idx: 120 |  Loss: (0.2965) |  Loss2: (0.0000) | Acc: (89.00%) (13910/15488)
Epoch: 68 | Batch_idx: 130 |  Loss: (0.2960) |  Loss2: (0.0000) | Acc: (89.00%) (15063/16768)
Epoch: 68 | Batch_idx: 140 |  Loss: (0.2942) |  Loss2: (0.0000) | Acc: (89.00%) (16229/18048)
Epoch: 68 | Batch_idx: 150 |  Loss: (0.2945) |  Loss2: (0.0000) | Acc: (89.00%) (17367/19328)
Epoch: 68 | Batch_idx: 160 |  Loss: (0.2944) |  Loss2: (0.0000) | Acc: (89.00%) (18521/20608)
Epoch: 68 | Batch_idx: 170 |  Loss: (0.2935) |  Loss2: (0.0000) | Acc: (89.00%) (19698/21888)
Epoch: 68 | Batch_idx: 180 |  Loss: (0.2952) |  Loss2: (0.0000) | Acc: (89.00%) (20832/23168)
Epoch: 68 | Batch_idx: 190 |  Loss: (0.2957) |  Loss2: (0.0000) | Acc: (89.00%) (21971/24448)
Epoch: 68 | Batch_idx: 200 |  Loss: (0.2948) |  Loss2: (0.0000) | Acc: (89.00%) (23139/25728)
Epoch: 68 | Batch_idx: 210 |  Loss: (0.2964) |  Loss2: (0.0000) | Acc: (89.00%) (24262/27008)
Epoch: 68 | Batch_idx: 220 |  Loss: (0.2981) |  Loss2: (0.0000) | Acc: (89.00%) (25400/28288)
Epoch: 68 | Batch_idx: 230 |  Loss: (0.3002) |  Loss2: (0.0000) | Acc: (89.00%) (26533/29568)
Epoch: 68 | Batch_idx: 240 |  Loss: (0.3009) |  Loss2: (0.0000) | Acc: (89.00%) (27669/30848)
Epoch: 68 | Batch_idx: 250 |  Loss: (0.3015) |  Loss2: (0.0000) | Acc: (89.00%) (28814/32128)
Epoch: 68 | Batch_idx: 260 |  Loss: (0.3028) |  Loss2: (0.0000) | Acc: (89.00%) (29949/33408)
Epoch: 68 | Batch_idx: 270 |  Loss: (0.3031) |  Loss2: (0.0000) | Acc: (89.00%) (31094/34688)
Epoch: 68 | Batch_idx: 280 |  Loss: (0.3031) |  Loss2: (0.0000) | Acc: (89.00%) (32244/35968)
Epoch: 68 | Batch_idx: 290 |  Loss: (0.3043) |  Loss2: (0.0000) | Acc: (89.00%) (33377/37248)
Epoch: 68 | Batch_idx: 300 |  Loss: (0.3051) |  Loss2: (0.0000) | Acc: (89.00%) (34518/38528)
Epoch: 68 | Batch_idx: 310 |  Loss: (0.3047) |  Loss2: (0.0000) | Acc: (89.00%) (35673/39808)
Epoch: 68 | Batch_idx: 320 |  Loss: (0.3052) |  Loss2: (0.0000) | Acc: (89.00%) (36825/41088)
Epoch: 68 | Batch_idx: 330 |  Loss: (0.3051) |  Loss2: (0.0000) | Acc: (89.00%) (37976/42368)
Epoch: 68 | Batch_idx: 340 |  Loss: (0.3061) |  Loss2: (0.0000) | Acc: (89.00%) (39101/43648)
Epoch: 68 | Batch_idx: 350 |  Loss: (0.3059) |  Loss2: (0.0000) | Acc: (89.00%) (40251/44928)
Epoch: 68 | Batch_idx: 360 |  Loss: (0.3062) |  Loss2: (0.0000) | Acc: (89.00%) (41390/46208)
Epoch: 68 | Batch_idx: 370 |  Loss: (0.3053) |  Loss2: (0.0000) | Acc: (89.00%) (42544/47488)
Epoch: 68 | Batch_idx: 380 |  Loss: (0.3054) |  Loss2: (0.0000) | Acc: (89.00%) (43689/48768)
Epoch: 68 | Batch_idx: 390 |  Loss: (0.3054) |  Loss2: (0.0000) | Acc: (89.00%) (44788/50000)
# TEST : Loss: (0.4227) | Acc: (86.00%) (8609/10000)
percent tensor([0.5547, 0.5716, 0.5512, 0.5451, 0.5546, 0.5432, 0.5707, 0.5624, 0.5664,
        0.5655, 0.5669, 0.5592, 0.5613, 0.5687, 0.5593, 0.5545],
       device='cuda:0') torch.Size([16])
percent tensor([0.4688, 0.4694, 0.4363, 0.4600, 0.4482, 0.4543, 0.4645, 0.4677, 0.4651,
        0.4573, 0.4579, 0.4398, 0.4739, 0.4815, 0.4630, 0.4662],
       device='cuda:0') torch.Size([16])
percent tensor([0.4827, 0.4600, 0.3978, 0.4690, 0.4009, 0.5058, 0.4558, 0.4508, 0.4704,
        0.4361, 0.4732, 0.4080, 0.4507, 0.5248, 0.4789, 0.4870],
       device='cuda:0') torch.Size([16])
percent tensor([0.5927, 0.5865, 0.5805, 0.5721, 0.5782, 0.5950, 0.5907, 0.5743, 0.5838,
        0.5860, 0.5853, 0.5891, 0.6013, 0.5724, 0.5955, 0.5916],
       device='cuda:0') torch.Size([16])
percent tensor([0.6750, 0.6417, 0.6444, 0.6339, 0.6410, 0.7105, 0.6562, 0.6277, 0.6942,
        0.6539, 0.6881, 0.6752, 0.6781, 0.7044, 0.6492, 0.6817],
       device='cuda:0') torch.Size([16])
percent tensor([0.7160, 0.6972, 0.7381, 0.7587, 0.7746, 0.7623, 0.7395, 0.7554, 0.7249,
        0.6996, 0.7105, 0.7268, 0.6876, 0.7429, 0.7183, 0.7379],
       device='cuda:0') torch.Size([16])
percent tensor([0.4350, 0.4815, 0.5437, 0.5192, 0.5862, 0.6955, 0.4880, 0.4000, 0.5448,
        0.4708, 0.5195, 0.4734, 0.4616, 0.5140, 0.3381, 0.4971],
       device='cuda:0') torch.Size([16])
percent tensor([0.9991, 0.9985, 0.9981, 0.9985, 0.9989, 0.9951, 0.9985, 0.9995, 0.9989,
        0.9987, 0.9987, 0.9990, 0.9988, 0.9984, 0.9986, 0.9994],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 69 | Batch_idx: 0 |  Loss: (0.2509) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 69 | Batch_idx: 10 |  Loss: (0.2993) |  Loss2: (0.0000) | Acc: (89.00%) (1259/1408)
Epoch: 69 | Batch_idx: 20 |  Loss: (0.3254) |  Loss2: (0.0000) | Acc: (88.00%) (2380/2688)
Epoch: 69 | Batch_idx: 30 |  Loss: (0.3393) |  Loss2: (0.0000) | Acc: (87.00%) (3488/3968)
Epoch: 69 | Batch_idx: 40 |  Loss: (0.3515) |  Loss2: (0.0000) | Acc: (87.00%) (4601/5248)
Epoch: 69 | Batch_idx: 50 |  Loss: (0.3577) |  Loss2: (0.0000) | Acc: (87.00%) (5705/6528)
Epoch: 69 | Batch_idx: 60 |  Loss: (0.3553) |  Loss2: (0.0000) | Acc: (87.00%) (6833/7808)
Epoch: 69 | Batch_idx: 70 |  Loss: (0.3583) |  Loss2: (0.0000) | Acc: (87.00%) (7932/9088)
Epoch: 69 | Batch_idx: 80 |  Loss: (0.3578) |  Loss2: (0.0000) | Acc: (87.00%) (9050/10368)
Epoch: 69 | Batch_idx: 90 |  Loss: (0.3587) |  Loss2: (0.0000) | Acc: (87.00%) (10177/11648)
Epoch: 69 | Batch_idx: 100 |  Loss: (0.3577) |  Loss2: (0.0000) | Acc: (87.00%) (11300/12928)
Epoch: 69 | Batch_idx: 110 |  Loss: (0.3548) |  Loss2: (0.0000) | Acc: (87.00%) (12442/14208)
Epoch: 69 | Batch_idx: 120 |  Loss: (0.3536) |  Loss2: (0.0000) | Acc: (87.00%) (13571/15488)
Epoch: 69 | Batch_idx: 130 |  Loss: (0.3521) |  Loss2: (0.0000) | Acc: (87.00%) (14694/16768)
Epoch: 69 | Batch_idx: 140 |  Loss: (0.3510) |  Loss2: (0.0000) | Acc: (87.00%) (15826/18048)
Epoch: 69 | Batch_idx: 150 |  Loss: (0.3515) |  Loss2: (0.0000) | Acc: (87.00%) (16941/19328)
Epoch: 69 | Batch_idx: 160 |  Loss: (0.3501) |  Loss2: (0.0000) | Acc: (87.00%) (18060/20608)
Epoch: 69 | Batch_idx: 170 |  Loss: (0.3507) |  Loss2: (0.0000) | Acc: (87.00%) (19180/21888)
Epoch: 69 | Batch_idx: 180 |  Loss: (0.3509) |  Loss2: (0.0000) | Acc: (87.00%) (20298/23168)
Epoch: 69 | Batch_idx: 190 |  Loss: (0.3496) |  Loss2: (0.0000) | Acc: (87.00%) (21426/24448)
Epoch: 69 | Batch_idx: 200 |  Loss: (0.3495) |  Loss2: (0.0000) | Acc: (87.00%) (22554/25728)
Epoch: 69 | Batch_idx: 210 |  Loss: (0.3493) |  Loss2: (0.0000) | Acc: (87.00%) (23692/27008)
Epoch: 69 | Batch_idx: 220 |  Loss: (0.3483) |  Loss2: (0.0000) | Acc: (87.00%) (24817/28288)
Epoch: 69 | Batch_idx: 230 |  Loss: (0.3484) |  Loss2: (0.0000) | Acc: (87.00%) (25948/29568)
Epoch: 69 | Batch_idx: 240 |  Loss: (0.3488) |  Loss2: (0.0000) | Acc: (87.00%) (27072/30848)
Epoch: 69 | Batch_idx: 250 |  Loss: (0.3479) |  Loss2: (0.0000) | Acc: (87.00%) (28206/32128)
Epoch: 69 | Batch_idx: 260 |  Loss: (0.3489) |  Loss2: (0.0000) | Acc: (87.00%) (29314/33408)
Epoch: 69 | Batch_idx: 270 |  Loss: (0.3490) |  Loss2: (0.0000) | Acc: (87.00%) (30445/34688)
Epoch: 69 | Batch_idx: 280 |  Loss: (0.3486) |  Loss2: (0.0000) | Acc: (87.00%) (31566/35968)
Epoch: 69 | Batch_idx: 290 |  Loss: (0.3473) |  Loss2: (0.0000) | Acc: (87.00%) (32695/37248)
Epoch: 69 | Batch_idx: 300 |  Loss: (0.3470) |  Loss2: (0.0000) | Acc: (87.00%) (33819/38528)
Epoch: 69 | Batch_idx: 310 |  Loss: (0.3469) |  Loss2: (0.0000) | Acc: (87.00%) (34957/39808)
Epoch: 69 | Batch_idx: 320 |  Loss: (0.3472) |  Loss2: (0.0000) | Acc: (87.00%) (36073/41088)
Epoch: 69 | Batch_idx: 330 |  Loss: (0.3467) |  Loss2: (0.0000) | Acc: (87.00%) (37202/42368)
Epoch: 69 | Batch_idx: 340 |  Loss: (0.3459) |  Loss2: (0.0000) | Acc: (87.00%) (38340/43648)
Epoch: 69 | Batch_idx: 350 |  Loss: (0.3456) |  Loss2: (0.0000) | Acc: (87.00%) (39480/44928)
Epoch: 69 | Batch_idx: 360 |  Loss: (0.3454) |  Loss2: (0.0000) | Acc: (87.00%) (40612/46208)
Epoch: 69 | Batch_idx: 370 |  Loss: (0.3452) |  Loss2: (0.0000) | Acc: (87.00%) (41743/47488)
Epoch: 69 | Batch_idx: 380 |  Loss: (0.3450) |  Loss2: (0.0000) | Acc: (87.00%) (42864/48768)
Epoch: 69 | Batch_idx: 390 |  Loss: (0.3449) |  Loss2: (0.0000) | Acc: (87.00%) (43962/50000)
# TEST : Loss: (0.4452) | Acc: (85.00%) (8512/10000)
percent tensor([0.5624, 0.5794, 0.5562, 0.5498, 0.5599, 0.5497, 0.5782, 0.5677, 0.5737,
        0.5722, 0.5749, 0.5637, 0.5690, 0.5762, 0.5656, 0.5619],
       device='cuda:0') torch.Size([16])
percent tensor([0.4716, 0.4735, 0.4342, 0.4582, 0.4509, 0.4590, 0.4689, 0.4687, 0.4667,
        0.4575, 0.4599, 0.4396, 0.4761, 0.4825, 0.4679, 0.4685],
       device='cuda:0') torch.Size([16])
percent tensor([0.4902, 0.4695, 0.4262, 0.4832, 0.4260, 0.5060, 0.4708, 0.4765, 0.4845,
        0.4510, 0.4805, 0.4305, 0.4584, 0.5287, 0.4882, 0.4937],
       device='cuda:0') torch.Size([16])
percent tensor([0.6007, 0.5920, 0.5722, 0.5686, 0.5721, 0.6118, 0.5915, 0.5671, 0.5843,
        0.5875, 0.5912, 0.5833, 0.6133, 0.5725, 0.6023, 0.6024],
       device='cuda:0') torch.Size([16])
percent tensor([0.6687, 0.6192, 0.6165, 0.6108, 0.6230, 0.7176, 0.6413, 0.6038, 0.6708,
        0.6257, 0.6680, 0.6333, 0.6627, 0.6721, 0.6386, 0.6704],
       device='cuda:0') torch.Size([16])
percent tensor([0.7156, 0.6991, 0.7334, 0.7477, 0.7737, 0.7739, 0.7409, 0.7445, 0.7229,
        0.6964, 0.7081, 0.7215, 0.6910, 0.7421, 0.7144, 0.7410],
       device='cuda:0') torch.Size([16])
percent tensor([0.4485, 0.4844, 0.5512, 0.5137, 0.5974, 0.7033, 0.5075, 0.4134, 0.5434,
        0.4570, 0.4947, 0.4710, 0.4500, 0.5185, 0.3367, 0.5186],
       device='cuda:0') torch.Size([16])
percent tensor([0.9991, 0.9984, 0.9985, 0.9987, 0.9989, 0.9969, 0.9987, 0.9996, 0.9989,
        0.9988, 0.9985, 0.9991, 0.9985, 0.9985, 0.9989, 0.9995],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 70 | Batch_idx: 0 |  Loss: (0.3519) |  Loss2: (0.0000) | Acc: (86.00%) (111/128)
Epoch: 70 | Batch_idx: 10 |  Loss: (0.2917) |  Loss2: (0.0000) | Acc: (89.00%) (1261/1408)
Epoch: 70 | Batch_idx: 20 |  Loss: (0.2903) |  Loss2: (0.0000) | Acc: (89.00%) (2402/2688)
Epoch: 70 | Batch_idx: 30 |  Loss: (0.2975) |  Loss2: (0.0000) | Acc: (89.00%) (3561/3968)
Epoch: 70 | Batch_idx: 40 |  Loss: (0.2950) |  Loss2: (0.0000) | Acc: (89.00%) (4722/5248)
Epoch: 70 | Batch_idx: 50 |  Loss: (0.2919) |  Loss2: (0.0000) | Acc: (90.00%) (5889/6528)
Epoch: 70 | Batch_idx: 60 |  Loss: (0.2889) |  Loss2: (0.0000) | Acc: (90.00%) (7043/7808)
Epoch: 70 | Batch_idx: 70 |  Loss: (0.2925) |  Loss2: (0.0000) | Acc: (90.00%) (8187/9088)
Epoch: 70 | Batch_idx: 80 |  Loss: (0.2948) |  Loss2: (0.0000) | Acc: (90.00%) (9340/10368)
Epoch: 70 | Batch_idx: 90 |  Loss: (0.3007) |  Loss2: (0.0000) | Acc: (89.00%) (10482/11648)
Epoch: 70 | Batch_idx: 100 |  Loss: (0.3024) |  Loss2: (0.0000) | Acc: (89.00%) (11610/12928)
Epoch: 70 | Batch_idx: 110 |  Loss: (0.2980) |  Loss2: (0.0000) | Acc: (89.00%) (12778/14208)
Epoch: 70 | Batch_idx: 120 |  Loss: (0.2966) |  Loss2: (0.0000) | Acc: (89.00%) (13934/15488)
Epoch: 70 | Batch_idx: 130 |  Loss: (0.2967) |  Loss2: (0.0000) | Acc: (89.00%) (15084/16768)
Epoch: 70 | Batch_idx: 140 |  Loss: (0.2979) |  Loss2: (0.0000) | Acc: (89.00%) (16216/18048)
Epoch: 70 | Batch_idx: 150 |  Loss: (0.2975) |  Loss2: (0.0000) | Acc: (89.00%) (17361/19328)
Epoch: 70 | Batch_idx: 160 |  Loss: (0.2990) |  Loss2: (0.0000) | Acc: (89.00%) (18498/20608)
Epoch: 70 | Batch_idx: 170 |  Loss: (0.2971) |  Loss2: (0.0000) | Acc: (89.00%) (19656/21888)
Epoch: 70 | Batch_idx: 180 |  Loss: (0.2963) |  Loss2: (0.0000) | Acc: (89.00%) (20821/23168)
Epoch: 70 | Batch_idx: 190 |  Loss: (0.2961) |  Loss2: (0.0000) | Acc: (89.00%) (21984/24448)
Epoch: 70 | Batch_idx: 200 |  Loss: (0.2965) |  Loss2: (0.0000) | Acc: (89.00%) (23131/25728)
Epoch: 70 | Batch_idx: 210 |  Loss: (0.2983) |  Loss2: (0.0000) | Acc: (89.00%) (24267/27008)
Epoch: 70 | Batch_idx: 220 |  Loss: (0.2989) |  Loss2: (0.0000) | Acc: (89.00%) (25418/28288)
Epoch: 70 | Batch_idx: 230 |  Loss: (0.2980) |  Loss2: (0.0000) | Acc: (89.00%) (26567/29568)
Epoch: 70 | Batch_idx: 240 |  Loss: (0.2979) |  Loss2: (0.0000) | Acc: (89.00%) (27723/30848)
Epoch: 70 | Batch_idx: 250 |  Loss: (0.2969) |  Loss2: (0.0000) | Acc: (89.00%) (28883/32128)
Epoch: 70 | Batch_idx: 260 |  Loss: (0.2979) |  Loss2: (0.0000) | Acc: (89.00%) (30012/33408)
Epoch: 70 | Batch_idx: 270 |  Loss: (0.2985) |  Loss2: (0.0000) | Acc: (89.00%) (31150/34688)
Epoch: 70 | Batch_idx: 280 |  Loss: (0.2981) |  Loss2: (0.0000) | Acc: (89.00%) (32301/35968)
Epoch: 70 | Batch_idx: 290 |  Loss: (0.2985) |  Loss2: (0.0000) | Acc: (89.00%) (33439/37248)
Epoch: 70 | Batch_idx: 300 |  Loss: (0.2989) |  Loss2: (0.0000) | Acc: (89.00%) (34584/38528)
Epoch: 70 | Batch_idx: 310 |  Loss: (0.2985) |  Loss2: (0.0000) | Acc: (89.00%) (35740/39808)
Epoch: 70 | Batch_idx: 320 |  Loss: (0.2972) |  Loss2: (0.0000) | Acc: (89.00%) (36907/41088)
Epoch: 70 | Batch_idx: 330 |  Loss: (0.2971) |  Loss2: (0.0000) | Acc: (89.00%) (38048/42368)
Epoch: 70 | Batch_idx: 340 |  Loss: (0.2980) |  Loss2: (0.0000) | Acc: (89.00%) (39168/43648)
Epoch: 70 | Batch_idx: 350 |  Loss: (0.2983) |  Loss2: (0.0000) | Acc: (89.00%) (40324/44928)
Epoch: 70 | Batch_idx: 360 |  Loss: (0.2982) |  Loss2: (0.0000) | Acc: (89.00%) (41480/46208)
Epoch: 70 | Batch_idx: 370 |  Loss: (0.2981) |  Loss2: (0.0000) | Acc: (89.00%) (42631/47488)
Epoch: 70 | Batch_idx: 380 |  Loss: (0.2986) |  Loss2: (0.0000) | Acc: (89.00%) (43765/48768)
Epoch: 70 | Batch_idx: 390 |  Loss: (0.2989) |  Loss2: (0.0000) | Acc: (89.00%) (44869/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_070.pth.tar'
# TEST : Loss: (0.4645) | Acc: (84.00%) (8413/10000)
percent tensor([0.5616, 0.5783, 0.5501, 0.5494, 0.5582, 0.5498, 0.5782, 0.5654, 0.5734,
        0.5722, 0.5738, 0.5607, 0.5683, 0.5751, 0.5662, 0.5610],
       device='cuda:0') torch.Size([16])
percent tensor([0.4696, 0.4707, 0.4392, 0.4567, 0.4523, 0.4549, 0.4680, 0.4726, 0.4662,
        0.4583, 0.4575, 0.4418, 0.4751, 0.4803, 0.4663, 0.4643],
       device='cuda:0') torch.Size([16])
percent tensor([0.4892, 0.4683, 0.4209, 0.4828, 0.4192, 0.5031, 0.4681, 0.4750, 0.4829,
        0.4486, 0.4809, 0.4238, 0.4597, 0.5259, 0.4877, 0.4928],
       device='cuda:0') torch.Size([16])
percent tensor([0.5978, 0.5904, 0.5735, 0.5643, 0.5708, 0.6030, 0.5906, 0.5639, 0.5831,
        0.5883, 0.5891, 0.5819, 0.6120, 0.5742, 0.5982, 0.5990],
       device='cuda:0') torch.Size([16])
percent tensor([0.6630, 0.6186, 0.6280, 0.6106, 0.6267, 0.7088, 0.6365, 0.6115, 0.6875,
        0.6319, 0.6771, 0.6522, 0.6591, 0.6896, 0.6363, 0.6747],
       device='cuda:0') torch.Size([16])
percent tensor([0.7183, 0.6905, 0.7419, 0.7543, 0.7758, 0.7653, 0.7402, 0.7594, 0.7219,
        0.6942, 0.7098, 0.7221, 0.6859, 0.7373, 0.7169, 0.7407],
       device='cuda:0') torch.Size([16])
percent tensor([0.4434, 0.4463, 0.5283, 0.5226, 0.5648, 0.6833, 0.4845, 0.3984, 0.5150,
        0.4313, 0.4859, 0.4255, 0.4245, 0.5061, 0.3235, 0.5180],
       device='cuda:0') torch.Size([16])
percent tensor([0.9994, 0.9985, 0.9984, 0.9981, 0.9988, 0.9960, 0.9985, 0.9996, 0.9988,
        0.9992, 0.9990, 0.9992, 0.9990, 0.9992, 0.9991, 0.9995],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(175.3121, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(804.8343, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(794.4488, device='cuda:0')
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1519.3481, device='cuda:0')
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(499.9774, device='cuda:0')
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2209.8535, device='cuda:0')
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4303.2251, device='cuda:0')
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1414.5327, device='cuda:0')
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6136.5347, device='cuda:0')
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11967.4902, device='cuda:0')
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3980.9902, device='cuda:0')
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16827.0762, device='cuda:0')
Epoch: 71 | Batch_idx: 0 |  Loss: (0.2811) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 71 | Batch_idx: 10 |  Loss: (0.3261) |  Loss2: (0.0000) | Acc: (89.00%) (1255/1408)
Epoch: 71 | Batch_idx: 20 |  Loss: (0.3293) |  Loss2: (0.0000) | Acc: (88.00%) (2373/2688)
Epoch: 71 | Batch_idx: 30 |  Loss: (0.3569) |  Loss2: (0.0000) | Acc: (87.00%) (3461/3968)
Epoch: 71 | Batch_idx: 40 |  Loss: (0.3630) |  Loss2: (0.0000) | Acc: (87.00%) (4566/5248)
Epoch: 71 | Batch_idx: 50 |  Loss: (0.3652) |  Loss2: (0.0000) | Acc: (87.00%) (5681/6528)
Epoch: 71 | Batch_idx: 60 |  Loss: (0.3705) |  Loss2: (0.0000) | Acc: (86.00%) (6777/7808)
Epoch: 71 | Batch_idx: 70 |  Loss: (0.3728) |  Loss2: (0.0000) | Acc: (86.00%) (7891/9088)
Epoch: 71 | Batch_idx: 80 |  Loss: (0.3750) |  Loss2: (0.0000) | Acc: (86.00%) (9005/10368)
Epoch: 71 | Batch_idx: 90 |  Loss: (0.3773) |  Loss2: (0.0000) | Acc: (86.00%) (10115/11648)
Epoch: 71 | Batch_idx: 100 |  Loss: (0.3727) |  Loss2: (0.0000) | Acc: (86.00%) (11242/12928)
Epoch: 71 | Batch_idx: 110 |  Loss: (0.3738) |  Loss2: (0.0000) | Acc: (86.00%) (12343/14208)
Epoch: 71 | Batch_idx: 120 |  Loss: (0.3743) |  Loss2: (0.0000) | Acc: (86.00%) (13461/15488)
Epoch: 71 | Batch_idx: 130 |  Loss: (0.3747) |  Loss2: (0.0000) | Acc: (86.00%) (14572/16768)
Epoch: 71 | Batch_idx: 140 |  Loss: (0.3724) |  Loss2: (0.0000) | Acc: (87.00%) (15705/18048)
Epoch: 71 | Batch_idx: 150 |  Loss: (0.3736) |  Loss2: (0.0000) | Acc: (86.00%) (16811/19328)
Epoch: 71 | Batch_idx: 160 |  Loss: (0.3726) |  Loss2: (0.0000) | Acc: (86.00%) (17919/20608)
Epoch: 71 | Batch_idx: 170 |  Loss: (0.3688) |  Loss2: (0.0000) | Acc: (87.00%) (19059/21888)
Epoch: 71 | Batch_idx: 180 |  Loss: (0.3695) |  Loss2: (0.0000) | Acc: (87.00%) (20181/23168)
Epoch: 71 | Batch_idx: 190 |  Loss: (0.3685) |  Loss2: (0.0000) | Acc: (87.00%) (21309/24448)
Epoch: 71 | Batch_idx: 200 |  Loss: (0.3666) |  Loss2: (0.0000) | Acc: (87.00%) (22433/25728)
Epoch: 71 | Batch_idx: 210 |  Loss: (0.3673) |  Loss2: (0.0000) | Acc: (87.00%) (23551/27008)
Epoch: 71 | Batch_idx: 220 |  Loss: (0.3670) |  Loss2: (0.0000) | Acc: (87.00%) (24659/28288)
Epoch: 71 | Batch_idx: 230 |  Loss: (0.3652) |  Loss2: (0.0000) | Acc: (87.00%) (25775/29568)
Epoch: 71 | Batch_idx: 240 |  Loss: (0.3640) |  Loss2: (0.0000) | Acc: (87.00%) (26931/30848)
Epoch: 71 | Batch_idx: 250 |  Loss: (0.3636) |  Loss2: (0.0000) | Acc: (87.00%) (28040/32128)
Epoch: 71 | Batch_idx: 260 |  Loss: (0.3636) |  Loss2: (0.0000) | Acc: (87.00%) (29155/33408)
Epoch: 71 | Batch_idx: 270 |  Loss: (0.3635) |  Loss2: (0.0000) | Acc: (87.00%) (30266/34688)
Epoch: 71 | Batch_idx: 280 |  Loss: (0.3637) |  Loss2: (0.0000) | Acc: (87.00%) (31375/35968)
Epoch: 71 | Batch_idx: 290 |  Loss: (0.3641) |  Loss2: (0.0000) | Acc: (87.00%) (32486/37248)
Epoch: 71 | Batch_idx: 300 |  Loss: (0.3639) |  Loss2: (0.0000) | Acc: (87.00%) (33614/38528)
Epoch: 71 | Batch_idx: 310 |  Loss: (0.3637) |  Loss2: (0.0000) | Acc: (87.00%) (34735/39808)
Epoch: 71 | Batch_idx: 320 |  Loss: (0.3628) |  Loss2: (0.0000) | Acc: (87.00%) (35862/41088)
Epoch: 71 | Batch_idx: 330 |  Loss: (0.3605) |  Loss2: (0.0000) | Acc: (87.00%) (37011/42368)
Epoch: 71 | Batch_idx: 340 |  Loss: (0.3595) |  Loss2: (0.0000) | Acc: (87.00%) (38140/43648)
Epoch: 71 | Batch_idx: 350 |  Loss: (0.3590) |  Loss2: (0.0000) | Acc: (87.00%) (39262/44928)
Epoch: 71 | Batch_idx: 360 |  Loss: (0.3586) |  Loss2: (0.0000) | Acc: (87.00%) (40388/46208)
Epoch: 71 | Batch_idx: 370 |  Loss: (0.3579) |  Loss2: (0.0000) | Acc: (87.00%) (41525/47488)
Epoch: 71 | Batch_idx: 380 |  Loss: (0.3579) |  Loss2: (0.0000) | Acc: (87.00%) (42656/48768)
Epoch: 71 | Batch_idx: 390 |  Loss: (0.3578) |  Loss2: (0.0000) | Acc: (87.00%) (43729/50000)
# TEST : Loss: (0.4587) | Acc: (84.00%) (8475/10000)
percent tensor([0.5723, 0.5904, 0.5634, 0.5603, 0.5714, 0.5617, 0.5902, 0.5792, 0.5849,
        0.5839, 0.5856, 0.5745, 0.5795, 0.5853, 0.5787, 0.5720],
       device='cuda:0') torch.Size([16])
percent tensor([0.4844, 0.4848, 0.4544, 0.4738, 0.4678, 0.4711, 0.4828, 0.4882, 0.4805,
        0.4727, 0.4711, 0.4575, 0.4872, 0.4969, 0.4799, 0.4796],
       device='cuda:0') torch.Size([16])
percent tensor([0.4738, 0.4430, 0.4326, 0.4998, 0.4362, 0.5082, 0.4511, 0.4850, 0.4743,
        0.4288, 0.4501, 0.4224, 0.4228, 0.5243, 0.4742, 0.4786],
       device='cuda:0') torch.Size([16])
percent tensor([0.6254, 0.6207, 0.5911, 0.5893, 0.5901, 0.6247, 0.6188, 0.5901, 0.6103,
        0.6177, 0.6210, 0.6054, 0.6395, 0.6101, 0.6265, 0.6282],
       device='cuda:0') torch.Size([16])
percent tensor([0.6467, 0.5994, 0.6331, 0.6188, 0.6298, 0.7071, 0.6314, 0.6059, 0.6958,
        0.6338, 0.6873, 0.6724, 0.6235, 0.7145, 0.6178, 0.6641],
       device='cuda:0') torch.Size([16])
percent tensor([0.6796, 0.6543, 0.6995, 0.7168, 0.7295, 0.7220, 0.6960, 0.7152, 0.6807,
        0.6601, 0.6689, 0.6844, 0.6512, 0.6912, 0.6718, 0.7023],
       device='cuda:0') torch.Size([16])
percent tensor([0.4797, 0.4755, 0.5811, 0.5619, 0.6005, 0.6990, 0.5224, 0.4586, 0.5388,
        0.4839, 0.5039, 0.4643, 0.4446, 0.5162, 0.3300, 0.5605],
       device='cuda:0') torch.Size([16])
percent tensor([0.9994, 0.9984, 0.9985, 0.9985, 0.9990, 0.9959, 0.9988, 0.9996, 0.9993,
        0.9993, 0.9994, 0.9992, 0.9990, 0.9993, 0.9989, 0.9994],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 72 | Batch_idx: 0 |  Loss: (0.3868) |  Loss2: (0.0000) | Acc: (87.00%) (112/128)
Epoch: 72 | Batch_idx: 10 |  Loss: (0.2894) |  Loss2: (0.0000) | Acc: (90.00%) (1274/1408)
Epoch: 72 | Batch_idx: 20 |  Loss: (0.2799) |  Loss2: (0.0000) | Acc: (90.00%) (2445/2688)
Epoch: 72 | Batch_idx: 30 |  Loss: (0.2787) |  Loss2: (0.0000) | Acc: (91.00%) (3611/3968)
Epoch: 72 | Batch_idx: 40 |  Loss: (0.2703) |  Loss2: (0.0000) | Acc: (91.00%) (4787/5248)
Epoch: 72 | Batch_idx: 50 |  Loss: (0.2757) |  Loss2: (0.0000) | Acc: (90.00%) (5934/6528)
Epoch: 72 | Batch_idx: 60 |  Loss: (0.2743) |  Loss2: (0.0000) | Acc: (90.00%) (7104/7808)
Epoch: 72 | Batch_idx: 70 |  Loss: (0.2752) |  Loss2: (0.0000) | Acc: (90.00%) (8263/9088)
Epoch: 72 | Batch_idx: 80 |  Loss: (0.2785) |  Loss2: (0.0000) | Acc: (90.00%) (9411/10368)
Epoch: 72 | Batch_idx: 90 |  Loss: (0.2779) |  Loss2: (0.0000) | Acc: (90.00%) (10572/11648)
Epoch: 72 | Batch_idx: 100 |  Loss: (0.2779) |  Loss2: (0.0000) | Acc: (90.00%) (11729/12928)
Epoch: 72 | Batch_idx: 110 |  Loss: (0.2805) |  Loss2: (0.0000) | Acc: (90.00%) (12880/14208)
Epoch: 72 | Batch_idx: 120 |  Loss: (0.2836) |  Loss2: (0.0000) | Acc: (90.00%) (14015/15488)
Epoch: 72 | Batch_idx: 130 |  Loss: (0.2836) |  Loss2: (0.0000) | Acc: (90.00%) (15177/16768)
Epoch: 72 | Batch_idx: 140 |  Loss: (0.2837) |  Loss2: (0.0000) | Acc: (90.00%) (16333/18048)
Epoch: 72 | Batch_idx: 150 |  Loss: (0.2841) |  Loss2: (0.0000) | Acc: (90.00%) (17479/19328)
Epoch: 72 | Batch_idx: 160 |  Loss: (0.2862) |  Loss2: (0.0000) | Acc: (90.00%) (18620/20608)
Epoch: 72 | Batch_idx: 170 |  Loss: (0.2854) |  Loss2: (0.0000) | Acc: (90.00%) (19781/21888)
Epoch: 72 | Batch_idx: 180 |  Loss: (0.2854) |  Loss2: (0.0000) | Acc: (90.00%) (20926/23168)
Epoch: 72 | Batch_idx: 190 |  Loss: (0.2843) |  Loss2: (0.0000) | Acc: (90.00%) (22090/24448)
Epoch: 72 | Batch_idx: 200 |  Loss: (0.2850) |  Loss2: (0.0000) | Acc: (90.00%) (23241/25728)
Epoch: 72 | Batch_idx: 210 |  Loss: (0.2874) |  Loss2: (0.0000) | Acc: (90.00%) (24366/27008)
Epoch: 72 | Batch_idx: 220 |  Loss: (0.2872) |  Loss2: (0.0000) | Acc: (90.00%) (25521/28288)
Epoch: 72 | Batch_idx: 230 |  Loss: (0.2879) |  Loss2: (0.0000) | Acc: (90.00%) (26666/29568)
Epoch: 72 | Batch_idx: 240 |  Loss: (0.2896) |  Loss2: (0.0000) | Acc: (90.00%) (27794/30848)
Epoch: 72 | Batch_idx: 250 |  Loss: (0.2893) |  Loss2: (0.0000) | Acc: (90.00%) (28960/32128)
Epoch: 72 | Batch_idx: 260 |  Loss: (0.2899) |  Loss2: (0.0000) | Acc: (90.00%) (30099/33408)
Epoch: 72 | Batch_idx: 270 |  Loss: (0.2902) |  Loss2: (0.0000) | Acc: (90.00%) (31245/34688)
Epoch: 72 | Batch_idx: 280 |  Loss: (0.2904) |  Loss2: (0.0000) | Acc: (90.00%) (32390/35968)
Epoch: 72 | Batch_idx: 290 |  Loss: (0.2901) |  Loss2: (0.0000) | Acc: (90.00%) (33552/37248)
Epoch: 72 | Batch_idx: 300 |  Loss: (0.2898) |  Loss2: (0.0000) | Acc: (90.00%) (34700/38528)
Epoch: 72 | Batch_idx: 310 |  Loss: (0.2896) |  Loss2: (0.0000) | Acc: (90.00%) (35857/39808)
Epoch: 72 | Batch_idx: 320 |  Loss: (0.2898) |  Loss2: (0.0000) | Acc: (90.00%) (37017/41088)
Epoch: 72 | Batch_idx: 330 |  Loss: (0.2901) |  Loss2: (0.0000) | Acc: (90.00%) (38158/42368)
Epoch: 72 | Batch_idx: 340 |  Loss: (0.2904) |  Loss2: (0.0000) | Acc: (90.00%) (39305/43648)
Epoch: 72 | Batch_idx: 350 |  Loss: (0.2905) |  Loss2: (0.0000) | Acc: (90.00%) (40451/44928)
Epoch: 72 | Batch_idx: 360 |  Loss: (0.2908) |  Loss2: (0.0000) | Acc: (90.00%) (41595/46208)
Epoch: 72 | Batch_idx: 370 |  Loss: (0.2913) |  Loss2: (0.0000) | Acc: (90.00%) (42741/47488)
Epoch: 72 | Batch_idx: 380 |  Loss: (0.2917) |  Loss2: (0.0000) | Acc: (89.00%) (43883/48768)
Epoch: 72 | Batch_idx: 390 |  Loss: (0.2910) |  Loss2: (0.0000) | Acc: (90.00%) (45007/50000)
# TEST : Loss: (0.4268) | Acc: (85.00%) (8548/10000)
percent tensor([0.5712, 0.5892, 0.5654, 0.5581, 0.5716, 0.5583, 0.5897, 0.5778, 0.5837,
        0.5827, 0.5838, 0.5752, 0.5780, 0.5852, 0.5758, 0.5704],
       device='cuda:0') torch.Size([16])
percent tensor([0.4860, 0.4858, 0.4532, 0.4778, 0.4678, 0.4740, 0.4823, 0.4895, 0.4821,
        0.4715, 0.4715, 0.4537, 0.4879, 0.4974, 0.4834, 0.4828],
       device='cuda:0') torch.Size([16])
percent tensor([0.4818, 0.4524, 0.4256, 0.4991, 0.4353, 0.5179, 0.4592, 0.4859, 0.4787,
        0.4284, 0.4611, 0.4183, 0.4260, 0.5329, 0.4836, 0.4869],
       device='cuda:0') torch.Size([16])
percent tensor([0.6236, 0.6195, 0.6038, 0.5915, 0.6002, 0.6257, 0.6200, 0.5948, 0.6103,
        0.6187, 0.6171, 0.6126, 0.6361, 0.6011, 0.6257, 0.6252],
       device='cuda:0') torch.Size([16])
percent tensor([0.6443, 0.6007, 0.6207, 0.6115, 0.6268, 0.6971, 0.6189, 0.5949, 0.6841,
        0.6278, 0.6748, 0.6376, 0.6220, 0.6942, 0.6034, 0.6585],
       device='cuda:0') torch.Size([16])
percent tensor([0.6808, 0.6573, 0.6960, 0.7074, 0.7282, 0.7285, 0.6975, 0.7060, 0.6824,
        0.6628, 0.6697, 0.6789, 0.6511, 0.6907, 0.6791, 0.7031],
       device='cuda:0') torch.Size([16])
percent tensor([0.4900, 0.4604, 0.5737, 0.5378, 0.6018, 0.7124, 0.5119, 0.4321, 0.5317,
        0.4737, 0.5075, 0.4774, 0.4505, 0.5017, 0.3311, 0.5623],
       device='cuda:0') torch.Size([16])
percent tensor([0.9991, 0.9988, 0.9983, 0.9984, 0.9987, 0.9971, 0.9984, 0.9995, 0.9986,
        0.9990, 0.9989, 0.9988, 0.9991, 0.9990, 0.9991, 0.9995],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 73 | Batch_idx: 0 |  Loss: (0.1714) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 73 | Batch_idx: 10 |  Loss: (0.2874) |  Loss2: (0.0000) | Acc: (89.00%) (1267/1408)
Epoch: 73 | Batch_idx: 20 |  Loss: (0.3031) |  Loss2: (0.0000) | Acc: (89.00%) (2402/2688)
Epoch: 73 | Batch_idx: 30 |  Loss: (0.3153) |  Loss2: (0.0000) | Acc: (88.00%) (3529/3968)
Epoch: 73 | Batch_idx: 40 |  Loss: (0.3290) |  Loss2: (0.0000) | Acc: (88.00%) (4637/5248)
Epoch: 73 | Batch_idx: 50 |  Loss: (0.3326) |  Loss2: (0.0000) | Acc: (88.00%) (5774/6528)
Epoch: 73 | Batch_idx: 60 |  Loss: (0.3398) |  Loss2: (0.0000) | Acc: (88.00%) (6888/7808)
Epoch: 73 | Batch_idx: 70 |  Loss: (0.3416) |  Loss2: (0.0000) | Acc: (88.00%) (8011/9088)
Epoch: 73 | Batch_idx: 80 |  Loss: (0.3384) |  Loss2: (0.0000) | Acc: (88.00%) (9154/10368)
Epoch: 73 | Batch_idx: 90 |  Loss: (0.3391) |  Loss2: (0.0000) | Acc: (88.00%) (10278/11648)
Epoch: 73 | Batch_idx: 100 |  Loss: (0.3373) |  Loss2: (0.0000) | Acc: (88.00%) (11427/12928)
Epoch: 73 | Batch_idx: 110 |  Loss: (0.3353) |  Loss2: (0.0000) | Acc: (88.00%) (12566/14208)
Epoch: 73 | Batch_idx: 120 |  Loss: (0.3349) |  Loss2: (0.0000) | Acc: (88.00%) (13699/15488)
Epoch: 73 | Batch_idx: 130 |  Loss: (0.3340) |  Loss2: (0.0000) | Acc: (88.00%) (14839/16768)
Epoch: 73 | Batch_idx: 140 |  Loss: (0.3305) |  Loss2: (0.0000) | Acc: (88.00%) (15988/18048)
Epoch: 73 | Batch_idx: 150 |  Loss: (0.3313) |  Loss2: (0.0000) | Acc: (88.00%) (17124/19328)
Epoch: 73 | Batch_idx: 160 |  Loss: (0.3312) |  Loss2: (0.0000) | Acc: (88.00%) (18249/20608)
Epoch: 73 | Batch_idx: 170 |  Loss: (0.3313) |  Loss2: (0.0000) | Acc: (88.00%) (19387/21888)
Epoch: 73 | Batch_idx: 180 |  Loss: (0.3316) |  Loss2: (0.0000) | Acc: (88.00%) (20514/23168)
Epoch: 73 | Batch_idx: 190 |  Loss: (0.3320) |  Loss2: (0.0000) | Acc: (88.00%) (21652/24448)
Epoch: 73 | Batch_idx: 200 |  Loss: (0.3324) |  Loss2: (0.0000) | Acc: (88.00%) (22792/25728)
Epoch: 73 | Batch_idx: 210 |  Loss: (0.3315) |  Loss2: (0.0000) | Acc: (88.00%) (23935/27008)
Epoch: 73 | Batch_idx: 220 |  Loss: (0.3304) |  Loss2: (0.0000) | Acc: (88.00%) (25074/28288)
Epoch: 73 | Batch_idx: 230 |  Loss: (0.3301) |  Loss2: (0.0000) | Acc: (88.00%) (26211/29568)
Epoch: 73 | Batch_idx: 240 |  Loss: (0.3299) |  Loss2: (0.0000) | Acc: (88.00%) (27342/30848)
Epoch: 73 | Batch_idx: 250 |  Loss: (0.3288) |  Loss2: (0.0000) | Acc: (88.00%) (28488/32128)
Epoch: 73 | Batch_idx: 260 |  Loss: (0.3287) |  Loss2: (0.0000) | Acc: (88.00%) (29625/33408)
Epoch: 73 | Batch_idx: 270 |  Loss: (0.3300) |  Loss2: (0.0000) | Acc: (88.00%) (30749/34688)
Epoch: 73 | Batch_idx: 280 |  Loss: (0.3306) |  Loss2: (0.0000) | Acc: (88.00%) (31886/35968)
Epoch: 73 | Batch_idx: 290 |  Loss: (0.3304) |  Loss2: (0.0000) | Acc: (88.00%) (33028/37248)
Epoch: 73 | Batch_idx: 300 |  Loss: (0.3314) |  Loss2: (0.0000) | Acc: (88.00%) (34145/38528)
Epoch: 73 | Batch_idx: 310 |  Loss: (0.3303) |  Loss2: (0.0000) | Acc: (88.00%) (35289/39808)
Epoch: 73 | Batch_idx: 320 |  Loss: (0.3309) |  Loss2: (0.0000) | Acc: (88.00%) (36415/41088)
Epoch: 73 | Batch_idx: 330 |  Loss: (0.3304) |  Loss2: (0.0000) | Acc: (88.00%) (37551/42368)
Epoch: 73 | Batch_idx: 340 |  Loss: (0.3287) |  Loss2: (0.0000) | Acc: (88.00%) (38714/43648)
Epoch: 73 | Batch_idx: 350 |  Loss: (0.3275) |  Loss2: (0.0000) | Acc: (88.00%) (39863/44928)
Epoch: 73 | Batch_idx: 360 |  Loss: (0.3268) |  Loss2: (0.0000) | Acc: (88.00%) (41023/46208)
Epoch: 73 | Batch_idx: 370 |  Loss: (0.3260) |  Loss2: (0.0000) | Acc: (88.00%) (42180/47488)
Epoch: 73 | Batch_idx: 380 |  Loss: (0.3247) |  Loss2: (0.0000) | Acc: (88.00%) (43331/48768)
Epoch: 73 | Batch_idx: 390 |  Loss: (0.3244) |  Loss2: (0.0000) | Acc: (88.00%) (44434/50000)
# TEST : Loss: (0.4301) | Acc: (85.00%) (8525/10000)
percent tensor([0.5592, 0.5737, 0.5535, 0.5451, 0.5563, 0.5457, 0.5741, 0.5619, 0.5705,
        0.5679, 0.5704, 0.5611, 0.5651, 0.5722, 0.5598, 0.5573],
       device='cuda:0') torch.Size([16])
percent tensor([0.4900, 0.4916, 0.4557, 0.4831, 0.4694, 0.4738, 0.4866, 0.4937, 0.4871,
        0.4780, 0.4784, 0.4587, 0.4935, 0.5037, 0.4878, 0.4866],
       device='cuda:0') torch.Size([16])
percent tensor([0.4569, 0.4207, 0.4084, 0.4676, 0.4149, 0.4936, 0.4383, 0.4643, 0.4565,
        0.4019, 0.4368, 0.3910, 0.4002, 0.5105, 0.4579, 0.4583],
       device='cuda:0') torch.Size([16])
percent tensor([0.5944, 0.5919, 0.5757, 0.5662, 0.5732, 0.5952, 0.5891, 0.5694, 0.5811,
        0.5906, 0.5853, 0.5819, 0.6067, 0.5717, 0.5944, 0.5971],
       device='cuda:0') torch.Size([16])
percent tensor([0.6366, 0.5975, 0.6045, 0.6177, 0.6201, 0.6686, 0.6206, 0.6011, 0.6857,
        0.6126, 0.6734, 0.6212, 0.6044, 0.7005, 0.6047, 0.6423],
       device='cuda:0') torch.Size([16])
percent tensor([0.6646, 0.6477, 0.6799, 0.6919, 0.7158, 0.7084, 0.6824, 0.6906, 0.6656,
        0.6498, 0.6522, 0.6646, 0.6337, 0.6771, 0.6660, 0.6863],
       device='cuda:0') torch.Size([16])
percent tensor([0.4832, 0.4808, 0.5590, 0.5226, 0.6085, 0.6840, 0.5108, 0.4249, 0.5139,
        0.4742, 0.4956, 0.4735, 0.4398, 0.5126, 0.3395, 0.5496],
       device='cuda:0') torch.Size([16])
percent tensor([0.9990, 0.9987, 0.9983, 0.9982, 0.9990, 0.9967, 0.9988, 0.9995, 0.9985,
        0.9990, 0.9991, 0.9989, 0.9991, 0.9989, 0.9991, 0.9994],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 74 | Batch_idx: 0 |  Loss: (0.4080) |  Loss2: (0.0000) | Acc: (86.00%) (111/128)
Epoch: 74 | Batch_idx: 10 |  Loss: (0.2862) |  Loss2: (0.0000) | Acc: (89.00%) (1265/1408)
Epoch: 74 | Batch_idx: 20 |  Loss: (0.2859) |  Loss2: (0.0000) | Acc: (89.00%) (2412/2688)
Epoch: 74 | Batch_idx: 30 |  Loss: (0.2886) |  Loss2: (0.0000) | Acc: (89.00%) (3569/3968)
Epoch: 74 | Batch_idx: 40 |  Loss: (0.2863) |  Loss2: (0.0000) | Acc: (90.00%) (4726/5248)
Epoch: 74 | Batch_idx: 50 |  Loss: (0.2839) |  Loss2: (0.0000) | Acc: (90.00%) (5879/6528)
Epoch: 74 | Batch_idx: 60 |  Loss: (0.2805) |  Loss2: (0.0000) | Acc: (90.00%) (7039/7808)
Epoch: 74 | Batch_idx: 70 |  Loss: (0.2777) |  Loss2: (0.0000) | Acc: (90.00%) (8206/9088)
Epoch: 74 | Batch_idx: 80 |  Loss: (0.2779) |  Loss2: (0.0000) | Acc: (90.00%) (9372/10368)
Epoch: 74 | Batch_idx: 90 |  Loss: (0.2822) |  Loss2: (0.0000) | Acc: (90.00%) (10504/11648)
Epoch: 74 | Batch_idx: 100 |  Loss: (0.2801) |  Loss2: (0.0000) | Acc: (90.00%) (11665/12928)
Epoch: 74 | Batch_idx: 110 |  Loss: (0.2771) |  Loss2: (0.0000) | Acc: (90.00%) (12841/14208)
Epoch: 74 | Batch_idx: 120 |  Loss: (0.2765) |  Loss2: (0.0000) | Acc: (90.00%) (14004/15488)
Epoch: 74 | Batch_idx: 130 |  Loss: (0.2783) |  Loss2: (0.0000) | Acc: (90.00%) (15155/16768)
Epoch: 74 | Batch_idx: 140 |  Loss: (0.2788) |  Loss2: (0.0000) | Acc: (90.00%) (16313/18048)
Epoch: 74 | Batch_idx: 150 |  Loss: (0.2795) |  Loss2: (0.0000) | Acc: (90.00%) (17453/19328)
Epoch: 74 | Batch_idx: 160 |  Loss: (0.2764) |  Loss2: (0.0000) | Acc: (90.00%) (18636/20608)
Epoch: 74 | Batch_idx: 170 |  Loss: (0.2789) |  Loss2: (0.0000) | Acc: (90.00%) (19769/21888)
Epoch: 74 | Batch_idx: 180 |  Loss: (0.2781) |  Loss2: (0.0000) | Acc: (90.00%) (20933/23168)
Epoch: 74 | Batch_idx: 190 |  Loss: (0.2787) |  Loss2: (0.0000) | Acc: (90.00%) (22082/24448)
Epoch: 74 | Batch_idx: 200 |  Loss: (0.2785) |  Loss2: (0.0000) | Acc: (90.00%) (23232/25728)
Epoch: 74 | Batch_idx: 210 |  Loss: (0.2777) |  Loss2: (0.0000) | Acc: (90.00%) (24394/27008)
Epoch: 74 | Batch_idx: 220 |  Loss: (0.2781) |  Loss2: (0.0000) | Acc: (90.00%) (25547/28288)
Epoch: 74 | Batch_idx: 230 |  Loss: (0.2783) |  Loss2: (0.0000) | Acc: (90.00%) (26701/29568)
Epoch: 74 | Batch_idx: 240 |  Loss: (0.2803) |  Loss2: (0.0000) | Acc: (90.00%) (27827/30848)
Epoch: 74 | Batch_idx: 250 |  Loss: (0.2819) |  Loss2: (0.0000) | Acc: (90.00%) (28955/32128)
Epoch: 74 | Batch_idx: 260 |  Loss: (0.2829) |  Loss2: (0.0000) | Acc: (90.00%) (30094/33408)
Epoch: 74 | Batch_idx: 270 |  Loss: (0.2826) |  Loss2: (0.0000) | Acc: (90.00%) (31245/34688)
Epoch: 74 | Batch_idx: 280 |  Loss: (0.2838) |  Loss2: (0.0000) | Acc: (90.00%) (32379/35968)
Epoch: 74 | Batch_idx: 290 |  Loss: (0.2841) |  Loss2: (0.0000) | Acc: (89.00%) (33521/37248)
Epoch: 74 | Batch_idx: 300 |  Loss: (0.2840) |  Loss2: (0.0000) | Acc: (90.00%) (34681/38528)
Epoch: 74 | Batch_idx: 310 |  Loss: (0.2851) |  Loss2: (0.0000) | Acc: (89.00%) (35811/39808)
Epoch: 74 | Batch_idx: 320 |  Loss: (0.2856) |  Loss2: (0.0000) | Acc: (89.00%) (36948/41088)
Epoch: 74 | Batch_idx: 330 |  Loss: (0.2861) |  Loss2: (0.0000) | Acc: (89.00%) (38098/42368)
Epoch: 74 | Batch_idx: 340 |  Loss: (0.2857) |  Loss2: (0.0000) | Acc: (89.00%) (39250/43648)
Epoch: 74 | Batch_idx: 350 |  Loss: (0.2861) |  Loss2: (0.0000) | Acc: (89.00%) (40409/44928)
Epoch: 74 | Batch_idx: 360 |  Loss: (0.2866) |  Loss2: (0.0000) | Acc: (89.00%) (41561/46208)
Epoch: 74 | Batch_idx: 370 |  Loss: (0.2868) |  Loss2: (0.0000) | Acc: (89.00%) (42717/47488)
Epoch: 74 | Batch_idx: 380 |  Loss: (0.2869) |  Loss2: (0.0000) | Acc: (89.00%) (43869/48768)
Epoch: 74 | Batch_idx: 390 |  Loss: (0.2870) |  Loss2: (0.0000) | Acc: (89.00%) (44973/50000)
# TEST : Loss: (0.4377) | Acc: (85.00%) (8522/10000)
percent tensor([0.5583, 0.5740, 0.5454, 0.5431, 0.5522, 0.5448, 0.5733, 0.5598, 0.5692,
        0.5672, 0.5707, 0.5554, 0.5643, 0.5724, 0.5601, 0.5564],
       device='cuda:0') torch.Size([16])
percent tensor([0.4934, 0.4904, 0.4620, 0.4823, 0.4762, 0.4796, 0.4883, 0.4957, 0.4886,
        0.4789, 0.4803, 0.4636, 0.4955, 0.5001, 0.4887, 0.4879],
       device='cuda:0') torch.Size([16])
percent tensor([0.4680, 0.4219, 0.4124, 0.4727, 0.4252, 0.5000, 0.4364, 0.4627, 0.4584,
        0.4071, 0.4386, 0.3965, 0.4082, 0.5104, 0.4616, 0.4665],
       device='cuda:0') torch.Size([16])
percent tensor([0.5944, 0.5947, 0.5716, 0.5643, 0.5698, 0.5983, 0.5903, 0.5670, 0.5790,
        0.5912, 0.5866, 0.5808, 0.6057, 0.5763, 0.5976, 0.5981],
       device='cuda:0') torch.Size([16])
percent tensor([0.6426, 0.5984, 0.6114, 0.6126, 0.6247, 0.6733, 0.6370, 0.5991, 0.6992,
        0.6291, 0.6859, 0.6476, 0.6167, 0.7168, 0.6061, 0.6434],
       device='cuda:0') torch.Size([16])
percent tensor([0.6634, 0.6506, 0.6804, 0.6964, 0.7182, 0.7114, 0.6826, 0.6915, 0.6692,
        0.6558, 0.6549, 0.6650, 0.6356, 0.6847, 0.6620, 0.6880],
       device='cuda:0') torch.Size([16])
percent tensor([0.4446, 0.4975, 0.5751, 0.5319, 0.6132, 0.6920, 0.5012, 0.4172, 0.5562,
        0.4962, 0.5066, 0.4917, 0.4467, 0.5272, 0.3474, 0.5209],
       device='cuda:0') torch.Size([16])
percent tensor([0.9994, 0.9988, 0.9981, 0.9987, 0.9988, 0.9966, 0.9988, 0.9994, 0.9992,
        0.9993, 0.9995, 0.9991, 0.9994, 0.9991, 0.9986, 0.9996],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 75 | Batch_idx: 0 |  Loss: (0.3003) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 75 | Batch_idx: 10 |  Loss: (0.3027) |  Loss2: (0.0000) | Acc: (89.00%) (1255/1408)
Epoch: 75 | Batch_idx: 20 |  Loss: (0.3215) |  Loss2: (0.0000) | Acc: (88.00%) (2388/2688)
Epoch: 75 | Batch_idx: 30 |  Loss: (0.3421) |  Loss2: (0.0000) | Acc: (88.00%) (3505/3968)
Epoch: 75 | Batch_idx: 40 |  Loss: (0.3508) |  Loss2: (0.0000) | Acc: (88.00%) (4628/5248)
Epoch: 75 | Batch_idx: 50 |  Loss: (0.3635) |  Loss2: (0.0000) | Acc: (87.00%) (5725/6528)
Epoch: 75 | Batch_idx: 60 |  Loss: (0.3712) |  Loss2: (0.0000) | Acc: (87.00%) (6819/7808)
Epoch: 75 | Batch_idx: 70 |  Loss: (0.3816) |  Loss2: (0.0000) | Acc: (86.00%) (7894/9088)
Epoch: 75 | Batch_idx: 80 |  Loss: (0.3824) |  Loss2: (0.0000) | Acc: (86.00%) (9002/10368)
Epoch: 75 | Batch_idx: 90 |  Loss: (0.3799) |  Loss2: (0.0000) | Acc: (86.00%) (10118/11648)
Epoch: 75 | Batch_idx: 100 |  Loss: (0.3814) |  Loss2: (0.0000) | Acc: (86.00%) (11227/12928)
Epoch: 75 | Batch_idx: 110 |  Loss: (0.3788) |  Loss2: (0.0000) | Acc: (86.00%) (12350/14208)
Epoch: 75 | Batch_idx: 120 |  Loss: (0.3783) |  Loss2: (0.0000) | Acc: (86.00%) (13466/15488)
Epoch: 75 | Batch_idx: 130 |  Loss: (0.3768) |  Loss2: (0.0000) | Acc: (86.00%) (14577/16768)
Epoch: 75 | Batch_idx: 140 |  Loss: (0.3763) |  Loss2: (0.0000) | Acc: (86.00%) (15691/18048)
Epoch: 75 | Batch_idx: 150 |  Loss: (0.3774) |  Loss2: (0.0000) | Acc: (86.00%) (16792/19328)
Epoch: 75 | Batch_idx: 160 |  Loss: (0.3764) |  Loss2: (0.0000) | Acc: (86.00%) (17908/20608)
Epoch: 75 | Batch_idx: 170 |  Loss: (0.3765) |  Loss2: (0.0000) | Acc: (86.00%) (19033/21888)
Epoch: 75 | Batch_idx: 180 |  Loss: (0.3746) |  Loss2: (0.0000) | Acc: (87.00%) (20164/23168)
Epoch: 75 | Batch_idx: 190 |  Loss: (0.3727) |  Loss2: (0.0000) | Acc: (87.00%) (21283/24448)
Epoch: 75 | Batch_idx: 200 |  Loss: (0.3716) |  Loss2: (0.0000) | Acc: (87.00%) (22406/25728)
Epoch: 75 | Batch_idx: 210 |  Loss: (0.3694) |  Loss2: (0.0000) | Acc: (87.00%) (23550/27008)
Epoch: 75 | Batch_idx: 220 |  Loss: (0.3679) |  Loss2: (0.0000) | Acc: (87.00%) (24679/28288)
Epoch: 75 | Batch_idx: 230 |  Loss: (0.3680) |  Loss2: (0.0000) | Acc: (87.00%) (25775/29568)
Epoch: 75 | Batch_idx: 240 |  Loss: (0.3670) |  Loss2: (0.0000) | Acc: (87.00%) (26905/30848)
Epoch: 75 | Batch_idx: 250 |  Loss: (0.3652) |  Loss2: (0.0000) | Acc: (87.00%) (28048/32128)
Epoch: 75 | Batch_idx: 260 |  Loss: (0.3647) |  Loss2: (0.0000) | Acc: (87.00%) (29177/33408)
Epoch: 75 | Batch_idx: 270 |  Loss: (0.3646) |  Loss2: (0.0000) | Acc: (87.00%) (30295/34688)
Epoch: 75 | Batch_idx: 280 |  Loss: (0.3642) |  Loss2: (0.0000) | Acc: (87.00%) (31417/35968)
Epoch: 75 | Batch_idx: 290 |  Loss: (0.3635) |  Loss2: (0.0000) | Acc: (87.00%) (32537/37248)
Epoch: 75 | Batch_idx: 300 |  Loss: (0.3621) |  Loss2: (0.0000) | Acc: (87.00%) (33682/38528)
Epoch: 75 | Batch_idx: 310 |  Loss: (0.3605) |  Loss2: (0.0000) | Acc: (87.00%) (34821/39808)
Epoch: 75 | Batch_idx: 320 |  Loss: (0.3605) |  Loss2: (0.0000) | Acc: (87.00%) (35943/41088)
Epoch: 75 | Batch_idx: 330 |  Loss: (0.3588) |  Loss2: (0.0000) | Acc: (87.00%) (37088/42368)
Epoch: 75 | Batch_idx: 340 |  Loss: (0.3576) |  Loss2: (0.0000) | Acc: (87.00%) (38222/43648)
Epoch: 75 | Batch_idx: 350 |  Loss: (0.3580) |  Loss2: (0.0000) | Acc: (87.00%) (39343/44928)
Epoch: 75 | Batch_idx: 360 |  Loss: (0.3567) |  Loss2: (0.0000) | Acc: (87.00%) (40481/46208)
Epoch: 75 | Batch_idx: 370 |  Loss: (0.3558) |  Loss2: (0.0000) | Acc: (87.00%) (41618/47488)
Epoch: 75 | Batch_idx: 380 |  Loss: (0.3549) |  Loss2: (0.0000) | Acc: (87.00%) (42759/48768)
Epoch: 75 | Batch_idx: 390 |  Loss: (0.3546) |  Loss2: (0.0000) | Acc: (87.00%) (43845/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_075.pth.tar'
# TEST : Loss: (0.4561) | Acc: (84.00%) (8451/10000)
percent tensor([0.5598, 0.5741, 0.5544, 0.5492, 0.5615, 0.5486, 0.5755, 0.5670, 0.5711,
        0.5696, 0.5710, 0.5642, 0.5661, 0.5710, 0.5634, 0.5583],
       device='cuda:0') torch.Size([16])
percent tensor([0.5140, 0.5099, 0.4898, 0.5094, 0.5003, 0.5026, 0.5075, 0.5150, 0.5103,
        0.5027, 0.5019, 0.4922, 0.5114, 0.5194, 0.5102, 0.5086],
       device='cuda:0') torch.Size([16])
percent tensor([0.4984, 0.4683, 0.4222, 0.5037, 0.4241, 0.5272, 0.4587, 0.4719, 0.4814,
        0.4487, 0.4821, 0.4249, 0.4489, 0.5382, 0.4995, 0.5061],
       device='cuda:0') torch.Size([16])
percent tensor([0.5761, 0.5790, 0.5527, 0.5485, 0.5503, 0.5771, 0.5702, 0.5481, 0.5658,
        0.5772, 0.5721, 0.5643, 0.5910, 0.5627, 0.5769, 0.5808],
       device='cuda:0') torch.Size([16])
percent tensor([0.6165, 0.5366, 0.6236, 0.6521, 0.6498, 0.6706, 0.6080, 0.6241, 0.7158,
        0.5990, 0.6698, 0.6503, 0.5308, 0.7230, 0.5672, 0.6299],
       device='cuda:0') torch.Size([16])
percent tensor([0.7297, 0.7136, 0.7380, 0.7570, 0.7811, 0.7742, 0.7474, 0.7646, 0.7311,
        0.7188, 0.7196, 0.7227, 0.6988, 0.7489, 0.7314, 0.7564],
       device='cuda:0') torch.Size([16])
percent tensor([0.4414, 0.5008, 0.5493, 0.4794, 0.5808, 0.6950, 0.4943, 0.3879, 0.5597,
        0.4878, 0.5059, 0.4552, 0.4424, 0.5089, 0.3450, 0.5351],
       device='cuda:0') torch.Size([16])
percent tensor([0.9993, 0.9987, 0.9984, 0.9989, 0.9990, 0.9943, 0.9989, 0.9995, 0.9991,
        0.9993, 0.9996, 0.9993, 0.9992, 0.9988, 0.9986, 0.9995],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 76 | Batch_idx: 0 |  Loss: (0.2066) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 76 | Batch_idx: 10 |  Loss: (0.2994) |  Loss2: (0.0000) | Acc: (90.00%) (1269/1408)
Epoch: 76 | Batch_idx: 20 |  Loss: (0.2909) |  Loss2: (0.0000) | Acc: (89.00%) (2416/2688)
Epoch: 76 | Batch_idx: 30 |  Loss: (0.2777) |  Loss2: (0.0000) | Acc: (90.00%) (3592/3968)
Epoch: 76 | Batch_idx: 40 |  Loss: (0.2776) |  Loss2: (0.0000) | Acc: (90.00%) (4758/5248)
Epoch: 76 | Batch_idx: 50 |  Loss: (0.2817) |  Loss2: (0.0000) | Acc: (90.00%) (5901/6528)
Epoch: 76 | Batch_idx: 60 |  Loss: (0.2818) |  Loss2: (0.0000) | Acc: (90.00%) (7053/7808)
Epoch: 76 | Batch_idx: 70 |  Loss: (0.2814) |  Loss2: (0.0000) | Acc: (90.00%) (8220/9088)
Epoch: 76 | Batch_idx: 80 |  Loss: (0.2792) |  Loss2: (0.0000) | Acc: (90.00%) (9387/10368)
Epoch: 76 | Batch_idx: 90 |  Loss: (0.2789) |  Loss2: (0.0000) | Acc: (90.00%) (10539/11648)
Epoch: 76 | Batch_idx: 100 |  Loss: (0.2771) |  Loss2: (0.0000) | Acc: (90.00%) (11713/12928)
Epoch: 76 | Batch_idx: 110 |  Loss: (0.2752) |  Loss2: (0.0000) | Acc: (90.00%) (12879/14208)
Epoch: 76 | Batch_idx: 120 |  Loss: (0.2757) |  Loss2: (0.0000) | Acc: (90.00%) (14040/15488)
Epoch: 76 | Batch_idx: 130 |  Loss: (0.2753) |  Loss2: (0.0000) | Acc: (90.00%) (15208/16768)
Epoch: 76 | Batch_idx: 140 |  Loss: (0.2752) |  Loss2: (0.0000) | Acc: (90.00%) (16382/18048)
Epoch: 76 | Batch_idx: 150 |  Loss: (0.2738) |  Loss2: (0.0000) | Acc: (90.00%) (17540/19328)
Epoch: 76 | Batch_idx: 160 |  Loss: (0.2747) |  Loss2: (0.0000) | Acc: (90.00%) (18703/20608)
Epoch: 76 | Batch_idx: 170 |  Loss: (0.2755) |  Loss2: (0.0000) | Acc: (90.00%) (19858/21888)
Epoch: 76 | Batch_idx: 180 |  Loss: (0.2747) |  Loss2: (0.0000) | Acc: (90.00%) (21021/23168)
Epoch: 76 | Batch_idx: 190 |  Loss: (0.2737) |  Loss2: (0.0000) | Acc: (90.00%) (22195/24448)
Epoch: 76 | Batch_idx: 200 |  Loss: (0.2753) |  Loss2: (0.0000) | Acc: (90.00%) (23339/25728)
Epoch: 76 | Batch_idx: 210 |  Loss: (0.2755) |  Loss2: (0.0000) | Acc: (90.00%) (24494/27008)
Epoch: 76 | Batch_idx: 220 |  Loss: (0.2772) |  Loss2: (0.0000) | Acc: (90.00%) (25636/28288)
Epoch: 76 | Batch_idx: 230 |  Loss: (0.2777) |  Loss2: (0.0000) | Acc: (90.00%) (26784/29568)
Epoch: 76 | Batch_idx: 240 |  Loss: (0.2772) |  Loss2: (0.0000) | Acc: (90.00%) (27941/30848)
Epoch: 76 | Batch_idx: 250 |  Loss: (0.2767) |  Loss2: (0.0000) | Acc: (90.00%) (29103/32128)
Epoch: 76 | Batch_idx: 260 |  Loss: (0.2787) |  Loss2: (0.0000) | Acc: (90.00%) (30231/33408)
Epoch: 76 | Batch_idx: 270 |  Loss: (0.2782) |  Loss2: (0.0000) | Acc: (90.00%) (31390/34688)
Epoch: 76 | Batch_idx: 280 |  Loss: (0.2773) |  Loss2: (0.0000) | Acc: (90.00%) (32562/35968)
Epoch: 76 | Batch_idx: 290 |  Loss: (0.2777) |  Loss2: (0.0000) | Acc: (90.00%) (33719/37248)
Epoch: 76 | Batch_idx: 300 |  Loss: (0.2779) |  Loss2: (0.0000) | Acc: (90.00%) (34867/38528)
Epoch: 76 | Batch_idx: 310 |  Loss: (0.2778) |  Loss2: (0.0000) | Acc: (90.00%) (36028/39808)
Epoch: 76 | Batch_idx: 320 |  Loss: (0.2786) |  Loss2: (0.0000) | Acc: (90.00%) (37179/41088)
Epoch: 76 | Batch_idx: 330 |  Loss: (0.2786) |  Loss2: (0.0000) | Acc: (90.00%) (38329/42368)
Epoch: 76 | Batch_idx: 340 |  Loss: (0.2791) |  Loss2: (0.0000) | Acc: (90.00%) (39479/43648)
Epoch: 76 | Batch_idx: 350 |  Loss: (0.2801) |  Loss2: (0.0000) | Acc: (90.00%) (40616/44928)
Epoch: 76 | Batch_idx: 360 |  Loss: (0.2799) |  Loss2: (0.0000) | Acc: (90.00%) (41775/46208)
Epoch: 76 | Batch_idx: 370 |  Loss: (0.2807) |  Loss2: (0.0000) | Acc: (90.00%) (42918/47488)
Epoch: 76 | Batch_idx: 380 |  Loss: (0.2805) |  Loss2: (0.0000) | Acc: (90.00%) (44078/48768)
Epoch: 76 | Batch_idx: 390 |  Loss: (0.2807) |  Loss2: (0.0000) | Acc: (90.00%) (45184/50000)
# TEST : Loss: (0.4263) | Acc: (85.00%) (8597/10000)
percent tensor([0.5588, 0.5743, 0.5531, 0.5499, 0.5579, 0.5477, 0.5742, 0.5657, 0.5696,
        0.5684, 0.5698, 0.5613, 0.5648, 0.5731, 0.5627, 0.5579],
       device='cuda:0') torch.Size([16])
percent tensor([0.5137, 0.5132, 0.4845, 0.5067, 0.4970, 0.5019, 0.5095, 0.5130, 0.5106,
        0.5037, 0.5046, 0.4875, 0.5116, 0.5226, 0.5108, 0.5095],
       device='cuda:0') torch.Size([16])
percent tensor([0.4973, 0.4703, 0.4258, 0.4910, 0.4230, 0.5257, 0.4662, 0.4733, 0.4889,
        0.4488, 0.4868, 0.4290, 0.4484, 0.5394, 0.4986, 0.5037],
       device='cuda:0') torch.Size([16])
percent tensor([0.5784, 0.5802, 0.5591, 0.5485, 0.5554, 0.5816, 0.5733, 0.5502, 0.5683,
        0.5780, 0.5750, 0.5687, 0.5936, 0.5649, 0.5767, 0.5812],
       device='cuda:0') torch.Size([16])
percent tensor([0.6092, 0.5580, 0.6159, 0.6363, 0.6420, 0.6537, 0.6167, 0.6133, 0.6932,
        0.6049, 0.6531, 0.6346, 0.5419, 0.7125, 0.5702, 0.6146],
       device='cuda:0') torch.Size([16])
percent tensor([0.7360, 0.7181, 0.7416, 0.7464, 0.7801, 0.7722, 0.7515, 0.7569, 0.7315,
        0.7248, 0.7284, 0.7243, 0.7049, 0.7510, 0.7340, 0.7543],
       device='cuda:0') torch.Size([16])
percent tensor([0.4606, 0.4898, 0.5376, 0.4991, 0.5799, 0.7078, 0.5026, 0.3772, 0.5673,
        0.5026, 0.5318, 0.4498, 0.4579, 0.5337, 0.3375, 0.5151],
       device='cuda:0') torch.Size([16])
percent tensor([0.9992, 0.9985, 0.9986, 0.9991, 0.9990, 0.9978, 0.9990, 0.9997, 0.9988,
        0.9993, 0.9994, 0.9990, 0.9989, 0.9989, 0.9989, 0.9994],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 77 | Batch_idx: 0 |  Loss: (0.2970) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 77 | Batch_idx: 10 |  Loss: (0.2747) |  Loss2: (0.0000) | Acc: (90.00%) (1274/1408)
Epoch: 77 | Batch_idx: 20 |  Loss: (0.2923) |  Loss2: (0.0000) | Acc: (89.00%) (2415/2688)
Epoch: 77 | Batch_idx: 30 |  Loss: (0.3002) |  Loss2: (0.0000) | Acc: (89.00%) (3553/3968)
Epoch: 77 | Batch_idx: 40 |  Loss: (0.3020) |  Loss2: (0.0000) | Acc: (89.00%) (4688/5248)
Epoch: 77 | Batch_idx: 50 |  Loss: (0.3094) |  Loss2: (0.0000) | Acc: (88.00%) (5808/6528)
Epoch: 77 | Batch_idx: 60 |  Loss: (0.3134) |  Loss2: (0.0000) | Acc: (88.00%) (6948/7808)
Epoch: 77 | Batch_idx: 70 |  Loss: (0.3200) |  Loss2: (0.0000) | Acc: (88.00%) (8072/9088)
Epoch: 77 | Batch_idx: 80 |  Loss: (0.3186) |  Loss2: (0.0000) | Acc: (88.00%) (9218/10368)
Epoch: 77 | Batch_idx: 90 |  Loss: (0.3190) |  Loss2: (0.0000) | Acc: (88.00%) (10348/11648)
Epoch: 77 | Batch_idx: 100 |  Loss: (0.3176) |  Loss2: (0.0000) | Acc: (88.00%) (11494/12928)
Epoch: 77 | Batch_idx: 110 |  Loss: (0.3158) |  Loss2: (0.0000) | Acc: (88.00%) (12635/14208)
Epoch: 77 | Batch_idx: 120 |  Loss: (0.3131) |  Loss2: (0.0000) | Acc: (89.00%) (13787/15488)
Epoch: 77 | Batch_idx: 130 |  Loss: (0.3115) |  Loss2: (0.0000) | Acc: (89.00%) (14941/16768)
Epoch: 77 | Batch_idx: 140 |  Loss: (0.3130) |  Loss2: (0.0000) | Acc: (89.00%) (16078/18048)
Epoch: 77 | Batch_idx: 150 |  Loss: (0.3169) |  Loss2: (0.0000) | Acc: (88.00%) (17193/19328)
Epoch: 77 | Batch_idx: 160 |  Loss: (0.3150) |  Loss2: (0.0000) | Acc: (89.00%) (18351/20608)
Epoch: 77 | Batch_idx: 170 |  Loss: (0.3133) |  Loss2: (0.0000) | Acc: (89.00%) (19499/21888)
Epoch: 77 | Batch_idx: 180 |  Loss: (0.3122) |  Loss2: (0.0000) | Acc: (89.00%) (20644/23168)
Epoch: 77 | Batch_idx: 190 |  Loss: (0.3117) |  Loss2: (0.0000) | Acc: (89.00%) (21786/24448)
Epoch: 77 | Batch_idx: 200 |  Loss: (0.3108) |  Loss2: (0.0000) | Acc: (89.00%) (22932/25728)
Epoch: 77 | Batch_idx: 210 |  Loss: (0.3087) |  Loss2: (0.0000) | Acc: (89.00%) (24079/27008)
Epoch: 77 | Batch_idx: 220 |  Loss: (0.3089) |  Loss2: (0.0000) | Acc: (89.00%) (25222/28288)
Epoch: 77 | Batch_idx: 230 |  Loss: (0.3083) |  Loss2: (0.0000) | Acc: (89.00%) (26377/29568)
Epoch: 77 | Batch_idx: 240 |  Loss: (0.3079) |  Loss2: (0.0000) | Acc: (89.00%) (27515/30848)
Epoch: 77 | Batch_idx: 250 |  Loss: (0.3080) |  Loss2: (0.0000) | Acc: (89.00%) (28664/32128)
Epoch: 77 | Batch_idx: 260 |  Loss: (0.3063) |  Loss2: (0.0000) | Acc: (89.00%) (29829/33408)
Epoch: 77 | Batch_idx: 270 |  Loss: (0.3061) |  Loss2: (0.0000) | Acc: (89.00%) (30975/34688)
Epoch: 77 | Batch_idx: 280 |  Loss: (0.3049) |  Loss2: (0.0000) | Acc: (89.00%) (32147/35968)
Epoch: 77 | Batch_idx: 290 |  Loss: (0.3038) |  Loss2: (0.0000) | Acc: (89.00%) (33294/37248)
Epoch: 77 | Batch_idx: 300 |  Loss: (0.3029) |  Loss2: (0.0000) | Acc: (89.00%) (34453/38528)
Epoch: 77 | Batch_idx: 310 |  Loss: (0.3027) |  Loss2: (0.0000) | Acc: (89.00%) (35604/39808)
Epoch: 77 | Batch_idx: 320 |  Loss: (0.3025) |  Loss2: (0.0000) | Acc: (89.00%) (36763/41088)
Epoch: 77 | Batch_idx: 330 |  Loss: (0.3028) |  Loss2: (0.0000) | Acc: (89.00%) (37891/42368)
Epoch: 77 | Batch_idx: 340 |  Loss: (0.3022) |  Loss2: (0.0000) | Acc: (89.00%) (39047/43648)
Epoch: 77 | Batch_idx: 350 |  Loss: (0.3024) |  Loss2: (0.0000) | Acc: (89.00%) (40189/44928)
Epoch: 77 | Batch_idx: 360 |  Loss: (0.3025) |  Loss2: (0.0000) | Acc: (89.00%) (41341/46208)
Epoch: 77 | Batch_idx: 370 |  Loss: (0.3021) |  Loss2: (0.0000) | Acc: (89.00%) (42493/47488)
Epoch: 77 | Batch_idx: 380 |  Loss: (0.3022) |  Loss2: (0.0000) | Acc: (89.00%) (43637/48768)
Epoch: 77 | Batch_idx: 390 |  Loss: (0.3021) |  Loss2: (0.0000) | Acc: (89.00%) (44734/50000)
# TEST : Loss: (0.4236) | Acc: (86.00%) (8608/10000)
percent tensor([0.5570, 0.5710, 0.5515, 0.5473, 0.5555, 0.5441, 0.5711, 0.5631, 0.5673,
        0.5660, 0.5673, 0.5589, 0.5627, 0.5689, 0.5599, 0.5556],
       device='cuda:0') torch.Size([16])
percent tensor([0.5161, 0.5178, 0.4789, 0.5061, 0.4945, 0.5023, 0.5131, 0.5165, 0.5134,
        0.5033, 0.5063, 0.4827, 0.5144, 0.5292, 0.5143, 0.5106],
       device='cuda:0') torch.Size([16])
percent tensor([0.5033, 0.4789, 0.4218, 0.4938, 0.4216, 0.5317, 0.4762, 0.4786, 0.4974,
        0.4539, 0.4958, 0.4295, 0.4575, 0.5502, 0.5076, 0.5130],
       device='cuda:0') torch.Size([16])
percent tensor([0.5907, 0.5970, 0.5619, 0.5556, 0.5594, 0.6026, 0.5813, 0.5531, 0.5770,
        0.5934, 0.5914, 0.5768, 0.6115, 0.5752, 0.5904, 0.6010],
       device='cuda:0') torch.Size([16])
percent tensor([0.6439, 0.5978, 0.6201, 0.6569, 0.6435, 0.6947, 0.6290, 0.6081, 0.7318,
        0.6462, 0.6995, 0.6579, 0.6011, 0.7371, 0.5944, 0.6641],
       device='cuda:0') torch.Size([16])
percent tensor([0.7091, 0.6948, 0.7104, 0.7224, 0.7474, 0.7577, 0.7217, 0.7202, 0.7116,
        0.6955, 0.7086, 0.6962, 0.6826, 0.7250, 0.7012, 0.7337],
       device='cuda:0') torch.Size([16])
percent tensor([0.5196, 0.5236, 0.5602, 0.5397, 0.5988, 0.7423, 0.5476, 0.4123, 0.5978,
        0.5194, 0.5734, 0.4534, 0.4888, 0.5520, 0.3497, 0.5701],
       device='cuda:0') torch.Size([16])
percent tensor([0.9991, 0.9985, 0.9985, 0.9990, 0.9992, 0.9976, 0.9987, 0.9997, 0.9992,
        0.9994, 0.9994, 0.9992, 0.9992, 0.9987, 0.9988, 0.9994],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 78 | Batch_idx: 0 |  Loss: (0.2238) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 78 | Batch_idx: 10 |  Loss: (0.2638) |  Loss2: (0.0000) | Acc: (91.00%) (1287/1408)
Epoch: 78 | Batch_idx: 20 |  Loss: (0.2609) |  Loss2: (0.0000) | Acc: (90.00%) (2446/2688)
Epoch: 78 | Batch_idx: 30 |  Loss: (0.2596) |  Loss2: (0.0000) | Acc: (91.00%) (3618/3968)
Epoch: 78 | Batch_idx: 40 |  Loss: (0.2664) |  Loss2: (0.0000) | Acc: (91.00%) (4779/5248)
Epoch: 78 | Batch_idx: 50 |  Loss: (0.2695) |  Loss2: (0.0000) | Acc: (90.00%) (5932/6528)
Epoch: 78 | Batch_idx: 60 |  Loss: (0.2724) |  Loss2: (0.0000) | Acc: (90.00%) (7080/7808)
Epoch: 78 | Batch_idx: 70 |  Loss: (0.2732) |  Loss2: (0.0000) | Acc: (90.00%) (8226/9088)
Epoch: 78 | Batch_idx: 80 |  Loss: (0.2701) |  Loss2: (0.0000) | Acc: (90.00%) (9388/10368)
Epoch: 78 | Batch_idx: 90 |  Loss: (0.2713) |  Loss2: (0.0000) | Acc: (90.00%) (10551/11648)
Epoch: 78 | Batch_idx: 100 |  Loss: (0.2687) |  Loss2: (0.0000) | Acc: (90.00%) (11721/12928)
Epoch: 78 | Batch_idx: 110 |  Loss: (0.2696) |  Loss2: (0.0000) | Acc: (90.00%) (12869/14208)
Epoch: 78 | Batch_idx: 120 |  Loss: (0.2681) |  Loss2: (0.0000) | Acc: (90.00%) (14043/15488)
Epoch: 78 | Batch_idx: 130 |  Loss: (0.2691) |  Loss2: (0.0000) | Acc: (90.00%) (15192/16768)
Epoch: 78 | Batch_idx: 140 |  Loss: (0.2703) |  Loss2: (0.0000) | Acc: (90.00%) (16350/18048)
Epoch: 78 | Batch_idx: 150 |  Loss: (0.2724) |  Loss2: (0.0000) | Acc: (90.00%) (17490/19328)
Epoch: 78 | Batch_idx: 160 |  Loss: (0.2723) |  Loss2: (0.0000) | Acc: (90.00%) (18646/20608)
Epoch: 78 | Batch_idx: 170 |  Loss: (0.2712) |  Loss2: (0.0000) | Acc: (90.00%) (19808/21888)
Epoch: 78 | Batch_idx: 180 |  Loss: (0.2713) |  Loss2: (0.0000) | Acc: (90.00%) (20972/23168)
Epoch: 78 | Batch_idx: 190 |  Loss: (0.2710) |  Loss2: (0.0000) | Acc: (90.00%) (22127/24448)
Epoch: 78 | Batch_idx: 200 |  Loss: (0.2688) |  Loss2: (0.0000) | Acc: (90.00%) (23312/25728)
Epoch: 78 | Batch_idx: 210 |  Loss: (0.2690) |  Loss2: (0.0000) | Acc: (90.00%) (24472/27008)
Epoch: 78 | Batch_idx: 220 |  Loss: (0.2685) |  Loss2: (0.0000) | Acc: (90.00%) (25635/28288)
Epoch: 78 | Batch_idx: 230 |  Loss: (0.2679) |  Loss2: (0.0000) | Acc: (90.00%) (26808/29568)
Epoch: 78 | Batch_idx: 240 |  Loss: (0.2675) |  Loss2: (0.0000) | Acc: (90.00%) (27971/30848)
Epoch: 78 | Batch_idx: 250 |  Loss: (0.2671) |  Loss2: (0.0000) | Acc: (90.00%) (29130/32128)
Epoch: 78 | Batch_idx: 260 |  Loss: (0.2679) |  Loss2: (0.0000) | Acc: (90.00%) (30294/33408)
Epoch: 78 | Batch_idx: 270 |  Loss: (0.2686) |  Loss2: (0.0000) | Acc: (90.00%) (31447/34688)
Epoch: 78 | Batch_idx: 280 |  Loss: (0.2683) |  Loss2: (0.0000) | Acc: (90.00%) (32608/35968)
Epoch: 78 | Batch_idx: 290 |  Loss: (0.2680) |  Loss2: (0.0000) | Acc: (90.00%) (33768/37248)
Epoch: 78 | Batch_idx: 300 |  Loss: (0.2688) |  Loss2: (0.0000) | Acc: (90.00%) (34918/38528)
Epoch: 78 | Batch_idx: 310 |  Loss: (0.2690) |  Loss2: (0.0000) | Acc: (90.00%) (36063/39808)
Epoch: 78 | Batch_idx: 320 |  Loss: (0.2690) |  Loss2: (0.0000) | Acc: (90.00%) (37217/41088)
Epoch: 78 | Batch_idx: 330 |  Loss: (0.2703) |  Loss2: (0.0000) | Acc: (90.00%) (38350/42368)
Epoch: 78 | Batch_idx: 340 |  Loss: (0.2697) |  Loss2: (0.0000) | Acc: (90.00%) (39525/43648)
Epoch: 78 | Batch_idx: 350 |  Loss: (0.2693) |  Loss2: (0.0000) | Acc: (90.00%) (40696/44928)
Epoch: 78 | Batch_idx: 360 |  Loss: (0.2696) |  Loss2: (0.0000) | Acc: (90.00%) (41844/46208)
Epoch: 78 | Batch_idx: 370 |  Loss: (0.2699) |  Loss2: (0.0000) | Acc: (90.00%) (43009/47488)
Epoch: 78 | Batch_idx: 380 |  Loss: (0.2708) |  Loss2: (0.0000) | Acc: (90.00%) (44154/48768)
Epoch: 78 | Batch_idx: 390 |  Loss: (0.2708) |  Loss2: (0.0000) | Acc: (90.00%) (45273/50000)
# TEST : Loss: (0.4414) | Acc: (85.00%) (8532/10000)
percent tensor([0.5573, 0.5710, 0.5543, 0.5480, 0.5583, 0.5443, 0.5723, 0.5650, 0.5684,
        0.5669, 0.5675, 0.5621, 0.5632, 0.5700, 0.5600, 0.5558],
       device='cuda:0') torch.Size([16])
percent tensor([0.5154, 0.5167, 0.4857, 0.5134, 0.5002, 0.5016, 0.5140, 0.5192, 0.5129,
        0.5039, 0.5039, 0.4879, 0.5144, 0.5273, 0.5144, 0.5108],
       device='cuda:0') torch.Size([16])
percent tensor([0.4960, 0.4786, 0.4243, 0.5040, 0.4255, 0.5307, 0.4726, 0.4811, 0.4926,
        0.4505, 0.4898, 0.4246, 0.4507, 0.5494, 0.5069, 0.5106],
       device='cuda:0') torch.Size([16])
percent tensor([0.5921, 0.5971, 0.5731, 0.5603, 0.5678, 0.6025, 0.5871, 0.5552, 0.5823,
        0.5983, 0.5926, 0.5851, 0.6150, 0.5783, 0.5917, 0.6039],
       device='cuda:0') torch.Size([16])
percent tensor([0.6483, 0.6036, 0.6220, 0.6403, 0.6465, 0.6904, 0.6526, 0.6102, 0.7373,
        0.6553, 0.7255, 0.6695, 0.6052, 0.7590, 0.5992, 0.6666],
       device='cuda:0') torch.Size([16])
percent tensor([0.7021, 0.6890, 0.7208, 0.7345, 0.7521, 0.7547, 0.7217, 0.7239, 0.7064,
        0.7003, 0.7062, 0.7047, 0.6774, 0.7362, 0.6979, 0.7316],
       device='cuda:0') torch.Size([16])
percent tensor([0.4961, 0.5081, 0.5872, 0.5592, 0.6211, 0.7282, 0.5351, 0.4335, 0.5734,
        0.5170, 0.5396, 0.4797, 0.4665, 0.5496, 0.3690, 0.5728],
       device='cuda:0') torch.Size([16])
percent tensor([0.9992, 0.9984, 0.9982, 0.9987, 0.9993, 0.9978, 0.9989, 0.9994, 0.9992,
        0.9984, 0.9992, 0.9983, 0.9985, 0.9990, 0.9987, 0.9995],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 79 | Batch_idx: 0 |  Loss: (0.3157) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 79 | Batch_idx: 10 |  Loss: (0.2808) |  Loss2: (0.0000) | Acc: (90.00%) (1275/1408)
Epoch: 79 | Batch_idx: 20 |  Loss: (0.2901) |  Loss2: (0.0000) | Acc: (89.00%) (2418/2688)
Epoch: 79 | Batch_idx: 30 |  Loss: (0.3014) |  Loss2: (0.0000) | Acc: (89.00%) (3563/3968)
Epoch: 79 | Batch_idx: 40 |  Loss: (0.3177) |  Loss2: (0.0000) | Acc: (89.00%) (4676/5248)
Epoch: 79 | Batch_idx: 50 |  Loss: (0.3109) |  Loss2: (0.0000) | Acc: (89.00%) (5831/6528)
Epoch: 79 | Batch_idx: 60 |  Loss: (0.3104) |  Loss2: (0.0000) | Acc: (89.00%) (6970/7808)
Epoch: 79 | Batch_idx: 70 |  Loss: (0.3208) |  Loss2: (0.0000) | Acc: (88.00%) (8081/9088)
Epoch: 79 | Batch_idx: 80 |  Loss: (0.3239) |  Loss2: (0.0000) | Acc: (88.00%) (9217/10368)
Epoch: 79 | Batch_idx: 90 |  Loss: (0.3205) |  Loss2: (0.0000) | Acc: (89.00%) (10378/11648)
Epoch: 79 | Batch_idx: 100 |  Loss: (0.3194) |  Loss2: (0.0000) | Acc: (89.00%) (11518/12928)
Epoch: 79 | Batch_idx: 110 |  Loss: (0.3195) |  Loss2: (0.0000) | Acc: (89.00%) (12652/14208)
Epoch: 79 | Batch_idx: 120 |  Loss: (0.3185) |  Loss2: (0.0000) | Acc: (89.00%) (13802/15488)
Epoch: 79 | Batch_idx: 130 |  Loss: (0.3174) |  Loss2: (0.0000) | Acc: (89.00%) (14940/16768)
Epoch: 79 | Batch_idx: 140 |  Loss: (0.3173) |  Loss2: (0.0000) | Acc: (89.00%) (16078/18048)
Epoch: 79 | Batch_idx: 150 |  Loss: (0.3184) |  Loss2: (0.0000) | Acc: (89.00%) (17210/19328)
Epoch: 79 | Batch_idx: 160 |  Loss: (0.3191) |  Loss2: (0.0000) | Acc: (89.00%) (18353/20608)
Epoch: 79 | Batch_idx: 170 |  Loss: (0.3182) |  Loss2: (0.0000) | Acc: (89.00%) (19499/21888)
Epoch: 79 | Batch_idx: 180 |  Loss: (0.3161) |  Loss2: (0.0000) | Acc: (89.00%) (20652/23168)
Epoch: 79 | Batch_idx: 190 |  Loss: (0.3165) |  Loss2: (0.0000) | Acc: (89.00%) (21787/24448)
Epoch: 79 | Batch_idx: 200 |  Loss: (0.3158) |  Loss2: (0.0000) | Acc: (89.00%) (22924/25728)
Epoch: 79 | Batch_idx: 210 |  Loss: (0.3157) |  Loss2: (0.0000) | Acc: (89.00%) (24067/27008)
Epoch: 79 | Batch_idx: 220 |  Loss: (0.3143) |  Loss2: (0.0000) | Acc: (89.00%) (25220/28288)
Epoch: 79 | Batch_idx: 230 |  Loss: (0.3134) |  Loss2: (0.0000) | Acc: (89.00%) (26370/29568)
Epoch: 79 | Batch_idx: 240 |  Loss: (0.3129) |  Loss2: (0.0000) | Acc: (89.00%) (27518/30848)
Epoch: 79 | Batch_idx: 250 |  Loss: (0.3124) |  Loss2: (0.0000) | Acc: (89.00%) (28653/32128)
Epoch: 79 | Batch_idx: 260 |  Loss: (0.3119) |  Loss2: (0.0000) | Acc: (89.00%) (29790/33408)
Epoch: 79 | Batch_idx: 270 |  Loss: (0.3123) |  Loss2: (0.0000) | Acc: (89.00%) (30922/34688)
Epoch: 79 | Batch_idx: 280 |  Loss: (0.3120) |  Loss2: (0.0000) | Acc: (89.00%) (32078/35968)
Epoch: 79 | Batch_idx: 290 |  Loss: (0.3112) |  Loss2: (0.0000) | Acc: (89.00%) (33232/37248)
Epoch: 79 | Batch_idx: 300 |  Loss: (0.3105) |  Loss2: (0.0000) | Acc: (89.00%) (34377/38528)
Epoch: 79 | Batch_idx: 310 |  Loss: (0.3099) |  Loss2: (0.0000) | Acc: (89.00%) (35522/39808)
Epoch: 79 | Batch_idx: 320 |  Loss: (0.3093) |  Loss2: (0.0000) | Acc: (89.00%) (36680/41088)
Epoch: 79 | Batch_idx: 330 |  Loss: (0.3090) |  Loss2: (0.0000) | Acc: (89.00%) (37814/42368)
Epoch: 79 | Batch_idx: 340 |  Loss: (0.3086) |  Loss2: (0.0000) | Acc: (89.00%) (38973/43648)
Epoch: 79 | Batch_idx: 350 |  Loss: (0.3084) |  Loss2: (0.0000) | Acc: (89.00%) (40124/44928)
Epoch: 79 | Batch_idx: 360 |  Loss: (0.3086) |  Loss2: (0.0000) | Acc: (89.00%) (41261/46208)
Epoch: 79 | Batch_idx: 370 |  Loss: (0.3079) |  Loss2: (0.0000) | Acc: (89.00%) (42407/47488)
Epoch: 79 | Batch_idx: 380 |  Loss: (0.3072) |  Loss2: (0.0000) | Acc: (89.00%) (43563/48768)
Epoch: 79 | Batch_idx: 390 |  Loss: (0.3065) |  Loss2: (0.0000) | Acc: (89.00%) (44682/50000)
# TEST : Loss: (0.4252) | Acc: (85.00%) (8564/10000)
percent tensor([0.5652, 0.5806, 0.5648, 0.5573, 0.5696, 0.5515, 0.5825, 0.5758, 0.5774,
        0.5769, 0.5760, 0.5742, 0.5719, 0.5782, 0.5691, 0.5640],
       device='cuda:0') torch.Size([16])
percent tensor([0.5137, 0.5166, 0.4872, 0.5109, 0.5028, 0.5027, 0.5147, 0.5201, 0.5112,
        0.5028, 0.5021, 0.4878, 0.5125, 0.5229, 0.5159, 0.5113],
       device='cuda:0') torch.Size([16])
percent tensor([0.4763, 0.4494, 0.4080, 0.4904, 0.4099, 0.5151, 0.4486, 0.4618, 0.4712,
        0.4282, 0.4689, 0.4049, 0.4300, 0.5329, 0.4826, 0.4906],
       device='cuda:0') torch.Size([16])
percent tensor([0.5904, 0.5956, 0.5784, 0.5635, 0.5743, 0.6030, 0.5880, 0.5612, 0.5813,
        0.5984, 0.5900, 0.5879, 0.6094, 0.5741, 0.5921, 0.6026],
       device='cuda:0') torch.Size([16])
percent tensor([0.6350, 0.5661, 0.6421, 0.6752, 0.6826, 0.6848, 0.6403, 0.6566, 0.7267,
        0.6324, 0.6952, 0.6704, 0.5451, 0.7463, 0.5843, 0.6561],
       device='cuda:0') torch.Size([16])
percent tensor([0.6967, 0.6894, 0.7164, 0.7304, 0.7484, 0.7435, 0.7232, 0.7211, 0.7052,
        0.7035, 0.7095, 0.7081, 0.6788, 0.7368, 0.6950, 0.7238],
       device='cuda:0') torch.Size([16])
percent tensor([0.4885, 0.5201, 0.5457, 0.5070, 0.5682, 0.6830, 0.5262, 0.4004, 0.5460,
        0.5246, 0.5366, 0.4541, 0.4738, 0.5291, 0.3628, 0.5433],
       device='cuda:0') torch.Size([16])
percent tensor([0.9992, 0.9985, 0.9983, 0.9989, 0.9992, 0.9981, 0.9989, 0.9995, 0.9995,
        0.9988, 0.9996, 0.9989, 0.9988, 0.9990, 0.9987, 0.9995],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 80 | Batch_idx: 0 |  Loss: (0.3236) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 80 | Batch_idx: 10 |  Loss: (0.2658) |  Loss2: (0.0000) | Acc: (90.00%) (1281/1408)
Epoch: 80 | Batch_idx: 20 |  Loss: (0.2613) |  Loss2: (0.0000) | Acc: (91.00%) (2447/2688)
Epoch: 80 | Batch_idx: 30 |  Loss: (0.2629) |  Loss2: (0.0000) | Acc: (91.00%) (3618/3968)
Epoch: 80 | Batch_idx: 40 |  Loss: (0.2598) |  Loss2: (0.0000) | Acc: (91.00%) (4800/5248)
Epoch: 80 | Batch_idx: 50 |  Loss: (0.2646) |  Loss2: (0.0000) | Acc: (91.00%) (5949/6528)
Epoch: 80 | Batch_idx: 60 |  Loss: (0.2659) |  Loss2: (0.0000) | Acc: (91.00%) (7109/7808)
Epoch: 80 | Batch_idx: 70 |  Loss: (0.2662) |  Loss2: (0.0000) | Acc: (91.00%) (8271/9088)
Epoch: 80 | Batch_idx: 80 |  Loss: (0.2637) |  Loss2: (0.0000) | Acc: (91.00%) (9449/10368)
Epoch: 80 | Batch_idx: 90 |  Loss: (0.2645) |  Loss2: (0.0000) | Acc: (91.00%) (10603/11648)
Epoch: 80 | Batch_idx: 100 |  Loss: (0.2598) |  Loss2: (0.0000) | Acc: (91.00%) (11783/12928)
Epoch: 80 | Batch_idx: 110 |  Loss: (0.2615) |  Loss2: (0.0000) | Acc: (91.00%) (12952/14208)
Epoch: 80 | Batch_idx: 120 |  Loss: (0.2630) |  Loss2: (0.0000) | Acc: (91.00%) (14115/15488)
Epoch: 80 | Batch_idx: 130 |  Loss: (0.2613) |  Loss2: (0.0000) | Acc: (91.00%) (15283/16768)
Epoch: 80 | Batch_idx: 140 |  Loss: (0.2614) |  Loss2: (0.0000) | Acc: (91.00%) (16444/18048)
Epoch: 80 | Batch_idx: 150 |  Loss: (0.2611) |  Loss2: (0.0000) | Acc: (91.00%) (17613/19328)
Epoch: 80 | Batch_idx: 160 |  Loss: (0.2617) |  Loss2: (0.0000) | Acc: (91.00%) (18778/20608)
Epoch: 80 | Batch_idx: 170 |  Loss: (0.2618) |  Loss2: (0.0000) | Acc: (91.00%) (19945/21888)
Epoch: 80 | Batch_idx: 180 |  Loss: (0.2629) |  Loss2: (0.0000) | Acc: (91.00%) (21095/23168)
Epoch: 80 | Batch_idx: 190 |  Loss: (0.2636) |  Loss2: (0.0000) | Acc: (91.00%) (22254/24448)
Epoch: 80 | Batch_idx: 200 |  Loss: (0.2649) |  Loss2: (0.0000) | Acc: (90.00%) (23406/25728)
Epoch: 80 | Batch_idx: 210 |  Loss: (0.2640) |  Loss2: (0.0000) | Acc: (90.00%) (24576/27008)
Epoch: 80 | Batch_idx: 220 |  Loss: (0.2628) |  Loss2: (0.0000) | Acc: (91.00%) (25754/28288)
Epoch: 80 | Batch_idx: 230 |  Loss: (0.2637) |  Loss2: (0.0000) | Acc: (90.00%) (26906/29568)
Epoch: 80 | Batch_idx: 240 |  Loss: (0.2650) |  Loss2: (0.0000) | Acc: (90.00%) (28057/30848)
Epoch: 80 | Batch_idx: 250 |  Loss: (0.2659) |  Loss2: (0.0000) | Acc: (90.00%) (29200/32128)
Epoch: 80 | Batch_idx: 260 |  Loss: (0.2664) |  Loss2: (0.0000) | Acc: (90.00%) (30354/33408)
Epoch: 80 | Batch_idx: 270 |  Loss: (0.2675) |  Loss2: (0.0000) | Acc: (90.00%) (31503/34688)
Epoch: 80 | Batch_idx: 280 |  Loss: (0.2676) |  Loss2: (0.0000) | Acc: (90.00%) (32668/35968)
Epoch: 80 | Batch_idx: 290 |  Loss: (0.2662) |  Loss2: (0.0000) | Acc: (90.00%) (33846/37248)
Epoch: 80 | Batch_idx: 300 |  Loss: (0.2663) |  Loss2: (0.0000) | Acc: (90.00%) (35006/38528)
Epoch: 80 | Batch_idx: 310 |  Loss: (0.2667) |  Loss2: (0.0000) | Acc: (90.00%) (36173/39808)
Epoch: 80 | Batch_idx: 320 |  Loss: (0.2665) |  Loss2: (0.0000) | Acc: (90.00%) (37331/41088)
Epoch: 80 | Batch_idx: 330 |  Loss: (0.2658) |  Loss2: (0.0000) | Acc: (90.00%) (38507/42368)
Epoch: 80 | Batch_idx: 340 |  Loss: (0.2661) |  Loss2: (0.0000) | Acc: (90.00%) (39660/43648)
Epoch: 80 | Batch_idx: 350 |  Loss: (0.2658) |  Loss2: (0.0000) | Acc: (90.00%) (40809/44928)
Epoch: 80 | Batch_idx: 360 |  Loss: (0.2667) |  Loss2: (0.0000) | Acc: (90.00%) (41965/46208)
Epoch: 80 | Batch_idx: 370 |  Loss: (0.2669) |  Loss2: (0.0000) | Acc: (90.00%) (43119/47488)
Epoch: 80 | Batch_idx: 380 |  Loss: (0.2667) |  Loss2: (0.0000) | Acc: (90.00%) (44282/48768)
Epoch: 80 | Batch_idx: 390 |  Loss: (0.2661) |  Loss2: (0.0000) | Acc: (90.00%) (45402/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_080.pth.tar'
# TEST : Loss: (0.4266) | Acc: (85.00%) (8538/10000)
percent tensor([0.5639, 0.5804, 0.5611, 0.5539, 0.5653, 0.5499, 0.5804, 0.5732, 0.5771,
        0.5748, 0.5758, 0.5688, 0.5710, 0.5786, 0.5673, 0.5626],
       device='cuda:0') torch.Size([16])
percent tensor([0.5147, 0.5175, 0.4832, 0.5077, 0.5006, 0.5027, 0.5142, 0.5190, 0.5118,
        0.5020, 0.5018, 0.4860, 0.5134, 0.5237, 0.5160, 0.5111],
       device='cuda:0') torch.Size([16])
percent tensor([0.4911, 0.4486, 0.4249, 0.4858, 0.4187, 0.5192, 0.4513, 0.4657, 0.4775,
        0.4358, 0.4723, 0.4153, 0.4372, 0.5285, 0.4864, 0.4932],
       device='cuda:0') torch.Size([16])
percent tensor([0.5872, 0.5944, 0.5761, 0.5619, 0.5724, 0.5986, 0.5849, 0.5612, 0.5760,
        0.5955, 0.5846, 0.5873, 0.6068, 0.5731, 0.5893, 0.6002],
       device='cuda:0') torch.Size([16])
percent tensor([0.6359, 0.5595, 0.6665, 0.6893, 0.7122, 0.6993, 0.6289, 0.6499, 0.7050,
        0.6231, 0.6654, 0.6607, 0.5499, 0.6876, 0.5830, 0.6622],
       device='cuda:0') torch.Size([16])
percent tensor([0.7031, 0.6840, 0.7193, 0.7305, 0.7622, 0.7491, 0.7235, 0.7219, 0.7147,
        0.7013, 0.7027, 0.6979, 0.6793, 0.7233, 0.6929, 0.7261],
       device='cuda:0') torch.Size([16])
percent tensor([0.4656, 0.4963, 0.5336, 0.4852, 0.6085, 0.6888, 0.5151, 0.3815, 0.5591,
        0.4955, 0.5220, 0.4380, 0.4739, 0.5345, 0.3343, 0.5335],
       device='cuda:0') torch.Size([16])
percent tensor([0.9991, 0.9991, 0.9981, 0.9987, 0.9986, 0.9957, 0.9992, 0.9996, 0.9994,
        0.9994, 0.9996, 0.9993, 0.9993, 0.9991, 0.9988, 0.9992],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(176.1736, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(807.5633, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(796.4160, device='cuda:0')
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1517.3416, device='cuda:0')
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(498.2311, device='cuda:0')
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2218.2651, device='cuda:0')
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4300.9668, device='cuda:0')
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1409.4258, device='cuda:0')
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6149.0361, device='cuda:0')
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11930.9912, device='cuda:0')
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3965.7144, device='cuda:0')
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16758.4004, device='cuda:0')
Epoch: 81 | Batch_idx: 0 |  Loss: (0.2549) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 81 | Batch_idx: 10 |  Loss: (0.2862) |  Loss2: (0.0000) | Acc: (90.00%) (1275/1408)
Epoch: 81 | Batch_idx: 20 |  Loss: (0.2982) |  Loss2: (0.0000) | Acc: (89.00%) (2418/2688)
Epoch: 81 | Batch_idx: 30 |  Loss: (0.3009) |  Loss2: (0.0000) | Acc: (89.00%) (3560/3968)
Epoch: 81 | Batch_idx: 40 |  Loss: (0.3024) |  Loss2: (0.0000) | Acc: (89.00%) (4705/5248)
Epoch: 81 | Batch_idx: 50 |  Loss: (0.3080) |  Loss2: (0.0000) | Acc: (89.00%) (5838/6528)
Epoch: 81 | Batch_idx: 60 |  Loss: (0.3115) |  Loss2: (0.0000) | Acc: (89.00%) (6981/7808)
Epoch: 81 | Batch_idx: 70 |  Loss: (0.3104) |  Loss2: (0.0000) | Acc: (89.00%) (8121/9088)
Epoch: 81 | Batch_idx: 80 |  Loss: (0.3105) |  Loss2: (0.0000) | Acc: (89.00%) (9249/10368)
Epoch: 81 | Batch_idx: 90 |  Loss: (0.3118) |  Loss2: (0.0000) | Acc: (89.00%) (10385/11648)
Epoch: 81 | Batch_idx: 100 |  Loss: (0.3119) |  Loss2: (0.0000) | Acc: (89.00%) (11531/12928)
Epoch: 81 | Batch_idx: 110 |  Loss: (0.3099) |  Loss2: (0.0000) | Acc: (89.00%) (12683/14208)
Epoch: 81 | Batch_idx: 120 |  Loss: (0.3090) |  Loss2: (0.0000) | Acc: (89.00%) (13835/15488)
Epoch: 81 | Batch_idx: 130 |  Loss: (0.3065) |  Loss2: (0.0000) | Acc: (89.00%) (14982/16768)
Epoch: 81 | Batch_idx: 140 |  Loss: (0.3089) |  Loss2: (0.0000) | Acc: (89.00%) (16105/18048)
Epoch: 81 | Batch_idx: 150 |  Loss: (0.3090) |  Loss2: (0.0000) | Acc: (89.00%) (17239/19328)
Epoch: 81 | Batch_idx: 160 |  Loss: (0.3085) |  Loss2: (0.0000) | Acc: (89.00%) (18385/20608)
Epoch: 81 | Batch_idx: 170 |  Loss: (0.3060) |  Loss2: (0.0000) | Acc: (89.00%) (19551/21888)
Epoch: 81 | Batch_idx: 180 |  Loss: (0.3057) |  Loss2: (0.0000) | Acc: (89.00%) (20697/23168)
Epoch: 81 | Batch_idx: 190 |  Loss: (0.3056) |  Loss2: (0.0000) | Acc: (89.00%) (21847/24448)
Epoch: 81 | Batch_idx: 200 |  Loss: (0.3041) |  Loss2: (0.0000) | Acc: (89.00%) (23006/25728)
Epoch: 81 | Batch_idx: 210 |  Loss: (0.3026) |  Loss2: (0.0000) | Acc: (89.00%) (24170/27008)
Epoch: 81 | Batch_idx: 220 |  Loss: (0.3032) |  Loss2: (0.0000) | Acc: (89.00%) (25301/28288)
Epoch: 81 | Batch_idx: 230 |  Loss: (0.3033) |  Loss2: (0.0000) | Acc: (89.00%) (26449/29568)
Epoch: 81 | Batch_idx: 240 |  Loss: (0.3016) |  Loss2: (0.0000) | Acc: (89.00%) (27605/30848)
Epoch: 81 | Batch_idx: 250 |  Loss: (0.3012) |  Loss2: (0.0000) | Acc: (89.00%) (28771/32128)
Epoch: 81 | Batch_idx: 260 |  Loss: (0.2999) |  Loss2: (0.0000) | Acc: (89.00%) (29928/33408)
Epoch: 81 | Batch_idx: 270 |  Loss: (0.3001) |  Loss2: (0.0000) | Acc: (89.00%) (31071/34688)
Epoch: 81 | Batch_idx: 280 |  Loss: (0.3002) |  Loss2: (0.0000) | Acc: (89.00%) (32218/35968)
Epoch: 81 | Batch_idx: 290 |  Loss: (0.2988) |  Loss2: (0.0000) | Acc: (89.00%) (33382/37248)
Epoch: 81 | Batch_idx: 300 |  Loss: (0.2979) |  Loss2: (0.0000) | Acc: (89.00%) (34547/38528)
Epoch: 81 | Batch_idx: 310 |  Loss: (0.2986) |  Loss2: (0.0000) | Acc: (89.00%) (35691/39808)
Epoch: 81 | Batch_idx: 320 |  Loss: (0.2985) |  Loss2: (0.0000) | Acc: (89.00%) (36843/41088)
Epoch: 81 | Batch_idx: 330 |  Loss: (0.2979) |  Loss2: (0.0000) | Acc: (89.00%) (37991/42368)
Epoch: 81 | Batch_idx: 340 |  Loss: (0.2987) |  Loss2: (0.0000) | Acc: (89.00%) (39124/43648)
Epoch: 81 | Batch_idx: 350 |  Loss: (0.2982) |  Loss2: (0.0000) | Acc: (89.00%) (40283/44928)
Epoch: 81 | Batch_idx: 360 |  Loss: (0.2995) |  Loss2: (0.0000) | Acc: (89.00%) (41415/46208)
Epoch: 81 | Batch_idx: 370 |  Loss: (0.2986) |  Loss2: (0.0000) | Acc: (89.00%) (42579/47488)
Epoch: 81 | Batch_idx: 380 |  Loss: (0.2979) |  Loss2: (0.0000) | Acc: (89.00%) (43730/48768)
Epoch: 81 | Batch_idx: 390 |  Loss: (0.2975) |  Loss2: (0.0000) | Acc: (89.00%) (44836/50000)
# TEST : Loss: (0.4237) | Acc: (85.00%) (8586/10000)
percent tensor([0.5580, 0.5764, 0.5518, 0.5484, 0.5562, 0.5433, 0.5743, 0.5676, 0.5716,
        0.5690, 0.5706, 0.5594, 0.5654, 0.5773, 0.5622, 0.5579],
       device='cuda:0') torch.Size([16])
percent tensor([0.5078, 0.5153, 0.4735, 0.4986, 0.4916, 0.4943, 0.5106, 0.5109, 0.5053,
        0.4964, 0.4974, 0.4777, 0.5091, 0.5199, 0.5095, 0.5049],
       device='cuda:0') torch.Size([16])
percent tensor([0.4807, 0.4570, 0.4061, 0.4757, 0.4019, 0.5113, 0.4530, 0.4528, 0.4677,
        0.4363, 0.4710, 0.4082, 0.4401, 0.5293, 0.4857, 0.4859],
       device='cuda:0') torch.Size([16])
percent tensor([0.5972, 0.6067, 0.5738, 0.5627, 0.5696, 0.6131, 0.5900, 0.5600, 0.5791,
        0.6053, 0.5937, 0.5905, 0.6206, 0.5806, 0.5999, 0.6128],
       device='cuda:0') torch.Size([16])
percent tensor([0.6466, 0.5619, 0.6605, 0.6805, 0.7058, 0.7195, 0.6341, 0.6372, 0.7194,
        0.6170, 0.6856, 0.6440, 0.5642, 0.6906, 0.5867, 0.6729],
       device='cuda:0') torch.Size([16])
percent tensor([0.7409, 0.7196, 0.7530, 0.7590, 0.7946, 0.7801, 0.7576, 0.7542, 0.7495,
        0.7355, 0.7397, 0.7342, 0.7173, 0.7562, 0.7268, 0.7605],
       device='cuda:0') torch.Size([16])
percent tensor([0.5237, 0.5412, 0.5744, 0.5195, 0.6276, 0.7238, 0.5525, 0.4147, 0.6016,
        0.5529, 0.5703, 0.5166, 0.5357, 0.5868, 0.3520, 0.5609],
       device='cuda:0') torch.Size([16])
percent tensor([0.9993, 0.9990, 0.9979, 0.9988, 0.9991, 0.9970, 0.9991, 0.9995, 0.9993,
        0.9993, 0.9993, 0.9988, 0.9992, 0.9987, 0.9987, 0.9995],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 82 | Batch_idx: 0 |  Loss: (0.3480) |  Loss2: (0.0000) | Acc: (86.00%) (111/128)
Epoch: 82 | Batch_idx: 10 |  Loss: (0.2394) |  Loss2: (0.0000) | Acc: (91.00%) (1290/1408)
Epoch: 82 | Batch_idx: 20 |  Loss: (0.2397) |  Loss2: (0.0000) | Acc: (91.00%) (2468/2688)
Epoch: 82 | Batch_idx: 30 |  Loss: (0.2436) |  Loss2: (0.0000) | Acc: (91.00%) (3637/3968)
Epoch: 82 | Batch_idx: 40 |  Loss: (0.2531) |  Loss2: (0.0000) | Acc: (91.00%) (4789/5248)
Epoch: 82 | Batch_idx: 50 |  Loss: (0.2487) |  Loss2: (0.0000) | Acc: (91.00%) (5968/6528)
Epoch: 82 | Batch_idx: 60 |  Loss: (0.2494) |  Loss2: (0.0000) | Acc: (91.00%) (7131/7808)
Epoch: 82 | Batch_idx: 70 |  Loss: (0.2511) |  Loss2: (0.0000) | Acc: (91.00%) (8289/9088)
Epoch: 82 | Batch_idx: 80 |  Loss: (0.2528) |  Loss2: (0.0000) | Acc: (91.00%) (9445/10368)
Epoch: 82 | Batch_idx: 90 |  Loss: (0.2513) |  Loss2: (0.0000) | Acc: (91.00%) (10621/11648)
Epoch: 82 | Batch_idx: 100 |  Loss: (0.2512) |  Loss2: (0.0000) | Acc: (91.00%) (11795/12928)
Epoch: 82 | Batch_idx: 110 |  Loss: (0.2510) |  Loss2: (0.0000) | Acc: (91.00%) (12966/14208)
Epoch: 82 | Batch_idx: 120 |  Loss: (0.2502) |  Loss2: (0.0000) | Acc: (91.00%) (14146/15488)
Epoch: 82 | Batch_idx: 130 |  Loss: (0.2507) |  Loss2: (0.0000) | Acc: (91.00%) (15301/16768)
Epoch: 82 | Batch_idx: 140 |  Loss: (0.2521) |  Loss2: (0.0000) | Acc: (91.00%) (16458/18048)
Epoch: 82 | Batch_idx: 150 |  Loss: (0.2517) |  Loss2: (0.0000) | Acc: (91.00%) (17638/19328)
Epoch: 82 | Batch_idx: 160 |  Loss: (0.2528) |  Loss2: (0.0000) | Acc: (91.00%) (18792/20608)
Epoch: 82 | Batch_idx: 170 |  Loss: (0.2525) |  Loss2: (0.0000) | Acc: (91.00%) (19959/21888)
Epoch: 82 | Batch_idx: 180 |  Loss: (0.2536) |  Loss2: (0.0000) | Acc: (91.00%) (21130/23168)
Epoch: 82 | Batch_idx: 190 |  Loss: (0.2547) |  Loss2: (0.0000) | Acc: (91.00%) (22281/24448)
Epoch: 82 | Batch_idx: 200 |  Loss: (0.2557) |  Loss2: (0.0000) | Acc: (91.00%) (23438/25728)
Epoch: 82 | Batch_idx: 210 |  Loss: (0.2578) |  Loss2: (0.0000) | Acc: (91.00%) (24582/27008)
Epoch: 82 | Batch_idx: 220 |  Loss: (0.2576) |  Loss2: (0.0000) | Acc: (91.00%) (25744/28288)
Epoch: 82 | Batch_idx: 230 |  Loss: (0.2570) |  Loss2: (0.0000) | Acc: (91.00%) (26916/29568)
Epoch: 82 | Batch_idx: 240 |  Loss: (0.2579) |  Loss2: (0.0000) | Acc: (90.00%) (28070/30848)
Epoch: 82 | Batch_idx: 250 |  Loss: (0.2582) |  Loss2: (0.0000) | Acc: (90.00%) (29236/32128)
Epoch: 82 | Batch_idx: 260 |  Loss: (0.2580) |  Loss2: (0.0000) | Acc: (91.00%) (30407/33408)
Epoch: 82 | Batch_idx: 270 |  Loss: (0.2578) |  Loss2: (0.0000) | Acc: (91.00%) (31577/34688)
Epoch: 82 | Batch_idx: 280 |  Loss: (0.2574) |  Loss2: (0.0000) | Acc: (91.00%) (32740/35968)
Epoch: 82 | Batch_idx: 290 |  Loss: (0.2570) |  Loss2: (0.0000) | Acc: (91.00%) (33915/37248)
Epoch: 82 | Batch_idx: 300 |  Loss: (0.2577) |  Loss2: (0.0000) | Acc: (91.00%) (35068/38528)
Epoch: 82 | Batch_idx: 310 |  Loss: (0.2580) |  Loss2: (0.0000) | Acc: (91.00%) (36227/39808)
Epoch: 82 | Batch_idx: 320 |  Loss: (0.2577) |  Loss2: (0.0000) | Acc: (91.00%) (37396/41088)
Epoch: 82 | Batch_idx: 330 |  Loss: (0.2585) |  Loss2: (0.0000) | Acc: (90.00%) (38551/42368)
Epoch: 82 | Batch_idx: 340 |  Loss: (0.2592) |  Loss2: (0.0000) | Acc: (90.00%) (39692/43648)
Epoch: 82 | Batch_idx: 350 |  Loss: (0.2588) |  Loss2: (0.0000) | Acc: (90.00%) (40870/44928)
Epoch: 82 | Batch_idx: 360 |  Loss: (0.2591) |  Loss2: (0.0000) | Acc: (90.00%) (42020/46208)
Epoch: 82 | Batch_idx: 370 |  Loss: (0.2584) |  Loss2: (0.0000) | Acc: (90.00%) (43195/47488)
Epoch: 82 | Batch_idx: 380 |  Loss: (0.2584) |  Loss2: (0.0000) | Acc: (90.00%) (44357/48768)
Epoch: 82 | Batch_idx: 390 |  Loss: (0.2589) |  Loss2: (0.0000) | Acc: (90.00%) (45480/50000)
# TEST : Loss: (0.4648) | Acc: (84.00%) (8446/10000)
percent tensor([0.5588, 0.5745, 0.5598, 0.5513, 0.5633, 0.5453, 0.5754, 0.5694, 0.5713,
        0.5703, 0.5693, 0.5666, 0.5653, 0.5725, 0.5622, 0.5577],
       device='cuda:0') torch.Size([16])
percent tensor([0.5059, 0.5129, 0.4803, 0.5010, 0.4968, 0.4922, 0.5103, 0.5147, 0.5039,
        0.4976, 0.4940, 0.4830, 0.5077, 0.5177, 0.5091, 0.5039],
       device='cuda:0') torch.Size([16])
percent tensor([0.4712, 0.4512, 0.4074, 0.4711, 0.4012, 0.5032, 0.4477, 0.4564, 0.4623,
        0.4316, 0.4638, 0.4071, 0.4279, 0.5329, 0.4748, 0.4798],
       device='cuda:0') torch.Size([16])
percent tensor([0.5974, 0.6145, 0.5738, 0.5668, 0.5692, 0.6120, 0.5957, 0.5624, 0.5838,
        0.6107, 0.5983, 0.5907, 0.6229, 0.5900, 0.6030, 0.6145],
       device='cuda:0') torch.Size([16])
percent tensor([0.6210, 0.5498, 0.6337, 0.6610, 0.6793, 0.6949, 0.6132, 0.6291, 0.7036,
        0.6197, 0.6665, 0.6375, 0.5438, 0.6899, 0.5713, 0.6606],
       device='cuda:0') torch.Size([16])
percent tensor([0.7361, 0.7221, 0.7435, 0.7521, 0.7823, 0.7730, 0.7558, 0.7555, 0.7371,
        0.7367, 0.7354, 0.7222, 0.7133, 0.7524, 0.7298, 0.7615],
       device='cuda:0') torch.Size([16])
percent tensor([0.4957, 0.5238, 0.6052, 0.5458, 0.6438, 0.7269, 0.5331, 0.4318, 0.5908,
        0.5188, 0.5433, 0.4719, 0.5011, 0.5459, 0.3612, 0.5599],
       device='cuda:0') torch.Size([16])
percent tensor([0.9990, 0.9989, 0.9983, 0.9988, 0.9985, 0.9959, 0.9993, 0.9994, 0.9996,
        0.9992, 0.9994, 0.9990, 0.9991, 0.9993, 0.9987, 0.9995],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 83 | Batch_idx: 0 |  Loss: (0.2198) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 83 | Batch_idx: 10 |  Loss: (0.2576) |  Loss2: (0.0000) | Acc: (91.00%) (1293/1408)
Epoch: 83 | Batch_idx: 20 |  Loss: (0.2609) |  Loss2: (0.0000) | Acc: (91.00%) (2461/2688)
Epoch: 83 | Batch_idx: 30 |  Loss: (0.2741) |  Loss2: (0.0000) | Acc: (90.00%) (3605/3968)
Epoch: 83 | Batch_idx: 40 |  Loss: (0.2737) |  Loss2: (0.0000) | Acc: (90.00%) (4763/5248)
Epoch: 83 | Batch_idx: 50 |  Loss: (0.2806) |  Loss2: (0.0000) | Acc: (90.00%) (5903/6528)
Epoch: 83 | Batch_idx: 60 |  Loss: (0.2791) |  Loss2: (0.0000) | Acc: (90.00%) (7060/7808)
Epoch: 83 | Batch_idx: 70 |  Loss: (0.2730) |  Loss2: (0.0000) | Acc: (90.00%) (8236/9088)
Epoch: 83 | Batch_idx: 80 |  Loss: (0.2734) |  Loss2: (0.0000) | Acc: (90.00%) (9396/10368)
Epoch: 83 | Batch_idx: 90 |  Loss: (0.2722) |  Loss2: (0.0000) | Acc: (90.00%) (10566/11648)
Epoch: 83 | Batch_idx: 100 |  Loss: (0.2711) |  Loss2: (0.0000) | Acc: (90.00%) (11729/12928)
Epoch: 83 | Batch_idx: 110 |  Loss: (0.2691) |  Loss2: (0.0000) | Acc: (90.00%) (12904/14208)
Epoch: 83 | Batch_idx: 120 |  Loss: (0.2707) |  Loss2: (0.0000) | Acc: (90.00%) (14067/15488)
Epoch: 83 | Batch_idx: 130 |  Loss: (0.2696) |  Loss2: (0.0000) | Acc: (90.00%) (15241/16768)
Epoch: 83 | Batch_idx: 140 |  Loss: (0.2695) |  Loss2: (0.0000) | Acc: (90.00%) (16401/18048)
Epoch: 83 | Batch_idx: 150 |  Loss: (0.2710) |  Loss2: (0.0000) | Acc: (90.00%) (17539/19328)
Epoch: 83 | Batch_idx: 160 |  Loss: (0.2711) |  Loss2: (0.0000) | Acc: (90.00%) (18707/20608)
Epoch: 83 | Batch_idx: 170 |  Loss: (0.2713) |  Loss2: (0.0000) | Acc: (90.00%) (19868/21888)
Epoch: 83 | Batch_idx: 180 |  Loss: (0.2730) |  Loss2: (0.0000) | Acc: (90.00%) (21019/23168)
Epoch: 83 | Batch_idx: 190 |  Loss: (0.2715) |  Loss2: (0.0000) | Acc: (90.00%) (22193/24448)
Epoch: 83 | Batch_idx: 200 |  Loss: (0.2716) |  Loss2: (0.0000) | Acc: (90.00%) (23354/25728)
Epoch: 83 | Batch_idx: 210 |  Loss: (0.2717) |  Loss2: (0.0000) | Acc: (90.00%) (24514/27008)
Epoch: 83 | Batch_idx: 220 |  Loss: (0.2718) |  Loss2: (0.0000) | Acc: (90.00%) (25671/28288)
Epoch: 83 | Batch_idx: 230 |  Loss: (0.2723) |  Loss2: (0.0000) | Acc: (90.00%) (26815/29568)
Epoch: 83 | Batch_idx: 240 |  Loss: (0.2719) |  Loss2: (0.0000) | Acc: (90.00%) (27976/30848)
Epoch: 83 | Batch_idx: 250 |  Loss: (0.2718) |  Loss2: (0.0000) | Acc: (90.00%) (29139/32128)
Epoch: 83 | Batch_idx: 260 |  Loss: (0.2727) |  Loss2: (0.0000) | Acc: (90.00%) (30282/33408)
Epoch: 83 | Batch_idx: 270 |  Loss: (0.2721) |  Loss2: (0.0000) | Acc: (90.00%) (31432/34688)
Epoch: 83 | Batch_idx: 280 |  Loss: (0.2728) |  Loss2: (0.0000) | Acc: (90.00%) (32585/35968)
Epoch: 83 | Batch_idx: 290 |  Loss: (0.2730) |  Loss2: (0.0000) | Acc: (90.00%) (33729/37248)
Epoch: 83 | Batch_idx: 300 |  Loss: (0.2729) |  Loss2: (0.0000) | Acc: (90.00%) (34876/38528)
Epoch: 83 | Batch_idx: 310 |  Loss: (0.2730) |  Loss2: (0.0000) | Acc: (90.00%) (36033/39808)
Epoch: 83 | Batch_idx: 320 |  Loss: (0.2730) |  Loss2: (0.0000) | Acc: (90.00%) (37192/41088)
Epoch: 83 | Batch_idx: 330 |  Loss: (0.2729) |  Loss2: (0.0000) | Acc: (90.00%) (38338/42368)
Epoch: 83 | Batch_idx: 340 |  Loss: (0.2731) |  Loss2: (0.0000) | Acc: (90.00%) (39501/43648)
Epoch: 83 | Batch_idx: 350 |  Loss: (0.2727) |  Loss2: (0.0000) | Acc: (90.00%) (40668/44928)
Epoch: 83 | Batch_idx: 360 |  Loss: (0.2720) |  Loss2: (0.0000) | Acc: (90.00%) (41840/46208)
Epoch: 83 | Batch_idx: 370 |  Loss: (0.2709) |  Loss2: (0.0000) | Acc: (90.00%) (43017/47488)
Epoch: 83 | Batch_idx: 380 |  Loss: (0.2706) |  Loss2: (0.0000) | Acc: (90.00%) (44181/48768)
Epoch: 83 | Batch_idx: 390 |  Loss: (0.2704) |  Loss2: (0.0000) | Acc: (90.00%) (45301/50000)
# TEST : Loss: (0.4075) | Acc: (86.00%) (8635/10000)
percent tensor([0.5595, 0.5756, 0.5593, 0.5508, 0.5635, 0.5453, 0.5765, 0.5698, 0.5726,
        0.5712, 0.5706, 0.5664, 0.5661, 0.5735, 0.5626, 0.5585],
       device='cuda:0') torch.Size([16])
percent tensor([0.5028, 0.5087, 0.4821, 0.5012, 0.4967, 0.4890, 0.5077, 0.5138, 0.5010,
        0.4947, 0.4903, 0.4825, 0.5048, 0.5145, 0.5062, 0.5008],
       device='cuda:0') torch.Size([16])
percent tensor([0.4722, 0.4385, 0.4129, 0.4778, 0.4001, 0.5082, 0.4402, 0.4583, 0.4609,
        0.4246, 0.4613, 0.3994, 0.4195, 0.5307, 0.4715, 0.4800],
       device='cuda:0') torch.Size([16])
percent tensor([0.6124, 0.6329, 0.5877, 0.5787, 0.5803, 0.6224, 0.6147, 0.5748, 0.5995,
        0.6295, 0.6170, 0.6141, 0.6427, 0.6092, 0.6187, 0.6284],
       device='cuda:0') torch.Size([16])
percent tensor([0.6527, 0.5867, 0.6369, 0.6638, 0.6719, 0.7026, 0.6412, 0.6296, 0.7376,
        0.6561, 0.7108, 0.6685, 0.5988, 0.7335, 0.5983, 0.6824],
       device='cuda:0') torch.Size([16])
percent tensor([0.7385, 0.7261, 0.7469, 0.7588, 0.7842, 0.7750, 0.7577, 0.7589, 0.7459,
        0.7390, 0.7431, 0.7201, 0.7144, 0.7603, 0.7309, 0.7610],
       device='cuda:0') torch.Size([16])
percent tensor([0.4642, 0.5100, 0.5870, 0.5079, 0.6393, 0.7133, 0.5119, 0.4024, 0.5778,
        0.5025, 0.5199, 0.4216, 0.4672, 0.5198, 0.3432, 0.5206],
       device='cuda:0') torch.Size([16])
percent tensor([0.9990, 0.9988, 0.9983, 0.9986, 0.9984, 0.9966, 0.9993, 0.9995, 0.9996,
        0.9992, 0.9995, 0.9989, 0.9993, 0.9986, 0.9988, 0.9993],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 84 | Batch_idx: 0 |  Loss: (0.2801) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 84 | Batch_idx: 10 |  Loss: (0.2327) |  Loss2: (0.0000) | Acc: (92.00%) (1304/1408)
Epoch: 84 | Batch_idx: 20 |  Loss: (0.2443) |  Loss2: (0.0000) | Acc: (92.00%) (2475/2688)
Epoch: 84 | Batch_idx: 30 |  Loss: (0.2448) |  Loss2: (0.0000) | Acc: (91.00%) (3637/3968)
Epoch: 84 | Batch_idx: 40 |  Loss: (0.2457) |  Loss2: (0.0000) | Acc: (91.00%) (4801/5248)
Epoch: 84 | Batch_idx: 50 |  Loss: (0.2429) |  Loss2: (0.0000) | Acc: (91.00%) (5985/6528)
Epoch: 84 | Batch_idx: 60 |  Loss: (0.2415) |  Loss2: (0.0000) | Acc: (91.00%) (7159/7808)
Epoch: 84 | Batch_idx: 70 |  Loss: (0.2423) |  Loss2: (0.0000) | Acc: (91.00%) (8326/9088)
Epoch: 84 | Batch_idx: 80 |  Loss: (0.2452) |  Loss2: (0.0000) | Acc: (91.00%) (9493/10368)
Epoch: 84 | Batch_idx: 90 |  Loss: (0.2448) |  Loss2: (0.0000) | Acc: (91.00%) (10665/11648)
Epoch: 84 | Batch_idx: 100 |  Loss: (0.2451) |  Loss2: (0.0000) | Acc: (91.00%) (11844/12928)
Epoch: 84 | Batch_idx: 110 |  Loss: (0.2473) |  Loss2: (0.0000) | Acc: (91.00%) (13015/14208)
Epoch: 84 | Batch_idx: 120 |  Loss: (0.2483) |  Loss2: (0.0000) | Acc: (91.00%) (14190/15488)
Epoch: 84 | Batch_idx: 130 |  Loss: (0.2462) |  Loss2: (0.0000) | Acc: (91.00%) (15374/16768)
Epoch: 84 | Batch_idx: 140 |  Loss: (0.2464) |  Loss2: (0.0000) | Acc: (91.00%) (16542/18048)
Epoch: 84 | Batch_idx: 150 |  Loss: (0.2449) |  Loss2: (0.0000) | Acc: (91.00%) (17726/19328)
Epoch: 84 | Batch_idx: 160 |  Loss: (0.2461) |  Loss2: (0.0000) | Acc: (91.00%) (18888/20608)
Epoch: 84 | Batch_idx: 170 |  Loss: (0.2470) |  Loss2: (0.0000) | Acc: (91.00%) (20055/21888)
Epoch: 84 | Batch_idx: 180 |  Loss: (0.2473) |  Loss2: (0.0000) | Acc: (91.00%) (21225/23168)
Epoch: 84 | Batch_idx: 190 |  Loss: (0.2480) |  Loss2: (0.0000) | Acc: (91.00%) (22397/24448)
Epoch: 84 | Batch_idx: 200 |  Loss: (0.2475) |  Loss2: (0.0000) | Acc: (91.00%) (23573/25728)
Epoch: 84 | Batch_idx: 210 |  Loss: (0.2477) |  Loss2: (0.0000) | Acc: (91.00%) (24742/27008)
Epoch: 84 | Batch_idx: 220 |  Loss: (0.2471) |  Loss2: (0.0000) | Acc: (91.00%) (25923/28288)
Epoch: 84 | Batch_idx: 230 |  Loss: (0.2483) |  Loss2: (0.0000) | Acc: (91.00%) (27086/29568)
Epoch: 84 | Batch_idx: 240 |  Loss: (0.2496) |  Loss2: (0.0000) | Acc: (91.00%) (28241/30848)
Epoch: 84 | Batch_idx: 250 |  Loss: (0.2498) |  Loss2: (0.0000) | Acc: (91.00%) (29409/32128)
Epoch: 84 | Batch_idx: 260 |  Loss: (0.2497) |  Loss2: (0.0000) | Acc: (91.00%) (30583/33408)
Epoch: 84 | Batch_idx: 270 |  Loss: (0.2501) |  Loss2: (0.0000) | Acc: (91.00%) (31742/34688)
Epoch: 84 | Batch_idx: 280 |  Loss: (0.2505) |  Loss2: (0.0000) | Acc: (91.00%) (32903/35968)
Epoch: 84 | Batch_idx: 290 |  Loss: (0.2510) |  Loss2: (0.0000) | Acc: (91.00%) (34062/37248)
Epoch: 84 | Batch_idx: 300 |  Loss: (0.2508) |  Loss2: (0.0000) | Acc: (91.00%) (35234/38528)
Epoch: 84 | Batch_idx: 310 |  Loss: (0.2502) |  Loss2: (0.0000) | Acc: (91.00%) (36416/39808)
Epoch: 84 | Batch_idx: 320 |  Loss: (0.2502) |  Loss2: (0.0000) | Acc: (91.00%) (37591/41088)
Epoch: 84 | Batch_idx: 330 |  Loss: (0.2504) |  Loss2: (0.0000) | Acc: (91.00%) (38749/42368)
Epoch: 84 | Batch_idx: 340 |  Loss: (0.2499) |  Loss2: (0.0000) | Acc: (91.00%) (39930/43648)
Epoch: 84 | Batch_idx: 350 |  Loss: (0.2503) |  Loss2: (0.0000) | Acc: (91.00%) (41088/44928)
Epoch: 84 | Batch_idx: 360 |  Loss: (0.2502) |  Loss2: (0.0000) | Acc: (91.00%) (42255/46208)
Epoch: 84 | Batch_idx: 370 |  Loss: (0.2505) |  Loss2: (0.0000) | Acc: (91.00%) (43423/47488)
Epoch: 84 | Batch_idx: 380 |  Loss: (0.2508) |  Loss2: (0.0000) | Acc: (91.00%) (44588/48768)
Epoch: 84 | Batch_idx: 390 |  Loss: (0.2520) |  Loss2: (0.0000) | Acc: (91.00%) (45679/50000)
# TEST : Loss: (0.4230) | Acc: (85.00%) (8571/10000)
percent tensor([0.5599, 0.5752, 0.5554, 0.5505, 0.5612, 0.5456, 0.5756, 0.5687, 0.5718,
        0.5703, 0.5706, 0.5639, 0.5664, 0.5733, 0.5631, 0.5582],
       device='cuda:0') torch.Size([16])
percent tensor([0.5064, 0.5059, 0.4862, 0.5022, 0.4998, 0.4917, 0.5070, 0.5147, 0.5030,
        0.4937, 0.4898, 0.4856, 0.5058, 0.5097, 0.5040, 0.5012],
       device='cuda:0') torch.Size([16])
percent tensor([0.4741, 0.4427, 0.4012, 0.4765, 0.3948, 0.5115, 0.4383, 0.4530, 0.4561,
        0.4198, 0.4567, 0.3926, 0.4190, 0.5317, 0.4729, 0.4808],
       device='cuda:0') torch.Size([16])
percent tensor([0.6221, 0.6328, 0.5830, 0.5813, 0.5805, 0.6344, 0.6166, 0.5740, 0.6103,
        0.6298, 0.6271, 0.6091, 0.6517, 0.6102, 0.6223, 0.6351],
       device='cuda:0') torch.Size([16])
percent tensor([0.6529, 0.5858, 0.6655, 0.6742, 0.6827, 0.7062, 0.6456, 0.6342, 0.7137,
        0.6614, 0.7059, 0.6947, 0.5926, 0.7337, 0.6043, 0.6745],
       device='cuda:0') torch.Size([16])
percent tensor([0.7372, 0.7246, 0.7463, 0.7608, 0.7862, 0.7751, 0.7572, 0.7606, 0.7408,
        0.7391, 0.7419, 0.7205, 0.7217, 0.7672, 0.7291, 0.7622],
       device='cuda:0') torch.Size([16])
percent tensor([0.4450, 0.5003, 0.5285, 0.5105, 0.6048, 0.6943, 0.4907, 0.3951, 0.5554,
        0.4947, 0.5021, 0.4243, 0.4792, 0.5100, 0.3487, 0.5197],
       device='cuda:0') torch.Size([16])
percent tensor([0.9994, 0.9990, 0.9988, 0.9987, 0.9993, 0.9973, 0.9991, 0.9994, 0.9995,
        0.9991, 0.9996, 0.9993, 0.9994, 0.9987, 0.9989, 0.9994],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 85 | Batch_idx: 0 |  Loss: (0.2492) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 85 | Batch_idx: 10 |  Loss: (0.2426) |  Loss2: (0.0000) | Acc: (91.00%) (1286/1408)
Epoch: 85 | Batch_idx: 20 |  Loss: (0.2550) |  Loss2: (0.0000) | Acc: (91.00%) (2449/2688)
Epoch: 85 | Batch_idx: 30 |  Loss: (0.2706) |  Loss2: (0.0000) | Acc: (90.00%) (3578/3968)
Epoch: 85 | Batch_idx: 40 |  Loss: (0.2687) |  Loss2: (0.0000) | Acc: (90.00%) (4744/5248)
Epoch: 85 | Batch_idx: 50 |  Loss: (0.2707) |  Loss2: (0.0000) | Acc: (90.00%) (5893/6528)
Epoch: 85 | Batch_idx: 60 |  Loss: (0.2742) |  Loss2: (0.0000) | Acc: (90.00%) (7041/7808)
Epoch: 85 | Batch_idx: 70 |  Loss: (0.2794) |  Loss2: (0.0000) | Acc: (90.00%) (8184/9088)
Epoch: 85 | Batch_idx: 80 |  Loss: (0.2764) |  Loss2: (0.0000) | Acc: (90.00%) (9352/10368)
Epoch: 85 | Batch_idx: 90 |  Loss: (0.2772) |  Loss2: (0.0000) | Acc: (90.00%) (10504/11648)
Epoch: 85 | Batch_idx: 100 |  Loss: (0.2747) |  Loss2: (0.0000) | Acc: (90.00%) (11675/12928)
Epoch: 85 | Batch_idx: 110 |  Loss: (0.2758) |  Loss2: (0.0000) | Acc: (90.00%) (12827/14208)
Epoch: 85 | Batch_idx: 120 |  Loss: (0.2757) |  Loss2: (0.0000) | Acc: (90.00%) (13992/15488)
Epoch: 85 | Batch_idx: 130 |  Loss: (0.2780) |  Loss2: (0.0000) | Acc: (90.00%) (15124/16768)
Epoch: 85 | Batch_idx: 140 |  Loss: (0.2795) |  Loss2: (0.0000) | Acc: (90.00%) (16288/18048)
Epoch: 85 | Batch_idx: 150 |  Loss: (0.2788) |  Loss2: (0.0000) | Acc: (90.00%) (17448/19328)
Epoch: 85 | Batch_idx: 160 |  Loss: (0.2776) |  Loss2: (0.0000) | Acc: (90.00%) (18609/20608)
Epoch: 85 | Batch_idx: 170 |  Loss: (0.2765) |  Loss2: (0.0000) | Acc: (90.00%) (19778/21888)
Epoch: 85 | Batch_idx: 180 |  Loss: (0.2758) |  Loss2: (0.0000) | Acc: (90.00%) (20940/23168)
Epoch: 85 | Batch_idx: 190 |  Loss: (0.2756) |  Loss2: (0.0000) | Acc: (90.00%) (22108/24448)
Epoch: 85 | Batch_idx: 200 |  Loss: (0.2774) |  Loss2: (0.0000) | Acc: (90.00%) (23248/25728)
Epoch: 85 | Batch_idx: 210 |  Loss: (0.2775) |  Loss2: (0.0000) | Acc: (90.00%) (24403/27008)
Epoch: 85 | Batch_idx: 220 |  Loss: (0.2774) |  Loss2: (0.0000) | Acc: (90.00%) (25556/28288)
Epoch: 85 | Batch_idx: 230 |  Loss: (0.2775) |  Loss2: (0.0000) | Acc: (90.00%) (26707/29568)
Epoch: 85 | Batch_idx: 240 |  Loss: (0.2768) |  Loss2: (0.0000) | Acc: (90.00%) (27863/30848)
Epoch: 85 | Batch_idx: 250 |  Loss: (0.2776) |  Loss2: (0.0000) | Acc: (90.00%) (29006/32128)
Epoch: 85 | Batch_idx: 260 |  Loss: (0.2787) |  Loss2: (0.0000) | Acc: (90.00%) (30144/33408)
Epoch: 85 | Batch_idx: 270 |  Loss: (0.2787) |  Loss2: (0.0000) | Acc: (90.00%) (31302/34688)
Epoch: 85 | Batch_idx: 280 |  Loss: (0.2785) |  Loss2: (0.0000) | Acc: (90.00%) (32463/35968)
Epoch: 85 | Batch_idx: 290 |  Loss: (0.2773) |  Loss2: (0.0000) | Acc: (90.00%) (33641/37248)
Epoch: 85 | Batch_idx: 300 |  Loss: (0.2772) |  Loss2: (0.0000) | Acc: (90.00%) (34789/38528)
Epoch: 85 | Batch_idx: 310 |  Loss: (0.2775) |  Loss2: (0.0000) | Acc: (90.00%) (35934/39808)
Epoch: 85 | Batch_idx: 320 |  Loss: (0.2771) |  Loss2: (0.0000) | Acc: (90.00%) (37105/41088)
Epoch: 85 | Batch_idx: 330 |  Loss: (0.2776) |  Loss2: (0.0000) | Acc: (90.00%) (38250/42368)
Epoch: 85 | Batch_idx: 340 |  Loss: (0.2766) |  Loss2: (0.0000) | Acc: (90.00%) (39418/43648)
Epoch: 85 | Batch_idx: 350 |  Loss: (0.2753) |  Loss2: (0.0000) | Acc: (90.00%) (40584/44928)
Epoch: 85 | Batch_idx: 360 |  Loss: (0.2752) |  Loss2: (0.0000) | Acc: (90.00%) (41748/46208)
Epoch: 85 | Batch_idx: 370 |  Loss: (0.2746) |  Loss2: (0.0000) | Acc: (90.00%) (42917/47488)
Epoch: 85 | Batch_idx: 380 |  Loss: (0.2744) |  Loss2: (0.0000) | Acc: (90.00%) (44079/48768)
Epoch: 85 | Batch_idx: 390 |  Loss: (0.2741) |  Loss2: (0.0000) | Acc: (90.00%) (45202/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_085.pth.tar'
# TEST : Loss: (0.4168) | Acc: (85.00%) (8599/10000)
percent tensor([0.5773, 0.5937, 0.5779, 0.5659, 0.5842, 0.5638, 0.5960, 0.5871, 0.5915,
        0.5886, 0.5896, 0.5866, 0.5844, 0.5893, 0.5814, 0.5749],
       device='cuda:0') torch.Size([16])
percent tensor([0.5007, 0.4965, 0.4786, 0.4943, 0.4921, 0.4866, 0.4979, 0.5102, 0.4960,
        0.4854, 0.4828, 0.4747, 0.5003, 0.5037, 0.4975, 0.4955],
       device='cuda:0') torch.Size([16])
percent tensor([0.4653, 0.4381, 0.4059, 0.4647, 0.3945, 0.4985, 0.4321, 0.4450, 0.4477,
        0.4193, 0.4492, 0.3972, 0.4149, 0.5253, 0.4619, 0.4697],
       device='cuda:0') torch.Size([16])
percent tensor([0.6222, 0.6345, 0.5780, 0.5737, 0.5760, 0.6429, 0.6159, 0.5641, 0.6070,
        0.6296, 0.6299, 0.6011, 0.6534, 0.6098, 0.6231, 0.6377],
       device='cuda:0') torch.Size([16])
percent tensor([0.6867, 0.6104, 0.6654, 0.6689, 0.6831, 0.7476, 0.6632, 0.6274, 0.7284,
        0.6871, 0.7234, 0.6859, 0.6408, 0.7309, 0.6255, 0.7056],
       device='cuda:0') torch.Size([16])
percent tensor([0.7377, 0.7217, 0.7489, 0.7617, 0.7878, 0.7787, 0.7564, 0.7602, 0.7425,
        0.7366, 0.7367, 0.7285, 0.7197, 0.7592, 0.7311, 0.7612],
       device='cuda:0') torch.Size([16])
percent tensor([0.4074, 0.4540, 0.5545, 0.5512, 0.6471, 0.6738, 0.4571, 0.4172, 0.5418,
        0.4570, 0.4755, 0.4676, 0.4334, 0.4617, 0.3348, 0.4770],
       device='cuda:0') torch.Size([16])
percent tensor([0.9992, 0.9990, 0.9988, 0.9987, 0.9992, 0.9960, 0.9993, 0.9994, 0.9994,
        0.9992, 0.9995, 0.9989, 0.9993, 0.9987, 0.9988, 0.9993],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 86 | Batch_idx: 0 |  Loss: (0.2512) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 86 | Batch_idx: 10 |  Loss: (0.2473) |  Loss2: (0.0000) | Acc: (91.00%) (1295/1408)
Epoch: 86 | Batch_idx: 20 |  Loss: (0.2483) |  Loss2: (0.0000) | Acc: (91.00%) (2463/2688)
Epoch: 86 | Batch_idx: 30 |  Loss: (0.2495) |  Loss2: (0.0000) | Acc: (91.00%) (3635/3968)
Epoch: 86 | Batch_idx: 40 |  Loss: (0.2489) |  Loss2: (0.0000) | Acc: (91.00%) (4813/5248)
Epoch: 86 | Batch_idx: 50 |  Loss: (0.2408) |  Loss2: (0.0000) | Acc: (91.00%) (6002/6528)
Epoch: 86 | Batch_idx: 60 |  Loss: (0.2403) |  Loss2: (0.0000) | Acc: (91.00%) (7175/7808)
Epoch: 86 | Batch_idx: 70 |  Loss: (0.2429) |  Loss2: (0.0000) | Acc: (91.00%) (8334/9088)
Epoch: 86 | Batch_idx: 80 |  Loss: (0.2432) |  Loss2: (0.0000) | Acc: (91.00%) (9511/10368)
Epoch: 86 | Batch_idx: 90 |  Loss: (0.2423) |  Loss2: (0.0000) | Acc: (91.00%) (10677/11648)
Epoch: 86 | Batch_idx: 100 |  Loss: (0.2440) |  Loss2: (0.0000) | Acc: (91.00%) (11837/12928)
Epoch: 86 | Batch_idx: 110 |  Loss: (0.2445) |  Loss2: (0.0000) | Acc: (91.00%) (13003/14208)
Epoch: 86 | Batch_idx: 120 |  Loss: (0.2440) |  Loss2: (0.0000) | Acc: (91.00%) (14180/15488)
Epoch: 86 | Batch_idx: 130 |  Loss: (0.2464) |  Loss2: (0.0000) | Acc: (91.00%) (15330/16768)
Epoch: 86 | Batch_idx: 140 |  Loss: (0.2476) |  Loss2: (0.0000) | Acc: (91.00%) (16488/18048)
Epoch: 86 | Batch_idx: 150 |  Loss: (0.2493) |  Loss2: (0.0000) | Acc: (91.00%) (17650/19328)
Epoch: 86 | Batch_idx: 160 |  Loss: (0.2491) |  Loss2: (0.0000) | Acc: (91.00%) (18821/20608)
Epoch: 86 | Batch_idx: 170 |  Loss: (0.2484) |  Loss2: (0.0000) | Acc: (91.00%) (19996/21888)
Epoch: 86 | Batch_idx: 180 |  Loss: (0.2483) |  Loss2: (0.0000) | Acc: (91.00%) (21177/23168)
Epoch: 86 | Batch_idx: 190 |  Loss: (0.2490) |  Loss2: (0.0000) | Acc: (91.00%) (22347/24448)
Epoch: 86 | Batch_idx: 200 |  Loss: (0.2487) |  Loss2: (0.0000) | Acc: (91.00%) (23519/25728)
Epoch: 86 | Batch_idx: 210 |  Loss: (0.2471) |  Loss2: (0.0000) | Acc: (91.00%) (24705/27008)
Epoch: 86 | Batch_idx: 220 |  Loss: (0.2468) |  Loss2: (0.0000) | Acc: (91.00%) (25876/28288)
Epoch: 86 | Batch_idx: 230 |  Loss: (0.2461) |  Loss2: (0.0000) | Acc: (91.00%) (27046/29568)
Epoch: 86 | Batch_idx: 240 |  Loss: (0.2458) |  Loss2: (0.0000) | Acc: (91.00%) (28227/30848)
Epoch: 86 | Batch_idx: 250 |  Loss: (0.2464) |  Loss2: (0.0000) | Acc: (91.00%) (29393/32128)
Epoch: 86 | Batch_idx: 260 |  Loss: (0.2452) |  Loss2: (0.0000) | Acc: (91.00%) (30579/33408)
Epoch: 86 | Batch_idx: 270 |  Loss: (0.2443) |  Loss2: (0.0000) | Acc: (91.00%) (31759/34688)
Epoch: 86 | Batch_idx: 280 |  Loss: (0.2451) |  Loss2: (0.0000) | Acc: (91.00%) (32926/35968)
Epoch: 86 | Batch_idx: 290 |  Loss: (0.2449) |  Loss2: (0.0000) | Acc: (91.00%) (34101/37248)
Epoch: 86 | Batch_idx: 300 |  Loss: (0.2450) |  Loss2: (0.0000) | Acc: (91.00%) (35265/38528)
Epoch: 86 | Batch_idx: 310 |  Loss: (0.2457) |  Loss2: (0.0000) | Acc: (91.00%) (36425/39808)
Epoch: 86 | Batch_idx: 320 |  Loss: (0.2462) |  Loss2: (0.0000) | Acc: (91.00%) (37600/41088)
Epoch: 86 | Batch_idx: 330 |  Loss: (0.2453) |  Loss2: (0.0000) | Acc: (91.00%) (38776/42368)
Epoch: 86 | Batch_idx: 340 |  Loss: (0.2450) |  Loss2: (0.0000) | Acc: (91.00%) (39958/43648)
Epoch: 86 | Batch_idx: 350 |  Loss: (0.2452) |  Loss2: (0.0000) | Acc: (91.00%) (41112/44928)
Epoch: 86 | Batch_idx: 360 |  Loss: (0.2452) |  Loss2: (0.0000) | Acc: (91.00%) (42278/46208)
Epoch: 86 | Batch_idx: 370 |  Loss: (0.2458) |  Loss2: (0.0000) | Acc: (91.00%) (43447/47488)
Epoch: 86 | Batch_idx: 380 |  Loss: (0.2458) |  Loss2: (0.0000) | Acc: (91.00%) (44617/48768)
Epoch: 86 | Batch_idx: 390 |  Loss: (0.2455) |  Loss2: (0.0000) | Acc: (91.00%) (45745/50000)
# TEST : Loss: (0.4120) | Acc: (86.00%) (8623/10000)
percent tensor([0.5759, 0.5968, 0.5724, 0.5656, 0.5793, 0.5632, 0.5960, 0.5870, 0.5908,
        0.5886, 0.5896, 0.5807, 0.5834, 0.5938, 0.5824, 0.5752],
       device='cuda:0') torch.Size([16])
percent tensor([0.4961, 0.4982, 0.4736, 0.4915, 0.4890, 0.4841, 0.4982, 0.5048, 0.4938,
        0.4847, 0.4809, 0.4725, 0.4974, 0.5074, 0.4972, 0.4923],
       device='cuda:0') torch.Size([16])
percent tensor([0.4586, 0.4356, 0.4025, 0.4591, 0.3911, 0.4980, 0.4339, 0.4420, 0.4486,
        0.4178, 0.4520, 0.3931, 0.4133, 0.5193, 0.4598, 0.4645],
       device='cuda:0') torch.Size([16])
percent tensor([0.6208, 0.6357, 0.5900, 0.5786, 0.5840, 0.6367, 0.6185, 0.5706, 0.6075,
        0.6315, 0.6245, 0.6036, 0.6496, 0.6085, 0.6209, 0.6389],
       device='cuda:0') torch.Size([16])
percent tensor([0.6663, 0.6093, 0.6340, 0.6539, 0.6702, 0.7270, 0.6529, 0.6218, 0.7208,
        0.6627, 0.7209, 0.6341, 0.6080, 0.7309, 0.6052, 0.6925],
       device='cuda:0') torch.Size([16])
percent tensor([0.7324, 0.7251, 0.7420, 0.7551, 0.7830, 0.7746, 0.7544, 0.7563, 0.7439,
        0.7367, 0.7396, 0.7231, 0.7156, 0.7601, 0.7298, 0.7598],
       device='cuda:0') torch.Size([16])
percent tensor([0.4331, 0.4982, 0.5620, 0.5147, 0.6217, 0.6748, 0.4856, 0.4156, 0.5609,
        0.4726, 0.4933, 0.4515, 0.4463, 0.5018, 0.3370, 0.4843],
       device='cuda:0') torch.Size([16])
percent tensor([0.9994, 0.9986, 0.9993, 0.9991, 0.9995, 0.9967, 0.9994, 0.9995, 0.9993,
        0.9991, 0.9994, 0.9996, 0.9989, 0.9987, 0.9989, 0.9994],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 87 | Batch_idx: 0 |  Loss: (0.1610) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 87 | Batch_idx: 10 |  Loss: (0.2524) |  Loss2: (0.0000) | Acc: (90.00%) (1281/1408)
Epoch: 87 | Batch_idx: 20 |  Loss: (0.2675) |  Loss2: (0.0000) | Acc: (90.00%) (2438/2688)
Epoch: 87 | Batch_idx: 30 |  Loss: (0.2709) |  Loss2: (0.0000) | Acc: (90.00%) (3592/3968)
Epoch: 87 | Batch_idx: 40 |  Loss: (0.2733) |  Loss2: (0.0000) | Acc: (90.00%) (4751/5248)
Epoch: 87 | Batch_idx: 50 |  Loss: (0.2688) |  Loss2: (0.0000) | Acc: (90.00%) (5919/6528)
Epoch: 87 | Batch_idx: 60 |  Loss: (0.2708) |  Loss2: (0.0000) | Acc: (90.00%) (7077/7808)
Epoch: 87 | Batch_idx: 70 |  Loss: (0.2732) |  Loss2: (0.0000) | Acc: (90.00%) (8224/9088)
Epoch: 87 | Batch_idx: 80 |  Loss: (0.2738) |  Loss2: (0.0000) | Acc: (90.00%) (9389/10368)
Epoch: 87 | Batch_idx: 90 |  Loss: (0.2728) |  Loss2: (0.0000) | Acc: (90.00%) (10552/11648)
Epoch: 87 | Batch_idx: 100 |  Loss: (0.2733) |  Loss2: (0.0000) | Acc: (90.00%) (11706/12928)
Epoch: 87 | Batch_idx: 110 |  Loss: (0.2729) |  Loss2: (0.0000) | Acc: (90.00%) (12872/14208)
Epoch: 87 | Batch_idx: 120 |  Loss: (0.2742) |  Loss2: (0.0000) | Acc: (90.00%) (14013/15488)
Epoch: 87 | Batch_idx: 130 |  Loss: (0.2724) |  Loss2: (0.0000) | Acc: (90.00%) (15184/16768)
Epoch: 87 | Batch_idx: 140 |  Loss: (0.2746) |  Loss2: (0.0000) | Acc: (90.00%) (16316/18048)
Epoch: 87 | Batch_idx: 150 |  Loss: (0.2733) |  Loss2: (0.0000) | Acc: (90.00%) (17482/19328)
Epoch: 87 | Batch_idx: 160 |  Loss: (0.2736) |  Loss2: (0.0000) | Acc: (90.00%) (18637/20608)
Epoch: 87 | Batch_idx: 170 |  Loss: (0.2741) |  Loss2: (0.0000) | Acc: (90.00%) (19797/21888)
Epoch: 87 | Batch_idx: 180 |  Loss: (0.2737) |  Loss2: (0.0000) | Acc: (90.00%) (20967/23168)
Epoch: 87 | Batch_idx: 190 |  Loss: (0.2725) |  Loss2: (0.0000) | Acc: (90.00%) (22127/24448)
Epoch: 87 | Batch_idx: 200 |  Loss: (0.2722) |  Loss2: (0.0000) | Acc: (90.00%) (23290/25728)
Epoch: 87 | Batch_idx: 210 |  Loss: (0.2725) |  Loss2: (0.0000) | Acc: (90.00%) (24444/27008)
Epoch: 87 | Batch_idx: 220 |  Loss: (0.2725) |  Loss2: (0.0000) | Acc: (90.00%) (25611/28288)
Epoch: 87 | Batch_idx: 230 |  Loss: (0.2718) |  Loss2: (0.0000) | Acc: (90.00%) (26783/29568)
Epoch: 87 | Batch_idx: 240 |  Loss: (0.2723) |  Loss2: (0.0000) | Acc: (90.00%) (27940/30848)
Epoch: 87 | Batch_idx: 250 |  Loss: (0.2720) |  Loss2: (0.0000) | Acc: (90.00%) (29113/32128)
Epoch: 87 | Batch_idx: 260 |  Loss: (0.2715) |  Loss2: (0.0000) | Acc: (90.00%) (30268/33408)
Epoch: 87 | Batch_idx: 270 |  Loss: (0.2710) |  Loss2: (0.0000) | Acc: (90.00%) (31438/34688)
Epoch: 87 | Batch_idx: 280 |  Loss: (0.2706) |  Loss2: (0.0000) | Acc: (90.00%) (32598/35968)
Epoch: 87 | Batch_idx: 290 |  Loss: (0.2705) |  Loss2: (0.0000) | Acc: (90.00%) (33768/37248)
Epoch: 87 | Batch_idx: 300 |  Loss: (0.2709) |  Loss2: (0.0000) | Acc: (90.00%) (34933/38528)
Epoch: 87 | Batch_idx: 310 |  Loss: (0.2705) |  Loss2: (0.0000) | Acc: (90.00%) (36109/39808)
Epoch: 87 | Batch_idx: 320 |  Loss: (0.2700) |  Loss2: (0.0000) | Acc: (90.00%) (37280/41088)
Epoch: 87 | Batch_idx: 330 |  Loss: (0.2695) |  Loss2: (0.0000) | Acc: (90.00%) (38449/42368)
Epoch: 87 | Batch_idx: 340 |  Loss: (0.2686) |  Loss2: (0.0000) | Acc: (90.00%) (39637/43648)
Epoch: 87 | Batch_idx: 350 |  Loss: (0.2685) |  Loss2: (0.0000) | Acc: (90.00%) (40809/44928)
Epoch: 87 | Batch_idx: 360 |  Loss: (0.2685) |  Loss2: (0.0000) | Acc: (90.00%) (41959/46208)
Epoch: 87 | Batch_idx: 370 |  Loss: (0.2674) |  Loss2: (0.0000) | Acc: (90.00%) (43142/47488)
Epoch: 87 | Batch_idx: 380 |  Loss: (0.2669) |  Loss2: (0.0000) | Acc: (90.00%) (44307/48768)
Epoch: 87 | Batch_idx: 390 |  Loss: (0.2673) |  Loss2: (0.0000) | Acc: (90.00%) (45424/50000)
# TEST : Loss: (0.4064) | Acc: (86.00%) (8646/10000)
percent tensor([0.5683, 0.5877, 0.5626, 0.5579, 0.5694, 0.5554, 0.5867, 0.5783, 0.5819,
        0.5798, 0.5810, 0.5705, 0.5751, 0.5859, 0.5740, 0.5671],
       device='cuda:0') torch.Size([16])
percent tensor([0.4939, 0.4969, 0.4709, 0.4900, 0.4896, 0.4810, 0.4984, 0.5056, 0.4918,
        0.4801, 0.4763, 0.4677, 0.4942, 0.5050, 0.4964, 0.4904],
       device='cuda:0') torch.Size([16])
percent tensor([0.4984, 0.4702, 0.4447, 0.5030, 0.4328, 0.5197, 0.4753, 0.4842, 0.4898,
        0.4565, 0.4893, 0.4430, 0.4555, 0.5388, 0.4959, 0.4995],
       device='cuda:0') torch.Size([16])
percent tensor([0.6265, 0.6356, 0.5896, 0.5774, 0.5862, 0.6407, 0.6204, 0.5760, 0.6087,
        0.6298, 0.6241, 0.6003, 0.6508, 0.6057, 0.6258, 0.6405],
       device='cuda:0') torch.Size([16])
percent tensor([0.6564, 0.5897, 0.6258, 0.6404, 0.6622, 0.7244, 0.6395, 0.6195, 0.7066,
        0.6416, 0.6954, 0.6197, 0.5932, 0.7163, 0.6040, 0.6852],
       device='cuda:0') torch.Size([16])
percent tensor([0.7584, 0.7465, 0.7642, 0.7841, 0.8057, 0.8000, 0.7803, 0.7813, 0.7688,
        0.7605, 0.7679, 0.7489, 0.7424, 0.7860, 0.7578, 0.7847],
       device='cuda:0') torch.Size([16])
percent tensor([0.4625, 0.5352, 0.5753, 0.5336, 0.6382, 0.7170, 0.5103, 0.4009, 0.6140,
        0.5287, 0.5531, 0.4928, 0.4899, 0.5516, 0.3377, 0.4961],
       device='cuda:0') torch.Size([16])
percent tensor([0.9995, 0.9988, 0.9992, 0.9990, 0.9996, 0.9977, 0.9992, 0.9994, 0.9993,
        0.9991, 0.9994, 0.9994, 0.9990, 0.9988, 0.9989, 0.9994],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 88 | Batch_idx: 0 |  Loss: (0.2163) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 88 | Batch_idx: 10 |  Loss: (0.2485) |  Loss2: (0.0000) | Acc: (91.00%) (1287/1408)
Epoch: 88 | Batch_idx: 20 |  Loss: (0.2443) |  Loss2: (0.0000) | Acc: (91.00%) (2471/2688)
Epoch: 88 | Batch_idx: 30 |  Loss: (0.2452) |  Loss2: (0.0000) | Acc: (91.00%) (3640/3968)
Epoch: 88 | Batch_idx: 40 |  Loss: (0.2378) |  Loss2: (0.0000) | Acc: (92.00%) (4833/5248)
Epoch: 88 | Batch_idx: 50 |  Loss: (0.2398) |  Loss2: (0.0000) | Acc: (91.00%) (6005/6528)
Epoch: 88 | Batch_idx: 60 |  Loss: (0.2365) |  Loss2: (0.0000) | Acc: (92.00%) (7192/7808)
Epoch: 88 | Batch_idx: 70 |  Loss: (0.2359) |  Loss2: (0.0000) | Acc: (92.00%) (8364/9088)
Epoch: 88 | Batch_idx: 80 |  Loss: (0.2401) |  Loss2: (0.0000) | Acc: (91.00%) (9518/10368)
Epoch: 88 | Batch_idx: 90 |  Loss: (0.2398) |  Loss2: (0.0000) | Acc: (91.00%) (10681/11648)
Epoch: 88 | Batch_idx: 100 |  Loss: (0.2385) |  Loss2: (0.0000) | Acc: (91.00%) (11868/12928)
Epoch: 88 | Batch_idx: 110 |  Loss: (0.2369) |  Loss2: (0.0000) | Acc: (91.00%) (13053/14208)
Epoch: 88 | Batch_idx: 120 |  Loss: (0.2365) |  Loss2: (0.0000) | Acc: (91.00%) (14232/15488)
Epoch: 88 | Batch_idx: 130 |  Loss: (0.2355) |  Loss2: (0.0000) | Acc: (91.00%) (15412/16768)
Epoch: 88 | Batch_idx: 140 |  Loss: (0.2345) |  Loss2: (0.0000) | Acc: (91.00%) (16597/18048)
Epoch: 88 | Batch_idx: 150 |  Loss: (0.2344) |  Loss2: (0.0000) | Acc: (91.00%) (17770/19328)
Epoch: 88 | Batch_idx: 160 |  Loss: (0.2365) |  Loss2: (0.0000) | Acc: (91.00%) (18935/20608)
Epoch: 88 | Batch_idx: 170 |  Loss: (0.2371) |  Loss2: (0.0000) | Acc: (91.00%) (20107/21888)
Epoch: 88 | Batch_idx: 180 |  Loss: (0.2380) |  Loss2: (0.0000) | Acc: (91.00%) (21285/23168)
Epoch: 88 | Batch_idx: 190 |  Loss: (0.2365) |  Loss2: (0.0000) | Acc: (91.00%) (22472/24448)
Epoch: 88 | Batch_idx: 200 |  Loss: (0.2365) |  Loss2: (0.0000) | Acc: (91.00%) (23654/25728)
Epoch: 88 | Batch_idx: 210 |  Loss: (0.2367) |  Loss2: (0.0000) | Acc: (91.00%) (24823/27008)
Epoch: 88 | Batch_idx: 220 |  Loss: (0.2369) |  Loss2: (0.0000) | Acc: (91.00%) (26008/28288)
Epoch: 88 | Batch_idx: 230 |  Loss: (0.2381) |  Loss2: (0.0000) | Acc: (91.00%) (27171/29568)
Epoch: 88 | Batch_idx: 240 |  Loss: (0.2380) |  Loss2: (0.0000) | Acc: (91.00%) (28348/30848)
Epoch: 88 | Batch_idx: 250 |  Loss: (0.2372) |  Loss2: (0.0000) | Acc: (91.00%) (29533/32128)
Epoch: 88 | Batch_idx: 260 |  Loss: (0.2369) |  Loss2: (0.0000) | Acc: (91.00%) (30711/33408)
Epoch: 88 | Batch_idx: 270 |  Loss: (0.2363) |  Loss2: (0.0000) | Acc: (91.00%) (31887/34688)
Epoch: 88 | Batch_idx: 280 |  Loss: (0.2353) |  Loss2: (0.0000) | Acc: (91.00%) (33071/35968)
Epoch: 88 | Batch_idx: 290 |  Loss: (0.2363) |  Loss2: (0.0000) | Acc: (91.00%) (34231/37248)
Epoch: 88 | Batch_idx: 300 |  Loss: (0.2358) |  Loss2: (0.0000) | Acc: (91.00%) (35425/38528)
Epoch: 88 | Batch_idx: 310 |  Loss: (0.2365) |  Loss2: (0.0000) | Acc: (91.00%) (36598/39808)
Epoch: 88 | Batch_idx: 320 |  Loss: (0.2373) |  Loss2: (0.0000) | Acc: (91.00%) (37759/41088)
Epoch: 88 | Batch_idx: 330 |  Loss: (0.2375) |  Loss2: (0.0000) | Acc: (91.00%) (38937/42368)
Epoch: 88 | Batch_idx: 340 |  Loss: (0.2373) |  Loss2: (0.0000) | Acc: (91.00%) (40114/43648)
Epoch: 88 | Batch_idx: 350 |  Loss: (0.2381) |  Loss2: (0.0000) | Acc: (91.00%) (41271/44928)
Epoch: 88 | Batch_idx: 360 |  Loss: (0.2381) |  Loss2: (0.0000) | Acc: (91.00%) (42438/46208)
Epoch: 88 | Batch_idx: 370 |  Loss: (0.2381) |  Loss2: (0.0000) | Acc: (91.00%) (43605/47488)
Epoch: 88 | Batch_idx: 380 |  Loss: (0.2388) |  Loss2: (0.0000) | Acc: (91.00%) (44773/48768)
Epoch: 88 | Batch_idx: 390 |  Loss: (0.2388) |  Loss2: (0.0000) | Acc: (91.00%) (45910/50000)
# TEST : Loss: (0.3976) | Acc: (86.00%) (8693/10000)
percent tensor([0.5693, 0.5872, 0.5677, 0.5582, 0.5731, 0.5571, 0.5872, 0.5785, 0.5821,
        0.5802, 0.5814, 0.5752, 0.5760, 0.5843, 0.5741, 0.5680],
       device='cuda:0') torch.Size([16])
percent tensor([0.4979, 0.4983, 0.4618, 0.4862, 0.4834, 0.4825, 0.4969, 0.5041, 0.4925,
        0.4796, 0.4806, 0.4617, 0.4986, 0.5062, 0.4985, 0.4922],
       device='cuda:0') torch.Size([16])
percent tensor([0.4933, 0.4666, 0.4292, 0.4944, 0.4221, 0.5103, 0.4678, 0.4823, 0.4931,
        0.4486, 0.4877, 0.4317, 0.4524, 0.5408, 0.4911, 0.4949],
       device='cuda:0') torch.Size([16])
percent tensor([0.6313, 0.6397, 0.6010, 0.5879, 0.5918, 0.6424, 0.6271, 0.5846, 0.6069,
        0.6352, 0.6278, 0.6157, 0.6572, 0.6068, 0.6314, 0.6438],
       device='cuda:0') torch.Size([16])
percent tensor([0.6473, 0.5872, 0.6309, 0.6462, 0.6537, 0.6995, 0.6408, 0.6226, 0.7101,
        0.6348, 0.6956, 0.6256, 0.5835, 0.7104, 0.5995, 0.6724],
       device='cuda:0') torch.Size([16])
percent tensor([0.7585, 0.7465, 0.7739, 0.7816, 0.8063, 0.7898, 0.7835, 0.7883, 0.7736,
        0.7572, 0.7677, 0.7541, 0.7397, 0.7853, 0.7565, 0.7810],
       device='cuda:0') torch.Size([16])
percent tensor([0.4677, 0.5492, 0.6144, 0.5564, 0.6597, 0.7310, 0.5459, 0.4227, 0.6063,
        0.5014, 0.5645, 0.4874, 0.4855, 0.5614, 0.3526, 0.5294],
       device='cuda:0') torch.Size([16])
percent tensor([0.9993, 0.9988, 0.9990, 0.9991, 0.9993, 0.9977, 0.9990, 0.9995, 0.9994,
        0.9990, 0.9992, 0.9992, 0.9991, 0.9981, 0.9985, 0.9995],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 89 | Batch_idx: 0 |  Loss: (0.2443) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 89 | Batch_idx: 10 |  Loss: (0.2634) |  Loss2: (0.0000) | Acc: (89.00%) (1267/1408)
Epoch: 89 | Batch_idx: 20 |  Loss: (0.2523) |  Loss2: (0.0000) | Acc: (90.00%) (2435/2688)
Epoch: 89 | Batch_idx: 30 |  Loss: (0.2557) |  Loss2: (0.0000) | Acc: (90.00%) (3607/3968)
Epoch: 89 | Batch_idx: 40 |  Loss: (0.2562) |  Loss2: (0.0000) | Acc: (91.00%) (4782/5248)
Epoch: 89 | Batch_idx: 50 |  Loss: (0.2540) |  Loss2: (0.0000) | Acc: (91.00%) (5952/6528)
Epoch: 89 | Batch_idx: 60 |  Loss: (0.2543) |  Loss2: (0.0000) | Acc: (91.00%) (7108/7808)
Epoch: 89 | Batch_idx: 70 |  Loss: (0.2592) |  Loss2: (0.0000) | Acc: (90.00%) (8266/9088)
Epoch: 89 | Batch_idx: 80 |  Loss: (0.2613) |  Loss2: (0.0000) | Acc: (90.00%) (9423/10368)
Epoch: 89 | Batch_idx: 90 |  Loss: (0.2631) |  Loss2: (0.0000) | Acc: (90.00%) (10575/11648)
Epoch: 89 | Batch_idx: 100 |  Loss: (0.2656) |  Loss2: (0.0000) | Acc: (90.00%) (11723/12928)
Epoch: 89 | Batch_idx: 110 |  Loss: (0.2679) |  Loss2: (0.0000) | Acc: (90.00%) (12874/14208)
Epoch: 89 | Batch_idx: 120 |  Loss: (0.2674) |  Loss2: (0.0000) | Acc: (90.00%) (14051/15488)
Epoch: 89 | Batch_idx: 130 |  Loss: (0.2682) |  Loss2: (0.0000) | Acc: (90.00%) (15200/16768)
Epoch: 89 | Batch_idx: 140 |  Loss: (0.2671) |  Loss2: (0.0000) | Acc: (90.00%) (16365/18048)
Epoch: 89 | Batch_idx: 150 |  Loss: (0.2683) |  Loss2: (0.0000) | Acc: (90.00%) (17515/19328)
Epoch: 89 | Batch_idx: 160 |  Loss: (0.2670) |  Loss2: (0.0000) | Acc: (90.00%) (18680/20608)
Epoch: 89 | Batch_idx: 170 |  Loss: (0.2668) |  Loss2: (0.0000) | Acc: (90.00%) (19850/21888)
Epoch: 89 | Batch_idx: 180 |  Loss: (0.2668) |  Loss2: (0.0000) | Acc: (90.00%) (21006/23168)
Epoch: 89 | Batch_idx: 190 |  Loss: (0.2666) |  Loss2: (0.0000) | Acc: (90.00%) (22166/24448)
Epoch: 89 | Batch_idx: 200 |  Loss: (0.2660) |  Loss2: (0.0000) | Acc: (90.00%) (23335/25728)
Epoch: 89 | Batch_idx: 210 |  Loss: (0.2656) |  Loss2: (0.0000) | Acc: (90.00%) (24503/27008)
Epoch: 89 | Batch_idx: 220 |  Loss: (0.2637) |  Loss2: (0.0000) | Acc: (90.00%) (25684/28288)
Epoch: 89 | Batch_idx: 230 |  Loss: (0.2619) |  Loss2: (0.0000) | Acc: (90.00%) (26867/29568)
Epoch: 89 | Batch_idx: 240 |  Loss: (0.2616) |  Loss2: (0.0000) | Acc: (90.00%) (28023/30848)
Epoch: 89 | Batch_idx: 250 |  Loss: (0.2619) |  Loss2: (0.0000) | Acc: (90.00%) (29185/32128)
Epoch: 89 | Batch_idx: 260 |  Loss: (0.2629) |  Loss2: (0.0000) | Acc: (90.00%) (30335/33408)
Epoch: 89 | Batch_idx: 270 |  Loss: (0.2623) |  Loss2: (0.0000) | Acc: (90.00%) (31499/34688)
Epoch: 89 | Batch_idx: 280 |  Loss: (0.2616) |  Loss2: (0.0000) | Acc: (90.00%) (32667/35968)
Epoch: 89 | Batch_idx: 290 |  Loss: (0.2607) |  Loss2: (0.0000) | Acc: (90.00%) (33839/37248)
Epoch: 89 | Batch_idx: 300 |  Loss: (0.2609) |  Loss2: (0.0000) | Acc: (90.00%) (34990/38528)
Epoch: 89 | Batch_idx: 310 |  Loss: (0.2610) |  Loss2: (0.0000) | Acc: (90.00%) (36148/39808)
Epoch: 89 | Batch_idx: 320 |  Loss: (0.2608) |  Loss2: (0.0000) | Acc: (90.00%) (37310/41088)
Epoch: 89 | Batch_idx: 330 |  Loss: (0.2603) |  Loss2: (0.0000) | Acc: (90.00%) (38473/42368)
Epoch: 89 | Batch_idx: 340 |  Loss: (0.2604) |  Loss2: (0.0000) | Acc: (90.00%) (39639/43648)
Epoch: 89 | Batch_idx: 350 |  Loss: (0.2606) |  Loss2: (0.0000) | Acc: (90.00%) (40796/44928)
Epoch: 89 | Batch_idx: 360 |  Loss: (0.2595) |  Loss2: (0.0000) | Acc: (90.00%) (41986/46208)
Epoch: 89 | Batch_idx: 370 |  Loss: (0.2592) |  Loss2: (0.0000) | Acc: (90.00%) (43159/47488)
Epoch: 89 | Batch_idx: 380 |  Loss: (0.2587) |  Loss2: (0.0000) | Acc: (90.00%) (44330/48768)
Epoch: 89 | Batch_idx: 390 |  Loss: (0.2583) |  Loss2: (0.0000) | Acc: (90.00%) (45441/50000)
# TEST : Loss: (0.4021) | Acc: (86.00%) (8668/10000)
percent tensor([0.5669, 0.5840, 0.5660, 0.5569, 0.5718, 0.5557, 0.5846, 0.5759, 0.5799,
        0.5776, 0.5789, 0.5732, 0.5734, 0.5816, 0.5712, 0.5655],
       device='cuda:0') torch.Size([16])
percent tensor([0.5095, 0.5133, 0.4768, 0.4995, 0.4978, 0.4928, 0.5109, 0.5181, 0.5048,
        0.4941, 0.4933, 0.4772, 0.5105, 0.5158, 0.5111, 0.5047],
       device='cuda:0') torch.Size([16])
percent tensor([0.4956, 0.4700, 0.4306, 0.4927, 0.4254, 0.5095, 0.4744, 0.4859, 0.4914,
        0.4511, 0.4881, 0.4314, 0.4559, 0.5366, 0.4954, 0.4987],
       device='cuda:0') torch.Size([16])
percent tensor([0.6380, 0.6559, 0.6011, 0.5933, 0.5927, 0.6504, 0.6352, 0.5893, 0.6149,
        0.6473, 0.6388, 0.6219, 0.6724, 0.6193, 0.6432, 0.6543],
       device='cuda:0') torch.Size([16])
percent tensor([0.6330, 0.5790, 0.6439, 0.6610, 0.6634, 0.6941, 0.6407, 0.6416, 0.6986,
        0.6289, 0.6784, 0.6267, 0.5575, 0.7049, 0.5948, 0.6710],
       device='cuda:0') torch.Size([16])
percent tensor([0.7663, 0.7572, 0.7859, 0.7904, 0.8135, 0.7944, 0.7931, 0.7964, 0.7761,
        0.7673, 0.7749, 0.7634, 0.7498, 0.7939, 0.7682, 0.7885],
       device='cuda:0') torch.Size([16])
percent tensor([0.5263, 0.6024, 0.6385, 0.5635, 0.6735, 0.7424, 0.5873, 0.4425, 0.6234,
        0.5397, 0.5813, 0.5264, 0.5257, 0.5896, 0.3748, 0.5787],
       device='cuda:0') torch.Size([16])
percent tensor([0.9993, 0.9989, 0.9991, 0.9988, 0.9995, 0.9977, 0.9992, 0.9996, 0.9994,
        0.9991, 0.9991, 0.9994, 0.9989, 0.9983, 0.9986, 0.9996],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 90 | Batch_idx: 0 |  Loss: (0.2061) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 90 | Batch_idx: 10 |  Loss: (0.1994) |  Loss2: (0.0000) | Acc: (93.00%) (1313/1408)
Epoch: 90 | Batch_idx: 20 |  Loss: (0.2004) |  Loss2: (0.0000) | Acc: (93.00%) (2504/2688)
Epoch: 90 | Batch_idx: 30 |  Loss: (0.2088) |  Loss2: (0.0000) | Acc: (92.00%) (3678/3968)
Epoch: 90 | Batch_idx: 40 |  Loss: (0.2162) |  Loss2: (0.0000) | Acc: (92.00%) (4853/5248)
Epoch: 90 | Batch_idx: 50 |  Loss: (0.2199) |  Loss2: (0.0000) | Acc: (92.00%) (6031/6528)
Epoch: 90 | Batch_idx: 60 |  Loss: (0.2225) |  Loss2: (0.0000) | Acc: (92.00%) (7207/7808)
Epoch: 90 | Batch_idx: 70 |  Loss: (0.2246) |  Loss2: (0.0000) | Acc: (92.00%) (8387/9088)
Epoch: 90 | Batch_idx: 80 |  Loss: (0.2253) |  Loss2: (0.0000) | Acc: (92.00%) (9561/10368)
Epoch: 90 | Batch_idx: 90 |  Loss: (0.2267) |  Loss2: (0.0000) | Acc: (92.00%) (10738/11648)
Epoch: 90 | Batch_idx: 100 |  Loss: (0.2267) |  Loss2: (0.0000) | Acc: (92.00%) (11917/12928)
Epoch: 90 | Batch_idx: 110 |  Loss: (0.2258) |  Loss2: (0.0000) | Acc: (92.00%) (13101/14208)
Epoch: 90 | Batch_idx: 120 |  Loss: (0.2278) |  Loss2: (0.0000) | Acc: (92.00%) (14274/15488)
Epoch: 90 | Batch_idx: 130 |  Loss: (0.2277) |  Loss2: (0.0000) | Acc: (92.00%) (15458/16768)
Epoch: 90 | Batch_idx: 140 |  Loss: (0.2287) |  Loss2: (0.0000) | Acc: (92.00%) (16642/18048)
Epoch: 90 | Batch_idx: 150 |  Loss: (0.2300) |  Loss2: (0.0000) | Acc: (92.00%) (17812/19328)
Epoch: 90 | Batch_idx: 160 |  Loss: (0.2299) |  Loss2: (0.0000) | Acc: (92.00%) (18996/20608)
Epoch: 90 | Batch_idx: 170 |  Loss: (0.2304) |  Loss2: (0.0000) | Acc: (92.00%) (20168/21888)
Epoch: 90 | Batch_idx: 180 |  Loss: (0.2309) |  Loss2: (0.0000) | Acc: (92.00%) (21337/23168)
Epoch: 90 | Batch_idx: 190 |  Loss: (0.2305) |  Loss2: (0.0000) | Acc: (92.00%) (22516/24448)
Epoch: 90 | Batch_idx: 200 |  Loss: (0.2310) |  Loss2: (0.0000) | Acc: (92.00%) (23688/25728)
Epoch: 90 | Batch_idx: 210 |  Loss: (0.2302) |  Loss2: (0.0000) | Acc: (92.00%) (24870/27008)
Epoch: 90 | Batch_idx: 220 |  Loss: (0.2301) |  Loss2: (0.0000) | Acc: (92.00%) (26046/28288)
Epoch: 90 | Batch_idx: 230 |  Loss: (0.2312) |  Loss2: (0.0000) | Acc: (92.00%) (27218/29568)
Epoch: 90 | Batch_idx: 240 |  Loss: (0.2308) |  Loss2: (0.0000) | Acc: (92.00%) (28393/30848)
Epoch: 90 | Batch_idx: 250 |  Loss: (0.2309) |  Loss2: (0.0000) | Acc: (92.00%) (29570/32128)
Epoch: 90 | Batch_idx: 260 |  Loss: (0.2302) |  Loss2: (0.0000) | Acc: (92.00%) (30765/33408)
Epoch: 90 | Batch_idx: 270 |  Loss: (0.2297) |  Loss2: (0.0000) | Acc: (92.00%) (31950/34688)
Epoch: 90 | Batch_idx: 280 |  Loss: (0.2302) |  Loss2: (0.0000) | Acc: (92.00%) (33119/35968)
Epoch: 90 | Batch_idx: 290 |  Loss: (0.2298) |  Loss2: (0.0000) | Acc: (92.00%) (34306/37248)
Epoch: 90 | Batch_idx: 300 |  Loss: (0.2299) |  Loss2: (0.0000) | Acc: (92.00%) (35480/38528)
Epoch: 90 | Batch_idx: 310 |  Loss: (0.2304) |  Loss2: (0.0000) | Acc: (92.00%) (36651/39808)
Epoch: 90 | Batch_idx: 320 |  Loss: (0.2310) |  Loss2: (0.0000) | Acc: (92.00%) (37823/41088)
Epoch: 90 | Batch_idx: 330 |  Loss: (0.2311) |  Loss2: (0.0000) | Acc: (92.00%) (39002/42368)
Epoch: 90 | Batch_idx: 340 |  Loss: (0.2329) |  Loss2: (0.0000) | Acc: (91.00%) (40154/43648)
Epoch: 90 | Batch_idx: 350 |  Loss: (0.2327) |  Loss2: (0.0000) | Acc: (92.00%) (41338/44928)
Epoch: 90 | Batch_idx: 360 |  Loss: (0.2325) |  Loss2: (0.0000) | Acc: (92.00%) (42515/46208)
Epoch: 90 | Batch_idx: 370 |  Loss: (0.2322) |  Loss2: (0.0000) | Acc: (91.00%) (43688/47488)
Epoch: 90 | Batch_idx: 380 |  Loss: (0.2324) |  Loss2: (0.0000) | Acc: (91.00%) (44856/48768)
Epoch: 90 | Batch_idx: 390 |  Loss: (0.2331) |  Loss2: (0.0000) | Acc: (91.00%) (45971/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_090.pth.tar'
# TEST : Loss: (0.4280) | Acc: (85.00%) (8587/10000)
percent tensor([0.5679, 0.5864, 0.5662, 0.5592, 0.5714, 0.5557, 0.5859, 0.5781, 0.5812,
        0.5795, 0.5804, 0.5735, 0.5749, 0.5847, 0.5730, 0.5669],
       device='cuda:0') torch.Size([16])
percent tensor([0.5082, 0.5129, 0.4775, 0.4997, 0.4962, 0.4931, 0.5110, 0.5172, 0.5034,
        0.4948, 0.4920, 0.4797, 0.5092, 0.5160, 0.5107, 0.5040],
       device='cuda:0') torch.Size([16])
percent tensor([0.4932, 0.4703, 0.4200, 0.4908, 0.4210, 0.5148, 0.4688, 0.4789, 0.4867,
        0.4461, 0.4848, 0.4219, 0.4508, 0.5354, 0.4977, 0.4994],
       device='cuda:0') torch.Size([16])
percent tensor([0.6415, 0.6576, 0.6070, 0.5941, 0.6004, 0.6618, 0.6375, 0.5875, 0.6153,
        0.6493, 0.6398, 0.6258, 0.6733, 0.6226, 0.6459, 0.6566],
       device='cuda:0') torch.Size([16])
percent tensor([0.6348, 0.5599, 0.6456, 0.6481, 0.6752, 0.6952, 0.6272, 0.6325, 0.6997,
        0.6215, 0.6683, 0.6277, 0.5681, 0.6889, 0.5813, 0.6614],
       device='cuda:0') torch.Size([16])
percent tensor([0.7684, 0.7521, 0.7746, 0.7859, 0.8138, 0.8064, 0.7869, 0.7931, 0.7813,
        0.7622, 0.7728, 0.7612, 0.7561, 0.7863, 0.7642, 0.7894],
       device='cuda:0') torch.Size([16])
percent tensor([0.5187, 0.5578, 0.6068, 0.5491, 0.6622, 0.7479, 0.5509, 0.4064, 0.6541,
        0.5395, 0.5989, 0.5489, 0.5599, 0.5954, 0.3719, 0.5488],
       device='cuda:0') torch.Size([16])
percent tensor([0.9993, 0.9993, 0.9991, 0.9986, 0.9994, 0.9966, 0.9994, 0.9994, 0.9992,
        0.9994, 0.9997, 0.9993, 0.9994, 0.9986, 0.9985, 0.9994],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(177.1057, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(810.1747, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(798.3813, device='cuda:0')
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1515.4226, device='cuda:0')
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(496.5756, device='cuda:0')
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2226.2690, device='cuda:0')
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4299.1035, device='cuda:0')
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1404.2233, device='cuda:0')
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6163.0464, device='cuda:0')
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11895.1621, device='cuda:0')
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3950.3799, device='cuda:0')
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16690.1855, device='cuda:0')
Epoch: 91 | Batch_idx: 0 |  Loss: (0.1863) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 91 | Batch_idx: 10 |  Loss: (0.2489) |  Loss2: (0.0000) | Acc: (91.00%) (1282/1408)
Epoch: 91 | Batch_idx: 20 |  Loss: (0.2649) |  Loss2: (0.0000) | Acc: (90.00%) (2427/2688)
Epoch: 91 | Batch_idx: 30 |  Loss: (0.2892) |  Loss2: (0.0000) | Acc: (89.00%) (3563/3968)
Epoch: 91 | Batch_idx: 40 |  Loss: (0.3024) |  Loss2: (0.0000) | Acc: (89.00%) (4695/5248)
Epoch: 91 | Batch_idx: 50 |  Loss: (0.2981) |  Loss2: (0.0000) | Acc: (89.00%) (5839/6528)
Epoch: 91 | Batch_idx: 60 |  Loss: (0.2919) |  Loss2: (0.0000) | Acc: (89.00%) (7008/7808)
Epoch: 91 | Batch_idx: 70 |  Loss: (0.2990) |  Loss2: (0.0000) | Acc: (89.00%) (8152/9088)
Epoch: 91 | Batch_idx: 80 |  Loss: (0.2987) |  Loss2: (0.0000) | Acc: (89.00%) (9288/10368)
Epoch: 91 | Batch_idx: 90 |  Loss: (0.3000) |  Loss2: (0.0000) | Acc: (89.00%) (10437/11648)
Epoch: 91 | Batch_idx: 100 |  Loss: (0.3047) |  Loss2: (0.0000) | Acc: (89.00%) (11565/12928)
Epoch: 91 | Batch_idx: 110 |  Loss: (0.3098) |  Loss2: (0.0000) | Acc: (89.00%) (12681/14208)
Epoch: 91 | Batch_idx: 120 |  Loss: (0.3099) |  Loss2: (0.0000) | Acc: (89.00%) (13822/15488)
Epoch: 91 | Batch_idx: 130 |  Loss: (0.3099) |  Loss2: (0.0000) | Acc: (89.00%) (14961/16768)
Epoch: 91 | Batch_idx: 140 |  Loss: (0.3062) |  Loss2: (0.0000) | Acc: (89.00%) (16108/18048)
Epoch: 91 | Batch_idx: 150 |  Loss: (0.3061) |  Loss2: (0.0000) | Acc: (89.00%) (17238/19328)
Epoch: 91 | Batch_idx: 160 |  Loss: (0.3069) |  Loss2: (0.0000) | Acc: (89.00%) (18386/20608)
Epoch: 91 | Batch_idx: 170 |  Loss: (0.3043) |  Loss2: (0.0000) | Acc: (89.00%) (19532/21888)
Epoch: 91 | Batch_idx: 180 |  Loss: (0.3037) |  Loss2: (0.0000) | Acc: (89.00%) (20681/23168)
Epoch: 91 | Batch_idx: 190 |  Loss: (0.3032) |  Loss2: (0.0000) | Acc: (89.00%) (21821/24448)
Epoch: 91 | Batch_idx: 200 |  Loss: (0.3024) |  Loss2: (0.0000) | Acc: (89.00%) (22965/25728)
Epoch: 91 | Batch_idx: 210 |  Loss: (0.3031) |  Loss2: (0.0000) | Acc: (89.00%) (24101/27008)
Epoch: 91 | Batch_idx: 220 |  Loss: (0.3022) |  Loss2: (0.0000) | Acc: (89.00%) (25256/28288)
Epoch: 91 | Batch_idx: 230 |  Loss: (0.3008) |  Loss2: (0.0000) | Acc: (89.00%) (26416/29568)
Epoch: 91 | Batch_idx: 240 |  Loss: (0.2992) |  Loss2: (0.0000) | Acc: (89.00%) (27578/30848)
Epoch: 91 | Batch_idx: 250 |  Loss: (0.2985) |  Loss2: (0.0000) | Acc: (89.00%) (28728/32128)
Epoch: 91 | Batch_idx: 260 |  Loss: (0.2971) |  Loss2: (0.0000) | Acc: (89.00%) (29899/33408)
Epoch: 91 | Batch_idx: 270 |  Loss: (0.2964) |  Loss2: (0.0000) | Acc: (89.00%) (31057/34688)
Epoch: 91 | Batch_idx: 280 |  Loss: (0.2959) |  Loss2: (0.0000) | Acc: (89.00%) (32217/35968)
Epoch: 91 | Batch_idx: 290 |  Loss: (0.2957) |  Loss2: (0.0000) | Acc: (89.00%) (33366/37248)
Epoch: 91 | Batch_idx: 300 |  Loss: (0.2944) |  Loss2: (0.0000) | Acc: (89.00%) (34533/38528)
Epoch: 91 | Batch_idx: 310 |  Loss: (0.2948) |  Loss2: (0.0000) | Acc: (89.00%) (35679/39808)
Epoch: 91 | Batch_idx: 320 |  Loss: (0.2945) |  Loss2: (0.0000) | Acc: (89.00%) (36817/41088)
Epoch: 91 | Batch_idx: 330 |  Loss: (0.2936) |  Loss2: (0.0000) | Acc: (89.00%) (37978/42368)
Epoch: 91 | Batch_idx: 340 |  Loss: (0.2930) |  Loss2: (0.0000) | Acc: (89.00%) (39141/43648)
Epoch: 91 | Batch_idx: 350 |  Loss: (0.2923) |  Loss2: (0.0000) | Acc: (89.00%) (40304/44928)
Epoch: 91 | Batch_idx: 360 |  Loss: (0.2917) |  Loss2: (0.0000) | Acc: (89.00%) (41454/46208)
Epoch: 91 | Batch_idx: 370 |  Loss: (0.2910) |  Loss2: (0.0000) | Acc: (89.00%) (42607/47488)
Epoch: 91 | Batch_idx: 380 |  Loss: (0.2914) |  Loss2: (0.0000) | Acc: (89.00%) (43748/48768)
Epoch: 91 | Batch_idx: 390 |  Loss: (0.2904) |  Loss2: (0.0000) | Acc: (89.00%) (44870/50000)
# TEST : Loss: (0.4277) | Acc: (85.00%) (8599/10000)
percent tensor([0.5558, 0.5734, 0.5508, 0.5466, 0.5572, 0.5422, 0.5727, 0.5665, 0.5699,
        0.5669, 0.5681, 0.5588, 0.5627, 0.5744, 0.5601, 0.5546],
       device='cuda:0') torch.Size([16])
percent tensor([0.5059, 0.5113, 0.4699, 0.4984, 0.4950, 0.4899, 0.5107, 0.5161, 0.5028,
        0.4884, 0.4883, 0.4716, 0.5054, 0.5174, 0.5094, 0.5020],
       device='cuda:0') torch.Size([16])
percent tensor([0.5062, 0.4765, 0.4246, 0.5094, 0.4208, 0.5294, 0.4706, 0.4794, 0.4941,
        0.4529, 0.4949, 0.4317, 0.4575, 0.5431, 0.5084, 0.5139],
       device='cuda:0') torch.Size([16])
percent tensor([0.6144, 0.6292, 0.5786, 0.5727, 0.5721, 0.6280, 0.6050, 0.5688, 0.5875,
        0.6205, 0.6079, 0.5911, 0.6419, 0.5942, 0.6173, 0.6275],
       device='cuda:0') torch.Size([16])
percent tensor([0.6677, 0.5802, 0.6874, 0.6911, 0.7078, 0.7371, 0.6460, 0.6653, 0.7333,
        0.6521, 0.7038, 0.6696, 0.5971, 0.7173, 0.6067, 0.7008],
       device='cuda:0') torch.Size([16])
percent tensor([0.7649, 0.7487, 0.7739, 0.7853, 0.8111, 0.8041, 0.7791, 0.7897, 0.7789,
        0.7581, 0.7695, 0.7585, 0.7513, 0.7882, 0.7574, 0.7858],
       device='cuda:0') torch.Size([16])
percent tensor([0.4804, 0.5641, 0.5744, 0.5267, 0.6428, 0.7170, 0.5365, 0.3912, 0.6314,
        0.5251, 0.5776, 0.5057, 0.5521, 0.6031, 0.3482, 0.4961],
       device='cuda:0') torch.Size([16])
percent tensor([0.9994, 0.9991, 0.9986, 0.9984, 0.9994, 0.9976, 0.9992, 0.9995, 0.9990,
        0.9995, 0.9995, 0.9989, 0.9994, 0.9985, 0.9987, 0.9994],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 92 | Batch_idx: 0 |  Loss: (0.2333) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 92 | Batch_idx: 10 |  Loss: (0.2229) |  Loss2: (0.0000) | Acc: (92.00%) (1298/1408)
Epoch: 92 | Batch_idx: 20 |  Loss: (0.2214) |  Loss2: (0.0000) | Acc: (92.00%) (2475/2688)
Epoch: 92 | Batch_idx: 30 |  Loss: (0.2153) |  Loss2: (0.0000) | Acc: (92.00%) (3672/3968)
Epoch: 92 | Batch_idx: 40 |  Loss: (0.2107) |  Loss2: (0.0000) | Acc: (92.00%) (4869/5248)
Epoch: 92 | Batch_idx: 50 |  Loss: (0.2105) |  Loss2: (0.0000) | Acc: (92.00%) (6065/6528)
Epoch: 92 | Batch_idx: 60 |  Loss: (0.2116) |  Loss2: (0.0000) | Acc: (92.00%) (7247/7808)
Epoch: 92 | Batch_idx: 70 |  Loss: (0.2158) |  Loss2: (0.0000) | Acc: (92.00%) (8421/9088)
Epoch: 92 | Batch_idx: 80 |  Loss: (0.2165) |  Loss2: (0.0000) | Acc: (92.00%) (9600/10368)
Epoch: 92 | Batch_idx: 90 |  Loss: (0.2181) |  Loss2: (0.0000) | Acc: (92.00%) (10779/11648)
Epoch: 92 | Batch_idx: 100 |  Loss: (0.2214) |  Loss2: (0.0000) | Acc: (92.00%) (11944/12928)
Epoch: 92 | Batch_idx: 110 |  Loss: (0.2196) |  Loss2: (0.0000) | Acc: (92.00%) (13139/14208)
Epoch: 92 | Batch_idx: 120 |  Loss: (0.2211) |  Loss2: (0.0000) | Acc: (92.00%) (14315/15488)
Epoch: 92 | Batch_idx: 130 |  Loss: (0.2217) |  Loss2: (0.0000) | Acc: (92.00%) (15498/16768)
Epoch: 92 | Batch_idx: 140 |  Loss: (0.2226) |  Loss2: (0.0000) | Acc: (92.00%) (16669/18048)
Epoch: 92 | Batch_idx: 150 |  Loss: (0.2237) |  Loss2: (0.0000) | Acc: (92.00%) (17853/19328)
Epoch: 92 | Batch_idx: 160 |  Loss: (0.2243) |  Loss2: (0.0000) | Acc: (92.00%) (19024/20608)
Epoch: 92 | Batch_idx: 170 |  Loss: (0.2254) |  Loss2: (0.0000) | Acc: (92.00%) (20192/21888)
Epoch: 92 | Batch_idx: 180 |  Loss: (0.2278) |  Loss2: (0.0000) | Acc: (92.00%) (21350/23168)
Epoch: 92 | Batch_idx: 190 |  Loss: (0.2275) |  Loss2: (0.0000) | Acc: (92.00%) (22541/24448)
Epoch: 92 | Batch_idx: 200 |  Loss: (0.2262) |  Loss2: (0.0000) | Acc: (92.00%) (23736/25728)
Epoch: 92 | Batch_idx: 210 |  Loss: (0.2266) |  Loss2: (0.0000) | Acc: (92.00%) (24907/27008)
Epoch: 92 | Batch_idx: 220 |  Loss: (0.2276) |  Loss2: (0.0000) | Acc: (92.00%) (26085/28288)
Epoch: 92 | Batch_idx: 230 |  Loss: (0.2266) |  Loss2: (0.0000) | Acc: (92.00%) (27278/29568)
Epoch: 92 | Batch_idx: 240 |  Loss: (0.2266) |  Loss2: (0.0000) | Acc: (92.00%) (28454/30848)
Epoch: 92 | Batch_idx: 250 |  Loss: (0.2258) |  Loss2: (0.0000) | Acc: (92.00%) (29646/32128)
Epoch: 92 | Batch_idx: 260 |  Loss: (0.2272) |  Loss2: (0.0000) | Acc: (92.00%) (30822/33408)
Epoch: 92 | Batch_idx: 270 |  Loss: (0.2283) |  Loss2: (0.0000) | Acc: (92.00%) (31983/34688)
Epoch: 92 | Batch_idx: 280 |  Loss: (0.2281) |  Loss2: (0.0000) | Acc: (92.00%) (33169/35968)
Epoch: 92 | Batch_idx: 290 |  Loss: (0.2287) |  Loss2: (0.0000) | Acc: (92.00%) (34350/37248)
Epoch: 92 | Batch_idx: 300 |  Loss: (0.2292) |  Loss2: (0.0000) | Acc: (92.00%) (35514/38528)
Epoch: 92 | Batch_idx: 310 |  Loss: (0.2290) |  Loss2: (0.0000) | Acc: (92.00%) (36691/39808)
Epoch: 92 | Batch_idx: 320 |  Loss: (0.2285) |  Loss2: (0.0000) | Acc: (92.00%) (37873/41088)
Epoch: 92 | Batch_idx: 330 |  Loss: (0.2289) |  Loss2: (0.0000) | Acc: (92.00%) (39044/42368)
Epoch: 92 | Batch_idx: 340 |  Loss: (0.2290) |  Loss2: (0.0000) | Acc: (92.00%) (40219/43648)
Epoch: 92 | Batch_idx: 350 |  Loss: (0.2294) |  Loss2: (0.0000) | Acc: (92.00%) (41398/44928)
Epoch: 92 | Batch_idx: 360 |  Loss: (0.2289) |  Loss2: (0.0000) | Acc: (92.00%) (42584/46208)
Epoch: 92 | Batch_idx: 370 |  Loss: (0.2294) |  Loss2: (0.0000) | Acc: (92.00%) (43745/47488)
Epoch: 92 | Batch_idx: 380 |  Loss: (0.2291) |  Loss2: (0.0000) | Acc: (92.00%) (44935/48768)
Epoch: 92 | Batch_idx: 390 |  Loss: (0.2295) |  Loss2: (0.0000) | Acc: (92.00%) (46069/50000)
# TEST : Loss: (0.4101) | Acc: (86.00%) (8669/10000)
percent tensor([0.5570, 0.5718, 0.5564, 0.5473, 0.5603, 0.5448, 0.5727, 0.5662, 0.5698,
        0.5668, 0.5684, 0.5631, 0.5632, 0.5707, 0.5602, 0.5547],
       device='cuda:0') torch.Size([16])
percent tensor([0.5037, 0.5111, 0.4707, 0.4986, 0.4959, 0.4896, 0.5104, 0.5173, 0.5005,
        0.4902, 0.4869, 0.4729, 0.5048, 0.5166, 0.5104, 0.5005],
       device='cuda:0') torch.Size([16])
percent tensor([0.5044, 0.4779, 0.4423, 0.5084, 0.4307, 0.5283, 0.4781, 0.4839, 0.4966,
        0.4615, 0.4929, 0.4464, 0.4573, 0.5479, 0.5105, 0.5085],
       device='cuda:0') torch.Size([16])
percent tensor([0.6161, 0.6313, 0.5817, 0.5723, 0.5738, 0.6311, 0.6081, 0.5657, 0.5963,
        0.6230, 0.6177, 0.5948, 0.6468, 0.5969, 0.6203, 0.6315],
       device='cuda:0') torch.Size([16])
percent tensor([0.6624, 0.5917, 0.6697, 0.6684, 0.7052, 0.7320, 0.6499, 0.6626, 0.7234,
        0.6420, 0.6922, 0.6409, 0.5753, 0.7146, 0.6129, 0.6983],
       device='cuda:0') torch.Size([16])
percent tensor([0.7596, 0.7443, 0.7759, 0.7796, 0.8155, 0.7905, 0.7828, 0.7885, 0.7706,
        0.7551, 0.7646, 0.7517, 0.7460, 0.7743, 0.7518, 0.7841],
       device='cuda:0') torch.Size([16])
percent tensor([0.4630, 0.5163, 0.6041, 0.5415, 0.6656, 0.7028, 0.5291, 0.4097, 0.6001,
        0.4816, 0.5339, 0.4614, 0.4816, 0.5146, 0.3355, 0.5119],
       device='cuda:0') torch.Size([16])
percent tensor([0.9995, 0.9992, 0.9990, 0.9991, 0.9995, 0.9973, 0.9994, 0.9996, 0.9993,
        0.9993, 0.9996, 0.9993, 0.9990, 0.9991, 0.9987, 0.9996],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 93 | Batch_idx: 0 |  Loss: (0.2893) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 93 | Batch_idx: 10 |  Loss: (0.2540) |  Loss2: (0.0000) | Acc: (90.00%) (1279/1408)
Epoch: 93 | Batch_idx: 20 |  Loss: (0.2533) |  Loss2: (0.0000) | Acc: (90.00%) (2441/2688)
Epoch: 93 | Batch_idx: 30 |  Loss: (0.2586) |  Loss2: (0.0000) | Acc: (90.00%) (3605/3968)
Epoch: 93 | Batch_idx: 40 |  Loss: (0.2559) |  Loss2: (0.0000) | Acc: (90.00%) (4775/5248)
Epoch: 93 | Batch_idx: 50 |  Loss: (0.2610) |  Loss2: (0.0000) | Acc: (90.00%) (5920/6528)
Epoch: 93 | Batch_idx: 60 |  Loss: (0.2645) |  Loss2: (0.0000) | Acc: (90.00%) (7068/7808)
Epoch: 93 | Batch_idx: 70 |  Loss: (0.2681) |  Loss2: (0.0000) | Acc: (90.00%) (8219/9088)
Epoch: 93 | Batch_idx: 80 |  Loss: (0.2708) |  Loss2: (0.0000) | Acc: (90.00%) (9374/10368)
Epoch: 93 | Batch_idx: 90 |  Loss: (0.2694) |  Loss2: (0.0000) | Acc: (90.00%) (10537/11648)
Epoch: 93 | Batch_idx: 100 |  Loss: (0.2690) |  Loss2: (0.0000) | Acc: (90.00%) (11697/12928)
Epoch: 93 | Batch_idx: 110 |  Loss: (0.2687) |  Loss2: (0.0000) | Acc: (90.00%) (12868/14208)
Epoch: 93 | Batch_idx: 120 |  Loss: (0.2656) |  Loss2: (0.0000) | Acc: (90.00%) (14043/15488)
Epoch: 93 | Batch_idx: 130 |  Loss: (0.2658) |  Loss2: (0.0000) | Acc: (90.00%) (15206/16768)
Epoch: 93 | Batch_idx: 140 |  Loss: (0.2652) |  Loss2: (0.0000) | Acc: (90.00%) (16368/18048)
Epoch: 93 | Batch_idx: 150 |  Loss: (0.2661) |  Loss2: (0.0000) | Acc: (90.00%) (17516/19328)
Epoch: 93 | Batch_idx: 160 |  Loss: (0.2662) |  Loss2: (0.0000) | Acc: (90.00%) (18680/20608)
Epoch: 93 | Batch_idx: 170 |  Loss: (0.2665) |  Loss2: (0.0000) | Acc: (90.00%) (19841/21888)
Epoch: 93 | Batch_idx: 180 |  Loss: (0.2645) |  Loss2: (0.0000) | Acc: (90.00%) (21022/23168)
Epoch: 93 | Batch_idx: 190 |  Loss: (0.2646) |  Loss2: (0.0000) | Acc: (90.00%) (22176/24448)
Epoch: 93 | Batch_idx: 200 |  Loss: (0.2639) |  Loss2: (0.0000) | Acc: (90.00%) (23343/25728)
Epoch: 93 | Batch_idx: 210 |  Loss: (0.2649) |  Loss2: (0.0000) | Acc: (90.00%) (24494/27008)
Epoch: 93 | Batch_idx: 220 |  Loss: (0.2649) |  Loss2: (0.0000) | Acc: (90.00%) (25655/28288)
Epoch: 93 | Batch_idx: 230 |  Loss: (0.2653) |  Loss2: (0.0000) | Acc: (90.00%) (26825/29568)
Epoch: 93 | Batch_idx: 240 |  Loss: (0.2636) |  Loss2: (0.0000) | Acc: (90.00%) (28015/30848)
Epoch: 93 | Batch_idx: 250 |  Loss: (0.2626) |  Loss2: (0.0000) | Acc: (90.00%) (29193/32128)
Epoch: 93 | Batch_idx: 260 |  Loss: (0.2606) |  Loss2: (0.0000) | Acc: (90.00%) (30387/33408)
Epoch: 93 | Batch_idx: 270 |  Loss: (0.2600) |  Loss2: (0.0000) | Acc: (90.00%) (31556/34688)
Epoch: 93 | Batch_idx: 280 |  Loss: (0.2594) |  Loss2: (0.0000) | Acc: (91.00%) (32735/35968)
Epoch: 93 | Batch_idx: 290 |  Loss: (0.2595) |  Loss2: (0.0000) | Acc: (91.00%) (33897/37248)
Epoch: 93 | Batch_idx: 300 |  Loss: (0.2601) |  Loss2: (0.0000) | Acc: (90.00%) (35050/38528)
Epoch: 93 | Batch_idx: 310 |  Loss: (0.2604) |  Loss2: (0.0000) | Acc: (90.00%) (36206/39808)
Epoch: 93 | Batch_idx: 320 |  Loss: (0.2586) |  Loss2: (0.0000) | Acc: (91.00%) (37398/41088)
Epoch: 93 | Batch_idx: 330 |  Loss: (0.2592) |  Loss2: (0.0000) | Acc: (90.00%) (38553/42368)
Epoch: 93 | Batch_idx: 340 |  Loss: (0.2597) |  Loss2: (0.0000) | Acc: (90.00%) (39710/43648)
Epoch: 93 | Batch_idx: 350 |  Loss: (0.2590) |  Loss2: (0.0000) | Acc: (91.00%) (40887/44928)
Epoch: 93 | Batch_idx: 360 |  Loss: (0.2583) |  Loss2: (0.0000) | Acc: (91.00%) (42069/46208)
Epoch: 93 | Batch_idx: 370 |  Loss: (0.2580) |  Loss2: (0.0000) | Acc: (91.00%) (43233/47488)
Epoch: 93 | Batch_idx: 380 |  Loss: (0.2579) |  Loss2: (0.0000) | Acc: (91.00%) (44388/48768)
Epoch: 93 | Batch_idx: 390 |  Loss: (0.2581) |  Loss2: (0.0000) | Acc: (91.00%) (45501/50000)
# TEST : Loss: (0.4049) | Acc: (86.00%) (8675/10000)
percent tensor([0.5645, 0.5790, 0.5689, 0.5565, 0.5730, 0.5528, 0.5815, 0.5753, 0.5774,
        0.5754, 0.5755, 0.5742, 0.5709, 0.5761, 0.5676, 0.5617],
       device='cuda:0') torch.Size([16])
percent tensor([0.5030, 0.5121, 0.4679, 0.4957, 0.4929, 0.4877, 0.5108, 0.5126, 0.4992,
        0.4904, 0.4867, 0.4747, 0.5053, 0.5185, 0.5093, 0.4992],
       device='cuda:0') torch.Size([16])
percent tensor([0.5118, 0.4945, 0.4543, 0.5137, 0.4438, 0.5311, 0.4924, 0.4916, 0.5033,
        0.4758, 0.5040, 0.4631, 0.4740, 0.5465, 0.5203, 0.5155],
       device='cuda:0') torch.Size([16])
percent tensor([0.6536, 0.6759, 0.6090, 0.6043, 0.6001, 0.6650, 0.6445, 0.5964, 0.6330,
        0.6636, 0.6617, 0.6304, 0.6899, 0.6417, 0.6622, 0.6687],
       device='cuda:0') torch.Size([16])
percent tensor([0.6492, 0.5800, 0.6498, 0.6611, 0.6701, 0.7147, 0.6297, 0.6454, 0.7081,
        0.6189, 0.6697, 0.6225, 0.5828, 0.6907, 0.6036, 0.6842],
       device='cuda:0') torch.Size([16])
percent tensor([0.7351, 0.7205, 0.7552, 0.7629, 0.7927, 0.7679, 0.7583, 0.7569, 0.7500,
        0.7289, 0.7378, 0.7294, 0.7211, 0.7527, 0.7208, 0.7635],
       device='cuda:0') torch.Size([16])
percent tensor([0.4332, 0.4621, 0.5602, 0.5180, 0.6390, 0.6688, 0.5233, 0.4277, 0.5510,
        0.4302, 0.4590, 0.4304, 0.4163, 0.4749, 0.3251, 0.5108],
       device='cuda:0') torch.Size([16])
percent tensor([0.9994, 0.9991, 0.9993, 0.9993, 0.9997, 0.9968, 0.9992, 0.9997, 0.9989,
        0.9990, 0.9993, 0.9992, 0.9990, 0.9984, 0.9989, 0.9996],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 94 | Batch_idx: 0 |  Loss: (0.2886) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 94 | Batch_idx: 10 |  Loss: (0.2250) |  Loss2: (0.0000) | Acc: (92.00%) (1302/1408)
Epoch: 94 | Batch_idx: 20 |  Loss: (0.2264) |  Loss2: (0.0000) | Acc: (92.00%) (2481/2688)
Epoch: 94 | Batch_idx: 30 |  Loss: (0.2291) |  Loss2: (0.0000) | Acc: (91.00%) (3649/3968)
Epoch: 94 | Batch_idx: 40 |  Loss: (0.2198) |  Loss2: (0.0000) | Acc: (92.00%) (4849/5248)
Epoch: 94 | Batch_idx: 50 |  Loss: (0.2165) |  Loss2: (0.0000) | Acc: (92.00%) (6038/6528)
Epoch: 94 | Batch_idx: 60 |  Loss: (0.2100) |  Loss2: (0.0000) | Acc: (92.00%) (7247/7808)
Epoch: 94 | Batch_idx: 70 |  Loss: (0.2105) |  Loss2: (0.0000) | Acc: (92.00%) (8432/9088)
Epoch: 94 | Batch_idx: 80 |  Loss: (0.2094) |  Loss2: (0.0000) | Acc: (92.00%) (9626/10368)
Epoch: 94 | Batch_idx: 90 |  Loss: (0.2083) |  Loss2: (0.0000) | Acc: (92.00%) (10821/11648)
Epoch: 94 | Batch_idx: 100 |  Loss: (0.2109) |  Loss2: (0.0000) | Acc: (92.00%) (12005/12928)
Epoch: 94 | Batch_idx: 110 |  Loss: (0.2103) |  Loss2: (0.0000) | Acc: (92.00%) (13203/14208)
Epoch: 94 | Batch_idx: 120 |  Loss: (0.2115) |  Loss2: (0.0000) | Acc: (92.00%) (14381/15488)
Epoch: 94 | Batch_idx: 130 |  Loss: (0.2122) |  Loss2: (0.0000) | Acc: (92.00%) (15565/16768)
Epoch: 94 | Batch_idx: 140 |  Loss: (0.2129) |  Loss2: (0.0000) | Acc: (92.00%) (16745/18048)
Epoch: 94 | Batch_idx: 150 |  Loss: (0.2116) |  Loss2: (0.0000) | Acc: (92.00%) (17941/19328)
Epoch: 94 | Batch_idx: 160 |  Loss: (0.2128) |  Loss2: (0.0000) | Acc: (92.00%) (19109/20608)
Epoch: 94 | Batch_idx: 170 |  Loss: (0.2116) |  Loss2: (0.0000) | Acc: (92.00%) (20299/21888)
Epoch: 94 | Batch_idx: 180 |  Loss: (0.2109) |  Loss2: (0.0000) | Acc: (92.00%) (21497/23168)
Epoch: 94 | Batch_idx: 190 |  Loss: (0.2112) |  Loss2: (0.0000) | Acc: (92.00%) (22671/24448)
Epoch: 94 | Batch_idx: 200 |  Loss: (0.2114) |  Loss2: (0.0000) | Acc: (92.00%) (23861/25728)
Epoch: 94 | Batch_idx: 210 |  Loss: (0.2132) |  Loss2: (0.0000) | Acc: (92.00%) (25040/27008)
Epoch: 94 | Batch_idx: 220 |  Loss: (0.2133) |  Loss2: (0.0000) | Acc: (92.00%) (26229/28288)
Epoch: 94 | Batch_idx: 230 |  Loss: (0.2135) |  Loss2: (0.0000) | Acc: (92.00%) (27409/29568)
Epoch: 94 | Batch_idx: 240 |  Loss: (0.2143) |  Loss2: (0.0000) | Acc: (92.00%) (28586/30848)
Epoch: 94 | Batch_idx: 250 |  Loss: (0.2158) |  Loss2: (0.0000) | Acc: (92.00%) (29756/32128)
Epoch: 94 | Batch_idx: 260 |  Loss: (0.2165) |  Loss2: (0.0000) | Acc: (92.00%) (30929/33408)
Epoch: 94 | Batch_idx: 270 |  Loss: (0.2169) |  Loss2: (0.0000) | Acc: (92.00%) (32106/34688)
Epoch: 94 | Batch_idx: 280 |  Loss: (0.2174) |  Loss2: (0.0000) | Acc: (92.00%) (33284/35968)
Epoch: 94 | Batch_idx: 290 |  Loss: (0.2180) |  Loss2: (0.0000) | Acc: (92.00%) (34447/37248)
Epoch: 94 | Batch_idx: 300 |  Loss: (0.2175) |  Loss2: (0.0000) | Acc: (92.00%) (35646/38528)
Epoch: 94 | Batch_idx: 310 |  Loss: (0.2175) |  Loss2: (0.0000) | Acc: (92.00%) (36819/39808)
Epoch: 94 | Batch_idx: 320 |  Loss: (0.2184) |  Loss2: (0.0000) | Acc: (92.00%) (37989/41088)
Epoch: 94 | Batch_idx: 330 |  Loss: (0.2183) |  Loss2: (0.0000) | Acc: (92.00%) (39183/42368)
Epoch: 94 | Batch_idx: 340 |  Loss: (0.2181) |  Loss2: (0.0000) | Acc: (92.00%) (40373/43648)
Epoch: 94 | Batch_idx: 350 |  Loss: (0.2172) |  Loss2: (0.0000) | Acc: (92.00%) (41576/44928)
Epoch: 94 | Batch_idx: 360 |  Loss: (0.2177) |  Loss2: (0.0000) | Acc: (92.00%) (42754/46208)
Epoch: 94 | Batch_idx: 370 |  Loss: (0.2179) |  Loss2: (0.0000) | Acc: (92.00%) (43941/47488)
Epoch: 94 | Batch_idx: 380 |  Loss: (0.2181) |  Loss2: (0.0000) | Acc: (92.00%) (45122/48768)
Epoch: 94 | Batch_idx: 390 |  Loss: (0.2184) |  Loss2: (0.0000) | Acc: (92.00%) (46262/50000)
# TEST : Loss: (0.4365) | Acc: (85.00%) (8582/10000)
percent tensor([0.5654, 0.5796, 0.5690, 0.5561, 0.5743, 0.5531, 0.5839, 0.5765, 0.5786,
        0.5769, 0.5755, 0.5770, 0.5715, 0.5773, 0.5684, 0.5623],
       device='cuda:0') torch.Size([16])
percent tensor([0.5085, 0.5087, 0.4773, 0.4954, 0.4956, 0.4894, 0.5108, 0.5149, 0.5028,
        0.4897, 0.4871, 0.4772, 0.5073, 0.5158, 0.5073, 0.4996],
       device='cuda:0') torch.Size([16])
percent tensor([0.5106, 0.4849, 0.4431, 0.5129, 0.4329, 0.5287, 0.4761, 0.4887, 0.4941,
        0.4615, 0.5004, 0.4373, 0.4667, 0.5405, 0.5139, 0.5157],
       device='cuda:0') torch.Size([16])
percent tensor([0.6558, 0.6657, 0.6311, 0.6052, 0.6173, 0.6641, 0.6487, 0.6053, 0.6373,
        0.6632, 0.6572, 0.6430, 0.6896, 0.6296, 0.6578, 0.6696],
       device='cuda:0') torch.Size([16])
percent tensor([0.6367, 0.5839, 0.6493, 0.6690, 0.6737, 0.7130, 0.6287, 0.6359, 0.7067,
        0.6310, 0.6793, 0.6519, 0.5691, 0.7225, 0.6126, 0.6766],
       device='cuda:0') torch.Size([16])
percent tensor([0.7329, 0.7237, 0.7465, 0.7615, 0.7835, 0.7692, 0.7535, 0.7530, 0.7491,
        0.7378, 0.7434, 0.7321, 0.7221, 0.7659, 0.7230, 0.7594],
       device='cuda:0') torch.Size([16])
percent tensor([0.4206, 0.4936, 0.5669, 0.4997, 0.6160, 0.6694, 0.5086, 0.4119, 0.5498,
        0.4777, 0.4773, 0.4426, 0.4354, 0.5031, 0.3431, 0.5022],
       device='cuda:0') torch.Size([16])
percent tensor([0.9995, 0.9991, 0.9991, 0.9992, 0.9995, 0.9974, 0.9991, 0.9995, 0.9991,
        0.9993, 0.9995, 0.9994, 0.9992, 0.9989, 0.9988, 0.9996],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 95 | Batch_idx: 0 |  Loss: (0.1890) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 95 | Batch_idx: 10 |  Loss: (0.2282) |  Loss2: (0.0000) | Acc: (91.00%) (1292/1408)
Epoch: 95 | Batch_idx: 20 |  Loss: (0.2486) |  Loss2: (0.0000) | Acc: (91.00%) (2448/2688)
Epoch: 95 | Batch_idx: 30 |  Loss: (0.2590) |  Loss2: (0.0000) | Acc: (90.00%) (3594/3968)
Epoch: 95 | Batch_idx: 40 |  Loss: (0.2704) |  Loss2: (0.0000) | Acc: (90.00%) (4735/5248)
Epoch: 95 | Batch_idx: 50 |  Loss: (0.2820) |  Loss2: (0.0000) | Acc: (89.00%) (5874/6528)
Epoch: 95 | Batch_idx: 60 |  Loss: (0.2888) |  Loss2: (0.0000) | Acc: (89.00%) (6997/7808)
Epoch: 95 | Batch_idx: 70 |  Loss: (0.2924) |  Loss2: (0.0000) | Acc: (89.00%) (8129/9088)
Epoch: 95 | Batch_idx: 80 |  Loss: (0.2933) |  Loss2: (0.0000) | Acc: (89.00%) (9276/10368)
Epoch: 95 | Batch_idx: 90 |  Loss: (0.2903) |  Loss2: (0.0000) | Acc: (89.00%) (10435/11648)
Epoch: 95 | Batch_idx: 100 |  Loss: (0.2907) |  Loss2: (0.0000) | Acc: (89.00%) (11584/12928)
Epoch: 95 | Batch_idx: 110 |  Loss: (0.2910) |  Loss2: (0.0000) | Acc: (89.00%) (12736/14208)
Epoch: 95 | Batch_idx: 120 |  Loss: (0.2931) |  Loss2: (0.0000) | Acc: (89.00%) (13889/15488)
Epoch: 95 | Batch_idx: 130 |  Loss: (0.2952) |  Loss2: (0.0000) | Acc: (89.00%) (15031/16768)
Epoch: 95 | Batch_idx: 140 |  Loss: (0.2946) |  Loss2: (0.0000) | Acc: (89.00%) (16180/18048)
Epoch: 95 | Batch_idx: 150 |  Loss: (0.2926) |  Loss2: (0.0000) | Acc: (89.00%) (17341/19328)
Epoch: 95 | Batch_idx: 160 |  Loss: (0.2919) |  Loss2: (0.0000) | Acc: (89.00%) (18500/20608)
Epoch: 95 | Batch_idx: 170 |  Loss: (0.2900) |  Loss2: (0.0000) | Acc: (89.00%) (19667/21888)
Epoch: 95 | Batch_idx: 180 |  Loss: (0.2887) |  Loss2: (0.0000) | Acc: (89.00%) (20833/23168)
Epoch: 95 | Batch_idx: 190 |  Loss: (0.2877) |  Loss2: (0.0000) | Acc: (89.00%) (21989/24448)
Epoch: 95 | Batch_idx: 200 |  Loss: (0.2877) |  Loss2: (0.0000) | Acc: (89.00%) (23144/25728)
Epoch: 95 | Batch_idx: 210 |  Loss: (0.2874) |  Loss2: (0.0000) | Acc: (89.00%) (24298/27008)
Epoch: 95 | Batch_idx: 220 |  Loss: (0.2864) |  Loss2: (0.0000) | Acc: (90.00%) (25462/28288)
Epoch: 95 | Batch_idx: 230 |  Loss: (0.2857) |  Loss2: (0.0000) | Acc: (90.00%) (26619/29568)
Epoch: 95 | Batch_idx: 240 |  Loss: (0.2855) |  Loss2: (0.0000) | Acc: (90.00%) (27768/30848)
Epoch: 95 | Batch_idx: 250 |  Loss: (0.2849) |  Loss2: (0.0000) | Acc: (90.00%) (28933/32128)
Epoch: 95 | Batch_idx: 260 |  Loss: (0.2844) |  Loss2: (0.0000) | Acc: (90.00%) (30083/33408)
Epoch: 95 | Batch_idx: 270 |  Loss: (0.2845) |  Loss2: (0.0000) | Acc: (90.00%) (31239/34688)
Epoch: 95 | Batch_idx: 280 |  Loss: (0.2848) |  Loss2: (0.0000) | Acc: (90.00%) (32384/35968)
Epoch: 95 | Batch_idx: 290 |  Loss: (0.2850) |  Loss2: (0.0000) | Acc: (90.00%) (33531/37248)
Epoch: 95 | Batch_idx: 300 |  Loss: (0.2859) |  Loss2: (0.0000) | Acc: (89.00%) (34674/38528)
Epoch: 95 | Batch_idx: 310 |  Loss: (0.2861) |  Loss2: (0.0000) | Acc: (90.00%) (35836/39808)
Epoch: 95 | Batch_idx: 320 |  Loss: (0.2845) |  Loss2: (0.0000) | Acc: (90.00%) (37012/41088)
Epoch: 95 | Batch_idx: 330 |  Loss: (0.2844) |  Loss2: (0.0000) | Acc: (90.00%) (38164/42368)
Epoch: 95 | Batch_idx: 340 |  Loss: (0.2835) |  Loss2: (0.0000) | Acc: (90.00%) (39338/43648)
Epoch: 95 | Batch_idx: 350 |  Loss: (0.2833) |  Loss2: (0.0000) | Acc: (90.00%) (40497/44928)
Epoch: 95 | Batch_idx: 360 |  Loss: (0.2823) |  Loss2: (0.0000) | Acc: (90.00%) (41666/46208)
Epoch: 95 | Batch_idx: 370 |  Loss: (0.2828) |  Loss2: (0.0000) | Acc: (90.00%) (42819/47488)
Epoch: 95 | Batch_idx: 380 |  Loss: (0.2823) |  Loss2: (0.0000) | Acc: (90.00%) (43978/48768)
Epoch: 95 | Batch_idx: 390 |  Loss: (0.2822) |  Loss2: (0.0000) | Acc: (90.00%) (45091/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_095.pth.tar'
# TEST : Loss: (0.4182) | Acc: (86.00%) (8645/10000)
percent tensor([0.5684, 0.5831, 0.5696, 0.5559, 0.5753, 0.5535, 0.5878, 0.5780, 0.5818,
        0.5801, 0.5789, 0.5799, 0.5747, 0.5803, 0.5704, 0.5641],
       device='cuda:0') torch.Size([16])
percent tensor([0.5225, 0.5251, 0.4827, 0.5031, 0.5018, 0.5015, 0.5248, 0.5248, 0.5145,
        0.5040, 0.5030, 0.4879, 0.5236, 0.5289, 0.5210, 0.5122],
       device='cuda:0') torch.Size([16])
percent tensor([0.5183, 0.4821, 0.4445, 0.5205, 0.4333, 0.5326, 0.4747, 0.4881, 0.4979,
        0.4651, 0.5024, 0.4393, 0.4746, 0.5395, 0.5150, 0.5252],
       device='cuda:0') torch.Size([16])
percent tensor([0.6845, 0.6988, 0.6418, 0.6243, 0.6332, 0.6884, 0.6779, 0.6236, 0.6707,
        0.6952, 0.6935, 0.6669, 0.7259, 0.6627, 0.6886, 0.6994],
       device='cuda:0') torch.Size([16])
percent tensor([0.6518, 0.6193, 0.6370, 0.6412, 0.6506, 0.6999, 0.6534, 0.6204, 0.7083,
        0.6660, 0.7047, 0.6726, 0.6173, 0.7243, 0.6402, 0.6910],
       device='cuda:0') torch.Size([16])
percent tensor([0.7475, 0.7343, 0.7646, 0.7773, 0.7976, 0.7854, 0.7662, 0.7719, 0.7610,
        0.7494, 0.7550, 0.7475, 0.7385, 0.7711, 0.7354, 0.7764],
       device='cuda:0') torch.Size([16])
percent tensor([0.4042, 0.4511, 0.6004, 0.5091, 0.6303, 0.6884, 0.4841, 0.4231, 0.5381,
        0.4424, 0.4340, 0.4571, 0.4170, 0.4716, 0.3320, 0.4987],
       device='cuda:0') torch.Size([16])
percent tensor([0.9995, 0.9991, 0.9990, 0.9990, 0.9994, 0.9978, 0.9992, 0.9994, 0.9992,
        0.9994, 0.9996, 0.9996, 0.9993, 0.9990, 0.9987, 0.9995],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 96 | Batch_idx: 0 |  Loss: (0.2704) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 96 | Batch_idx: 10 |  Loss: (0.2405) |  Loss2: (0.0000) | Acc: (91.00%) (1292/1408)
Epoch: 96 | Batch_idx: 20 |  Loss: (0.2232) |  Loss2: (0.0000) | Acc: (92.00%) (2483/2688)
Epoch: 96 | Batch_idx: 30 |  Loss: (0.2150) |  Loss2: (0.0000) | Acc: (92.00%) (3675/3968)
Epoch: 96 | Batch_idx: 40 |  Loss: (0.2130) |  Loss2: (0.0000) | Acc: (92.00%) (4862/5248)
Epoch: 96 | Batch_idx: 50 |  Loss: (0.2128) |  Loss2: (0.0000) | Acc: (92.00%) (6056/6528)
Epoch: 96 | Batch_idx: 60 |  Loss: (0.2134) |  Loss2: (0.0000) | Acc: (92.00%) (7241/7808)
Epoch: 96 | Batch_idx: 70 |  Loss: (0.2140) |  Loss2: (0.0000) | Acc: (92.00%) (8418/9088)
Epoch: 96 | Batch_idx: 80 |  Loss: (0.2158) |  Loss2: (0.0000) | Acc: (92.00%) (9594/10368)
Epoch: 96 | Batch_idx: 90 |  Loss: (0.2165) |  Loss2: (0.0000) | Acc: (92.00%) (10774/11648)
Epoch: 96 | Batch_idx: 100 |  Loss: (0.2184) |  Loss2: (0.0000) | Acc: (92.00%) (11953/12928)
Epoch: 96 | Batch_idx: 110 |  Loss: (0.2185) |  Loss2: (0.0000) | Acc: (92.00%) (13140/14208)
Epoch: 96 | Batch_idx: 120 |  Loss: (0.2180) |  Loss2: (0.0000) | Acc: (92.00%) (14319/15488)
Epoch: 96 | Batch_idx: 130 |  Loss: (0.2190) |  Loss2: (0.0000) | Acc: (92.00%) (15497/16768)
Epoch: 96 | Batch_idx: 140 |  Loss: (0.2190) |  Loss2: (0.0000) | Acc: (92.00%) (16683/18048)
Epoch: 96 | Batch_idx: 150 |  Loss: (0.2203) |  Loss2: (0.0000) | Acc: (92.00%) (17869/19328)
Epoch: 96 | Batch_idx: 160 |  Loss: (0.2197) |  Loss2: (0.0000) | Acc: (92.00%) (19061/20608)
Epoch: 96 | Batch_idx: 170 |  Loss: (0.2209) |  Loss2: (0.0000) | Acc: (92.00%) (20235/21888)
Epoch: 96 | Batch_idx: 180 |  Loss: (0.2208) |  Loss2: (0.0000) | Acc: (92.00%) (21427/23168)
Epoch: 96 | Batch_idx: 190 |  Loss: (0.2209) |  Loss2: (0.0000) | Acc: (92.00%) (22608/24448)
Epoch: 96 | Batch_idx: 200 |  Loss: (0.2199) |  Loss2: (0.0000) | Acc: (92.00%) (23801/25728)
Epoch: 96 | Batch_idx: 210 |  Loss: (0.2185) |  Loss2: (0.0000) | Acc: (92.00%) (25004/27008)
Epoch: 96 | Batch_idx: 220 |  Loss: (0.2171) |  Loss2: (0.0000) | Acc: (92.00%) (26207/28288)
Epoch: 96 | Batch_idx: 230 |  Loss: (0.2158) |  Loss2: (0.0000) | Acc: (92.00%) (27407/29568)
Epoch: 96 | Batch_idx: 240 |  Loss: (0.2174) |  Loss2: (0.0000) | Acc: (92.00%) (28564/30848)
Epoch: 96 | Batch_idx: 250 |  Loss: (0.2168) |  Loss2: (0.0000) | Acc: (92.00%) (29758/32128)
Epoch: 96 | Batch_idx: 260 |  Loss: (0.2156) |  Loss2: (0.0000) | Acc: (92.00%) (30956/33408)
Epoch: 96 | Batch_idx: 270 |  Loss: (0.2155) |  Loss2: (0.0000) | Acc: (92.00%) (32138/34688)
Epoch: 96 | Batch_idx: 280 |  Loss: (0.2152) |  Loss2: (0.0000) | Acc: (92.00%) (33325/35968)
Epoch: 96 | Batch_idx: 290 |  Loss: (0.2146) |  Loss2: (0.0000) | Acc: (92.00%) (34512/37248)
Epoch: 96 | Batch_idx: 300 |  Loss: (0.2144) |  Loss2: (0.0000) | Acc: (92.00%) (35698/38528)
Epoch: 96 | Batch_idx: 310 |  Loss: (0.2139) |  Loss2: (0.0000) | Acc: (92.00%) (36891/39808)
Epoch: 96 | Batch_idx: 320 |  Loss: (0.2145) |  Loss2: (0.0000) | Acc: (92.00%) (38081/41088)
Epoch: 96 | Batch_idx: 330 |  Loss: (0.2152) |  Loss2: (0.0000) | Acc: (92.00%) (39253/42368)
Epoch: 96 | Batch_idx: 340 |  Loss: (0.2154) |  Loss2: (0.0000) | Acc: (92.00%) (40433/43648)
Epoch: 96 | Batch_idx: 350 |  Loss: (0.2147) |  Loss2: (0.0000) | Acc: (92.00%) (41625/44928)
Epoch: 96 | Batch_idx: 360 |  Loss: (0.2148) |  Loss2: (0.0000) | Acc: (92.00%) (42812/46208)
Epoch: 96 | Batch_idx: 370 |  Loss: (0.2151) |  Loss2: (0.0000) | Acc: (92.00%) (43986/47488)
Epoch: 96 | Batch_idx: 380 |  Loss: (0.2148) |  Loss2: (0.0000) | Acc: (92.00%) (45177/48768)
Epoch: 96 | Batch_idx: 390 |  Loss: (0.2147) |  Loss2: (0.0000) | Acc: (92.00%) (46316/50000)
# TEST : Loss: (0.4174) | Acc: (86.00%) (8644/10000)
percent tensor([0.5656, 0.5833, 0.5619, 0.5527, 0.5684, 0.5509, 0.5846, 0.5749, 0.5805,
        0.5774, 0.5785, 0.5705, 0.5727, 0.5822, 0.5690, 0.5634],
       device='cuda:0') torch.Size([16])
percent tensor([0.5145, 0.5269, 0.4765, 0.5030, 0.5009, 0.4946, 0.5227, 0.5241, 0.5090,
        0.5006, 0.4984, 0.4798, 0.5180, 0.5272, 0.5209, 0.5100],
       device='cuda:0') torch.Size([16])
percent tensor([0.5109, 0.4868, 0.4500, 0.5179, 0.4431, 0.5305, 0.4858, 0.4926, 0.4986,
        0.4668, 0.5008, 0.4486, 0.4680, 0.5470, 0.5103, 0.5176],
       device='cuda:0') torch.Size([16])
percent tensor([0.6805, 0.6999, 0.6474, 0.6268, 0.6376, 0.6809, 0.6806, 0.6273, 0.6666,
        0.6977, 0.6890, 0.6772, 0.7235, 0.6641, 0.6839, 0.6931],
       device='cuda:0') torch.Size([16])
percent tensor([0.6683, 0.6099, 0.6400, 0.6367, 0.6466, 0.7033, 0.6541, 0.6172, 0.7194,
        0.6649, 0.7144, 0.6698, 0.6402, 0.7167, 0.6264, 0.6748],
       device='cuda:0') torch.Size([16])
percent tensor([0.7423, 0.7265, 0.7592, 0.7688, 0.7898, 0.7857, 0.7622, 0.7677, 0.7609,
        0.7389, 0.7507, 0.7397, 0.7299, 0.7740, 0.7342, 0.7611],
       device='cuda:0') torch.Size([16])
percent tensor([0.3969, 0.4607, 0.5393, 0.4808, 0.6017, 0.6750, 0.4912, 0.4028, 0.5379,
        0.4287, 0.4703, 0.4444, 0.4185, 0.4752, 0.3320, 0.4607],
       device='cuda:0') torch.Size([16])
percent tensor([0.9995, 0.9993, 0.9984, 0.9991, 0.9987, 0.9975, 0.9989, 0.9996, 0.9993,
        0.9995, 0.9996, 0.9993, 0.9995, 0.9988, 0.9988, 0.9994],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 97 | Batch_idx: 0 |  Loss: (0.1705) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 97 | Batch_idx: 10 |  Loss: (0.2090) |  Loss2: (0.0000) | Acc: (93.00%) (1315/1408)
Epoch: 97 | Batch_idx: 20 |  Loss: (0.2126) |  Loss2: (0.0000) | Acc: (93.00%) (2503/2688)
Epoch: 97 | Batch_idx: 30 |  Loss: (0.2187) |  Loss2: (0.0000) | Acc: (92.00%) (3679/3968)
Epoch: 97 | Batch_idx: 40 |  Loss: (0.2323) |  Loss2: (0.0000) | Acc: (92.00%) (4831/5248)
Epoch: 97 | Batch_idx: 50 |  Loss: (0.2342) |  Loss2: (0.0000) | Acc: (92.00%) (6012/6528)
Epoch: 97 | Batch_idx: 60 |  Loss: (0.2359) |  Loss2: (0.0000) | Acc: (92.00%) (7188/7808)
Epoch: 97 | Batch_idx: 70 |  Loss: (0.2366) |  Loss2: (0.0000) | Acc: (91.00%) (8347/9088)
Epoch: 97 | Batch_idx: 80 |  Loss: (0.2361) |  Loss2: (0.0000) | Acc: (91.00%) (9528/10368)
Epoch: 97 | Batch_idx: 90 |  Loss: (0.2401) |  Loss2: (0.0000) | Acc: (91.00%) (10679/11648)
Epoch: 97 | Batch_idx: 100 |  Loss: (0.2407) |  Loss2: (0.0000) | Acc: (91.00%) (11847/12928)
Epoch: 97 | Batch_idx: 110 |  Loss: (0.2406) |  Loss2: (0.0000) | Acc: (91.00%) (13018/14208)
Epoch: 97 | Batch_idx: 120 |  Loss: (0.2403) |  Loss2: (0.0000) | Acc: (91.00%) (14195/15488)
Epoch: 97 | Batch_idx: 130 |  Loss: (0.2388) |  Loss2: (0.0000) | Acc: (91.00%) (15378/16768)
Epoch: 97 | Batch_idx: 140 |  Loss: (0.2386) |  Loss2: (0.0000) | Acc: (91.00%) (16559/18048)
Epoch: 97 | Batch_idx: 150 |  Loss: (0.2399) |  Loss2: (0.0000) | Acc: (91.00%) (17726/19328)
Epoch: 97 | Batch_idx: 160 |  Loss: (0.2423) |  Loss2: (0.0000) | Acc: (91.00%) (18881/20608)
Epoch: 97 | Batch_idx: 170 |  Loss: (0.2431) |  Loss2: (0.0000) | Acc: (91.00%) (20043/21888)
Epoch: 97 | Batch_idx: 180 |  Loss: (0.2411) |  Loss2: (0.0000) | Acc: (91.00%) (21237/23168)
Epoch: 97 | Batch_idx: 190 |  Loss: (0.2398) |  Loss2: (0.0000) | Acc: (91.00%) (22424/24448)
Epoch: 97 | Batch_idx: 200 |  Loss: (0.2387) |  Loss2: (0.0000) | Acc: (91.00%) (23613/25728)
Epoch: 97 | Batch_idx: 210 |  Loss: (0.2390) |  Loss2: (0.0000) | Acc: (91.00%) (24778/27008)
Epoch: 97 | Batch_idx: 220 |  Loss: (0.2391) |  Loss2: (0.0000) | Acc: (91.00%) (25957/28288)
Epoch: 97 | Batch_idx: 230 |  Loss: (0.2398) |  Loss2: (0.0000) | Acc: (91.00%) (27128/29568)
Epoch: 97 | Batch_idx: 240 |  Loss: (0.2397) |  Loss2: (0.0000) | Acc: (91.00%) (28302/30848)
Epoch: 97 | Batch_idx: 250 |  Loss: (0.2398) |  Loss2: (0.0000) | Acc: (91.00%) (29470/32128)
Epoch: 97 | Batch_idx: 260 |  Loss: (0.2398) |  Loss2: (0.0000) | Acc: (91.00%) (30640/33408)
Epoch: 97 | Batch_idx: 270 |  Loss: (0.2391) |  Loss2: (0.0000) | Acc: (91.00%) (31818/34688)
Epoch: 97 | Batch_idx: 280 |  Loss: (0.2381) |  Loss2: (0.0000) | Acc: (91.00%) (33010/35968)
Epoch: 97 | Batch_idx: 290 |  Loss: (0.2372) |  Loss2: (0.0000) | Acc: (91.00%) (34197/37248)
Epoch: 97 | Batch_idx: 300 |  Loss: (0.2365) |  Loss2: (0.0000) | Acc: (91.00%) (35377/38528)
Epoch: 97 | Batch_idx: 310 |  Loss: (0.2371) |  Loss2: (0.0000) | Acc: (91.00%) (36545/39808)
Epoch: 97 | Batch_idx: 320 |  Loss: (0.2360) |  Loss2: (0.0000) | Acc: (91.00%) (37728/41088)
Epoch: 97 | Batch_idx: 330 |  Loss: (0.2352) |  Loss2: (0.0000) | Acc: (91.00%) (38917/42368)
Epoch: 97 | Batch_idx: 340 |  Loss: (0.2350) |  Loss2: (0.0000) | Acc: (91.00%) (40100/43648)
Epoch: 97 | Batch_idx: 350 |  Loss: (0.2345) |  Loss2: (0.0000) | Acc: (91.00%) (41283/44928)
Epoch: 97 | Batch_idx: 360 |  Loss: (0.2342) |  Loss2: (0.0000) | Acc: (91.00%) (42466/46208)
Epoch: 97 | Batch_idx: 370 |  Loss: (0.2336) |  Loss2: (0.0000) | Acc: (91.00%) (43647/47488)
Epoch: 97 | Batch_idx: 380 |  Loss: (0.2335) |  Loss2: (0.0000) | Acc: (91.00%) (44814/48768)
Epoch: 97 | Batch_idx: 390 |  Loss: (0.2331) |  Loss2: (0.0000) | Acc: (91.00%) (45957/50000)
# TEST : Loss: (0.3993) | Acc: (87.00%) (8705/10000)
percent tensor([0.5549, 0.5681, 0.5527, 0.5432, 0.5570, 0.5420, 0.5699, 0.5625, 0.5678,
        0.5638, 0.5659, 0.5585, 0.5608, 0.5676, 0.5568, 0.5524],
       device='cuda:0') torch.Size([16])
percent tensor([0.5142, 0.5269, 0.4635, 0.4968, 0.4914, 0.4926, 0.5187, 0.5175, 0.5046,
        0.4973, 0.4980, 0.4694, 0.5171, 0.5308, 0.5212, 0.5092],
       device='cuda:0') torch.Size([16])
percent tensor([0.5064, 0.4694, 0.4421, 0.5215, 0.4370, 0.5310, 0.4685, 0.4895, 0.4960,
        0.4507, 0.4890, 0.4345, 0.4491, 0.5498, 0.5066, 0.5124],
       device='cuda:0') torch.Size([16])
percent tensor([0.6899, 0.7077, 0.6511, 0.6307, 0.6445, 0.6928, 0.6889, 0.6262, 0.6695,
        0.7050, 0.6975, 0.6820, 0.7335, 0.6658, 0.6933, 0.7050],
       device='cuda:0') torch.Size([16])
percent tensor([0.6653, 0.6062, 0.6340, 0.6247, 0.6424, 0.6929, 0.6511, 0.6122, 0.7061,
        0.6569, 0.7018, 0.6586, 0.6327, 0.6847, 0.6182, 0.6710],
       device='cuda:0') torch.Size([16])
percent tensor([0.7302, 0.7121, 0.7416, 0.7512, 0.7747, 0.7779, 0.7475, 0.7516, 0.7493,
        0.7219, 0.7340, 0.7213, 0.7108, 0.7497, 0.7172, 0.7515],
       device='cuda:0') torch.Size([16])
percent tensor([0.4376, 0.4673, 0.5673, 0.5174, 0.6284, 0.6762, 0.5243, 0.4809, 0.5504,
        0.4406, 0.4748, 0.4617, 0.4112, 0.4776, 0.3538, 0.5052],
       device='cuda:0') torch.Size([16])
percent tensor([0.9995, 0.9991, 0.9986, 0.9992, 0.9986, 0.9969, 0.9990, 0.9997, 0.9992,
        0.9995, 0.9995, 0.9993, 0.9995, 0.9988, 0.9989, 0.9996],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 98 | Batch_idx: 0 |  Loss: (0.1437) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 98 | Batch_idx: 10 |  Loss: (0.2099) |  Loss2: (0.0000) | Acc: (92.00%) (1307/1408)
Epoch: 98 | Batch_idx: 20 |  Loss: (0.1996) |  Loss2: (0.0000) | Acc: (93.00%) (2507/2688)
Epoch: 98 | Batch_idx: 30 |  Loss: (0.1994) |  Loss2: (0.0000) | Acc: (93.00%) (3696/3968)
Epoch: 98 | Batch_idx: 40 |  Loss: (0.2026) |  Loss2: (0.0000) | Acc: (92.00%) (4880/5248)
Epoch: 98 | Batch_idx: 50 |  Loss: (0.2026) |  Loss2: (0.0000) | Acc: (93.00%) (6087/6528)
Epoch: 98 | Batch_idx: 60 |  Loss: (0.2062) |  Loss2: (0.0000) | Acc: (93.00%) (7262/7808)
Epoch: 98 | Batch_idx: 70 |  Loss: (0.2057) |  Loss2: (0.0000) | Acc: (93.00%) (8456/9088)
Epoch: 98 | Batch_idx: 80 |  Loss: (0.2059) |  Loss2: (0.0000) | Acc: (92.00%) (9642/10368)
Epoch: 98 | Batch_idx: 90 |  Loss: (0.2111) |  Loss2: (0.0000) | Acc: (92.00%) (10802/11648)
Epoch: 98 | Batch_idx: 100 |  Loss: (0.2125) |  Loss2: (0.0000) | Acc: (92.00%) (11982/12928)
Epoch: 98 | Batch_idx: 110 |  Loss: (0.2114) |  Loss2: (0.0000) | Acc: (92.00%) (13165/14208)
Epoch: 98 | Batch_idx: 120 |  Loss: (0.2131) |  Loss2: (0.0000) | Acc: (92.00%) (14344/15488)
Epoch: 98 | Batch_idx: 130 |  Loss: (0.2125) |  Loss2: (0.0000) | Acc: (92.00%) (15533/16768)
Epoch: 98 | Batch_idx: 140 |  Loss: (0.2142) |  Loss2: (0.0000) | Acc: (92.00%) (16703/18048)
Epoch: 98 | Batch_idx: 150 |  Loss: (0.2119) |  Loss2: (0.0000) | Acc: (92.00%) (17904/19328)
Epoch: 98 | Batch_idx: 160 |  Loss: (0.2107) |  Loss2: (0.0000) | Acc: (92.00%) (19097/20608)
Epoch: 98 | Batch_idx: 170 |  Loss: (0.2117) |  Loss2: (0.0000) | Acc: (92.00%) (20278/21888)
Epoch: 98 | Batch_idx: 180 |  Loss: (0.2120) |  Loss2: (0.0000) | Acc: (92.00%) (21469/23168)
Epoch: 98 | Batch_idx: 190 |  Loss: (0.2118) |  Loss2: (0.0000) | Acc: (92.00%) (22656/24448)
Epoch: 98 | Batch_idx: 200 |  Loss: (0.2121) |  Loss2: (0.0000) | Acc: (92.00%) (23836/25728)
Epoch: 98 | Batch_idx: 210 |  Loss: (0.2123) |  Loss2: (0.0000) | Acc: (92.00%) (25024/27008)
Epoch: 98 | Batch_idx: 220 |  Loss: (0.2120) |  Loss2: (0.0000) | Acc: (92.00%) (26212/28288)
Epoch: 98 | Batch_idx: 230 |  Loss: (0.2128) |  Loss2: (0.0000) | Acc: (92.00%) (27389/29568)
Epoch: 98 | Batch_idx: 240 |  Loss: (0.2121) |  Loss2: (0.0000) | Acc: (92.00%) (28588/30848)
Epoch: 98 | Batch_idx: 250 |  Loss: (0.2126) |  Loss2: (0.0000) | Acc: (92.00%) (29771/32128)
Epoch: 98 | Batch_idx: 260 |  Loss: (0.2134) |  Loss2: (0.0000) | Acc: (92.00%) (30941/33408)
Epoch: 98 | Batch_idx: 270 |  Loss: (0.2142) |  Loss2: (0.0000) | Acc: (92.00%) (32119/34688)
Epoch: 98 | Batch_idx: 280 |  Loss: (0.2143) |  Loss2: (0.0000) | Acc: (92.00%) (33301/35968)
Epoch: 98 | Batch_idx: 290 |  Loss: (0.2136) |  Loss2: (0.0000) | Acc: (92.00%) (34500/37248)
Epoch: 98 | Batch_idx: 300 |  Loss: (0.2137) |  Loss2: (0.0000) | Acc: (92.00%) (35679/38528)
Epoch: 98 | Batch_idx: 310 |  Loss: (0.2139) |  Loss2: (0.0000) | Acc: (92.00%) (36860/39808)
Epoch: 98 | Batch_idx: 320 |  Loss: (0.2138) |  Loss2: (0.0000) | Acc: (92.00%) (38056/41088)
Epoch: 98 | Batch_idx: 330 |  Loss: (0.2133) |  Loss2: (0.0000) | Acc: (92.00%) (39247/42368)
Epoch: 98 | Batch_idx: 340 |  Loss: (0.2135) |  Loss2: (0.0000) | Acc: (92.00%) (40431/43648)
Epoch: 98 | Batch_idx: 350 |  Loss: (0.2131) |  Loss2: (0.0000) | Acc: (92.00%) (41625/44928)
Epoch: 98 | Batch_idx: 360 |  Loss: (0.2133) |  Loss2: (0.0000) | Acc: (92.00%) (42810/46208)
Epoch: 98 | Batch_idx: 370 |  Loss: (0.2134) |  Loss2: (0.0000) | Acc: (92.00%) (44000/47488)
Epoch: 98 | Batch_idx: 380 |  Loss: (0.2135) |  Loss2: (0.0000) | Acc: (92.00%) (45180/48768)
Epoch: 98 | Batch_idx: 390 |  Loss: (0.2136) |  Loss2: (0.0000) | Acc: (92.00%) (46318/50000)
# TEST : Loss: (0.4201) | Acc: (85.00%) (8585/10000)
percent tensor([0.5539, 0.5680, 0.5475, 0.5434, 0.5543, 0.5399, 0.5694, 0.5615, 0.5658,
        0.5637, 0.5644, 0.5567, 0.5601, 0.5681, 0.5564, 0.5515],
       device='cuda:0') torch.Size([16])
percent tensor([0.5149, 0.5224, 0.4694, 0.4980, 0.4953, 0.4962, 0.5196, 0.5204, 0.5072,
        0.4964, 0.4970, 0.4755, 0.5164, 0.5264, 0.5194, 0.5083],
       device='cuda:0') torch.Size([16])
percent tensor([0.5091, 0.4686, 0.4290, 0.5172, 0.4314, 0.5381, 0.4708, 0.4850, 0.4944,
        0.4487, 0.4922, 0.4302, 0.4461, 0.5471, 0.5114, 0.5168],
       device='cuda:0') torch.Size([16])
percent tensor([0.6844, 0.7041, 0.6403, 0.6235, 0.6317, 0.6861, 0.6850, 0.6256, 0.6685,
        0.7036, 0.6945, 0.6754, 0.7275, 0.6681, 0.6908, 0.6971],
       device='cuda:0') torch.Size([16])
percent tensor([0.6601, 0.6042, 0.6244, 0.6286, 0.6334, 0.6869, 0.6458, 0.6103, 0.7034,
        0.6473, 0.7016, 0.6521, 0.6227, 0.7068, 0.6214, 0.6756],
       device='cuda:0') torch.Size([16])
percent tensor([0.7272, 0.7105, 0.7468, 0.7555, 0.7780, 0.7697, 0.7429, 0.7549, 0.7419,
        0.7159, 0.7331, 0.7170, 0.7084, 0.7522, 0.7163, 0.7522],
       device='cuda:0') torch.Size([16])
percent tensor([0.4337, 0.4895, 0.5900, 0.5441, 0.6410, 0.6931, 0.5342, 0.4458, 0.5490,
        0.4476, 0.4895, 0.4752, 0.4287, 0.5160, 0.3427, 0.5101],
       device='cuda:0') torch.Size([16])
percent tensor([0.9993, 0.9994, 0.9990, 0.9992, 0.9994, 0.9968, 0.9990, 0.9995, 0.9995,
        0.9995, 0.9998, 0.9992, 0.9995, 0.9990, 0.9989, 0.9992],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 99 | Batch_idx: 0 |  Loss: (0.2269) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 99 | Batch_idx: 10 |  Loss: (0.2048) |  Loss2: (0.0000) | Acc: (93.00%) (1310/1408)
Epoch: 99 | Batch_idx: 20 |  Loss: (0.2116) |  Loss2: (0.0000) | Acc: (92.00%) (2483/2688)
Epoch: 99 | Batch_idx: 30 |  Loss: (0.2154) |  Loss2: (0.0000) | Acc: (92.00%) (3656/3968)
Epoch: 99 | Batch_idx: 40 |  Loss: (0.2245) |  Loss2: (0.0000) | Acc: (91.00%) (4818/5248)
Epoch: 99 | Batch_idx: 50 |  Loss: (0.2279) |  Loss2: (0.0000) | Acc: (91.00%) (5995/6528)
Epoch: 99 | Batch_idx: 60 |  Loss: (0.2288) |  Loss2: (0.0000) | Acc: (91.00%) (7165/7808)
Epoch: 99 | Batch_idx: 70 |  Loss: (0.2286) |  Loss2: (0.0000) | Acc: (91.00%) (8339/9088)
Epoch: 99 | Batch_idx: 80 |  Loss: (0.2322) |  Loss2: (0.0000) | Acc: (91.00%) (9498/10368)
Epoch: 99 | Batch_idx: 90 |  Loss: (0.2319) |  Loss2: (0.0000) | Acc: (91.00%) (10668/11648)
Epoch: 99 | Batch_idx: 100 |  Loss: (0.2328) |  Loss2: (0.0000) | Acc: (91.00%) (11842/12928)
Epoch: 99 | Batch_idx: 110 |  Loss: (0.2330) |  Loss2: (0.0000) | Acc: (91.00%) (13015/14208)
Epoch: 99 | Batch_idx: 120 |  Loss: (0.2328) |  Loss2: (0.0000) | Acc: (91.00%) (14199/15488)
Epoch: 99 | Batch_idx: 130 |  Loss: (0.2325) |  Loss2: (0.0000) | Acc: (91.00%) (15368/16768)
Epoch: 99 | Batch_idx: 140 |  Loss: (0.2339) |  Loss2: (0.0000) | Acc: (91.00%) (16546/18048)
Epoch: 99 | Batch_idx: 150 |  Loss: (0.2356) |  Loss2: (0.0000) | Acc: (91.00%) (17707/19328)
Epoch: 99 | Batch_idx: 160 |  Loss: (0.2356) |  Loss2: (0.0000) | Acc: (91.00%) (18892/20608)
Epoch: 99 | Batch_idx: 170 |  Loss: (0.2343) |  Loss2: (0.0000) | Acc: (91.00%) (20078/21888)
Epoch: 99 | Batch_idx: 180 |  Loss: (0.2339) |  Loss2: (0.0000) | Acc: (91.00%) (21257/23168)
Epoch: 99 | Batch_idx: 190 |  Loss: (0.2338) |  Loss2: (0.0000) | Acc: (91.00%) (22437/24448)
Epoch: 99 | Batch_idx: 200 |  Loss: (0.2350) |  Loss2: (0.0000) | Acc: (91.00%) (23598/25728)
Epoch: 99 | Batch_idx: 210 |  Loss: (0.2343) |  Loss2: (0.0000) | Acc: (91.00%) (24776/27008)
Epoch: 99 | Batch_idx: 220 |  Loss: (0.2338) |  Loss2: (0.0000) | Acc: (91.00%) (25951/28288)
Epoch: 99 | Batch_idx: 230 |  Loss: (0.2340) |  Loss2: (0.0000) | Acc: (91.00%) (27122/29568)
Epoch: 99 | Batch_idx: 240 |  Loss: (0.2349) |  Loss2: (0.0000) | Acc: (91.00%) (28287/30848)
Epoch: 99 | Batch_idx: 250 |  Loss: (0.2347) |  Loss2: (0.0000) | Acc: (91.00%) (29465/32128)
Epoch: 99 | Batch_idx: 260 |  Loss: (0.2350) |  Loss2: (0.0000) | Acc: (91.00%) (30636/33408)
Epoch: 99 | Batch_idx: 270 |  Loss: (0.2360) |  Loss2: (0.0000) | Acc: (91.00%) (31788/34688)
Epoch: 99 | Batch_idx: 280 |  Loss: (0.2356) |  Loss2: (0.0000) | Acc: (91.00%) (32968/35968)
Epoch: 99 | Batch_idx: 290 |  Loss: (0.2359) |  Loss2: (0.0000) | Acc: (91.00%) (34138/37248)
Epoch: 99 | Batch_idx: 300 |  Loss: (0.2357) |  Loss2: (0.0000) | Acc: (91.00%) (35299/38528)
Epoch: 99 | Batch_idx: 310 |  Loss: (0.2356) |  Loss2: (0.0000) | Acc: (91.00%) (36473/39808)
Epoch: 99 | Batch_idx: 320 |  Loss: (0.2353) |  Loss2: (0.0000) | Acc: (91.00%) (37647/41088)
Epoch: 99 | Batch_idx: 330 |  Loss: (0.2356) |  Loss2: (0.0000) | Acc: (91.00%) (38817/42368)
Epoch: 99 | Batch_idx: 340 |  Loss: (0.2355) |  Loss2: (0.0000) | Acc: (91.00%) (39995/43648)
Epoch: 99 | Batch_idx: 350 |  Loss: (0.2353) |  Loss2: (0.0000) | Acc: (91.00%) (41171/44928)
Epoch: 99 | Batch_idx: 360 |  Loss: (0.2346) |  Loss2: (0.0000) | Acc: (91.00%) (42353/46208)
Epoch: 99 | Batch_idx: 370 |  Loss: (0.2342) |  Loss2: (0.0000) | Acc: (91.00%) (43527/47488)
Epoch: 99 | Batch_idx: 380 |  Loss: (0.2339) |  Loss2: (0.0000) | Acc: (91.00%) (44704/48768)
Epoch: 99 | Batch_idx: 390 |  Loss: (0.2332) |  Loss2: (0.0000) | Acc: (91.00%) (45853/50000)
# TEST : Loss: (0.4054) | Acc: (86.00%) (8618/10000)
percent tensor([0.5589, 0.5749, 0.5507, 0.5482, 0.5584, 0.5426, 0.5756, 0.5681, 0.5714,
        0.5704, 0.5701, 0.5607, 0.5658, 0.5739, 0.5623, 0.5572],
       device='cuda:0') torch.Size([16])
percent tensor([0.5248, 0.5313, 0.4877, 0.5095, 0.5116, 0.5054, 0.5321, 0.5338, 0.5175,
        0.5083, 0.5049, 0.4929, 0.5262, 0.5295, 0.5286, 0.5175],
       device='cuda:0') torch.Size([16])
percent tensor([0.5034, 0.4671, 0.4275, 0.5119, 0.4274, 0.5309, 0.4711, 0.4868, 0.4850,
        0.4467, 0.4843, 0.4243, 0.4463, 0.5427, 0.5053, 0.5111],
       device='cuda:0') torch.Size([16])
percent tensor([0.6790, 0.6958, 0.6296, 0.6150, 0.6157, 0.6840, 0.6727, 0.6072, 0.6588,
        0.6959, 0.6889, 0.6604, 0.7228, 0.6599, 0.6785, 0.6944],
       device='cuda:0') torch.Size([16])
percent tensor([0.6676, 0.6089, 0.6347, 0.6320, 0.6400, 0.7075, 0.6499, 0.6133, 0.7062,
        0.6486, 0.7096, 0.6633, 0.6230, 0.7052, 0.6335, 0.6857],
       device='cuda:0') torch.Size([16])
percent tensor([0.7320, 0.7144, 0.7460, 0.7543, 0.7778, 0.7780, 0.7451, 0.7534, 0.7475,
        0.7210, 0.7413, 0.7159, 0.7165, 0.7552, 0.7181, 0.7560],
       device='cuda:0') torch.Size([16])
percent tensor([0.4640, 0.5186, 0.5865, 0.5531, 0.6379, 0.7059, 0.5427, 0.4346, 0.5867,
        0.4793, 0.5245, 0.4669, 0.4645, 0.5374, 0.3513, 0.5074],
       device='cuda:0') torch.Size([16])
percent tensor([0.9995, 0.9993, 0.9991, 0.9992, 0.9995, 0.9972, 0.9990, 0.9995, 0.9995,
        0.9995, 0.9998, 0.9992, 0.9994, 0.9993, 0.9987, 0.9993],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 100 | Batch_idx: 0 |  Loss: (0.2216) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 100 | Batch_idx: 10 |  Loss: (0.2145) |  Loss2: (0.0000) | Acc: (91.00%) (1295/1408)
Epoch: 100 | Batch_idx: 20 |  Loss: (0.2099) |  Loss2: (0.0000) | Acc: (92.00%) (2495/2688)
Epoch: 100 | Batch_idx: 30 |  Loss: (0.2071) |  Loss2: (0.0000) | Acc: (92.00%) (3686/3968)
Epoch: 100 | Batch_idx: 40 |  Loss: (0.2066) |  Loss2: (0.0000) | Acc: (92.00%) (4879/5248)
Epoch: 100 | Batch_idx: 50 |  Loss: (0.2017) |  Loss2: (0.0000) | Acc: (93.00%) (6082/6528)
Epoch: 100 | Batch_idx: 60 |  Loss: (0.2004) |  Loss2: (0.0000) | Acc: (93.00%) (7277/7808)
Epoch: 100 | Batch_idx: 70 |  Loss: (0.1994) |  Loss2: (0.0000) | Acc: (93.00%) (8483/9088)
Epoch: 100 | Batch_idx: 80 |  Loss: (0.2006) |  Loss2: (0.0000) | Acc: (93.00%) (9671/10368)
Epoch: 100 | Batch_idx: 90 |  Loss: (0.2014) |  Loss2: (0.0000) | Acc: (93.00%) (10860/11648)
Epoch: 100 | Batch_idx: 100 |  Loss: (0.2018) |  Loss2: (0.0000) | Acc: (93.00%) (12058/12928)
Epoch: 100 | Batch_idx: 110 |  Loss: (0.2015) |  Loss2: (0.0000) | Acc: (93.00%) (13253/14208)
Epoch: 100 | Batch_idx: 120 |  Loss: (0.2050) |  Loss2: (0.0000) | Acc: (93.00%) (14422/15488)
Epoch: 100 | Batch_idx: 130 |  Loss: (0.2028) |  Loss2: (0.0000) | Acc: (93.00%) (15625/16768)
Epoch: 100 | Batch_idx: 140 |  Loss: (0.2033) |  Loss2: (0.0000) | Acc: (93.00%) (16810/18048)
Epoch: 100 | Batch_idx: 150 |  Loss: (0.2042) |  Loss2: (0.0000) | Acc: (93.00%) (17988/19328)
Epoch: 100 | Batch_idx: 160 |  Loss: (0.2039) |  Loss2: (0.0000) | Acc: (93.00%) (19183/20608)
Epoch: 100 | Batch_idx: 170 |  Loss: (0.2048) |  Loss2: (0.0000) | Acc: (93.00%) (20361/21888)
Epoch: 100 | Batch_idx: 180 |  Loss: (0.2048) |  Loss2: (0.0000) | Acc: (93.00%) (21557/23168)
Epoch: 100 | Batch_idx: 190 |  Loss: (0.2050) |  Loss2: (0.0000) | Acc: (93.00%) (22753/24448)
Epoch: 100 | Batch_idx: 200 |  Loss: (0.2057) |  Loss2: (0.0000) | Acc: (93.00%) (23940/25728)
Epoch: 100 | Batch_idx: 210 |  Loss: (0.2077) |  Loss2: (0.0000) | Acc: (92.00%) (25101/27008)
Epoch: 100 | Batch_idx: 220 |  Loss: (0.2083) |  Loss2: (0.0000) | Acc: (92.00%) (26292/28288)
Epoch: 100 | Batch_idx: 230 |  Loss: (0.2081) |  Loss2: (0.0000) | Acc: (92.00%) (27476/29568)
Epoch: 100 | Batch_idx: 240 |  Loss: (0.2085) |  Loss2: (0.0000) | Acc: (92.00%) (28660/30848)
Epoch: 100 | Batch_idx: 250 |  Loss: (0.2083) |  Loss2: (0.0000) | Acc: (92.00%) (29859/32128)
Epoch: 100 | Batch_idx: 260 |  Loss: (0.2079) |  Loss2: (0.0000) | Acc: (92.00%) (31056/33408)
Epoch: 100 | Batch_idx: 270 |  Loss: (0.2075) |  Loss2: (0.0000) | Acc: (92.00%) (32254/34688)
Epoch: 100 | Batch_idx: 280 |  Loss: (0.2067) |  Loss2: (0.0000) | Acc: (93.00%) (33451/35968)
Epoch: 100 | Batch_idx: 290 |  Loss: (0.2068) |  Loss2: (0.0000) | Acc: (92.00%) (34631/37248)
Epoch: 100 | Batch_idx: 300 |  Loss: (0.2066) |  Loss2: (0.0000) | Acc: (92.00%) (35818/38528)
Epoch: 100 | Batch_idx: 310 |  Loss: (0.2070) |  Loss2: (0.0000) | Acc: (92.00%) (37012/39808)
Epoch: 100 | Batch_idx: 320 |  Loss: (0.2074) |  Loss2: (0.0000) | Acc: (92.00%) (38202/41088)
Epoch: 100 | Batch_idx: 330 |  Loss: (0.2072) |  Loss2: (0.0000) | Acc: (93.00%) (39403/42368)
Epoch: 100 | Batch_idx: 340 |  Loss: (0.2074) |  Loss2: (0.0000) | Acc: (92.00%) (40588/43648)
Epoch: 100 | Batch_idx: 350 |  Loss: (0.2069) |  Loss2: (0.0000) | Acc: (93.00%) (41785/44928)
Epoch: 100 | Batch_idx: 360 |  Loss: (0.2073) |  Loss2: (0.0000) | Acc: (92.00%) (42960/46208)
Epoch: 100 | Batch_idx: 370 |  Loss: (0.2076) |  Loss2: (0.0000) | Acc: (92.00%) (44144/47488)
Epoch: 100 | Batch_idx: 380 |  Loss: (0.2071) |  Loss2: (0.0000) | Acc: (92.00%) (45343/48768)
Epoch: 100 | Batch_idx: 390 |  Loss: (0.2074) |  Loss2: (0.0000) | Acc: (92.00%) (46486/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_100.pth.tar'
# TEST : Loss: (0.4386) | Acc: (86.00%) (8618/10000)
percent tensor([0.5589, 0.5748, 0.5575, 0.5493, 0.5621, 0.5442, 0.5762, 0.5695, 0.5720,
        0.5706, 0.5705, 0.5649, 0.5657, 0.5736, 0.5624, 0.5573],
       device='cuda:0') torch.Size([16])
percent tensor([0.5216, 0.5330, 0.4805, 0.5067, 0.5059, 0.5032, 0.5284, 0.5313, 0.5158,
        0.5071, 0.5054, 0.4849, 0.5267, 0.5304, 0.5295, 0.5175],
       device='cuda:0') torch.Size([16])
percent tensor([0.5026, 0.4689, 0.4414, 0.5047, 0.4367, 0.5259, 0.4721, 0.4856, 0.4899,
        0.4544, 0.4858, 0.4380, 0.4499, 0.5426, 0.5014, 0.5103],
       device='cuda:0') torch.Size([16])
percent tensor([0.6759, 0.6918, 0.6270, 0.6052, 0.6186, 0.6925, 0.6674, 0.6016, 0.6554,
        0.6833, 0.6814, 0.6580, 0.7187, 0.6581, 0.6772, 0.6878],
       device='cuda:0') torch.Size([16])
percent tensor([0.6593, 0.6175, 0.6373, 0.6440, 0.6449, 0.7008, 0.6522, 0.6239, 0.7055,
        0.6593, 0.7046, 0.6497, 0.6239, 0.7145, 0.6297, 0.6844],
       device='cuda:0') torch.Size([16])
percent tensor([0.7333, 0.7246, 0.7426, 0.7508, 0.7790, 0.7815, 0.7515, 0.7480, 0.7417,
        0.7315, 0.7403, 0.7171, 0.7202, 0.7639, 0.7187, 0.7603],
       device='cuda:0') torch.Size([16])
percent tensor([0.4812, 0.5257, 0.5937, 0.5393, 0.6535, 0.7171, 0.5387, 0.4657, 0.5650,
        0.4961, 0.5152, 0.4650, 0.4700, 0.5647, 0.3608, 0.5407],
       device='cuda:0') torch.Size([16])
percent tensor([0.9995, 0.9993, 0.9989, 0.9993, 0.9994, 0.9974, 0.9994, 0.9997, 0.9993,
        0.9992, 0.9994, 0.9994, 0.9992, 0.9983, 0.9987, 0.9997],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(177.7347, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(812.2816, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(800.2430, device='cuda:0')
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1513.6113, device='cuda:0')
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(494.8691, device='cuda:0')
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2234.9805, device='cuda:0')
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4297.4297, device='cuda:0')
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1399.0842, device='cuda:0')
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6177., device='cuda:0')
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11859.2207, device='cuda:0')
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3935.1482, device='cuda:0')
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16622.0508, device='cuda:0')
Epoch: 101 | Batch_idx: 0 |  Loss: (0.3340) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 101 | Batch_idx: 10 |  Loss: (0.2318) |  Loss2: (0.0000) | Acc: (91.00%) (1289/1408)
Epoch: 101 | Batch_idx: 20 |  Loss: (0.2313) |  Loss2: (0.0000) | Acc: (91.00%) (2464/2688)
Epoch: 101 | Batch_idx: 30 |  Loss: (0.2267) |  Loss2: (0.0000) | Acc: (91.00%) (3647/3968)
Epoch: 101 | Batch_idx: 40 |  Loss: (0.2391) |  Loss2: (0.0000) | Acc: (91.00%) (4805/5248)
Epoch: 101 | Batch_idx: 50 |  Loss: (0.2465) |  Loss2: (0.0000) | Acc: (91.00%) (5960/6528)
Epoch: 101 | Batch_idx: 60 |  Loss: (0.2472) |  Loss2: (0.0000) | Acc: (91.00%) (7123/7808)
Epoch: 101 | Batch_idx: 70 |  Loss: (0.2483) |  Loss2: (0.0000) | Acc: (91.00%) (8283/9088)
Epoch: 101 | Batch_idx: 80 |  Loss: (0.2483) |  Loss2: (0.0000) | Acc: (91.00%) (9455/10368)
Epoch: 101 | Batch_idx: 90 |  Loss: (0.2509) |  Loss2: (0.0000) | Acc: (91.00%) (10609/11648)
Epoch: 101 | Batch_idx: 100 |  Loss: (0.2511) |  Loss2: (0.0000) | Acc: (91.00%) (11773/12928)
Epoch: 101 | Batch_idx: 110 |  Loss: (0.2525) |  Loss2: (0.0000) | Acc: (91.00%) (12940/14208)
Epoch: 101 | Batch_idx: 120 |  Loss: (0.2526) |  Loss2: (0.0000) | Acc: (91.00%) (14112/15488)
Epoch: 101 | Batch_idx: 130 |  Loss: (0.2541) |  Loss2: (0.0000) | Acc: (91.00%) (15267/16768)
Epoch: 101 | Batch_idx: 140 |  Loss: (0.2525) |  Loss2: (0.0000) | Acc: (91.00%) (16445/18048)
Epoch: 101 | Batch_idx: 150 |  Loss: (0.2500) |  Loss2: (0.0000) | Acc: (91.00%) (17630/19328)
Epoch: 101 | Batch_idx: 160 |  Loss: (0.2495) |  Loss2: (0.0000) | Acc: (91.00%) (18803/20608)
Epoch: 101 | Batch_idx: 170 |  Loss: (0.2498) |  Loss2: (0.0000) | Acc: (91.00%) (19965/21888)
Epoch: 101 | Batch_idx: 180 |  Loss: (0.2514) |  Loss2: (0.0000) | Acc: (91.00%) (21123/23168)
Epoch: 101 | Batch_idx: 190 |  Loss: (0.2524) |  Loss2: (0.0000) | Acc: (91.00%) (22272/24448)
Epoch: 101 | Batch_idx: 200 |  Loss: (0.2513) |  Loss2: (0.0000) | Acc: (91.00%) (23465/25728)
Epoch: 101 | Batch_idx: 210 |  Loss: (0.2522) |  Loss2: (0.0000) | Acc: (91.00%) (24626/27008)
Epoch: 101 | Batch_idx: 220 |  Loss: (0.2521) |  Loss2: (0.0000) | Acc: (91.00%) (25794/28288)
Epoch: 101 | Batch_idx: 230 |  Loss: (0.2513) |  Loss2: (0.0000) | Acc: (91.00%) (26979/29568)
Epoch: 101 | Batch_idx: 240 |  Loss: (0.2506) |  Loss2: (0.0000) | Acc: (91.00%) (28146/30848)
Epoch: 101 | Batch_idx: 250 |  Loss: (0.2521) |  Loss2: (0.0000) | Acc: (91.00%) (29299/32128)
Epoch: 101 | Batch_idx: 260 |  Loss: (0.2523) |  Loss2: (0.0000) | Acc: (91.00%) (30465/33408)
Epoch: 101 | Batch_idx: 270 |  Loss: (0.2520) |  Loss2: (0.0000) | Acc: (91.00%) (31631/34688)
Epoch: 101 | Batch_idx: 280 |  Loss: (0.2505) |  Loss2: (0.0000) | Acc: (91.00%) (32818/35968)
Epoch: 101 | Batch_idx: 290 |  Loss: (0.2501) |  Loss2: (0.0000) | Acc: (91.00%) (33994/37248)
Epoch: 101 | Batch_idx: 300 |  Loss: (0.2494) |  Loss2: (0.0000) | Acc: (91.00%) (35182/38528)
Epoch: 101 | Batch_idx: 310 |  Loss: (0.2491) |  Loss2: (0.0000) | Acc: (91.00%) (36351/39808)
Epoch: 101 | Batch_idx: 320 |  Loss: (0.2483) |  Loss2: (0.0000) | Acc: (91.00%) (37540/41088)
Epoch: 101 | Batch_idx: 330 |  Loss: (0.2487) |  Loss2: (0.0000) | Acc: (91.00%) (38713/42368)
Epoch: 101 | Batch_idx: 340 |  Loss: (0.2481) |  Loss2: (0.0000) | Acc: (91.00%) (39894/43648)
Epoch: 101 | Batch_idx: 350 |  Loss: (0.2479) |  Loss2: (0.0000) | Acc: (91.00%) (41070/44928)
Epoch: 101 | Batch_idx: 360 |  Loss: (0.2482) |  Loss2: (0.0000) | Acc: (91.00%) (42230/46208)
Epoch: 101 | Batch_idx: 370 |  Loss: (0.2482) |  Loss2: (0.0000) | Acc: (91.00%) (43405/47488)
Epoch: 101 | Batch_idx: 380 |  Loss: (0.2469) |  Loss2: (0.0000) | Acc: (91.00%) (44590/48768)
Epoch: 101 | Batch_idx: 390 |  Loss: (0.2464) |  Loss2: (0.0000) | Acc: (91.00%) (45718/50000)
# TEST : Loss: (0.4070) | Acc: (86.00%) (8660/10000)
percent tensor([0.5615, 0.5774, 0.5561, 0.5511, 0.5626, 0.5459, 0.5786, 0.5716, 0.5745,
        0.5731, 0.5734, 0.5651, 0.5682, 0.5765, 0.5650, 0.5594],
       device='cuda:0') torch.Size([16])
percent tensor([0.5304, 0.5387, 0.4918, 0.5158, 0.5153, 0.5146, 0.5363, 0.5383, 0.5241,
        0.5155, 0.5132, 0.4974, 0.5340, 0.5369, 0.5373, 0.5236],
       device='cuda:0') torch.Size([16])
percent tensor([0.4949, 0.4581, 0.4404, 0.5096, 0.4338, 0.5206, 0.4647, 0.4862, 0.4843,
        0.4484, 0.4776, 0.4328, 0.4424, 0.5338, 0.4973, 0.5044],
       device='cuda:0') torch.Size([16])
percent tensor([0.6771, 0.6874, 0.6237, 0.6026, 0.6192, 0.6965, 0.6651, 0.5993, 0.6554,
        0.6825, 0.6810, 0.6553, 0.7176, 0.6577, 0.6749, 0.6885],
       device='cuda:0') torch.Size([16])
percent tensor([0.6879, 0.6226, 0.6681, 0.6776, 0.6746, 0.7478, 0.6686, 0.6460, 0.7225,
        0.6733, 0.7229, 0.6616, 0.6226, 0.7321, 0.6469, 0.7175],
       device='cuda:0') torch.Size([16])
percent tensor([0.7461, 0.7363, 0.7539, 0.7674, 0.7925, 0.7968, 0.7638, 0.7641, 0.7483,
        0.7413, 0.7505, 0.7279, 0.7297, 0.7747, 0.7351, 0.7724],
       device='cuda:0') torch.Size([16])
percent tensor([0.5096, 0.5556, 0.6237, 0.5648, 0.6877, 0.7303, 0.5704, 0.5124, 0.5489,
        0.4993, 0.5182, 0.4837, 0.4718, 0.5622, 0.3908, 0.5691],
       device='cuda:0') torch.Size([16])
percent tensor([0.9994, 0.9991, 0.9988, 0.9993, 0.9994, 0.9974, 0.9991, 0.9996, 0.9993,
        0.9990, 0.9994, 0.9996, 0.9991, 0.9985, 0.9986, 0.9995],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 102 | Batch_idx: 0 |  Loss: (0.1731) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 102 | Batch_idx: 10 |  Loss: (0.2070) |  Loss2: (0.0000) | Acc: (92.00%) (1303/1408)
Epoch: 102 | Batch_idx: 20 |  Loss: (0.1947) |  Loss2: (0.0000) | Acc: (93.00%) (2507/2688)
Epoch: 102 | Batch_idx: 30 |  Loss: (0.2024) |  Loss2: (0.0000) | Acc: (93.00%) (3695/3968)
Epoch: 102 | Batch_idx: 40 |  Loss: (0.1952) |  Loss2: (0.0000) | Acc: (93.00%) (4902/5248)
Epoch: 102 | Batch_idx: 50 |  Loss: (0.1956) |  Loss2: (0.0000) | Acc: (93.00%) (6100/6528)
Epoch: 102 | Batch_idx: 60 |  Loss: (0.1922) |  Loss2: (0.0000) | Acc: (93.00%) (7300/7808)
Epoch: 102 | Batch_idx: 70 |  Loss: (0.1913) |  Loss2: (0.0000) | Acc: (93.00%) (8499/9088)
Epoch: 102 | Batch_idx: 80 |  Loss: (0.1897) |  Loss2: (0.0000) | Acc: (93.00%) (9697/10368)
Epoch: 102 | Batch_idx: 90 |  Loss: (0.1925) |  Loss2: (0.0000) | Acc: (93.00%) (10874/11648)
Epoch: 102 | Batch_idx: 100 |  Loss: (0.1946) |  Loss2: (0.0000) | Acc: (93.00%) (12057/12928)
Epoch: 102 | Batch_idx: 110 |  Loss: (0.1959) |  Loss2: (0.0000) | Acc: (93.00%) (13246/14208)
Epoch: 102 | Batch_idx: 120 |  Loss: (0.1975) |  Loss2: (0.0000) | Acc: (93.00%) (14426/15488)
Epoch: 102 | Batch_idx: 130 |  Loss: (0.1971) |  Loss2: (0.0000) | Acc: (93.00%) (15616/16768)
Epoch: 102 | Batch_idx: 140 |  Loss: (0.1973) |  Loss2: (0.0000) | Acc: (93.00%) (16815/18048)
Epoch: 102 | Batch_idx: 150 |  Loss: (0.1970) |  Loss2: (0.0000) | Acc: (93.00%) (18018/19328)
Epoch: 102 | Batch_idx: 160 |  Loss: (0.1974) |  Loss2: (0.0000) | Acc: (93.00%) (19204/20608)
Epoch: 102 | Batch_idx: 170 |  Loss: (0.1986) |  Loss2: (0.0000) | Acc: (93.00%) (20378/21888)
Epoch: 102 | Batch_idx: 180 |  Loss: (0.1997) |  Loss2: (0.0000) | Acc: (93.00%) (21575/23168)
Epoch: 102 | Batch_idx: 190 |  Loss: (0.1995) |  Loss2: (0.0000) | Acc: (93.00%) (22763/24448)
Epoch: 102 | Batch_idx: 200 |  Loss: (0.2006) |  Loss2: (0.0000) | Acc: (93.00%) (23941/25728)
Epoch: 102 | Batch_idx: 210 |  Loss: (0.2009) |  Loss2: (0.0000) | Acc: (93.00%) (25128/27008)
Epoch: 102 | Batch_idx: 220 |  Loss: (0.2010) |  Loss2: (0.0000) | Acc: (93.00%) (26321/28288)
Epoch: 102 | Batch_idx: 230 |  Loss: (0.2009) |  Loss2: (0.0000) | Acc: (93.00%) (27508/29568)
Epoch: 102 | Batch_idx: 240 |  Loss: (0.2008) |  Loss2: (0.0000) | Acc: (93.00%) (28702/30848)
Epoch: 102 | Batch_idx: 250 |  Loss: (0.2010) |  Loss2: (0.0000) | Acc: (93.00%) (29900/32128)
Epoch: 102 | Batch_idx: 260 |  Loss: (0.1999) |  Loss2: (0.0000) | Acc: (93.00%) (31104/33408)
Epoch: 102 | Batch_idx: 270 |  Loss: (0.2001) |  Loss2: (0.0000) | Acc: (93.00%) (32284/34688)
Epoch: 102 | Batch_idx: 280 |  Loss: (0.2001) |  Loss2: (0.0000) | Acc: (93.00%) (33476/35968)
Epoch: 102 | Batch_idx: 290 |  Loss: (0.2015) |  Loss2: (0.0000) | Acc: (93.00%) (34645/37248)
Epoch: 102 | Batch_idx: 300 |  Loss: (0.2017) |  Loss2: (0.0000) | Acc: (92.00%) (35827/38528)
Epoch: 102 | Batch_idx: 310 |  Loss: (0.2024) |  Loss2: (0.0000) | Acc: (92.00%) (37007/39808)
Epoch: 102 | Batch_idx: 320 |  Loss: (0.2024) |  Loss2: (0.0000) | Acc: (92.00%) (38187/41088)
Epoch: 102 | Batch_idx: 330 |  Loss: (0.2028) |  Loss2: (0.0000) | Acc: (92.00%) (39374/42368)
Epoch: 102 | Batch_idx: 340 |  Loss: (0.2031) |  Loss2: (0.0000) | Acc: (92.00%) (40562/43648)
Epoch: 102 | Batch_idx: 350 |  Loss: (0.2030) |  Loss2: (0.0000) | Acc: (92.00%) (41754/44928)
Epoch: 102 | Batch_idx: 360 |  Loss: (0.2030) |  Loss2: (0.0000) | Acc: (92.00%) (42940/46208)
Epoch: 102 | Batch_idx: 370 |  Loss: (0.2034) |  Loss2: (0.0000) | Acc: (92.00%) (44124/47488)
Epoch: 102 | Batch_idx: 380 |  Loss: (0.2035) |  Loss2: (0.0000) | Acc: (92.00%) (45309/48768)
Epoch: 102 | Batch_idx: 390 |  Loss: (0.2039) |  Loss2: (0.0000) | Acc: (92.00%) (46450/50000)
# TEST : Loss: (0.4688) | Acc: (84.00%) (8488/10000)
percent tensor([0.5618, 0.5771, 0.5607, 0.5514, 0.5661, 0.5484, 0.5787, 0.5712, 0.5750,
        0.5724, 0.5726, 0.5677, 0.5683, 0.5763, 0.5649, 0.5594],
       device='cuda:0') torch.Size([16])
percent tensor([0.5277, 0.5374, 0.4876, 0.5132, 0.5113, 0.5081, 0.5340, 0.5361, 0.5209,
        0.5132, 0.5114, 0.4919, 0.5327, 0.5349, 0.5343, 0.5227],
       device='cuda:0') torch.Size([16])
percent tensor([0.4939, 0.4633, 0.4325, 0.5009, 0.4258, 0.5241, 0.4644, 0.4793, 0.4788,
        0.4447, 0.4778, 0.4242, 0.4418, 0.5396, 0.4987, 0.5049],
       device='cuda:0') torch.Size([16])
percent tensor([0.6743, 0.6960, 0.6276, 0.6062, 0.6163, 0.6805, 0.6680, 0.6034, 0.6562,
        0.6890, 0.6849, 0.6594, 0.7233, 0.6558, 0.6801, 0.6901],
       device='cuda:0') torch.Size([16])
percent tensor([0.6900, 0.6231, 0.6661, 0.6687, 0.6737, 0.7395, 0.6655, 0.6502, 0.7293,
        0.6691, 0.7308, 0.6731, 0.6244, 0.7368, 0.6581, 0.7160],
       device='cuda:0') torch.Size([16])
percent tensor([0.7409, 0.7309, 0.7579, 0.7600, 0.7906, 0.7815, 0.7609, 0.7641, 0.7575,
        0.7400, 0.7556, 0.7321, 0.7237, 0.7727, 0.7336, 0.7693],
       device='cuda:0') torch.Size([16])
percent tensor([0.4691, 0.5329, 0.6207, 0.5404, 0.6767, 0.7232, 0.5383, 0.4915, 0.5773,
        0.5066, 0.5247, 0.4724, 0.4486, 0.5342, 0.3725, 0.5635],
       device='cuda:0') torch.Size([16])
percent tensor([0.9995, 0.9993, 0.9987, 0.9992, 0.9993, 0.9986, 0.9991, 0.9995, 0.9994,
        0.9993, 0.9997, 0.9993, 0.9993, 0.9993, 0.9986, 0.9996],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 103 | Batch_idx: 0 |  Loss: (0.1352) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 103 | Batch_idx: 10 |  Loss: (0.2273) |  Loss2: (0.0000) | Acc: (92.00%) (1301/1408)
Epoch: 103 | Batch_idx: 20 |  Loss: (0.2499) |  Loss2: (0.0000) | Acc: (91.00%) (2451/2688)
Epoch: 103 | Batch_idx: 30 |  Loss: (0.2608) |  Loss2: (0.0000) | Acc: (90.00%) (3604/3968)
Epoch: 103 | Batch_idx: 40 |  Loss: (0.2731) |  Loss2: (0.0000) | Acc: (90.00%) (4736/5248)
Epoch: 103 | Batch_idx: 50 |  Loss: (0.2720) |  Loss2: (0.0000) | Acc: (90.00%) (5889/6528)
Epoch: 103 | Batch_idx: 60 |  Loss: (0.2784) |  Loss2: (0.0000) | Acc: (90.00%) (7034/7808)
Epoch: 103 | Batch_idx: 70 |  Loss: (0.2827) |  Loss2: (0.0000) | Acc: (89.00%) (8175/9088)
Epoch: 103 | Batch_idx: 80 |  Loss: (0.2805) |  Loss2: (0.0000) | Acc: (90.00%) (9339/10368)
Epoch: 103 | Batch_idx: 90 |  Loss: (0.2782) |  Loss2: (0.0000) | Acc: (90.00%) (10515/11648)
Epoch: 103 | Batch_idx: 100 |  Loss: (0.2791) |  Loss2: (0.0000) | Acc: (90.00%) (11662/12928)
Epoch: 103 | Batch_idx: 110 |  Loss: (0.2778) |  Loss2: (0.0000) | Acc: (90.00%) (12813/14208)
Epoch: 103 | Batch_idx: 120 |  Loss: (0.2769) |  Loss2: (0.0000) | Acc: (90.00%) (13971/15488)
Epoch: 103 | Batch_idx: 130 |  Loss: (0.2731) |  Loss2: (0.0000) | Acc: (90.00%) (15144/16768)
Epoch: 103 | Batch_idx: 140 |  Loss: (0.2738) |  Loss2: (0.0000) | Acc: (90.00%) (16298/18048)
Epoch: 103 | Batch_idx: 150 |  Loss: (0.2721) |  Loss2: (0.0000) | Acc: (90.00%) (17477/19328)
Epoch: 103 | Batch_idx: 160 |  Loss: (0.2726) |  Loss2: (0.0000) | Acc: (90.00%) (18626/20608)
Epoch: 103 | Batch_idx: 170 |  Loss: (0.2720) |  Loss2: (0.0000) | Acc: (90.00%) (19785/21888)
Epoch: 103 | Batch_idx: 180 |  Loss: (0.2698) |  Loss2: (0.0000) | Acc: (90.00%) (20957/23168)
Epoch: 103 | Batch_idx: 190 |  Loss: (0.2678) |  Loss2: (0.0000) | Acc: (90.00%) (22128/24448)
Epoch: 103 | Batch_idx: 200 |  Loss: (0.2664) |  Loss2: (0.0000) | Acc: (90.00%) (23299/25728)
Epoch: 103 | Batch_idx: 210 |  Loss: (0.2656) |  Loss2: (0.0000) | Acc: (90.00%) (24469/27008)
Epoch: 103 | Batch_idx: 220 |  Loss: (0.2654) |  Loss2: (0.0000) | Acc: (90.00%) (25629/28288)
Epoch: 103 | Batch_idx: 230 |  Loss: (0.2636) |  Loss2: (0.0000) | Acc: (90.00%) (26811/29568)
Epoch: 103 | Batch_idx: 240 |  Loss: (0.2628) |  Loss2: (0.0000) | Acc: (90.00%) (27982/30848)
Epoch: 103 | Batch_idx: 250 |  Loss: (0.2618) |  Loss2: (0.0000) | Acc: (90.00%) (29163/32128)
Epoch: 103 | Batch_idx: 260 |  Loss: (0.2607) |  Loss2: (0.0000) | Acc: (90.00%) (30339/33408)
Epoch: 103 | Batch_idx: 270 |  Loss: (0.2602) |  Loss2: (0.0000) | Acc: (90.00%) (31522/34688)
Epoch: 103 | Batch_idx: 280 |  Loss: (0.2595) |  Loss2: (0.0000) | Acc: (90.00%) (32699/35968)
Epoch: 103 | Batch_idx: 290 |  Loss: (0.2571) |  Loss2: (0.0000) | Acc: (91.00%) (33905/37248)
Epoch: 103 | Batch_idx: 300 |  Loss: (0.2562) |  Loss2: (0.0000) | Acc: (91.00%) (35088/38528)
Epoch: 103 | Batch_idx: 310 |  Loss: (0.2557) |  Loss2: (0.0000) | Acc: (91.00%) (36260/39808)
Epoch: 103 | Batch_idx: 320 |  Loss: (0.2551) |  Loss2: (0.0000) | Acc: (91.00%) (37435/41088)
Epoch: 103 | Batch_idx: 330 |  Loss: (0.2541) |  Loss2: (0.0000) | Acc: (91.00%) (38620/42368)
Epoch: 103 | Batch_idx: 340 |  Loss: (0.2534) |  Loss2: (0.0000) | Acc: (91.00%) (39798/43648)
Epoch: 103 | Batch_idx: 350 |  Loss: (0.2524) |  Loss2: (0.0000) | Acc: (91.00%) (40987/44928)
Epoch: 103 | Batch_idx: 360 |  Loss: (0.2517) |  Loss2: (0.0000) | Acc: (91.00%) (42172/46208)
Epoch: 103 | Batch_idx: 370 |  Loss: (0.2507) |  Loss2: (0.0000) | Acc: (91.00%) (43358/47488)
Epoch: 103 | Batch_idx: 380 |  Loss: (0.2498) |  Loss2: (0.0000) | Acc: (91.00%) (44543/48768)
Epoch: 103 | Batch_idx: 390 |  Loss: (0.2503) |  Loss2: (0.0000) | Acc: (91.00%) (45671/50000)
# TEST : Loss: (0.4098) | Acc: (86.00%) (8619/10000)
percent tensor([0.5581, 0.5747, 0.5578, 0.5494, 0.5622, 0.5450, 0.5747, 0.5689, 0.5721,
        0.5691, 0.5693, 0.5640, 0.5652, 0.5750, 0.5613, 0.5562],
       device='cuda:0') torch.Size([16])
percent tensor([0.5191, 0.5276, 0.4815, 0.5114, 0.5086, 0.5003, 0.5266, 0.5324, 0.5138,
        0.5029, 0.4999, 0.4830, 0.5205, 0.5295, 0.5266, 0.5158],
       device='cuda:0') torch.Size([16])
percent tensor([0.5049, 0.4746, 0.4546, 0.5285, 0.4480, 0.5318, 0.4803, 0.5065, 0.4883,
        0.4605, 0.4872, 0.4425, 0.4519, 0.5472, 0.5152, 0.5147],
       device='cuda:0') torch.Size([16])
percent tensor([0.6351, 0.6539, 0.5950, 0.5793, 0.5861, 0.6476, 0.6266, 0.5789, 0.6176,
        0.6449, 0.6412, 0.6160, 0.6741, 0.6152, 0.6426, 0.6519],
       device='cuda:0') torch.Size([16])
percent tensor([0.6717, 0.5934, 0.6712, 0.6876, 0.7019, 0.7256, 0.6629, 0.6922, 0.7203,
        0.6466, 0.7034, 0.6377, 0.5483, 0.7288, 0.6525, 0.6964],
       device='cuda:0') torch.Size([16])
percent tensor([0.7049, 0.7015, 0.7248, 0.7397, 0.7633, 0.7531, 0.7307, 0.7373, 0.7333,
        0.7067, 0.7232, 0.7039, 0.6862, 0.7472, 0.7024, 0.7350],
       device='cuda:0') torch.Size([16])
percent tensor([0.4993, 0.5857, 0.5948, 0.5168, 0.6412, 0.6966, 0.5620, 0.4576, 0.6146,
        0.5475, 0.5722, 0.5021, 0.4999, 0.5964, 0.3933, 0.5618],
       device='cuda:0') torch.Size([16])
percent tensor([0.9993, 0.9991, 0.9991, 0.9992, 0.9995, 0.9982, 0.9989, 0.9994, 0.9993,
        0.9993, 0.9996, 0.9995, 0.9992, 0.9990, 0.9984, 0.9997],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 104 | Batch_idx: 0 |  Loss: (0.1435) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 104 | Batch_idx: 10 |  Loss: (0.1953) |  Loss2: (0.0000) | Acc: (93.00%) (1319/1408)
Epoch: 104 | Batch_idx: 20 |  Loss: (0.1900) |  Loss2: (0.0000) | Acc: (93.00%) (2521/2688)
Epoch: 104 | Batch_idx: 30 |  Loss: (0.1825) |  Loss2: (0.0000) | Acc: (94.00%) (3732/3968)
Epoch: 104 | Batch_idx: 40 |  Loss: (0.1828) |  Loss2: (0.0000) | Acc: (93.00%) (4931/5248)
Epoch: 104 | Batch_idx: 50 |  Loss: (0.1834) |  Loss2: (0.0000) | Acc: (94.00%) (6137/6528)
Epoch: 104 | Batch_idx: 60 |  Loss: (0.1874) |  Loss2: (0.0000) | Acc: (93.00%) (7328/7808)
Epoch: 104 | Batch_idx: 70 |  Loss: (0.1890) |  Loss2: (0.0000) | Acc: (93.00%) (8527/9088)
Epoch: 104 | Batch_idx: 80 |  Loss: (0.1898) |  Loss2: (0.0000) | Acc: (93.00%) (9718/10368)
Epoch: 104 | Batch_idx: 90 |  Loss: (0.1903) |  Loss2: (0.0000) | Acc: (93.00%) (10905/11648)
Epoch: 104 | Batch_idx: 100 |  Loss: (0.1899) |  Loss2: (0.0000) | Acc: (93.00%) (12105/12928)
Epoch: 104 | Batch_idx: 110 |  Loss: (0.1910) |  Loss2: (0.0000) | Acc: (93.00%) (13294/14208)
Epoch: 104 | Batch_idx: 120 |  Loss: (0.1926) |  Loss2: (0.0000) | Acc: (93.00%) (14472/15488)
Epoch: 104 | Batch_idx: 130 |  Loss: (0.1913) |  Loss2: (0.0000) | Acc: (93.00%) (15674/16768)
Epoch: 104 | Batch_idx: 140 |  Loss: (0.1906) |  Loss2: (0.0000) | Acc: (93.00%) (16886/18048)
Epoch: 104 | Batch_idx: 150 |  Loss: (0.1914) |  Loss2: (0.0000) | Acc: (93.00%) (18082/19328)
Epoch: 104 | Batch_idx: 160 |  Loss: (0.1907) |  Loss2: (0.0000) | Acc: (93.00%) (19282/20608)
Epoch: 104 | Batch_idx: 170 |  Loss: (0.1900) |  Loss2: (0.0000) | Acc: (93.00%) (20486/21888)
Epoch: 104 | Batch_idx: 180 |  Loss: (0.1922) |  Loss2: (0.0000) | Acc: (93.00%) (21663/23168)
Epoch: 104 | Batch_idx: 190 |  Loss: (0.1922) |  Loss2: (0.0000) | Acc: (93.00%) (22861/24448)
Epoch: 104 | Batch_idx: 200 |  Loss: (0.1918) |  Loss2: (0.0000) | Acc: (93.00%) (24069/25728)
Epoch: 104 | Batch_idx: 210 |  Loss: (0.1931) |  Loss2: (0.0000) | Acc: (93.00%) (25249/27008)
Epoch: 104 | Batch_idx: 220 |  Loss: (0.1948) |  Loss2: (0.0000) | Acc: (93.00%) (26438/28288)
Epoch: 104 | Batch_idx: 230 |  Loss: (0.1946) |  Loss2: (0.0000) | Acc: (93.00%) (27641/29568)
Epoch: 104 | Batch_idx: 240 |  Loss: (0.1954) |  Loss2: (0.0000) | Acc: (93.00%) (28822/30848)
Epoch: 104 | Batch_idx: 250 |  Loss: (0.1958) |  Loss2: (0.0000) | Acc: (93.00%) (30005/32128)
Epoch: 104 | Batch_idx: 260 |  Loss: (0.1959) |  Loss2: (0.0000) | Acc: (93.00%) (31189/33408)
Epoch: 104 | Batch_idx: 270 |  Loss: (0.1944) |  Loss2: (0.0000) | Acc: (93.00%) (32399/34688)
Epoch: 104 | Batch_idx: 280 |  Loss: (0.1946) |  Loss2: (0.0000) | Acc: (93.00%) (33593/35968)
Epoch: 104 | Batch_idx: 290 |  Loss: (0.1952) |  Loss2: (0.0000) | Acc: (93.00%) (34779/37248)
Epoch: 104 | Batch_idx: 300 |  Loss: (0.1951) |  Loss2: (0.0000) | Acc: (93.00%) (35968/38528)
Epoch: 104 | Batch_idx: 310 |  Loss: (0.1945) |  Loss2: (0.0000) | Acc: (93.00%) (37174/39808)
Epoch: 104 | Batch_idx: 320 |  Loss: (0.1951) |  Loss2: (0.0000) | Acc: (93.00%) (38360/41088)
Epoch: 104 | Batch_idx: 330 |  Loss: (0.1949) |  Loss2: (0.0000) | Acc: (93.00%) (39558/42368)
Epoch: 104 | Batch_idx: 340 |  Loss: (0.1944) |  Loss2: (0.0000) | Acc: (93.00%) (40767/43648)
Epoch: 104 | Batch_idx: 350 |  Loss: (0.1943) |  Loss2: (0.0000) | Acc: (93.00%) (41954/44928)
Epoch: 104 | Batch_idx: 360 |  Loss: (0.1940) |  Loss2: (0.0000) | Acc: (93.00%) (43151/46208)
Epoch: 104 | Batch_idx: 370 |  Loss: (0.1938) |  Loss2: (0.0000) | Acc: (93.00%) (44346/47488)
Epoch: 104 | Batch_idx: 380 |  Loss: (0.1934) |  Loss2: (0.0000) | Acc: (93.00%) (45541/48768)
Epoch: 104 | Batch_idx: 390 |  Loss: (0.1939) |  Loss2: (0.0000) | Acc: (93.00%) (46681/50000)
# TEST : Loss: (0.3848) | Acc: (87.00%) (8722/10000)
percent tensor([0.5574, 0.5738, 0.5537, 0.5496, 0.5585, 0.5445, 0.5736, 0.5678, 0.5706,
        0.5684, 0.5691, 0.5616, 0.5643, 0.5748, 0.5611, 0.5563],
       device='cuda:0') torch.Size([16])
percent tensor([0.5183, 0.5302, 0.4826, 0.5056, 0.5079, 0.4998, 0.5280, 0.5336, 0.5145,
        0.5026, 0.4993, 0.4829, 0.5194, 0.5305, 0.5252, 0.5137],
       device='cuda:0') torch.Size([16])
percent tensor([0.5037, 0.4712, 0.4597, 0.5214, 0.4482, 0.5290, 0.4752, 0.5063, 0.4922,
        0.4529, 0.4827, 0.4456, 0.4513, 0.5424, 0.5090, 0.5107],
       device='cuda:0') torch.Size([16])
percent tensor([0.6368, 0.6538, 0.6034, 0.5887, 0.5946, 0.6523, 0.6275, 0.5789, 0.6178,
        0.6487, 0.6414, 0.6228, 0.6757, 0.6143, 0.6392, 0.6523],
       device='cuda:0') torch.Size([16])
percent tensor([0.6705, 0.5999, 0.6675, 0.6728, 0.7049, 0.7269, 0.6701, 0.6647, 0.7159,
        0.6506, 0.7054, 0.6399, 0.5774, 0.7355, 0.6349, 0.6951],
       device='cuda:0') torch.Size([16])
percent tensor([0.7045, 0.6962, 0.7263, 0.7286, 0.7612, 0.7571, 0.7313, 0.7303, 0.7258,
        0.7092, 0.7181, 0.7017, 0.6935, 0.7414, 0.6959, 0.7346],
       device='cuda:0') torch.Size([16])
percent tensor([0.4899, 0.5508, 0.6042, 0.5506, 0.6252, 0.7095, 0.5565, 0.4514, 0.6046,
        0.5371, 0.5456, 0.4964, 0.5185, 0.5660, 0.3716, 0.5363],
       device='cuda:0') torch.Size([16])
percent tensor([0.9995, 0.9993, 0.9989, 0.9993, 0.9994, 0.9972, 0.9988, 0.9996, 0.9996,
        0.9994, 0.9998, 0.9995, 0.9992, 0.9992, 0.9987, 0.9994],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 105 | Batch_idx: 0 |  Loss: (0.1799) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 105 | Batch_idx: 10 |  Loss: (0.1830) |  Loss2: (0.0000) | Acc: (93.00%) (1319/1408)
Epoch: 105 | Batch_idx: 20 |  Loss: (0.2075) |  Loss2: (0.0000) | Acc: (92.00%) (2497/2688)
Epoch: 105 | Batch_idx: 30 |  Loss: (0.2111) |  Loss2: (0.0000) | Acc: (92.00%) (3681/3968)
Epoch: 105 | Batch_idx: 40 |  Loss: (0.2105) |  Loss2: (0.0000) | Acc: (92.00%) (4864/5248)
Epoch: 105 | Batch_idx: 50 |  Loss: (0.2153) |  Loss2: (0.0000) | Acc: (92.00%) (6034/6528)
Epoch: 105 | Batch_idx: 60 |  Loss: (0.2149) |  Loss2: (0.0000) | Acc: (92.00%) (7217/7808)
Epoch: 105 | Batch_idx: 70 |  Loss: (0.2145) |  Loss2: (0.0000) | Acc: (92.00%) (8411/9088)
Epoch: 105 | Batch_idx: 80 |  Loss: (0.2176) |  Loss2: (0.0000) | Acc: (92.00%) (9587/10368)
Epoch: 105 | Batch_idx: 90 |  Loss: (0.2190) |  Loss2: (0.0000) | Acc: (92.00%) (10762/11648)
Epoch: 105 | Batch_idx: 100 |  Loss: (0.2177) |  Loss2: (0.0000) | Acc: (92.00%) (11960/12928)
Epoch: 105 | Batch_idx: 110 |  Loss: (0.2186) |  Loss2: (0.0000) | Acc: (92.00%) (13136/14208)
Epoch: 105 | Batch_idx: 120 |  Loss: (0.2175) |  Loss2: (0.0000) | Acc: (92.00%) (14333/15488)
Epoch: 105 | Batch_idx: 130 |  Loss: (0.2186) |  Loss2: (0.0000) | Acc: (92.00%) (15508/16768)
Epoch: 105 | Batch_idx: 140 |  Loss: (0.2192) |  Loss2: (0.0000) | Acc: (92.00%) (16696/18048)
Epoch: 105 | Batch_idx: 150 |  Loss: (0.2196) |  Loss2: (0.0000) | Acc: (92.00%) (17884/19328)
Epoch: 105 | Batch_idx: 160 |  Loss: (0.2189) |  Loss2: (0.0000) | Acc: (92.00%) (19067/20608)
Epoch: 105 | Batch_idx: 170 |  Loss: (0.2179) |  Loss2: (0.0000) | Acc: (92.00%) (20256/21888)
Epoch: 105 | Batch_idx: 180 |  Loss: (0.2175) |  Loss2: (0.0000) | Acc: (92.00%) (21446/23168)
Epoch: 105 | Batch_idx: 190 |  Loss: (0.2168) |  Loss2: (0.0000) | Acc: (92.00%) (22637/24448)
Epoch: 105 | Batch_idx: 200 |  Loss: (0.2165) |  Loss2: (0.0000) | Acc: (92.00%) (23815/25728)
Epoch: 105 | Batch_idx: 210 |  Loss: (0.2159) |  Loss2: (0.0000) | Acc: (92.00%) (25007/27008)
Epoch: 105 | Batch_idx: 220 |  Loss: (0.2158) |  Loss2: (0.0000) | Acc: (92.00%) (26188/28288)
Epoch: 105 | Batch_idx: 230 |  Loss: (0.2155) |  Loss2: (0.0000) | Acc: (92.00%) (27377/29568)
Epoch: 105 | Batch_idx: 240 |  Loss: (0.2149) |  Loss2: (0.0000) | Acc: (92.00%) (28564/30848)
Epoch: 105 | Batch_idx: 250 |  Loss: (0.2144) |  Loss2: (0.0000) | Acc: (92.00%) (29759/32128)
Epoch: 105 | Batch_idx: 260 |  Loss: (0.2140) |  Loss2: (0.0000) | Acc: (92.00%) (30955/33408)
Epoch: 105 | Batch_idx: 270 |  Loss: (0.2144) |  Loss2: (0.0000) | Acc: (92.00%) (32134/34688)
Epoch: 105 | Batch_idx: 280 |  Loss: (0.2139) |  Loss2: (0.0000) | Acc: (92.00%) (33323/35968)
Epoch: 105 | Batch_idx: 290 |  Loss: (0.2135) |  Loss2: (0.0000) | Acc: (92.00%) (34512/37248)
Epoch: 105 | Batch_idx: 300 |  Loss: (0.2138) |  Loss2: (0.0000) | Acc: (92.00%) (35697/38528)
Epoch: 105 | Batch_idx: 310 |  Loss: (0.2137) |  Loss2: (0.0000) | Acc: (92.00%) (36880/39808)
Epoch: 105 | Batch_idx: 320 |  Loss: (0.2142) |  Loss2: (0.0000) | Acc: (92.00%) (38057/41088)
Epoch: 105 | Batch_idx: 330 |  Loss: (0.2145) |  Loss2: (0.0000) | Acc: (92.00%) (39239/42368)
Epoch: 105 | Batch_idx: 340 |  Loss: (0.2146) |  Loss2: (0.0000) | Acc: (92.00%) (40422/43648)
Epoch: 105 | Batch_idx: 350 |  Loss: (0.2139) |  Loss2: (0.0000) | Acc: (92.00%) (41617/44928)
Epoch: 105 | Batch_idx: 360 |  Loss: (0.2146) |  Loss2: (0.0000) | Acc: (92.00%) (42778/46208)
Epoch: 105 | Batch_idx: 370 |  Loss: (0.2146) |  Loss2: (0.0000) | Acc: (92.00%) (43965/47488)
Epoch: 105 | Batch_idx: 380 |  Loss: (0.2150) |  Loss2: (0.0000) | Acc: (92.00%) (45140/48768)
Epoch: 105 | Batch_idx: 390 |  Loss: (0.2150) |  Loss2: (0.0000) | Acc: (92.00%) (46276/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_105.pth.tar'
# TEST : Loss: (0.3973) | Acc: (87.00%) (8727/10000)
percent tensor([0.5602, 0.5784, 0.5551, 0.5532, 0.5612, 0.5481, 0.5777, 0.5721, 0.5737,
        0.5722, 0.5725, 0.5647, 0.5675, 0.5790, 0.5656, 0.5596],
       device='cuda:0') torch.Size([16])
percent tensor([0.5111, 0.5201, 0.4800, 0.4988, 0.5050, 0.4944, 0.5204, 0.5294, 0.5071,
        0.4941, 0.4903, 0.4781, 0.5116, 0.5170, 0.5169, 0.5061],
       device='cuda:0') torch.Size([16])
percent tensor([0.5204, 0.4814, 0.4723, 0.5362, 0.4530, 0.5415, 0.4839, 0.5150, 0.5044,
        0.4676, 0.4977, 0.4590, 0.4704, 0.5543, 0.5205, 0.5259],
       device='cuda:0') torch.Size([16])
percent tensor([0.6498, 0.6687, 0.6158, 0.6045, 0.6124, 0.6611, 0.6434, 0.5968, 0.6341,
        0.6659, 0.6570, 0.6417, 0.6890, 0.6297, 0.6535, 0.6642],
       device='cuda:0') torch.Size([16])
percent tensor([0.6520, 0.5863, 0.6588, 0.6652, 0.6957, 0.7161, 0.6604, 0.6568, 0.7042,
        0.6415, 0.6928, 0.6524, 0.5632, 0.7305, 0.6249, 0.6847],
       device='cuda:0') torch.Size([16])
percent tensor([0.6978, 0.6925, 0.7246, 0.7259, 0.7569, 0.7541, 0.7259, 0.7292, 0.7233,
        0.7042, 0.7158, 0.6936, 0.6874, 0.7440, 0.6879, 0.7298],
       device='cuda:0') torch.Size([16])
percent tensor([0.4774, 0.5554, 0.6310, 0.5612, 0.6642, 0.6978, 0.5846, 0.5160, 0.6031,
        0.5280, 0.5317, 0.4974, 0.4941, 0.5510, 0.3722, 0.5435],
       device='cuda:0') torch.Size([16])
percent tensor([0.9995, 0.9991, 0.9991, 0.9993, 0.9995, 0.9965, 0.9989, 0.9998, 0.9993,
        0.9993, 0.9998, 0.9995, 0.9989, 0.9991, 0.9986, 0.9995],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 106 | Batch_idx: 0 |  Loss: (0.1737) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 106 | Batch_idx: 10 |  Loss: (0.1875) |  Loss2: (0.0000) | Acc: (93.00%) (1319/1408)
Epoch: 106 | Batch_idx: 20 |  Loss: (0.1957) |  Loss2: (0.0000) | Acc: (93.00%) (2507/2688)
Epoch: 106 | Batch_idx: 30 |  Loss: (0.1948) |  Loss2: (0.0000) | Acc: (93.00%) (3704/3968)
Epoch: 106 | Batch_idx: 40 |  Loss: (0.1864) |  Loss2: (0.0000) | Acc: (93.00%) (4922/5248)
Epoch: 106 | Batch_idx: 50 |  Loss: (0.1863) |  Loss2: (0.0000) | Acc: (93.00%) (6112/6528)
Epoch: 106 | Batch_idx: 60 |  Loss: (0.1852) |  Loss2: (0.0000) | Acc: (93.00%) (7308/7808)
Epoch: 106 | Batch_idx: 70 |  Loss: (0.1842) |  Loss2: (0.0000) | Acc: (93.00%) (8506/9088)
Epoch: 106 | Batch_idx: 80 |  Loss: (0.1832) |  Loss2: (0.0000) | Acc: (93.00%) (9710/10368)
Epoch: 106 | Batch_idx: 90 |  Loss: (0.1832) |  Loss2: (0.0000) | Acc: (93.00%) (10905/11648)
Epoch: 106 | Batch_idx: 100 |  Loss: (0.1837) |  Loss2: (0.0000) | Acc: (93.00%) (12103/12928)
Epoch: 106 | Batch_idx: 110 |  Loss: (0.1855) |  Loss2: (0.0000) | Acc: (93.00%) (13290/14208)
Epoch: 106 | Batch_idx: 120 |  Loss: (0.1856) |  Loss2: (0.0000) | Acc: (93.00%) (14486/15488)
Epoch: 106 | Batch_idx: 130 |  Loss: (0.1855) |  Loss2: (0.0000) | Acc: (93.00%) (15688/16768)
Epoch: 106 | Batch_idx: 140 |  Loss: (0.1840) |  Loss2: (0.0000) | Acc: (93.00%) (16891/18048)
Epoch: 106 | Batch_idx: 150 |  Loss: (0.1857) |  Loss2: (0.0000) | Acc: (93.00%) (18072/19328)
Epoch: 106 | Batch_idx: 160 |  Loss: (0.1875) |  Loss2: (0.0000) | Acc: (93.00%) (19260/20608)
Epoch: 106 | Batch_idx: 170 |  Loss: (0.1873) |  Loss2: (0.0000) | Acc: (93.00%) (20459/21888)
Epoch: 106 | Batch_idx: 180 |  Loss: (0.1865) |  Loss2: (0.0000) | Acc: (93.00%) (21658/23168)
Epoch: 106 | Batch_idx: 190 |  Loss: (0.1883) |  Loss2: (0.0000) | Acc: (93.00%) (22837/24448)
Epoch: 106 | Batch_idx: 200 |  Loss: (0.1891) |  Loss2: (0.0000) | Acc: (93.00%) (24016/25728)
Epoch: 106 | Batch_idx: 210 |  Loss: (0.1896) |  Loss2: (0.0000) | Acc: (93.00%) (25208/27008)
Epoch: 106 | Batch_idx: 220 |  Loss: (0.1900) |  Loss2: (0.0000) | Acc: (93.00%) (26397/28288)
Epoch: 106 | Batch_idx: 230 |  Loss: (0.1900) |  Loss2: (0.0000) | Acc: (93.00%) (27602/29568)
Epoch: 106 | Batch_idx: 240 |  Loss: (0.1905) |  Loss2: (0.0000) | Acc: (93.00%) (28792/30848)
Epoch: 106 | Batch_idx: 250 |  Loss: (0.1895) |  Loss2: (0.0000) | Acc: (93.00%) (30001/32128)
Epoch: 106 | Batch_idx: 260 |  Loss: (0.1896) |  Loss2: (0.0000) | Acc: (93.00%) (31196/33408)
Epoch: 106 | Batch_idx: 270 |  Loss: (0.1902) |  Loss2: (0.0000) | Acc: (93.00%) (32386/34688)
Epoch: 106 | Batch_idx: 280 |  Loss: (0.1906) |  Loss2: (0.0000) | Acc: (93.00%) (33576/35968)
Epoch: 106 | Batch_idx: 290 |  Loss: (0.1906) |  Loss2: (0.0000) | Acc: (93.00%) (34772/37248)
Epoch: 106 | Batch_idx: 300 |  Loss: (0.1923) |  Loss2: (0.0000) | Acc: (93.00%) (35933/38528)
Epoch: 106 | Batch_idx: 310 |  Loss: (0.1922) |  Loss2: (0.0000) | Acc: (93.00%) (37120/39808)
Epoch: 106 | Batch_idx: 320 |  Loss: (0.1933) |  Loss2: (0.0000) | Acc: (93.00%) (38304/41088)
Epoch: 106 | Batch_idx: 330 |  Loss: (0.1937) |  Loss2: (0.0000) | Acc: (93.00%) (39487/42368)
Epoch: 106 | Batch_idx: 340 |  Loss: (0.1941) |  Loss2: (0.0000) | Acc: (93.00%) (40675/43648)
Epoch: 106 | Batch_idx: 350 |  Loss: (0.1950) |  Loss2: (0.0000) | Acc: (93.00%) (41853/44928)
Epoch: 106 | Batch_idx: 360 |  Loss: (0.1947) |  Loss2: (0.0000) | Acc: (93.00%) (43052/46208)
Epoch: 106 | Batch_idx: 370 |  Loss: (0.1950) |  Loss2: (0.0000) | Acc: (93.00%) (44244/47488)
Epoch: 106 | Batch_idx: 380 |  Loss: (0.1952) |  Loss2: (0.0000) | Acc: (93.00%) (45439/48768)
Epoch: 106 | Batch_idx: 390 |  Loss: (0.1951) |  Loss2: (0.0000) | Acc: (93.00%) (46587/50000)
# TEST : Loss: (0.3786) | Acc: (87.00%) (8761/10000)
percent tensor([0.5610, 0.5775, 0.5632, 0.5546, 0.5680, 0.5476, 0.5790, 0.5732, 0.5740,
        0.5728, 0.5713, 0.5702, 0.5678, 0.5763, 0.5653, 0.5585],
       device='cuda:0') torch.Size([16])
percent tensor([0.5109, 0.5185, 0.4759, 0.4985, 0.5039, 0.4912, 0.5191, 0.5257, 0.5054,
        0.4922, 0.4890, 0.4729, 0.5109, 0.5161, 0.5174, 0.5056],
       device='cuda:0') torch.Size([16])
percent tensor([0.5150, 0.4782, 0.4583, 0.5331, 0.4521, 0.5405, 0.4822, 0.5041, 0.5068,
        0.4618, 0.4963, 0.4570, 0.4640, 0.5556, 0.5182, 0.5232],
       device='cuda:0') torch.Size([16])
percent tensor([0.6509, 0.6682, 0.6143, 0.5980, 0.6072, 0.6564, 0.6449, 0.5965, 0.6362,
        0.6623, 0.6575, 0.6392, 0.6899, 0.6323, 0.6512, 0.6661],
       device='cuda:0') torch.Size([16])
percent tensor([0.6497, 0.5730, 0.6471, 0.6583, 0.6834, 0.7175, 0.6445, 0.6466, 0.7041,
        0.6433, 0.6848, 0.6448, 0.5702, 0.6963, 0.6174, 0.6840],
       device='cuda:0') torch.Size([16])
percent tensor([0.7008, 0.6889, 0.7266, 0.7340, 0.7613, 0.7538, 0.7226, 0.7212, 0.7249,
        0.7057, 0.7170, 0.7059, 0.6859, 0.7378, 0.6843, 0.7316],
       device='cuda:0') torch.Size([16])
percent tensor([0.4953, 0.5619, 0.6343, 0.5612, 0.6628, 0.7086, 0.5735, 0.4859, 0.5963,
        0.5413, 0.5393, 0.5503, 0.4911, 0.5759, 0.3769, 0.5666],
       device='cuda:0') torch.Size([16])
percent tensor([0.9995, 0.9995, 0.9991, 0.9995, 0.9993, 0.9987, 0.9992, 0.9995, 0.9994,
        0.9996, 0.9997, 0.9995, 0.9989, 0.9993, 0.9988, 0.9995],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 107 | Batch_idx: 0 |  Loss: (0.1707) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 107 | Batch_idx: 10 |  Loss: (0.1557) |  Loss2: (0.0000) | Acc: (95.00%) (1346/1408)
Epoch: 107 | Batch_idx: 20 |  Loss: (0.1837) |  Loss2: (0.0000) | Acc: (94.00%) (2532/2688)
Epoch: 107 | Batch_idx: 30 |  Loss: (0.1913) |  Loss2: (0.0000) | Acc: (93.00%) (3722/3968)
Epoch: 107 | Batch_idx: 40 |  Loss: (0.2034) |  Loss2: (0.0000) | Acc: (93.00%) (4889/5248)
Epoch: 107 | Batch_idx: 50 |  Loss: (0.2054) |  Loss2: (0.0000) | Acc: (93.00%) (6076/6528)
Epoch: 107 | Batch_idx: 60 |  Loss: (0.2073) |  Loss2: (0.0000) | Acc: (92.00%) (7251/7808)
Epoch: 107 | Batch_idx: 70 |  Loss: (0.2091) |  Loss2: (0.0000) | Acc: (92.00%) (8428/9088)
Epoch: 107 | Batch_idx: 80 |  Loss: (0.2098) |  Loss2: (0.0000) | Acc: (92.00%) (9612/10368)
Epoch: 107 | Batch_idx: 90 |  Loss: (0.2076) |  Loss2: (0.0000) | Acc: (92.00%) (10821/11648)
Epoch: 107 | Batch_idx: 100 |  Loss: (0.2083) |  Loss2: (0.0000) | Acc: (92.00%) (12007/12928)
Epoch: 107 | Batch_idx: 110 |  Loss: (0.2076) |  Loss2: (0.0000) | Acc: (92.00%) (13200/14208)
Epoch: 107 | Batch_idx: 120 |  Loss: (0.2086) |  Loss2: (0.0000) | Acc: (92.00%) (14382/15488)
Epoch: 107 | Batch_idx: 130 |  Loss: (0.2086) |  Loss2: (0.0000) | Acc: (92.00%) (15565/16768)
Epoch: 107 | Batch_idx: 140 |  Loss: (0.2108) |  Loss2: (0.0000) | Acc: (92.00%) (16736/18048)
Epoch: 107 | Batch_idx: 150 |  Loss: (0.2106) |  Loss2: (0.0000) | Acc: (92.00%) (17924/19328)
Epoch: 107 | Batch_idx: 160 |  Loss: (0.2121) |  Loss2: (0.0000) | Acc: (92.00%) (19103/20608)
Epoch: 107 | Batch_idx: 170 |  Loss: (0.2122) |  Loss2: (0.0000) | Acc: (92.00%) (20288/21888)
Epoch: 107 | Batch_idx: 180 |  Loss: (0.2121) |  Loss2: (0.0000) | Acc: (92.00%) (21478/23168)
Epoch: 107 | Batch_idx: 190 |  Loss: (0.2108) |  Loss2: (0.0000) | Acc: (92.00%) (22682/24448)
Epoch: 107 | Batch_idx: 200 |  Loss: (0.2102) |  Loss2: (0.0000) | Acc: (92.00%) (23870/25728)
Epoch: 107 | Batch_idx: 210 |  Loss: (0.2113) |  Loss2: (0.0000) | Acc: (92.00%) (25041/27008)
Epoch: 107 | Batch_idx: 220 |  Loss: (0.2107) |  Loss2: (0.0000) | Acc: (92.00%) (26224/28288)
Epoch: 107 | Batch_idx: 230 |  Loss: (0.2112) |  Loss2: (0.0000) | Acc: (92.00%) (27404/29568)
Epoch: 107 | Batch_idx: 240 |  Loss: (0.2113) |  Loss2: (0.0000) | Acc: (92.00%) (28584/30848)
Epoch: 107 | Batch_idx: 250 |  Loss: (0.2114) |  Loss2: (0.0000) | Acc: (92.00%) (29770/32128)
Epoch: 107 | Batch_idx: 260 |  Loss: (0.2115) |  Loss2: (0.0000) | Acc: (92.00%) (30960/33408)
Epoch: 107 | Batch_idx: 270 |  Loss: (0.2105) |  Loss2: (0.0000) | Acc: (92.00%) (32164/34688)
Epoch: 107 | Batch_idx: 280 |  Loss: (0.2093) |  Loss2: (0.0000) | Acc: (92.00%) (33371/35968)
Epoch: 107 | Batch_idx: 290 |  Loss: (0.2093) |  Loss2: (0.0000) | Acc: (92.00%) (34559/37248)
Epoch: 107 | Batch_idx: 300 |  Loss: (0.2092) |  Loss2: (0.0000) | Acc: (92.00%) (35760/38528)
Epoch: 107 | Batch_idx: 310 |  Loss: (0.2085) |  Loss2: (0.0000) | Acc: (92.00%) (36962/39808)
Epoch: 107 | Batch_idx: 320 |  Loss: (0.2068) |  Loss2: (0.0000) | Acc: (92.00%) (38179/41088)
Epoch: 107 | Batch_idx: 330 |  Loss: (0.2072) |  Loss2: (0.0000) | Acc: (92.00%) (39357/42368)
Epoch: 107 | Batch_idx: 340 |  Loss: (0.2068) |  Loss2: (0.0000) | Acc: (92.00%) (40555/43648)
Epoch: 107 | Batch_idx: 350 |  Loss: (0.2067) |  Loss2: (0.0000) | Acc: (92.00%) (41744/44928)
Epoch: 107 | Batch_idx: 360 |  Loss: (0.2061) |  Loss2: (0.0000) | Acc: (92.00%) (42944/46208)
Epoch: 107 | Batch_idx: 370 |  Loss: (0.2059) |  Loss2: (0.0000) | Acc: (92.00%) (44132/47488)
Epoch: 107 | Batch_idx: 380 |  Loss: (0.2058) |  Loss2: (0.0000) | Acc: (92.00%) (45317/48768)
Epoch: 107 | Batch_idx: 390 |  Loss: (0.2052) |  Loss2: (0.0000) | Acc: (92.00%) (46475/50000)
# TEST : Loss: (0.3875) | Acc: (87.00%) (8715/10000)
percent tensor([0.5616, 0.5770, 0.5648, 0.5558, 0.5700, 0.5497, 0.5792, 0.5738, 0.5747,
        0.5731, 0.5719, 0.5719, 0.5684, 0.5754, 0.5660, 0.5591],
       device='cuda:0') torch.Size([16])
percent tensor([0.5062, 0.5095, 0.4730, 0.4924, 0.4993, 0.4880, 0.5120, 0.5212, 0.4996,
        0.4847, 0.4829, 0.4666, 0.5052, 0.5075, 0.5110, 0.4998],
       device='cuda:0') torch.Size([16])
percent tensor([0.5144, 0.4626, 0.4612, 0.5344, 0.4498, 0.5392, 0.4726, 0.5028, 0.5060,
        0.4538, 0.4899, 0.4514, 0.4577, 0.5530, 0.5084, 0.5177],
       device='cuda:0') torch.Size([16])
percent tensor([0.6713, 0.6829, 0.6332, 0.6174, 0.6279, 0.6771, 0.6625, 0.6146, 0.6568,
        0.6792, 0.6761, 0.6612, 0.7108, 0.6489, 0.6709, 0.6840],
       device='cuda:0') torch.Size([16])
percent tensor([0.6672, 0.5676, 0.6787, 0.6770, 0.7033, 0.7400, 0.6468, 0.6707, 0.7090,
        0.6391, 0.6783, 0.6691, 0.5796, 0.6887, 0.6269, 0.6925],
       device='cuda:0') torch.Size([16])
percent tensor([0.6979, 0.6855, 0.7262, 0.7307, 0.7607, 0.7591, 0.7185, 0.7192, 0.7240,
        0.7011, 0.7129, 0.7054, 0.6833, 0.7391, 0.6809, 0.7272],
       device='cuda:0') torch.Size([16])
percent tensor([0.4491, 0.5359, 0.5871, 0.5171, 0.6377, 0.7047, 0.5322, 0.4327, 0.5605,
        0.4968, 0.4953, 0.4945, 0.4581, 0.5410, 0.3526, 0.5261],
       device='cuda:0') torch.Size([16])
percent tensor([0.9995, 0.9994, 0.9991, 0.9996, 0.9994, 0.9984, 0.9992, 0.9995, 0.9995,
        0.9996, 0.9998, 0.9996, 0.9990, 0.9993, 0.9989, 0.9993],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 108 | Batch_idx: 0 |  Loss: (0.2087) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 108 | Batch_idx: 10 |  Loss: (0.1690) |  Loss2: (0.0000) | Acc: (93.00%) (1323/1408)
Epoch: 108 | Batch_idx: 20 |  Loss: (0.1692) |  Loss2: (0.0000) | Acc: (94.00%) (2527/2688)
Epoch: 108 | Batch_idx: 30 |  Loss: (0.1689) |  Loss2: (0.0000) | Acc: (94.00%) (3739/3968)
Epoch: 108 | Batch_idx: 40 |  Loss: (0.1744) |  Loss2: (0.0000) | Acc: (94.00%) (4935/5248)
Epoch: 108 | Batch_idx: 50 |  Loss: (0.1725) |  Loss2: (0.0000) | Acc: (94.00%) (6144/6528)
Epoch: 108 | Batch_idx: 60 |  Loss: (0.1742) |  Loss2: (0.0000) | Acc: (94.00%) (7346/7808)
Epoch: 108 | Batch_idx: 70 |  Loss: (0.1730) |  Loss2: (0.0000) | Acc: (94.00%) (8560/9088)
Epoch: 108 | Batch_idx: 80 |  Loss: (0.1733) |  Loss2: (0.0000) | Acc: (94.00%) (9766/10368)
Epoch: 108 | Batch_idx: 90 |  Loss: (0.1744) |  Loss2: (0.0000) | Acc: (94.00%) (10971/11648)
Epoch: 108 | Batch_idx: 100 |  Loss: (0.1769) |  Loss2: (0.0000) | Acc: (94.00%) (12164/12928)
Epoch: 108 | Batch_idx: 110 |  Loss: (0.1785) |  Loss2: (0.0000) | Acc: (94.00%) (13359/14208)
Epoch: 108 | Batch_idx: 120 |  Loss: (0.1799) |  Loss2: (0.0000) | Acc: (93.00%) (14552/15488)
Epoch: 108 | Batch_idx: 130 |  Loss: (0.1824) |  Loss2: (0.0000) | Acc: (93.00%) (15739/16768)
Epoch: 108 | Batch_idx: 140 |  Loss: (0.1846) |  Loss2: (0.0000) | Acc: (93.00%) (16927/18048)
Epoch: 108 | Batch_idx: 150 |  Loss: (0.1844) |  Loss2: (0.0000) | Acc: (93.00%) (18124/19328)
Epoch: 108 | Batch_idx: 160 |  Loss: (0.1832) |  Loss2: (0.0000) | Acc: (93.00%) (19331/20608)
Epoch: 108 | Batch_idx: 170 |  Loss: (0.1828) |  Loss2: (0.0000) | Acc: (93.00%) (20534/21888)
Epoch: 108 | Batch_idx: 180 |  Loss: (0.1819) |  Loss2: (0.0000) | Acc: (93.00%) (21742/23168)
Epoch: 108 | Batch_idx: 190 |  Loss: (0.1831) |  Loss2: (0.0000) | Acc: (93.00%) (22932/24448)
Epoch: 108 | Batch_idx: 200 |  Loss: (0.1825) |  Loss2: (0.0000) | Acc: (93.00%) (24143/25728)
Epoch: 108 | Batch_idx: 210 |  Loss: (0.1842) |  Loss2: (0.0000) | Acc: (93.00%) (25320/27008)
Epoch: 108 | Batch_idx: 220 |  Loss: (0.1836) |  Loss2: (0.0000) | Acc: (93.00%) (26524/28288)
Epoch: 108 | Batch_idx: 230 |  Loss: (0.1833) |  Loss2: (0.0000) | Acc: (93.00%) (27724/29568)
Epoch: 108 | Batch_idx: 240 |  Loss: (0.1834) |  Loss2: (0.0000) | Acc: (93.00%) (28927/30848)
Epoch: 108 | Batch_idx: 250 |  Loss: (0.1830) |  Loss2: (0.0000) | Acc: (93.00%) (30129/32128)
Epoch: 108 | Batch_idx: 260 |  Loss: (0.1845) |  Loss2: (0.0000) | Acc: (93.00%) (31317/33408)
Epoch: 108 | Batch_idx: 270 |  Loss: (0.1844) |  Loss2: (0.0000) | Acc: (93.00%) (32519/34688)
Epoch: 108 | Batch_idx: 280 |  Loss: (0.1848) |  Loss2: (0.0000) | Acc: (93.00%) (33713/35968)
Epoch: 108 | Batch_idx: 290 |  Loss: (0.1858) |  Loss2: (0.0000) | Acc: (93.00%) (34898/37248)
Epoch: 108 | Batch_idx: 300 |  Loss: (0.1857) |  Loss2: (0.0000) | Acc: (93.00%) (36102/38528)
Epoch: 108 | Batch_idx: 310 |  Loss: (0.1867) |  Loss2: (0.0000) | Acc: (93.00%) (37283/39808)
Epoch: 108 | Batch_idx: 320 |  Loss: (0.1866) |  Loss2: (0.0000) | Acc: (93.00%) (38484/41088)
Epoch: 108 | Batch_idx: 330 |  Loss: (0.1859) |  Loss2: (0.0000) | Acc: (93.00%) (39694/42368)
Epoch: 108 | Batch_idx: 340 |  Loss: (0.1860) |  Loss2: (0.0000) | Acc: (93.00%) (40890/43648)
Epoch: 108 | Batch_idx: 350 |  Loss: (0.1863) |  Loss2: (0.0000) | Acc: (93.00%) (42084/44928)
Epoch: 108 | Batch_idx: 360 |  Loss: (0.1861) |  Loss2: (0.0000) | Acc: (93.00%) (43281/46208)
Epoch: 108 | Batch_idx: 370 |  Loss: (0.1858) |  Loss2: (0.0000) | Acc: (93.00%) (44490/47488)
Epoch: 108 | Batch_idx: 380 |  Loss: (0.1858) |  Loss2: (0.0000) | Acc: (93.00%) (45686/48768)
Epoch: 108 | Batch_idx: 390 |  Loss: (0.1862) |  Loss2: (0.0000) | Acc: (93.00%) (46838/50000)
# TEST : Loss: (0.4306) | Acc: (86.00%) (8639/10000)
percent tensor([0.5610, 0.5783, 0.5591, 0.5546, 0.5647, 0.5493, 0.5778, 0.5714, 0.5737,
        0.5715, 0.5722, 0.5661, 0.5678, 0.5779, 0.5658, 0.5595],
       device='cuda:0') torch.Size([16])
percent tensor([0.5086, 0.5118, 0.4738, 0.4962, 0.5021, 0.4918, 0.5156, 0.5202, 0.5030,
        0.4861, 0.4851, 0.4715, 0.5084, 0.5110, 0.5137, 0.5014],
       device='cuda:0') torch.Size([16])
percent tensor([0.5117, 0.4654, 0.4661, 0.5341, 0.4583, 0.5383, 0.4802, 0.5087, 0.5056,
        0.4550, 0.4882, 0.4591, 0.4578, 0.5548, 0.5120, 0.5150],
       device='cuda:0') torch.Size([16])
percent tensor([0.6700, 0.6847, 0.6355, 0.6216, 0.6266, 0.6791, 0.6638, 0.6162, 0.6560,
        0.6820, 0.6778, 0.6625, 0.7099, 0.6533, 0.6743, 0.6848],
       device='cuda:0') torch.Size([16])
percent tensor([0.6724, 0.5938, 0.6772, 0.6851, 0.7040, 0.7409, 0.6611, 0.6710, 0.7001,
        0.6511, 0.6846, 0.6593, 0.5793, 0.7173, 0.6488, 0.6987],
       device='cuda:0') torch.Size([16])
percent tensor([0.6969, 0.6971, 0.7302, 0.7376, 0.7671, 0.7410, 0.7259, 0.7266, 0.7234,
        0.7094, 0.7132, 0.7027, 0.6847, 0.7457, 0.6870, 0.7268],
       device='cuda:0') torch.Size([16])
percent tensor([0.4508, 0.5692, 0.6098, 0.5435, 0.6585, 0.6834, 0.5429, 0.4552, 0.5761,
        0.5542, 0.5104, 0.4906, 0.4750, 0.5513, 0.3539, 0.5230],
       device='cuda:0') torch.Size([16])
percent tensor([0.9995, 0.9991, 0.9988, 0.9993, 0.9990, 0.9986, 0.9990, 0.9997, 0.9992,
        0.9989, 0.9996, 0.9994, 0.9992, 0.9987, 0.9989, 0.9994],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 109 | Batch_idx: 0 |  Loss: (0.1402) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 109 | Batch_idx: 10 |  Loss: (0.1649) |  Loss2: (0.0000) | Acc: (94.00%) (1325/1408)
Epoch: 109 | Batch_idx: 20 |  Loss: (0.1670) |  Loss2: (0.0000) | Acc: (94.00%) (2527/2688)
Epoch: 109 | Batch_idx: 30 |  Loss: (0.1782) |  Loss2: (0.0000) | Acc: (93.00%) (3724/3968)
Epoch: 109 | Batch_idx: 40 |  Loss: (0.1841) |  Loss2: (0.0000) | Acc: (93.00%) (4915/5248)
Epoch: 109 | Batch_idx: 50 |  Loss: (0.1849) |  Loss2: (0.0000) | Acc: (93.00%) (6108/6528)
Epoch: 109 | Batch_idx: 60 |  Loss: (0.1868) |  Loss2: (0.0000) | Acc: (93.00%) (7303/7808)
Epoch: 109 | Batch_idx: 70 |  Loss: (0.1922) |  Loss2: (0.0000) | Acc: (93.00%) (8480/9088)
Epoch: 109 | Batch_idx: 80 |  Loss: (0.1936) |  Loss2: (0.0000) | Acc: (93.00%) (9670/10368)
Epoch: 109 | Batch_idx: 90 |  Loss: (0.1974) |  Loss2: (0.0000) | Acc: (93.00%) (10849/11648)
Epoch: 109 | Batch_idx: 100 |  Loss: (0.1986) |  Loss2: (0.0000) | Acc: (93.00%) (12035/12928)
Epoch: 109 | Batch_idx: 110 |  Loss: (0.2000) |  Loss2: (0.0000) | Acc: (93.00%) (13223/14208)
Epoch: 109 | Batch_idx: 120 |  Loss: (0.2024) |  Loss2: (0.0000) | Acc: (93.00%) (14407/15488)
Epoch: 109 | Batch_idx: 130 |  Loss: (0.2024) |  Loss2: (0.0000) | Acc: (93.00%) (15603/16768)
Epoch: 109 | Batch_idx: 140 |  Loss: (0.2020) |  Loss2: (0.0000) | Acc: (93.00%) (16801/18048)
Epoch: 109 | Batch_idx: 150 |  Loss: (0.2044) |  Loss2: (0.0000) | Acc: (92.00%) (17974/19328)
Epoch: 109 | Batch_idx: 160 |  Loss: (0.2041) |  Loss2: (0.0000) | Acc: (93.00%) (19173/20608)
Epoch: 109 | Batch_idx: 170 |  Loss: (0.2035) |  Loss2: (0.0000) | Acc: (93.00%) (20367/21888)
Epoch: 109 | Batch_idx: 180 |  Loss: (0.2038) |  Loss2: (0.0000) | Acc: (92.00%) (21546/23168)
Epoch: 109 | Batch_idx: 190 |  Loss: (0.2029) |  Loss2: (0.0000) | Acc: (93.00%) (22745/24448)
Epoch: 109 | Batch_idx: 200 |  Loss: (0.2034) |  Loss2: (0.0000) | Acc: (92.00%) (23924/25728)
Epoch: 109 | Batch_idx: 210 |  Loss: (0.2031) |  Loss2: (0.0000) | Acc: (92.00%) (25103/27008)
Epoch: 109 | Batch_idx: 220 |  Loss: (0.2019) |  Loss2: (0.0000) | Acc: (92.00%) (26299/28288)
Epoch: 109 | Batch_idx: 230 |  Loss: (0.2026) |  Loss2: (0.0000) | Acc: (92.00%) (27482/29568)
Epoch: 109 | Batch_idx: 240 |  Loss: (0.2022) |  Loss2: (0.0000) | Acc: (92.00%) (28666/30848)
Epoch: 109 | Batch_idx: 250 |  Loss: (0.2016) |  Loss2: (0.0000) | Acc: (92.00%) (29862/32128)
Epoch: 109 | Batch_idx: 260 |  Loss: (0.2006) |  Loss2: (0.0000) | Acc: (92.00%) (31059/33408)
Epoch: 109 | Batch_idx: 270 |  Loss: (0.2003) |  Loss2: (0.0000) | Acc: (92.00%) (32247/34688)
Epoch: 109 | Batch_idx: 280 |  Loss: (0.2002) |  Loss2: (0.0000) | Acc: (92.00%) (33440/35968)
Epoch: 109 | Batch_idx: 290 |  Loss: (0.2009) |  Loss2: (0.0000) | Acc: (92.00%) (34622/37248)
Epoch: 109 | Batch_idx: 300 |  Loss: (0.2000) |  Loss2: (0.0000) | Acc: (92.00%) (35827/38528)
Epoch: 109 | Batch_idx: 310 |  Loss: (0.2005) |  Loss2: (0.0000) | Acc: (92.00%) (37021/39808)
Epoch: 109 | Batch_idx: 320 |  Loss: (0.2009) |  Loss2: (0.0000) | Acc: (92.00%) (38209/41088)
Epoch: 109 | Batch_idx: 330 |  Loss: (0.2014) |  Loss2: (0.0000) | Acc: (92.00%) (39390/42368)
Epoch: 109 | Batch_idx: 340 |  Loss: (0.2014) |  Loss2: (0.0000) | Acc: (92.00%) (40581/43648)
Epoch: 109 | Batch_idx: 350 |  Loss: (0.2012) |  Loss2: (0.0000) | Acc: (92.00%) (41776/44928)
Epoch: 109 | Batch_idx: 360 |  Loss: (0.2012) |  Loss2: (0.0000) | Acc: (92.00%) (42968/46208)
Epoch: 109 | Batch_idx: 370 |  Loss: (0.2010) |  Loss2: (0.0000) | Acc: (93.00%) (44164/47488)
Epoch: 109 | Batch_idx: 380 |  Loss: (0.2008) |  Loss2: (0.0000) | Acc: (92.00%) (45354/48768)
Epoch: 109 | Batch_idx: 390 |  Loss: (0.2011) |  Loss2: (0.0000) | Acc: (92.00%) (46495/50000)
# TEST : Loss: (0.4034) | Acc: (86.00%) (8680/10000)
percent tensor([0.5672, 0.5864, 0.5660, 0.5610, 0.5728, 0.5556, 0.5861, 0.5792, 0.5816,
        0.5791, 0.5797, 0.5738, 0.5746, 0.5860, 0.5731, 0.5662],
       device='cuda:0') torch.Size([16])
percent tensor([0.5172, 0.5212, 0.4804, 0.5024, 0.5082, 0.5012, 0.5238, 0.5297, 0.5112,
        0.4951, 0.4957, 0.4782, 0.5169, 0.5197, 0.5243, 0.5094],
       device='cuda:0') torch.Size([16])
percent tensor([0.5060, 0.4669, 0.4583, 0.5219, 0.4519, 0.5323, 0.4765, 0.5010, 0.5009,
        0.4547, 0.4886, 0.4534, 0.4589, 0.5472, 0.5082, 0.5115],
       device='cuda:0') torch.Size([16])
percent tensor([0.6732, 0.6903, 0.6419, 0.6286, 0.6338, 0.6834, 0.6706, 0.6228, 0.6590,
        0.6870, 0.6819, 0.6677, 0.7111, 0.6598, 0.6811, 0.6878],
       device='cuda:0') torch.Size([16])
percent tensor([0.6667, 0.5864, 0.6866, 0.6860, 0.7136, 0.7354, 0.6564, 0.6767, 0.6941,
        0.6464, 0.6770, 0.6546, 0.5564, 0.7146, 0.6386, 0.6941],
       device='cuda:0') torch.Size([16])
percent tensor([0.7204, 0.7181, 0.7505, 0.7542, 0.7889, 0.7674, 0.7468, 0.7539, 0.7400,
        0.7316, 0.7318, 0.7257, 0.7065, 0.7606, 0.7155, 0.7525],
       device='cuda:0') torch.Size([16])
percent tensor([0.4600, 0.5833, 0.6157, 0.5563, 0.6696, 0.7100, 0.5522, 0.4734, 0.5716,
        0.5620, 0.5028, 0.5103, 0.4890, 0.5328, 0.3741, 0.5571],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9992, 0.9987, 0.9993, 0.9987, 0.9990, 0.9991, 0.9996, 0.9995,
        0.9993, 0.9996, 0.9996, 0.9993, 0.9992, 0.9987, 0.9994],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 110 | Batch_idx: 0 |  Loss: (0.2536) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 110 | Batch_idx: 10 |  Loss: (0.1804) |  Loss2: (0.0000) | Acc: (93.00%) (1322/1408)
Epoch: 110 | Batch_idx: 20 |  Loss: (0.1830) |  Loss2: (0.0000) | Acc: (94.00%) (2527/2688)
Epoch: 110 | Batch_idx: 30 |  Loss: (0.1794) |  Loss2: (0.0000) | Acc: (94.00%) (3733/3968)
Epoch: 110 | Batch_idx: 40 |  Loss: (0.1745) |  Loss2: (0.0000) | Acc: (94.00%) (4948/5248)
Epoch: 110 | Batch_idx: 50 |  Loss: (0.1765) |  Loss2: (0.0000) | Acc: (94.00%) (6138/6528)
Epoch: 110 | Batch_idx: 60 |  Loss: (0.1726) |  Loss2: (0.0000) | Acc: (94.00%) (7353/7808)
Epoch: 110 | Batch_idx: 70 |  Loss: (0.1735) |  Loss2: (0.0000) | Acc: (94.00%) (8548/9088)
Epoch: 110 | Batch_idx: 80 |  Loss: (0.1736) |  Loss2: (0.0000) | Acc: (94.00%) (9754/10368)
Epoch: 110 | Batch_idx: 90 |  Loss: (0.1726) |  Loss2: (0.0000) | Acc: (94.00%) (10958/11648)
Epoch: 110 | Batch_idx: 100 |  Loss: (0.1725) |  Loss2: (0.0000) | Acc: (94.00%) (12161/12928)
Epoch: 110 | Batch_idx: 110 |  Loss: (0.1739) |  Loss2: (0.0000) | Acc: (94.00%) (13356/14208)
Epoch: 110 | Batch_idx: 120 |  Loss: (0.1746) |  Loss2: (0.0000) | Acc: (93.00%) (14558/15488)
Epoch: 110 | Batch_idx: 130 |  Loss: (0.1746) |  Loss2: (0.0000) | Acc: (93.00%) (15759/16768)
Epoch: 110 | Batch_idx: 140 |  Loss: (0.1760) |  Loss2: (0.0000) | Acc: (93.00%) (16942/18048)
Epoch: 110 | Batch_idx: 150 |  Loss: (0.1778) |  Loss2: (0.0000) | Acc: (93.00%) (18127/19328)
Epoch: 110 | Batch_idx: 160 |  Loss: (0.1763) |  Loss2: (0.0000) | Acc: (93.00%) (19344/20608)
Epoch: 110 | Batch_idx: 170 |  Loss: (0.1774) |  Loss2: (0.0000) | Acc: (93.00%) (20544/21888)
Epoch: 110 | Batch_idx: 180 |  Loss: (0.1774) |  Loss2: (0.0000) | Acc: (93.00%) (21748/23168)
Epoch: 110 | Batch_idx: 190 |  Loss: (0.1767) |  Loss2: (0.0000) | Acc: (93.00%) (22958/24448)
Epoch: 110 | Batch_idx: 200 |  Loss: (0.1770) |  Loss2: (0.0000) | Acc: (93.00%) (24163/25728)
Epoch: 110 | Batch_idx: 210 |  Loss: (0.1775) |  Loss2: (0.0000) | Acc: (93.00%) (25357/27008)
Epoch: 110 | Batch_idx: 220 |  Loss: (0.1774) |  Loss2: (0.0000) | Acc: (93.00%) (26557/28288)
Epoch: 110 | Batch_idx: 230 |  Loss: (0.1785) |  Loss2: (0.0000) | Acc: (93.00%) (27748/29568)
Epoch: 110 | Batch_idx: 240 |  Loss: (0.1786) |  Loss2: (0.0000) | Acc: (93.00%) (28951/30848)
Epoch: 110 | Batch_idx: 250 |  Loss: (0.1796) |  Loss2: (0.0000) | Acc: (93.00%) (30148/32128)
Epoch: 110 | Batch_idx: 260 |  Loss: (0.1807) |  Loss2: (0.0000) | Acc: (93.00%) (31338/33408)
Epoch: 110 | Batch_idx: 270 |  Loss: (0.1810) |  Loss2: (0.0000) | Acc: (93.00%) (32537/34688)
Epoch: 110 | Batch_idx: 280 |  Loss: (0.1814) |  Loss2: (0.0000) | Acc: (93.00%) (33723/35968)
Epoch: 110 | Batch_idx: 290 |  Loss: (0.1810) |  Loss2: (0.0000) | Acc: (93.00%) (34926/37248)
Epoch: 110 | Batch_idx: 300 |  Loss: (0.1815) |  Loss2: (0.0000) | Acc: (93.00%) (36118/38528)
Epoch: 110 | Batch_idx: 310 |  Loss: (0.1816) |  Loss2: (0.0000) | Acc: (93.00%) (37321/39808)
Epoch: 110 | Batch_idx: 320 |  Loss: (0.1817) |  Loss2: (0.0000) | Acc: (93.00%) (38520/41088)
Epoch: 110 | Batch_idx: 330 |  Loss: (0.1810) |  Loss2: (0.0000) | Acc: (93.00%) (39730/42368)
Epoch: 110 | Batch_idx: 340 |  Loss: (0.1812) |  Loss2: (0.0000) | Acc: (93.00%) (40939/43648)
Epoch: 110 | Batch_idx: 350 |  Loss: (0.1819) |  Loss2: (0.0000) | Acc: (93.00%) (42135/44928)
Epoch: 110 | Batch_idx: 360 |  Loss: (0.1813) |  Loss2: (0.0000) | Acc: (93.00%) (43346/46208)
Epoch: 110 | Batch_idx: 370 |  Loss: (0.1819) |  Loss2: (0.0000) | Acc: (93.00%) (44534/47488)
Epoch: 110 | Batch_idx: 380 |  Loss: (0.1820) |  Loss2: (0.0000) | Acc: (93.00%) (45735/48768)
Epoch: 110 | Batch_idx: 390 |  Loss: (0.1827) |  Loss2: (0.0000) | Acc: (93.00%) (46876/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_110.pth.tar'
# TEST : Loss: (0.4786) | Acc: (85.00%) (8547/10000)
percent tensor([0.5683, 0.5843, 0.5719, 0.5613, 0.5776, 0.5553, 0.5867, 0.5799, 0.5826,
        0.5796, 0.5792, 0.5781, 0.5752, 0.5824, 0.5719, 0.5654],
       device='cuda:0') torch.Size([16])
percent tensor([0.5156, 0.5210, 0.4804, 0.5052, 0.5104, 0.5016, 0.5225, 0.5300, 0.5110,
        0.4934, 0.4930, 0.4779, 0.5140, 0.5211, 0.5236, 0.5091],
       device='cuda:0') torch.Size([16])
percent tensor([0.5057, 0.4762, 0.4510, 0.5258, 0.4385, 0.5352, 0.4819, 0.5016, 0.4968,
        0.4548, 0.4909, 0.4498, 0.4595, 0.5566, 0.5119, 0.5138],
       device='cuda:0') torch.Size([16])
percent tensor([0.6720, 0.6899, 0.6305, 0.6221, 0.6267, 0.6757, 0.6660, 0.6186, 0.6592,
        0.6828, 0.6809, 0.6639, 0.7096, 0.6572, 0.6791, 0.6840],
       device='cuda:0') torch.Size([16])
percent tensor([0.6754, 0.5845, 0.6560, 0.6866, 0.6941, 0.7399, 0.6519, 0.6836, 0.7144,
        0.6346, 0.6881, 0.6332, 0.5681, 0.7028, 0.6387, 0.6946],
       device='cuda:0') torch.Size([16])
percent tensor([0.7182, 0.7126, 0.7404, 0.7550, 0.7794, 0.7637, 0.7443, 0.7509, 0.7426,
        0.7212, 0.7376, 0.7212, 0.7050, 0.7563, 0.7074, 0.7474],
       device='cuda:0') torch.Size([16])
percent tensor([0.4758, 0.5478, 0.6046, 0.5362, 0.6697, 0.6846, 0.5580, 0.4552, 0.5733,
        0.5079, 0.5142, 0.5133, 0.4886, 0.5339, 0.3663, 0.5392],
       device='cuda:0') torch.Size([16])
percent tensor([0.9995, 0.9995, 0.9992, 0.9994, 0.9993, 0.9988, 0.9991, 0.9997, 0.9996,
        0.9996, 0.9997, 0.9997, 0.9996, 0.9993, 0.9988, 0.9996],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(178.4659, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(814.2661, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(802.0301, device='cuda:0')
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1511.8464, device='cuda:0')
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(493.3139, device='cuda:0')
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2242.5193, device='cuda:0')
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4296.3770, device='cuda:0')
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1393.9868, device='cuda:0')
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6191.2197, device='cuda:0')
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11824.7256, device='cuda:0')
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3919.9277, device='cuda:0')
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16554.5645, device='cuda:0')
Epoch: 111 | Batch_idx: 0 |  Loss: (0.2493) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 111 | Batch_idx: 10 |  Loss: (0.2050) |  Loss2: (0.0000) | Acc: (91.00%) (1291/1408)
Epoch: 111 | Batch_idx: 20 |  Loss: (0.2096) |  Loss2: (0.0000) | Acc: (92.00%) (2475/2688)
Epoch: 111 | Batch_idx: 30 |  Loss: (0.2086) |  Loss2: (0.0000) | Acc: (92.00%) (3665/3968)
Epoch: 111 | Batch_idx: 40 |  Loss: (0.2114) |  Loss2: (0.0000) | Acc: (92.00%) (4845/5248)
Epoch: 111 | Batch_idx: 50 |  Loss: (0.2154) |  Loss2: (0.0000) | Acc: (92.00%) (6022/6528)
Epoch: 111 | Batch_idx: 60 |  Loss: (0.2177) |  Loss2: (0.0000) | Acc: (92.00%) (7201/7808)
Epoch: 111 | Batch_idx: 70 |  Loss: (0.2198) |  Loss2: (0.0000) | Acc: (92.00%) (8381/9088)
Epoch: 111 | Batch_idx: 80 |  Loss: (0.2179) |  Loss2: (0.0000) | Acc: (92.00%) (9570/10368)
Epoch: 111 | Batch_idx: 90 |  Loss: (0.2180) |  Loss2: (0.0000) | Acc: (92.00%) (10747/11648)
Epoch: 111 | Batch_idx: 100 |  Loss: (0.2226) |  Loss2: (0.0000) | Acc: (92.00%) (11900/12928)
Epoch: 111 | Batch_idx: 110 |  Loss: (0.2224) |  Loss2: (0.0000) | Acc: (92.00%) (13081/14208)
Epoch: 111 | Batch_idx: 120 |  Loss: (0.2236) |  Loss2: (0.0000) | Acc: (92.00%) (14255/15488)
Epoch: 111 | Batch_idx: 130 |  Loss: (0.2223) |  Loss2: (0.0000) | Acc: (92.00%) (15438/16768)
Epoch: 111 | Batch_idx: 140 |  Loss: (0.2216) |  Loss2: (0.0000) | Acc: (92.00%) (16612/18048)
Epoch: 111 | Batch_idx: 150 |  Loss: (0.2218) |  Loss2: (0.0000) | Acc: (92.00%) (17788/19328)
Epoch: 111 | Batch_idx: 160 |  Loss: (0.2223) |  Loss2: (0.0000) | Acc: (92.00%) (18966/20608)
Epoch: 111 | Batch_idx: 170 |  Loss: (0.2223) |  Loss2: (0.0000) | Acc: (92.00%) (20157/21888)
Epoch: 111 | Batch_idx: 180 |  Loss: (0.2206) |  Loss2: (0.0000) | Acc: (92.00%) (21345/23168)
Epoch: 111 | Batch_idx: 190 |  Loss: (0.2207) |  Loss2: (0.0000) | Acc: (92.00%) (22529/24448)
Epoch: 111 | Batch_idx: 200 |  Loss: (0.2213) |  Loss2: (0.0000) | Acc: (92.00%) (23703/25728)
Epoch: 111 | Batch_idx: 210 |  Loss: (0.2204) |  Loss2: (0.0000) | Acc: (92.00%) (24902/27008)
Epoch: 111 | Batch_idx: 220 |  Loss: (0.2199) |  Loss2: (0.0000) | Acc: (92.00%) (26090/28288)
Epoch: 111 | Batch_idx: 230 |  Loss: (0.2211) |  Loss2: (0.0000) | Acc: (92.00%) (27256/29568)
Epoch: 111 | Batch_idx: 240 |  Loss: (0.2213) |  Loss2: (0.0000) | Acc: (92.00%) (28431/30848)
Epoch: 111 | Batch_idx: 250 |  Loss: (0.2196) |  Loss2: (0.0000) | Acc: (92.00%) (29631/32128)
Epoch: 111 | Batch_idx: 260 |  Loss: (0.2182) |  Loss2: (0.0000) | Acc: (92.00%) (30827/33408)
Epoch: 111 | Batch_idx: 270 |  Loss: (0.2182) |  Loss2: (0.0000) | Acc: (92.00%) (31997/34688)
Epoch: 111 | Batch_idx: 280 |  Loss: (0.2168) |  Loss2: (0.0000) | Acc: (92.00%) (33197/35968)
Epoch: 111 | Batch_idx: 290 |  Loss: (0.2168) |  Loss2: (0.0000) | Acc: (92.00%) (34381/37248)
Epoch: 111 | Batch_idx: 300 |  Loss: (0.2164) |  Loss2: (0.0000) | Acc: (92.00%) (35576/38528)
Epoch: 111 | Batch_idx: 310 |  Loss: (0.2164) |  Loss2: (0.0000) | Acc: (92.00%) (36760/39808)
Epoch: 111 | Batch_idx: 320 |  Loss: (0.2159) |  Loss2: (0.0000) | Acc: (92.00%) (37950/41088)
Epoch: 111 | Batch_idx: 330 |  Loss: (0.2157) |  Loss2: (0.0000) | Acc: (92.00%) (39138/42368)
Epoch: 111 | Batch_idx: 340 |  Loss: (0.2164) |  Loss2: (0.0000) | Acc: (92.00%) (40311/43648)
Epoch: 111 | Batch_idx: 350 |  Loss: (0.2155) |  Loss2: (0.0000) | Acc: (92.00%) (41508/44928)
Epoch: 111 | Batch_idx: 360 |  Loss: (0.2149) |  Loss2: (0.0000) | Acc: (92.00%) (42703/46208)
Epoch: 111 | Batch_idx: 370 |  Loss: (0.2145) |  Loss2: (0.0000) | Acc: (92.00%) (43905/47488)
Epoch: 111 | Batch_idx: 380 |  Loss: (0.2143) |  Loss2: (0.0000) | Acc: (92.00%) (45094/48768)
Epoch: 111 | Batch_idx: 390 |  Loss: (0.2139) |  Loss2: (0.0000) | Acc: (92.00%) (46251/50000)
# TEST : Loss: (0.4143) | Acc: (86.00%) (8673/10000)
percent tensor([0.5722, 0.5877, 0.5747, 0.5650, 0.5816, 0.5593, 0.5900, 0.5833, 0.5861,
        0.5830, 0.5826, 0.5810, 0.5791, 0.5855, 0.5754, 0.5693],
       device='cuda:0') torch.Size([16])
percent tensor([0.5091, 0.5156, 0.4689, 0.4958, 0.4966, 0.4923, 0.5143, 0.5181, 0.5037,
        0.4876, 0.4883, 0.4689, 0.5087, 0.5214, 0.5151, 0.5031],
       device='cuda:0') torch.Size([16])
percent tensor([0.5039, 0.4741, 0.4526, 0.5207, 0.4350, 0.5310, 0.4828, 0.4958, 0.4964,
        0.4560, 0.4933, 0.4551, 0.4631, 0.5548, 0.5075, 0.5100],
       device='cuda:0') torch.Size([16])
percent tensor([0.6617, 0.6816, 0.6230, 0.6196, 0.6168, 0.6629, 0.6546, 0.6100, 0.6515,
        0.6775, 0.6740, 0.6535, 0.7004, 0.6526, 0.6666, 0.6759],
       device='cuda:0') torch.Size([16])
percent tensor([0.6804, 0.6075, 0.6734, 0.7005, 0.7074, 0.7433, 0.6657, 0.6959, 0.7288,
        0.6525, 0.7092, 0.6501, 0.5842, 0.7269, 0.6471, 0.7044],
       device='cuda:0') torch.Size([16])
percent tensor([0.6772, 0.6691, 0.7075, 0.7229, 0.7502, 0.7318, 0.7026, 0.7110, 0.7025,
        0.6732, 0.6924, 0.6810, 0.6562, 0.7106, 0.6622, 0.7052],
       device='cuda:0') torch.Size([16])
percent tensor([0.4883, 0.5399, 0.6448, 0.5939, 0.7161, 0.6948, 0.5860, 0.5210, 0.5590,
        0.4984, 0.4916, 0.5268, 0.4615, 0.5156, 0.3761, 0.5421],
       device='cuda:0') torch.Size([16])
percent tensor([0.9995, 0.9994, 0.9992, 0.9994, 0.9994, 0.9987, 0.9988, 0.9997, 0.9997,
        0.9996, 0.9996, 0.9998, 0.9995, 0.9991, 0.9989, 0.9996],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 112 | Batch_idx: 0 |  Loss: (0.2244) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 112 | Batch_idx: 10 |  Loss: (0.1716) |  Loss2: (0.0000) | Acc: (93.00%) (1319/1408)
Epoch: 112 | Batch_idx: 20 |  Loss: (0.1701) |  Loss2: (0.0000) | Acc: (93.00%) (2521/2688)
Epoch: 112 | Batch_idx: 30 |  Loss: (0.1634) |  Loss2: (0.0000) | Acc: (94.00%) (3735/3968)
Epoch: 112 | Batch_idx: 40 |  Loss: (0.1603) |  Loss2: (0.0000) | Acc: (94.00%) (4951/5248)
Epoch: 112 | Batch_idx: 50 |  Loss: (0.1611) |  Loss2: (0.0000) | Acc: (94.00%) (6164/6528)
Epoch: 112 | Batch_idx: 60 |  Loss: (0.1599) |  Loss2: (0.0000) | Acc: (94.00%) (7382/7808)
Epoch: 112 | Batch_idx: 70 |  Loss: (0.1615) |  Loss2: (0.0000) | Acc: (94.00%) (8582/9088)
Epoch: 112 | Batch_idx: 80 |  Loss: (0.1660) |  Loss2: (0.0000) | Acc: (94.00%) (9781/10368)
Epoch: 112 | Batch_idx: 90 |  Loss: (0.1662) |  Loss2: (0.0000) | Acc: (94.00%) (10984/11648)
Epoch: 112 | Batch_idx: 100 |  Loss: (0.1690) |  Loss2: (0.0000) | Acc: (94.00%) (12173/12928)
Epoch: 112 | Batch_idx: 110 |  Loss: (0.1690) |  Loss2: (0.0000) | Acc: (94.00%) (13377/14208)
Epoch: 112 | Batch_idx: 120 |  Loss: (0.1688) |  Loss2: (0.0000) | Acc: (94.00%) (14584/15488)
Epoch: 112 | Batch_idx: 130 |  Loss: (0.1692) |  Loss2: (0.0000) | Acc: (94.00%) (15784/16768)
Epoch: 112 | Batch_idx: 140 |  Loss: (0.1706) |  Loss2: (0.0000) | Acc: (94.00%) (16986/18048)
Epoch: 112 | Batch_idx: 150 |  Loss: (0.1706) |  Loss2: (0.0000) | Acc: (94.00%) (18190/19328)
Epoch: 112 | Batch_idx: 160 |  Loss: (0.1710) |  Loss2: (0.0000) | Acc: (94.00%) (19385/20608)
Epoch: 112 | Batch_idx: 170 |  Loss: (0.1714) |  Loss2: (0.0000) | Acc: (94.00%) (20582/21888)
Epoch: 112 | Batch_idx: 180 |  Loss: (0.1729) |  Loss2: (0.0000) | Acc: (93.00%) (21772/23168)
Epoch: 112 | Batch_idx: 190 |  Loss: (0.1729) |  Loss2: (0.0000) | Acc: (93.00%) (22978/24448)
Epoch: 112 | Batch_idx: 200 |  Loss: (0.1730) |  Loss2: (0.0000) | Acc: (93.00%) (24178/25728)
Epoch: 112 | Batch_idx: 210 |  Loss: (0.1718) |  Loss2: (0.0000) | Acc: (94.00%) (25391/27008)
Epoch: 112 | Batch_idx: 220 |  Loss: (0.1722) |  Loss2: (0.0000) | Acc: (94.00%) (26594/28288)
Epoch: 112 | Batch_idx: 230 |  Loss: (0.1728) |  Loss2: (0.0000) | Acc: (93.00%) (27780/29568)
Epoch: 112 | Batch_idx: 240 |  Loss: (0.1736) |  Loss2: (0.0000) | Acc: (93.00%) (28979/30848)
Epoch: 112 | Batch_idx: 250 |  Loss: (0.1752) |  Loss2: (0.0000) | Acc: (93.00%) (30164/32128)
Epoch: 112 | Batch_idx: 260 |  Loss: (0.1752) |  Loss2: (0.0000) | Acc: (93.00%) (31361/33408)
Epoch: 112 | Batch_idx: 270 |  Loss: (0.1750) |  Loss2: (0.0000) | Acc: (93.00%) (32561/34688)
Epoch: 112 | Batch_idx: 280 |  Loss: (0.1746) |  Loss2: (0.0000) | Acc: (93.00%) (33772/35968)
Epoch: 112 | Batch_idx: 290 |  Loss: (0.1748) |  Loss2: (0.0000) | Acc: (93.00%) (34972/37248)
Epoch: 112 | Batch_idx: 300 |  Loss: (0.1751) |  Loss2: (0.0000) | Acc: (93.00%) (36184/38528)
Epoch: 112 | Batch_idx: 310 |  Loss: (0.1753) |  Loss2: (0.0000) | Acc: (93.00%) (37380/39808)
Epoch: 112 | Batch_idx: 320 |  Loss: (0.1759) |  Loss2: (0.0000) | Acc: (93.00%) (38566/41088)
Epoch: 112 | Batch_idx: 330 |  Loss: (0.1767) |  Loss2: (0.0000) | Acc: (93.00%) (39755/42368)
Epoch: 112 | Batch_idx: 340 |  Loss: (0.1766) |  Loss2: (0.0000) | Acc: (93.00%) (40958/43648)
Epoch: 112 | Batch_idx: 350 |  Loss: (0.1771) |  Loss2: (0.0000) | Acc: (93.00%) (42148/44928)
Epoch: 112 | Batch_idx: 360 |  Loss: (0.1776) |  Loss2: (0.0000) | Acc: (93.00%) (43340/46208)
Epoch: 112 | Batch_idx: 370 |  Loss: (0.1780) |  Loss2: (0.0000) | Acc: (93.00%) (44528/47488)
Epoch: 112 | Batch_idx: 380 |  Loss: (0.1779) |  Loss2: (0.0000) | Acc: (93.00%) (45729/48768)
Epoch: 112 | Batch_idx: 390 |  Loss: (0.1784) |  Loss2: (0.0000) | Acc: (93.00%) (46878/50000)
# TEST : Loss: (0.4086) | Acc: (87.00%) (8714/10000)
percent tensor([0.5714, 0.5862, 0.5744, 0.5648, 0.5796, 0.5598, 0.5885, 0.5836, 0.5847,
        0.5824, 0.5818, 0.5804, 0.5780, 0.5836, 0.5752, 0.5690],
       device='cuda:0') torch.Size([16])
percent tensor([0.5140, 0.5173, 0.4766, 0.5004, 0.4999, 0.4922, 0.5161, 0.5226, 0.5054,
        0.4923, 0.4899, 0.4741, 0.5134, 0.5200, 0.5162, 0.5055],
       device='cuda:0') torch.Size([16])
percent tensor([0.5076, 0.4731, 0.4632, 0.5238, 0.4403, 0.5271, 0.4783, 0.4989, 0.4995,
        0.4612, 0.4948, 0.4589, 0.4630, 0.5536, 0.5015, 0.5107],
       device='cuda:0') torch.Size([16])
percent tensor([0.6574, 0.6733, 0.6275, 0.6180, 0.6199, 0.6628, 0.6536, 0.6075, 0.6463,
        0.6713, 0.6658, 0.6531, 0.6981, 0.6454, 0.6608, 0.6710],
       device='cuda:0') torch.Size([16])
percent tensor([0.6878, 0.6016, 0.7067, 0.7015, 0.7252, 0.7548, 0.6624, 0.6903, 0.7345,
        0.6566, 0.7069, 0.6739, 0.5967, 0.7127, 0.6535, 0.7045],
       device='cuda:0') torch.Size([16])
percent tensor([0.6827, 0.6751, 0.7175, 0.7135, 0.7529, 0.7398, 0.7033, 0.7097, 0.7048,
        0.6875, 0.6963, 0.6852, 0.6612, 0.7064, 0.6700, 0.7102],
       device='cuda:0') torch.Size([16])
percent tensor([0.4856, 0.5750, 0.6220, 0.5577, 0.6907, 0.7256, 0.5874, 0.4925, 0.5852,
        0.5505, 0.5133, 0.5039, 0.4892, 0.5390, 0.3868, 0.5642],
       device='cuda:0') torch.Size([16])
percent tensor([0.9994, 0.9992, 0.9985, 0.9993, 0.9993, 0.9989, 0.9993, 0.9996, 0.9995,
        0.9993, 0.9996, 0.9995, 0.9991, 0.9987, 0.9982, 0.9995],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 113 | Batch_idx: 0 |  Loss: (0.1494) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 113 | Batch_idx: 10 |  Loss: (0.1716) |  Loss2: (0.0000) | Acc: (93.00%) (1317/1408)
Epoch: 113 | Batch_idx: 20 |  Loss: (0.1761) |  Loss2: (0.0000) | Acc: (93.00%) (2522/2688)
Epoch: 113 | Batch_idx: 30 |  Loss: (0.1778) |  Loss2: (0.0000) | Acc: (93.00%) (3722/3968)
Epoch: 113 | Batch_idx: 40 |  Loss: (0.1948) |  Loss2: (0.0000) | Acc: (93.00%) (4898/5248)
Epoch: 113 | Batch_idx: 50 |  Loss: (0.1996) |  Loss2: (0.0000) | Acc: (93.00%) (6075/6528)
Epoch: 113 | Batch_idx: 60 |  Loss: (0.2066) |  Loss2: (0.0000) | Acc: (92.00%) (7253/7808)
Epoch: 113 | Batch_idx: 70 |  Loss: (0.2077) |  Loss2: (0.0000) | Acc: (92.00%) (8438/9088)
Epoch: 113 | Batch_idx: 80 |  Loss: (0.2071) |  Loss2: (0.0000) | Acc: (92.00%) (9629/10368)
Epoch: 113 | Batch_idx: 90 |  Loss: (0.2048) |  Loss2: (0.0000) | Acc: (92.00%) (10826/11648)
Epoch: 113 | Batch_idx: 100 |  Loss: (0.2036) |  Loss2: (0.0000) | Acc: (93.00%) (12025/12928)
Epoch: 113 | Batch_idx: 110 |  Loss: (0.2036) |  Loss2: (0.0000) | Acc: (93.00%) (13219/14208)
Epoch: 113 | Batch_idx: 120 |  Loss: (0.2045) |  Loss2: (0.0000) | Acc: (93.00%) (14409/15488)
Epoch: 113 | Batch_idx: 130 |  Loss: (0.2073) |  Loss2: (0.0000) | Acc: (92.00%) (15584/16768)
Epoch: 113 | Batch_idx: 140 |  Loss: (0.2079) |  Loss2: (0.0000) | Acc: (92.00%) (16768/18048)
Epoch: 113 | Batch_idx: 150 |  Loss: (0.2075) |  Loss2: (0.0000) | Acc: (92.00%) (17950/19328)
Epoch: 113 | Batch_idx: 160 |  Loss: (0.2079) |  Loss2: (0.0000) | Acc: (92.00%) (19133/20608)
Epoch: 113 | Batch_idx: 170 |  Loss: (0.2073) |  Loss2: (0.0000) | Acc: (92.00%) (20320/21888)
Epoch: 113 | Batch_idx: 180 |  Loss: (0.2069) |  Loss2: (0.0000) | Acc: (92.00%) (21514/23168)
Epoch: 113 | Batch_idx: 190 |  Loss: (0.2066) |  Loss2: (0.0000) | Acc: (92.00%) (22705/24448)
Epoch: 113 | Batch_idx: 200 |  Loss: (0.2063) |  Loss2: (0.0000) | Acc: (92.00%) (23899/25728)
Epoch: 113 | Batch_idx: 210 |  Loss: (0.2071) |  Loss2: (0.0000) | Acc: (92.00%) (25076/27008)
Epoch: 113 | Batch_idx: 220 |  Loss: (0.2061) |  Loss2: (0.0000) | Acc: (92.00%) (26276/28288)
Epoch: 113 | Batch_idx: 230 |  Loss: (0.2057) |  Loss2: (0.0000) | Acc: (92.00%) (27464/29568)
Epoch: 113 | Batch_idx: 240 |  Loss: (0.2053) |  Loss2: (0.0000) | Acc: (92.00%) (28655/30848)
Epoch: 113 | Batch_idx: 250 |  Loss: (0.2054) |  Loss2: (0.0000) | Acc: (92.00%) (29848/32128)
Epoch: 113 | Batch_idx: 260 |  Loss: (0.2056) |  Loss2: (0.0000) | Acc: (92.00%) (31034/33408)
Epoch: 113 | Batch_idx: 270 |  Loss: (0.2058) |  Loss2: (0.0000) | Acc: (92.00%) (32224/34688)
Epoch: 113 | Batch_idx: 280 |  Loss: (0.2055) |  Loss2: (0.0000) | Acc: (92.00%) (33404/35968)
Epoch: 113 | Batch_idx: 290 |  Loss: (0.2053) |  Loss2: (0.0000) | Acc: (92.00%) (34595/37248)
Epoch: 113 | Batch_idx: 300 |  Loss: (0.2054) |  Loss2: (0.0000) | Acc: (92.00%) (35785/38528)
Epoch: 113 | Batch_idx: 310 |  Loss: (0.2057) |  Loss2: (0.0000) | Acc: (92.00%) (36971/39808)
Epoch: 113 | Batch_idx: 320 |  Loss: (0.2061) |  Loss2: (0.0000) | Acc: (92.00%) (38153/41088)
Epoch: 113 | Batch_idx: 330 |  Loss: (0.2064) |  Loss2: (0.0000) | Acc: (92.00%) (39330/42368)
Epoch: 113 | Batch_idx: 340 |  Loss: (0.2053) |  Loss2: (0.0000) | Acc: (92.00%) (40538/43648)
Epoch: 113 | Batch_idx: 350 |  Loss: (0.2048) |  Loss2: (0.0000) | Acc: (92.00%) (41735/44928)
Epoch: 113 | Batch_idx: 360 |  Loss: (0.2049) |  Loss2: (0.0000) | Acc: (92.00%) (42931/46208)
Epoch: 113 | Batch_idx: 370 |  Loss: (0.2045) |  Loss2: (0.0000) | Acc: (92.00%) (44118/47488)
Epoch: 113 | Batch_idx: 380 |  Loss: (0.2041) |  Loss2: (0.0000) | Acc: (92.00%) (45310/48768)
Epoch: 113 | Batch_idx: 390 |  Loss: (0.2032) |  Loss2: (0.0000) | Acc: (92.00%) (46466/50000)
# TEST : Loss: (0.4075) | Acc: (87.00%) (8711/10000)
percent tensor([0.5687, 0.5819, 0.5722, 0.5621, 0.5759, 0.5571, 0.5843, 0.5805, 0.5816,
        0.5788, 0.5785, 0.5768, 0.5750, 0.5802, 0.5711, 0.5660],
       device='cuda:0') torch.Size([16])
percent tensor([0.5249, 0.5320, 0.4866, 0.5138, 0.5128, 0.5065, 0.5291, 0.5363, 0.5175,
        0.5048, 0.5031, 0.4853, 0.5242, 0.5340, 0.5313, 0.5182],
       device='cuda:0') torch.Size([16])
percent tensor([0.5086, 0.4788, 0.4663, 0.5326, 0.4565, 0.5360, 0.4867, 0.5070, 0.5018,
        0.4630, 0.4956, 0.4613, 0.4602, 0.5561, 0.5154, 0.5171],
       device='cuda:0') torch.Size([16])
percent tensor([0.6720, 0.6843, 0.6419, 0.6349, 0.6351, 0.6790, 0.6667, 0.6231, 0.6578,
        0.6840, 0.6782, 0.6649, 0.7102, 0.6547, 0.6761, 0.6875],
       device='cuda:0') torch.Size([16])
percent tensor([0.6718, 0.5771, 0.7021, 0.6940, 0.7338, 0.7467, 0.6531, 0.6881, 0.7151,
        0.6223, 0.6843, 0.6469, 0.5514, 0.7039, 0.6412, 0.6807],
       device='cuda:0') torch.Size([16])
percent tensor([0.6888, 0.6825, 0.7219, 0.7202, 0.7600, 0.7411, 0.7134, 0.7116, 0.7078,
        0.6926, 0.6999, 0.6926, 0.6723, 0.7192, 0.6727, 0.7174],
       device='cuda:0') torch.Size([16])
percent tensor([0.5156, 0.5872, 0.6276, 0.5527, 0.6726, 0.7167, 0.5977, 0.4959, 0.5749,
        0.5552, 0.4903, 0.4928, 0.4988, 0.5399, 0.3727, 0.6073],
       device='cuda:0') torch.Size([16])
percent tensor([0.9995, 0.9993, 0.9986, 0.9992, 0.9995, 0.9995, 0.9993, 0.9995, 0.9995,
        0.9995, 0.9995, 0.9994, 0.9992, 0.9992, 0.9986, 0.9997],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 114 | Batch_idx: 0 |  Loss: (0.1388) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 114 | Batch_idx: 10 |  Loss: (0.1471) |  Loss2: (0.0000) | Acc: (95.00%) (1338/1408)
Epoch: 114 | Batch_idx: 20 |  Loss: (0.1496) |  Loss2: (0.0000) | Acc: (94.00%) (2550/2688)
Epoch: 114 | Batch_idx: 30 |  Loss: (0.1586) |  Loss2: (0.0000) | Acc: (94.00%) (3750/3968)
Epoch: 114 | Batch_idx: 40 |  Loss: (0.1632) |  Loss2: (0.0000) | Acc: (94.00%) (4950/5248)
Epoch: 114 | Batch_idx: 50 |  Loss: (0.1642) |  Loss2: (0.0000) | Acc: (94.00%) (6154/6528)
Epoch: 114 | Batch_idx: 60 |  Loss: (0.1674) |  Loss2: (0.0000) | Acc: (94.00%) (7348/7808)
Epoch: 114 | Batch_idx: 70 |  Loss: (0.1678) |  Loss2: (0.0000) | Acc: (94.00%) (8550/9088)
Epoch: 114 | Batch_idx: 80 |  Loss: (0.1676) |  Loss2: (0.0000) | Acc: (94.00%) (9756/10368)
Epoch: 114 | Batch_idx: 90 |  Loss: (0.1671) |  Loss2: (0.0000) | Acc: (94.00%) (10960/11648)
Epoch: 114 | Batch_idx: 100 |  Loss: (0.1678) |  Loss2: (0.0000) | Acc: (94.00%) (12162/12928)
Epoch: 114 | Batch_idx: 110 |  Loss: (0.1674) |  Loss2: (0.0000) | Acc: (94.00%) (13368/14208)
Epoch: 114 | Batch_idx: 120 |  Loss: (0.1669) |  Loss2: (0.0000) | Acc: (94.00%) (14584/15488)
Epoch: 114 | Batch_idx: 130 |  Loss: (0.1662) |  Loss2: (0.0000) | Acc: (94.00%) (15792/16768)
Epoch: 114 | Batch_idx: 140 |  Loss: (0.1666) |  Loss2: (0.0000) | Acc: (94.00%) (16992/18048)
Epoch: 114 | Batch_idx: 150 |  Loss: (0.1680) |  Loss2: (0.0000) | Acc: (94.00%) (18186/19328)
Epoch: 114 | Batch_idx: 160 |  Loss: (0.1679) |  Loss2: (0.0000) | Acc: (94.00%) (19393/20608)
Epoch: 114 | Batch_idx: 170 |  Loss: (0.1685) |  Loss2: (0.0000) | Acc: (94.00%) (20586/21888)
Epoch: 114 | Batch_idx: 180 |  Loss: (0.1675) |  Loss2: (0.0000) | Acc: (94.00%) (21807/23168)
Epoch: 114 | Batch_idx: 190 |  Loss: (0.1688) |  Loss2: (0.0000) | Acc: (94.00%) (23001/24448)
Epoch: 114 | Batch_idx: 200 |  Loss: (0.1677) |  Loss2: (0.0000) | Acc: (94.00%) (24216/25728)
Epoch: 114 | Batch_idx: 210 |  Loss: (0.1680) |  Loss2: (0.0000) | Acc: (94.00%) (25426/27008)
Epoch: 114 | Batch_idx: 220 |  Loss: (0.1670) |  Loss2: (0.0000) | Acc: (94.00%) (26642/28288)
Epoch: 114 | Batch_idx: 230 |  Loss: (0.1678) |  Loss2: (0.0000) | Acc: (94.00%) (27841/29568)
Epoch: 114 | Batch_idx: 240 |  Loss: (0.1680) |  Loss2: (0.0000) | Acc: (94.00%) (29041/30848)
Epoch: 114 | Batch_idx: 250 |  Loss: (0.1688) |  Loss2: (0.0000) | Acc: (94.00%) (30243/32128)
Epoch: 114 | Batch_idx: 260 |  Loss: (0.1694) |  Loss2: (0.0000) | Acc: (94.00%) (31444/33408)
Epoch: 114 | Batch_idx: 270 |  Loss: (0.1689) |  Loss2: (0.0000) | Acc: (94.00%) (32651/34688)
Epoch: 114 | Batch_idx: 280 |  Loss: (0.1692) |  Loss2: (0.0000) | Acc: (94.00%) (33857/35968)
Epoch: 114 | Batch_idx: 290 |  Loss: (0.1697) |  Loss2: (0.0000) | Acc: (94.00%) (35046/37248)
Epoch: 114 | Batch_idx: 300 |  Loss: (0.1702) |  Loss2: (0.0000) | Acc: (94.00%) (36237/38528)
Epoch: 114 | Batch_idx: 310 |  Loss: (0.1703) |  Loss2: (0.0000) | Acc: (94.00%) (37442/39808)
Epoch: 114 | Batch_idx: 320 |  Loss: (0.1711) |  Loss2: (0.0000) | Acc: (94.00%) (38641/41088)
Epoch: 114 | Batch_idx: 330 |  Loss: (0.1714) |  Loss2: (0.0000) | Acc: (94.00%) (39843/42368)
Epoch: 114 | Batch_idx: 340 |  Loss: (0.1715) |  Loss2: (0.0000) | Acc: (94.00%) (41040/43648)
Epoch: 114 | Batch_idx: 350 |  Loss: (0.1722) |  Loss2: (0.0000) | Acc: (94.00%) (42236/44928)
Epoch: 114 | Batch_idx: 360 |  Loss: (0.1722) |  Loss2: (0.0000) | Acc: (93.00%) (43432/46208)
Epoch: 114 | Batch_idx: 370 |  Loss: (0.1725) |  Loss2: (0.0000) | Acc: (93.00%) (44633/47488)
Epoch: 114 | Batch_idx: 380 |  Loss: (0.1728) |  Loss2: (0.0000) | Acc: (93.00%) (45839/48768)
Epoch: 114 | Batch_idx: 390 |  Loss: (0.1728) |  Loss2: (0.0000) | Acc: (93.00%) (46994/50000)
# TEST : Loss: (0.4176) | Acc: (86.00%) (8698/10000)
percent tensor([0.5684, 0.5809, 0.5714, 0.5621, 0.5772, 0.5560, 0.5843, 0.5792, 0.5806,
        0.5781, 0.5771, 0.5773, 0.5743, 0.5785, 0.5706, 0.5653],
       device='cuda:0') torch.Size([16])
percent tensor([0.5232, 0.5258, 0.4938, 0.5143, 0.5189, 0.5085, 0.5288, 0.5386, 0.5164,
        0.5018, 0.4988, 0.4901, 0.5201, 0.5270, 0.5289, 0.5167],
       device='cuda:0') torch.Size([16])
percent tensor([0.5112, 0.4771, 0.4697, 0.5337, 0.4592, 0.5361, 0.4836, 0.5152, 0.4995,
        0.4642, 0.4909, 0.4644, 0.4595, 0.5522, 0.5178, 0.5164],
       device='cuda:0') torch.Size([16])
percent tensor([0.6740, 0.6875, 0.6417, 0.6308, 0.6299, 0.6836, 0.6683, 0.6219, 0.6636,
        0.6856, 0.6823, 0.6704, 0.7121, 0.6609, 0.6773, 0.6881],
       device='cuda:0') torch.Size([16])
percent tensor([0.6653, 0.5786, 0.6651, 0.6796, 0.7000, 0.7498, 0.6468, 0.6737, 0.7116,
        0.6344, 0.6998, 0.6392, 0.5627, 0.7180, 0.6352, 0.6913],
       device='cuda:0') torch.Size([16])
percent tensor([0.6897, 0.6869, 0.7179, 0.7222, 0.7561, 0.7545, 0.7101, 0.7116, 0.7090,
        0.6945, 0.7101, 0.6826, 0.6751, 0.7234, 0.6730, 0.7243],
       device='cuda:0') torch.Size([16])
percent tensor([0.5123, 0.5838, 0.6325, 0.5479, 0.6952, 0.7495, 0.5825, 0.4757, 0.5975,
        0.5795, 0.5541, 0.5015, 0.4878, 0.5702, 0.3820, 0.6090],
       device='cuda:0') torch.Size([16])
percent tensor([0.9994, 0.9993, 0.9994, 0.9995, 0.9995, 0.9984, 0.9993, 0.9996, 0.9997,
        0.9995, 0.9997, 0.9996, 0.9995, 0.9989, 0.9987, 0.9997],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 115 | Batch_idx: 0 |  Loss: (0.1745) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 115 | Batch_idx: 10 |  Loss: (0.1973) |  Loss2: (0.0000) | Acc: (92.00%) (1305/1408)
Epoch: 115 | Batch_idx: 20 |  Loss: (0.1975) |  Loss2: (0.0000) | Acc: (92.00%) (2491/2688)
Epoch: 115 | Batch_idx: 30 |  Loss: (0.2044) |  Loss2: (0.0000) | Acc: (92.00%) (3668/3968)
Epoch: 115 | Batch_idx: 40 |  Loss: (0.2082) |  Loss2: (0.0000) | Acc: (92.00%) (4846/5248)
Epoch: 115 | Batch_idx: 50 |  Loss: (0.2082) |  Loss2: (0.0000) | Acc: (92.00%) (6042/6528)
Epoch: 115 | Batch_idx: 60 |  Loss: (0.2094) |  Loss2: (0.0000) | Acc: (92.00%) (7227/7808)
Epoch: 115 | Batch_idx: 70 |  Loss: (0.2093) |  Loss2: (0.0000) | Acc: (92.00%) (8420/9088)
Epoch: 115 | Batch_idx: 80 |  Loss: (0.2067) |  Loss2: (0.0000) | Acc: (92.00%) (9620/10368)
Epoch: 115 | Batch_idx: 90 |  Loss: (0.2044) |  Loss2: (0.0000) | Acc: (92.00%) (10822/11648)
Epoch: 115 | Batch_idx: 100 |  Loss: (0.2038) |  Loss2: (0.0000) | Acc: (92.00%) (12008/12928)
Epoch: 115 | Batch_idx: 110 |  Loss: (0.2040) |  Loss2: (0.0000) | Acc: (92.00%) (13181/14208)
Epoch: 115 | Batch_idx: 120 |  Loss: (0.2019) |  Loss2: (0.0000) | Acc: (92.00%) (14372/15488)
Epoch: 115 | Batch_idx: 130 |  Loss: (0.2032) |  Loss2: (0.0000) | Acc: (92.00%) (15551/16768)
Epoch: 115 | Batch_idx: 140 |  Loss: (0.2038) |  Loss2: (0.0000) | Acc: (92.00%) (16731/18048)
Epoch: 115 | Batch_idx: 150 |  Loss: (0.2032) |  Loss2: (0.0000) | Acc: (92.00%) (17929/19328)
Epoch: 115 | Batch_idx: 160 |  Loss: (0.2000) |  Loss2: (0.0000) | Acc: (92.00%) (19148/20608)
Epoch: 115 | Batch_idx: 170 |  Loss: (0.2005) |  Loss2: (0.0000) | Acc: (92.00%) (20342/21888)
Epoch: 115 | Batch_idx: 180 |  Loss: (0.2004) |  Loss2: (0.0000) | Acc: (92.00%) (21529/23168)
Epoch: 115 | Batch_idx: 190 |  Loss: (0.1997) |  Loss2: (0.0000) | Acc: (92.00%) (22728/24448)
Epoch: 115 | Batch_idx: 200 |  Loss: (0.1993) |  Loss2: (0.0000) | Acc: (92.00%) (23922/25728)
Epoch: 115 | Batch_idx: 210 |  Loss: (0.2002) |  Loss2: (0.0000) | Acc: (92.00%) (25098/27008)
Epoch: 115 | Batch_idx: 220 |  Loss: (0.2003) |  Loss2: (0.0000) | Acc: (92.00%) (26288/28288)
Epoch: 115 | Batch_idx: 230 |  Loss: (0.2000) |  Loss2: (0.0000) | Acc: (92.00%) (27484/29568)
Epoch: 115 | Batch_idx: 240 |  Loss: (0.1998) |  Loss2: (0.0000) | Acc: (92.00%) (28678/30848)
Epoch: 115 | Batch_idx: 250 |  Loss: (0.1992) |  Loss2: (0.0000) | Acc: (92.00%) (29877/32128)
Epoch: 115 | Batch_idx: 260 |  Loss: (0.1999) |  Loss2: (0.0000) | Acc: (92.00%) (31054/33408)
Epoch: 115 | Batch_idx: 270 |  Loss: (0.2002) |  Loss2: (0.0000) | Acc: (92.00%) (32238/34688)
Epoch: 115 | Batch_idx: 280 |  Loss: (0.2004) |  Loss2: (0.0000) | Acc: (92.00%) (33429/35968)
Epoch: 115 | Batch_idx: 290 |  Loss: (0.1999) |  Loss2: (0.0000) | Acc: (92.00%) (34623/37248)
Epoch: 115 | Batch_idx: 300 |  Loss: (0.1994) |  Loss2: (0.0000) | Acc: (92.00%) (35807/38528)
Epoch: 115 | Batch_idx: 310 |  Loss: (0.1987) |  Loss2: (0.0000) | Acc: (92.00%) (37001/39808)
Epoch: 115 | Batch_idx: 320 |  Loss: (0.1989) |  Loss2: (0.0000) | Acc: (92.00%) (38187/41088)
Epoch: 115 | Batch_idx: 330 |  Loss: (0.1986) |  Loss2: (0.0000) | Acc: (92.00%) (39394/42368)
Epoch: 115 | Batch_idx: 340 |  Loss: (0.1992) |  Loss2: (0.0000) | Acc: (92.00%) (40565/43648)
Epoch: 115 | Batch_idx: 350 |  Loss: (0.1987) |  Loss2: (0.0000) | Acc: (92.00%) (41767/44928)
Epoch: 115 | Batch_idx: 360 |  Loss: (0.1988) |  Loss2: (0.0000) | Acc: (92.00%) (42959/46208)
Epoch: 115 | Batch_idx: 370 |  Loss: (0.1985) |  Loss2: (0.0000) | Acc: (92.00%) (44156/47488)
Epoch: 115 | Batch_idx: 380 |  Loss: (0.1973) |  Loss2: (0.0000) | Acc: (93.00%) (45364/48768)
Epoch: 115 | Batch_idx: 390 |  Loss: (0.1967) |  Loss2: (0.0000) | Acc: (93.00%) (46526/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_115.pth.tar'
# TEST : Loss: (0.4023) | Acc: (86.00%) (8699/10000)
percent tensor([0.5626, 0.5737, 0.5633, 0.5567, 0.5705, 0.5512, 0.5769, 0.5729, 0.5738,
        0.5710, 0.5705, 0.5700, 0.5679, 0.5718, 0.5645, 0.5597],
       device='cuda:0') torch.Size([16])
percent tensor([0.5210, 0.5211, 0.4931, 0.5129, 0.5163, 0.5068, 0.5242, 0.5389, 0.5121,
        0.4970, 0.4945, 0.4849, 0.5162, 0.5237, 0.5259, 0.5142],
       device='cuda:0') torch.Size([16])
percent tensor([0.4939, 0.4618, 0.4549, 0.5235, 0.4355, 0.5204, 0.4633, 0.4996, 0.4842,
        0.4501, 0.4750, 0.4488, 0.4460, 0.5417, 0.4986, 0.4984],
       device='cuda:0') torch.Size([16])
percent tensor([0.6508, 0.6593, 0.6187, 0.6132, 0.6087, 0.6709, 0.6402, 0.5994, 0.6388,
        0.6585, 0.6539, 0.6419, 0.6844, 0.6354, 0.6534, 0.6682],
       device='cuda:0') torch.Size([16])
percent tensor([0.6959, 0.5892, 0.7004, 0.6985, 0.7139, 0.7857, 0.6601, 0.6796, 0.7370,
        0.6562, 0.7233, 0.6650, 0.5907, 0.7394, 0.6414, 0.7256],
       device='cuda:0') torch.Size([16])
percent tensor([0.7172, 0.7065, 0.7404, 0.7448, 0.7740, 0.7794, 0.7330, 0.7393, 0.7361,
        0.7197, 0.7372, 0.7122, 0.6966, 0.7545, 0.7013, 0.7499],
       device='cuda:0') torch.Size([16])
percent tensor([0.5161, 0.5996, 0.6340, 0.5437, 0.6786, 0.7493, 0.5896, 0.4566, 0.6258,
        0.5953, 0.5834, 0.5457, 0.5133, 0.5780, 0.3957, 0.6045],
       device='cuda:0') torch.Size([16])
percent tensor([0.9995, 0.9994, 0.9994, 0.9995, 0.9994, 0.9988, 0.9994, 0.9996, 0.9996,
        0.9994, 0.9998, 0.9996, 0.9995, 0.9993, 0.9985, 0.9997],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 116 | Batch_idx: 0 |  Loss: (0.1128) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 116 | Batch_idx: 10 |  Loss: (0.1647) |  Loss2: (0.0000) | Acc: (93.00%) (1320/1408)
Epoch: 116 | Batch_idx: 20 |  Loss: (0.1569) |  Loss2: (0.0000) | Acc: (94.00%) (2541/2688)
Epoch: 116 | Batch_idx: 30 |  Loss: (0.1601) |  Loss2: (0.0000) | Acc: (94.00%) (3753/3968)
Epoch: 116 | Batch_idx: 40 |  Loss: (0.1612) |  Loss2: (0.0000) | Acc: (94.00%) (4959/5248)
Epoch: 116 | Batch_idx: 50 |  Loss: (0.1603) |  Loss2: (0.0000) | Acc: (94.00%) (6172/6528)
Epoch: 116 | Batch_idx: 60 |  Loss: (0.1633) |  Loss2: (0.0000) | Acc: (94.00%) (7371/7808)
Epoch: 116 | Batch_idx: 70 |  Loss: (0.1645) |  Loss2: (0.0000) | Acc: (94.00%) (8571/9088)
Epoch: 116 | Batch_idx: 80 |  Loss: (0.1633) |  Loss2: (0.0000) | Acc: (94.00%) (9782/10368)
Epoch: 116 | Batch_idx: 90 |  Loss: (0.1634) |  Loss2: (0.0000) | Acc: (94.00%) (10991/11648)
Epoch: 116 | Batch_idx: 100 |  Loss: (0.1654) |  Loss2: (0.0000) | Acc: (94.00%) (12196/12928)
Epoch: 116 | Batch_idx: 110 |  Loss: (0.1669) |  Loss2: (0.0000) | Acc: (94.00%) (13394/14208)
Epoch: 116 | Batch_idx: 120 |  Loss: (0.1666) |  Loss2: (0.0000) | Acc: (94.00%) (14597/15488)
Epoch: 116 | Batch_idx: 130 |  Loss: (0.1669) |  Loss2: (0.0000) | Acc: (94.00%) (15810/16768)
Epoch: 116 | Batch_idx: 140 |  Loss: (0.1667) |  Loss2: (0.0000) | Acc: (94.00%) (17011/18048)
Epoch: 116 | Batch_idx: 150 |  Loss: (0.1669) |  Loss2: (0.0000) | Acc: (94.00%) (18211/19328)
Epoch: 116 | Batch_idx: 160 |  Loss: (0.1665) |  Loss2: (0.0000) | Acc: (94.00%) (19429/20608)
Epoch: 116 | Batch_idx: 170 |  Loss: (0.1662) |  Loss2: (0.0000) | Acc: (94.00%) (20639/21888)
Epoch: 116 | Batch_idx: 180 |  Loss: (0.1680) |  Loss2: (0.0000) | Acc: (94.00%) (21837/23168)
Epoch: 116 | Batch_idx: 190 |  Loss: (0.1683) |  Loss2: (0.0000) | Acc: (94.00%) (23047/24448)
Epoch: 116 | Batch_idx: 200 |  Loss: (0.1688) |  Loss2: (0.0000) | Acc: (94.00%) (24259/25728)
Epoch: 116 | Batch_idx: 210 |  Loss: (0.1686) |  Loss2: (0.0000) | Acc: (94.00%) (25471/27008)
Epoch: 116 | Batch_idx: 220 |  Loss: (0.1703) |  Loss2: (0.0000) | Acc: (94.00%) (26653/28288)
Epoch: 116 | Batch_idx: 230 |  Loss: (0.1703) |  Loss2: (0.0000) | Acc: (94.00%) (27859/29568)
Epoch: 116 | Batch_idx: 240 |  Loss: (0.1695) |  Loss2: (0.0000) | Acc: (94.00%) (29072/30848)
Epoch: 116 | Batch_idx: 250 |  Loss: (0.1695) |  Loss2: (0.0000) | Acc: (94.00%) (30273/32128)
Epoch: 116 | Batch_idx: 260 |  Loss: (0.1695) |  Loss2: (0.0000) | Acc: (94.00%) (31481/33408)
Epoch: 116 | Batch_idx: 270 |  Loss: (0.1699) |  Loss2: (0.0000) | Acc: (94.00%) (32684/34688)
Epoch: 116 | Batch_idx: 280 |  Loss: (0.1704) |  Loss2: (0.0000) | Acc: (94.00%) (33884/35968)
Epoch: 116 | Batch_idx: 290 |  Loss: (0.1721) |  Loss2: (0.0000) | Acc: (94.00%) (35070/37248)
Epoch: 116 | Batch_idx: 300 |  Loss: (0.1716) |  Loss2: (0.0000) | Acc: (94.00%) (36282/38528)
Epoch: 116 | Batch_idx: 310 |  Loss: (0.1723) |  Loss2: (0.0000) | Acc: (94.00%) (37475/39808)
Epoch: 116 | Batch_idx: 320 |  Loss: (0.1722) |  Loss2: (0.0000) | Acc: (94.00%) (38681/41088)
Epoch: 116 | Batch_idx: 330 |  Loss: (0.1725) |  Loss2: (0.0000) | Acc: (94.00%) (39876/42368)
Epoch: 116 | Batch_idx: 340 |  Loss: (0.1721) |  Loss2: (0.0000) | Acc: (94.00%) (41083/43648)
Epoch: 116 | Batch_idx: 350 |  Loss: (0.1725) |  Loss2: (0.0000) | Acc: (94.00%) (42279/44928)
Epoch: 116 | Batch_idx: 360 |  Loss: (0.1726) |  Loss2: (0.0000) | Acc: (94.00%) (43480/46208)
Epoch: 116 | Batch_idx: 370 |  Loss: (0.1729) |  Loss2: (0.0000) | Acc: (94.00%) (44681/47488)
Epoch: 116 | Batch_idx: 380 |  Loss: (0.1725) |  Loss2: (0.0000) | Acc: (94.00%) (45890/48768)
Epoch: 116 | Batch_idx: 390 |  Loss: (0.1726) |  Loss2: (0.0000) | Acc: (94.00%) (47050/50000)
# TEST : Loss: (0.3962) | Acc: (87.00%) (8763/10000)
percent tensor([0.5622, 0.5750, 0.5635, 0.5551, 0.5677, 0.5505, 0.5768, 0.5727, 0.5750,
        0.5713, 0.5718, 0.5691, 0.5681, 0.5743, 0.5645, 0.5599],
       device='cuda:0') torch.Size([16])
percent tensor([0.5145, 0.5201, 0.4784, 0.5091, 0.5059, 0.4982, 0.5182, 0.5346, 0.5052,
        0.4923, 0.4888, 0.4727, 0.5120, 0.5237, 0.5236, 0.5102],
       device='cuda:0') torch.Size([16])
percent tensor([0.4914, 0.4648, 0.4504, 0.5251, 0.4313, 0.5259, 0.4632, 0.4984, 0.4818,
        0.4487, 0.4761, 0.4411, 0.4442, 0.5495, 0.5002, 0.5020],
       device='cuda:0') torch.Size([16])
percent tensor([0.6478, 0.6652, 0.6256, 0.6143, 0.6134, 0.6594, 0.6450, 0.6042, 0.6406,
        0.6598, 0.6532, 0.6466, 0.6855, 0.6383, 0.6524, 0.6631],
       device='cuda:0') torch.Size([16])
percent tensor([0.6792, 0.5935, 0.7033, 0.6909, 0.7266, 0.7556, 0.6633, 0.6944, 0.7342,
        0.6566, 0.7049, 0.6640, 0.5772, 0.7408, 0.6337, 0.7036],
       device='cuda:0') torch.Size([16])
percent tensor([0.7081, 0.7082, 0.7397, 0.7446, 0.7739, 0.7597, 0.7361, 0.7363, 0.7382,
        0.7253, 0.7322, 0.7200, 0.6989, 0.7540, 0.6983, 0.7423],
       device='cuda:0') torch.Size([16])
percent tensor([0.5065, 0.5860, 0.6163, 0.5650, 0.6629, 0.7117, 0.5919, 0.4554, 0.6176,
        0.5900, 0.5554, 0.5234, 0.5185, 0.5604, 0.3676, 0.5826],
       device='cuda:0') torch.Size([16])
percent tensor([0.9995, 0.9994, 0.9992, 0.9995, 0.9996, 0.9986, 0.9994, 0.9995, 0.9997,
        0.9995, 0.9997, 0.9996, 0.9994, 0.9990, 0.9988, 0.9996],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 117 | Batch_idx: 0 |  Loss: (0.1606) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 117 | Batch_idx: 10 |  Loss: (0.1584) |  Loss2: (0.0000) | Acc: (95.00%) (1338/1408)
Epoch: 117 | Batch_idx: 20 |  Loss: (0.1837) |  Loss2: (0.0000) | Acc: (93.00%) (2524/2688)
Epoch: 117 | Batch_idx: 30 |  Loss: (0.2053) |  Loss2: (0.0000) | Acc: (93.00%) (3694/3968)
Epoch: 117 | Batch_idx: 40 |  Loss: (0.2095) |  Loss2: (0.0000) | Acc: (92.00%) (4873/5248)
Epoch: 117 | Batch_idx: 50 |  Loss: (0.2061) |  Loss2: (0.0000) | Acc: (92.00%) (6065/6528)
Epoch: 117 | Batch_idx: 60 |  Loss: (0.2078) |  Loss2: (0.0000) | Acc: (92.00%) (7256/7808)
Epoch: 117 | Batch_idx: 70 |  Loss: (0.2074) |  Loss2: (0.0000) | Acc: (92.00%) (8444/9088)
Epoch: 117 | Batch_idx: 80 |  Loss: (0.2077) |  Loss2: (0.0000) | Acc: (92.00%) (9629/10368)
Epoch: 117 | Batch_idx: 90 |  Loss: (0.2037) |  Loss2: (0.0000) | Acc: (93.00%) (10839/11648)
Epoch: 117 | Batch_idx: 100 |  Loss: (0.2034) |  Loss2: (0.0000) | Acc: (92.00%) (12016/12928)
Epoch: 117 | Batch_idx: 110 |  Loss: (0.2040) |  Loss2: (0.0000) | Acc: (92.00%) (13204/14208)
Epoch: 117 | Batch_idx: 120 |  Loss: (0.2035) |  Loss2: (0.0000) | Acc: (92.00%) (14393/15488)
Epoch: 117 | Batch_idx: 130 |  Loss: (0.2035) |  Loss2: (0.0000) | Acc: (92.00%) (15582/16768)
Epoch: 117 | Batch_idx: 140 |  Loss: (0.2027) |  Loss2: (0.0000) | Acc: (92.00%) (16777/18048)
Epoch: 117 | Batch_idx: 150 |  Loss: (0.2009) |  Loss2: (0.0000) | Acc: (93.00%) (17981/19328)
Epoch: 117 | Batch_idx: 160 |  Loss: (0.2005) |  Loss2: (0.0000) | Acc: (93.00%) (19176/20608)
Epoch: 117 | Batch_idx: 170 |  Loss: (0.2002) |  Loss2: (0.0000) | Acc: (93.00%) (20364/21888)
Epoch: 117 | Batch_idx: 180 |  Loss: (0.2002) |  Loss2: (0.0000) | Acc: (93.00%) (21563/23168)
Epoch: 117 | Batch_idx: 190 |  Loss: (0.2007) |  Loss2: (0.0000) | Acc: (93.00%) (22751/24448)
Epoch: 117 | Batch_idx: 200 |  Loss: (0.2002) |  Loss2: (0.0000) | Acc: (93.00%) (23951/25728)
Epoch: 117 | Batch_idx: 210 |  Loss: (0.1993) |  Loss2: (0.0000) | Acc: (93.00%) (25163/27008)
Epoch: 117 | Batch_idx: 220 |  Loss: (0.1997) |  Loss2: (0.0000) | Acc: (93.00%) (26347/28288)
Epoch: 117 | Batch_idx: 230 |  Loss: (0.1985) |  Loss2: (0.0000) | Acc: (93.00%) (27551/29568)
Epoch: 117 | Batch_idx: 240 |  Loss: (0.1980) |  Loss2: (0.0000) | Acc: (93.00%) (28749/30848)
Epoch: 117 | Batch_idx: 250 |  Loss: (0.1986) |  Loss2: (0.0000) | Acc: (93.00%) (29937/32128)
Epoch: 117 | Batch_idx: 260 |  Loss: (0.1973) |  Loss2: (0.0000) | Acc: (93.00%) (31142/33408)
Epoch: 117 | Batch_idx: 270 |  Loss: (0.1963) |  Loss2: (0.0000) | Acc: (93.00%) (32351/34688)
Epoch: 117 | Batch_idx: 280 |  Loss: (0.1969) |  Loss2: (0.0000) | Acc: (93.00%) (33534/35968)
Epoch: 117 | Batch_idx: 290 |  Loss: (0.1964) |  Loss2: (0.0000) | Acc: (93.00%) (34732/37248)
Epoch: 117 | Batch_idx: 300 |  Loss: (0.1965) |  Loss2: (0.0000) | Acc: (93.00%) (35930/38528)
Epoch: 117 | Batch_idx: 310 |  Loss: (0.1956) |  Loss2: (0.0000) | Acc: (93.00%) (37138/39808)
Epoch: 117 | Batch_idx: 320 |  Loss: (0.1959) |  Loss2: (0.0000) | Acc: (93.00%) (38335/41088)
Epoch: 117 | Batch_idx: 330 |  Loss: (0.1956) |  Loss2: (0.0000) | Acc: (93.00%) (39536/42368)
Epoch: 117 | Batch_idx: 340 |  Loss: (0.1957) |  Loss2: (0.0000) | Acc: (93.00%) (40737/43648)
Epoch: 117 | Batch_idx: 350 |  Loss: (0.1951) |  Loss2: (0.0000) | Acc: (93.00%) (41947/44928)
Epoch: 117 | Batch_idx: 360 |  Loss: (0.1952) |  Loss2: (0.0000) | Acc: (93.00%) (43138/46208)
Epoch: 117 | Batch_idx: 370 |  Loss: (0.1949) |  Loss2: (0.0000) | Acc: (93.00%) (44334/47488)
Epoch: 117 | Batch_idx: 380 |  Loss: (0.1945) |  Loss2: (0.0000) | Acc: (93.00%) (45528/48768)
Epoch: 117 | Batch_idx: 390 |  Loss: (0.1947) |  Loss2: (0.0000) | Acc: (93.00%) (46677/50000)
# TEST : Loss: (0.3896) | Acc: (87.00%) (8761/10000)
percent tensor([0.5665, 0.5798, 0.5678, 0.5597, 0.5724, 0.5538, 0.5818, 0.5780, 0.5794,
        0.5763, 0.5758, 0.5745, 0.5726, 0.5789, 0.5689, 0.5637],
       device='cuda:0') torch.Size([16])
percent tensor([0.5326, 0.5376, 0.5003, 0.5292, 0.5266, 0.5152, 0.5360, 0.5544, 0.5238,
        0.5115, 0.5065, 0.4934, 0.5298, 0.5364, 0.5399, 0.5280],
       device='cuda:0') torch.Size([16])
percent tensor([0.5023, 0.4698, 0.4579, 0.5301, 0.4453, 0.5340, 0.4758, 0.5061, 0.4919,
        0.4563, 0.4854, 0.4518, 0.4471, 0.5521, 0.5094, 0.5131],
       device='cuda:0') torch.Size([16])
percent tensor([0.6310, 0.6447, 0.6162, 0.6022, 0.6040, 0.6419, 0.6252, 0.5920, 0.6240,
        0.6443, 0.6328, 0.6325, 0.6668, 0.6140, 0.6331, 0.6462],
       device='cuda:0') torch.Size([16])
percent tensor([0.6872, 0.5854, 0.7199, 0.7155, 0.7465, 0.7759, 0.6701, 0.7120, 0.7436,
        0.6593, 0.7139, 0.6822, 0.5814, 0.7452, 0.6348, 0.7116],
       device='cuda:0') torch.Size([16])
percent tensor([0.7184, 0.7145, 0.7517, 0.7531, 0.7818, 0.7635, 0.7475, 0.7461, 0.7442,
        0.7332, 0.7422, 0.7303, 0.7081, 0.7628, 0.7052, 0.7481],
       device='cuda:0') torch.Size([16])
percent tensor([0.4898, 0.5615, 0.5759, 0.5155, 0.6152, 0.6616, 0.5812, 0.4351, 0.5861,
        0.5691, 0.5258, 0.5105, 0.4976, 0.5520, 0.3693, 0.5500],
       device='cuda:0') torch.Size([16])
percent tensor([0.9996, 0.9993, 0.9990, 0.9995, 0.9995, 0.9984, 0.9993, 0.9995, 0.9997,
        0.9995, 0.9997, 0.9996, 0.9994, 0.9989, 0.9988, 0.9995],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 118 | Batch_idx: 0 |  Loss: (0.1735) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 118 | Batch_idx: 10 |  Loss: (0.1712) |  Loss2: (0.0000) | Acc: (94.00%) (1331/1408)
Epoch: 118 | Batch_idx: 20 |  Loss: (0.1675) |  Loss2: (0.0000) | Acc: (94.00%) (2543/2688)
Epoch: 118 | Batch_idx: 30 |  Loss: (0.1705) |  Loss2: (0.0000) | Acc: (94.00%) (3747/3968)
Epoch: 118 | Batch_idx: 40 |  Loss: (0.1672) |  Loss2: (0.0000) | Acc: (94.00%) (4954/5248)
Epoch: 118 | Batch_idx: 50 |  Loss: (0.1635) |  Loss2: (0.0000) | Acc: (94.00%) (6176/6528)
Epoch: 118 | Batch_idx: 60 |  Loss: (0.1636) |  Loss2: (0.0000) | Acc: (94.00%) (7390/7808)
Epoch: 118 | Batch_idx: 70 |  Loss: (0.1634) |  Loss2: (0.0000) | Acc: (94.00%) (8595/9088)
Epoch: 118 | Batch_idx: 80 |  Loss: (0.1618) |  Loss2: (0.0000) | Acc: (94.00%) (9796/10368)
Epoch: 118 | Batch_idx: 90 |  Loss: (0.1627) |  Loss2: (0.0000) | Acc: (94.00%) (11002/11648)
Epoch: 118 | Batch_idx: 100 |  Loss: (0.1620) |  Loss2: (0.0000) | Acc: (94.00%) (12212/12928)
Epoch: 118 | Batch_idx: 110 |  Loss: (0.1630) |  Loss2: (0.0000) | Acc: (94.00%) (13413/14208)
Epoch: 118 | Batch_idx: 120 |  Loss: (0.1613) |  Loss2: (0.0000) | Acc: (94.00%) (14635/15488)
Epoch: 118 | Batch_idx: 130 |  Loss: (0.1615) |  Loss2: (0.0000) | Acc: (94.00%) (15836/16768)
Epoch: 118 | Batch_idx: 140 |  Loss: (0.1614) |  Loss2: (0.0000) | Acc: (94.00%) (17042/18048)
Epoch: 118 | Batch_idx: 150 |  Loss: (0.1617) |  Loss2: (0.0000) | Acc: (94.00%) (18252/19328)
Epoch: 118 | Batch_idx: 160 |  Loss: (0.1614) |  Loss2: (0.0000) | Acc: (94.00%) (19458/20608)
Epoch: 118 | Batch_idx: 170 |  Loss: (0.1615) |  Loss2: (0.0000) | Acc: (94.00%) (20676/21888)
Epoch: 118 | Batch_idx: 180 |  Loss: (0.1622) |  Loss2: (0.0000) | Acc: (94.00%) (21876/23168)
Epoch: 118 | Batch_idx: 190 |  Loss: (0.1624) |  Loss2: (0.0000) | Acc: (94.00%) (23083/24448)
Epoch: 118 | Batch_idx: 200 |  Loss: (0.1623) |  Loss2: (0.0000) | Acc: (94.00%) (24282/25728)
Epoch: 118 | Batch_idx: 210 |  Loss: (0.1635) |  Loss2: (0.0000) | Acc: (94.00%) (25474/27008)
Epoch: 118 | Batch_idx: 220 |  Loss: (0.1633) |  Loss2: (0.0000) | Acc: (94.00%) (26692/28288)
Epoch: 118 | Batch_idx: 230 |  Loss: (0.1636) |  Loss2: (0.0000) | Acc: (94.00%) (27890/29568)
Epoch: 118 | Batch_idx: 240 |  Loss: (0.1645) |  Loss2: (0.0000) | Acc: (94.00%) (29085/30848)
Epoch: 118 | Batch_idx: 250 |  Loss: (0.1637) |  Loss2: (0.0000) | Acc: (94.00%) (30301/32128)
Epoch: 118 | Batch_idx: 260 |  Loss: (0.1648) |  Loss2: (0.0000) | Acc: (94.00%) (31505/33408)
Epoch: 118 | Batch_idx: 270 |  Loss: (0.1647) |  Loss2: (0.0000) | Acc: (94.00%) (32711/34688)
Epoch: 118 | Batch_idx: 280 |  Loss: (0.1650) |  Loss2: (0.0000) | Acc: (94.00%) (33913/35968)
Epoch: 118 | Batch_idx: 290 |  Loss: (0.1662) |  Loss2: (0.0000) | Acc: (94.00%) (35096/37248)
Epoch: 118 | Batch_idx: 300 |  Loss: (0.1667) |  Loss2: (0.0000) | Acc: (94.00%) (36287/38528)
Epoch: 118 | Batch_idx: 310 |  Loss: (0.1676) |  Loss2: (0.0000) | Acc: (94.00%) (37468/39808)
Epoch: 118 | Batch_idx: 320 |  Loss: (0.1682) |  Loss2: (0.0000) | Acc: (94.00%) (38661/41088)
Epoch: 118 | Batch_idx: 330 |  Loss: (0.1684) |  Loss2: (0.0000) | Acc: (94.00%) (39867/42368)
Epoch: 118 | Batch_idx: 340 |  Loss: (0.1689) |  Loss2: (0.0000) | Acc: (94.00%) (41069/43648)
Epoch: 118 | Batch_idx: 350 |  Loss: (0.1694) |  Loss2: (0.0000) | Acc: (94.00%) (42271/44928)
Epoch: 118 | Batch_idx: 360 |  Loss: (0.1695) |  Loss2: (0.0000) | Acc: (94.00%) (43472/46208)
Epoch: 118 | Batch_idx: 370 |  Loss: (0.1698) |  Loss2: (0.0000) | Acc: (94.00%) (44677/47488)
Epoch: 118 | Batch_idx: 380 |  Loss: (0.1702) |  Loss2: (0.0000) | Acc: (94.00%) (45880/48768)
Epoch: 118 | Batch_idx: 390 |  Loss: (0.1708) |  Loss2: (0.0000) | Acc: (94.00%) (47030/50000)
# TEST : Loss: (0.4157) | Acc: (86.00%) (8693/10000)
percent tensor([0.5652, 0.5796, 0.5697, 0.5598, 0.5749, 0.5540, 0.5819, 0.5774, 0.5790,
        0.5755, 0.5753, 0.5737, 0.5716, 0.5786, 0.5683, 0.5633],
       device='cuda:0') torch.Size([16])
percent tensor([0.5353, 0.5381, 0.4992, 0.5257, 0.5273, 0.5168, 0.5380, 0.5511, 0.5300,
        0.5112, 0.5116, 0.4956, 0.5327, 0.5400, 0.5423, 0.5283],
       device='cuda:0') torch.Size([16])
percent tensor([0.5046, 0.4625, 0.4533, 0.5268, 0.4465, 0.5349, 0.4675, 0.5003, 0.4931,
        0.4541, 0.4829, 0.4538, 0.4479, 0.5448, 0.5118, 0.5107],
       device='cuda:0') torch.Size([16])
percent tensor([0.6353, 0.6468, 0.6130, 0.6071, 0.6047, 0.6540, 0.6293, 0.5914, 0.6254,
        0.6439, 0.6358, 0.6296, 0.6677, 0.6228, 0.6356, 0.6519],
       device='cuda:0') torch.Size([16])
percent tensor([0.6798, 0.5927, 0.7019, 0.6947, 0.7306, 0.7656, 0.6684, 0.6941, 0.7294,
        0.6574, 0.7022, 0.6554, 0.5672, 0.7344, 0.6356, 0.7105],
       device='cuda:0') torch.Size([16])
percent tensor([0.7201, 0.7067, 0.7534, 0.7451, 0.7862, 0.7700, 0.7479, 0.7421, 0.7405,
        0.7211, 0.7284, 0.7155, 0.6963, 0.7570, 0.7056, 0.7496],
       device='cuda:0') torch.Size([16])
percent tensor([0.4997, 0.5536, 0.5969, 0.5302, 0.6440, 0.6890, 0.5960, 0.4655, 0.5844,
        0.5234, 0.4987, 0.4956, 0.4842, 0.5474, 0.3796, 0.5746],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9995, 0.9991, 0.9995, 0.9996, 0.9986, 0.9992, 0.9996, 0.9996,
        0.9995, 0.9997, 0.9996, 0.9993, 0.9990, 0.9987, 0.9996],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 119 | Batch_idx: 0 |  Loss: (0.1683) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 119 | Batch_idx: 10 |  Loss: (0.1973) |  Loss2: (0.0000) | Acc: (93.00%) (1311/1408)
Epoch: 119 | Batch_idx: 20 |  Loss: (0.1887) |  Loss2: (0.0000) | Acc: (93.00%) (2501/2688)
Epoch: 119 | Batch_idx: 30 |  Loss: (0.1921) |  Loss2: (0.0000) | Acc: (93.00%) (3697/3968)
Epoch: 119 | Batch_idx: 40 |  Loss: (0.1986) |  Loss2: (0.0000) | Acc: (93.00%) (4888/5248)
Epoch: 119 | Batch_idx: 50 |  Loss: (0.2039) |  Loss2: (0.0000) | Acc: (92.00%) (6071/6528)
Epoch: 119 | Batch_idx: 60 |  Loss: (0.2100) |  Loss2: (0.0000) | Acc: (92.00%) (7244/7808)
Epoch: 119 | Batch_idx: 70 |  Loss: (0.2061) |  Loss2: (0.0000) | Acc: (92.00%) (8441/9088)
Epoch: 119 | Batch_idx: 80 |  Loss: (0.2052) |  Loss2: (0.0000) | Acc: (92.00%) (9635/10368)
Epoch: 119 | Batch_idx: 90 |  Loss: (0.2067) |  Loss2: (0.0000) | Acc: (92.00%) (10822/11648)
Epoch: 119 | Batch_idx: 100 |  Loss: (0.2057) |  Loss2: (0.0000) | Acc: (92.00%) (12021/12928)
Epoch: 119 | Batch_idx: 110 |  Loss: (0.2046) |  Loss2: (0.0000) | Acc: (93.00%) (13217/14208)
Epoch: 119 | Batch_idx: 120 |  Loss: (0.2041) |  Loss2: (0.0000) | Acc: (93.00%) (14407/15488)
Epoch: 119 | Batch_idx: 130 |  Loss: (0.2042) |  Loss2: (0.0000) | Acc: (93.00%) (15599/16768)
Epoch: 119 | Batch_idx: 140 |  Loss: (0.2045) |  Loss2: (0.0000) | Acc: (92.00%) (16784/18048)
Epoch: 119 | Batch_idx: 150 |  Loss: (0.2059) |  Loss2: (0.0000) | Acc: (92.00%) (17964/19328)
Epoch: 119 | Batch_idx: 160 |  Loss: (0.2056) |  Loss2: (0.0000) | Acc: (92.00%) (19150/20608)
Epoch: 119 | Batch_idx: 170 |  Loss: (0.2060) |  Loss2: (0.0000) | Acc: (92.00%) (20339/21888)
Epoch: 119 | Batch_idx: 180 |  Loss: (0.2061) |  Loss2: (0.0000) | Acc: (92.00%) (21528/23168)
Epoch: 119 | Batch_idx: 190 |  Loss: (0.2054) |  Loss2: (0.0000) | Acc: (92.00%) (22729/24448)
Epoch: 119 | Batch_idx: 200 |  Loss: (0.2047) |  Loss2: (0.0000) | Acc: (92.00%) (23925/25728)
Epoch: 119 | Batch_idx: 210 |  Loss: (0.2025) |  Loss2: (0.0000) | Acc: (93.00%) (25131/27008)
Epoch: 119 | Batch_idx: 220 |  Loss: (0.2024) |  Loss2: (0.0000) | Acc: (93.00%) (26330/28288)
Epoch: 119 | Batch_idx: 230 |  Loss: (0.2024) |  Loss2: (0.0000) | Acc: (93.00%) (27512/29568)
Epoch: 119 | Batch_idx: 240 |  Loss: (0.2024) |  Loss2: (0.0000) | Acc: (93.00%) (28710/30848)
Epoch: 119 | Batch_idx: 250 |  Loss: (0.2027) |  Loss2: (0.0000) | Acc: (93.00%) (29898/32128)
Epoch: 119 | Batch_idx: 260 |  Loss: (0.2022) |  Loss2: (0.0000) | Acc: (93.00%) (31089/33408)
Epoch: 119 | Batch_idx: 270 |  Loss: (0.2019) |  Loss2: (0.0000) | Acc: (93.00%) (32284/34688)
Epoch: 119 | Batch_idx: 280 |  Loss: (0.2009) |  Loss2: (0.0000) | Acc: (93.00%) (33484/35968)
Epoch: 119 | Batch_idx: 290 |  Loss: (0.2010) |  Loss2: (0.0000) | Acc: (93.00%) (34667/37248)
Epoch: 119 | Batch_idx: 300 |  Loss: (0.2009) |  Loss2: (0.0000) | Acc: (93.00%) (35863/38528)
Epoch: 119 | Batch_idx: 310 |  Loss: (0.2001) |  Loss2: (0.0000) | Acc: (93.00%) (37074/39808)
Epoch: 119 | Batch_idx: 320 |  Loss: (0.1998) |  Loss2: (0.0000) | Acc: (93.00%) (38270/41088)
Epoch: 119 | Batch_idx: 330 |  Loss: (0.1995) |  Loss2: (0.0000) | Acc: (93.00%) (39474/42368)
Epoch: 119 | Batch_idx: 340 |  Loss: (0.1984) |  Loss2: (0.0000) | Acc: (93.00%) (40677/43648)
Epoch: 119 | Batch_idx: 350 |  Loss: (0.1977) |  Loss2: (0.0000) | Acc: (93.00%) (41882/44928)
Epoch: 119 | Batch_idx: 360 |  Loss: (0.1984) |  Loss2: (0.0000) | Acc: (93.00%) (43065/46208)
Epoch: 119 | Batch_idx: 370 |  Loss: (0.1979) |  Loss2: (0.0000) | Acc: (93.00%) (44262/47488)
Epoch: 119 | Batch_idx: 380 |  Loss: (0.1977) |  Loss2: (0.0000) | Acc: (93.00%) (45463/48768)
Epoch: 119 | Batch_idx: 390 |  Loss: (0.1973) |  Loss2: (0.0000) | Acc: (93.00%) (46622/50000)
# TEST : Loss: (0.4039) | Acc: (87.00%) (8724/10000)
percent tensor([0.5608, 0.5742, 0.5661, 0.5558, 0.5703, 0.5502, 0.5768, 0.5731, 0.5742,
        0.5711, 0.5706, 0.5698, 0.5668, 0.5734, 0.5634, 0.5591],
       device='cuda:0') torch.Size([16])
percent tensor([0.5299, 0.5320, 0.4905, 0.5240, 0.5198, 0.5122, 0.5306, 0.5476, 0.5222,
        0.5036, 0.5049, 0.4854, 0.5266, 0.5374, 0.5383, 0.5233],
       device='cuda:0') torch.Size([16])
percent tensor([0.5233, 0.4802, 0.4638, 0.5349, 0.4532, 0.5459, 0.4858, 0.5100, 0.5107,
        0.4710, 0.5086, 0.4614, 0.4743, 0.5567, 0.5285, 0.5287],
       device='cuda:0') torch.Size([16])
percent tensor([0.6212, 0.6362, 0.6038, 0.5971, 0.5983, 0.6400, 0.6196, 0.5846, 0.6121,
        0.6315, 0.6216, 0.6232, 0.6536, 0.6122, 0.6272, 0.6366],
       device='cuda:0') torch.Size([16])
percent tensor([0.6672, 0.5767, 0.7132, 0.7240, 0.7519, 0.7645, 0.6691, 0.7223, 0.7437,
        0.6401, 0.6839, 0.6781, 0.5481, 0.7482, 0.6302, 0.7116],
       device='cuda:0') torch.Size([16])
percent tensor([0.6956, 0.6855, 0.7254, 0.7202, 0.7603, 0.7423, 0.7253, 0.7129, 0.7212,
        0.6978, 0.7087, 0.6874, 0.6772, 0.7356, 0.6765, 0.7244],
       device='cuda:0') torch.Size([16])
percent tensor([0.5258, 0.5788, 0.5737, 0.5098, 0.6292, 0.6803, 0.6062, 0.4395, 0.6044,
        0.5503, 0.5271, 0.4675, 0.5218, 0.5495, 0.3789, 0.5858],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9995, 0.9990, 0.9994, 0.9995, 0.9980, 0.9991, 0.9995, 0.9996,
        0.9995, 0.9997, 0.9997, 0.9993, 0.9991, 0.9986, 0.9996],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 120 | Batch_idx: 0 |  Loss: (0.1743) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 120 | Batch_idx: 10 |  Loss: (0.1801) |  Loss2: (0.0000) | Acc: (93.00%) (1315/1408)
Epoch: 120 | Batch_idx: 20 |  Loss: (0.1824) |  Loss2: (0.0000) | Acc: (93.00%) (2512/2688)
Epoch: 120 | Batch_idx: 30 |  Loss: (0.1747) |  Loss2: (0.0000) | Acc: (93.00%) (3718/3968)
Epoch: 120 | Batch_idx: 40 |  Loss: (0.1741) |  Loss2: (0.0000) | Acc: (93.00%) (4916/5248)
Epoch: 120 | Batch_idx: 50 |  Loss: (0.1778) |  Loss2: (0.0000) | Acc: (93.00%) (6108/6528)
Epoch: 120 | Batch_idx: 60 |  Loss: (0.1741) |  Loss2: (0.0000) | Acc: (93.00%) (7322/7808)
Epoch: 120 | Batch_idx: 70 |  Loss: (0.1744) |  Loss2: (0.0000) | Acc: (93.00%) (8526/9088)
Epoch: 120 | Batch_idx: 80 |  Loss: (0.1715) |  Loss2: (0.0000) | Acc: (93.00%) (9736/10368)
Epoch: 120 | Batch_idx: 90 |  Loss: (0.1677) |  Loss2: (0.0000) | Acc: (94.00%) (10965/11648)
Epoch: 120 | Batch_idx: 100 |  Loss: (0.1673) |  Loss2: (0.0000) | Acc: (94.00%) (12180/12928)
Epoch: 120 | Batch_idx: 110 |  Loss: (0.1672) |  Loss2: (0.0000) | Acc: (94.00%) (13384/14208)
Epoch: 120 | Batch_idx: 120 |  Loss: (0.1646) |  Loss2: (0.0000) | Acc: (94.00%) (14604/15488)
Epoch: 120 | Batch_idx: 130 |  Loss: (0.1647) |  Loss2: (0.0000) | Acc: (94.00%) (15813/16768)
Epoch: 120 | Batch_idx: 140 |  Loss: (0.1646) |  Loss2: (0.0000) | Acc: (94.00%) (17027/18048)
Epoch: 120 | Batch_idx: 150 |  Loss: (0.1634) |  Loss2: (0.0000) | Acc: (94.00%) (18243/19328)
Epoch: 120 | Batch_idx: 160 |  Loss: (0.1624) |  Loss2: (0.0000) | Acc: (94.00%) (19455/20608)
Epoch: 120 | Batch_idx: 170 |  Loss: (0.1622) |  Loss2: (0.0000) | Acc: (94.00%) (20670/21888)
Epoch: 120 | Batch_idx: 180 |  Loss: (0.1613) |  Loss2: (0.0000) | Acc: (94.00%) (21883/23168)
Epoch: 120 | Batch_idx: 190 |  Loss: (0.1615) |  Loss2: (0.0000) | Acc: (94.00%) (23096/24448)
Epoch: 120 | Batch_idx: 200 |  Loss: (0.1616) |  Loss2: (0.0000) | Acc: (94.00%) (24304/25728)
Epoch: 120 | Batch_idx: 210 |  Loss: (0.1608) |  Loss2: (0.0000) | Acc: (94.00%) (25520/27008)
Epoch: 120 | Batch_idx: 220 |  Loss: (0.1622) |  Loss2: (0.0000) | Acc: (94.00%) (26719/28288)
Epoch: 120 | Batch_idx: 230 |  Loss: (0.1625) |  Loss2: (0.0000) | Acc: (94.00%) (27927/29568)
Epoch: 120 | Batch_idx: 240 |  Loss: (0.1614) |  Loss2: (0.0000) | Acc: (94.00%) (29151/30848)
Epoch: 120 | Batch_idx: 250 |  Loss: (0.1615) |  Loss2: (0.0000) | Acc: (94.00%) (30355/32128)
Epoch: 120 | Batch_idx: 260 |  Loss: (0.1615) |  Loss2: (0.0000) | Acc: (94.00%) (31563/33408)
Epoch: 120 | Batch_idx: 270 |  Loss: (0.1615) |  Loss2: (0.0000) | Acc: (94.00%) (32761/34688)
Epoch: 120 | Batch_idx: 280 |  Loss: (0.1618) |  Loss2: (0.0000) | Acc: (94.00%) (33961/35968)
Epoch: 120 | Batch_idx: 290 |  Loss: (0.1617) |  Loss2: (0.0000) | Acc: (94.00%) (35172/37248)
Epoch: 120 | Batch_idx: 300 |  Loss: (0.1615) |  Loss2: (0.0000) | Acc: (94.00%) (36388/38528)
Epoch: 120 | Batch_idx: 310 |  Loss: (0.1611) |  Loss2: (0.0000) | Acc: (94.00%) (37603/39808)
Epoch: 120 | Batch_idx: 320 |  Loss: (0.1615) |  Loss2: (0.0000) | Acc: (94.00%) (38809/41088)
Epoch: 120 | Batch_idx: 330 |  Loss: (0.1625) |  Loss2: (0.0000) | Acc: (94.00%) (40005/42368)
Epoch: 120 | Batch_idx: 340 |  Loss: (0.1620) |  Loss2: (0.0000) | Acc: (94.00%) (41218/43648)
Epoch: 120 | Batch_idx: 350 |  Loss: (0.1620) |  Loss2: (0.0000) | Acc: (94.00%) (42429/44928)
Epoch: 120 | Batch_idx: 360 |  Loss: (0.1627) |  Loss2: (0.0000) | Acc: (94.00%) (43625/46208)
Epoch: 120 | Batch_idx: 370 |  Loss: (0.1631) |  Loss2: (0.0000) | Acc: (94.00%) (44826/47488)
Epoch: 120 | Batch_idx: 380 |  Loss: (0.1632) |  Loss2: (0.0000) | Acc: (94.00%) (46028/48768)
Epoch: 120 | Batch_idx: 390 |  Loss: (0.1633) |  Loss2: (0.0000) | Acc: (94.00%) (47192/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_120.pth.tar'
# TEST : Loss: (0.3766) | Acc: (88.00%) (8800/10000)
percent tensor([0.5620, 0.5722, 0.5678, 0.5565, 0.5709, 0.5502, 0.5765, 0.5732, 0.5738,
        0.5713, 0.5702, 0.5724, 0.5675, 0.5708, 0.5629, 0.5585],
       device='cuda:0') torch.Size([16])
percent tensor([0.5263, 0.5316, 0.4907, 0.5220, 0.5192, 0.5104, 0.5308, 0.5466, 0.5189,
        0.5045, 0.4994, 0.4879, 0.5235, 0.5370, 0.5368, 0.5217],
       device='cuda:0') torch.Size([16])
percent tensor([0.5188, 0.4843, 0.4567, 0.5314, 0.4485, 0.5436, 0.4885, 0.5085, 0.5063,
        0.4692, 0.5035, 0.4525, 0.4645, 0.5578, 0.5265, 0.5274],
       device='cuda:0') torch.Size([16])
percent tensor([0.6228, 0.6325, 0.6073, 0.6004, 0.5977, 0.6425, 0.6156, 0.5849, 0.6157,
        0.6295, 0.6234, 0.6246, 0.6541, 0.6128, 0.6279, 0.6346],
       device='cuda:0') torch.Size([16])
percent tensor([0.6684, 0.5843, 0.7086, 0.7320, 0.7458, 0.7656, 0.6631, 0.7170, 0.7395,
        0.6487, 0.6962, 0.6674, 0.5528, 0.7209, 0.6410, 0.7194],
       device='cuda:0') torch.Size([16])
percent tensor([0.6960, 0.6883, 0.7247, 0.7212, 0.7552, 0.7437, 0.7144, 0.7069, 0.7150,
        0.7047, 0.7100, 0.6844, 0.6825, 0.7174, 0.6807, 0.7260],
       device='cuda:0') torch.Size([16])
percent tensor([0.5081, 0.5741, 0.6122, 0.5100, 0.6362, 0.6931, 0.5791, 0.4478, 0.5672,
        0.5667, 0.5304, 0.4967, 0.5058, 0.5172, 0.3887, 0.5858],
       device='cuda:0') torch.Size([16])
percent tensor([0.9996, 0.9995, 0.9991, 0.9992, 0.9994, 0.9986, 0.9995, 0.9996, 0.9995,
        0.9997, 0.9998, 0.9997, 0.9993, 0.9990, 0.9986, 0.9997],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(179.0599, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(816.1526, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(803.7243, device='cuda:0')
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1510.0752, device='cuda:0')
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(491.5718, device='cuda:0')
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2249.6333, device='cuda:0')
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4295.2974, device='cuda:0')
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1388.9448, device='cuda:0')
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6206.0029, device='cuda:0')
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11790.8984, device='cuda:0')
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3904.8245, device='cuda:0')
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16487.6484, device='cuda:0')
Epoch: 121 | Batch_idx: 0 |  Loss: (0.1029) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 121 | Batch_idx: 10 |  Loss: (0.1549) |  Loss2: (0.0000) | Acc: (93.00%) (1323/1408)
Epoch: 121 | Batch_idx: 20 |  Loss: (0.1730) |  Loss2: (0.0000) | Acc: (93.00%) (2504/2688)
Epoch: 121 | Batch_idx: 30 |  Loss: (0.1745) |  Loss2: (0.0000) | Acc: (93.00%) (3704/3968)
Epoch: 121 | Batch_idx: 40 |  Loss: (0.1763) |  Loss2: (0.0000) | Acc: (93.00%) (4891/5248)
Epoch: 121 | Batch_idx: 50 |  Loss: (0.1804) |  Loss2: (0.0000) | Acc: (93.00%) (6086/6528)
Epoch: 121 | Batch_idx: 60 |  Loss: (0.1836) |  Loss2: (0.0000) | Acc: (93.00%) (7276/7808)
Epoch: 121 | Batch_idx: 70 |  Loss: (0.1905) |  Loss2: (0.0000) | Acc: (92.00%) (8444/9088)
Epoch: 121 | Batch_idx: 80 |  Loss: (0.1925) |  Loss2: (0.0000) | Acc: (92.00%) (9632/10368)
Epoch: 121 | Batch_idx: 90 |  Loss: (0.1943) |  Loss2: (0.0000) | Acc: (92.00%) (10823/11648)
Epoch: 121 | Batch_idx: 100 |  Loss: (0.1918) |  Loss2: (0.0000) | Acc: (93.00%) (12031/12928)
Epoch: 121 | Batch_idx: 110 |  Loss: (0.1899) |  Loss2: (0.0000) | Acc: (93.00%) (13235/14208)
Epoch: 121 | Batch_idx: 120 |  Loss: (0.1892) |  Loss2: (0.0000) | Acc: (93.00%) (14437/15488)
Epoch: 121 | Batch_idx: 130 |  Loss: (0.1895) |  Loss2: (0.0000) | Acc: (93.00%) (15633/16768)
Epoch: 121 | Batch_idx: 140 |  Loss: (0.1889) |  Loss2: (0.0000) | Acc: (93.00%) (16833/18048)
Epoch: 121 | Batch_idx: 150 |  Loss: (0.1891) |  Loss2: (0.0000) | Acc: (93.00%) (18033/19328)
Epoch: 121 | Batch_idx: 160 |  Loss: (0.1890) |  Loss2: (0.0000) | Acc: (93.00%) (19236/20608)
Epoch: 121 | Batch_idx: 170 |  Loss: (0.1896) |  Loss2: (0.0000) | Acc: (93.00%) (20425/21888)
Epoch: 121 | Batch_idx: 180 |  Loss: (0.1892) |  Loss2: (0.0000) | Acc: (93.00%) (21612/23168)
Epoch: 121 | Batch_idx: 190 |  Loss: (0.1894) |  Loss2: (0.0000) | Acc: (93.00%) (22803/24448)
Epoch: 121 | Batch_idx: 200 |  Loss: (0.1896) |  Loss2: (0.0000) | Acc: (93.00%) (23993/25728)
Epoch: 121 | Batch_idx: 210 |  Loss: (0.1894) |  Loss2: (0.0000) | Acc: (93.00%) (25193/27008)
Epoch: 121 | Batch_idx: 220 |  Loss: (0.1895) |  Loss2: (0.0000) | Acc: (93.00%) (26394/28288)
Epoch: 121 | Batch_idx: 230 |  Loss: (0.1882) |  Loss2: (0.0000) | Acc: (93.00%) (27605/29568)
Epoch: 121 | Batch_idx: 240 |  Loss: (0.1872) |  Loss2: (0.0000) | Acc: (93.00%) (28817/30848)
Epoch: 121 | Batch_idx: 250 |  Loss: (0.1867) |  Loss2: (0.0000) | Acc: (93.00%) (30021/32128)
Epoch: 121 | Batch_idx: 260 |  Loss: (0.1857) |  Loss2: (0.0000) | Acc: (93.00%) (31233/33408)
Epoch: 121 | Batch_idx: 270 |  Loss: (0.1858) |  Loss2: (0.0000) | Acc: (93.00%) (32429/34688)
Epoch: 121 | Batch_idx: 280 |  Loss: (0.1853) |  Loss2: (0.0000) | Acc: (93.00%) (33631/35968)
Epoch: 121 | Batch_idx: 290 |  Loss: (0.1850) |  Loss2: (0.0000) | Acc: (93.00%) (34830/37248)
Epoch: 121 | Batch_idx: 300 |  Loss: (0.1847) |  Loss2: (0.0000) | Acc: (93.00%) (36033/38528)
Epoch: 121 | Batch_idx: 310 |  Loss: (0.1842) |  Loss2: (0.0000) | Acc: (93.00%) (37236/39808)
Epoch: 121 | Batch_idx: 320 |  Loss: (0.1839) |  Loss2: (0.0000) | Acc: (93.00%) (38439/41088)
Epoch: 121 | Batch_idx: 330 |  Loss: (0.1830) |  Loss2: (0.0000) | Acc: (93.00%) (39655/42368)
Epoch: 121 | Batch_idx: 340 |  Loss: (0.1824) |  Loss2: (0.0000) | Acc: (93.00%) (40865/43648)
Epoch: 121 | Batch_idx: 350 |  Loss: (0.1822) |  Loss2: (0.0000) | Acc: (93.00%) (42079/44928)
Epoch: 121 | Batch_idx: 360 |  Loss: (0.1820) |  Loss2: (0.0000) | Acc: (93.00%) (43280/46208)
Epoch: 121 | Batch_idx: 370 |  Loss: (0.1817) |  Loss2: (0.0000) | Acc: (93.00%) (44487/47488)
Epoch: 121 | Batch_idx: 380 |  Loss: (0.1823) |  Loss2: (0.0000) | Acc: (93.00%) (45670/48768)
Epoch: 121 | Batch_idx: 390 |  Loss: (0.1823) |  Loss2: (0.0000) | Acc: (93.00%) (46818/50000)
# TEST : Loss: (0.4046) | Acc: (87.00%) (8711/10000)
percent tensor([0.5599, 0.5706, 0.5651, 0.5546, 0.5688, 0.5481, 0.5745, 0.5711, 0.5719,
        0.5693, 0.5682, 0.5699, 0.5657, 0.5698, 0.5608, 0.5566],
       device='cuda:0') torch.Size([16])
percent tensor([0.5341, 0.5412, 0.4922, 0.5301, 0.5221, 0.5195, 0.5367, 0.5500, 0.5243,
        0.5119, 0.5090, 0.4911, 0.5323, 0.5449, 0.5464, 0.5303],
       device='cuda:0') torch.Size([16])
percent tensor([0.5197, 0.4831, 0.4471, 0.5195, 0.4349, 0.5481, 0.4854, 0.4954, 0.4949,
        0.4641, 0.5017, 0.4397, 0.4584, 0.5621, 0.5294, 0.5289],
       device='cuda:0') torch.Size([16])
percent tensor([0.6656, 0.6779, 0.6439, 0.6430, 0.6348, 0.6816, 0.6602, 0.6218, 0.6586,
        0.6789, 0.6719, 0.6673, 0.7014, 0.6583, 0.6730, 0.6824],
       device='cuda:0') torch.Size([16])
percent tensor([0.6482, 0.5625, 0.6897, 0.7271, 0.7350, 0.7559, 0.6422, 0.7004, 0.7254,
        0.6290, 0.6738, 0.6619, 0.5373, 0.7055, 0.6242, 0.7060],
       device='cuda:0') torch.Size([16])
percent tensor([0.7085, 0.7012, 0.7348, 0.7348, 0.7667, 0.7570, 0.7277, 0.7207, 0.7202,
        0.7179, 0.7167, 0.6963, 0.6933, 0.7284, 0.6937, 0.7375],
       device='cuda:0') torch.Size([16])
percent tensor([0.4794, 0.5438, 0.5594, 0.4830, 0.5708, 0.6738, 0.5322, 0.4165, 0.5152,
        0.5378, 0.4754, 0.4480, 0.4767, 0.4626, 0.3590, 0.5539],
       device='cuda:0') torch.Size([16])
percent tensor([0.9996, 0.9995, 0.9992, 0.9994, 0.9991, 0.9984, 0.9995, 0.9996, 0.9995,
        0.9997, 0.9997, 0.9997, 0.9993, 0.9989, 0.9989, 0.9995],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 122 | Batch_idx: 0 |  Loss: (0.2745) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 122 | Batch_idx: 10 |  Loss: (0.1774) |  Loss2: (0.0000) | Acc: (93.00%) (1320/1408)
Epoch: 122 | Batch_idx: 20 |  Loss: (0.1569) |  Loss2: (0.0000) | Acc: (94.00%) (2545/2688)
Epoch: 122 | Batch_idx: 30 |  Loss: (0.1520) |  Loss2: (0.0000) | Acc: (94.00%) (3766/3968)
Epoch: 122 | Batch_idx: 40 |  Loss: (0.1513) |  Loss2: (0.0000) | Acc: (94.00%) (4984/5248)
Epoch: 122 | Batch_idx: 50 |  Loss: (0.1500) |  Loss2: (0.0000) | Acc: (94.00%) (6198/6528)
Epoch: 122 | Batch_idx: 60 |  Loss: (0.1563) |  Loss2: (0.0000) | Acc: (94.00%) (7396/7808)
Epoch: 122 | Batch_idx: 70 |  Loss: (0.1538) |  Loss2: (0.0000) | Acc: (94.00%) (8618/9088)
Epoch: 122 | Batch_idx: 80 |  Loss: (0.1532) |  Loss2: (0.0000) | Acc: (94.00%) (9837/10368)
Epoch: 122 | Batch_idx: 90 |  Loss: (0.1532) |  Loss2: (0.0000) | Acc: (94.00%) (11043/11648)
Epoch: 122 | Batch_idx: 100 |  Loss: (0.1551) |  Loss2: (0.0000) | Acc: (94.00%) (12243/12928)
Epoch: 122 | Batch_idx: 110 |  Loss: (0.1556) |  Loss2: (0.0000) | Acc: (94.00%) (13454/14208)
Epoch: 122 | Batch_idx: 120 |  Loss: (0.1543) |  Loss2: (0.0000) | Acc: (94.00%) (14674/15488)
Epoch: 122 | Batch_idx: 130 |  Loss: (0.1553) |  Loss2: (0.0000) | Acc: (94.00%) (15880/16768)
Epoch: 122 | Batch_idx: 140 |  Loss: (0.1562) |  Loss2: (0.0000) | Acc: (94.00%) (17088/18048)
Epoch: 122 | Batch_idx: 150 |  Loss: (0.1561) |  Loss2: (0.0000) | Acc: (94.00%) (18297/19328)
Epoch: 122 | Batch_idx: 160 |  Loss: (0.1565) |  Loss2: (0.0000) | Acc: (94.00%) (19501/20608)
Epoch: 122 | Batch_idx: 170 |  Loss: (0.1575) |  Loss2: (0.0000) | Acc: (94.00%) (20700/21888)
Epoch: 122 | Batch_idx: 180 |  Loss: (0.1560) |  Loss2: (0.0000) | Acc: (94.00%) (21915/23168)
Epoch: 122 | Batch_idx: 190 |  Loss: (0.1559) |  Loss2: (0.0000) | Acc: (94.00%) (23129/24448)
Epoch: 122 | Batch_idx: 200 |  Loss: (0.1553) |  Loss2: (0.0000) | Acc: (94.00%) (24344/25728)
Epoch: 122 | Batch_idx: 210 |  Loss: (0.1551) |  Loss2: (0.0000) | Acc: (94.00%) (25557/27008)
Epoch: 122 | Batch_idx: 220 |  Loss: (0.1554) |  Loss2: (0.0000) | Acc: (94.00%) (26764/28288)
Epoch: 122 | Batch_idx: 230 |  Loss: (0.1564) |  Loss2: (0.0000) | Acc: (94.00%) (27960/29568)
Epoch: 122 | Batch_idx: 240 |  Loss: (0.1582) |  Loss2: (0.0000) | Acc: (94.00%) (29149/30848)
Epoch: 122 | Batch_idx: 250 |  Loss: (0.1583) |  Loss2: (0.0000) | Acc: (94.00%) (30362/32128)
Epoch: 122 | Batch_idx: 260 |  Loss: (0.1587) |  Loss2: (0.0000) | Acc: (94.00%) (31568/33408)
Epoch: 122 | Batch_idx: 270 |  Loss: (0.1589) |  Loss2: (0.0000) | Acc: (94.00%) (32778/34688)
Epoch: 122 | Batch_idx: 280 |  Loss: (0.1584) |  Loss2: (0.0000) | Acc: (94.00%) (34001/35968)
Epoch: 122 | Batch_idx: 290 |  Loss: (0.1587) |  Loss2: (0.0000) | Acc: (94.00%) (35207/37248)
Epoch: 122 | Batch_idx: 300 |  Loss: (0.1595) |  Loss2: (0.0000) | Acc: (94.00%) (36408/38528)
Epoch: 122 | Batch_idx: 310 |  Loss: (0.1597) |  Loss2: (0.0000) | Acc: (94.00%) (37612/39808)
Epoch: 122 | Batch_idx: 320 |  Loss: (0.1602) |  Loss2: (0.0000) | Acc: (94.00%) (38817/41088)
Epoch: 122 | Batch_idx: 330 |  Loss: (0.1604) |  Loss2: (0.0000) | Acc: (94.00%) (40019/42368)
Epoch: 122 | Batch_idx: 340 |  Loss: (0.1609) |  Loss2: (0.0000) | Acc: (94.00%) (41217/43648)
Epoch: 122 | Batch_idx: 350 |  Loss: (0.1609) |  Loss2: (0.0000) | Acc: (94.00%) (42425/44928)
Epoch: 122 | Batch_idx: 360 |  Loss: (0.1612) |  Loss2: (0.0000) | Acc: (94.00%) (43633/46208)
Epoch: 122 | Batch_idx: 370 |  Loss: (0.1609) |  Loss2: (0.0000) | Acc: (94.00%) (44837/47488)
Epoch: 122 | Batch_idx: 380 |  Loss: (0.1612) |  Loss2: (0.0000) | Acc: (94.00%) (46046/48768)
Epoch: 122 | Batch_idx: 390 |  Loss: (0.1608) |  Loss2: (0.0000) | Acc: (94.00%) (47220/50000)
# TEST : Loss: (0.3946) | Acc: (87.00%) (8758/10000)
percent tensor([0.5593, 0.5708, 0.5631, 0.5528, 0.5671, 0.5469, 0.5744, 0.5705, 0.5730,
        0.5691, 0.5689, 0.5689, 0.5654, 0.5701, 0.5607, 0.5562],
       device='cuda:0') torch.Size([16])
percent tensor([0.5376, 0.5401, 0.5091, 0.5363, 0.5354, 0.5196, 0.5425, 0.5590, 0.5307,
        0.5159, 0.5089, 0.5045, 0.5345, 0.5403, 0.5465, 0.5316],
       device='cuda:0') torch.Size([16])
percent tensor([0.5220, 0.4774, 0.4619, 0.5318, 0.4473, 0.5448, 0.4852, 0.5106, 0.4985,
        0.4652, 0.4949, 0.4527, 0.4621, 0.5585, 0.5235, 0.5268],
       device='cuda:0') torch.Size([16])
percent tensor([0.6643, 0.6812, 0.6377, 0.6311, 0.6297, 0.6750, 0.6620, 0.6208, 0.6554,
        0.6799, 0.6691, 0.6649, 0.7006, 0.6579, 0.6679, 0.6791],
       device='cuda:0') torch.Size([16])
percent tensor([0.6569, 0.5777, 0.7067, 0.7094, 0.7419, 0.7543, 0.6552, 0.6873, 0.7271,
        0.6450, 0.6875, 0.6769, 0.5623, 0.7306, 0.6257, 0.7055],
       device='cuda:0') torch.Size([16])
percent tensor([0.7113, 0.7033, 0.7391, 0.7353, 0.7770, 0.7574, 0.7329, 0.7261, 0.7305,
        0.7148, 0.7249, 0.6977, 0.6965, 0.7406, 0.6970, 0.7388],
       device='cuda:0') torch.Size([16])
percent tensor([0.4831, 0.5701, 0.5616, 0.4698, 0.6099, 0.6613, 0.5564, 0.4278, 0.5576,
        0.5215, 0.5148, 0.4387, 0.4930, 0.5338, 0.3734, 0.5317],
       device='cuda:0') torch.Size([16])
percent tensor([0.9996, 0.9994, 0.9991, 0.9997, 0.9995, 0.9989, 0.9995, 0.9995, 0.9995,
        0.9994, 0.9998, 0.9997, 0.9989, 0.9991, 0.9986, 0.9998],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 123 | Batch_idx: 0 |  Loss: (0.1200) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 123 | Batch_idx: 10 |  Loss: (0.1506) |  Loss2: (0.0000) | Acc: (94.00%) (1336/1408)
Epoch: 123 | Batch_idx: 20 |  Loss: (0.1538) |  Loss2: (0.0000) | Acc: (94.00%) (2545/2688)
Epoch: 123 | Batch_idx: 30 |  Loss: (0.1643) |  Loss2: (0.0000) | Acc: (94.00%) (3740/3968)
Epoch: 123 | Batch_idx: 40 |  Loss: (0.1609) |  Loss2: (0.0000) | Acc: (94.00%) (4949/5248)
Epoch: 123 | Batch_idx: 50 |  Loss: (0.1647) |  Loss2: (0.0000) | Acc: (94.00%) (6158/6528)
Epoch: 123 | Batch_idx: 60 |  Loss: (0.1670) |  Loss2: (0.0000) | Acc: (94.00%) (7353/7808)
Epoch: 123 | Batch_idx: 70 |  Loss: (0.1660) |  Loss2: (0.0000) | Acc: (94.00%) (8550/9088)
Epoch: 123 | Batch_idx: 80 |  Loss: (0.1671) |  Loss2: (0.0000) | Acc: (94.00%) (9758/10368)
Epoch: 123 | Batch_idx: 90 |  Loss: (0.1649) |  Loss2: (0.0000) | Acc: (94.00%) (10972/11648)
Epoch: 123 | Batch_idx: 100 |  Loss: (0.1642) |  Loss2: (0.0000) | Acc: (94.00%) (12178/12928)
Epoch: 123 | Batch_idx: 110 |  Loss: (0.1620) |  Loss2: (0.0000) | Acc: (94.00%) (13402/14208)
Epoch: 123 | Batch_idx: 120 |  Loss: (0.1594) |  Loss2: (0.0000) | Acc: (94.00%) (14629/15488)
Epoch: 123 | Batch_idx: 130 |  Loss: (0.1592) |  Loss2: (0.0000) | Acc: (94.00%) (15835/16768)
Epoch: 123 | Batch_idx: 140 |  Loss: (0.1596) |  Loss2: (0.0000) | Acc: (94.00%) (17048/18048)
Epoch: 123 | Batch_idx: 150 |  Loss: (0.1597) |  Loss2: (0.0000) | Acc: (94.00%) (18263/19328)
Epoch: 123 | Batch_idx: 160 |  Loss: (0.1617) |  Loss2: (0.0000) | Acc: (94.00%) (19464/20608)
Epoch: 123 | Batch_idx: 170 |  Loss: (0.1619) |  Loss2: (0.0000) | Acc: (94.00%) (20671/21888)
Epoch: 123 | Batch_idx: 180 |  Loss: (0.1617) |  Loss2: (0.0000) | Acc: (94.00%) (21878/23168)
Epoch: 123 | Batch_idx: 190 |  Loss: (0.1619) |  Loss2: (0.0000) | Acc: (94.00%) (23085/24448)
Epoch: 123 | Batch_idx: 200 |  Loss: (0.1614) |  Loss2: (0.0000) | Acc: (94.00%) (24296/25728)
Epoch: 123 | Batch_idx: 210 |  Loss: (0.1610) |  Loss2: (0.0000) | Acc: (94.00%) (25504/27008)
Epoch: 123 | Batch_idx: 220 |  Loss: (0.1611) |  Loss2: (0.0000) | Acc: (94.00%) (26708/28288)
Epoch: 123 | Batch_idx: 230 |  Loss: (0.1610) |  Loss2: (0.0000) | Acc: (94.00%) (27928/29568)
Epoch: 123 | Batch_idx: 240 |  Loss: (0.1607) |  Loss2: (0.0000) | Acc: (94.00%) (29140/30848)
Epoch: 123 | Batch_idx: 250 |  Loss: (0.1613) |  Loss2: (0.0000) | Acc: (94.00%) (30339/32128)
Epoch: 123 | Batch_idx: 260 |  Loss: (0.1614) |  Loss2: (0.0000) | Acc: (94.00%) (31547/33408)
Epoch: 123 | Batch_idx: 270 |  Loss: (0.1612) |  Loss2: (0.0000) | Acc: (94.00%) (32760/34688)
Epoch: 123 | Batch_idx: 280 |  Loss: (0.1605) |  Loss2: (0.0000) | Acc: (94.00%) (33988/35968)
Epoch: 123 | Batch_idx: 290 |  Loss: (0.1604) |  Loss2: (0.0000) | Acc: (94.00%) (35196/37248)
Epoch: 123 | Batch_idx: 300 |  Loss: (0.1603) |  Loss2: (0.0000) | Acc: (94.00%) (36413/38528)
Epoch: 123 | Batch_idx: 310 |  Loss: (0.1610) |  Loss2: (0.0000) | Acc: (94.00%) (37607/39808)
Epoch: 123 | Batch_idx: 320 |  Loss: (0.1615) |  Loss2: (0.0000) | Acc: (94.00%) (38807/41088)
Epoch: 123 | Batch_idx: 330 |  Loss: (0.1613) |  Loss2: (0.0000) | Acc: (94.00%) (40018/42368)
Epoch: 123 | Batch_idx: 340 |  Loss: (0.1615) |  Loss2: (0.0000) | Acc: (94.00%) (41231/43648)
Epoch: 123 | Batch_idx: 350 |  Loss: (0.1620) |  Loss2: (0.0000) | Acc: (94.00%) (42431/44928)
Epoch: 123 | Batch_idx: 360 |  Loss: (0.1617) |  Loss2: (0.0000) | Acc: (94.00%) (43643/46208)
Epoch: 123 | Batch_idx: 370 |  Loss: (0.1614) |  Loss2: (0.0000) | Acc: (94.00%) (44863/47488)
Epoch: 123 | Batch_idx: 380 |  Loss: (0.1618) |  Loss2: (0.0000) | Acc: (94.00%) (46063/48768)
Epoch: 123 | Batch_idx: 390 |  Loss: (0.1612) |  Loss2: (0.0000) | Acc: (94.00%) (47241/50000)
# TEST : Loss: (0.3783) | Acc: (88.00%) (8804/10000)
percent tensor([0.5611, 0.5732, 0.5657, 0.5542, 0.5698, 0.5492, 0.5772, 0.5725, 0.5757,
        0.5714, 0.5714, 0.5716, 0.5674, 0.5730, 0.5628, 0.5583],
       device='cuda:0') torch.Size([16])
percent tensor([0.5315, 0.5372, 0.5033, 0.5321, 0.5314, 0.5104, 0.5391, 0.5572, 0.5261,
        0.5113, 0.5037, 0.4968, 0.5303, 0.5381, 0.5423, 0.5261],
       device='cuda:0') torch.Size([16])
percent tensor([0.5184, 0.4707, 0.4635, 0.5280, 0.4488, 0.5427, 0.4822, 0.5103, 0.4955,
        0.4583, 0.4885, 0.4491, 0.4577, 0.5573, 0.5182, 0.5202],
       device='cuda:0') torch.Size([16])
percent tensor([0.6663, 0.6825, 0.6397, 0.6304, 0.6315, 0.6752, 0.6648, 0.6238, 0.6549,
        0.6784, 0.6687, 0.6647, 0.7010, 0.6568, 0.6712, 0.6784],
       device='cuda:0') torch.Size([16])
percent tensor([0.6561, 0.5603, 0.7072, 0.7162, 0.7493, 0.7620, 0.6522, 0.6962, 0.7280,
        0.6237, 0.6733, 0.6648, 0.5533, 0.7159, 0.6244, 0.7062],
       device='cuda:0') torch.Size([16])
percent tensor([0.7629, 0.7497, 0.7848, 0.7802, 0.8197, 0.8007, 0.7865, 0.7766, 0.7810,
        0.7638, 0.7755, 0.7522, 0.7494, 0.7865, 0.7503, 0.7859],
       device='cuda:0') torch.Size([16])
percent tensor([0.4892, 0.5680, 0.5417, 0.4544, 0.5808, 0.6618, 0.5580, 0.3906, 0.5775,
        0.5428, 0.5483, 0.4739, 0.5242, 0.5398, 0.3733, 0.5133],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9996, 0.9990, 0.9997, 0.9995, 0.9987, 0.9994, 0.9995, 0.9997,
        0.9996, 0.9998, 0.9997, 0.9990, 0.9992, 0.9988, 0.9997],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 124 | Batch_idx: 0 |  Loss: (0.2271) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 124 | Batch_idx: 10 |  Loss: (0.1539) |  Loss2: (0.0000) | Acc: (95.00%) (1339/1408)
Epoch: 124 | Batch_idx: 20 |  Loss: (0.1487) |  Loss2: (0.0000) | Acc: (94.00%) (2553/2688)
Epoch: 124 | Batch_idx: 30 |  Loss: (0.1526) |  Loss2: (0.0000) | Acc: (94.00%) (3761/3968)
Epoch: 124 | Batch_idx: 40 |  Loss: (0.1460) |  Loss2: (0.0000) | Acc: (95.00%) (4990/5248)
Epoch: 124 | Batch_idx: 50 |  Loss: (0.1457) |  Loss2: (0.0000) | Acc: (95.00%) (6203/6528)
Epoch: 124 | Batch_idx: 60 |  Loss: (0.1473) |  Loss2: (0.0000) | Acc: (94.00%) (7417/7808)
Epoch: 124 | Batch_idx: 70 |  Loss: (0.1475) |  Loss2: (0.0000) | Acc: (95.00%) (8636/9088)
Epoch: 124 | Batch_idx: 80 |  Loss: (0.1494) |  Loss2: (0.0000) | Acc: (94.00%) (9839/10368)
Epoch: 124 | Batch_idx: 90 |  Loss: (0.1482) |  Loss2: (0.0000) | Acc: (94.00%) (11053/11648)
Epoch: 124 | Batch_idx: 100 |  Loss: (0.1472) |  Loss2: (0.0000) | Acc: (94.00%) (12266/12928)
Epoch: 124 | Batch_idx: 110 |  Loss: (0.1483) |  Loss2: (0.0000) | Acc: (94.00%) (13475/14208)
Epoch: 124 | Batch_idx: 120 |  Loss: (0.1468) |  Loss2: (0.0000) | Acc: (94.00%) (14700/15488)
Epoch: 124 | Batch_idx: 130 |  Loss: (0.1472) |  Loss2: (0.0000) | Acc: (94.00%) (15913/16768)
Epoch: 124 | Batch_idx: 140 |  Loss: (0.1480) |  Loss2: (0.0000) | Acc: (94.00%) (17112/18048)
Epoch: 124 | Batch_idx: 150 |  Loss: (0.1497) |  Loss2: (0.0000) | Acc: (94.00%) (18314/19328)
Epoch: 124 | Batch_idx: 160 |  Loss: (0.1514) |  Loss2: (0.0000) | Acc: (94.00%) (19514/20608)
Epoch: 124 | Batch_idx: 170 |  Loss: (0.1519) |  Loss2: (0.0000) | Acc: (94.00%) (20720/21888)
Epoch: 124 | Batch_idx: 180 |  Loss: (0.1518) |  Loss2: (0.0000) | Acc: (94.00%) (21927/23168)
Epoch: 124 | Batch_idx: 190 |  Loss: (0.1523) |  Loss2: (0.0000) | Acc: (94.00%) (23138/24448)
Epoch: 124 | Batch_idx: 200 |  Loss: (0.1538) |  Loss2: (0.0000) | Acc: (94.00%) (24330/25728)
Epoch: 124 | Batch_idx: 210 |  Loss: (0.1551) |  Loss2: (0.0000) | Acc: (94.00%) (25530/27008)
Epoch: 124 | Batch_idx: 220 |  Loss: (0.1560) |  Loss2: (0.0000) | Acc: (94.00%) (26736/28288)
Epoch: 124 | Batch_idx: 230 |  Loss: (0.1561) |  Loss2: (0.0000) | Acc: (94.00%) (27946/29568)
Epoch: 124 | Batch_idx: 240 |  Loss: (0.1565) |  Loss2: (0.0000) | Acc: (94.00%) (29158/30848)
Epoch: 124 | Batch_idx: 250 |  Loss: (0.1563) |  Loss2: (0.0000) | Acc: (94.00%) (30371/32128)
Epoch: 124 | Batch_idx: 260 |  Loss: (0.1559) |  Loss2: (0.0000) | Acc: (94.00%) (31588/33408)
Epoch: 124 | Batch_idx: 270 |  Loss: (0.1564) |  Loss2: (0.0000) | Acc: (94.00%) (32786/34688)
Epoch: 124 | Batch_idx: 280 |  Loss: (0.1571) |  Loss2: (0.0000) | Acc: (94.00%) (33984/35968)
Epoch: 124 | Batch_idx: 290 |  Loss: (0.1572) |  Loss2: (0.0000) | Acc: (94.00%) (35187/37248)
Epoch: 124 | Batch_idx: 300 |  Loss: (0.1574) |  Loss2: (0.0000) | Acc: (94.00%) (36391/38528)
Epoch: 124 | Batch_idx: 310 |  Loss: (0.1568) |  Loss2: (0.0000) | Acc: (94.00%) (37617/39808)
Epoch: 124 | Batch_idx: 320 |  Loss: (0.1562) |  Loss2: (0.0000) | Acc: (94.00%) (38839/41088)
Epoch: 124 | Batch_idx: 330 |  Loss: (0.1558) |  Loss2: (0.0000) | Acc: (94.00%) (40051/42368)
Epoch: 124 | Batch_idx: 340 |  Loss: (0.1554) |  Loss2: (0.0000) | Acc: (94.00%) (41266/43648)
Epoch: 124 | Batch_idx: 350 |  Loss: (0.1554) |  Loss2: (0.0000) | Acc: (94.00%) (42482/44928)
Epoch: 124 | Batch_idx: 360 |  Loss: (0.1560) |  Loss2: (0.0000) | Acc: (94.00%) (43689/46208)
Epoch: 124 | Batch_idx: 370 |  Loss: (0.1560) |  Loss2: (0.0000) | Acc: (94.00%) (44893/47488)
Epoch: 124 | Batch_idx: 380 |  Loss: (0.1560) |  Loss2: (0.0000) | Acc: (94.00%) (46097/48768)
Epoch: 124 | Batch_idx: 390 |  Loss: (0.1560) |  Loss2: (0.0000) | Acc: (94.00%) (47264/50000)
# TEST : Loss: (0.4048) | Acc: (87.00%) (8749/10000)
percent tensor([0.5615, 0.5741, 0.5658, 0.5558, 0.5695, 0.5494, 0.5775, 0.5734, 0.5747,
        0.5717, 0.5706, 0.5717, 0.5675, 0.5729, 0.5636, 0.5588],
       device='cuda:0') torch.Size([16])
percent tensor([0.5321, 0.5360, 0.5000, 0.5300, 0.5290, 0.5126, 0.5364, 0.5556, 0.5234,
        0.5093, 0.5034, 0.4923, 0.5305, 0.5369, 0.5411, 0.5264],
       device='cuda:0') torch.Size([16])
percent tensor([0.5167, 0.4787, 0.4495, 0.5315, 0.4407, 0.5426, 0.4852, 0.5067, 0.4964,
        0.4598, 0.4956, 0.4428, 0.4562, 0.5638, 0.5227, 0.5245],
       device='cuda:0') torch.Size([16])
percent tensor([0.6716, 0.6825, 0.6508, 0.6387, 0.6414, 0.6843, 0.6671, 0.6281, 0.6604,
        0.6808, 0.6725, 0.6736, 0.7043, 0.6541, 0.6753, 0.6815],
       device='cuda:0') torch.Size([16])
percent tensor([0.6507, 0.5555, 0.7116, 0.7288, 0.7453, 0.7655, 0.6375, 0.6957, 0.7209,
        0.6230, 0.6742, 0.6740, 0.5397, 0.7230, 0.6133, 0.6993],
       device='cuda:0') torch.Size([16])
percent tensor([0.7570, 0.7550, 0.7776, 0.7778, 0.8106, 0.7959, 0.7791, 0.7744, 0.7787,
        0.7649, 0.7730, 0.7531, 0.7485, 0.7877, 0.7461, 0.7795],
       device='cuda:0') torch.Size([16])
percent tensor([0.4937, 0.5639, 0.5530, 0.4626, 0.5824, 0.6576, 0.5609, 0.3928, 0.5831,
        0.5296, 0.5313, 0.4764, 0.5132, 0.5255, 0.3588, 0.5142],
       device='cuda:0') torch.Size([16])
percent tensor([0.9996, 0.9996, 0.9991, 0.9995, 0.9990, 0.9991, 0.9993, 0.9997, 0.9994,
        0.9996, 0.9998, 0.9995, 0.9993, 0.9995, 0.9992, 0.9997],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 125 | Batch_idx: 0 |  Loss: (0.1087) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 125 | Batch_idx: 10 |  Loss: (0.1486) |  Loss2: (0.0000) | Acc: (94.00%) (1335/1408)
Epoch: 125 | Batch_idx: 20 |  Loss: (0.1780) |  Loss2: (0.0000) | Acc: (93.00%) (2525/2688)
Epoch: 125 | Batch_idx: 30 |  Loss: (0.1880) |  Loss2: (0.0000) | Acc: (93.00%) (3704/3968)
Epoch: 125 | Batch_idx: 40 |  Loss: (0.1918) |  Loss2: (0.0000) | Acc: (93.00%) (4889/5248)
Epoch: 125 | Batch_idx: 50 |  Loss: (0.1906) |  Loss2: (0.0000) | Acc: (93.00%) (6080/6528)
Epoch: 125 | Batch_idx: 60 |  Loss: (0.1979) |  Loss2: (0.0000) | Acc: (92.00%) (7259/7808)
Epoch: 125 | Batch_idx: 70 |  Loss: (0.1964) |  Loss2: (0.0000) | Acc: (93.00%) (8454/9088)
Epoch: 125 | Batch_idx: 80 |  Loss: (0.2000) |  Loss2: (0.0000) | Acc: (92.00%) (9626/10368)
Epoch: 125 | Batch_idx: 90 |  Loss: (0.2001) |  Loss2: (0.0000) | Acc: (92.00%) (10811/11648)
Epoch: 125 | Batch_idx: 100 |  Loss: (0.1972) |  Loss2: (0.0000) | Acc: (92.00%) (12008/12928)
Epoch: 125 | Batch_idx: 110 |  Loss: (0.1970) |  Loss2: (0.0000) | Acc: (92.00%) (13197/14208)
Epoch: 125 | Batch_idx: 120 |  Loss: (0.1965) |  Loss2: (0.0000) | Acc: (92.00%) (14398/15488)
Epoch: 125 | Batch_idx: 130 |  Loss: (0.1970) |  Loss2: (0.0000) | Acc: (92.00%) (15590/16768)
Epoch: 125 | Batch_idx: 140 |  Loss: (0.1987) |  Loss2: (0.0000) | Acc: (92.00%) (16767/18048)
Epoch: 125 | Batch_idx: 150 |  Loss: (0.1970) |  Loss2: (0.0000) | Acc: (92.00%) (17960/19328)
Epoch: 125 | Batch_idx: 160 |  Loss: (0.1975) |  Loss2: (0.0000) | Acc: (92.00%) (19147/20608)
Epoch: 125 | Batch_idx: 170 |  Loss: (0.1964) |  Loss2: (0.0000) | Acc: (92.00%) (20346/21888)
Epoch: 125 | Batch_idx: 180 |  Loss: (0.1951) |  Loss2: (0.0000) | Acc: (93.00%) (21550/23168)
Epoch: 125 | Batch_idx: 190 |  Loss: (0.1941) |  Loss2: (0.0000) | Acc: (93.00%) (22752/24448)
Epoch: 125 | Batch_idx: 200 |  Loss: (0.1943) |  Loss2: (0.0000) | Acc: (93.00%) (23949/25728)
Epoch: 125 | Batch_idx: 210 |  Loss: (0.1938) |  Loss2: (0.0000) | Acc: (93.00%) (25144/27008)
Epoch: 125 | Batch_idx: 220 |  Loss: (0.1943) |  Loss2: (0.0000) | Acc: (93.00%) (26328/28288)
Epoch: 125 | Batch_idx: 230 |  Loss: (0.1933) |  Loss2: (0.0000) | Acc: (93.00%) (27523/29568)
Epoch: 125 | Batch_idx: 240 |  Loss: (0.1917) |  Loss2: (0.0000) | Acc: (93.00%) (28731/30848)
Epoch: 125 | Batch_idx: 250 |  Loss: (0.1910) |  Loss2: (0.0000) | Acc: (93.00%) (29928/32128)
Epoch: 125 | Batch_idx: 260 |  Loss: (0.1914) |  Loss2: (0.0000) | Acc: (93.00%) (31118/33408)
Epoch: 125 | Batch_idx: 270 |  Loss: (0.1918) |  Loss2: (0.0000) | Acc: (93.00%) (32302/34688)
Epoch: 125 | Batch_idx: 280 |  Loss: (0.1920) |  Loss2: (0.0000) | Acc: (93.00%) (33496/35968)
Epoch: 125 | Batch_idx: 290 |  Loss: (0.1921) |  Loss2: (0.0000) | Acc: (93.00%) (34690/37248)
Epoch: 125 | Batch_idx: 300 |  Loss: (0.1914) |  Loss2: (0.0000) | Acc: (93.00%) (35898/38528)
Epoch: 125 | Batch_idx: 310 |  Loss: (0.1911) |  Loss2: (0.0000) | Acc: (93.00%) (37096/39808)
Epoch: 125 | Batch_idx: 320 |  Loss: (0.1906) |  Loss2: (0.0000) | Acc: (93.00%) (38294/41088)
Epoch: 125 | Batch_idx: 330 |  Loss: (0.1898) |  Loss2: (0.0000) | Acc: (93.00%) (39498/42368)
Epoch: 125 | Batch_idx: 340 |  Loss: (0.1893) |  Loss2: (0.0000) | Acc: (93.00%) (40706/43648)
Epoch: 125 | Batch_idx: 350 |  Loss: (0.1892) |  Loss2: (0.0000) | Acc: (93.00%) (41908/44928)
Epoch: 125 | Batch_idx: 360 |  Loss: (0.1890) |  Loss2: (0.0000) | Acc: (93.00%) (43120/46208)
Epoch: 125 | Batch_idx: 370 |  Loss: (0.1887) |  Loss2: (0.0000) | Acc: (93.00%) (44317/47488)
Epoch: 125 | Batch_idx: 380 |  Loss: (0.1877) |  Loss2: (0.0000) | Acc: (93.00%) (45532/48768)
Epoch: 125 | Batch_idx: 390 |  Loss: (0.1872) |  Loss2: (0.0000) | Acc: (93.00%) (46695/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_125.pth.tar'
# TEST : Loss: (0.3970) | Acc: (87.00%) (8731/10000)
percent tensor([0.5647, 0.5791, 0.5675, 0.5585, 0.5716, 0.5516, 0.5815, 0.5773, 0.5786,
        0.5755, 0.5744, 0.5739, 0.5712, 0.5782, 0.5674, 0.5624],
       device='cuda:0') torch.Size([16])
percent tensor([0.5331, 0.5339, 0.4995, 0.5295, 0.5277, 0.5136, 0.5347, 0.5530, 0.5235,
        0.5093, 0.5038, 0.4938, 0.5298, 0.5370, 0.5393, 0.5265],
       device='cuda:0') torch.Size([16])
percent tensor([0.5480, 0.4984, 0.4864, 0.5654, 0.4750, 0.5686, 0.5123, 0.5446, 0.5274,
        0.4875, 0.5223, 0.4766, 0.4833, 0.5896, 0.5513, 0.5534],
       device='cuda:0') torch.Size([16])
percent tensor([0.6549, 0.6688, 0.6329, 0.6196, 0.6251, 0.6684, 0.6504, 0.6102, 0.6470,
        0.6674, 0.6593, 0.6554, 0.6874, 0.6443, 0.6561, 0.6670],
       device='cuda:0') torch.Size([16])
percent tensor([0.6536, 0.5634, 0.6916, 0.7048, 0.7249, 0.7651, 0.6495, 0.6753, 0.7246,
        0.6221, 0.6840, 0.6721, 0.5638, 0.7290, 0.6175, 0.7032],
       device='cuda:0') torch.Size([16])
percent tensor([0.7443, 0.7420, 0.7628, 0.7664, 0.8002, 0.7953, 0.7642, 0.7549, 0.7686,
        0.7498, 0.7620, 0.7339, 0.7360, 0.7732, 0.7314, 0.7690],
       device='cuda:0') torch.Size([16])
percent tensor([0.4611, 0.5244, 0.5228, 0.4539, 0.5709, 0.6621, 0.5216, 0.3718, 0.5620,
        0.4757, 0.4983, 0.4469, 0.4803, 0.5017, 0.3400, 0.4897],
       device='cuda:0') torch.Size([16])
percent tensor([0.9995, 0.9996, 0.9992, 0.9996, 0.9994, 0.9986, 0.9994, 0.9996, 0.9994,
        0.9995, 0.9998, 0.9995, 0.9994, 0.9992, 0.9992, 0.9997],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 126 | Batch_idx: 0 |  Loss: (0.1956) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 126 | Batch_idx: 10 |  Loss: (0.1555) |  Loss2: (0.0000) | Acc: (94.00%) (1336/1408)
Epoch: 126 | Batch_idx: 20 |  Loss: (0.1482) |  Loss2: (0.0000) | Acc: (95.00%) (2555/2688)
Epoch: 126 | Batch_idx: 30 |  Loss: (0.1407) |  Loss2: (0.0000) | Acc: (95.00%) (3778/3968)
Epoch: 126 | Batch_idx: 40 |  Loss: (0.1440) |  Loss2: (0.0000) | Acc: (95.00%) (4987/5248)
Epoch: 126 | Batch_idx: 50 |  Loss: (0.1484) |  Loss2: (0.0000) | Acc: (94.00%) (6191/6528)
Epoch: 126 | Batch_idx: 60 |  Loss: (0.1474) |  Loss2: (0.0000) | Acc: (94.00%) (7409/7808)
Epoch: 126 | Batch_idx: 70 |  Loss: (0.1457) |  Loss2: (0.0000) | Acc: (94.00%) (8626/9088)
Epoch: 126 | Batch_idx: 80 |  Loss: (0.1495) |  Loss2: (0.0000) | Acc: (94.00%) (9824/10368)
Epoch: 126 | Batch_idx: 90 |  Loss: (0.1486) |  Loss2: (0.0000) | Acc: (94.00%) (11051/11648)
Epoch: 126 | Batch_idx: 100 |  Loss: (0.1471) |  Loss2: (0.0000) | Acc: (94.00%) (12270/12928)
Epoch: 126 | Batch_idx: 110 |  Loss: (0.1494) |  Loss2: (0.0000) | Acc: (94.00%) (13470/14208)
Epoch: 126 | Batch_idx: 120 |  Loss: (0.1501) |  Loss2: (0.0000) | Acc: (94.00%) (14680/15488)
Epoch: 126 | Batch_idx: 130 |  Loss: (0.1484) |  Loss2: (0.0000) | Acc: (94.00%) (15907/16768)
Epoch: 126 | Batch_idx: 140 |  Loss: (0.1495) |  Loss2: (0.0000) | Acc: (94.00%) (17111/18048)
Epoch: 126 | Batch_idx: 150 |  Loss: (0.1496) |  Loss2: (0.0000) | Acc: (94.00%) (18321/19328)
Epoch: 126 | Batch_idx: 160 |  Loss: (0.1496) |  Loss2: (0.0000) | Acc: (94.00%) (19537/20608)
Epoch: 126 | Batch_idx: 170 |  Loss: (0.1509) |  Loss2: (0.0000) | Acc: (94.00%) (20747/21888)
Epoch: 126 | Batch_idx: 180 |  Loss: (0.1505) |  Loss2: (0.0000) | Acc: (94.00%) (21959/23168)
Epoch: 126 | Batch_idx: 190 |  Loss: (0.1513) |  Loss2: (0.0000) | Acc: (94.00%) (23158/24448)
Epoch: 126 | Batch_idx: 200 |  Loss: (0.1513) |  Loss2: (0.0000) | Acc: (94.00%) (24380/25728)
Epoch: 126 | Batch_idx: 210 |  Loss: (0.1504) |  Loss2: (0.0000) | Acc: (94.00%) (25599/27008)
Epoch: 126 | Batch_idx: 220 |  Loss: (0.1504) |  Loss2: (0.0000) | Acc: (94.00%) (26813/28288)
Epoch: 126 | Batch_idx: 230 |  Loss: (0.1505) |  Loss2: (0.0000) | Acc: (94.00%) (28019/29568)
Epoch: 126 | Batch_idx: 240 |  Loss: (0.1500) |  Loss2: (0.0000) | Acc: (94.00%) (29243/30848)
Epoch: 126 | Batch_idx: 250 |  Loss: (0.1507) |  Loss2: (0.0000) | Acc: (94.00%) (30443/32128)
Epoch: 126 | Batch_idx: 260 |  Loss: (0.1509) |  Loss2: (0.0000) | Acc: (94.00%) (31654/33408)
Epoch: 126 | Batch_idx: 270 |  Loss: (0.1513) |  Loss2: (0.0000) | Acc: (94.00%) (32868/34688)
Epoch: 126 | Batch_idx: 280 |  Loss: (0.1506) |  Loss2: (0.0000) | Acc: (94.00%) (34099/35968)
Epoch: 126 | Batch_idx: 290 |  Loss: (0.1502) |  Loss2: (0.0000) | Acc: (94.00%) (35317/37248)
Epoch: 126 | Batch_idx: 300 |  Loss: (0.1495) |  Loss2: (0.0000) | Acc: (94.00%) (36544/38528)
Epoch: 126 | Batch_idx: 310 |  Loss: (0.1501) |  Loss2: (0.0000) | Acc: (94.00%) (37757/39808)
Epoch: 126 | Batch_idx: 320 |  Loss: (0.1495) |  Loss2: (0.0000) | Acc: (94.00%) (38980/41088)
Epoch: 126 | Batch_idx: 330 |  Loss: (0.1488) |  Loss2: (0.0000) | Acc: (94.00%) (40202/42368)
Epoch: 126 | Batch_idx: 340 |  Loss: (0.1482) |  Loss2: (0.0000) | Acc: (94.00%) (41428/43648)
Epoch: 126 | Batch_idx: 350 |  Loss: (0.1481) |  Loss2: (0.0000) | Acc: (94.00%) (42642/44928)
Epoch: 126 | Batch_idx: 360 |  Loss: (0.1480) |  Loss2: (0.0000) | Acc: (94.00%) (43862/46208)
Epoch: 126 | Batch_idx: 370 |  Loss: (0.1482) |  Loss2: (0.0000) | Acc: (94.00%) (45063/47488)
Epoch: 126 | Batch_idx: 380 |  Loss: (0.1481) |  Loss2: (0.0000) | Acc: (94.00%) (46282/48768)
Epoch: 126 | Batch_idx: 390 |  Loss: (0.1489) |  Loss2: (0.0000) | Acc: (94.00%) (47436/50000)
# TEST : Loss: (0.3967) | Acc: (87.00%) (8769/10000)
percent tensor([0.5659, 0.5775, 0.5708, 0.5584, 0.5738, 0.5518, 0.5815, 0.5772, 0.5793,
        0.5758, 0.5749, 0.5761, 0.5722, 0.5766, 0.5664, 0.5621],
       device='cuda:0') torch.Size([16])
percent tensor([0.5307, 0.5325, 0.4963, 0.5234, 0.5254, 0.5115, 0.5340, 0.5479, 0.5218,
        0.5082, 0.5022, 0.4919, 0.5260, 0.5361, 0.5376, 0.5233],
       device='cuda:0') torch.Size([16])
percent tensor([0.5453, 0.5054, 0.4721, 0.5456, 0.4658, 0.5733, 0.5201, 0.5324, 0.5341,
        0.4855, 0.5277, 0.4694, 0.4857, 0.5959, 0.5522, 0.5539],
       device='cuda:0') torch.Size([16])
percent tensor([0.6546, 0.6682, 0.6259, 0.6175, 0.6191, 0.6684, 0.6483, 0.6057, 0.6455,
        0.6669, 0.6622, 0.6506, 0.6863, 0.6476, 0.6571, 0.6674],
       device='cuda:0') torch.Size([16])
percent tensor([0.6567, 0.5653, 0.6892, 0.7113, 0.7266, 0.7624, 0.6517, 0.6796, 0.7187,
        0.6210, 0.6749, 0.6621, 0.5534, 0.6975, 0.6250, 0.7061],
       device='cuda:0') torch.Size([16])
percent tensor([0.7455, 0.7449, 0.7747, 0.7698, 0.8065, 0.7906, 0.7751, 0.7617, 0.7616,
        0.7565, 0.7607, 0.7426, 0.7335, 0.7706, 0.7349, 0.7724],
       device='cuda:0') torch.Size([16])
percent tensor([0.4463, 0.5370, 0.5560, 0.4811, 0.6116, 0.6518, 0.5370, 0.3994, 0.5357,
        0.4971, 0.4988, 0.4431, 0.4705, 0.4801, 0.3523, 0.4892],
       device='cuda:0') torch.Size([16])
percent tensor([0.9996, 0.9996, 0.9992, 0.9996, 0.9992, 0.9986, 0.9996, 0.9996, 0.9998,
        0.9995, 0.9998, 0.9997, 0.9994, 0.9991, 0.9989, 0.9998],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 127 | Batch_idx: 0 |  Loss: (0.1561) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 127 | Batch_idx: 10 |  Loss: (0.1629) |  Loss2: (0.0000) | Acc: (94.00%) (1329/1408)
Epoch: 127 | Batch_idx: 20 |  Loss: (0.1635) |  Loss2: (0.0000) | Acc: (94.00%) (2531/2688)
Epoch: 127 | Batch_idx: 30 |  Loss: (0.1871) |  Loss2: (0.0000) | Acc: (93.00%) (3696/3968)
Epoch: 127 | Batch_idx: 40 |  Loss: (0.1915) |  Loss2: (0.0000) | Acc: (92.00%) (4878/5248)
Epoch: 127 | Batch_idx: 50 |  Loss: (0.1886) |  Loss2: (0.0000) | Acc: (93.00%) (6083/6528)
Epoch: 127 | Batch_idx: 60 |  Loss: (0.1869) |  Loss2: (0.0000) | Acc: (93.00%) (7279/7808)
Epoch: 127 | Batch_idx: 70 |  Loss: (0.1838) |  Loss2: (0.0000) | Acc: (93.00%) (8489/9088)
Epoch: 127 | Batch_idx: 80 |  Loss: (0.1819) |  Loss2: (0.0000) | Acc: (93.00%) (9685/10368)
Epoch: 127 | Batch_idx: 90 |  Loss: (0.1813) |  Loss2: (0.0000) | Acc: (93.00%) (10890/11648)
Epoch: 127 | Batch_idx: 100 |  Loss: (0.1805) |  Loss2: (0.0000) | Acc: (93.00%) (12097/12928)
Epoch: 127 | Batch_idx: 110 |  Loss: (0.1814) |  Loss2: (0.0000) | Acc: (93.00%) (13286/14208)
Epoch: 127 | Batch_idx: 120 |  Loss: (0.1820) |  Loss2: (0.0000) | Acc: (93.00%) (14480/15488)
Epoch: 127 | Batch_idx: 130 |  Loss: (0.1800) |  Loss2: (0.0000) | Acc: (93.00%) (15690/16768)
Epoch: 127 | Batch_idx: 140 |  Loss: (0.1791) |  Loss2: (0.0000) | Acc: (93.00%) (16887/18048)
Epoch: 127 | Batch_idx: 150 |  Loss: (0.1804) |  Loss2: (0.0000) | Acc: (93.00%) (18074/19328)
Epoch: 127 | Batch_idx: 160 |  Loss: (0.1799) |  Loss2: (0.0000) | Acc: (93.00%) (19270/20608)
Epoch: 127 | Batch_idx: 170 |  Loss: (0.1794) |  Loss2: (0.0000) | Acc: (93.00%) (20475/21888)
Epoch: 127 | Batch_idx: 180 |  Loss: (0.1788) |  Loss2: (0.0000) | Acc: (93.00%) (21670/23168)
Epoch: 127 | Batch_idx: 190 |  Loss: (0.1775) |  Loss2: (0.0000) | Acc: (93.00%) (22884/24448)
Epoch: 127 | Batch_idx: 200 |  Loss: (0.1786) |  Loss2: (0.0000) | Acc: (93.00%) (24076/25728)
Epoch: 127 | Batch_idx: 210 |  Loss: (0.1769) |  Loss2: (0.0000) | Acc: (93.00%) (25287/27008)
Epoch: 127 | Batch_idx: 220 |  Loss: (0.1768) |  Loss2: (0.0000) | Acc: (93.00%) (26481/28288)
Epoch: 127 | Batch_idx: 230 |  Loss: (0.1766) |  Loss2: (0.0000) | Acc: (93.00%) (27689/29568)
Epoch: 127 | Batch_idx: 240 |  Loss: (0.1768) |  Loss2: (0.0000) | Acc: (93.00%) (28878/30848)
Epoch: 127 | Batch_idx: 250 |  Loss: (0.1765) |  Loss2: (0.0000) | Acc: (93.00%) (30080/32128)
Epoch: 127 | Batch_idx: 260 |  Loss: (0.1773) |  Loss2: (0.0000) | Acc: (93.00%) (31274/33408)
Epoch: 127 | Batch_idx: 270 |  Loss: (0.1768) |  Loss2: (0.0000) | Acc: (93.00%) (32481/34688)
Epoch: 127 | Batch_idx: 280 |  Loss: (0.1764) |  Loss2: (0.0000) | Acc: (93.00%) (33686/35968)
Epoch: 127 | Batch_idx: 290 |  Loss: (0.1763) |  Loss2: (0.0000) | Acc: (93.00%) (34883/37248)
Epoch: 127 | Batch_idx: 300 |  Loss: (0.1771) |  Loss2: (0.0000) | Acc: (93.00%) (36073/38528)
Epoch: 127 | Batch_idx: 310 |  Loss: (0.1778) |  Loss2: (0.0000) | Acc: (93.00%) (37259/39808)
Epoch: 127 | Batch_idx: 320 |  Loss: (0.1773) |  Loss2: (0.0000) | Acc: (93.00%) (38469/41088)
Epoch: 127 | Batch_idx: 330 |  Loss: (0.1770) |  Loss2: (0.0000) | Acc: (93.00%) (39667/42368)
Epoch: 127 | Batch_idx: 340 |  Loss: (0.1773) |  Loss2: (0.0000) | Acc: (93.00%) (40862/43648)
Epoch: 127 | Batch_idx: 350 |  Loss: (0.1767) |  Loss2: (0.0000) | Acc: (93.00%) (42066/44928)
Epoch: 127 | Batch_idx: 360 |  Loss: (0.1769) |  Loss2: (0.0000) | Acc: (93.00%) (43270/46208)
Epoch: 127 | Batch_idx: 370 |  Loss: (0.1761) |  Loss2: (0.0000) | Acc: (93.00%) (44493/47488)
Epoch: 127 | Batch_idx: 380 |  Loss: (0.1753) |  Loss2: (0.0000) | Acc: (93.00%) (45710/48768)
Epoch: 127 | Batch_idx: 390 |  Loss: (0.1750) |  Loss2: (0.0000) | Acc: (93.00%) (46871/50000)
# TEST : Loss: (0.3907) | Acc: (87.00%) (8765/10000)
percent tensor([0.5621, 0.5719, 0.5682, 0.5559, 0.5711, 0.5491, 0.5764, 0.5730, 0.5749,
        0.5712, 0.5697, 0.5723, 0.5677, 0.5714, 0.5616, 0.5581],
       device='cuda:0') torch.Size([16])
percent tensor([0.5410, 0.5400, 0.5141, 0.5353, 0.5430, 0.5209, 0.5448, 0.5613, 0.5345,
        0.5175, 0.5113, 0.5070, 0.5344, 0.5448, 0.5464, 0.5311],
       device='cuda:0') torch.Size([16])
percent tensor([0.5335, 0.4889, 0.4667, 0.5375, 0.4672, 0.5648, 0.5089, 0.5286, 0.5254,
        0.4677, 0.5104, 0.4596, 0.4677, 0.5889, 0.5396, 0.5373],
       device='cuda:0') torch.Size([16])
percent tensor([0.6472, 0.6609, 0.6218, 0.6103, 0.6178, 0.6589, 0.6443, 0.6015, 0.6389,
        0.6603, 0.6547, 0.6467, 0.6776, 0.6372, 0.6514, 0.6580],
       device='cuda:0') torch.Size([16])
percent tensor([0.6628, 0.5539, 0.7066, 0.7283, 0.7476, 0.7623, 0.6533, 0.6989, 0.7249,
        0.6199, 0.6672, 0.6711, 0.5436, 0.6900, 0.6173, 0.7003],
       device='cuda:0') torch.Size([16])
percent tensor([0.7371, 0.7374, 0.7631, 0.7626, 0.7966, 0.7830, 0.7684, 0.7484, 0.7558,
        0.7498, 0.7543, 0.7289, 0.7297, 0.7666, 0.7223, 0.7638],
       device='cuda:0') torch.Size([16])
percent tensor([0.4492, 0.5464, 0.5163, 0.4415, 0.5616, 0.6331, 0.5271, 0.3574, 0.5338,
        0.5183, 0.4962, 0.4187, 0.4946, 0.4808, 0.3345, 0.4862],
       device='cuda:0') torch.Size([16])
percent tensor([0.9995, 0.9997, 0.9991, 0.9995, 0.9991, 0.9989, 0.9996, 0.9996, 0.9997,
        0.9996, 0.9998, 0.9996, 0.9995, 0.9992, 0.9990, 0.9998],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 128 | Batch_idx: 0 |  Loss: (0.1004) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 128 | Batch_idx: 10 |  Loss: (0.1461) |  Loss2: (0.0000) | Acc: (95.00%) (1345/1408)
Epoch: 128 | Batch_idx: 20 |  Loss: (0.1472) |  Loss2: (0.0000) | Acc: (95.00%) (2564/2688)
Epoch: 128 | Batch_idx: 30 |  Loss: (0.1430) |  Loss2: (0.0000) | Acc: (95.00%) (3785/3968)
Epoch: 128 | Batch_idx: 40 |  Loss: (0.1484) |  Loss2: (0.0000) | Acc: (95.00%) (4989/5248)
Epoch: 128 | Batch_idx: 50 |  Loss: (0.1473) |  Loss2: (0.0000) | Acc: (94.00%) (6200/6528)
Epoch: 128 | Batch_idx: 60 |  Loss: (0.1449) |  Loss2: (0.0000) | Acc: (95.00%) (7419/7808)
Epoch: 128 | Batch_idx: 70 |  Loss: (0.1417) |  Loss2: (0.0000) | Acc: (95.00%) (8649/9088)
Epoch: 128 | Batch_idx: 80 |  Loss: (0.1429) |  Loss2: (0.0000) | Acc: (95.00%) (9862/10368)
Epoch: 128 | Batch_idx: 90 |  Loss: (0.1435) |  Loss2: (0.0000) | Acc: (95.00%) (11069/11648)
Epoch: 128 | Batch_idx: 100 |  Loss: (0.1435) |  Loss2: (0.0000) | Acc: (95.00%) (12290/12928)
Epoch: 128 | Batch_idx: 110 |  Loss: (0.1452) |  Loss2: (0.0000) | Acc: (95.00%) (13499/14208)
Epoch: 128 | Batch_idx: 120 |  Loss: (0.1426) |  Loss2: (0.0000) | Acc: (95.00%) (14732/15488)
Epoch: 128 | Batch_idx: 130 |  Loss: (0.1415) |  Loss2: (0.0000) | Acc: (95.00%) (15951/16768)
Epoch: 128 | Batch_idx: 140 |  Loss: (0.1418) |  Loss2: (0.0000) | Acc: (95.00%) (17174/18048)
Epoch: 128 | Batch_idx: 150 |  Loss: (0.1423) |  Loss2: (0.0000) | Acc: (95.00%) (18395/19328)
Epoch: 128 | Batch_idx: 160 |  Loss: (0.1418) |  Loss2: (0.0000) | Acc: (95.00%) (19626/20608)
Epoch: 128 | Batch_idx: 170 |  Loss: (0.1426) |  Loss2: (0.0000) | Acc: (95.00%) (20837/21888)
Epoch: 128 | Batch_idx: 180 |  Loss: (0.1435) |  Loss2: (0.0000) | Acc: (95.00%) (22040/23168)
Epoch: 128 | Batch_idx: 190 |  Loss: (0.1443) |  Loss2: (0.0000) | Acc: (95.00%) (23253/24448)
Epoch: 128 | Batch_idx: 200 |  Loss: (0.1450) |  Loss2: (0.0000) | Acc: (95.00%) (24474/25728)
Epoch: 128 | Batch_idx: 210 |  Loss: (0.1459) |  Loss2: (0.0000) | Acc: (95.00%) (25682/27008)
Epoch: 128 | Batch_idx: 220 |  Loss: (0.1462) |  Loss2: (0.0000) | Acc: (95.00%) (26896/28288)
Epoch: 128 | Batch_idx: 230 |  Loss: (0.1454) |  Loss2: (0.0000) | Acc: (95.00%) (28122/29568)
Epoch: 128 | Batch_idx: 240 |  Loss: (0.1470) |  Loss2: (0.0000) | Acc: (95.00%) (29316/30848)
Epoch: 128 | Batch_idx: 250 |  Loss: (0.1482) |  Loss2: (0.0000) | Acc: (95.00%) (30523/32128)
Epoch: 128 | Batch_idx: 260 |  Loss: (0.1485) |  Loss2: (0.0000) | Acc: (94.00%) (31732/33408)
Epoch: 128 | Batch_idx: 270 |  Loss: (0.1487) |  Loss2: (0.0000) | Acc: (94.00%) (32943/34688)
Epoch: 128 | Batch_idx: 280 |  Loss: (0.1487) |  Loss2: (0.0000) | Acc: (94.00%) (34157/35968)
Epoch: 128 | Batch_idx: 290 |  Loss: (0.1478) |  Loss2: (0.0000) | Acc: (94.00%) (35382/37248)
Epoch: 128 | Batch_idx: 300 |  Loss: (0.1475) |  Loss2: (0.0000) | Acc: (95.00%) (36604/38528)
Epoch: 128 | Batch_idx: 310 |  Loss: (0.1474) |  Loss2: (0.0000) | Acc: (95.00%) (37825/39808)
Epoch: 128 | Batch_idx: 320 |  Loss: (0.1474) |  Loss2: (0.0000) | Acc: (95.00%) (39039/41088)
Epoch: 128 | Batch_idx: 330 |  Loss: (0.1482) |  Loss2: (0.0000) | Acc: (94.00%) (40238/42368)
Epoch: 128 | Batch_idx: 340 |  Loss: (0.1478) |  Loss2: (0.0000) | Acc: (94.00%) (41463/43648)
Epoch: 128 | Batch_idx: 350 |  Loss: (0.1472) |  Loss2: (0.0000) | Acc: (95.00%) (42684/44928)
Epoch: 128 | Batch_idx: 360 |  Loss: (0.1469) |  Loss2: (0.0000) | Acc: (95.00%) (43904/46208)
Epoch: 128 | Batch_idx: 370 |  Loss: (0.1471) |  Loss2: (0.0000) | Acc: (94.00%) (45113/47488)
Epoch: 128 | Batch_idx: 380 |  Loss: (0.1474) |  Loss2: (0.0000) | Acc: (94.00%) (46316/48768)
Epoch: 128 | Batch_idx: 390 |  Loss: (0.1482) |  Loss2: (0.0000) | Acc: (94.00%) (47471/50000)
# TEST : Loss: (0.4556) | Acc: (85.00%) (8585/10000)
percent tensor([0.5610, 0.5717, 0.5661, 0.5538, 0.5682, 0.5478, 0.5758, 0.5724, 0.5747,
        0.5707, 0.5702, 0.5710, 0.5667, 0.5718, 0.5613, 0.5576],
       device='cuda:0') torch.Size([16])
percent tensor([0.5412, 0.5442, 0.5118, 0.5403, 0.5413, 0.5236, 0.5469, 0.5661, 0.5345,
        0.5190, 0.5129, 0.5067, 0.5354, 0.5470, 0.5507, 0.5331],
       device='cuda:0') torch.Size([16])
percent tensor([0.5323, 0.4885, 0.4699, 0.5483, 0.4698, 0.5620, 0.5084, 0.5284, 0.5238,
        0.4740, 0.5056, 0.4643, 0.4641, 0.5891, 0.5393, 0.5377],
       device='cuda:0') torch.Size([16])
percent tensor([0.6456, 0.6598, 0.6219, 0.6117, 0.6157, 0.6568, 0.6434, 0.6088, 0.6364,
        0.6582, 0.6526, 0.6434, 0.6736, 0.6339, 0.6521, 0.6571],
       device='cuda:0') torch.Size([16])
percent tensor([0.6548, 0.5584, 0.6696, 0.6984, 0.7313, 0.7651, 0.6503, 0.6867, 0.7238,
        0.6128, 0.6723, 0.6148, 0.5517, 0.7046, 0.6171, 0.7027],
       device='cuda:0') torch.Size([16])
percent tensor([0.7390, 0.7340, 0.7659, 0.7621, 0.7987, 0.7873, 0.7638, 0.7442, 0.7540,
        0.7456, 0.7551, 0.7304, 0.7307, 0.7680, 0.7188, 0.7665],
       device='cuda:0') torch.Size([16])
percent tensor([0.4616, 0.5436, 0.5298, 0.4533, 0.5622, 0.6463, 0.5454, 0.3637, 0.5370,
        0.5110, 0.5145, 0.4390, 0.5014, 0.5191, 0.3399, 0.5026],
       device='cuda:0') torch.Size([16])
percent tensor([0.9995, 0.9996, 0.9995, 0.9997, 0.9996, 0.9983, 0.9997, 0.9997, 0.9997,
        0.9996, 0.9998, 0.9997, 0.9995, 0.9988, 0.9989, 0.9994],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 129 | Batch_idx: 0 |  Loss: (0.2323) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 129 | Batch_idx: 10 |  Loss: (0.1757) |  Loss2: (0.0000) | Acc: (93.00%) (1323/1408)
Epoch: 129 | Batch_idx: 20 |  Loss: (0.1930) |  Loss2: (0.0000) | Acc: (93.00%) (2507/2688)
Epoch: 129 | Batch_idx: 30 |  Loss: (0.1985) |  Loss2: (0.0000) | Acc: (93.00%) (3695/3968)
Epoch: 129 | Batch_idx: 40 |  Loss: (0.1992) |  Loss2: (0.0000) | Acc: (93.00%) (4893/5248)
Epoch: 129 | Batch_idx: 50 |  Loss: (0.2006) |  Loss2: (0.0000) | Acc: (93.00%) (6078/6528)
Epoch: 129 | Batch_idx: 60 |  Loss: (0.1996) |  Loss2: (0.0000) | Acc: (93.00%) (7262/7808)
Epoch: 129 | Batch_idx: 70 |  Loss: (0.1979) |  Loss2: (0.0000) | Acc: (93.00%) (8458/9088)
Epoch: 129 | Batch_idx: 80 |  Loss: (0.1962) |  Loss2: (0.0000) | Acc: (93.00%) (9655/10368)
Epoch: 129 | Batch_idx: 90 |  Loss: (0.2002) |  Loss2: (0.0000) | Acc: (92.00%) (10824/11648)
Epoch: 129 | Batch_idx: 100 |  Loss: (0.2006) |  Loss2: (0.0000) | Acc: (92.00%) (12009/12928)
Epoch: 129 | Batch_idx: 110 |  Loss: (0.1989) |  Loss2: (0.0000) | Acc: (92.00%) (13210/14208)
Epoch: 129 | Batch_idx: 120 |  Loss: (0.1987) |  Loss2: (0.0000) | Acc: (92.00%) (14396/15488)
Epoch: 129 | Batch_idx: 130 |  Loss: (0.1972) |  Loss2: (0.0000) | Acc: (93.00%) (15595/16768)
Epoch: 129 | Batch_idx: 140 |  Loss: (0.1959) |  Loss2: (0.0000) | Acc: (93.00%) (16795/18048)
Epoch: 129 | Batch_idx: 150 |  Loss: (0.1946) |  Loss2: (0.0000) | Acc: (93.00%) (17993/19328)
Epoch: 129 | Batch_idx: 160 |  Loss: (0.1936) |  Loss2: (0.0000) | Acc: (93.00%) (19183/20608)
Epoch: 129 | Batch_idx: 170 |  Loss: (0.1931) |  Loss2: (0.0000) | Acc: (93.00%) (20375/21888)
Epoch: 129 | Batch_idx: 180 |  Loss: (0.1922) |  Loss2: (0.0000) | Acc: (93.00%) (21578/23168)
Epoch: 129 | Batch_idx: 190 |  Loss: (0.1916) |  Loss2: (0.0000) | Acc: (93.00%) (22776/24448)
Epoch: 129 | Batch_idx: 200 |  Loss: (0.1898) |  Loss2: (0.0000) | Acc: (93.00%) (23985/25728)
Epoch: 129 | Batch_idx: 210 |  Loss: (0.1893) |  Loss2: (0.0000) | Acc: (93.00%) (25180/27008)
Epoch: 129 | Batch_idx: 220 |  Loss: (0.1888) |  Loss2: (0.0000) | Acc: (93.00%) (26372/28288)
Epoch: 129 | Batch_idx: 230 |  Loss: (0.1887) |  Loss2: (0.0000) | Acc: (93.00%) (27565/29568)
Epoch: 129 | Batch_idx: 240 |  Loss: (0.1882) |  Loss2: (0.0000) | Acc: (93.00%) (28765/30848)
Epoch: 129 | Batch_idx: 250 |  Loss: (0.1884) |  Loss2: (0.0000) | Acc: (93.00%) (29959/32128)
Epoch: 129 | Batch_idx: 260 |  Loss: (0.1881) |  Loss2: (0.0000) | Acc: (93.00%) (31157/33408)
Epoch: 129 | Batch_idx: 270 |  Loss: (0.1883) |  Loss2: (0.0000) | Acc: (93.00%) (32351/34688)
Epoch: 129 | Batch_idx: 280 |  Loss: (0.1879) |  Loss2: (0.0000) | Acc: (93.00%) (33546/35968)
Epoch: 129 | Batch_idx: 290 |  Loss: (0.1878) |  Loss2: (0.0000) | Acc: (93.00%) (34745/37248)
Epoch: 129 | Batch_idx: 300 |  Loss: (0.1880) |  Loss2: (0.0000) | Acc: (93.00%) (35943/38528)
Epoch: 129 | Batch_idx: 310 |  Loss: (0.1880) |  Loss2: (0.0000) | Acc: (93.00%) (37135/39808)
Epoch: 129 | Batch_idx: 320 |  Loss: (0.1874) |  Loss2: (0.0000) | Acc: (93.00%) (38338/41088)
Epoch: 129 | Batch_idx: 330 |  Loss: (0.1875) |  Loss2: (0.0000) | Acc: (93.00%) (39532/42368)
Epoch: 129 | Batch_idx: 340 |  Loss: (0.1869) |  Loss2: (0.0000) | Acc: (93.00%) (40741/43648)
Epoch: 129 | Batch_idx: 350 |  Loss: (0.1857) |  Loss2: (0.0000) | Acc: (93.00%) (41959/44928)
Epoch: 129 | Batch_idx: 360 |  Loss: (0.1853) |  Loss2: (0.0000) | Acc: (93.00%) (43166/46208)
Epoch: 129 | Batch_idx: 370 |  Loss: (0.1850) |  Loss2: (0.0000) | Acc: (93.00%) (44365/47488)
Epoch: 129 | Batch_idx: 380 |  Loss: (0.1844) |  Loss2: (0.0000) | Acc: (93.00%) (45577/48768)
Epoch: 129 | Batch_idx: 390 |  Loss: (0.1844) |  Loss2: (0.0000) | Acc: (93.00%) (46733/50000)
# TEST : Loss: (0.4103) | Acc: (87.00%) (8705/10000)
percent tensor([0.5608, 0.5710, 0.5684, 0.5542, 0.5696, 0.5475, 0.5761, 0.5731, 0.5752,
        0.5708, 0.5699, 0.5722, 0.5669, 0.5711, 0.5607, 0.5572],
       device='cuda:0') torch.Size([16])
percent tensor([0.5214, 0.5247, 0.4911, 0.5160, 0.5199, 0.5009, 0.5261, 0.5431, 0.5145,
        0.4986, 0.4928, 0.4868, 0.5174, 0.5283, 0.5293, 0.5113],
       device='cuda:0') torch.Size([16])
percent tensor([0.5131, 0.4647, 0.4402, 0.5250, 0.4382, 0.5485, 0.4789, 0.4995, 0.5016,
        0.4468, 0.4876, 0.4300, 0.4477, 0.5732, 0.5152, 0.5164],
       device='cuda:0') torch.Size([16])
percent tensor([0.6675, 0.6878, 0.6453, 0.6345, 0.6353, 0.6771, 0.6678, 0.6292, 0.6595,
        0.6889, 0.6801, 0.6725, 0.6989, 0.6631, 0.6744, 0.6837],
       device='cuda:0') torch.Size([16])
percent tensor([0.6777, 0.5959, 0.6924, 0.7217, 0.7487, 0.7852, 0.6724, 0.6934, 0.7493,
        0.6500, 0.7052, 0.6568, 0.5884, 0.7363, 0.6320, 0.7328],
       device='cuda:0') torch.Size([16])
percent tensor([0.7657, 0.7589, 0.7841, 0.7847, 0.8212, 0.8076, 0.7880, 0.7716, 0.7830,
        0.7711, 0.7826, 0.7497, 0.7571, 0.8008, 0.7448, 0.7917],
       device='cuda:0') torch.Size([16])
percent tensor([0.4866, 0.5554, 0.5572, 0.4709, 0.5935, 0.6566, 0.5605, 0.4033, 0.5603,
        0.5250, 0.5241, 0.4358, 0.5137, 0.5162, 0.3537, 0.5165],
       device='cuda:0') torch.Size([16])
percent tensor([0.9996, 0.9996, 0.9995, 0.9995, 0.9996, 0.9984, 0.9997, 0.9996, 0.9995,
        0.9997, 0.9998, 0.9997, 0.9994, 0.9988, 0.9989, 0.9996],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 130 | Batch_idx: 0 |  Loss: (0.1298) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 130 | Batch_idx: 10 |  Loss: (0.1460) |  Loss2: (0.0000) | Acc: (95.00%) (1349/1408)
Epoch: 130 | Batch_idx: 20 |  Loss: (0.1380) |  Loss2: (0.0000) | Acc: (95.00%) (2576/2688)
Epoch: 130 | Batch_idx: 30 |  Loss: (0.1341) |  Loss2: (0.0000) | Acc: (95.00%) (3800/3968)
Epoch: 130 | Batch_idx: 40 |  Loss: (0.1340) |  Loss2: (0.0000) | Acc: (95.00%) (5020/5248)
Epoch: 130 | Batch_idx: 50 |  Loss: (0.1332) |  Loss2: (0.0000) | Acc: (95.00%) (6241/6528)
Epoch: 130 | Batch_idx: 60 |  Loss: (0.1347) |  Loss2: (0.0000) | Acc: (95.00%) (7453/7808)
Epoch: 130 | Batch_idx: 70 |  Loss: (0.1384) |  Loss2: (0.0000) | Acc: (95.00%) (8666/9088)
Epoch: 130 | Batch_idx: 80 |  Loss: (0.1396) |  Loss2: (0.0000) | Acc: (95.00%) (9875/10368)
Epoch: 130 | Batch_idx: 90 |  Loss: (0.1424) |  Loss2: (0.0000) | Acc: (95.00%) (11081/11648)
Epoch: 130 | Batch_idx: 100 |  Loss: (0.1418) |  Loss2: (0.0000) | Acc: (95.00%) (12304/12928)
Epoch: 130 | Batch_idx: 110 |  Loss: (0.1444) |  Loss2: (0.0000) | Acc: (95.00%) (13513/14208)
Epoch: 130 | Batch_idx: 120 |  Loss: (0.1438) |  Loss2: (0.0000) | Acc: (95.00%) (14726/15488)
Epoch: 130 | Batch_idx: 130 |  Loss: (0.1444) |  Loss2: (0.0000) | Acc: (95.00%) (15940/16768)
Epoch: 130 | Batch_idx: 140 |  Loss: (0.1442) |  Loss2: (0.0000) | Acc: (95.00%) (17154/18048)
Epoch: 130 | Batch_idx: 150 |  Loss: (0.1452) |  Loss2: (0.0000) | Acc: (94.00%) (18356/19328)
Epoch: 130 | Batch_idx: 160 |  Loss: (0.1449) |  Loss2: (0.0000) | Acc: (94.00%) (19573/20608)
Epoch: 130 | Batch_idx: 170 |  Loss: (0.1454) |  Loss2: (0.0000) | Acc: (94.00%) (20780/21888)
Epoch: 130 | Batch_idx: 180 |  Loss: (0.1464) |  Loss2: (0.0000) | Acc: (94.00%) (21989/23168)
Epoch: 130 | Batch_idx: 190 |  Loss: (0.1463) |  Loss2: (0.0000) | Acc: (94.00%) (23200/24448)
Epoch: 130 | Batch_idx: 200 |  Loss: (0.1455) |  Loss2: (0.0000) | Acc: (94.00%) (24422/25728)
Epoch: 130 | Batch_idx: 210 |  Loss: (0.1455) |  Loss2: (0.0000) | Acc: (94.00%) (25638/27008)
Epoch: 130 | Batch_idx: 220 |  Loss: (0.1453) |  Loss2: (0.0000) | Acc: (94.00%) (26853/28288)
Epoch: 130 | Batch_idx: 230 |  Loss: (0.1460) |  Loss2: (0.0000) | Acc: (94.00%) (28064/29568)
Epoch: 130 | Batch_idx: 240 |  Loss: (0.1458) |  Loss2: (0.0000) | Acc: (94.00%) (29277/30848)
Epoch: 130 | Batch_idx: 250 |  Loss: (0.1459) |  Loss2: (0.0000) | Acc: (94.00%) (30493/32128)
Epoch: 130 | Batch_idx: 260 |  Loss: (0.1464) |  Loss2: (0.0000) | Acc: (94.00%) (31708/33408)
Epoch: 130 | Batch_idx: 270 |  Loss: (0.1462) |  Loss2: (0.0000) | Acc: (94.00%) (32924/34688)
Epoch: 130 | Batch_idx: 280 |  Loss: (0.1457) |  Loss2: (0.0000) | Acc: (94.00%) (34150/35968)
Epoch: 130 | Batch_idx: 290 |  Loss: (0.1458) |  Loss2: (0.0000) | Acc: (94.00%) (35363/37248)
Epoch: 130 | Batch_idx: 300 |  Loss: (0.1456) |  Loss2: (0.0000) | Acc: (94.00%) (36577/38528)
Epoch: 130 | Batch_idx: 310 |  Loss: (0.1458) |  Loss2: (0.0000) | Acc: (94.00%) (37786/39808)
Epoch: 130 | Batch_idx: 320 |  Loss: (0.1462) |  Loss2: (0.0000) | Acc: (94.00%) (38992/41088)
Epoch: 130 | Batch_idx: 330 |  Loss: (0.1458) |  Loss2: (0.0000) | Acc: (94.00%) (40208/42368)
Epoch: 130 | Batch_idx: 340 |  Loss: (0.1463) |  Loss2: (0.0000) | Acc: (94.00%) (41419/43648)
Epoch: 130 | Batch_idx: 350 |  Loss: (0.1467) |  Loss2: (0.0000) | Acc: (94.00%) (42634/44928)
Epoch: 130 | Batch_idx: 360 |  Loss: (0.1471) |  Loss2: (0.0000) | Acc: (94.00%) (43839/46208)
Epoch: 130 | Batch_idx: 370 |  Loss: (0.1476) |  Loss2: (0.0000) | Acc: (94.00%) (45061/47488)
Epoch: 130 | Batch_idx: 380 |  Loss: (0.1470) |  Loss2: (0.0000) | Acc: (94.00%) (46285/48768)
Epoch: 130 | Batch_idx: 390 |  Loss: (0.1474) |  Loss2: (0.0000) | Acc: (94.00%) (47441/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_130.pth.tar'
# TEST : Loss: (0.3975) | Acc: (87.00%) (8744/10000)
percent tensor([0.5597, 0.5704, 0.5651, 0.5535, 0.5681, 0.5463, 0.5748, 0.5717, 0.5742,
        0.5699, 0.5688, 0.5702, 0.5658, 0.5712, 0.5599, 0.5562],
       device='cuda:0') torch.Size([16])
percent tensor([0.5202, 0.5201, 0.4980, 0.5159, 0.5250, 0.5025, 0.5262, 0.5420, 0.5145,
        0.4962, 0.4909, 0.4897, 0.5162, 0.5216, 0.5272, 0.5099],
       device='cuda:0') torch.Size([16])
percent tensor([0.5136, 0.4631, 0.4585, 0.5256, 0.4505, 0.5533, 0.4860, 0.5095, 0.4969,
        0.4463, 0.4868, 0.4428, 0.4426, 0.5695, 0.5141, 0.5184],
       device='cuda:0') torch.Size([16])
percent tensor([0.6673, 0.6828, 0.6482, 0.6325, 0.6369, 0.6689, 0.6639, 0.6286, 0.6615,
        0.6875, 0.6762, 0.6736, 0.7003, 0.6634, 0.6670, 0.6791],
       device='cuda:0') torch.Size([16])
percent tensor([0.6829, 0.5939, 0.7289, 0.7285, 0.7612, 0.7789, 0.6735, 0.6994, 0.7638,
        0.6697, 0.7104, 0.7156, 0.5949, 0.7448, 0.6269, 0.7188],
       device='cuda:0') torch.Size([16])
percent tensor([0.7650, 0.7606, 0.7897, 0.7868, 0.8169, 0.8100, 0.7909, 0.7639, 0.7948,
        0.7770, 0.7904, 0.7542, 0.7602, 0.8102, 0.7482, 0.7941],
       device='cuda:0') torch.Size([16])
percent tensor([0.4580, 0.5484, 0.5408, 0.4870, 0.5848, 0.6525, 0.5068, 0.3755, 0.5738,
        0.5139, 0.5073, 0.4271, 0.5156, 0.5002, 0.3224, 0.5102],
       device='cuda:0') torch.Size([16])
percent tensor([0.9996, 0.9997, 0.9988, 0.9996, 0.9992, 0.9990, 0.9996, 0.9997, 0.9995,
        0.9996, 0.9998, 0.9995, 0.9994, 0.9993, 0.9992, 0.9995],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(179.5937, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(817.3963, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(805.1938, device='cuda:0')
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1508.0284, device='cuda:0')
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(489.9064, device='cuda:0')
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2256.1240, device='cuda:0')
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4293.8657, device='cuda:0')
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1383.8293, device='cuda:0')
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6221.7441, device='cuda:0')
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11755.9424, device='cuda:0')
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3889.7986, device='cuda:0')
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16420.3301, device='cuda:0')
Epoch: 131 | Batch_idx: 0 |  Loss: (0.0784) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 131 | Batch_idx: 10 |  Loss: (0.1448) |  Loss2: (0.0000) | Acc: (95.00%) (1339/1408)
Epoch: 131 | Batch_idx: 20 |  Loss: (0.1720) |  Loss2: (0.0000) | Acc: (93.00%) (2521/2688)
Epoch: 131 | Batch_idx: 30 |  Loss: (0.1809) |  Loss2: (0.0000) | Acc: (93.00%) (3718/3968)
Epoch: 131 | Batch_idx: 40 |  Loss: (0.1832) |  Loss2: (0.0000) | Acc: (93.00%) (4910/5248)
Epoch: 131 | Batch_idx: 50 |  Loss: (0.1891) |  Loss2: (0.0000) | Acc: (93.00%) (6096/6528)
Epoch: 131 | Batch_idx: 60 |  Loss: (0.1877) |  Loss2: (0.0000) | Acc: (93.00%) (7291/7808)
Epoch: 131 | Batch_idx: 70 |  Loss: (0.1841) |  Loss2: (0.0000) | Acc: (93.00%) (8497/9088)
Epoch: 131 | Batch_idx: 80 |  Loss: (0.1862) |  Loss2: (0.0000) | Acc: (93.00%) (9689/10368)
Epoch: 131 | Batch_idx: 90 |  Loss: (0.1848) |  Loss2: (0.0000) | Acc: (93.00%) (10891/11648)
Epoch: 131 | Batch_idx: 100 |  Loss: (0.1834) |  Loss2: (0.0000) | Acc: (93.00%) (12096/12928)
Epoch: 131 | Batch_idx: 110 |  Loss: (0.1822) |  Loss2: (0.0000) | Acc: (93.00%) (13293/14208)
Epoch: 131 | Batch_idx: 120 |  Loss: (0.1824) |  Loss2: (0.0000) | Acc: (93.00%) (14494/15488)
Epoch: 131 | Batch_idx: 130 |  Loss: (0.1812) |  Loss2: (0.0000) | Acc: (93.00%) (15695/16768)
Epoch: 131 | Batch_idx: 140 |  Loss: (0.1800) |  Loss2: (0.0000) | Acc: (93.00%) (16887/18048)
Epoch: 131 | Batch_idx: 150 |  Loss: (0.1797) |  Loss2: (0.0000) | Acc: (93.00%) (18084/19328)
Epoch: 131 | Batch_idx: 160 |  Loss: (0.1799) |  Loss2: (0.0000) | Acc: (93.00%) (19288/20608)
Epoch: 131 | Batch_idx: 170 |  Loss: (0.1790) |  Loss2: (0.0000) | Acc: (93.00%) (20496/21888)
Epoch: 131 | Batch_idx: 180 |  Loss: (0.1796) |  Loss2: (0.0000) | Acc: (93.00%) (21691/23168)
Epoch: 131 | Batch_idx: 190 |  Loss: (0.1804) |  Loss2: (0.0000) | Acc: (93.00%) (22880/24448)
Epoch: 131 | Batch_idx: 200 |  Loss: (0.1805) |  Loss2: (0.0000) | Acc: (93.00%) (24079/25728)
Epoch: 131 | Batch_idx: 210 |  Loss: (0.1802) |  Loss2: (0.0000) | Acc: (93.00%) (25278/27008)
Epoch: 131 | Batch_idx: 220 |  Loss: (0.1791) |  Loss2: (0.0000) | Acc: (93.00%) (26498/28288)
Epoch: 131 | Batch_idx: 230 |  Loss: (0.1793) |  Loss2: (0.0000) | Acc: (93.00%) (27697/29568)
Epoch: 131 | Batch_idx: 240 |  Loss: (0.1786) |  Loss2: (0.0000) | Acc: (93.00%) (28913/30848)
Epoch: 131 | Batch_idx: 250 |  Loss: (0.1784) |  Loss2: (0.0000) | Acc: (93.00%) (30116/32128)
Epoch: 131 | Batch_idx: 260 |  Loss: (0.1788) |  Loss2: (0.0000) | Acc: (93.00%) (31315/33408)
Epoch: 131 | Batch_idx: 270 |  Loss: (0.1783) |  Loss2: (0.0000) | Acc: (93.00%) (32520/34688)
Epoch: 131 | Batch_idx: 280 |  Loss: (0.1784) |  Loss2: (0.0000) | Acc: (93.00%) (33717/35968)
Epoch: 131 | Batch_idx: 290 |  Loss: (0.1783) |  Loss2: (0.0000) | Acc: (93.00%) (34920/37248)
Epoch: 131 | Batch_idx: 300 |  Loss: (0.1772) |  Loss2: (0.0000) | Acc: (93.00%) (36141/38528)
Epoch: 131 | Batch_idx: 310 |  Loss: (0.1765) |  Loss2: (0.0000) | Acc: (93.00%) (37354/39808)
Epoch: 131 | Batch_idx: 320 |  Loss: (0.1758) |  Loss2: (0.0000) | Acc: (93.00%) (38568/41088)
Epoch: 131 | Batch_idx: 330 |  Loss: (0.1754) |  Loss2: (0.0000) | Acc: (93.00%) (39776/42368)
Epoch: 131 | Batch_idx: 340 |  Loss: (0.1752) |  Loss2: (0.0000) | Acc: (93.00%) (40981/43648)
Epoch: 131 | Batch_idx: 350 |  Loss: (0.1751) |  Loss2: (0.0000) | Acc: (93.00%) (42181/44928)
Epoch: 131 | Batch_idx: 360 |  Loss: (0.1747) |  Loss2: (0.0000) | Acc: (93.00%) (43395/46208)
Epoch: 131 | Batch_idx: 370 |  Loss: (0.1746) |  Loss2: (0.0000) | Acc: (93.00%) (44588/47488)
Epoch: 131 | Batch_idx: 380 |  Loss: (0.1745) |  Loss2: (0.0000) | Acc: (93.00%) (45797/48768)
Epoch: 131 | Batch_idx: 390 |  Loss: (0.1742) |  Loss2: (0.0000) | Acc: (93.00%) (46956/50000)
# TEST : Loss: (0.3863) | Acc: (87.00%) (8792/10000)
percent tensor([0.5595, 0.5686, 0.5664, 0.5523, 0.5678, 0.5453, 0.5737, 0.5713, 0.5735,
        0.5689, 0.5679, 0.5702, 0.5654, 0.5694, 0.5588, 0.5555],
       device='cuda:0') torch.Size([16])
percent tensor([0.5239, 0.5221, 0.4928, 0.5136, 0.5192, 0.5027, 0.5257, 0.5356, 0.5160,
        0.4982, 0.4964, 0.4875, 0.5209, 0.5277, 0.5269, 0.5120],
       device='cuda:0') torch.Size([16])
percent tensor([0.5149, 0.4579, 0.4557, 0.5280, 0.4451, 0.5566, 0.4764, 0.5031, 0.5004,
        0.4428, 0.4890, 0.4408, 0.4433, 0.5707, 0.5125, 0.5165],
       device='cuda:0') torch.Size([16])
percent tensor([0.6548, 0.6765, 0.6255, 0.6097, 0.6134, 0.6563, 0.6496, 0.6071, 0.6514,
        0.6776, 0.6691, 0.6538, 0.6934, 0.6562, 0.6504, 0.6687],
       device='cuda:0') torch.Size([16])
percent tensor([0.7081, 0.6318, 0.7274, 0.7236, 0.7491, 0.7952, 0.6993, 0.6984, 0.7754,
        0.6934, 0.7313, 0.7248, 0.6431, 0.7606, 0.6484, 0.7499],
       device='cuda:0') torch.Size([16])
percent tensor([0.7751, 0.7685, 0.7932, 0.7928, 0.8237, 0.8186, 0.8014, 0.7698, 0.7972,
        0.7847, 0.7949, 0.7647, 0.7676, 0.8141, 0.7598, 0.8054],
       device='cuda:0') torch.Size([16])
percent tensor([0.4491, 0.4966, 0.5593, 0.4867, 0.6230, 0.6469, 0.5006, 0.4191, 0.5293,
        0.4642, 0.4485, 0.4130, 0.4426, 0.4382, 0.3107, 0.4986],
       device='cuda:0') torch.Size([16])
percent tensor([0.9995, 0.9996, 0.9993, 0.9996, 0.9995, 0.9989, 0.9996, 0.9997, 0.9996,
        0.9996, 0.9997, 0.9996, 0.9994, 0.9990, 0.9991, 0.9995],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 132 | Batch_idx: 0 |  Loss: (0.1400) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 132 | Batch_idx: 10 |  Loss: (0.1315) |  Loss2: (0.0000) | Acc: (95.00%) (1344/1408)
Epoch: 132 | Batch_idx: 20 |  Loss: (0.1230) |  Loss2: (0.0000) | Acc: (96.00%) (2584/2688)
Epoch: 132 | Batch_idx: 30 |  Loss: (0.1211) |  Loss2: (0.0000) | Acc: (96.00%) (3814/3968)
Epoch: 132 | Batch_idx: 40 |  Loss: (0.1242) |  Loss2: (0.0000) | Acc: (95.00%) (5037/5248)
Epoch: 132 | Batch_idx: 50 |  Loss: (0.1308) |  Loss2: (0.0000) | Acc: (95.00%) (6250/6528)
Epoch: 132 | Batch_idx: 60 |  Loss: (0.1332) |  Loss2: (0.0000) | Acc: (95.00%) (7462/7808)
Epoch: 132 | Batch_idx: 70 |  Loss: (0.1356) |  Loss2: (0.0000) | Acc: (95.00%) (8681/9088)
Epoch: 132 | Batch_idx: 80 |  Loss: (0.1364) |  Loss2: (0.0000) | Acc: (95.00%) (9896/10368)
Epoch: 132 | Batch_idx: 90 |  Loss: (0.1381) |  Loss2: (0.0000) | Acc: (95.00%) (11112/11648)
Epoch: 132 | Batch_idx: 100 |  Loss: (0.1389) |  Loss2: (0.0000) | Acc: (95.00%) (12318/12928)
Epoch: 132 | Batch_idx: 110 |  Loss: (0.1403) |  Loss2: (0.0000) | Acc: (95.00%) (13530/14208)
Epoch: 132 | Batch_idx: 120 |  Loss: (0.1402) |  Loss2: (0.0000) | Acc: (95.00%) (14755/15488)
Epoch: 132 | Batch_idx: 130 |  Loss: (0.1402) |  Loss2: (0.0000) | Acc: (95.00%) (15972/16768)
Epoch: 132 | Batch_idx: 140 |  Loss: (0.1406) |  Loss2: (0.0000) | Acc: (95.00%) (17195/18048)
Epoch: 132 | Batch_idx: 150 |  Loss: (0.1391) |  Loss2: (0.0000) | Acc: (95.00%) (18425/19328)
Epoch: 132 | Batch_idx: 160 |  Loss: (0.1385) |  Loss2: (0.0000) | Acc: (95.00%) (19643/20608)
Epoch: 132 | Batch_idx: 170 |  Loss: (0.1375) |  Loss2: (0.0000) | Acc: (95.00%) (20877/21888)
Epoch: 132 | Batch_idx: 180 |  Loss: (0.1363) |  Loss2: (0.0000) | Acc: (95.00%) (22111/23168)
Epoch: 132 | Batch_idx: 190 |  Loss: (0.1365) |  Loss2: (0.0000) | Acc: (95.00%) (23319/24448)
Epoch: 132 | Batch_idx: 200 |  Loss: (0.1373) |  Loss2: (0.0000) | Acc: (95.00%) (24531/25728)
Epoch: 132 | Batch_idx: 210 |  Loss: (0.1364) |  Loss2: (0.0000) | Acc: (95.00%) (25766/27008)
Epoch: 132 | Batch_idx: 220 |  Loss: (0.1369) |  Loss2: (0.0000) | Acc: (95.00%) (26986/28288)
Epoch: 132 | Batch_idx: 230 |  Loss: (0.1375) |  Loss2: (0.0000) | Acc: (95.00%) (28196/29568)
Epoch: 132 | Batch_idx: 240 |  Loss: (0.1383) |  Loss2: (0.0000) | Acc: (95.00%) (29411/30848)
Epoch: 132 | Batch_idx: 250 |  Loss: (0.1391) |  Loss2: (0.0000) | Acc: (95.00%) (30615/32128)
Epoch: 132 | Batch_idx: 260 |  Loss: (0.1394) |  Loss2: (0.0000) | Acc: (95.00%) (31834/33408)
Epoch: 132 | Batch_idx: 270 |  Loss: (0.1393) |  Loss2: (0.0000) | Acc: (95.00%) (33056/34688)
Epoch: 132 | Batch_idx: 280 |  Loss: (0.1401) |  Loss2: (0.0000) | Acc: (95.00%) (34266/35968)
Epoch: 132 | Batch_idx: 290 |  Loss: (0.1404) |  Loss2: (0.0000) | Acc: (95.00%) (35477/37248)
Epoch: 132 | Batch_idx: 300 |  Loss: (0.1406) |  Loss2: (0.0000) | Acc: (95.00%) (36695/38528)
Epoch: 132 | Batch_idx: 310 |  Loss: (0.1401) |  Loss2: (0.0000) | Acc: (95.00%) (37918/39808)
Epoch: 132 | Batch_idx: 320 |  Loss: (0.1410) |  Loss2: (0.0000) | Acc: (95.00%) (39119/41088)
Epoch: 132 | Batch_idx: 330 |  Loss: (0.1407) |  Loss2: (0.0000) | Acc: (95.00%) (40338/42368)
Epoch: 132 | Batch_idx: 340 |  Loss: (0.1411) |  Loss2: (0.0000) | Acc: (95.00%) (41547/43648)
Epoch: 132 | Batch_idx: 350 |  Loss: (0.1410) |  Loss2: (0.0000) | Acc: (95.00%) (42759/44928)
Epoch: 132 | Batch_idx: 360 |  Loss: (0.1411) |  Loss2: (0.0000) | Acc: (95.00%) (43967/46208)
Epoch: 132 | Batch_idx: 370 |  Loss: (0.1418) |  Loss2: (0.0000) | Acc: (95.00%) (45179/47488)
Epoch: 132 | Batch_idx: 380 |  Loss: (0.1418) |  Loss2: (0.0000) | Acc: (95.00%) (46388/48768)
Epoch: 132 | Batch_idx: 390 |  Loss: (0.1419) |  Loss2: (0.0000) | Acc: (95.00%) (47554/50000)
# TEST : Loss: (0.4147) | Acc: (87.00%) (8743/10000)
percent tensor([0.5583, 0.5697, 0.5631, 0.5511, 0.5660, 0.5442, 0.5734, 0.5696, 0.5730,
        0.5681, 0.5674, 0.5675, 0.5645, 0.5716, 0.5587, 0.5549],
       device='cuda:0') torch.Size([16])
percent tensor([0.5238, 0.5208, 0.4895, 0.5139, 0.5147, 0.5024, 0.5241, 0.5376, 0.5164,
        0.4975, 0.4961, 0.4823, 0.5201, 0.5307, 0.5263, 0.5130],
       device='cuda:0') torch.Size([16])
percent tensor([0.5196, 0.4599, 0.4537, 0.5216, 0.4311, 0.5583, 0.4758, 0.5014, 0.5034,
        0.4479, 0.4948, 0.4423, 0.4477, 0.5749, 0.5113, 0.5187],
       device='cuda:0') torch.Size([16])
percent tensor([0.6511, 0.6745, 0.6313, 0.6200, 0.6219, 0.6618, 0.6479, 0.6058, 0.6468,
        0.6749, 0.6659, 0.6549, 0.6881, 0.6505, 0.6560, 0.6706],
       device='cuda:0') torch.Size([16])
percent tensor([0.7037, 0.6268, 0.7315, 0.7523, 0.7615, 0.7874, 0.7048, 0.7253, 0.7802,
        0.7047, 0.7315, 0.7170, 0.6110, 0.7620, 0.6540, 0.7570],
       device='cuda:0') torch.Size([16])
percent tensor([0.7714, 0.7673, 0.7970, 0.7965, 0.8239, 0.8086, 0.7998, 0.7800, 0.8005,
        0.7852, 0.7962, 0.7639, 0.7636, 0.8152, 0.7551, 0.7982],
       device='cuda:0') torch.Size([16])
percent tensor([0.4493, 0.5113, 0.5370, 0.4882, 0.5860, 0.6458, 0.5275, 0.4150, 0.5449,
        0.4907, 0.4655, 0.4384, 0.4766, 0.4832, 0.3511, 0.4927],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9998, 0.9994, 0.9997, 0.9992, 0.9992, 0.9996, 0.9998, 0.9996,
        0.9996, 0.9998, 0.9996, 0.9995, 0.9991, 0.9989, 0.9998],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 133 | Batch_idx: 0 |  Loss: (0.0618) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 133 | Batch_idx: 10 |  Loss: (0.1139) |  Loss2: (0.0000) | Acc: (95.00%) (1351/1408)
Epoch: 133 | Batch_idx: 20 |  Loss: (0.1353) |  Loss2: (0.0000) | Acc: (95.00%) (2560/2688)
Epoch: 133 | Batch_idx: 30 |  Loss: (0.1357) |  Loss2: (0.0000) | Acc: (95.00%) (3782/3968)
Epoch: 133 | Batch_idx: 40 |  Loss: (0.1494) |  Loss2: (0.0000) | Acc: (94.00%) (4970/5248)
Epoch: 133 | Batch_idx: 50 |  Loss: (0.1549) |  Loss2: (0.0000) | Acc: (94.00%) (6168/6528)
Epoch: 133 | Batch_idx: 60 |  Loss: (0.1551) |  Loss2: (0.0000) | Acc: (94.00%) (7379/7808)
Epoch: 133 | Batch_idx: 70 |  Loss: (0.1558) |  Loss2: (0.0000) | Acc: (94.00%) (8587/9088)
Epoch: 133 | Batch_idx: 80 |  Loss: (0.1529) |  Loss2: (0.0000) | Acc: (94.00%) (9808/10368)
Epoch: 133 | Batch_idx: 90 |  Loss: (0.1557) |  Loss2: (0.0000) | Acc: (94.00%) (11009/11648)
Epoch: 133 | Batch_idx: 100 |  Loss: (0.1558) |  Loss2: (0.0000) | Acc: (94.00%) (12218/12928)
Epoch: 133 | Batch_idx: 110 |  Loss: (0.1558) |  Loss2: (0.0000) | Acc: (94.00%) (13426/14208)
Epoch: 133 | Batch_idx: 120 |  Loss: (0.1562) |  Loss2: (0.0000) | Acc: (94.00%) (14636/15488)
Epoch: 133 | Batch_idx: 130 |  Loss: (0.1556) |  Loss2: (0.0000) | Acc: (94.00%) (15858/16768)
Epoch: 133 | Batch_idx: 140 |  Loss: (0.1551) |  Loss2: (0.0000) | Acc: (94.00%) (17073/18048)
Epoch: 133 | Batch_idx: 150 |  Loss: (0.1551) |  Loss2: (0.0000) | Acc: (94.00%) (18283/19328)
Epoch: 133 | Batch_idx: 160 |  Loss: (0.1557) |  Loss2: (0.0000) | Acc: (94.00%) (19488/20608)
Epoch: 133 | Batch_idx: 170 |  Loss: (0.1562) |  Loss2: (0.0000) | Acc: (94.00%) (20692/21888)
Epoch: 133 | Batch_idx: 180 |  Loss: (0.1562) |  Loss2: (0.0000) | Acc: (94.00%) (21895/23168)
Epoch: 133 | Batch_idx: 190 |  Loss: (0.1551) |  Loss2: (0.0000) | Acc: (94.00%) (23118/24448)
Epoch: 133 | Batch_idx: 200 |  Loss: (0.1559) |  Loss2: (0.0000) | Acc: (94.00%) (24313/25728)
Epoch: 133 | Batch_idx: 210 |  Loss: (0.1566) |  Loss2: (0.0000) | Acc: (94.00%) (25519/27008)
Epoch: 133 | Batch_idx: 220 |  Loss: (0.1563) |  Loss2: (0.0000) | Acc: (94.00%) (26731/28288)
Epoch: 133 | Batch_idx: 230 |  Loss: (0.1564) |  Loss2: (0.0000) | Acc: (94.00%) (27931/29568)
Epoch: 133 | Batch_idx: 240 |  Loss: (0.1565) |  Loss2: (0.0000) | Acc: (94.00%) (29141/30848)
Epoch: 133 | Batch_idx: 250 |  Loss: (0.1562) |  Loss2: (0.0000) | Acc: (94.00%) (30351/32128)
Epoch: 133 | Batch_idx: 260 |  Loss: (0.1571) |  Loss2: (0.0000) | Acc: (94.00%) (31551/33408)
Epoch: 133 | Batch_idx: 270 |  Loss: (0.1575) |  Loss2: (0.0000) | Acc: (94.00%) (32763/34688)
Epoch: 133 | Batch_idx: 280 |  Loss: (0.1570) |  Loss2: (0.0000) | Acc: (94.00%) (33974/35968)
Epoch: 133 | Batch_idx: 290 |  Loss: (0.1568) |  Loss2: (0.0000) | Acc: (94.00%) (35189/37248)
Epoch: 133 | Batch_idx: 300 |  Loss: (0.1567) |  Loss2: (0.0000) | Acc: (94.00%) (36407/38528)
Epoch: 133 | Batch_idx: 310 |  Loss: (0.1561) |  Loss2: (0.0000) | Acc: (94.00%) (37627/39808)
Epoch: 133 | Batch_idx: 320 |  Loss: (0.1569) |  Loss2: (0.0000) | Acc: (94.00%) (38827/41088)
Epoch: 133 | Batch_idx: 330 |  Loss: (0.1563) |  Loss2: (0.0000) | Acc: (94.00%) (40043/42368)
Epoch: 133 | Batch_idx: 340 |  Loss: (0.1568) |  Loss2: (0.0000) | Acc: (94.00%) (41233/43648)
Epoch: 133 | Batch_idx: 350 |  Loss: (0.1576) |  Loss2: (0.0000) | Acc: (94.00%) (42432/44928)
Epoch: 133 | Batch_idx: 360 |  Loss: (0.1574) |  Loss2: (0.0000) | Acc: (94.00%) (43650/46208)
Epoch: 133 | Batch_idx: 370 |  Loss: (0.1579) |  Loss2: (0.0000) | Acc: (94.00%) (44842/47488)
Epoch: 133 | Batch_idx: 380 |  Loss: (0.1575) |  Loss2: (0.0000) | Acc: (94.00%) (46059/48768)
Epoch: 133 | Batch_idx: 390 |  Loss: (0.1576) |  Loss2: (0.0000) | Acc: (94.00%) (47226/50000)
# TEST : Loss: (0.3843) | Acc: (88.00%) (8808/10000)
percent tensor([0.5658, 0.5779, 0.5695, 0.5580, 0.5746, 0.5510, 0.5822, 0.5773, 0.5810,
        0.5762, 0.5752, 0.5756, 0.5724, 0.5785, 0.5666, 0.5616],
       device='cuda:0') torch.Size([16])
percent tensor([0.5377, 0.5365, 0.5061, 0.5255, 0.5310, 0.5173, 0.5403, 0.5539, 0.5309,
        0.5132, 0.5121, 0.4994, 0.5360, 0.5411, 0.5410, 0.5260],
       device='cuda:0') torch.Size([16])
percent tensor([0.5224, 0.4628, 0.4453, 0.5176, 0.4274, 0.5667, 0.4763, 0.5014, 0.5033,
        0.4477, 0.4982, 0.4417, 0.4507, 0.5816, 0.5166, 0.5243],
       device='cuda:0') torch.Size([16])
percent tensor([0.6562, 0.6809, 0.6317, 0.6222, 0.6207, 0.6616, 0.6543, 0.6029, 0.6509,
        0.6816, 0.6725, 0.6635, 0.7008, 0.6537, 0.6599, 0.6723],
       device='cuda:0') torch.Size([16])
percent tensor([0.6884, 0.6161, 0.6958, 0.7084, 0.7063, 0.7557, 0.6849, 0.6688, 0.7563,
        0.6978, 0.7258, 0.7170, 0.6339, 0.7379, 0.6333, 0.7314],
       device='cuda:0') torch.Size([16])
percent tensor([0.7666, 0.7584, 0.7915, 0.7860, 0.8175, 0.8098, 0.7924, 0.7726, 0.7942,
        0.7762, 0.7899, 0.7521, 0.7558, 0.8094, 0.7448, 0.7925],
       device='cuda:0') torch.Size([16])
percent tensor([0.4667, 0.5314, 0.5729, 0.5264, 0.6351, 0.7047, 0.5544, 0.4491, 0.5847,
        0.4902, 0.4844, 0.4529, 0.4745, 0.5355, 0.3546, 0.5329],
       device='cuda:0') torch.Size([16])
percent tensor([0.9996, 0.9997, 0.9992, 0.9996, 0.9994, 0.9988, 0.9996, 0.9998, 0.9995,
        0.9995, 0.9998, 0.9996, 0.9994, 0.9990, 0.9988, 0.9996],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 134 | Batch_idx: 0 |  Loss: (0.1553) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 134 | Batch_idx: 10 |  Loss: (0.1376) |  Loss2: (0.0000) | Acc: (95.00%) (1348/1408)
Epoch: 134 | Batch_idx: 20 |  Loss: (0.1357) |  Loss2: (0.0000) | Acc: (95.00%) (2577/2688)
Epoch: 134 | Batch_idx: 30 |  Loss: (0.1324) |  Loss2: (0.0000) | Acc: (95.00%) (3803/3968)
Epoch: 134 | Batch_idx: 40 |  Loss: (0.1325) |  Loss2: (0.0000) | Acc: (95.00%) (5034/5248)
Epoch: 134 | Batch_idx: 50 |  Loss: (0.1344) |  Loss2: (0.0000) | Acc: (95.00%) (6258/6528)
Epoch: 134 | Batch_idx: 60 |  Loss: (0.1369) |  Loss2: (0.0000) | Acc: (95.00%) (7471/7808)
Epoch: 134 | Batch_idx: 70 |  Loss: (0.1376) |  Loss2: (0.0000) | Acc: (95.00%) (8680/9088)
Epoch: 134 | Batch_idx: 80 |  Loss: (0.1371) |  Loss2: (0.0000) | Acc: (95.00%) (9902/10368)
Epoch: 134 | Batch_idx: 90 |  Loss: (0.1367) |  Loss2: (0.0000) | Acc: (95.00%) (11120/11648)
Epoch: 134 | Batch_idx: 100 |  Loss: (0.1373) |  Loss2: (0.0000) | Acc: (95.00%) (12336/12928)
Epoch: 134 | Batch_idx: 110 |  Loss: (0.1381) |  Loss2: (0.0000) | Acc: (95.00%) (13541/14208)
Epoch: 134 | Batch_idx: 120 |  Loss: (0.1392) |  Loss2: (0.0000) | Acc: (95.00%) (14768/15488)
Epoch: 134 | Batch_idx: 130 |  Loss: (0.1398) |  Loss2: (0.0000) | Acc: (95.00%) (15987/16768)
Epoch: 134 | Batch_idx: 140 |  Loss: (0.1397) |  Loss2: (0.0000) | Acc: (95.00%) (17210/18048)
Epoch: 134 | Batch_idx: 150 |  Loss: (0.1390) |  Loss2: (0.0000) | Acc: (95.00%) (18436/19328)
Epoch: 134 | Batch_idx: 160 |  Loss: (0.1392) |  Loss2: (0.0000) | Acc: (95.00%) (19659/20608)
Epoch: 134 | Batch_idx: 170 |  Loss: (0.1393) |  Loss2: (0.0000) | Acc: (95.00%) (20879/21888)
Epoch: 134 | Batch_idx: 180 |  Loss: (0.1396) |  Loss2: (0.0000) | Acc: (95.00%) (22098/23168)
Epoch: 134 | Batch_idx: 190 |  Loss: (0.1395) |  Loss2: (0.0000) | Acc: (95.00%) (23317/24448)
Epoch: 134 | Batch_idx: 200 |  Loss: (0.1406) |  Loss2: (0.0000) | Acc: (95.00%) (24519/25728)
Epoch: 134 | Batch_idx: 210 |  Loss: (0.1403) |  Loss2: (0.0000) | Acc: (95.00%) (25741/27008)
Epoch: 134 | Batch_idx: 220 |  Loss: (0.1401) |  Loss2: (0.0000) | Acc: (95.00%) (26962/28288)
Epoch: 134 | Batch_idx: 230 |  Loss: (0.1400) |  Loss2: (0.0000) | Acc: (95.00%) (28182/29568)
Epoch: 134 | Batch_idx: 240 |  Loss: (0.1408) |  Loss2: (0.0000) | Acc: (95.00%) (29385/30848)
Epoch: 134 | Batch_idx: 250 |  Loss: (0.1409) |  Loss2: (0.0000) | Acc: (95.00%) (30592/32128)
Epoch: 134 | Batch_idx: 260 |  Loss: (0.1406) |  Loss2: (0.0000) | Acc: (95.00%) (31821/33408)
Epoch: 134 | Batch_idx: 270 |  Loss: (0.1401) |  Loss2: (0.0000) | Acc: (95.00%) (33047/34688)
Epoch: 134 | Batch_idx: 280 |  Loss: (0.1398) |  Loss2: (0.0000) | Acc: (95.00%) (34271/35968)
Epoch: 134 | Batch_idx: 290 |  Loss: (0.1402) |  Loss2: (0.0000) | Acc: (95.00%) (35473/37248)
Epoch: 134 | Batch_idx: 300 |  Loss: (0.1409) |  Loss2: (0.0000) | Acc: (95.00%) (36687/38528)
Epoch: 134 | Batch_idx: 310 |  Loss: (0.1410) |  Loss2: (0.0000) | Acc: (95.00%) (37907/39808)
Epoch: 134 | Batch_idx: 320 |  Loss: (0.1415) |  Loss2: (0.0000) | Acc: (95.00%) (39119/41088)
Epoch: 134 | Batch_idx: 330 |  Loss: (0.1416) |  Loss2: (0.0000) | Acc: (95.00%) (40330/42368)
Epoch: 134 | Batch_idx: 340 |  Loss: (0.1414) |  Loss2: (0.0000) | Acc: (95.00%) (41553/43648)
Epoch: 134 | Batch_idx: 350 |  Loss: (0.1415) |  Loss2: (0.0000) | Acc: (95.00%) (42768/44928)
Epoch: 134 | Batch_idx: 360 |  Loss: (0.1415) |  Loss2: (0.0000) | Acc: (95.00%) (43987/46208)
Epoch: 134 | Batch_idx: 370 |  Loss: (0.1416) |  Loss2: (0.0000) | Acc: (95.00%) (45212/47488)
Epoch: 134 | Batch_idx: 380 |  Loss: (0.1416) |  Loss2: (0.0000) | Acc: (95.00%) (46429/48768)
Epoch: 134 | Batch_idx: 390 |  Loss: (0.1412) |  Loss2: (0.0000) | Acc: (95.00%) (47605/50000)
# TEST : Loss: (0.3669) | Acc: (88.00%) (8817/10000)
percent tensor([0.5653, 0.5773, 0.5707, 0.5576, 0.5745, 0.5496, 0.5819, 0.5783, 0.5811,
        0.5762, 0.5752, 0.5764, 0.5718, 0.5781, 0.5659, 0.5616],
       device='cuda:0') torch.Size([16])
percent tensor([0.5383, 0.5355, 0.5042, 0.5291, 0.5288, 0.5170, 0.5384, 0.5540, 0.5273,
        0.5120, 0.5087, 0.4983, 0.5345, 0.5413, 0.5412, 0.5270],
       device='cuda:0') torch.Size([16])
percent tensor([0.5253, 0.4697, 0.4594, 0.5279, 0.4423, 0.5626, 0.4827, 0.5120, 0.5023,
        0.4562, 0.4934, 0.4486, 0.4487, 0.5824, 0.5199, 0.5260],
       device='cuda:0') torch.Size([16])
percent tensor([0.6516, 0.6792, 0.6333, 0.6135, 0.6198, 0.6553, 0.6554, 0.6015, 0.6464,
        0.6814, 0.6696, 0.6646, 0.6989, 0.6455, 0.6591, 0.6665],
       device='cuda:0') torch.Size([16])
percent tensor([0.6784, 0.6087, 0.7084, 0.6997, 0.7152, 0.7625, 0.6745, 0.6698, 0.7580,
        0.6923, 0.7147, 0.7224, 0.6349, 0.7332, 0.6319, 0.7274],
       device='cuda:0') torch.Size([16])
percent tensor([0.7662, 0.7549, 0.7924, 0.7846, 0.8184, 0.8190, 0.7896, 0.7635, 0.7975,
        0.7759, 0.7850, 0.7548, 0.7609, 0.8105, 0.7454, 0.7916],
       device='cuda:0') torch.Size([16])
percent tensor([0.4806, 0.5460, 0.5994, 0.5155, 0.6617, 0.7269, 0.5604, 0.4511, 0.5697,
        0.4908, 0.4660, 0.4339, 0.4831, 0.5314, 0.3519, 0.5220],
       device='cuda:0') torch.Size([16])
percent tensor([0.9996, 0.9997, 0.9994, 0.9997, 0.9993, 0.9991, 0.9995, 0.9995, 0.9994,
        0.9996, 0.9998, 0.9996, 0.9995, 0.9992, 0.9988, 0.9995],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 135 | Batch_idx: 0 |  Loss: (0.0952) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 135 | Batch_idx: 10 |  Loss: (0.1219) |  Loss2: (0.0000) | Acc: (95.00%) (1347/1408)
Epoch: 135 | Batch_idx: 20 |  Loss: (0.1284) |  Loss2: (0.0000) | Acc: (95.00%) (2561/2688)
Epoch: 135 | Batch_idx: 30 |  Loss: (0.1415) |  Loss2: (0.0000) | Acc: (94.00%) (3761/3968)
Epoch: 135 | Batch_idx: 40 |  Loss: (0.1366) |  Loss2: (0.0000) | Acc: (95.00%) (4988/5248)
Epoch: 135 | Batch_idx: 50 |  Loss: (0.1398) |  Loss2: (0.0000) | Acc: (94.00%) (6194/6528)
Epoch: 135 | Batch_idx: 60 |  Loss: (0.1408) |  Loss2: (0.0000) | Acc: (94.00%) (7414/7808)
Epoch: 135 | Batch_idx: 70 |  Loss: (0.1428) |  Loss2: (0.0000) | Acc: (94.00%) (8629/9088)
Epoch: 135 | Batch_idx: 80 |  Loss: (0.1455) |  Loss2: (0.0000) | Acc: (94.00%) (9837/10368)
Epoch: 135 | Batch_idx: 90 |  Loss: (0.1452) |  Loss2: (0.0000) | Acc: (94.00%) (11060/11648)
Epoch: 135 | Batch_idx: 100 |  Loss: (0.1456) |  Loss2: (0.0000) | Acc: (94.00%) (12275/12928)
Epoch: 135 | Batch_idx: 110 |  Loss: (0.1465) |  Loss2: (0.0000) | Acc: (94.00%) (13481/14208)
Epoch: 135 | Batch_idx: 120 |  Loss: (0.1455) |  Loss2: (0.0000) | Acc: (94.00%) (14701/15488)
Epoch: 135 | Batch_idx: 130 |  Loss: (0.1453) |  Loss2: (0.0000) | Acc: (94.00%) (15916/16768)
Epoch: 135 | Batch_idx: 140 |  Loss: (0.1442) |  Loss2: (0.0000) | Acc: (94.00%) (17139/18048)
Epoch: 135 | Batch_idx: 150 |  Loss: (0.1440) |  Loss2: (0.0000) | Acc: (94.00%) (18359/19328)
Epoch: 135 | Batch_idx: 160 |  Loss: (0.1435) |  Loss2: (0.0000) | Acc: (94.00%) (19575/20608)
Epoch: 135 | Batch_idx: 170 |  Loss: (0.1446) |  Loss2: (0.0000) | Acc: (94.00%) (20781/21888)
Epoch: 135 | Batch_idx: 180 |  Loss: (0.1445) |  Loss2: (0.0000) | Acc: (94.00%) (21993/23168)
Epoch: 135 | Batch_idx: 190 |  Loss: (0.1446) |  Loss2: (0.0000) | Acc: (94.00%) (23203/24448)
Epoch: 135 | Batch_idx: 200 |  Loss: (0.1447) |  Loss2: (0.0000) | Acc: (94.00%) (24407/25728)
Epoch: 135 | Batch_idx: 210 |  Loss: (0.1430) |  Loss2: (0.0000) | Acc: (94.00%) (25635/27008)
Epoch: 135 | Batch_idx: 220 |  Loss: (0.1425) |  Loss2: (0.0000) | Acc: (94.00%) (26859/28288)
Epoch: 135 | Batch_idx: 230 |  Loss: (0.1426) |  Loss2: (0.0000) | Acc: (94.00%) (28080/29568)
Epoch: 135 | Batch_idx: 240 |  Loss: (0.1425) |  Loss2: (0.0000) | Acc: (94.00%) (29301/30848)
Epoch: 135 | Batch_idx: 250 |  Loss: (0.1424) |  Loss2: (0.0000) | Acc: (94.00%) (30519/32128)
Epoch: 135 | Batch_idx: 260 |  Loss: (0.1435) |  Loss2: (0.0000) | Acc: (94.00%) (31722/33408)
Epoch: 135 | Batch_idx: 270 |  Loss: (0.1435) |  Loss2: (0.0000) | Acc: (94.00%) (32941/34688)
Epoch: 135 | Batch_idx: 280 |  Loss: (0.1430) |  Loss2: (0.0000) | Acc: (94.00%) (34158/35968)
Epoch: 135 | Batch_idx: 290 |  Loss: (0.1422) |  Loss2: (0.0000) | Acc: (95.00%) (35388/37248)
Epoch: 135 | Batch_idx: 300 |  Loss: (0.1422) |  Loss2: (0.0000) | Acc: (95.00%) (36607/38528)
Epoch: 135 | Batch_idx: 310 |  Loss: (0.1415) |  Loss2: (0.0000) | Acc: (95.00%) (37824/39808)
Epoch: 135 | Batch_idx: 320 |  Loss: (0.1413) |  Loss2: (0.0000) | Acc: (95.00%) (39043/41088)
Epoch: 135 | Batch_idx: 330 |  Loss: (0.1407) |  Loss2: (0.0000) | Acc: (95.00%) (40268/42368)
Epoch: 135 | Batch_idx: 340 |  Loss: (0.1401) |  Loss2: (0.0000) | Acc: (95.00%) (41500/43648)
Epoch: 135 | Batch_idx: 350 |  Loss: (0.1401) |  Loss2: (0.0000) | Acc: (95.00%) (42712/44928)
Epoch: 135 | Batch_idx: 360 |  Loss: (0.1398) |  Loss2: (0.0000) | Acc: (95.00%) (43935/46208)
Epoch: 135 | Batch_idx: 370 |  Loss: (0.1393) |  Loss2: (0.0000) | Acc: (95.00%) (45161/47488)
Epoch: 135 | Batch_idx: 380 |  Loss: (0.1386) |  Loss2: (0.0000) | Acc: (95.00%) (46394/48768)
Epoch: 135 | Batch_idx: 390 |  Loss: (0.1388) |  Loss2: (0.0000) | Acc: (95.00%) (47568/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_135.pth.tar'
# TEST : Loss: (0.3682) | Acc: (88.00%) (8812/10000)
percent tensor([0.5654, 0.5777, 0.5700, 0.5579, 0.5745, 0.5503, 0.5820, 0.5784, 0.5810,
        0.5763, 0.5755, 0.5760, 0.5719, 0.5781, 0.5665, 0.5618],
       device='cuda:0') torch.Size([16])
percent tensor([0.5385, 0.5338, 0.5026, 0.5268, 0.5281, 0.5197, 0.5386, 0.5537, 0.5275,
        0.5106, 0.5087, 0.4976, 0.5332, 0.5424, 0.5417, 0.5273],
       device='cuda:0') torch.Size([16])
percent tensor([0.5073, 0.4512, 0.4457, 0.5190, 0.4294, 0.5513, 0.4653, 0.5018, 0.4907,
        0.4386, 0.4800, 0.4337, 0.4289, 0.5803, 0.5025, 0.5114],
       device='cuda:0') torch.Size([16])
percent tensor([0.6438, 0.6649, 0.6233, 0.6101, 0.6115, 0.6516, 0.6409, 0.5927, 0.6366,
        0.6697, 0.6570, 0.6481, 0.6881, 0.6306, 0.6477, 0.6600],
       device='cuda:0') torch.Size([16])
percent tensor([0.6733, 0.6000, 0.6914, 0.6882, 0.7052, 0.7453, 0.6669, 0.6672, 0.7471,
        0.6793, 0.7028, 0.7037, 0.6211, 0.7243, 0.6307, 0.7145],
       device='cuda:0') torch.Size([16])
percent tensor([0.7685, 0.7597, 0.7956, 0.7870, 0.8179, 0.8195, 0.7905, 0.7673, 0.7971,
        0.7784, 0.7879, 0.7604, 0.7649, 0.8121, 0.7490, 0.7939],
       device='cuda:0') torch.Size([16])
percent tensor([0.4490, 0.5323, 0.5626, 0.4745, 0.6177, 0.6997, 0.5396, 0.4113, 0.5371,
        0.4749, 0.4542, 0.4308, 0.4673, 0.5007, 0.3339, 0.5018],
       device='cuda:0') torch.Size([16])
percent tensor([0.9996, 0.9996, 0.9993, 0.9997, 0.9993, 0.9991, 0.9994, 0.9996, 0.9991,
        0.9996, 0.9998, 0.9996, 0.9994, 0.9991, 0.9990, 0.9996],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 136 | Batch_idx: 0 |  Loss: (0.1618) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 136 | Batch_idx: 10 |  Loss: (0.1514) |  Loss2: (0.0000) | Acc: (94.00%) (1330/1408)
Epoch: 136 | Batch_idx: 20 |  Loss: (0.1380) |  Loss2: (0.0000) | Acc: (94.00%) (2550/2688)
Epoch: 136 | Batch_idx: 30 |  Loss: (0.1304) |  Loss2: (0.0000) | Acc: (95.00%) (3782/3968)
Epoch: 136 | Batch_idx: 40 |  Loss: (0.1292) |  Loss2: (0.0000) | Acc: (95.00%) (5012/5248)
Epoch: 136 | Batch_idx: 50 |  Loss: (0.1308) |  Loss2: (0.0000) | Acc: (95.00%) (6236/6528)
Epoch: 136 | Batch_idx: 60 |  Loss: (0.1305) |  Loss2: (0.0000) | Acc: (95.00%) (7457/7808)
Epoch: 136 | Batch_idx: 70 |  Loss: (0.1319) |  Loss2: (0.0000) | Acc: (95.00%) (8680/9088)
Epoch: 136 | Batch_idx: 80 |  Loss: (0.1330) |  Loss2: (0.0000) | Acc: (95.00%) (9909/10368)
Epoch: 136 | Batch_idx: 90 |  Loss: (0.1327) |  Loss2: (0.0000) | Acc: (95.00%) (11134/11648)
Epoch: 136 | Batch_idx: 100 |  Loss: (0.1348) |  Loss2: (0.0000) | Acc: (95.00%) (12345/12928)
Epoch: 136 | Batch_idx: 110 |  Loss: (0.1352) |  Loss2: (0.0000) | Acc: (95.00%) (13563/14208)
Epoch: 136 | Batch_idx: 120 |  Loss: (0.1362) |  Loss2: (0.0000) | Acc: (95.00%) (14776/15488)
Epoch: 136 | Batch_idx: 130 |  Loss: (0.1375) |  Loss2: (0.0000) | Acc: (95.00%) (16002/16768)
Epoch: 136 | Batch_idx: 140 |  Loss: (0.1370) |  Loss2: (0.0000) | Acc: (95.00%) (17223/18048)
Epoch: 136 | Batch_idx: 150 |  Loss: (0.1370) |  Loss2: (0.0000) | Acc: (95.00%) (18446/19328)
Epoch: 136 | Batch_idx: 160 |  Loss: (0.1372) |  Loss2: (0.0000) | Acc: (95.00%) (19662/20608)
Epoch: 136 | Batch_idx: 170 |  Loss: (0.1367) |  Loss2: (0.0000) | Acc: (95.00%) (20889/21888)
Epoch: 136 | Batch_idx: 180 |  Loss: (0.1367) |  Loss2: (0.0000) | Acc: (95.00%) (22117/23168)
Epoch: 136 | Batch_idx: 190 |  Loss: (0.1363) |  Loss2: (0.0000) | Acc: (95.00%) (23330/24448)
Epoch: 136 | Batch_idx: 200 |  Loss: (0.1366) |  Loss2: (0.0000) | Acc: (95.00%) (24538/25728)
Epoch: 136 | Batch_idx: 210 |  Loss: (0.1358) |  Loss2: (0.0000) | Acc: (95.00%) (25771/27008)
Epoch: 136 | Batch_idx: 220 |  Loss: (0.1356) |  Loss2: (0.0000) | Acc: (95.00%) (26997/28288)
Epoch: 136 | Batch_idx: 230 |  Loss: (0.1357) |  Loss2: (0.0000) | Acc: (95.00%) (28208/29568)
Epoch: 136 | Batch_idx: 240 |  Loss: (0.1354) |  Loss2: (0.0000) | Acc: (95.00%) (29435/30848)
Epoch: 136 | Batch_idx: 250 |  Loss: (0.1359) |  Loss2: (0.0000) | Acc: (95.00%) (30645/32128)
Epoch: 136 | Batch_idx: 260 |  Loss: (0.1363) |  Loss2: (0.0000) | Acc: (95.00%) (31863/33408)
Epoch: 136 | Batch_idx: 270 |  Loss: (0.1362) |  Loss2: (0.0000) | Acc: (95.00%) (33083/34688)
Epoch: 136 | Batch_idx: 280 |  Loss: (0.1363) |  Loss2: (0.0000) | Acc: (95.00%) (34303/35968)
Epoch: 136 | Batch_idx: 290 |  Loss: (0.1369) |  Loss2: (0.0000) | Acc: (95.00%) (35515/37248)
Epoch: 136 | Batch_idx: 300 |  Loss: (0.1367) |  Loss2: (0.0000) | Acc: (95.00%) (36735/38528)
Epoch: 136 | Batch_idx: 310 |  Loss: (0.1364) |  Loss2: (0.0000) | Acc: (95.00%) (37956/39808)
Epoch: 136 | Batch_idx: 320 |  Loss: (0.1369) |  Loss2: (0.0000) | Acc: (95.00%) (39166/41088)
Epoch: 136 | Batch_idx: 330 |  Loss: (0.1372) |  Loss2: (0.0000) | Acc: (95.00%) (40377/42368)
Epoch: 136 | Batch_idx: 340 |  Loss: (0.1371) |  Loss2: (0.0000) | Acc: (95.00%) (41591/43648)
Epoch: 136 | Batch_idx: 350 |  Loss: (0.1363) |  Loss2: (0.0000) | Acc: (95.00%) (42824/44928)
Epoch: 136 | Batch_idx: 360 |  Loss: (0.1363) |  Loss2: (0.0000) | Acc: (95.00%) (44042/46208)
Epoch: 136 | Batch_idx: 370 |  Loss: (0.1363) |  Loss2: (0.0000) | Acc: (95.00%) (45261/47488)
Epoch: 136 | Batch_idx: 380 |  Loss: (0.1359) |  Loss2: (0.0000) | Acc: (95.00%) (46486/48768)
Epoch: 136 | Batch_idx: 390 |  Loss: (0.1357) |  Loss2: (0.0000) | Acc: (95.00%) (47656/50000)
# TEST : Loss: (0.4005) | Acc: (87.00%) (8755/10000)
percent tensor([0.5639, 0.5782, 0.5672, 0.5577, 0.5718, 0.5492, 0.5811, 0.5778, 0.5794,
        0.5754, 0.5742, 0.5736, 0.5709, 0.5788, 0.5663, 0.5612],
       device='cuda:0') torch.Size([16])
percent tensor([0.5406, 0.5354, 0.5028, 0.5268, 0.5288, 0.5207, 0.5387, 0.5517, 0.5318,
        0.5113, 0.5101, 0.4974, 0.5358, 0.5424, 0.5430, 0.5284],
       device='cuda:0') torch.Size([16])
percent tensor([0.5168, 0.4501, 0.4619, 0.5264, 0.4406, 0.5609, 0.4660, 0.5075, 0.5035,
        0.4459, 0.4874, 0.4480, 0.4427, 0.5771, 0.5123, 0.5155],
       device='cuda:0') torch.Size([16])
percent tensor([0.6352, 0.6638, 0.6188, 0.5991, 0.6027, 0.6438, 0.6366, 0.5880, 0.6367,
        0.6651, 0.6540, 0.6464, 0.6839, 0.6375, 0.6393, 0.6528],
       device='cuda:0') torch.Size([16])
percent tensor([0.6694, 0.5961, 0.6859, 0.6932, 0.6987, 0.7472, 0.6752, 0.6604, 0.7411,
        0.6669, 0.7019, 0.6978, 0.6028, 0.7464, 0.6267, 0.7033],
       device='cuda:0') torch.Size([16])
percent tensor([0.7674, 0.7674, 0.7928, 0.7878, 0.8203, 0.8161, 0.7952, 0.7716, 0.7893,
        0.7834, 0.7959, 0.7647, 0.7631, 0.8131, 0.7553, 0.7958],
       device='cuda:0') torch.Size([16])
percent tensor([0.4428, 0.5409, 0.5506, 0.4911, 0.6082, 0.6997, 0.5222, 0.4098, 0.5210,
        0.4894, 0.4682, 0.4267, 0.4631, 0.4737, 0.3522, 0.5047],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9996, 0.9994, 0.9997, 0.9993, 0.9989, 0.9993, 0.9997, 0.9997,
        0.9996, 0.9998, 0.9997, 0.9997, 0.9990, 0.9989, 0.9996],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 137 | Batch_idx: 0 |  Loss: (0.1665) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 137 | Batch_idx: 10 |  Loss: (0.1408) |  Loss2: (0.0000) | Acc: (94.00%) (1332/1408)
Epoch: 137 | Batch_idx: 20 |  Loss: (0.1463) |  Loss2: (0.0000) | Acc: (94.00%) (2553/2688)
Epoch: 137 | Batch_idx: 30 |  Loss: (0.1496) |  Loss2: (0.0000) | Acc: (94.00%) (3765/3968)
Epoch: 137 | Batch_idx: 40 |  Loss: (0.1497) |  Loss2: (0.0000) | Acc: (94.00%) (4981/5248)
Epoch: 137 | Batch_idx: 50 |  Loss: (0.1517) |  Loss2: (0.0000) | Acc: (94.00%) (6183/6528)
Epoch: 137 | Batch_idx: 60 |  Loss: (0.1506) |  Loss2: (0.0000) | Acc: (94.00%) (7396/7808)
Epoch: 137 | Batch_idx: 70 |  Loss: (0.1554) |  Loss2: (0.0000) | Acc: (94.00%) (8597/9088)
Epoch: 137 | Batch_idx: 80 |  Loss: (0.1565) |  Loss2: (0.0000) | Acc: (94.00%) (9805/10368)
Epoch: 137 | Batch_idx: 90 |  Loss: (0.1572) |  Loss2: (0.0000) | Acc: (94.00%) (11006/11648)
Epoch: 137 | Batch_idx: 100 |  Loss: (0.1586) |  Loss2: (0.0000) | Acc: (94.00%) (12211/12928)
Epoch: 137 | Batch_idx: 110 |  Loss: (0.1584) |  Loss2: (0.0000) | Acc: (94.00%) (13421/14208)
Epoch: 137 | Batch_idx: 120 |  Loss: (0.1581) |  Loss2: (0.0000) | Acc: (94.00%) (14630/15488)
Epoch: 137 | Batch_idx: 130 |  Loss: (0.1602) |  Loss2: (0.0000) | Acc: (94.00%) (15821/16768)
Epoch: 137 | Batch_idx: 140 |  Loss: (0.1594) |  Loss2: (0.0000) | Acc: (94.00%) (17032/18048)
Epoch: 137 | Batch_idx: 150 |  Loss: (0.1592) |  Loss2: (0.0000) | Acc: (94.00%) (18242/19328)
Epoch: 137 | Batch_idx: 160 |  Loss: (0.1589) |  Loss2: (0.0000) | Acc: (94.00%) (19456/20608)
Epoch: 137 | Batch_idx: 170 |  Loss: (0.1590) |  Loss2: (0.0000) | Acc: (94.00%) (20668/21888)
Epoch: 137 | Batch_idx: 180 |  Loss: (0.1583) |  Loss2: (0.0000) | Acc: (94.00%) (21883/23168)
Epoch: 137 | Batch_idx: 190 |  Loss: (0.1575) |  Loss2: (0.0000) | Acc: (94.00%) (23098/24448)
Epoch: 137 | Batch_idx: 200 |  Loss: (0.1570) |  Loss2: (0.0000) | Acc: (94.00%) (24312/25728)
Epoch: 137 | Batch_idx: 210 |  Loss: (0.1563) |  Loss2: (0.0000) | Acc: (94.00%) (25534/27008)
Epoch: 137 | Batch_idx: 220 |  Loss: (0.1556) |  Loss2: (0.0000) | Acc: (94.00%) (26750/28288)
Epoch: 137 | Batch_idx: 230 |  Loss: (0.1550) |  Loss2: (0.0000) | Acc: (94.00%) (27975/29568)
Epoch: 137 | Batch_idx: 240 |  Loss: (0.1549) |  Loss2: (0.0000) | Acc: (94.00%) (29183/30848)
Epoch: 137 | Batch_idx: 250 |  Loss: (0.1540) |  Loss2: (0.0000) | Acc: (94.00%) (30404/32128)
Epoch: 137 | Batch_idx: 260 |  Loss: (0.1537) |  Loss2: (0.0000) | Acc: (94.00%) (31621/33408)
Epoch: 137 | Batch_idx: 270 |  Loss: (0.1527) |  Loss2: (0.0000) | Acc: (94.00%) (32847/34688)
Epoch: 137 | Batch_idx: 280 |  Loss: (0.1533) |  Loss2: (0.0000) | Acc: (94.00%) (34058/35968)
Epoch: 137 | Batch_idx: 290 |  Loss: (0.1529) |  Loss2: (0.0000) | Acc: (94.00%) (35278/37248)
Epoch: 137 | Batch_idx: 300 |  Loss: (0.1530) |  Loss2: (0.0000) | Acc: (94.00%) (36495/38528)
Epoch: 137 | Batch_idx: 310 |  Loss: (0.1535) |  Loss2: (0.0000) | Acc: (94.00%) (37693/39808)
Epoch: 137 | Batch_idx: 320 |  Loss: (0.1529) |  Loss2: (0.0000) | Acc: (94.00%) (38919/41088)
Epoch: 137 | Batch_idx: 330 |  Loss: (0.1530) |  Loss2: (0.0000) | Acc: (94.00%) (40125/42368)
Epoch: 137 | Batch_idx: 340 |  Loss: (0.1528) |  Loss2: (0.0000) | Acc: (94.00%) (41340/43648)
Epoch: 137 | Batch_idx: 350 |  Loss: (0.1529) |  Loss2: (0.0000) | Acc: (94.00%) (42556/44928)
Epoch: 137 | Batch_idx: 360 |  Loss: (0.1529) |  Loss2: (0.0000) | Acc: (94.00%) (43762/46208)
Epoch: 137 | Batch_idx: 370 |  Loss: (0.1523) |  Loss2: (0.0000) | Acc: (94.00%) (44978/47488)
Epoch: 137 | Batch_idx: 380 |  Loss: (0.1522) |  Loss2: (0.0000) | Acc: (94.00%) (46182/48768)
Epoch: 137 | Batch_idx: 390 |  Loss: (0.1522) |  Loss2: (0.0000) | Acc: (94.00%) (47345/50000)
# TEST : Loss: (0.3988) | Acc: (87.00%) (8798/10000)
percent tensor([0.5617, 0.5761, 0.5654, 0.5560, 0.5699, 0.5477, 0.5785, 0.5756, 0.5772,
        0.5729, 0.5720, 0.5710, 0.5685, 0.5778, 0.5641, 0.5593],
       device='cuda:0') torch.Size([16])
percent tensor([0.5402, 0.5380, 0.5029, 0.5310, 0.5288, 0.5194, 0.5396, 0.5553, 0.5325,
        0.5129, 0.5116, 0.4961, 0.5370, 0.5450, 0.5453, 0.5299],
       device='cuda:0') torch.Size([16])
percent tensor([0.5192, 0.4656, 0.4680, 0.5396, 0.4460, 0.5647, 0.4747, 0.5148, 0.5044,
        0.4560, 0.4931, 0.4578, 0.4517, 0.5832, 0.5218, 0.5226],
       device='cuda:0') torch.Size([16])
percent tensor([0.6556, 0.6786, 0.6358, 0.6188, 0.6211, 0.6607, 0.6548, 0.6096, 0.6527,
        0.6800, 0.6694, 0.6632, 0.7005, 0.6516, 0.6593, 0.6701],
       device='cuda:0') torch.Size([16])
percent tensor([0.7000, 0.6226, 0.7120, 0.7262, 0.7227, 0.7712, 0.7028, 0.6898, 0.7709,
        0.6908, 0.7268, 0.7309, 0.6349, 0.7692, 0.6561, 0.7325],
       device='cuda:0') torch.Size([16])
percent tensor([0.7808, 0.7802, 0.8055, 0.8035, 0.8324, 0.8284, 0.8064, 0.7859, 0.8045,
        0.7964, 0.8107, 0.7773, 0.7782, 0.8283, 0.7694, 0.8078],
       device='cuda:0') torch.Size([16])
percent tensor([0.4534, 0.5830, 0.5662, 0.5113, 0.6100, 0.7044, 0.5381, 0.4015, 0.5792,
        0.5499, 0.5311, 0.4779, 0.5196, 0.5235, 0.3735, 0.4990],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9996, 0.9994, 0.9997, 0.9994, 0.9986, 0.9993, 0.9996, 0.9996,
        0.9996, 0.9998, 0.9997, 0.9997, 0.9990, 0.9989, 0.9994],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 138 | Batch_idx: 0 |  Loss: (0.1096) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 138 | Batch_idx: 10 |  Loss: (0.1198) |  Loss2: (0.0000) | Acc: (95.00%) (1348/1408)
Epoch: 138 | Batch_idx: 20 |  Loss: (0.1221) |  Loss2: (0.0000) | Acc: (95.00%) (2573/2688)
Epoch: 138 | Batch_idx: 30 |  Loss: (0.1310) |  Loss2: (0.0000) | Acc: (95.00%) (3791/3968)
Epoch: 138 | Batch_idx: 40 |  Loss: (0.1291) |  Loss2: (0.0000) | Acc: (95.00%) (5016/5248)
Epoch: 138 | Batch_idx: 50 |  Loss: (0.1287) |  Loss2: (0.0000) | Acc: (95.00%) (6244/6528)
Epoch: 138 | Batch_idx: 60 |  Loss: (0.1303) |  Loss2: (0.0000) | Acc: (95.00%) (7467/7808)
Epoch: 138 | Batch_idx: 70 |  Loss: (0.1304) |  Loss2: (0.0000) | Acc: (95.00%) (8691/9088)
Epoch: 138 | Batch_idx: 80 |  Loss: (0.1324) |  Loss2: (0.0000) | Acc: (95.00%) (9909/10368)
Epoch: 138 | Batch_idx: 90 |  Loss: (0.1315) |  Loss2: (0.0000) | Acc: (95.00%) (11138/11648)
Epoch: 138 | Batch_idx: 100 |  Loss: (0.1320) |  Loss2: (0.0000) | Acc: (95.00%) (12347/12928)
Epoch: 138 | Batch_idx: 110 |  Loss: (0.1334) |  Loss2: (0.0000) | Acc: (95.00%) (13565/14208)
Epoch: 138 | Batch_idx: 120 |  Loss: (0.1318) |  Loss2: (0.0000) | Acc: (95.00%) (14796/15488)
Epoch: 138 | Batch_idx: 130 |  Loss: (0.1316) |  Loss2: (0.0000) | Acc: (95.00%) (16025/16768)
Epoch: 138 | Batch_idx: 140 |  Loss: (0.1325) |  Loss2: (0.0000) | Acc: (95.00%) (17243/18048)
Epoch: 138 | Batch_idx: 150 |  Loss: (0.1328) |  Loss2: (0.0000) | Acc: (95.00%) (18461/19328)
Epoch: 138 | Batch_idx: 160 |  Loss: (0.1331) |  Loss2: (0.0000) | Acc: (95.00%) (19684/20608)
Epoch: 138 | Batch_idx: 170 |  Loss: (0.1323) |  Loss2: (0.0000) | Acc: (95.00%) (20908/21888)
Epoch: 138 | Batch_idx: 180 |  Loss: (0.1328) |  Loss2: (0.0000) | Acc: (95.00%) (22132/23168)
Epoch: 138 | Batch_idx: 190 |  Loss: (0.1328) |  Loss2: (0.0000) | Acc: (95.00%) (23344/24448)
Epoch: 138 | Batch_idx: 200 |  Loss: (0.1334) |  Loss2: (0.0000) | Acc: (95.00%) (24566/25728)
Epoch: 138 | Batch_idx: 210 |  Loss: (0.1334) |  Loss2: (0.0000) | Acc: (95.00%) (25788/27008)
Epoch: 138 | Batch_idx: 220 |  Loss: (0.1336) |  Loss2: (0.0000) | Acc: (95.00%) (26997/28288)
Epoch: 138 | Batch_idx: 230 |  Loss: (0.1339) |  Loss2: (0.0000) | Acc: (95.00%) (28212/29568)
Epoch: 138 | Batch_idx: 240 |  Loss: (0.1344) |  Loss2: (0.0000) | Acc: (95.00%) (29432/30848)
Epoch: 138 | Batch_idx: 250 |  Loss: (0.1345) |  Loss2: (0.0000) | Acc: (95.00%) (30655/32128)
Epoch: 138 | Batch_idx: 260 |  Loss: (0.1344) |  Loss2: (0.0000) | Acc: (95.00%) (31882/33408)
Epoch: 138 | Batch_idx: 270 |  Loss: (0.1344) |  Loss2: (0.0000) | Acc: (95.00%) (33101/34688)
Epoch: 138 | Batch_idx: 280 |  Loss: (0.1348) |  Loss2: (0.0000) | Acc: (95.00%) (34316/35968)
Epoch: 138 | Batch_idx: 290 |  Loss: (0.1343) |  Loss2: (0.0000) | Acc: (95.00%) (35536/37248)
Epoch: 138 | Batch_idx: 300 |  Loss: (0.1336) |  Loss2: (0.0000) | Acc: (95.00%) (36762/38528)
Epoch: 138 | Batch_idx: 310 |  Loss: (0.1338) |  Loss2: (0.0000) | Acc: (95.00%) (37979/39808)
Epoch: 138 | Batch_idx: 320 |  Loss: (0.1340) |  Loss2: (0.0000) | Acc: (95.00%) (39193/41088)
Epoch: 138 | Batch_idx: 330 |  Loss: (0.1344) |  Loss2: (0.0000) | Acc: (95.00%) (40413/42368)
Epoch: 138 | Batch_idx: 340 |  Loss: (0.1348) |  Loss2: (0.0000) | Acc: (95.00%) (41628/43648)
Epoch: 138 | Batch_idx: 350 |  Loss: (0.1354) |  Loss2: (0.0000) | Acc: (95.00%) (42838/44928)
Epoch: 138 | Batch_idx: 360 |  Loss: (0.1354) |  Loss2: (0.0000) | Acc: (95.00%) (44052/46208)
Epoch: 138 | Batch_idx: 370 |  Loss: (0.1354) |  Loss2: (0.0000) | Acc: (95.00%) (45272/47488)
Epoch: 138 | Batch_idx: 380 |  Loss: (0.1359) |  Loss2: (0.0000) | Acc: (95.00%) (46480/48768)
Epoch: 138 | Batch_idx: 390 |  Loss: (0.1367) |  Loss2: (0.0000) | Acc: (95.00%) (47638/50000)
# TEST : Loss: (0.4310) | Acc: (86.00%) (8687/10000)
percent tensor([0.5649, 0.5754, 0.5729, 0.5585, 0.5764, 0.5507, 0.5814, 0.5776, 0.5792,
        0.5749, 0.5736, 0.5779, 0.5708, 0.5749, 0.5655, 0.5606],
       device='cuda:0') torch.Size([16])
percent tensor([0.5385, 0.5382, 0.4993, 0.5314, 0.5263, 0.5160, 0.5385, 0.5539, 0.5284,
        0.5126, 0.5103, 0.4961, 0.5357, 0.5452, 0.5439, 0.5282],
       device='cuda:0') torch.Size([16])
percent tensor([0.5188, 0.4676, 0.4547, 0.5304, 0.4332, 0.5624, 0.4755, 0.5083, 0.5067,
        0.4574, 0.4940, 0.4478, 0.4515, 0.5911, 0.5143, 0.5226],
       device='cuda:0') torch.Size([16])
percent tensor([0.6578, 0.6866, 0.6309, 0.6200, 0.6192, 0.6619, 0.6599, 0.6064, 0.6566,
        0.6851, 0.6734, 0.6616, 0.7028, 0.6620, 0.6643, 0.6729],
       device='cuda:0') torch.Size([16])
percent tensor([0.6938, 0.6186, 0.6855, 0.7144, 0.7045, 0.7689, 0.6920, 0.6754, 0.7622,
        0.6856, 0.7240, 0.7041, 0.6314, 0.7618, 0.6550, 0.7363],
       device='cuda:0') torch.Size([16])
percent tensor([0.7786, 0.7735, 0.8031, 0.8012, 0.8267, 0.8214, 0.8017, 0.7821, 0.8055,
        0.7973, 0.8086, 0.7836, 0.7782, 0.8225, 0.7674, 0.8009],
       device='cuda:0') torch.Size([16])
percent tensor([0.4311, 0.5406, 0.5818, 0.5383, 0.6263, 0.6793, 0.5273, 0.4166, 0.5758,
        0.5092, 0.5215, 0.5008, 0.5027, 0.4967, 0.3250, 0.4608],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9996, 0.9992, 0.9997, 0.9993, 0.9990, 0.9994, 0.9996, 0.9996,
        0.9995, 0.9998, 0.9996, 0.9993, 0.9993, 0.9991, 0.9997],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 139 | Batch_idx: 0 |  Loss: (0.1402) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 139 | Batch_idx: 10 |  Loss: (0.1625) |  Loss2: (0.0000) | Acc: (94.00%) (1330/1408)
Epoch: 139 | Batch_idx: 20 |  Loss: (0.1615) |  Loss2: (0.0000) | Acc: (94.00%) (2537/2688)
Epoch: 139 | Batch_idx: 30 |  Loss: (0.1796) |  Loss2: (0.0000) | Acc: (93.00%) (3719/3968)
Epoch: 139 | Batch_idx: 40 |  Loss: (0.1801) |  Loss2: (0.0000) | Acc: (93.00%) (4922/5248)
Epoch: 139 | Batch_idx: 50 |  Loss: (0.1794) |  Loss2: (0.0000) | Acc: (93.00%) (6121/6528)
Epoch: 139 | Batch_idx: 60 |  Loss: (0.1851) |  Loss2: (0.0000) | Acc: (93.00%) (7298/7808)
Epoch: 139 | Batch_idx: 70 |  Loss: (0.1860) |  Loss2: (0.0000) | Acc: (93.00%) (8497/9088)
Epoch: 139 | Batch_idx: 80 |  Loss: (0.1885) |  Loss2: (0.0000) | Acc: (93.00%) (9685/10368)
Epoch: 139 | Batch_idx: 90 |  Loss: (0.1891) |  Loss2: (0.0000) | Acc: (93.00%) (10875/11648)
Epoch: 139 | Batch_idx: 100 |  Loss: (0.1893) |  Loss2: (0.0000) | Acc: (93.00%) (12074/12928)
Epoch: 139 | Batch_idx: 110 |  Loss: (0.1855) |  Loss2: (0.0000) | Acc: (93.00%) (13291/14208)
Epoch: 139 | Batch_idx: 120 |  Loss: (0.1833) |  Loss2: (0.0000) | Acc: (93.00%) (14501/15488)
Epoch: 139 | Batch_idx: 130 |  Loss: (0.1830) |  Loss2: (0.0000) | Acc: (93.00%) (15701/16768)
Epoch: 139 | Batch_idx: 140 |  Loss: (0.1832) |  Loss2: (0.0000) | Acc: (93.00%) (16907/18048)
Epoch: 139 | Batch_idx: 150 |  Loss: (0.1845) |  Loss2: (0.0000) | Acc: (93.00%) (18091/19328)
Epoch: 139 | Batch_idx: 160 |  Loss: (0.1849) |  Loss2: (0.0000) | Acc: (93.00%) (19283/20608)
Epoch: 139 | Batch_idx: 170 |  Loss: (0.1843) |  Loss2: (0.0000) | Acc: (93.00%) (20480/21888)
Epoch: 139 | Batch_idx: 180 |  Loss: (0.1834) |  Loss2: (0.0000) | Acc: (93.00%) (21684/23168)
Epoch: 139 | Batch_idx: 190 |  Loss: (0.1822) |  Loss2: (0.0000) | Acc: (93.00%) (22893/24448)
Epoch: 139 | Batch_idx: 200 |  Loss: (0.1823) |  Loss2: (0.0000) | Acc: (93.00%) (24084/25728)
Epoch: 139 | Batch_idx: 210 |  Loss: (0.1823) |  Loss2: (0.0000) | Acc: (93.00%) (25288/27008)
Epoch: 139 | Batch_idx: 220 |  Loss: (0.1825) |  Loss2: (0.0000) | Acc: (93.00%) (26476/28288)
Epoch: 139 | Batch_idx: 230 |  Loss: (0.1825) |  Loss2: (0.0000) | Acc: (93.00%) (27672/29568)
Epoch: 139 | Batch_idx: 240 |  Loss: (0.1809) |  Loss2: (0.0000) | Acc: (93.00%) (28882/30848)
Epoch: 139 | Batch_idx: 250 |  Loss: (0.1801) |  Loss2: (0.0000) | Acc: (93.00%) (30091/32128)
Epoch: 139 | Batch_idx: 260 |  Loss: (0.1794) |  Loss2: (0.0000) | Acc: (93.00%) (31299/33408)
Epoch: 139 | Batch_idx: 270 |  Loss: (0.1788) |  Loss2: (0.0000) | Acc: (93.00%) (32508/34688)
Epoch: 139 | Batch_idx: 280 |  Loss: (0.1781) |  Loss2: (0.0000) | Acc: (93.00%) (33711/35968)
Epoch: 139 | Batch_idx: 290 |  Loss: (0.1775) |  Loss2: (0.0000) | Acc: (93.00%) (34923/37248)
Epoch: 139 | Batch_idx: 300 |  Loss: (0.1767) |  Loss2: (0.0000) | Acc: (93.00%) (36131/38528)
Epoch: 139 | Batch_idx: 310 |  Loss: (0.1762) |  Loss2: (0.0000) | Acc: (93.00%) (37336/39808)
Epoch: 139 | Batch_idx: 320 |  Loss: (0.1762) |  Loss2: (0.0000) | Acc: (93.00%) (38532/41088)
Epoch: 139 | Batch_idx: 330 |  Loss: (0.1750) |  Loss2: (0.0000) | Acc: (93.00%) (39751/42368)
Epoch: 139 | Batch_idx: 340 |  Loss: (0.1744) |  Loss2: (0.0000) | Acc: (93.00%) (40956/43648)
Epoch: 139 | Batch_idx: 350 |  Loss: (0.1738) |  Loss2: (0.0000) | Acc: (93.00%) (42174/44928)
Epoch: 139 | Batch_idx: 360 |  Loss: (0.1727) |  Loss2: (0.0000) | Acc: (93.00%) (43400/46208)
Epoch: 139 | Batch_idx: 370 |  Loss: (0.1728) |  Loss2: (0.0000) | Acc: (93.00%) (44594/47488)
Epoch: 139 | Batch_idx: 380 |  Loss: (0.1724) |  Loss2: (0.0000) | Acc: (93.00%) (45804/48768)
Epoch: 139 | Batch_idx: 390 |  Loss: (0.1728) |  Loss2: (0.0000) | Acc: (93.00%) (46952/50000)
# TEST : Loss: (0.4058) | Acc: (87.00%) (8753/10000)
percent tensor([0.5772, 0.5900, 0.5872, 0.5714, 0.5933, 0.5630, 0.5968, 0.5919, 0.5933,
        0.5890, 0.5871, 0.5930, 0.5841, 0.5883, 0.5788, 0.5727],
       device='cuda:0') torch.Size([16])
percent tensor([0.5479, 0.5460, 0.5125, 0.5454, 0.5366, 0.5280, 0.5466, 0.5620, 0.5395,
        0.5239, 0.5213, 0.5121, 0.5437, 0.5569, 0.5519, 0.5392],
       device='cuda:0') torch.Size([16])
percent tensor([0.5342, 0.4681, 0.4826, 0.5513, 0.4583, 0.5744, 0.4853, 0.5265, 0.5226,
        0.4629, 0.5006, 0.4713, 0.4534, 0.5994, 0.5228, 0.5329],
       device='cuda:0') torch.Size([16])
percent tensor([0.6479, 0.6753, 0.6202, 0.6123, 0.6098, 0.6452, 0.6501, 0.6007, 0.6483,
        0.6785, 0.6642, 0.6545, 0.6934, 0.6478, 0.6514, 0.6632],
       device='cuda:0') torch.Size([16])
percent tensor([0.6836, 0.5988, 0.6972, 0.7218, 0.7147, 0.7595, 0.6879, 0.6920, 0.7636,
        0.6828, 0.7125, 0.7274, 0.6071, 0.7650, 0.6491, 0.7288],
       device='cuda:0') torch.Size([16])
percent tensor([0.7809, 0.7752, 0.8084, 0.8106, 0.8358, 0.8292, 0.8020, 0.7961, 0.8056,
        0.7997, 0.8060, 0.7854, 0.7780, 0.8216, 0.7706, 0.8057],
       device='cuda:0') torch.Size([16])
percent tensor([0.3837, 0.5027, 0.5095, 0.4761, 0.5834, 0.6559, 0.4684, 0.3776, 0.5187,
        0.4920, 0.4697, 0.4297, 0.4626, 0.4506, 0.3082, 0.4317],
       device='cuda:0') torch.Size([16])
percent tensor([0.9996, 0.9996, 0.9994, 0.9998, 0.9995, 0.9991, 0.9991, 0.9996, 0.9998,
        0.9996, 0.9998, 0.9996, 0.9995, 0.9991, 0.9992, 0.9996],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 140 | Batch_idx: 0 |  Loss: (0.1174) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 140 | Batch_idx: 10 |  Loss: (0.1404) |  Loss2: (0.0000) | Acc: (95.00%) (1346/1408)
Epoch: 140 | Batch_idx: 20 |  Loss: (0.1329) |  Loss2: (0.0000) | Acc: (95.00%) (2571/2688)
Epoch: 140 | Batch_idx: 30 |  Loss: (0.1276) |  Loss2: (0.0000) | Acc: (95.00%) (3801/3968)
Epoch: 140 | Batch_idx: 40 |  Loss: (0.1302) |  Loss2: (0.0000) | Acc: (95.00%) (5021/5248)
Epoch: 140 | Batch_idx: 50 |  Loss: (0.1297) |  Loss2: (0.0000) | Acc: (95.00%) (6244/6528)
Epoch: 140 | Batch_idx: 60 |  Loss: (0.1327) |  Loss2: (0.0000) | Acc: (95.00%) (7453/7808)
Epoch: 140 | Batch_idx: 70 |  Loss: (0.1335) |  Loss2: (0.0000) | Acc: (95.00%) (8672/9088)
Epoch: 140 | Batch_idx: 80 |  Loss: (0.1320) |  Loss2: (0.0000) | Acc: (95.00%) (9898/10368)
Epoch: 140 | Batch_idx: 90 |  Loss: (0.1313) |  Loss2: (0.0000) | Acc: (95.00%) (11128/11648)
Epoch: 140 | Batch_idx: 100 |  Loss: (0.1281) |  Loss2: (0.0000) | Acc: (95.00%) (12373/12928)
Epoch: 140 | Batch_idx: 110 |  Loss: (0.1270) |  Loss2: (0.0000) | Acc: (95.00%) (13608/14208)
Epoch: 140 | Batch_idx: 120 |  Loss: (0.1272) |  Loss2: (0.0000) | Acc: (95.00%) (14834/15488)
Epoch: 140 | Batch_idx: 130 |  Loss: (0.1280) |  Loss2: (0.0000) | Acc: (95.00%) (16058/16768)
Epoch: 140 | Batch_idx: 140 |  Loss: (0.1279) |  Loss2: (0.0000) | Acc: (95.00%) (17291/18048)
Epoch: 140 | Batch_idx: 150 |  Loss: (0.1268) |  Loss2: (0.0000) | Acc: (95.00%) (18518/19328)
Epoch: 140 | Batch_idx: 160 |  Loss: (0.1275) |  Loss2: (0.0000) | Acc: (95.00%) (19742/20608)
Epoch: 140 | Batch_idx: 170 |  Loss: (0.1274) |  Loss2: (0.0000) | Acc: (95.00%) (20968/21888)
Epoch: 140 | Batch_idx: 180 |  Loss: (0.1272) |  Loss2: (0.0000) | Acc: (95.00%) (22190/23168)
Epoch: 140 | Batch_idx: 190 |  Loss: (0.1262) |  Loss2: (0.0000) | Acc: (95.00%) (23423/24448)
Epoch: 140 | Batch_idx: 200 |  Loss: (0.1266) |  Loss2: (0.0000) | Acc: (95.00%) (24640/25728)
Epoch: 140 | Batch_idx: 210 |  Loss: (0.1266) |  Loss2: (0.0000) | Acc: (95.00%) (25863/27008)
Epoch: 140 | Batch_idx: 220 |  Loss: (0.1272) |  Loss2: (0.0000) | Acc: (95.00%) (27073/28288)
Epoch: 140 | Batch_idx: 230 |  Loss: (0.1280) |  Loss2: (0.0000) | Acc: (95.00%) (28285/29568)
Epoch: 140 | Batch_idx: 240 |  Loss: (0.1282) |  Loss2: (0.0000) | Acc: (95.00%) (29498/30848)
Epoch: 140 | Batch_idx: 250 |  Loss: (0.1278) |  Loss2: (0.0000) | Acc: (95.00%) (30718/32128)
Epoch: 140 | Batch_idx: 260 |  Loss: (0.1276) |  Loss2: (0.0000) | Acc: (95.00%) (31949/33408)
Epoch: 140 | Batch_idx: 270 |  Loss: (0.1275) |  Loss2: (0.0000) | Acc: (95.00%) (33172/34688)
Epoch: 140 | Batch_idx: 280 |  Loss: (0.1283) |  Loss2: (0.0000) | Acc: (95.00%) (34383/35968)
Epoch: 140 | Batch_idx: 290 |  Loss: (0.1283) |  Loss2: (0.0000) | Acc: (95.00%) (35605/37248)
Epoch: 140 | Batch_idx: 300 |  Loss: (0.1285) |  Loss2: (0.0000) | Acc: (95.00%) (36822/38528)
Epoch: 140 | Batch_idx: 310 |  Loss: (0.1286) |  Loss2: (0.0000) | Acc: (95.00%) (38047/39808)
Epoch: 140 | Batch_idx: 320 |  Loss: (0.1287) |  Loss2: (0.0000) | Acc: (95.00%) (39268/41088)
Epoch: 140 | Batch_idx: 330 |  Loss: (0.1289) |  Loss2: (0.0000) | Acc: (95.00%) (40488/42368)
Epoch: 140 | Batch_idx: 340 |  Loss: (0.1289) |  Loss2: (0.0000) | Acc: (95.00%) (41711/43648)
Epoch: 140 | Batch_idx: 350 |  Loss: (0.1290) |  Loss2: (0.0000) | Acc: (95.00%) (42931/44928)
Epoch: 140 | Batch_idx: 360 |  Loss: (0.1289) |  Loss2: (0.0000) | Acc: (95.00%) (44155/46208)
Epoch: 140 | Batch_idx: 370 |  Loss: (0.1287) |  Loss2: (0.0000) | Acc: (95.00%) (45379/47488)
Epoch: 140 | Batch_idx: 380 |  Loss: (0.1283) |  Loss2: (0.0000) | Acc: (95.00%) (46614/48768)
Epoch: 140 | Batch_idx: 390 |  Loss: (0.1282) |  Loss2: (0.0000) | Acc: (95.00%) (47793/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_140.pth.tar'
# TEST : Loss: (0.4338) | Acc: (87.00%) (8702/10000)
percent tensor([0.5773, 0.5913, 0.5880, 0.5706, 0.5912, 0.5634, 0.5973, 0.5925, 0.5929,
        0.5906, 0.5872, 0.5936, 0.5845, 0.5898, 0.5794, 0.5731],
       device='cuda:0') torch.Size([16])
percent tensor([0.5500, 0.5442, 0.5154, 0.5412, 0.5404, 0.5255, 0.5475, 0.5616, 0.5391,
        0.5241, 0.5194, 0.5134, 0.5446, 0.5538, 0.5506, 0.5352],
       device='cuda:0') torch.Size([16])
percent tensor([0.5300, 0.4676, 0.4733, 0.5426, 0.4576, 0.5755, 0.4847, 0.5275, 0.5152,
        0.4564, 0.5032, 0.4576, 0.4536, 0.5928, 0.5317, 0.5302],
       device='cuda:0') torch.Size([16])
percent tensor([0.6472, 0.6750, 0.6213, 0.6104, 0.6103, 0.6470, 0.6498, 0.6039, 0.6475,
        0.6746, 0.6644, 0.6503, 0.6893, 0.6467, 0.6521, 0.6662],
       device='cuda:0') torch.Size([16])
percent tensor([0.6787, 0.6333, 0.6991, 0.7074, 0.7224, 0.7577, 0.7067, 0.6870, 0.7648,
        0.7064, 0.7447, 0.7104, 0.6113, 0.7778, 0.6645, 0.7391],
       device='cuda:0') torch.Size([16])
percent tensor([0.7779, 0.7894, 0.8052, 0.8065, 0.8325, 0.8206, 0.8132, 0.7914, 0.8112,
        0.8078, 0.8180, 0.7810, 0.7804, 0.8338, 0.7768, 0.8101],
       device='cuda:0') torch.Size([16])
percent tensor([0.4230, 0.5155, 0.5270, 0.4742, 0.6125, 0.6473, 0.5198, 0.3787, 0.5726,
        0.5022, 0.4961, 0.4478, 0.4661, 0.5154, 0.3257, 0.4464],
       device='cuda:0') torch.Size([16])
percent tensor([0.9996, 0.9996, 0.9991, 0.9997, 0.9992, 0.9989, 0.9995, 0.9994, 0.9994,
        0.9995, 0.9999, 0.9998, 0.9994, 0.9992, 0.9990, 0.9996],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(180.0778, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(819.0283, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(806.2812, device='cuda:0')
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1505.8981, device='cuda:0')
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(488.2645, device='cuda:0')
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2262.5762, device='cuda:0')
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4293.1851, device='cuda:0')
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1378.6033, device='cuda:0')
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6236.7871, device='cuda:0')
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11721.3193, device='cuda:0')
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3874.7971, device='cuda:0')
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16353.9688, device='cuda:0')
Epoch: 141 | Batch_idx: 0 |  Loss: (0.1128) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 141 | Batch_idx: 10 |  Loss: (0.1327) |  Loss2: (0.0000) | Acc: (95.00%) (1347/1408)
Epoch: 141 | Batch_idx: 20 |  Loss: (0.1580) |  Loss2: (0.0000) | Acc: (94.00%) (2539/2688)
Epoch: 141 | Batch_idx: 30 |  Loss: (0.1746) |  Loss2: (0.0000) | Acc: (93.00%) (3721/3968)
Epoch: 141 | Batch_idx: 40 |  Loss: (0.1791) |  Loss2: (0.0000) | Acc: (93.00%) (4911/5248)
Epoch: 141 | Batch_idx: 50 |  Loss: (0.1811) |  Loss2: (0.0000) | Acc: (93.00%) (6116/6528)
Epoch: 141 | Batch_idx: 60 |  Loss: (0.1830) |  Loss2: (0.0000) | Acc: (93.00%) (7315/7808)
Epoch: 141 | Batch_idx: 70 |  Loss: (0.1807) |  Loss2: (0.0000) | Acc: (93.00%) (8517/9088)
Epoch: 141 | Batch_idx: 80 |  Loss: (0.1845) |  Loss2: (0.0000) | Acc: (93.00%) (9711/10368)
Epoch: 141 | Batch_idx: 90 |  Loss: (0.1822) |  Loss2: (0.0000) | Acc: (93.00%) (10921/11648)
Epoch: 141 | Batch_idx: 100 |  Loss: (0.1822) |  Loss2: (0.0000) | Acc: (93.00%) (12120/12928)
Epoch: 141 | Batch_idx: 110 |  Loss: (0.1838) |  Loss2: (0.0000) | Acc: (93.00%) (13307/14208)
Epoch: 141 | Batch_idx: 120 |  Loss: (0.1829) |  Loss2: (0.0000) | Acc: (93.00%) (14518/15488)
Epoch: 141 | Batch_idx: 130 |  Loss: (0.1819) |  Loss2: (0.0000) | Acc: (93.00%) (15723/16768)
Epoch: 141 | Batch_idx: 140 |  Loss: (0.1811) |  Loss2: (0.0000) | Acc: (93.00%) (16929/18048)
Epoch: 141 | Batch_idx: 150 |  Loss: (0.1805) |  Loss2: (0.0000) | Acc: (93.00%) (18127/19328)
Epoch: 141 | Batch_idx: 160 |  Loss: (0.1802) |  Loss2: (0.0000) | Acc: (93.00%) (19320/20608)
Epoch: 141 | Batch_idx: 170 |  Loss: (0.1811) |  Loss2: (0.0000) | Acc: (93.00%) (20508/21888)
Epoch: 141 | Batch_idx: 180 |  Loss: (0.1807) |  Loss2: (0.0000) | Acc: (93.00%) (21707/23168)
Epoch: 141 | Batch_idx: 190 |  Loss: (0.1806) |  Loss2: (0.0000) | Acc: (93.00%) (22914/24448)
Epoch: 141 | Batch_idx: 200 |  Loss: (0.1806) |  Loss2: (0.0000) | Acc: (93.00%) (24111/25728)
Epoch: 141 | Batch_idx: 210 |  Loss: (0.1801) |  Loss2: (0.0000) | Acc: (93.00%) (25314/27008)
Epoch: 141 | Batch_idx: 220 |  Loss: (0.1789) |  Loss2: (0.0000) | Acc: (93.00%) (26531/28288)
Epoch: 141 | Batch_idx: 230 |  Loss: (0.1784) |  Loss2: (0.0000) | Acc: (93.00%) (27735/29568)
Epoch: 141 | Batch_idx: 240 |  Loss: (0.1779) |  Loss2: (0.0000) | Acc: (93.00%) (28942/30848)
Epoch: 141 | Batch_idx: 250 |  Loss: (0.1775) |  Loss2: (0.0000) | Acc: (93.00%) (30139/32128)
Epoch: 141 | Batch_idx: 260 |  Loss: (0.1766) |  Loss2: (0.0000) | Acc: (93.00%) (31344/33408)
Epoch: 141 | Batch_idx: 270 |  Loss: (0.1768) |  Loss2: (0.0000) | Acc: (93.00%) (32538/34688)
Epoch: 141 | Batch_idx: 280 |  Loss: (0.1761) |  Loss2: (0.0000) | Acc: (93.00%) (33746/35968)
Epoch: 141 | Batch_idx: 290 |  Loss: (0.1756) |  Loss2: (0.0000) | Acc: (93.00%) (34952/37248)
Epoch: 141 | Batch_idx: 300 |  Loss: (0.1751) |  Loss2: (0.0000) | Acc: (93.00%) (36154/38528)
Epoch: 141 | Batch_idx: 310 |  Loss: (0.1747) |  Loss2: (0.0000) | Acc: (93.00%) (37357/39808)
Epoch: 141 | Batch_idx: 320 |  Loss: (0.1738) |  Loss2: (0.0000) | Acc: (93.00%) (38574/41088)
Epoch: 141 | Batch_idx: 330 |  Loss: (0.1736) |  Loss2: (0.0000) | Acc: (93.00%) (39774/42368)
Epoch: 141 | Batch_idx: 340 |  Loss: (0.1734) |  Loss2: (0.0000) | Acc: (93.00%) (40979/43648)
Epoch: 141 | Batch_idx: 350 |  Loss: (0.1733) |  Loss2: (0.0000) | Acc: (93.00%) (42188/44928)
Epoch: 141 | Batch_idx: 360 |  Loss: (0.1729) |  Loss2: (0.0000) | Acc: (93.00%) (43402/46208)
Epoch: 141 | Batch_idx: 370 |  Loss: (0.1726) |  Loss2: (0.0000) | Acc: (93.00%) (44612/47488)
Epoch: 141 | Batch_idx: 380 |  Loss: (0.1720) |  Loss2: (0.0000) | Acc: (93.00%) (45827/48768)
Epoch: 141 | Batch_idx: 390 |  Loss: (0.1711) |  Loss2: (0.0000) | Acc: (93.00%) (46998/50000)
# TEST : Loss: (0.4244) | Acc: (87.00%) (8721/10000)
percent tensor([0.5833, 0.5969, 0.5976, 0.5770, 0.6003, 0.5688, 0.6047, 0.5986, 0.5994,
        0.5977, 0.5930, 0.6038, 0.5903, 0.5944, 0.5853, 0.5787],
       device='cuda:0') torch.Size([16])
percent tensor([0.5487, 0.5386, 0.5231, 0.5409, 0.5439, 0.5251, 0.5469, 0.5609, 0.5378,
        0.5232, 0.5157, 0.5206, 0.5420, 0.5476, 0.5477, 0.5324],
       device='cuda:0') torch.Size([16])
percent tensor([0.5222, 0.4330, 0.4677, 0.5320, 0.4660, 0.5724, 0.4705, 0.5313, 0.5047,
        0.4254, 0.4690, 0.4362, 0.4281, 0.5810, 0.5132, 0.5144],
       device='cuda:0') torch.Size([16])
percent tensor([0.6719, 0.7026, 0.6412, 0.6307, 0.6306, 0.6755, 0.6762, 0.6227, 0.6727,
        0.6998, 0.6915, 0.6720, 0.7185, 0.6700, 0.6784, 0.6938],
       device='cuda:0') torch.Size([16])
percent tensor([0.6423, 0.5967, 0.6392, 0.6523, 0.6616, 0.7092, 0.6674, 0.6254, 0.7256,
        0.6747, 0.7102, 0.6669, 0.5915, 0.7263, 0.6210, 0.6933],
       device='cuda:0') torch.Size([16])
percent tensor([0.7680, 0.7827, 0.7930, 0.7973, 0.8210, 0.8073, 0.7999, 0.7817, 0.7947,
        0.7995, 0.8018, 0.7742, 0.7743, 0.8180, 0.7675, 0.7968],
       device='cuda:0') torch.Size([16])
percent tensor([0.4643, 0.5532, 0.5602, 0.4978, 0.6298, 0.6395, 0.5456, 0.4267, 0.5620,
        0.5419, 0.4911, 0.4890, 0.4899, 0.5149, 0.3572, 0.4884],
       device='cuda:0') torch.Size([16])
percent tensor([0.9996, 0.9995, 0.9991, 0.9996, 0.9994, 0.9989, 0.9996, 0.9994, 0.9995,
        0.9995, 0.9999, 0.9998, 0.9993, 0.9994, 0.9990, 0.9996],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 142 | Batch_idx: 0 |  Loss: (0.1826) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 142 | Batch_idx: 10 |  Loss: (0.1519) |  Loss2: (0.0000) | Acc: (94.00%) (1334/1408)
Epoch: 142 | Batch_idx: 20 |  Loss: (0.1350) |  Loss2: (0.0000) | Acc: (95.00%) (2563/2688)
Epoch: 142 | Batch_idx: 30 |  Loss: (0.1298) |  Loss2: (0.0000) | Acc: (95.00%) (3789/3968)
Epoch: 142 | Batch_idx: 40 |  Loss: (0.1289) |  Loss2: (0.0000) | Acc: (95.00%) (5019/5248)
Epoch: 142 | Batch_idx: 50 |  Loss: (0.1238) |  Loss2: (0.0000) | Acc: (95.00%) (6259/6528)
Epoch: 142 | Batch_idx: 60 |  Loss: (0.1241) |  Loss2: (0.0000) | Acc: (95.00%) (7482/7808)
Epoch: 142 | Batch_idx: 70 |  Loss: (0.1256) |  Loss2: (0.0000) | Acc: (95.00%) (8706/9088)
Epoch: 142 | Batch_idx: 80 |  Loss: (0.1237) |  Loss2: (0.0000) | Acc: (95.00%) (9942/10368)
Epoch: 142 | Batch_idx: 90 |  Loss: (0.1256) |  Loss2: (0.0000) | Acc: (95.00%) (11156/11648)
Epoch: 142 | Batch_idx: 100 |  Loss: (0.1248) |  Loss2: (0.0000) | Acc: (95.00%) (12388/12928)
Epoch: 142 | Batch_idx: 110 |  Loss: (0.1252) |  Loss2: (0.0000) | Acc: (95.00%) (13607/14208)
Epoch: 142 | Batch_idx: 120 |  Loss: (0.1248) |  Loss2: (0.0000) | Acc: (95.00%) (14836/15488)
Epoch: 142 | Batch_idx: 130 |  Loss: (0.1243) |  Loss2: (0.0000) | Acc: (95.00%) (16066/16768)
Epoch: 142 | Batch_idx: 140 |  Loss: (0.1234) |  Loss2: (0.0000) | Acc: (95.00%) (17294/18048)
Epoch: 142 | Batch_idx: 150 |  Loss: (0.1224) |  Loss2: (0.0000) | Acc: (95.00%) (18531/19328)
Epoch: 142 | Batch_idx: 160 |  Loss: (0.1241) |  Loss2: (0.0000) | Acc: (95.00%) (19731/20608)
Epoch: 142 | Batch_idx: 170 |  Loss: (0.1237) |  Loss2: (0.0000) | Acc: (95.00%) (20959/21888)
Epoch: 142 | Batch_idx: 180 |  Loss: (0.1259) |  Loss2: (0.0000) | Acc: (95.00%) (22167/23168)
Epoch: 142 | Batch_idx: 190 |  Loss: (0.1252) |  Loss2: (0.0000) | Acc: (95.00%) (23398/24448)
Epoch: 142 | Batch_idx: 200 |  Loss: (0.1253) |  Loss2: (0.0000) | Acc: (95.00%) (24619/25728)
Epoch: 142 | Batch_idx: 210 |  Loss: (0.1256) |  Loss2: (0.0000) | Acc: (95.00%) (25842/27008)
Epoch: 142 | Batch_idx: 220 |  Loss: (0.1259) |  Loss2: (0.0000) | Acc: (95.00%) (27063/28288)
Epoch: 142 | Batch_idx: 230 |  Loss: (0.1265) |  Loss2: (0.0000) | Acc: (95.00%) (28281/29568)
Epoch: 142 | Batch_idx: 240 |  Loss: (0.1264) |  Loss2: (0.0000) | Acc: (95.00%) (29509/30848)
Epoch: 142 | Batch_idx: 250 |  Loss: (0.1259) |  Loss2: (0.0000) | Acc: (95.00%) (30738/32128)
Epoch: 142 | Batch_idx: 260 |  Loss: (0.1261) |  Loss2: (0.0000) | Acc: (95.00%) (31960/33408)
Epoch: 142 | Batch_idx: 270 |  Loss: (0.1258) |  Loss2: (0.0000) | Acc: (95.00%) (33184/34688)
Epoch: 142 | Batch_idx: 280 |  Loss: (0.1257) |  Loss2: (0.0000) | Acc: (95.00%) (34407/35968)
Epoch: 142 | Batch_idx: 290 |  Loss: (0.1257) |  Loss2: (0.0000) | Acc: (95.00%) (35632/37248)
Epoch: 142 | Batch_idx: 300 |  Loss: (0.1255) |  Loss2: (0.0000) | Acc: (95.00%) (36862/38528)
Epoch: 142 | Batch_idx: 310 |  Loss: (0.1261) |  Loss2: (0.0000) | Acc: (95.00%) (38070/39808)
Epoch: 142 | Batch_idx: 320 |  Loss: (0.1253) |  Loss2: (0.0000) | Acc: (95.00%) (39307/41088)
Epoch: 142 | Batch_idx: 330 |  Loss: (0.1257) |  Loss2: (0.0000) | Acc: (95.00%) (40528/42368)
Epoch: 142 | Batch_idx: 340 |  Loss: (0.1257) |  Loss2: (0.0000) | Acc: (95.00%) (41750/43648)
Epoch: 142 | Batch_idx: 350 |  Loss: (0.1261) |  Loss2: (0.0000) | Acc: (95.00%) (42966/44928)
Epoch: 142 | Batch_idx: 360 |  Loss: (0.1262) |  Loss2: (0.0000) | Acc: (95.00%) (44183/46208)
Epoch: 142 | Batch_idx: 370 |  Loss: (0.1264) |  Loss2: (0.0000) | Acc: (95.00%) (45401/47488)
Epoch: 142 | Batch_idx: 380 |  Loss: (0.1261) |  Loss2: (0.0000) | Acc: (95.00%) (46632/48768)
Epoch: 142 | Batch_idx: 390 |  Loss: (0.1263) |  Loss2: (0.0000) | Acc: (95.00%) (47803/50000)
# TEST : Loss: (0.3976) | Acc: (87.00%) (8795/10000)
percent tensor([0.5814, 0.5960, 0.5927, 0.5764, 0.5973, 0.5651, 0.6016, 0.5984, 0.5978,
        0.5950, 0.5913, 0.5978, 0.5890, 0.5935, 0.5840, 0.5774],
       device='cuda:0') torch.Size([16])
percent tensor([0.5444, 0.5404, 0.5120, 0.5383, 0.5372, 0.5294, 0.5446, 0.5587, 0.5354,
        0.5176, 0.5147, 0.5072, 0.5375, 0.5516, 0.5493, 0.5344],
       device='cuda:0') torch.Size([16])
percent tensor([0.5199, 0.4423, 0.4557, 0.5222, 0.4596, 0.5732, 0.4748, 0.5233, 0.5056,
        0.4287, 0.4717, 0.4359, 0.4275, 0.5851, 0.5145, 0.5149],
       device='cuda:0') torch.Size([16])
percent tensor([0.6760, 0.7055, 0.6502, 0.6407, 0.6373, 0.6795, 0.6763, 0.6251, 0.6712,
        0.7049, 0.6921, 0.6823, 0.7206, 0.6756, 0.6824, 0.6923],
       device='cuda:0') torch.Size([16])
percent tensor([0.6443, 0.5858, 0.6237, 0.6445, 0.6575, 0.7080, 0.6558, 0.6317, 0.7225,
        0.6527, 0.6945, 0.6620, 0.5890, 0.7317, 0.6119, 0.6910],
       device='cuda:0') torch.Size([16])
percent tensor([0.7678, 0.7688, 0.7952, 0.7934, 0.8227, 0.8076, 0.7969, 0.7760, 0.7912,
        0.7780, 0.7891, 0.7696, 0.7645, 0.8125, 0.7576, 0.7952],
       device='cuda:0') torch.Size([16])
percent tensor([0.4389, 0.5306, 0.5568, 0.4988, 0.6138, 0.6428, 0.5309, 0.4094, 0.5386,
        0.4977, 0.4665, 0.4666, 0.4791, 0.4832, 0.3386, 0.4717],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9997, 0.9993, 0.9997, 0.9993, 0.9989, 0.9997, 0.9996, 0.9997,
        0.9997, 0.9999, 0.9998, 0.9996, 0.9996, 0.9990, 0.9995],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 143 | Batch_idx: 0 |  Loss: (0.1285) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 143 | Batch_idx: 10 |  Loss: (0.1433) |  Loss2: (0.0000) | Acc: (95.00%) (1342/1408)
Epoch: 143 | Batch_idx: 20 |  Loss: (0.1517) |  Loss2: (0.0000) | Acc: (94.00%) (2549/2688)
Epoch: 143 | Batch_idx: 30 |  Loss: (0.1506) |  Loss2: (0.0000) | Acc: (94.00%) (3764/3968)
Epoch: 143 | Batch_idx: 40 |  Loss: (0.1609) |  Loss2: (0.0000) | Acc: (94.00%) (4957/5248)
Epoch: 143 | Batch_idx: 50 |  Loss: (0.1615) |  Loss2: (0.0000) | Acc: (94.00%) (6157/6528)
Epoch: 143 | Batch_idx: 60 |  Loss: (0.1667) |  Loss2: (0.0000) | Acc: (94.00%) (7342/7808)
Epoch: 143 | Batch_idx: 70 |  Loss: (0.1639) |  Loss2: (0.0000) | Acc: (94.00%) (8553/9088)
Epoch: 143 | Batch_idx: 80 |  Loss: (0.1670) |  Loss2: (0.0000) | Acc: (94.00%) (9753/10368)
Epoch: 143 | Batch_idx: 90 |  Loss: (0.1649) |  Loss2: (0.0000) | Acc: (94.00%) (10968/11648)
Epoch: 143 | Batch_idx: 100 |  Loss: (0.1635) |  Loss2: (0.0000) | Acc: (94.00%) (12194/12928)
Epoch: 143 | Batch_idx: 110 |  Loss: (0.1622) |  Loss2: (0.0000) | Acc: (94.00%) (13403/14208)
Epoch: 143 | Batch_idx: 120 |  Loss: (0.1607) |  Loss2: (0.0000) | Acc: (94.00%) (14616/15488)
Epoch: 143 | Batch_idx: 130 |  Loss: (0.1598) |  Loss2: (0.0000) | Acc: (94.00%) (15828/16768)
Epoch: 143 | Batch_idx: 140 |  Loss: (0.1584) |  Loss2: (0.0000) | Acc: (94.00%) (17039/18048)
Epoch: 143 | Batch_idx: 150 |  Loss: (0.1561) |  Loss2: (0.0000) | Acc: (94.00%) (18261/19328)
Epoch: 143 | Batch_idx: 160 |  Loss: (0.1552) |  Loss2: (0.0000) | Acc: (94.00%) (19479/20608)
Epoch: 143 | Batch_idx: 170 |  Loss: (0.1540) |  Loss2: (0.0000) | Acc: (94.00%) (20695/21888)
Epoch: 143 | Batch_idx: 180 |  Loss: (0.1533) |  Loss2: (0.0000) | Acc: (94.00%) (21911/23168)
Epoch: 143 | Batch_idx: 190 |  Loss: (0.1512) |  Loss2: (0.0000) | Acc: (94.00%) (23141/24448)
Epoch: 143 | Batch_idx: 200 |  Loss: (0.1512) |  Loss2: (0.0000) | Acc: (94.00%) (24353/25728)
Epoch: 143 | Batch_idx: 210 |  Loss: (0.1509) |  Loss2: (0.0000) | Acc: (94.00%) (25570/27008)
Epoch: 143 | Batch_idx: 220 |  Loss: (0.1495) |  Loss2: (0.0000) | Acc: (94.00%) (26793/28288)
Epoch: 143 | Batch_idx: 230 |  Loss: (0.1505) |  Loss2: (0.0000) | Acc: (94.00%) (27992/29568)
Epoch: 143 | Batch_idx: 240 |  Loss: (0.1502) |  Loss2: (0.0000) | Acc: (94.00%) (29204/30848)
Epoch: 143 | Batch_idx: 250 |  Loss: (0.1496) |  Loss2: (0.0000) | Acc: (94.00%) (30426/32128)
Epoch: 143 | Batch_idx: 260 |  Loss: (0.1486) |  Loss2: (0.0000) | Acc: (94.00%) (31653/33408)
Epoch: 143 | Batch_idx: 270 |  Loss: (0.1477) |  Loss2: (0.0000) | Acc: (94.00%) (32879/34688)
Epoch: 143 | Batch_idx: 280 |  Loss: (0.1473) |  Loss2: (0.0000) | Acc: (94.00%) (34100/35968)
Epoch: 143 | Batch_idx: 290 |  Loss: (0.1466) |  Loss2: (0.0000) | Acc: (94.00%) (35326/37248)
Epoch: 143 | Batch_idx: 300 |  Loss: (0.1461) |  Loss2: (0.0000) | Acc: (94.00%) (36547/38528)
Epoch: 143 | Batch_idx: 310 |  Loss: (0.1455) |  Loss2: (0.0000) | Acc: (94.00%) (37779/39808)
Epoch: 143 | Batch_idx: 320 |  Loss: (0.1450) |  Loss2: (0.0000) | Acc: (94.00%) (38995/41088)
Epoch: 143 | Batch_idx: 330 |  Loss: (0.1445) |  Loss2: (0.0000) | Acc: (94.00%) (40216/42368)
Epoch: 143 | Batch_idx: 340 |  Loss: (0.1444) |  Loss2: (0.0000) | Acc: (94.00%) (41437/43648)
Epoch: 143 | Batch_idx: 350 |  Loss: (0.1446) |  Loss2: (0.0000) | Acc: (94.00%) (42645/44928)
Epoch: 143 | Batch_idx: 360 |  Loss: (0.1444) |  Loss2: (0.0000) | Acc: (94.00%) (43866/46208)
Epoch: 143 | Batch_idx: 370 |  Loss: (0.1440) |  Loss2: (0.0000) | Acc: (94.00%) (45088/47488)
Epoch: 143 | Batch_idx: 380 |  Loss: (0.1439) |  Loss2: (0.0000) | Acc: (94.00%) (46301/48768)
Epoch: 143 | Batch_idx: 390 |  Loss: (0.1434) |  Loss2: (0.0000) | Acc: (94.00%) (47481/50000)
# TEST : Loss: (0.4083) | Acc: (87.00%) (8768/10000)
percent tensor([0.5787, 0.5907, 0.5901, 0.5732, 0.5947, 0.5633, 0.5975, 0.5944, 0.5945,
        0.5911, 0.5880, 0.5951, 0.5856, 0.5889, 0.5802, 0.5741],
       device='cuda:0') torch.Size([16])
percent tensor([0.5391, 0.5314, 0.5109, 0.5332, 0.5298, 0.5230, 0.5364, 0.5523, 0.5294,
        0.5132, 0.5086, 0.5045, 0.5320, 0.5449, 0.5411, 0.5283],
       device='cuda:0') torch.Size([16])
percent tensor([0.5134, 0.4353, 0.4582, 0.5129, 0.4632, 0.5590, 0.4713, 0.5212, 0.4922,
        0.4228, 0.4595, 0.4291, 0.4252, 0.5605, 0.5074, 0.5067],
       device='cuda:0') torch.Size([16])
percent tensor([0.6604, 0.6836, 0.6358, 0.6220, 0.6206, 0.6630, 0.6595, 0.6093, 0.6521,
        0.6823, 0.6728, 0.6629, 0.7025, 0.6531, 0.6650, 0.6714],
       device='cuda:0') torch.Size([16])
percent tensor([0.6358, 0.5767, 0.6208, 0.6287, 0.6426, 0.6928, 0.6471, 0.6214, 0.7171,
        0.6422, 0.6798, 0.6620, 0.5932, 0.7205, 0.6003, 0.6719],
       device='cuda:0') torch.Size([16])
percent tensor([0.7513, 0.7502, 0.7849, 0.7816, 0.8111, 0.8036, 0.7765, 0.7597, 0.7781,
        0.7595, 0.7694, 0.7527, 0.7473, 0.7935, 0.7404, 0.7783],
       device='cuda:0') torch.Size([16])
percent tensor([0.4482, 0.5366, 0.5841, 0.5222, 0.6455, 0.6717, 0.5366, 0.4155, 0.5564,
        0.5013, 0.4692, 0.4768, 0.4764, 0.4895, 0.3513, 0.4886],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9995, 0.9993, 0.9997, 0.9990, 0.9987, 0.9995, 0.9995, 0.9996,
        0.9996, 0.9998, 0.9997, 0.9994, 0.9995, 0.9989, 0.9996],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 144 | Batch_idx: 0 |  Loss: (0.2400) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 144 | Batch_idx: 10 |  Loss: (0.1454) |  Loss2: (0.0000) | Acc: (94.00%) (1337/1408)
Epoch: 144 | Batch_idx: 20 |  Loss: (0.1303) |  Loss2: (0.0000) | Acc: (95.00%) (2563/2688)
Epoch: 144 | Batch_idx: 30 |  Loss: (0.1301) |  Loss2: (0.0000) | Acc: (95.00%) (3781/3968)
Epoch: 144 | Batch_idx: 40 |  Loss: (0.1266) |  Loss2: (0.0000) | Acc: (95.00%) (5008/5248)
Epoch: 144 | Batch_idx: 50 |  Loss: (0.1301) |  Loss2: (0.0000) | Acc: (95.00%) (6230/6528)
Epoch: 144 | Batch_idx: 60 |  Loss: (0.1278) |  Loss2: (0.0000) | Acc: (95.00%) (7463/7808)
Epoch: 144 | Batch_idx: 70 |  Loss: (0.1274) |  Loss2: (0.0000) | Acc: (95.00%) (8694/9088)
Epoch: 144 | Batch_idx: 80 |  Loss: (0.1237) |  Loss2: (0.0000) | Acc: (95.00%) (9934/10368)
Epoch: 144 | Batch_idx: 90 |  Loss: (0.1244) |  Loss2: (0.0000) | Acc: (95.00%) (11156/11648)
Epoch: 144 | Batch_idx: 100 |  Loss: (0.1235) |  Loss2: (0.0000) | Acc: (95.00%) (12390/12928)
Epoch: 144 | Batch_idx: 110 |  Loss: (0.1237) |  Loss2: (0.0000) | Acc: (95.00%) (13621/14208)
Epoch: 144 | Batch_idx: 120 |  Loss: (0.1236) |  Loss2: (0.0000) | Acc: (95.00%) (14842/15488)
Epoch: 144 | Batch_idx: 130 |  Loss: (0.1235) |  Loss2: (0.0000) | Acc: (95.00%) (16065/16768)
Epoch: 144 | Batch_idx: 140 |  Loss: (0.1248) |  Loss2: (0.0000) | Acc: (95.00%) (17281/18048)
Epoch: 144 | Batch_idx: 150 |  Loss: (0.1264) |  Loss2: (0.0000) | Acc: (95.00%) (18494/19328)
Epoch: 144 | Batch_idx: 160 |  Loss: (0.1260) |  Loss2: (0.0000) | Acc: (95.00%) (19727/20608)
Epoch: 144 | Batch_idx: 170 |  Loss: (0.1257) |  Loss2: (0.0000) | Acc: (95.00%) (20951/21888)
Epoch: 144 | Batch_idx: 180 |  Loss: (0.1253) |  Loss2: (0.0000) | Acc: (95.00%) (22182/23168)
Epoch: 144 | Batch_idx: 190 |  Loss: (0.1259) |  Loss2: (0.0000) | Acc: (95.00%) (23403/24448)
Epoch: 144 | Batch_idx: 200 |  Loss: (0.1257) |  Loss2: (0.0000) | Acc: (95.00%) (24631/25728)
Epoch: 144 | Batch_idx: 210 |  Loss: (0.1254) |  Loss2: (0.0000) | Acc: (95.00%) (25861/27008)
Epoch: 144 | Batch_idx: 220 |  Loss: (0.1261) |  Loss2: (0.0000) | Acc: (95.00%) (27082/28288)
Epoch: 144 | Batch_idx: 230 |  Loss: (0.1258) |  Loss2: (0.0000) | Acc: (95.00%) (28316/29568)
Epoch: 144 | Batch_idx: 240 |  Loss: (0.1248) |  Loss2: (0.0000) | Acc: (95.00%) (29557/30848)
Epoch: 144 | Batch_idx: 250 |  Loss: (0.1237) |  Loss2: (0.0000) | Acc: (95.00%) (30802/32128)
Epoch: 144 | Batch_idx: 260 |  Loss: (0.1237) |  Loss2: (0.0000) | Acc: (95.00%) (32032/33408)
Epoch: 144 | Batch_idx: 270 |  Loss: (0.1240) |  Loss2: (0.0000) | Acc: (95.00%) (33259/34688)
Epoch: 144 | Batch_idx: 280 |  Loss: (0.1233) |  Loss2: (0.0000) | Acc: (95.00%) (34497/35968)
Epoch: 144 | Batch_idx: 290 |  Loss: (0.1239) |  Loss2: (0.0000) | Acc: (95.00%) (35714/37248)
Epoch: 144 | Batch_idx: 300 |  Loss: (0.1252) |  Loss2: (0.0000) | Acc: (95.00%) (36922/38528)
Epoch: 144 | Batch_idx: 310 |  Loss: (0.1252) |  Loss2: (0.0000) | Acc: (95.00%) (38145/39808)
Epoch: 144 | Batch_idx: 320 |  Loss: (0.1256) |  Loss2: (0.0000) | Acc: (95.00%) (39361/41088)
Epoch: 144 | Batch_idx: 330 |  Loss: (0.1251) |  Loss2: (0.0000) | Acc: (95.00%) (40591/42368)
Epoch: 144 | Batch_idx: 340 |  Loss: (0.1251) |  Loss2: (0.0000) | Acc: (95.00%) (41813/43648)
Epoch: 144 | Batch_idx: 350 |  Loss: (0.1244) |  Loss2: (0.0000) | Acc: (95.00%) (43055/44928)
Epoch: 144 | Batch_idx: 360 |  Loss: (0.1242) |  Loss2: (0.0000) | Acc: (95.00%) (44280/46208)
Epoch: 144 | Batch_idx: 370 |  Loss: (0.1238) |  Loss2: (0.0000) | Acc: (95.00%) (45510/47488)
Epoch: 144 | Batch_idx: 380 |  Loss: (0.1240) |  Loss2: (0.0000) | Acc: (95.00%) (46730/48768)
Epoch: 144 | Batch_idx: 390 |  Loss: (0.1241) |  Loss2: (0.0000) | Acc: (95.00%) (47906/50000)
# TEST : Loss: (0.4134) | Acc: (87.00%) (8738/10000)
percent tensor([0.5776, 0.5905, 0.5843, 0.5710, 0.5895, 0.5623, 0.5952, 0.5916, 0.5927,
        0.5891, 0.5874, 0.5906, 0.5844, 0.5883, 0.5796, 0.5731],
       device='cuda:0') torch.Size([16])
percent tensor([0.5363, 0.5285, 0.5058, 0.5313, 0.5259, 0.5182, 0.5330, 0.5510, 0.5292,
        0.5098, 0.5069, 0.4993, 0.5302, 0.5433, 0.5363, 0.5262],
       device='cuda:0') torch.Size([16])
percent tensor([0.5101, 0.4413, 0.4622, 0.5212, 0.4594, 0.5588, 0.4765, 0.5234, 0.4979,
        0.4289, 0.4611, 0.4336, 0.4292, 0.5730, 0.5031, 0.5098],
       device='cuda:0') torch.Size([16])
percent tensor([0.6604, 0.6846, 0.6318, 0.6199, 0.6182, 0.6637, 0.6574, 0.6089, 0.6526,
        0.6817, 0.6745, 0.6610, 0.7025, 0.6528, 0.6664, 0.6702],
       device='cuda:0') torch.Size([16])
percent tensor([0.6496, 0.5811, 0.6362, 0.6531, 0.6648, 0.7093, 0.6443, 0.6319, 0.7332,
        0.6446, 0.6979, 0.6659, 0.6025, 0.7065, 0.6171, 0.6795],
       device='cuda:0') torch.Size([16])
percent tensor([0.7509, 0.7584, 0.7845, 0.7788, 0.8122, 0.7977, 0.7803, 0.7624, 0.7806,
        0.7652, 0.7813, 0.7522, 0.7504, 0.7906, 0.7380, 0.7777],
       device='cuda:0') torch.Size([16])
percent tensor([0.4469, 0.5685, 0.5919, 0.5237, 0.6513, 0.6702, 0.5468, 0.4322, 0.5574,
        0.5473, 0.4941, 0.4846, 0.4879, 0.4871, 0.3516, 0.4889],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9997, 0.9992, 0.9996, 0.9994, 0.9992, 0.9996, 0.9994, 0.9998,
        0.9998, 0.9999, 0.9998, 0.9997, 0.9994, 0.9991, 0.9997],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 145 | Batch_idx: 0 |  Loss: (0.0768) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 145 | Batch_idx: 10 |  Loss: (0.1308) |  Loss2: (0.0000) | Acc: (95.00%) (1342/1408)
Epoch: 145 | Batch_idx: 20 |  Loss: (0.1369) |  Loss2: (0.0000) | Acc: (94.00%) (2553/2688)
Epoch: 145 | Batch_idx: 30 |  Loss: (0.1454) |  Loss2: (0.0000) | Acc: (94.00%) (3757/3968)
Epoch: 145 | Batch_idx: 40 |  Loss: (0.1490) |  Loss2: (0.0000) | Acc: (94.00%) (4956/5248)
Epoch: 145 | Batch_idx: 50 |  Loss: (0.1475) |  Loss2: (0.0000) | Acc: (94.00%) (6177/6528)
Epoch: 145 | Batch_idx: 60 |  Loss: (0.1472) |  Loss2: (0.0000) | Acc: (94.00%) (7392/7808)
Epoch: 145 | Batch_idx: 70 |  Loss: (0.1486) |  Loss2: (0.0000) | Acc: (94.00%) (8605/9088)
Epoch: 145 | Batch_idx: 80 |  Loss: (0.1527) |  Loss2: (0.0000) | Acc: (94.00%) (9802/10368)
Epoch: 145 | Batch_idx: 90 |  Loss: (0.1508) |  Loss2: (0.0000) | Acc: (94.00%) (11018/11648)
Epoch: 145 | Batch_idx: 100 |  Loss: (0.1509) |  Loss2: (0.0000) | Acc: (94.00%) (12230/12928)
Epoch: 145 | Batch_idx: 110 |  Loss: (0.1523) |  Loss2: (0.0000) | Acc: (94.00%) (13436/14208)
Epoch: 145 | Batch_idx: 120 |  Loss: (0.1537) |  Loss2: (0.0000) | Acc: (94.00%) (14647/15488)
Epoch: 145 | Batch_idx: 130 |  Loss: (0.1525) |  Loss2: (0.0000) | Acc: (94.00%) (15865/16768)
Epoch: 145 | Batch_idx: 140 |  Loss: (0.1522) |  Loss2: (0.0000) | Acc: (94.00%) (17083/18048)
Epoch: 145 | Batch_idx: 150 |  Loss: (0.1513) |  Loss2: (0.0000) | Acc: (94.00%) (18301/19328)
Epoch: 145 | Batch_idx: 160 |  Loss: (0.1521) |  Loss2: (0.0000) | Acc: (94.00%) (19509/20608)
Epoch: 145 | Batch_idx: 170 |  Loss: (0.1514) |  Loss2: (0.0000) | Acc: (94.00%) (20726/21888)
Epoch: 145 | Batch_idx: 180 |  Loss: (0.1523) |  Loss2: (0.0000) | Acc: (94.00%) (21921/23168)
Epoch: 145 | Batch_idx: 190 |  Loss: (0.1522) |  Loss2: (0.0000) | Acc: (94.00%) (23140/24448)
Epoch: 145 | Batch_idx: 200 |  Loss: (0.1507) |  Loss2: (0.0000) | Acc: (94.00%) (24371/25728)
Epoch: 145 | Batch_idx: 210 |  Loss: (0.1496) |  Loss2: (0.0000) | Acc: (94.00%) (25591/27008)
Epoch: 145 | Batch_idx: 220 |  Loss: (0.1496) |  Loss2: (0.0000) | Acc: (94.00%) (26798/28288)
Epoch: 145 | Batch_idx: 230 |  Loss: (0.1494) |  Loss2: (0.0000) | Acc: (94.00%) (28008/29568)
Epoch: 145 | Batch_idx: 240 |  Loss: (0.1504) |  Loss2: (0.0000) | Acc: (94.00%) (29211/30848)
Epoch: 145 | Batch_idx: 250 |  Loss: (0.1487) |  Loss2: (0.0000) | Acc: (94.00%) (30450/32128)
Epoch: 145 | Batch_idx: 260 |  Loss: (0.1484) |  Loss2: (0.0000) | Acc: (94.00%) (31666/33408)
Epoch: 145 | Batch_idx: 270 |  Loss: (0.1479) |  Loss2: (0.0000) | Acc: (94.00%) (32887/34688)
Epoch: 145 | Batch_idx: 280 |  Loss: (0.1477) |  Loss2: (0.0000) | Acc: (94.00%) (34099/35968)
Epoch: 145 | Batch_idx: 290 |  Loss: (0.1476) |  Loss2: (0.0000) | Acc: (94.00%) (35312/37248)
Epoch: 145 | Batch_idx: 300 |  Loss: (0.1474) |  Loss2: (0.0000) | Acc: (94.00%) (36528/38528)
Epoch: 145 | Batch_idx: 310 |  Loss: (0.1476) |  Loss2: (0.0000) | Acc: (94.00%) (37738/39808)
Epoch: 145 | Batch_idx: 320 |  Loss: (0.1476) |  Loss2: (0.0000) | Acc: (94.00%) (38959/41088)
Epoch: 145 | Batch_idx: 330 |  Loss: (0.1470) |  Loss2: (0.0000) | Acc: (94.00%) (40180/42368)
Epoch: 145 | Batch_idx: 340 |  Loss: (0.1466) |  Loss2: (0.0000) | Acc: (94.00%) (41403/43648)
Epoch: 145 | Batch_idx: 350 |  Loss: (0.1460) |  Loss2: (0.0000) | Acc: (94.00%) (42620/44928)
Epoch: 145 | Batch_idx: 360 |  Loss: (0.1456) |  Loss2: (0.0000) | Acc: (94.00%) (43844/46208)
Epoch: 145 | Batch_idx: 370 |  Loss: (0.1448) |  Loss2: (0.0000) | Acc: (94.00%) (45073/47488)
Epoch: 145 | Batch_idx: 380 |  Loss: (0.1445) |  Loss2: (0.0000) | Acc: (94.00%) (46292/48768)
Epoch: 145 | Batch_idx: 390 |  Loss: (0.1436) |  Loss2: (0.0000) | Acc: (94.00%) (47478/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_145.pth.tar'
# TEST : Loss: (0.3822) | Acc: (88.00%) (8814/10000)
percent tensor([0.5839, 0.5971, 0.5927, 0.5767, 0.5976, 0.5675, 0.6022, 0.5987, 0.5995,
        0.5957, 0.5938, 0.5976, 0.5910, 0.5938, 0.5857, 0.5788],
       device='cuda:0') torch.Size([16])
percent tensor([0.5389, 0.5264, 0.5026, 0.5309, 0.5230, 0.5182, 0.5308, 0.5481, 0.5299,
        0.5073, 0.5080, 0.4962, 0.5303, 0.5479, 0.5343, 0.5273],
       device='cuda:0') torch.Size([16])
percent tensor([0.5224, 0.4398, 0.4731, 0.5348, 0.4636, 0.5705, 0.4758, 0.5275, 0.5126,
        0.4329, 0.4737, 0.4461, 0.4386, 0.5863, 0.5082, 0.5185],
       device='cuda:0') torch.Size([16])
percent tensor([0.6570, 0.6795, 0.6241, 0.6157, 0.6096, 0.6528, 0.6536, 0.6045, 0.6509,
        0.6780, 0.6707, 0.6579, 0.6995, 0.6523, 0.6600, 0.6650],
       device='cuda:0') torch.Size([16])
percent tensor([0.6619, 0.5998, 0.6446, 0.6678, 0.6710, 0.6990, 0.6674, 0.6519, 0.7246,
        0.6530, 0.7019, 0.6732, 0.6100, 0.7002, 0.6379, 0.6818],
       device='cuda:0') torch.Size([16])
percent tensor([0.7538, 0.7596, 0.7889, 0.7848, 0.8186, 0.7987, 0.7836, 0.7675, 0.7827,
        0.7648, 0.7818, 0.7539, 0.7525, 0.7917, 0.7404, 0.7814],
       device='cuda:0') torch.Size([16])
percent tensor([0.3927, 0.5272, 0.5882, 0.4982, 0.6568, 0.6532, 0.5098, 0.4263, 0.5178,
        0.4919, 0.4396, 0.4315, 0.4365, 0.4262, 0.3238, 0.4447],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9997, 0.9992, 0.9997, 0.9994, 0.9991, 0.9995, 0.9996, 0.9997,
        0.9997, 0.9998, 0.9998, 0.9996, 0.9993, 0.9991, 0.9997],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 146 | Batch_idx: 0 |  Loss: (0.1563) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 146 | Batch_idx: 10 |  Loss: (0.1272) |  Loss2: (0.0000) | Acc: (95.00%) (1351/1408)
Epoch: 146 | Batch_idx: 20 |  Loss: (0.1215) |  Loss2: (0.0000) | Acc: (95.00%) (2580/2688)
Epoch: 146 | Batch_idx: 30 |  Loss: (0.1173) |  Loss2: (0.0000) | Acc: (96.00%) (3810/3968)
Epoch: 146 | Batch_idx: 40 |  Loss: (0.1164) |  Loss2: (0.0000) | Acc: (96.00%) (5040/5248)
Epoch: 146 | Batch_idx: 50 |  Loss: (0.1168) |  Loss2: (0.0000) | Acc: (95.00%) (6266/6528)
Epoch: 146 | Batch_idx: 60 |  Loss: (0.1151) |  Loss2: (0.0000) | Acc: (96.00%) (7498/7808)
Epoch: 146 | Batch_idx: 70 |  Loss: (0.1152) |  Loss2: (0.0000) | Acc: (95.00%) (8720/9088)
Epoch: 146 | Batch_idx: 80 |  Loss: (0.1138) |  Loss2: (0.0000) | Acc: (96.00%) (9957/10368)
Epoch: 146 | Batch_idx: 90 |  Loss: (0.1145) |  Loss2: (0.0000) | Acc: (95.00%) (11181/11648)
Epoch: 146 | Batch_idx: 100 |  Loss: (0.1148) |  Loss2: (0.0000) | Acc: (95.00%) (12408/12928)
Epoch: 146 | Batch_idx: 110 |  Loss: (0.1151) |  Loss2: (0.0000) | Acc: (96.00%) (13640/14208)
Epoch: 146 | Batch_idx: 120 |  Loss: (0.1159) |  Loss2: (0.0000) | Acc: (95.00%) (14863/15488)
Epoch: 146 | Batch_idx: 130 |  Loss: (0.1148) |  Loss2: (0.0000) | Acc: (96.00%) (16102/16768)
Epoch: 146 | Batch_idx: 140 |  Loss: (0.1153) |  Loss2: (0.0000) | Acc: (95.00%) (17326/18048)
Epoch: 146 | Batch_idx: 150 |  Loss: (0.1150) |  Loss2: (0.0000) | Acc: (96.00%) (18559/19328)
Epoch: 146 | Batch_idx: 160 |  Loss: (0.1142) |  Loss2: (0.0000) | Acc: (96.00%) (19802/20608)
Epoch: 146 | Batch_idx: 170 |  Loss: (0.1151) |  Loss2: (0.0000) | Acc: (96.00%) (21031/21888)
Epoch: 146 | Batch_idx: 180 |  Loss: (0.1140) |  Loss2: (0.0000) | Acc: (96.00%) (22270/23168)
Epoch: 146 | Batch_idx: 190 |  Loss: (0.1144) |  Loss2: (0.0000) | Acc: (96.00%) (23497/24448)
Epoch: 146 | Batch_idx: 200 |  Loss: (0.1138) |  Loss2: (0.0000) | Acc: (96.00%) (24732/25728)
Epoch: 146 | Batch_idx: 210 |  Loss: (0.1146) |  Loss2: (0.0000) | Acc: (96.00%) (25958/27008)
Epoch: 146 | Batch_idx: 220 |  Loss: (0.1145) |  Loss2: (0.0000) | Acc: (96.00%) (27187/28288)
Epoch: 146 | Batch_idx: 230 |  Loss: (0.1146) |  Loss2: (0.0000) | Acc: (96.00%) (28417/29568)
Epoch: 146 | Batch_idx: 240 |  Loss: (0.1152) |  Loss2: (0.0000) | Acc: (96.00%) (29639/30848)
Epoch: 146 | Batch_idx: 250 |  Loss: (0.1160) |  Loss2: (0.0000) | Acc: (96.00%) (30861/32128)
Epoch: 146 | Batch_idx: 260 |  Loss: (0.1158) |  Loss2: (0.0000) | Acc: (96.00%) (32087/33408)
Epoch: 146 | Batch_idx: 270 |  Loss: (0.1163) |  Loss2: (0.0000) | Acc: (96.00%) (33311/34688)
Epoch: 146 | Batch_idx: 280 |  Loss: (0.1174) |  Loss2: (0.0000) | Acc: (95.00%) (34527/35968)
Epoch: 146 | Batch_idx: 290 |  Loss: (0.1182) |  Loss2: (0.0000) | Acc: (95.00%) (35750/37248)
Epoch: 146 | Batch_idx: 300 |  Loss: (0.1183) |  Loss2: (0.0000) | Acc: (95.00%) (36976/38528)
Epoch: 146 | Batch_idx: 310 |  Loss: (0.1184) |  Loss2: (0.0000) | Acc: (95.00%) (38199/39808)
Epoch: 146 | Batch_idx: 320 |  Loss: (0.1184) |  Loss2: (0.0000) | Acc: (95.00%) (39429/41088)
Epoch: 146 | Batch_idx: 330 |  Loss: (0.1184) |  Loss2: (0.0000) | Acc: (95.00%) (40653/42368)
Epoch: 146 | Batch_idx: 340 |  Loss: (0.1185) |  Loss2: (0.0000) | Acc: (95.00%) (41877/43648)
Epoch: 146 | Batch_idx: 350 |  Loss: (0.1189) |  Loss2: (0.0000) | Acc: (95.00%) (43096/44928)
Epoch: 146 | Batch_idx: 360 |  Loss: (0.1195) |  Loss2: (0.0000) | Acc: (95.00%) (44312/46208)
Epoch: 146 | Batch_idx: 370 |  Loss: (0.1193) |  Loss2: (0.0000) | Acc: (95.00%) (45543/47488)
Epoch: 146 | Batch_idx: 380 |  Loss: (0.1194) |  Loss2: (0.0000) | Acc: (95.00%) (46769/48768)
Epoch: 146 | Batch_idx: 390 |  Loss: (0.1194) |  Loss2: (0.0000) | Acc: (95.00%) (47952/50000)
# TEST : Loss: (0.4018) | Acc: (87.00%) (8770/10000)
percent tensor([0.5833, 0.5949, 0.5911, 0.5747, 0.5961, 0.5664, 0.6013, 0.5965, 0.5988,
        0.5945, 0.5934, 0.5974, 0.5901, 0.5916, 0.5839, 0.5774],
       device='cuda:0') torch.Size([16])
percent tensor([0.5365, 0.5231, 0.5058, 0.5330, 0.5237, 0.5134, 0.5304, 0.5504, 0.5275,
        0.5079, 0.5054, 0.4999, 0.5281, 0.5434, 0.5330, 0.5231],
       device='cuda:0') torch.Size([16])
percent tensor([0.5176, 0.4440, 0.4698, 0.5406, 0.4623, 0.5616, 0.4741, 0.5322, 0.5076,
        0.4341, 0.4699, 0.4418, 0.4342, 0.5853, 0.5069, 0.5139],
       device='cuda:0') torch.Size([16])
percent tensor([0.6624, 0.6770, 0.6284, 0.6213, 0.6152, 0.6606, 0.6575, 0.6042, 0.6564,
        0.6770, 0.6727, 0.6603, 0.7026, 0.6570, 0.6593, 0.6684],
       device='cuda:0') torch.Size([16])
percent tensor([0.6681, 0.5971, 0.6704, 0.6855, 0.6846, 0.7116, 0.6722, 0.6530, 0.7307,
        0.6666, 0.6943, 0.6969, 0.6085, 0.7219, 0.6309, 0.6910],
       device='cuda:0') torch.Size([16])
percent tensor([0.7564, 0.7654, 0.7898, 0.7933, 0.8195, 0.8108, 0.7795, 0.7668, 0.7819,
        0.7745, 0.7795, 0.7564, 0.7530, 0.8042, 0.7471, 0.7884],
       device='cuda:0') torch.Size([16])
percent tensor([0.3981, 0.5469, 0.5738, 0.4980, 0.6349, 0.6902, 0.5032, 0.3962, 0.5193,
        0.5133, 0.4494, 0.4344, 0.4467, 0.4979, 0.3447, 0.4609],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9996, 0.9988, 0.9994, 0.9995, 0.9992, 0.9995, 0.9993, 0.9997,
        0.9996, 0.9998, 0.9995, 0.9995, 0.9992, 0.9992, 0.9996],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 147 | Batch_idx: 0 |  Loss: (0.0859) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 147 | Batch_idx: 10 |  Loss: (0.1428) |  Loss2: (0.0000) | Acc: (95.00%) (1343/1408)
Epoch: 147 | Batch_idx: 20 |  Loss: (0.1415) |  Loss2: (0.0000) | Acc: (95.00%) (2566/2688)
Epoch: 147 | Batch_idx: 30 |  Loss: (0.1346) |  Loss2: (0.0000) | Acc: (95.00%) (3789/3968)
Epoch: 147 | Batch_idx: 40 |  Loss: (0.1441) |  Loss2: (0.0000) | Acc: (95.00%) (4995/5248)
Epoch: 147 | Batch_idx: 50 |  Loss: (0.1458) |  Loss2: (0.0000) | Acc: (95.00%) (6206/6528)
Epoch: 147 | Batch_idx: 60 |  Loss: (0.1466) |  Loss2: (0.0000) | Acc: (94.00%) (7416/7808)
Epoch: 147 | Batch_idx: 70 |  Loss: (0.1473) |  Loss2: (0.0000) | Acc: (94.00%) (8623/9088)
Epoch: 147 | Batch_idx: 80 |  Loss: (0.1504) |  Loss2: (0.0000) | Acc: (94.00%) (9828/10368)
Epoch: 147 | Batch_idx: 90 |  Loss: (0.1520) |  Loss2: (0.0000) | Acc: (94.00%) (11031/11648)
Epoch: 147 | Batch_idx: 100 |  Loss: (0.1496) |  Loss2: (0.0000) | Acc: (94.00%) (12255/12928)
Epoch: 147 | Batch_idx: 110 |  Loss: (0.1461) |  Loss2: (0.0000) | Acc: (94.00%) (13492/14208)
Epoch: 147 | Batch_idx: 120 |  Loss: (0.1461) |  Loss2: (0.0000) | Acc: (94.00%) (14707/15488)
Epoch: 147 | Batch_idx: 130 |  Loss: (0.1452) |  Loss2: (0.0000) | Acc: (94.00%) (15927/16768)
Epoch: 147 | Batch_idx: 140 |  Loss: (0.1467) |  Loss2: (0.0000) | Acc: (94.00%) (17131/18048)
Epoch: 147 | Batch_idx: 150 |  Loss: (0.1464) |  Loss2: (0.0000) | Acc: (94.00%) (18352/19328)
Epoch: 147 | Batch_idx: 160 |  Loss: (0.1446) |  Loss2: (0.0000) | Acc: (95.00%) (19578/20608)
Epoch: 147 | Batch_idx: 170 |  Loss: (0.1442) |  Loss2: (0.0000) | Acc: (95.00%) (20798/21888)
Epoch: 147 | Batch_idx: 180 |  Loss: (0.1446) |  Loss2: (0.0000) | Acc: (94.00%) (22006/23168)
Epoch: 147 | Batch_idx: 190 |  Loss: (0.1441) |  Loss2: (0.0000) | Acc: (94.00%) (23225/24448)
Epoch: 147 | Batch_idx: 200 |  Loss: (0.1444) |  Loss2: (0.0000) | Acc: (94.00%) (24435/25728)
Epoch: 147 | Batch_idx: 210 |  Loss: (0.1451) |  Loss2: (0.0000) | Acc: (94.00%) (25643/27008)
Epoch: 147 | Batch_idx: 220 |  Loss: (0.1456) |  Loss2: (0.0000) | Acc: (94.00%) (26859/28288)
Epoch: 147 | Batch_idx: 230 |  Loss: (0.1445) |  Loss2: (0.0000) | Acc: (95.00%) (28097/29568)
Epoch: 147 | Batch_idx: 240 |  Loss: (0.1442) |  Loss2: (0.0000) | Acc: (95.00%) (29319/30848)
Epoch: 147 | Batch_idx: 250 |  Loss: (0.1441) |  Loss2: (0.0000) | Acc: (95.00%) (30533/32128)
Epoch: 147 | Batch_idx: 260 |  Loss: (0.1431) |  Loss2: (0.0000) | Acc: (95.00%) (31762/33408)
Epoch: 147 | Batch_idx: 270 |  Loss: (0.1418) |  Loss2: (0.0000) | Acc: (95.00%) (32996/34688)
Epoch: 147 | Batch_idx: 280 |  Loss: (0.1417) |  Loss2: (0.0000) | Acc: (95.00%) (34219/35968)
Epoch: 147 | Batch_idx: 290 |  Loss: (0.1412) |  Loss2: (0.0000) | Acc: (95.00%) (35446/37248)
Epoch: 147 | Batch_idx: 300 |  Loss: (0.1420) |  Loss2: (0.0000) | Acc: (95.00%) (36645/38528)
Epoch: 147 | Batch_idx: 310 |  Loss: (0.1422) |  Loss2: (0.0000) | Acc: (95.00%) (37858/39808)
Epoch: 147 | Batch_idx: 320 |  Loss: (0.1424) |  Loss2: (0.0000) | Acc: (95.00%) (39074/41088)
Epoch: 147 | Batch_idx: 330 |  Loss: (0.1419) |  Loss2: (0.0000) | Acc: (95.00%) (40299/42368)
Epoch: 147 | Batch_idx: 340 |  Loss: (0.1417) |  Loss2: (0.0000) | Acc: (95.00%) (41514/43648)
Epoch: 147 | Batch_idx: 350 |  Loss: (0.1416) |  Loss2: (0.0000) | Acc: (95.00%) (42738/44928)
Epoch: 147 | Batch_idx: 360 |  Loss: (0.1414) |  Loss2: (0.0000) | Acc: (95.00%) (43964/46208)
Epoch: 147 | Batch_idx: 370 |  Loss: (0.1405) |  Loss2: (0.0000) | Acc: (95.00%) (45197/47488)
Epoch: 147 | Batch_idx: 380 |  Loss: (0.1404) |  Loss2: (0.0000) | Acc: (95.00%) (46420/48768)
Epoch: 147 | Batch_idx: 390 |  Loss: (0.1404) |  Loss2: (0.0000) | Acc: (95.00%) (47586/50000)
# TEST : Loss: (0.3923) | Acc: (88.00%) (8812/10000)
percent tensor([0.5851, 0.5953, 0.5932, 0.5763, 0.5982, 0.5690, 0.6023, 0.5980, 0.5998,
        0.5955, 0.5945, 0.5987, 0.5913, 0.5914, 0.5853, 0.5788],
       device='cuda:0') torch.Size([16])
percent tensor([0.5404, 0.5263, 0.5136, 0.5380, 0.5275, 0.5217, 0.5318, 0.5513, 0.5306,
        0.5121, 0.5092, 0.5066, 0.5301, 0.5439, 0.5379, 0.5289],
       device='cuda:0') torch.Size([16])
percent tensor([0.5393, 0.4751, 0.4810, 0.5507, 0.4714, 0.5778, 0.4946, 0.5389, 0.5212,
        0.4591, 0.4947, 0.4547, 0.4624, 0.5976, 0.5305, 0.5390],
       device='cuda:0') torch.Size([16])
percent tensor([0.6659, 0.6816, 0.6306, 0.6206, 0.6173, 0.6695, 0.6632, 0.6042, 0.6579,
        0.6796, 0.6764, 0.6632, 0.7061, 0.6613, 0.6664, 0.6739],
       device='cuda:0') torch.Size([16])
percent tensor([0.6444, 0.5799, 0.6538, 0.6657, 0.6698, 0.7041, 0.6589, 0.6354, 0.7108,
        0.6521, 0.6743, 0.6965, 0.5796, 0.7052, 0.6140, 0.6734],
       device='cuda:0') torch.Size([16])
percent tensor([0.7675, 0.7772, 0.7964, 0.8039, 0.8280, 0.8191, 0.7916, 0.7830, 0.7891,
        0.7865, 0.7893, 0.7633, 0.7615, 0.8109, 0.7619, 0.8002],
       device='cuda:0') torch.Size([16])
percent tensor([0.4327, 0.5697, 0.5989, 0.5245, 0.6589, 0.6884, 0.5373, 0.4477, 0.5454,
        0.5474, 0.4808, 0.4678, 0.4749, 0.5264, 0.3722, 0.4910],
       device='cuda:0') torch.Size([16])
percent tensor([0.9996, 0.9995, 0.9993, 0.9995, 0.9995, 0.9989, 0.9996, 0.9995, 0.9997,
        0.9996, 0.9999, 0.9996, 0.9995, 0.9992, 0.9992, 0.9996],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 148 | Batch_idx: 0 |  Loss: (0.1354) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 148 | Batch_idx: 10 |  Loss: (0.1172) |  Loss2: (0.0000) | Acc: (96.00%) (1355/1408)
Epoch: 148 | Batch_idx: 20 |  Loss: (0.1098) |  Loss2: (0.0000) | Acc: (96.00%) (2592/2688)
Epoch: 148 | Batch_idx: 30 |  Loss: (0.1081) |  Loss2: (0.0000) | Acc: (96.00%) (3828/3968)
Epoch: 148 | Batch_idx: 40 |  Loss: (0.1060) |  Loss2: (0.0000) | Acc: (96.00%) (5071/5248)
Epoch: 148 | Batch_idx: 50 |  Loss: (0.1083) |  Loss2: (0.0000) | Acc: (96.00%) (6303/6528)
Epoch: 148 | Batch_idx: 60 |  Loss: (0.1090) |  Loss2: (0.0000) | Acc: (96.00%) (7531/7808)
Epoch: 148 | Batch_idx: 70 |  Loss: (0.1082) |  Loss2: (0.0000) | Acc: (96.00%) (8765/9088)
Epoch: 148 | Batch_idx: 80 |  Loss: (0.1092) |  Loss2: (0.0000) | Acc: (96.00%) (9981/10368)
Epoch: 148 | Batch_idx: 90 |  Loss: (0.1095) |  Loss2: (0.0000) | Acc: (96.00%) (11207/11648)
Epoch: 148 | Batch_idx: 100 |  Loss: (0.1083) |  Loss2: (0.0000) | Acc: (96.00%) (12446/12928)
Epoch: 148 | Batch_idx: 110 |  Loss: (0.1083) |  Loss2: (0.0000) | Acc: (96.00%) (13679/14208)
Epoch: 148 | Batch_idx: 120 |  Loss: (0.1098) |  Loss2: (0.0000) | Acc: (96.00%) (14904/15488)
Epoch: 148 | Batch_idx: 130 |  Loss: (0.1102) |  Loss2: (0.0000) | Acc: (96.00%) (16138/16768)
Epoch: 148 | Batch_idx: 140 |  Loss: (0.1101) |  Loss2: (0.0000) | Acc: (96.00%) (17372/18048)
Epoch: 148 | Batch_idx: 150 |  Loss: (0.1124) |  Loss2: (0.0000) | Acc: (96.00%) (18587/19328)
Epoch: 148 | Batch_idx: 160 |  Loss: (0.1124) |  Loss2: (0.0000) | Acc: (96.00%) (19817/20608)
Epoch: 148 | Batch_idx: 170 |  Loss: (0.1119) |  Loss2: (0.0000) | Acc: (96.00%) (21054/21888)
Epoch: 148 | Batch_idx: 180 |  Loss: (0.1137) |  Loss2: (0.0000) | Acc: (96.00%) (22275/23168)
Epoch: 148 | Batch_idx: 190 |  Loss: (0.1149) |  Loss2: (0.0000) | Acc: (96.00%) (23495/24448)
Epoch: 148 | Batch_idx: 200 |  Loss: (0.1145) |  Loss2: (0.0000) | Acc: (96.00%) (24727/25728)
Epoch: 148 | Batch_idx: 210 |  Loss: (0.1140) |  Loss2: (0.0000) | Acc: (96.00%) (25965/27008)
Epoch: 148 | Batch_idx: 220 |  Loss: (0.1155) |  Loss2: (0.0000) | Acc: (96.00%) (27179/28288)
Epoch: 148 | Batch_idx: 230 |  Loss: (0.1157) |  Loss2: (0.0000) | Acc: (96.00%) (28408/29568)
Epoch: 148 | Batch_idx: 240 |  Loss: (0.1163) |  Loss2: (0.0000) | Acc: (96.00%) (29628/30848)
Epoch: 148 | Batch_idx: 250 |  Loss: (0.1170) |  Loss2: (0.0000) | Acc: (96.00%) (30853/32128)
Epoch: 148 | Batch_idx: 260 |  Loss: (0.1166) |  Loss2: (0.0000) | Acc: (96.00%) (32087/33408)
Epoch: 148 | Batch_idx: 270 |  Loss: (0.1164) |  Loss2: (0.0000) | Acc: (96.00%) (33321/34688)
Epoch: 148 | Batch_idx: 280 |  Loss: (0.1177) |  Loss2: (0.0000) | Acc: (96.00%) (34534/35968)
Epoch: 148 | Batch_idx: 290 |  Loss: (0.1184) |  Loss2: (0.0000) | Acc: (95.00%) (35754/37248)
Epoch: 148 | Batch_idx: 300 |  Loss: (0.1182) |  Loss2: (0.0000) | Acc: (95.00%) (36983/38528)
Epoch: 148 | Batch_idx: 310 |  Loss: (0.1183) |  Loss2: (0.0000) | Acc: (95.00%) (38213/39808)
Epoch: 148 | Batch_idx: 320 |  Loss: (0.1185) |  Loss2: (0.0000) | Acc: (96.00%) (39445/41088)
Epoch: 148 | Batch_idx: 330 |  Loss: (0.1187) |  Loss2: (0.0000) | Acc: (95.00%) (40670/42368)
Epoch: 148 | Batch_idx: 340 |  Loss: (0.1193) |  Loss2: (0.0000) | Acc: (95.00%) (41883/43648)
Epoch: 148 | Batch_idx: 350 |  Loss: (0.1190) |  Loss2: (0.0000) | Acc: (95.00%) (43117/44928)
Epoch: 148 | Batch_idx: 360 |  Loss: (0.1187) |  Loss2: (0.0000) | Acc: (95.00%) (44349/46208)
Epoch: 148 | Batch_idx: 370 |  Loss: (0.1185) |  Loss2: (0.0000) | Acc: (95.00%) (45582/47488)
Epoch: 148 | Batch_idx: 380 |  Loss: (0.1192) |  Loss2: (0.0000) | Acc: (95.00%) (46804/48768)
Epoch: 148 | Batch_idx: 390 |  Loss: (0.1197) |  Loss2: (0.0000) | Acc: (95.00%) (47984/50000)
# TEST : Loss: (0.4613) | Acc: (86.00%) (8643/10000)
percent tensor([0.5842, 0.5978, 0.5879, 0.5752, 0.5946, 0.5667, 0.6029, 0.5989, 0.6002,
        0.5962, 0.5951, 0.5954, 0.5916, 0.5956, 0.5858, 0.5790],
       device='cuda:0') torch.Size([16])
percent tensor([0.5378, 0.5274, 0.5076, 0.5292, 0.5241, 0.5174, 0.5306, 0.5496, 0.5287,
        0.5113, 0.5082, 0.4990, 0.5284, 0.5421, 0.5366, 0.5269],
       device='cuda:0') torch.Size([16])
percent tensor([0.5400, 0.4761, 0.4778, 0.5412, 0.4770, 0.5783, 0.5012, 0.5318, 0.5224,
        0.4632, 0.4969, 0.4613, 0.4610, 0.5943, 0.5337, 0.5438],
       device='cuda:0') torch.Size([16])
percent tensor([0.6635, 0.6842, 0.6343, 0.6188, 0.6199, 0.6677, 0.6631, 0.6080, 0.6556,
        0.6816, 0.6789, 0.6622, 0.7079, 0.6589, 0.6665, 0.6751],
       device='cuda:0') torch.Size([16])
percent tensor([0.6460, 0.5916, 0.6514, 0.6582, 0.6655, 0.7079, 0.6633, 0.6272, 0.7182,
        0.6525, 0.6913, 0.6758, 0.6103, 0.7158, 0.6189, 0.6781],
       device='cuda:0') torch.Size([16])
percent tensor([0.7751, 0.7670, 0.8048, 0.7952, 0.8292, 0.8152, 0.7971, 0.7823, 0.7869,
        0.7770, 0.7913, 0.7687, 0.7652, 0.8032, 0.7623, 0.7961],
       device='cuda:0') torch.Size([16])
percent tensor([0.4340, 0.5394, 0.6265, 0.5289, 0.6712, 0.6870, 0.5186, 0.4458, 0.5457,
        0.5116, 0.4952, 0.5066, 0.4661, 0.5053, 0.3474, 0.4575],
       device='cuda:0') torch.Size([16])
percent tensor([0.9995, 0.9997, 0.9994, 0.9998, 0.9995, 0.9986, 0.9995, 0.9997, 0.9997,
        0.9996, 0.9999, 0.9997, 0.9996, 0.9991, 0.9993, 0.9995],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 149 | Batch_idx: 0 |  Loss: (0.1134) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 149 | Batch_idx: 10 |  Loss: (0.1136) |  Loss2: (0.0000) | Acc: (95.00%) (1350/1408)
Epoch: 149 | Batch_idx: 20 |  Loss: (0.1180) |  Loss2: (0.0000) | Acc: (95.00%) (2579/2688)
Epoch: 149 | Batch_idx: 30 |  Loss: (0.1225) |  Loss2: (0.0000) | Acc: (95.00%) (3800/3968)
Epoch: 149 | Batch_idx: 40 |  Loss: (0.1322) |  Loss2: (0.0000) | Acc: (95.00%) (5004/5248)
Epoch: 149 | Batch_idx: 50 |  Loss: (0.1355) |  Loss2: (0.0000) | Acc: (95.00%) (6222/6528)
Epoch: 149 | Batch_idx: 60 |  Loss: (0.1438) |  Loss2: (0.0000) | Acc: (94.00%) (7414/7808)
Epoch: 149 | Batch_idx: 70 |  Loss: (0.1455) |  Loss2: (0.0000) | Acc: (94.00%) (8625/9088)
Epoch: 149 | Batch_idx: 80 |  Loss: (0.1474) |  Loss2: (0.0000) | Acc: (94.00%) (9832/10368)
Epoch: 149 | Batch_idx: 90 |  Loss: (0.1450) |  Loss2: (0.0000) | Acc: (94.00%) (11064/11648)
Epoch: 149 | Batch_idx: 100 |  Loss: (0.1464) |  Loss2: (0.0000) | Acc: (94.00%) (12271/12928)
Epoch: 149 | Batch_idx: 110 |  Loss: (0.1488) |  Loss2: (0.0000) | Acc: (94.00%) (13472/14208)
Epoch: 149 | Batch_idx: 120 |  Loss: (0.1487) |  Loss2: (0.0000) | Acc: (94.00%) (14679/15488)
Epoch: 149 | Batch_idx: 130 |  Loss: (0.1500) |  Loss2: (0.0000) | Acc: (94.00%) (15877/16768)
Epoch: 149 | Batch_idx: 140 |  Loss: (0.1509) |  Loss2: (0.0000) | Acc: (94.00%) (17085/18048)
Epoch: 149 | Batch_idx: 150 |  Loss: (0.1516) |  Loss2: (0.0000) | Acc: (94.00%) (18285/19328)
Epoch: 149 | Batch_idx: 160 |  Loss: (0.1513) |  Loss2: (0.0000) | Acc: (94.00%) (19504/20608)
Epoch: 149 | Batch_idx: 170 |  Loss: (0.1514) |  Loss2: (0.0000) | Acc: (94.00%) (20704/21888)
Epoch: 149 | Batch_idx: 180 |  Loss: (0.1516) |  Loss2: (0.0000) | Acc: (94.00%) (21912/23168)
Epoch: 149 | Batch_idx: 190 |  Loss: (0.1518) |  Loss2: (0.0000) | Acc: (94.00%) (23120/24448)
Epoch: 149 | Batch_idx: 200 |  Loss: (0.1508) |  Loss2: (0.0000) | Acc: (94.00%) (24344/25728)
Epoch: 149 | Batch_idx: 210 |  Loss: (0.1511) |  Loss2: (0.0000) | Acc: (94.00%) (25554/27008)
Epoch: 149 | Batch_idx: 220 |  Loss: (0.1507) |  Loss2: (0.0000) | Acc: (94.00%) (26771/28288)
Epoch: 149 | Batch_idx: 230 |  Loss: (0.1505) |  Loss2: (0.0000) | Acc: (94.00%) (27984/29568)
Epoch: 149 | Batch_idx: 240 |  Loss: (0.1498) |  Loss2: (0.0000) | Acc: (94.00%) (29205/30848)
Epoch: 149 | Batch_idx: 250 |  Loss: (0.1494) |  Loss2: (0.0000) | Acc: (94.00%) (30414/32128)
Epoch: 149 | Batch_idx: 260 |  Loss: (0.1478) |  Loss2: (0.0000) | Acc: (94.00%) (31651/33408)
Epoch: 149 | Batch_idx: 270 |  Loss: (0.1478) |  Loss2: (0.0000) | Acc: (94.00%) (32859/34688)
Epoch: 149 | Batch_idx: 280 |  Loss: (0.1473) |  Loss2: (0.0000) | Acc: (94.00%) (34080/35968)
Epoch: 149 | Batch_idx: 290 |  Loss: (0.1465) |  Loss2: (0.0000) | Acc: (94.00%) (35313/37248)
Epoch: 149 | Batch_idx: 300 |  Loss: (0.1464) |  Loss2: (0.0000) | Acc: (94.00%) (36528/38528)
Epoch: 149 | Batch_idx: 310 |  Loss: (0.1466) |  Loss2: (0.0000) | Acc: (94.00%) (37733/39808)
Epoch: 149 | Batch_idx: 320 |  Loss: (0.1462) |  Loss2: (0.0000) | Acc: (94.00%) (38957/41088)
Epoch: 149 | Batch_idx: 330 |  Loss: (0.1466) |  Loss2: (0.0000) | Acc: (94.00%) (40151/42368)
Epoch: 149 | Batch_idx: 340 |  Loss: (0.1465) |  Loss2: (0.0000) | Acc: (94.00%) (41367/43648)
Epoch: 149 | Batch_idx: 350 |  Loss: (0.1461) |  Loss2: (0.0000) | Acc: (94.00%) (42584/44928)
Epoch: 149 | Batch_idx: 360 |  Loss: (0.1458) |  Loss2: (0.0000) | Acc: (94.00%) (43799/46208)
Epoch: 149 | Batch_idx: 370 |  Loss: (0.1454) |  Loss2: (0.0000) | Acc: (94.00%) (45012/47488)
Epoch: 149 | Batch_idx: 380 |  Loss: (0.1453) |  Loss2: (0.0000) | Acc: (94.00%) (46224/48768)
Epoch: 149 | Batch_idx: 390 |  Loss: (0.1451) |  Loss2: (0.0000) | Acc: (94.00%) (47399/50000)
# TEST : Loss: (0.4147) | Acc: (87.00%) (8766/10000)
percent tensor([0.5859, 0.6003, 0.5920, 0.5779, 0.5980, 0.5690, 0.6050, 0.6022, 0.6033,
        0.5984, 0.5974, 0.5983, 0.5937, 0.5982, 0.5879, 0.5810],
       device='cuda:0') torch.Size([16])
percent tensor([0.5471, 0.5355, 0.5150, 0.5400, 0.5296, 0.5243, 0.5375, 0.5547, 0.5380,
        0.5197, 0.5172, 0.5081, 0.5360, 0.5526, 0.5439, 0.5355],
       device='cuda:0') torch.Size([16])
percent tensor([0.5453, 0.4828, 0.4852, 0.5454, 0.4757, 0.5775, 0.5111, 0.5386, 0.5261,
        0.4706, 0.5008, 0.4700, 0.4659, 0.6008, 0.5388, 0.5432],
       device='cuda:0') torch.Size([16])
percent tensor([0.6616, 0.6787, 0.6374, 0.6220, 0.6242, 0.6743, 0.6606, 0.6116, 0.6550,
        0.6762, 0.6745, 0.6619, 0.7035, 0.6591, 0.6669, 0.6742],
       device='cuda:0') torch.Size([16])
percent tensor([0.6616, 0.5952, 0.6798, 0.6857, 0.6843, 0.7472, 0.6767, 0.6405, 0.7326,
        0.6563, 0.6965, 0.6846, 0.6159, 0.7298, 0.6295, 0.6997],
       device='cuda:0') torch.Size([16])
percent tensor([0.7745, 0.7669, 0.8039, 0.7946, 0.8301, 0.8145, 0.7988, 0.7813, 0.7900,
        0.7751, 0.7894, 0.7689, 0.7625, 0.8015, 0.7616, 0.7966],
       device='cuda:0') torch.Size([16])
percent tensor([0.4318, 0.5558, 0.6337, 0.5304, 0.6746, 0.6961, 0.5118, 0.4747, 0.5540,
        0.5126, 0.4851, 0.5028, 0.4666, 0.5042, 0.3634, 0.4465],
       device='cuda:0') torch.Size([16])
percent tensor([0.9995, 0.9996, 0.9994, 0.9998, 0.9997, 0.9987, 0.9995, 0.9997, 0.9996,
        0.9996, 0.9998, 0.9997, 0.9996, 0.9990, 0.9991, 0.9995],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 150 | Batch_idx: 0 |  Loss: (0.1455) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 150 | Batch_idx: 10 |  Loss: (0.1300) |  Loss2: (0.0000) | Acc: (95.00%) (1344/1408)
Epoch: 150 | Batch_idx: 20 |  Loss: (0.1094) |  Loss2: (0.0000) | Acc: (96.00%) (2588/2688)
Epoch: 150 | Batch_idx: 30 |  Loss: (0.1076) |  Loss2: (0.0000) | Acc: (96.00%) (3826/3968)
Epoch: 150 | Batch_idx: 40 |  Loss: (0.1060) |  Loss2: (0.0000) | Acc: (96.00%) (5064/5248)
Epoch: 150 | Batch_idx: 50 |  Loss: (0.1065) |  Loss2: (0.0000) | Acc: (96.00%) (6299/6528)
Epoch: 150 | Batch_idx: 60 |  Loss: (0.1076) |  Loss2: (0.0000) | Acc: (96.00%) (7527/7808)
Epoch: 150 | Batch_idx: 70 |  Loss: (0.1065) |  Loss2: (0.0000) | Acc: (96.00%) (8758/9088)
Epoch: 150 | Batch_idx: 80 |  Loss: (0.1065) |  Loss2: (0.0000) | Acc: (96.00%) (9991/10368)
Epoch: 150 | Batch_idx: 90 |  Loss: (0.1069) |  Loss2: (0.0000) | Acc: (96.00%) (11226/11648)
Epoch: 150 | Batch_idx: 100 |  Loss: (0.1079) |  Loss2: (0.0000) | Acc: (96.00%) (12457/12928)
Epoch: 150 | Batch_idx: 110 |  Loss: (0.1091) |  Loss2: (0.0000) | Acc: (96.00%) (13683/14208)
Epoch: 150 | Batch_idx: 120 |  Loss: (0.1080) |  Loss2: (0.0000) | Acc: (96.00%) (14915/15488)
Epoch: 150 | Batch_idx: 130 |  Loss: (0.1076) |  Loss2: (0.0000) | Acc: (96.00%) (16152/16768)
Epoch: 150 | Batch_idx: 140 |  Loss: (0.1073) |  Loss2: (0.0000) | Acc: (96.00%) (17387/18048)
Epoch: 150 | Batch_idx: 150 |  Loss: (0.1070) |  Loss2: (0.0000) | Acc: (96.00%) (18626/19328)
Epoch: 150 | Batch_idx: 160 |  Loss: (0.1070) |  Loss2: (0.0000) | Acc: (96.00%) (19855/20608)
Epoch: 150 | Batch_idx: 170 |  Loss: (0.1066) |  Loss2: (0.0000) | Acc: (96.00%) (21091/21888)
Epoch: 150 | Batch_idx: 180 |  Loss: (0.1071) |  Loss2: (0.0000) | Acc: (96.00%) (22320/23168)
Epoch: 150 | Batch_idx: 190 |  Loss: (0.1069) |  Loss2: (0.0000) | Acc: (96.00%) (23552/24448)
Epoch: 150 | Batch_idx: 200 |  Loss: (0.1070) |  Loss2: (0.0000) | Acc: (96.00%) (24785/25728)
Epoch: 150 | Batch_idx: 210 |  Loss: (0.1073) |  Loss2: (0.0000) | Acc: (96.00%) (26015/27008)
Epoch: 150 | Batch_idx: 220 |  Loss: (0.1087) |  Loss2: (0.0000) | Acc: (96.00%) (27246/28288)
Epoch: 150 | Batch_idx: 230 |  Loss: (0.1092) |  Loss2: (0.0000) | Acc: (96.00%) (28478/29568)
Epoch: 150 | Batch_idx: 240 |  Loss: (0.1093) |  Loss2: (0.0000) | Acc: (96.00%) (29708/30848)
Epoch: 150 | Batch_idx: 250 |  Loss: (0.1096) |  Loss2: (0.0000) | Acc: (96.00%) (30938/32128)
Epoch: 150 | Batch_idx: 260 |  Loss: (0.1110) |  Loss2: (0.0000) | Acc: (96.00%) (32153/33408)
Epoch: 150 | Batch_idx: 270 |  Loss: (0.1104) |  Loss2: (0.0000) | Acc: (96.00%) (33394/34688)
Epoch: 150 | Batch_idx: 280 |  Loss: (0.1106) |  Loss2: (0.0000) | Acc: (96.00%) (34626/35968)
Epoch: 150 | Batch_idx: 290 |  Loss: (0.1114) |  Loss2: (0.0000) | Acc: (96.00%) (35848/37248)
Epoch: 150 | Batch_idx: 300 |  Loss: (0.1121) |  Loss2: (0.0000) | Acc: (96.00%) (37065/38528)
Epoch: 150 | Batch_idx: 310 |  Loss: (0.1126) |  Loss2: (0.0000) | Acc: (96.00%) (38290/39808)
Epoch: 150 | Batch_idx: 320 |  Loss: (0.1128) |  Loss2: (0.0000) | Acc: (96.00%) (39520/41088)
Epoch: 150 | Batch_idx: 330 |  Loss: (0.1132) |  Loss2: (0.0000) | Acc: (96.00%) (40734/42368)
Epoch: 150 | Batch_idx: 340 |  Loss: (0.1132) |  Loss2: (0.0000) | Acc: (96.00%) (41967/43648)
Epoch: 150 | Batch_idx: 350 |  Loss: (0.1133) |  Loss2: (0.0000) | Acc: (96.00%) (43191/44928)
Epoch: 150 | Batch_idx: 360 |  Loss: (0.1132) |  Loss2: (0.0000) | Acc: (96.00%) (44420/46208)
Epoch: 150 | Batch_idx: 370 |  Loss: (0.1132) |  Loss2: (0.0000) | Acc: (96.00%) (45652/47488)
Epoch: 150 | Batch_idx: 380 |  Loss: (0.1130) |  Loss2: (0.0000) | Acc: (96.00%) (46884/48768)
Epoch: 150 | Batch_idx: 390 |  Loss: (0.1138) |  Loss2: (0.0000) | Acc: (96.00%) (48062/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_150.pth.tar'
# TEST : Loss: (0.4495) | Acc: (86.00%) (8664/10000)
percent tensor([0.5852, 0.5994, 0.5910, 0.5776, 0.5976, 0.5691, 0.6044, 0.6004, 0.6015,
        0.5975, 0.5961, 0.5984, 0.5928, 0.5976, 0.5872, 0.5806],
       device='cuda:0') torch.Size([16])
percent tensor([0.5447, 0.5348, 0.5102, 0.5356, 0.5297, 0.5310, 0.5374, 0.5505, 0.5370,
        0.5165, 0.5172, 0.5056, 0.5342, 0.5501, 0.5452, 0.5338],
       device='cuda:0') torch.Size([16])
percent tensor([0.5445, 0.4741, 0.4769, 0.5414, 0.4773, 0.5890, 0.5045, 0.5313, 0.5200,
        0.4588, 0.4991, 0.4599, 0.4608, 0.5925, 0.5408, 0.5419],
       device='cuda:0') torch.Size([16])
percent tensor([0.6639, 0.6867, 0.6417, 0.6325, 0.6259, 0.6642, 0.6661, 0.6152, 0.6647,
        0.6861, 0.6796, 0.6698, 0.7090, 0.6652, 0.6659, 0.6769],
       device='cuda:0') torch.Size([16])
percent tensor([0.6523, 0.5890, 0.6792, 0.6963, 0.6869, 0.7317, 0.6625, 0.6483, 0.7332,
        0.6586, 0.6859, 0.6890, 0.6077, 0.7200, 0.6224, 0.6911],
       device='cuda:0') torch.Size([16])
percent tensor([0.7684, 0.7657, 0.8064, 0.7961, 0.8305, 0.8133, 0.7951, 0.7842, 0.7861,
        0.7713, 0.7808, 0.7684, 0.7664, 0.8050, 0.7556, 0.7920],
       device='cuda:0') torch.Size([16])
percent tensor([0.4546, 0.5617, 0.6047, 0.5195, 0.6557, 0.6852, 0.5428, 0.4486, 0.5567,
        0.5033, 0.5070, 0.5058, 0.4954, 0.5412, 0.3667, 0.4708],
       device='cuda:0') torch.Size([16])
percent tensor([0.9995, 0.9996, 0.9988, 0.9997, 0.9995, 0.9987, 0.9993, 0.9996, 0.9997,
        0.9994, 0.9998, 0.9997, 0.9997, 0.9992, 0.9991, 0.9995],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(180.5000, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(820.4903, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(807.6180, device='cuda:0')
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1503.9965, device='cuda:0')
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(486.7299, device='cuda:0')
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2268.0588, device='cuda:0')
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4290.6992, device='cuda:0')
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1373.6338, device='cuda:0')
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6250.8086, device='cuda:0')
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11687.2227, device='cuda:0')
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3859.8950, device='cuda:0')
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16287.6943, device='cuda:0')
Epoch: 151 | Batch_idx: 0 |  Loss: (0.0833) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 151 | Batch_idx: 10 |  Loss: (0.0973) |  Loss2: (0.0000) | Acc: (96.00%) (1363/1408)
Epoch: 151 | Batch_idx: 20 |  Loss: (0.1114) |  Loss2: (0.0000) | Acc: (96.00%) (2585/2688)
Epoch: 151 | Batch_idx: 30 |  Loss: (0.1170) |  Loss2: (0.0000) | Acc: (96.00%) (3810/3968)
Epoch: 151 | Batch_idx: 40 |  Loss: (0.1252) |  Loss2: (0.0000) | Acc: (95.00%) (5019/5248)
Epoch: 151 | Batch_idx: 50 |  Loss: (0.1280) |  Loss2: (0.0000) | Acc: (95.00%) (6233/6528)
Epoch: 151 | Batch_idx: 60 |  Loss: (0.1270) |  Loss2: (0.0000) | Acc: (95.00%) (7464/7808)
Epoch: 151 | Batch_idx: 70 |  Loss: (0.1315) |  Loss2: (0.0000) | Acc: (95.00%) (8666/9088)
Epoch: 151 | Batch_idx: 80 |  Loss: (0.1299) |  Loss2: (0.0000) | Acc: (95.00%) (9887/10368)
Epoch: 151 | Batch_idx: 90 |  Loss: (0.1315) |  Loss2: (0.0000) | Acc: (95.00%) (11095/11648)
Epoch: 151 | Batch_idx: 100 |  Loss: (0.1353) |  Loss2: (0.0000) | Acc: (95.00%) (12300/12928)
Epoch: 151 | Batch_idx: 110 |  Loss: (0.1367) |  Loss2: (0.0000) | Acc: (95.00%) (13517/14208)
Epoch: 151 | Batch_idx: 120 |  Loss: (0.1405) |  Loss2: (0.0000) | Acc: (95.00%) (14721/15488)
Epoch: 151 | Batch_idx: 130 |  Loss: (0.1401) |  Loss2: (0.0000) | Acc: (95.00%) (15939/16768)
Epoch: 151 | Batch_idx: 140 |  Loss: (0.1394) |  Loss2: (0.0000) | Acc: (95.00%) (17160/18048)
Epoch: 151 | Batch_idx: 150 |  Loss: (0.1415) |  Loss2: (0.0000) | Acc: (94.00%) (18360/19328)
Epoch: 151 | Batch_idx: 160 |  Loss: (0.1419) |  Loss2: (0.0000) | Acc: (94.00%) (19572/20608)
Epoch: 151 | Batch_idx: 170 |  Loss: (0.1432) |  Loss2: (0.0000) | Acc: (94.00%) (20780/21888)
Epoch: 151 | Batch_idx: 180 |  Loss: (0.1427) |  Loss2: (0.0000) | Acc: (94.00%) (21998/23168)
Epoch: 151 | Batch_idx: 190 |  Loss: (0.1420) |  Loss2: (0.0000) | Acc: (95.00%) (23227/24448)
Epoch: 151 | Batch_idx: 200 |  Loss: (0.1429) |  Loss2: (0.0000) | Acc: (94.00%) (24432/25728)
Epoch: 151 | Batch_idx: 210 |  Loss: (0.1431) |  Loss2: (0.0000) | Acc: (94.00%) (25648/27008)
Epoch: 151 | Batch_idx: 220 |  Loss: (0.1425) |  Loss2: (0.0000) | Acc: (94.00%) (26870/28288)
Epoch: 151 | Batch_idx: 230 |  Loss: (0.1425) |  Loss2: (0.0000) | Acc: (95.00%) (28093/29568)
Epoch: 151 | Batch_idx: 240 |  Loss: (0.1422) |  Loss2: (0.0000) | Acc: (95.00%) (29315/30848)
Epoch: 151 | Batch_idx: 250 |  Loss: (0.1417) |  Loss2: (0.0000) | Acc: (95.00%) (30540/32128)
Epoch: 151 | Batch_idx: 260 |  Loss: (0.1411) |  Loss2: (0.0000) | Acc: (95.00%) (31767/33408)
Epoch: 151 | Batch_idx: 270 |  Loss: (0.1405) |  Loss2: (0.0000) | Acc: (95.00%) (32988/34688)
Epoch: 151 | Batch_idx: 280 |  Loss: (0.1399) |  Loss2: (0.0000) | Acc: (95.00%) (34215/35968)
Epoch: 151 | Batch_idx: 290 |  Loss: (0.1395) |  Loss2: (0.0000) | Acc: (95.00%) (35441/37248)
Epoch: 151 | Batch_idx: 300 |  Loss: (0.1391) |  Loss2: (0.0000) | Acc: (95.00%) (36664/38528)
Epoch: 151 | Batch_idx: 310 |  Loss: (0.1384) |  Loss2: (0.0000) | Acc: (95.00%) (37893/39808)
Epoch: 151 | Batch_idx: 320 |  Loss: (0.1388) |  Loss2: (0.0000) | Acc: (95.00%) (39109/41088)
Epoch: 151 | Batch_idx: 330 |  Loss: (0.1385) |  Loss2: (0.0000) | Acc: (95.00%) (40328/42368)
Epoch: 151 | Batch_idx: 340 |  Loss: (0.1380) |  Loss2: (0.0000) | Acc: (95.00%) (41561/43648)
Epoch: 151 | Batch_idx: 350 |  Loss: (0.1374) |  Loss2: (0.0000) | Acc: (95.00%) (42793/44928)
Epoch: 151 | Batch_idx: 360 |  Loss: (0.1374) |  Loss2: (0.0000) | Acc: (95.00%) (44009/46208)
Epoch: 151 | Batch_idx: 370 |  Loss: (0.1369) |  Loss2: (0.0000) | Acc: (95.00%) (45234/47488)
Epoch: 151 | Batch_idx: 380 |  Loss: (0.1367) |  Loss2: (0.0000) | Acc: (95.00%) (46463/48768)
Epoch: 151 | Batch_idx: 390 |  Loss: (0.1366) |  Loss2: (0.0000) | Acc: (95.00%) (47648/50000)
# TEST : Loss: (0.3923) | Acc: (88.00%) (8810/10000)
percent tensor([0.5742, 0.5854, 0.5785, 0.5653, 0.5843, 0.5577, 0.5911, 0.5869, 0.5887,
        0.5846, 0.5839, 0.5858, 0.5808, 0.5845, 0.5745, 0.5687],
       device='cuda:0') torch.Size([16])
percent tensor([0.5435, 0.5324, 0.5081, 0.5325, 0.5245, 0.5290, 0.5342, 0.5456, 0.5357,
        0.5166, 0.5182, 0.5043, 0.5322, 0.5481, 0.5403, 0.5343],
       device='cuda:0') torch.Size([16])
percent tensor([0.5456, 0.4720, 0.4557, 0.5293, 0.4586, 0.5876, 0.4995, 0.5196, 0.5157,
        0.4529, 0.4989, 0.4437, 0.4610, 0.5902, 0.5380, 0.5452],
       device='cuda:0') torch.Size([16])
percent tensor([0.6339, 0.6607, 0.6118, 0.5997, 0.5935, 0.6304, 0.6352, 0.5828, 0.6368,
        0.6633, 0.6537, 0.6435, 0.6837, 0.6377, 0.6341, 0.6482],
       device='cuda:0') torch.Size([16])
percent tensor([0.6564, 0.6010, 0.6808, 0.6932, 0.6833, 0.7352, 0.6688, 0.6385, 0.7525,
        0.6813, 0.7077, 0.7089, 0.6165, 0.7382, 0.6211, 0.7016],
       device='cuda:0') torch.Size([16])
percent tensor([0.7935, 0.7898, 0.8223, 0.8168, 0.8502, 0.8324, 0.8161, 0.8121, 0.8071,
        0.7966, 0.8064, 0.7913, 0.7891, 0.8260, 0.7854, 0.8149],
       device='cuda:0') torch.Size([16])
percent tensor([0.4298, 0.5622, 0.6063, 0.5291, 0.6724, 0.6686, 0.5415, 0.4786, 0.5326,
        0.4919, 0.4910, 0.5035, 0.4871, 0.5031, 0.3939, 0.4305],
       device='cuda:0') torch.Size([16])
percent tensor([0.9995, 0.9995, 0.9991, 0.9997, 0.9996, 0.9988, 0.9995, 0.9996, 0.9997,
        0.9995, 0.9999, 0.9997, 0.9997, 0.9993, 0.9991, 0.9996],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 152 | Batch_idx: 0 |  Loss: (0.0747) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 152 | Batch_idx: 10 |  Loss: (0.1133) |  Loss2: (0.0000) | Acc: (96.00%) (1352/1408)
Epoch: 152 | Batch_idx: 20 |  Loss: (0.1128) |  Loss2: (0.0000) | Acc: (96.00%) (2584/2688)
Epoch: 152 | Batch_idx: 30 |  Loss: (0.1120) |  Loss2: (0.0000) | Acc: (96.00%) (3810/3968)
Epoch: 152 | Batch_idx: 40 |  Loss: (0.1119) |  Loss2: (0.0000) | Acc: (96.00%) (5041/5248)
Epoch: 152 | Batch_idx: 50 |  Loss: (0.1153) |  Loss2: (0.0000) | Acc: (95.00%) (6264/6528)
Epoch: 152 | Batch_idx: 60 |  Loss: (0.1131) |  Loss2: (0.0000) | Acc: (96.00%) (7502/7808)
Epoch: 152 | Batch_idx: 70 |  Loss: (0.1097) |  Loss2: (0.0000) | Acc: (96.00%) (8747/9088)
Epoch: 152 | Batch_idx: 80 |  Loss: (0.1074) |  Loss2: (0.0000) | Acc: (96.00%) (9989/10368)
Epoch: 152 | Batch_idx: 90 |  Loss: (0.1082) |  Loss2: (0.0000) | Acc: (96.00%) (11215/11648)
Epoch: 152 | Batch_idx: 100 |  Loss: (0.1086) |  Loss2: (0.0000) | Acc: (96.00%) (12453/12928)
Epoch: 152 | Batch_idx: 110 |  Loss: (0.1103) |  Loss2: (0.0000) | Acc: (96.00%) (13675/14208)
Epoch: 152 | Batch_idx: 120 |  Loss: (0.1091) |  Loss2: (0.0000) | Acc: (96.00%) (14916/15488)
Epoch: 152 | Batch_idx: 130 |  Loss: (0.1089) |  Loss2: (0.0000) | Acc: (96.00%) (16150/16768)
Epoch: 152 | Batch_idx: 140 |  Loss: (0.1087) |  Loss2: (0.0000) | Acc: (96.00%) (17375/18048)
Epoch: 152 | Batch_idx: 150 |  Loss: (0.1089) |  Loss2: (0.0000) | Acc: (96.00%) (18604/19328)
Epoch: 152 | Batch_idx: 160 |  Loss: (0.1083) |  Loss2: (0.0000) | Acc: (96.00%) (19837/20608)
Epoch: 152 | Batch_idx: 170 |  Loss: (0.1080) |  Loss2: (0.0000) | Acc: (96.00%) (21077/21888)
Epoch: 152 | Batch_idx: 180 |  Loss: (0.1067) |  Loss2: (0.0000) | Acc: (96.00%) (22326/23168)
Epoch: 152 | Batch_idx: 190 |  Loss: (0.1072) |  Loss2: (0.0000) | Acc: (96.00%) (23557/24448)
Epoch: 152 | Batch_idx: 200 |  Loss: (0.1073) |  Loss2: (0.0000) | Acc: (96.00%) (24792/25728)
Epoch: 152 | Batch_idx: 210 |  Loss: (0.1081) |  Loss2: (0.0000) | Acc: (96.00%) (26024/27008)
Epoch: 152 | Batch_idx: 220 |  Loss: (0.1085) |  Loss2: (0.0000) | Acc: (96.00%) (27256/28288)
Epoch: 152 | Batch_idx: 230 |  Loss: (0.1093) |  Loss2: (0.0000) | Acc: (96.00%) (28484/29568)
Epoch: 152 | Batch_idx: 240 |  Loss: (0.1100) |  Loss2: (0.0000) | Acc: (96.00%) (29700/30848)
Epoch: 152 | Batch_idx: 250 |  Loss: (0.1093) |  Loss2: (0.0000) | Acc: (96.00%) (30940/32128)
Epoch: 152 | Batch_idx: 260 |  Loss: (0.1091) |  Loss2: (0.0000) | Acc: (96.00%) (32168/33408)
Epoch: 152 | Batch_idx: 270 |  Loss: (0.1094) |  Loss2: (0.0000) | Acc: (96.00%) (33399/34688)
Epoch: 152 | Batch_idx: 280 |  Loss: (0.1098) |  Loss2: (0.0000) | Acc: (96.00%) (34622/35968)
Epoch: 152 | Batch_idx: 290 |  Loss: (0.1092) |  Loss2: (0.0000) | Acc: (96.00%) (35864/37248)
Epoch: 152 | Batch_idx: 300 |  Loss: (0.1098) |  Loss2: (0.0000) | Acc: (96.00%) (37094/38528)
Epoch: 152 | Batch_idx: 310 |  Loss: (0.1102) |  Loss2: (0.0000) | Acc: (96.00%) (38314/39808)
Epoch: 152 | Batch_idx: 320 |  Loss: (0.1100) |  Loss2: (0.0000) | Acc: (96.00%) (39545/41088)
Epoch: 152 | Batch_idx: 330 |  Loss: (0.1102) |  Loss2: (0.0000) | Acc: (96.00%) (40772/42368)
Epoch: 152 | Batch_idx: 340 |  Loss: (0.1106) |  Loss2: (0.0000) | Acc: (96.00%) (41996/43648)
Epoch: 152 | Batch_idx: 350 |  Loss: (0.1110) |  Loss2: (0.0000) | Acc: (96.00%) (43229/44928)
Epoch: 152 | Batch_idx: 360 |  Loss: (0.1113) |  Loss2: (0.0000) | Acc: (96.00%) (44455/46208)
Epoch: 152 | Batch_idx: 370 |  Loss: (0.1113) |  Loss2: (0.0000) | Acc: (96.00%) (45692/47488)
Epoch: 152 | Batch_idx: 380 |  Loss: (0.1114) |  Loss2: (0.0000) | Acc: (96.00%) (46923/48768)
Epoch: 152 | Batch_idx: 390 |  Loss: (0.1111) |  Loss2: (0.0000) | Acc: (96.00%) (48109/50000)
# TEST : Loss: (0.4258) | Acc: (87.00%) (8744/10000)
percent tensor([0.5746, 0.5849, 0.5812, 0.5659, 0.5856, 0.5581, 0.5914, 0.5872, 0.5892,
        0.5849, 0.5840, 0.5870, 0.5807, 0.5841, 0.5745, 0.5691],
       device='cuda:0') torch.Size([16])
percent tensor([0.5435, 0.5338, 0.5100, 0.5331, 0.5227, 0.5270, 0.5342, 0.5459, 0.5339,
        0.5166, 0.5148, 0.5073, 0.5334, 0.5525, 0.5406, 0.5326],
       device='cuda:0') torch.Size([16])
percent tensor([0.5380, 0.4790, 0.4609, 0.5345, 0.4584, 0.5784, 0.5015, 0.5239, 0.5130,
        0.4559, 0.4918, 0.4515, 0.4596, 0.6004, 0.5353, 0.5366],
       device='cuda:0') torch.Size([16])
percent tensor([0.6395, 0.6626, 0.6142, 0.5986, 0.5965, 0.6469, 0.6409, 0.5824, 0.6376,
        0.6589, 0.6545, 0.6391, 0.6841, 0.6437, 0.6400, 0.6501],
       device='cuda:0') torch.Size([16])
percent tensor([0.6673, 0.6002, 0.6710, 0.6902, 0.6749, 0.7383, 0.6719, 0.6363, 0.7411,
        0.6754, 0.7131, 0.6892, 0.6128, 0.7474, 0.6218, 0.7091],
       device='cuda:0') torch.Size([16])
percent tensor([0.7985, 0.7898, 0.8255, 0.8181, 0.8494, 0.8364, 0.8134, 0.8136, 0.8102,
        0.7989, 0.8127, 0.7841, 0.7861, 0.8208, 0.7838, 0.8258],
       device='cuda:0') torch.Size([16])
percent tensor([0.4491, 0.5740, 0.6137, 0.5385, 0.6603, 0.6746, 0.5318, 0.4805, 0.5675,
        0.5212, 0.5198, 0.4791, 0.4958, 0.5020, 0.3955, 0.4637],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9997, 0.9990, 0.9997, 0.9997, 0.9987, 0.9996, 0.9996, 0.9999,
        0.9997, 0.9999, 0.9997, 0.9997, 0.9989, 0.9988, 0.9996],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 153 | Batch_idx: 0 |  Loss: (0.0760) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 153 | Batch_idx: 10 |  Loss: (0.1056) |  Loss2: (0.0000) | Acc: (95.00%) (1348/1408)
Epoch: 153 | Batch_idx: 20 |  Loss: (0.1136) |  Loss2: (0.0000) | Acc: (95.00%) (2566/2688)
Epoch: 153 | Batch_idx: 30 |  Loss: (0.1156) |  Loss2: (0.0000) | Acc: (95.00%) (3794/3968)
Epoch: 153 | Batch_idx: 40 |  Loss: (0.1214) |  Loss2: (0.0000) | Acc: (95.00%) (5013/5248)
Epoch: 153 | Batch_idx: 50 |  Loss: (0.1225) |  Loss2: (0.0000) | Acc: (95.00%) (6233/6528)
Epoch: 153 | Batch_idx: 60 |  Loss: (0.1247) |  Loss2: (0.0000) | Acc: (95.00%) (7455/7808)
Epoch: 153 | Batch_idx: 70 |  Loss: (0.1228) |  Loss2: (0.0000) | Acc: (95.00%) (8687/9088)
Epoch: 153 | Batch_idx: 80 |  Loss: (0.1219) |  Loss2: (0.0000) | Acc: (95.00%) (9916/10368)
Epoch: 153 | Batch_idx: 90 |  Loss: (0.1213) |  Loss2: (0.0000) | Acc: (95.00%) (11146/11648)
Epoch: 153 | Batch_idx: 100 |  Loss: (0.1222) |  Loss2: (0.0000) | Acc: (95.00%) (12368/12928)
Epoch: 153 | Batch_idx: 110 |  Loss: (0.1217) |  Loss2: (0.0000) | Acc: (95.00%) (13595/14208)
Epoch: 153 | Batch_idx: 120 |  Loss: (0.1207) |  Loss2: (0.0000) | Acc: (95.00%) (14827/15488)
Epoch: 153 | Batch_idx: 130 |  Loss: (0.1212) |  Loss2: (0.0000) | Acc: (95.00%) (16055/16768)
Epoch: 153 | Batch_idx: 140 |  Loss: (0.1228) |  Loss2: (0.0000) | Acc: (95.00%) (17266/18048)
Epoch: 153 | Batch_idx: 150 |  Loss: (0.1235) |  Loss2: (0.0000) | Acc: (95.00%) (18489/19328)
Epoch: 153 | Batch_idx: 160 |  Loss: (0.1222) |  Loss2: (0.0000) | Acc: (95.00%) (19721/20608)
Epoch: 153 | Batch_idx: 170 |  Loss: (0.1217) |  Loss2: (0.0000) | Acc: (95.00%) (20943/21888)
Epoch: 153 | Batch_idx: 180 |  Loss: (0.1208) |  Loss2: (0.0000) | Acc: (95.00%) (22175/23168)
Epoch: 153 | Batch_idx: 190 |  Loss: (0.1203) |  Loss2: (0.0000) | Acc: (95.00%) (23404/24448)
Epoch: 153 | Batch_idx: 200 |  Loss: (0.1195) |  Loss2: (0.0000) | Acc: (95.00%) (24643/25728)
Epoch: 153 | Batch_idx: 210 |  Loss: (0.1197) |  Loss2: (0.0000) | Acc: (95.00%) (25873/27008)
Epoch: 153 | Batch_idx: 220 |  Loss: (0.1198) |  Loss2: (0.0000) | Acc: (95.00%) (27097/28288)
Epoch: 153 | Batch_idx: 230 |  Loss: (0.1197) |  Loss2: (0.0000) | Acc: (95.00%) (28326/29568)
Epoch: 153 | Batch_idx: 240 |  Loss: (0.1200) |  Loss2: (0.0000) | Acc: (95.00%) (29546/30848)
Epoch: 153 | Batch_idx: 250 |  Loss: (0.1199) |  Loss2: (0.0000) | Acc: (95.00%) (30772/32128)
Epoch: 153 | Batch_idx: 260 |  Loss: (0.1199) |  Loss2: (0.0000) | Acc: (95.00%) (31992/33408)
Epoch: 153 | Batch_idx: 270 |  Loss: (0.1201) |  Loss2: (0.0000) | Acc: (95.00%) (33208/34688)
Epoch: 153 | Batch_idx: 280 |  Loss: (0.1192) |  Loss2: (0.0000) | Acc: (95.00%) (34445/35968)
Epoch: 153 | Batch_idx: 290 |  Loss: (0.1195) |  Loss2: (0.0000) | Acc: (95.00%) (35665/37248)
Epoch: 153 | Batch_idx: 300 |  Loss: (0.1191) |  Loss2: (0.0000) | Acc: (95.00%) (36895/38528)
Epoch: 153 | Batch_idx: 310 |  Loss: (0.1192) |  Loss2: (0.0000) | Acc: (95.00%) (38117/39808)
Epoch: 153 | Batch_idx: 320 |  Loss: (0.1185) |  Loss2: (0.0000) | Acc: (95.00%) (39360/41088)
Epoch: 153 | Batch_idx: 330 |  Loss: (0.1184) |  Loss2: (0.0000) | Acc: (95.00%) (40587/42368)
Epoch: 153 | Batch_idx: 340 |  Loss: (0.1176) |  Loss2: (0.0000) | Acc: (95.00%) (41815/43648)
Epoch: 153 | Batch_idx: 350 |  Loss: (0.1174) |  Loss2: (0.0000) | Acc: (95.00%) (43045/44928)
Epoch: 153 | Batch_idx: 360 |  Loss: (0.1174) |  Loss2: (0.0000) | Acc: (95.00%) (44276/46208)
Epoch: 153 | Batch_idx: 370 |  Loss: (0.1176) |  Loss2: (0.0000) | Acc: (95.00%) (45505/47488)
Epoch: 153 | Batch_idx: 380 |  Loss: (0.1172) |  Loss2: (0.0000) | Acc: (95.00%) (46739/48768)
Epoch: 153 | Batch_idx: 390 |  Loss: (0.1174) |  Loss2: (0.0000) | Acc: (95.00%) (47918/50000)
# TEST : Loss: (0.3846) | Acc: (88.00%) (8842/10000)
percent tensor([0.5745, 0.5848, 0.5836, 0.5668, 0.5875, 0.5589, 0.5916, 0.5879, 0.5894,
        0.5851, 0.5838, 0.5886, 0.5806, 0.5839, 0.5745, 0.5690],
       device='cuda:0') torch.Size([16])
percent tensor([0.5346, 0.5265, 0.5050, 0.5261, 0.5192, 0.5181, 0.5281, 0.5397, 0.5250,
        0.5086, 0.5045, 0.5014, 0.5254, 0.5411, 0.5337, 0.5236],
       device='cuda:0') torch.Size([16])
percent tensor([0.5405, 0.4760, 0.4653, 0.5438, 0.4616, 0.5868, 0.5009, 0.5260, 0.5157,
        0.4559, 0.4927, 0.4556, 0.4580, 0.6011, 0.5380, 0.5405],
       device='cuda:0') torch.Size([16])
percent tensor([0.6341, 0.6588, 0.6100, 0.5945, 0.5937, 0.6406, 0.6368, 0.5804, 0.6325,
        0.6561, 0.6503, 0.6362, 0.6788, 0.6370, 0.6355, 0.6453],
       device='cuda:0') torch.Size([16])
percent tensor([0.6671, 0.6149, 0.6649, 0.6808, 0.6759, 0.7355, 0.6836, 0.6340, 0.7547,
        0.6879, 0.7302, 0.6916, 0.6243, 0.7654, 0.6215, 0.7115],
       device='cuda:0') torch.Size([16])
percent tensor([0.8293, 0.8218, 0.8501, 0.8436, 0.8747, 0.8606, 0.8458, 0.8422, 0.8408,
        0.8291, 0.8442, 0.8137, 0.8181, 0.8555, 0.8166, 0.8533],
       device='cuda:0') torch.Size([16])
percent tensor([0.4657, 0.5952, 0.6202, 0.5556, 0.6750, 0.6775, 0.5600, 0.4930, 0.5889,
        0.5398, 0.5486, 0.4917, 0.5147, 0.5220, 0.4030, 0.4686],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9997, 0.9989, 0.9997, 0.9996, 0.9985, 0.9996, 0.9996, 0.9998,
        0.9997, 0.9999, 0.9998, 0.9996, 0.9992, 0.9989, 0.9995],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 154 | Batch_idx: 0 |  Loss: (0.1045) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 154 | Batch_idx: 10 |  Loss: (0.1039) |  Loss2: (0.0000) | Acc: (96.00%) (1356/1408)
Epoch: 154 | Batch_idx: 20 |  Loss: (0.1037) |  Loss2: (0.0000) | Acc: (96.00%) (2590/2688)
Epoch: 154 | Batch_idx: 30 |  Loss: (0.1078) |  Loss2: (0.0000) | Acc: (96.00%) (3822/3968)
Epoch: 154 | Batch_idx: 40 |  Loss: (0.1055) |  Loss2: (0.0000) | Acc: (96.00%) (5057/5248)
Epoch: 154 | Batch_idx: 50 |  Loss: (0.1002) |  Loss2: (0.0000) | Acc: (96.00%) (6308/6528)
Epoch: 154 | Batch_idx: 60 |  Loss: (0.0984) |  Loss2: (0.0000) | Acc: (96.00%) (7553/7808)
Epoch: 154 | Batch_idx: 70 |  Loss: (0.0990) |  Loss2: (0.0000) | Acc: (96.00%) (8786/9088)
Epoch: 154 | Batch_idx: 80 |  Loss: (0.0993) |  Loss2: (0.0000) | Acc: (96.00%) (10020/10368)
Epoch: 154 | Batch_idx: 90 |  Loss: (0.0995) |  Loss2: (0.0000) | Acc: (96.00%) (11253/11648)
Epoch: 154 | Batch_idx: 100 |  Loss: (0.0998) |  Loss2: (0.0000) | Acc: (96.00%) (12486/12928)
Epoch: 154 | Batch_idx: 110 |  Loss: (0.0997) |  Loss2: (0.0000) | Acc: (96.00%) (13725/14208)
Epoch: 154 | Batch_idx: 120 |  Loss: (0.1004) |  Loss2: (0.0000) | Acc: (96.00%) (14958/15488)
Epoch: 154 | Batch_idx: 130 |  Loss: (0.1004) |  Loss2: (0.0000) | Acc: (96.00%) (16194/16768)
Epoch: 154 | Batch_idx: 140 |  Loss: (0.1007) |  Loss2: (0.0000) | Acc: (96.00%) (17430/18048)
Epoch: 154 | Batch_idx: 150 |  Loss: (0.1028) |  Loss2: (0.0000) | Acc: (96.00%) (18644/19328)
Epoch: 154 | Batch_idx: 160 |  Loss: (0.1029) |  Loss2: (0.0000) | Acc: (96.00%) (19879/20608)
Epoch: 154 | Batch_idx: 170 |  Loss: (0.1032) |  Loss2: (0.0000) | Acc: (96.00%) (21111/21888)
Epoch: 154 | Batch_idx: 180 |  Loss: (0.1051) |  Loss2: (0.0000) | Acc: (96.00%) (22326/23168)
Epoch: 154 | Batch_idx: 190 |  Loss: (0.1056) |  Loss2: (0.0000) | Acc: (96.00%) (23553/24448)
Epoch: 154 | Batch_idx: 200 |  Loss: (0.1060) |  Loss2: (0.0000) | Acc: (96.00%) (24784/25728)
Epoch: 154 | Batch_idx: 210 |  Loss: (0.1072) |  Loss2: (0.0000) | Acc: (96.00%) (26006/27008)
Epoch: 154 | Batch_idx: 220 |  Loss: (0.1070) |  Loss2: (0.0000) | Acc: (96.00%) (27241/28288)
Epoch: 154 | Batch_idx: 230 |  Loss: (0.1063) |  Loss2: (0.0000) | Acc: (96.00%) (28485/29568)
Epoch: 154 | Batch_idx: 240 |  Loss: (0.1056) |  Loss2: (0.0000) | Acc: (96.00%) (29731/30848)
Epoch: 154 | Batch_idx: 250 |  Loss: (0.1054) |  Loss2: (0.0000) | Acc: (96.00%) (30967/32128)
Epoch: 154 | Batch_idx: 260 |  Loss: (0.1056) |  Loss2: (0.0000) | Acc: (96.00%) (32201/33408)
Epoch: 154 | Batch_idx: 270 |  Loss: (0.1069) |  Loss2: (0.0000) | Acc: (96.00%) (33421/34688)
Epoch: 154 | Batch_idx: 280 |  Loss: (0.1075) |  Loss2: (0.0000) | Acc: (96.00%) (34642/35968)
Epoch: 154 | Batch_idx: 290 |  Loss: (0.1083) |  Loss2: (0.0000) | Acc: (96.00%) (35867/37248)
Epoch: 154 | Batch_idx: 300 |  Loss: (0.1084) |  Loss2: (0.0000) | Acc: (96.00%) (37097/38528)
Epoch: 154 | Batch_idx: 310 |  Loss: (0.1100) |  Loss2: (0.0000) | Acc: (96.00%) (38307/39808)
Epoch: 154 | Batch_idx: 320 |  Loss: (0.1094) |  Loss2: (0.0000) | Acc: (96.00%) (39544/41088)
Epoch: 154 | Batch_idx: 330 |  Loss: (0.1096) |  Loss2: (0.0000) | Acc: (96.00%) (40771/42368)
Epoch: 154 | Batch_idx: 340 |  Loss: (0.1102) |  Loss2: (0.0000) | Acc: (96.00%) (41997/43648)
Epoch: 154 | Batch_idx: 350 |  Loss: (0.1100) |  Loss2: (0.0000) | Acc: (96.00%) (43229/44928)
Epoch: 154 | Batch_idx: 360 |  Loss: (0.1101) |  Loss2: (0.0000) | Acc: (96.00%) (44460/46208)
Epoch: 154 | Batch_idx: 370 |  Loss: (0.1103) |  Loss2: (0.0000) | Acc: (96.00%) (45689/47488)
Epoch: 154 | Batch_idx: 380 |  Loss: (0.1098) |  Loss2: (0.0000) | Acc: (96.00%) (46934/48768)
Epoch: 154 | Batch_idx: 390 |  Loss: (0.1100) |  Loss2: (0.0000) | Acc: (96.00%) (48116/50000)
# TEST : Loss: (0.3917) | Acc: (88.00%) (8842/10000)
percent tensor([0.5747, 0.5850, 0.5810, 0.5668, 0.5857, 0.5594, 0.5904, 0.5864, 0.5895,
        0.5842, 0.5838, 0.5862, 0.5809, 0.5839, 0.5745, 0.5690],
       device='cuda:0') torch.Size([16])
percent tensor([0.5327, 0.5244, 0.5043, 0.5260, 0.5203, 0.5184, 0.5268, 0.5398, 0.5264,
        0.5058, 0.5047, 0.4992, 0.5241, 0.5386, 0.5337, 0.5224],
       device='cuda:0') torch.Size([16])
percent tensor([0.5377, 0.4685, 0.4852, 0.5396, 0.4788, 0.5829, 0.4993, 0.5384, 0.5193,
        0.4583, 0.4939, 0.4619, 0.4559, 0.5929, 0.5338, 0.5387],
       device='cuda:0') torch.Size([16])
percent tensor([0.6357, 0.6585, 0.6111, 0.6070, 0.5951, 0.6366, 0.6342, 0.5834, 0.6321,
        0.6558, 0.6483, 0.6397, 0.6777, 0.6387, 0.6359, 0.6477],
       device='cuda:0') torch.Size([16])
percent tensor([0.6657, 0.6055, 0.6698, 0.6844, 0.6880, 0.7411, 0.6851, 0.6486, 0.7583,
        0.6789, 0.7160, 0.6972, 0.6116, 0.7635, 0.6291, 0.7137],
       device='cuda:0') torch.Size([16])
percent tensor([0.8212, 0.8192, 0.8504, 0.8433, 0.8757, 0.8531, 0.8416, 0.8425, 0.8353,
        0.8266, 0.8348, 0.8155, 0.8179, 0.8492, 0.8193, 0.8509],
       device='cuda:0') torch.Size([16])
percent tensor([0.4441, 0.5889, 0.6263, 0.5616, 0.6852, 0.6831, 0.5311, 0.4697, 0.5800,
        0.5409, 0.5320, 0.5054, 0.5111, 0.5162, 0.3965, 0.4708],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9997, 0.9992, 0.9997, 0.9997, 0.9989, 0.9996, 0.9998, 0.9997,
        0.9996, 0.9999, 0.9996, 0.9994, 0.9989, 0.9989, 0.9995],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 155 | Batch_idx: 0 |  Loss: (0.0515) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 155 | Batch_idx: 10 |  Loss: (0.1063) |  Loss2: (0.0000) | Acc: (96.00%) (1355/1408)
Epoch: 155 | Batch_idx: 20 |  Loss: (0.1078) |  Loss2: (0.0000) | Acc: (95.00%) (2580/2688)
Epoch: 155 | Batch_idx: 30 |  Loss: (0.1084) |  Loss2: (0.0000) | Acc: (96.00%) (3811/3968)
Epoch: 155 | Batch_idx: 40 |  Loss: (0.1156) |  Loss2: (0.0000) | Acc: (95.00%) (5019/5248)
Epoch: 155 | Batch_idx: 50 |  Loss: (0.1186) |  Loss2: (0.0000) | Acc: (95.00%) (6245/6528)
Epoch: 155 | Batch_idx: 60 |  Loss: (0.1216) |  Loss2: (0.0000) | Acc: (95.00%) (7464/7808)
Epoch: 155 | Batch_idx: 70 |  Loss: (0.1278) |  Loss2: (0.0000) | Acc: (95.00%) (8668/9088)
Epoch: 155 | Batch_idx: 80 |  Loss: (0.1276) |  Loss2: (0.0000) | Acc: (95.00%) (9901/10368)
Epoch: 155 | Batch_idx: 90 |  Loss: (0.1318) |  Loss2: (0.0000) | Acc: (95.00%) (11107/11648)
Epoch: 155 | Batch_idx: 100 |  Loss: (0.1296) |  Loss2: (0.0000) | Acc: (95.00%) (12337/12928)
Epoch: 155 | Batch_idx: 110 |  Loss: (0.1293) |  Loss2: (0.0000) | Acc: (95.00%) (13554/14208)
Epoch: 155 | Batch_idx: 120 |  Loss: (0.1293) |  Loss2: (0.0000) | Acc: (95.00%) (14773/15488)
Epoch: 155 | Batch_idx: 130 |  Loss: (0.1276) |  Loss2: (0.0000) | Acc: (95.00%) (16006/16768)
Epoch: 155 | Batch_idx: 140 |  Loss: (0.1278) |  Loss2: (0.0000) | Acc: (95.00%) (17224/18048)
Epoch: 155 | Batch_idx: 150 |  Loss: (0.1274) |  Loss2: (0.0000) | Acc: (95.00%) (18451/19328)
Epoch: 155 | Batch_idx: 160 |  Loss: (0.1284) |  Loss2: (0.0000) | Acc: (95.00%) (19663/20608)
Epoch: 155 | Batch_idx: 170 |  Loss: (0.1286) |  Loss2: (0.0000) | Acc: (95.00%) (20879/21888)
Epoch: 155 | Batch_idx: 180 |  Loss: (0.1280) |  Loss2: (0.0000) | Acc: (95.00%) (22115/23168)
Epoch: 155 | Batch_idx: 190 |  Loss: (0.1279) |  Loss2: (0.0000) | Acc: (95.00%) (23336/24448)
Epoch: 155 | Batch_idx: 200 |  Loss: (0.1280) |  Loss2: (0.0000) | Acc: (95.00%) (24560/25728)
Epoch: 155 | Batch_idx: 210 |  Loss: (0.1272) |  Loss2: (0.0000) | Acc: (95.00%) (25787/27008)
Epoch: 155 | Batch_idx: 220 |  Loss: (0.1267) |  Loss2: (0.0000) | Acc: (95.00%) (27018/28288)
Epoch: 155 | Batch_idx: 230 |  Loss: (0.1267) |  Loss2: (0.0000) | Acc: (95.00%) (28248/29568)
Epoch: 155 | Batch_idx: 240 |  Loss: (0.1261) |  Loss2: (0.0000) | Acc: (95.00%) (29484/30848)
Epoch: 155 | Batch_idx: 250 |  Loss: (0.1257) |  Loss2: (0.0000) | Acc: (95.00%) (30710/32128)
Epoch: 155 | Batch_idx: 260 |  Loss: (0.1248) |  Loss2: (0.0000) | Acc: (95.00%) (31937/33408)
Epoch: 155 | Batch_idx: 270 |  Loss: (0.1251) |  Loss2: (0.0000) | Acc: (95.00%) (33157/34688)
Epoch: 155 | Batch_idx: 280 |  Loss: (0.1253) |  Loss2: (0.0000) | Acc: (95.00%) (34381/35968)
Epoch: 155 | Batch_idx: 290 |  Loss: (0.1239) |  Loss2: (0.0000) | Acc: (95.00%) (35627/37248)
Epoch: 155 | Batch_idx: 300 |  Loss: (0.1239) |  Loss2: (0.0000) | Acc: (95.00%) (36852/38528)
Epoch: 155 | Batch_idx: 310 |  Loss: (0.1242) |  Loss2: (0.0000) | Acc: (95.00%) (38066/39808)
Epoch: 155 | Batch_idx: 320 |  Loss: (0.1234) |  Loss2: (0.0000) | Acc: (95.00%) (39300/41088)
Epoch: 155 | Batch_idx: 330 |  Loss: (0.1230) |  Loss2: (0.0000) | Acc: (95.00%) (40526/42368)
Epoch: 155 | Batch_idx: 340 |  Loss: (0.1232) |  Loss2: (0.0000) | Acc: (95.00%) (41752/43648)
Epoch: 155 | Batch_idx: 350 |  Loss: (0.1235) |  Loss2: (0.0000) | Acc: (95.00%) (42976/44928)
Epoch: 155 | Batch_idx: 360 |  Loss: (0.1232) |  Loss2: (0.0000) | Acc: (95.00%) (44205/46208)
Epoch: 155 | Batch_idx: 370 |  Loss: (0.1232) |  Loss2: (0.0000) | Acc: (95.00%) (45431/47488)
Epoch: 155 | Batch_idx: 380 |  Loss: (0.1230) |  Loss2: (0.0000) | Acc: (95.00%) (46662/48768)
Epoch: 155 | Batch_idx: 390 |  Loss: (0.1232) |  Loss2: (0.0000) | Acc: (95.00%) (47836/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_155.pth.tar'
# TEST : Loss: (0.3935) | Acc: (88.00%) (8853/10000)
percent tensor([0.5754, 0.5858, 0.5816, 0.5682, 0.5869, 0.5610, 0.5911, 0.5878, 0.5900,
        0.5849, 0.5843, 0.5867, 0.5817, 0.5847, 0.5756, 0.5696],
       device='cuda:0') torch.Size([16])
percent tensor([0.5327, 0.5228, 0.5000, 0.5250, 0.5183, 0.5164, 0.5261, 0.5392, 0.5234,
        0.5048, 0.5034, 0.4951, 0.5263, 0.5368, 0.5322, 0.5223],
       device='cuda:0') torch.Size([16])
percent tensor([0.5559, 0.4839, 0.5010, 0.5532, 0.4875, 0.5987, 0.5152, 0.5457, 0.5355,
        0.4755, 0.5142, 0.4817, 0.4754, 0.6092, 0.5504, 0.5566],
       device='cuda:0') torch.Size([16])
percent tensor([0.6726, 0.6970, 0.6423, 0.6400, 0.6264, 0.6754, 0.6711, 0.6162, 0.6674,
        0.6934, 0.6868, 0.6746, 0.7163, 0.6792, 0.6755, 0.6857],
       device='cuda:0') torch.Size([16])
percent tensor([0.6559, 0.5922, 0.6731, 0.6800, 0.6813, 0.7439, 0.6725, 0.6442, 0.7453,
        0.6665, 0.6979, 0.6818, 0.6085, 0.7427, 0.6185, 0.7111],
       device='cuda:0') torch.Size([16])
percent tensor([0.8226, 0.8178, 0.8502, 0.8471, 0.8755, 0.8554, 0.8419, 0.8425, 0.8357,
        0.8289, 0.8342, 0.8180, 0.8190, 0.8441, 0.8211, 0.8513],
       device='cuda:0') torch.Size([16])
percent tensor([0.4174, 0.5517, 0.6094, 0.5416, 0.6601, 0.6742, 0.5140, 0.4571, 0.5595,
        0.5044, 0.4982, 0.4793, 0.4860, 0.4673, 0.3787, 0.4492],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9997, 0.9992, 0.9997, 0.9997, 0.9990, 0.9996, 0.9997, 0.9997,
        0.9997, 0.9999, 0.9997, 0.9995, 0.9991, 0.9988, 0.9995],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 156 | Batch_idx: 0 |  Loss: (0.1413) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 156 | Batch_idx: 10 |  Loss: (0.1035) |  Loss2: (0.0000) | Acc: (96.00%) (1353/1408)
Epoch: 156 | Batch_idx: 20 |  Loss: (0.1084) |  Loss2: (0.0000) | Acc: (96.00%) (2585/2688)
Epoch: 156 | Batch_idx: 30 |  Loss: (0.1101) |  Loss2: (0.0000) | Acc: (96.00%) (3819/3968)
Epoch: 156 | Batch_idx: 40 |  Loss: (0.1095) |  Loss2: (0.0000) | Acc: (96.00%) (5061/5248)
Epoch: 156 | Batch_idx: 50 |  Loss: (0.1102) |  Loss2: (0.0000) | Acc: (96.00%) (6290/6528)
Epoch: 156 | Batch_idx: 60 |  Loss: (0.1111) |  Loss2: (0.0000) | Acc: (96.00%) (7519/7808)
Epoch: 156 | Batch_idx: 70 |  Loss: (0.1082) |  Loss2: (0.0000) | Acc: (96.00%) (8763/9088)
Epoch: 156 | Batch_idx: 80 |  Loss: (0.1058) |  Loss2: (0.0000) | Acc: (96.00%) (10007/10368)
Epoch: 156 | Batch_idx: 90 |  Loss: (0.1052) |  Loss2: (0.0000) | Acc: (96.00%) (11244/11648)
Epoch: 156 | Batch_idx: 100 |  Loss: (0.1041) |  Loss2: (0.0000) | Acc: (96.00%) (12483/12928)
Epoch: 156 | Batch_idx: 110 |  Loss: (0.1032) |  Loss2: (0.0000) | Acc: (96.00%) (13721/14208)
Epoch: 156 | Batch_idx: 120 |  Loss: (0.1019) |  Loss2: (0.0000) | Acc: (96.00%) (14968/15488)
Epoch: 156 | Batch_idx: 130 |  Loss: (0.1024) |  Loss2: (0.0000) | Acc: (96.00%) (16207/16768)
Epoch: 156 | Batch_idx: 140 |  Loss: (0.1020) |  Loss2: (0.0000) | Acc: (96.00%) (17445/18048)
Epoch: 156 | Batch_idx: 150 |  Loss: (0.1021) |  Loss2: (0.0000) | Acc: (96.00%) (18677/19328)
Epoch: 156 | Batch_idx: 160 |  Loss: (0.1022) |  Loss2: (0.0000) | Acc: (96.00%) (19908/20608)
Epoch: 156 | Batch_idx: 170 |  Loss: (0.1020) |  Loss2: (0.0000) | Acc: (96.00%) (21145/21888)
Epoch: 156 | Batch_idx: 180 |  Loss: (0.1033) |  Loss2: (0.0000) | Acc: (96.00%) (22369/23168)
Epoch: 156 | Batch_idx: 190 |  Loss: (0.1033) |  Loss2: (0.0000) | Acc: (96.00%) (23595/24448)
Epoch: 156 | Batch_idx: 200 |  Loss: (0.1034) |  Loss2: (0.0000) | Acc: (96.00%) (24833/25728)
Epoch: 156 | Batch_idx: 210 |  Loss: (0.1022) |  Loss2: (0.0000) | Acc: (96.00%) (26081/27008)
Epoch: 156 | Batch_idx: 220 |  Loss: (0.1017) |  Loss2: (0.0000) | Acc: (96.00%) (27321/28288)
Epoch: 156 | Batch_idx: 230 |  Loss: (0.1019) |  Loss2: (0.0000) | Acc: (96.00%) (28550/29568)
Epoch: 156 | Batch_idx: 240 |  Loss: (0.1020) |  Loss2: (0.0000) | Acc: (96.00%) (29791/30848)
Epoch: 156 | Batch_idx: 250 |  Loss: (0.1022) |  Loss2: (0.0000) | Acc: (96.00%) (31029/32128)
Epoch: 156 | Batch_idx: 260 |  Loss: (0.1031) |  Loss2: (0.0000) | Acc: (96.00%) (32254/33408)
Epoch: 156 | Batch_idx: 270 |  Loss: (0.1038) |  Loss2: (0.0000) | Acc: (96.00%) (33479/34688)
Epoch: 156 | Batch_idx: 280 |  Loss: (0.1039) |  Loss2: (0.0000) | Acc: (96.00%) (34719/35968)
Epoch: 156 | Batch_idx: 290 |  Loss: (0.1044) |  Loss2: (0.0000) | Acc: (96.00%) (35946/37248)
Epoch: 156 | Batch_idx: 300 |  Loss: (0.1044) |  Loss2: (0.0000) | Acc: (96.00%) (37176/38528)
Epoch: 156 | Batch_idx: 310 |  Loss: (0.1046) |  Loss2: (0.0000) | Acc: (96.00%) (38413/39808)
Epoch: 156 | Batch_idx: 320 |  Loss: (0.1046) |  Loss2: (0.0000) | Acc: (96.00%) (39646/41088)
Epoch: 156 | Batch_idx: 330 |  Loss: (0.1052) |  Loss2: (0.0000) | Acc: (96.00%) (40874/42368)
Epoch: 156 | Batch_idx: 340 |  Loss: (0.1053) |  Loss2: (0.0000) | Acc: (96.00%) (42101/43648)
Epoch: 156 | Batch_idx: 350 |  Loss: (0.1055) |  Loss2: (0.0000) | Acc: (96.00%) (43333/44928)
Epoch: 156 | Batch_idx: 360 |  Loss: (0.1058) |  Loss2: (0.0000) | Acc: (96.00%) (44562/46208)
Epoch: 156 | Batch_idx: 370 |  Loss: (0.1062) |  Loss2: (0.0000) | Acc: (96.00%) (45779/47488)
Epoch: 156 | Batch_idx: 380 |  Loss: (0.1060) |  Loss2: (0.0000) | Acc: (96.00%) (47013/48768)
Epoch: 156 | Batch_idx: 390 |  Loss: (0.1061) |  Loss2: (0.0000) | Acc: (96.00%) (48193/50000)
# TEST : Loss: (0.3908) | Acc: (88.00%) (8865/10000)
percent tensor([0.5742, 0.5849, 0.5796, 0.5685, 0.5846, 0.5585, 0.5896, 0.5874, 0.5889,
        0.5839, 0.5829, 0.5848, 0.5807, 0.5840, 0.5746, 0.5689],
       device='cuda:0') torch.Size([16])
percent tensor([0.5315, 0.5200, 0.5013, 0.5250, 0.5190, 0.5163, 0.5247, 0.5392, 0.5216,
        0.5033, 0.5009, 0.4946, 0.5230, 0.5321, 0.5310, 0.5208],
       device='cuda:0') torch.Size([16])
percent tensor([0.5563, 0.4910, 0.4895, 0.5572, 0.4783, 0.6020, 0.5162, 0.5450, 0.5316,
        0.4748, 0.5114, 0.4727, 0.4765, 0.6139, 0.5559, 0.5601],
       device='cuda:0') torch.Size([16])
percent tensor([0.6696, 0.6963, 0.6398, 0.6284, 0.6242, 0.6745, 0.6692, 0.6116, 0.6661,
        0.6973, 0.6868, 0.6775, 0.7172, 0.6770, 0.6762, 0.6840],
       device='cuda:0') torch.Size([16])
percent tensor([0.6561, 0.5921, 0.6662, 0.6726, 0.6815, 0.7467, 0.6669, 0.6403, 0.7328,
        0.6664, 0.6998, 0.6809, 0.5992, 0.7303, 0.6128, 0.7036],
       device='cuda:0') torch.Size([16])
percent tensor([0.8212, 0.8208, 0.8503, 0.8459, 0.8733, 0.8553, 0.8434, 0.8392, 0.8350,
        0.8301, 0.8370, 0.8142, 0.8190, 0.8478, 0.8131, 0.8444],
       device='cuda:0') torch.Size([16])
percent tensor([0.4141, 0.5616, 0.6033, 0.5364, 0.6690, 0.6813, 0.4929, 0.4523, 0.5567,
        0.4922, 0.4882, 0.4755, 0.4846, 0.4635, 0.3657, 0.4337],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9997, 0.9996, 0.9997, 0.9996, 0.9996, 0.9997, 0.9997, 0.9997,
        0.9996, 0.9999, 0.9997, 0.9995, 0.9990, 0.9992, 0.9997],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 157 | Batch_idx: 0 |  Loss: (0.0957) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 157 | Batch_idx: 10 |  Loss: (0.1070) |  Loss2: (0.0000) | Acc: (96.00%) (1357/1408)
Epoch: 157 | Batch_idx: 20 |  Loss: (0.1219) |  Loss2: (0.0000) | Acc: (95.00%) (2580/2688)
Epoch: 157 | Batch_idx: 30 |  Loss: (0.1245) |  Loss2: (0.0000) | Acc: (95.00%) (3800/3968)
Epoch: 157 | Batch_idx: 40 |  Loss: (0.1228) |  Loss2: (0.0000) | Acc: (95.00%) (5026/5248)
Epoch: 157 | Batch_idx: 50 |  Loss: (0.1262) |  Loss2: (0.0000) | Acc: (95.00%) (6241/6528)
Epoch: 157 | Batch_idx: 60 |  Loss: (0.1290) |  Loss2: (0.0000) | Acc: (95.00%) (7456/7808)
Epoch: 157 | Batch_idx: 70 |  Loss: (0.1283) |  Loss2: (0.0000) | Acc: (95.00%) (8682/9088)
Epoch: 157 | Batch_idx: 80 |  Loss: (0.1299) |  Loss2: (0.0000) | Acc: (95.00%) (9904/10368)
Epoch: 157 | Batch_idx: 90 |  Loss: (0.1308) |  Loss2: (0.0000) | Acc: (95.00%) (11117/11648)
Epoch: 157 | Batch_idx: 100 |  Loss: (0.1311) |  Loss2: (0.0000) | Acc: (95.00%) (12341/12928)
Epoch: 157 | Batch_idx: 110 |  Loss: (0.1315) |  Loss2: (0.0000) | Acc: (95.00%) (13549/14208)
Epoch: 157 | Batch_idx: 120 |  Loss: (0.1292) |  Loss2: (0.0000) | Acc: (95.00%) (14779/15488)
Epoch: 157 | Batch_idx: 130 |  Loss: (0.1297) |  Loss2: (0.0000) | Acc: (95.00%) (16003/16768)
Epoch: 157 | Batch_idx: 140 |  Loss: (0.1309) |  Loss2: (0.0000) | Acc: (95.00%) (17207/18048)
Epoch: 157 | Batch_idx: 150 |  Loss: (0.1301) |  Loss2: (0.0000) | Acc: (95.00%) (18437/19328)
Epoch: 157 | Batch_idx: 160 |  Loss: (0.1288) |  Loss2: (0.0000) | Acc: (95.00%) (19667/20608)
Epoch: 157 | Batch_idx: 170 |  Loss: (0.1288) |  Loss2: (0.0000) | Acc: (95.00%) (20890/21888)
Epoch: 157 | Batch_idx: 180 |  Loss: (0.1283) |  Loss2: (0.0000) | Acc: (95.00%) (22114/23168)
Epoch: 157 | Batch_idx: 190 |  Loss: (0.1276) |  Loss2: (0.0000) | Acc: (95.00%) (23339/24448)
Epoch: 157 | Batch_idx: 200 |  Loss: (0.1281) |  Loss2: (0.0000) | Acc: (95.00%) (24551/25728)
Epoch: 157 | Batch_idx: 210 |  Loss: (0.1279) |  Loss2: (0.0000) | Acc: (95.00%) (25779/27008)
Epoch: 157 | Batch_idx: 220 |  Loss: (0.1272) |  Loss2: (0.0000) | Acc: (95.00%) (27015/28288)
Epoch: 157 | Batch_idx: 230 |  Loss: (0.1270) |  Loss2: (0.0000) | Acc: (95.00%) (28241/29568)
Epoch: 157 | Batch_idx: 240 |  Loss: (0.1262) |  Loss2: (0.0000) | Acc: (95.00%) (29472/30848)
Epoch: 157 | Batch_idx: 250 |  Loss: (0.1252) |  Loss2: (0.0000) | Acc: (95.00%) (30711/32128)
Epoch: 157 | Batch_idx: 260 |  Loss: (0.1247) |  Loss2: (0.0000) | Acc: (95.00%) (31938/33408)
Epoch: 157 | Batch_idx: 270 |  Loss: (0.1248) |  Loss2: (0.0000) | Acc: (95.00%) (33164/34688)
Epoch: 157 | Batch_idx: 280 |  Loss: (0.1247) |  Loss2: (0.0000) | Acc: (95.00%) (34396/35968)
Epoch: 157 | Batch_idx: 290 |  Loss: (0.1244) |  Loss2: (0.0000) | Acc: (95.00%) (35626/37248)
Epoch: 157 | Batch_idx: 300 |  Loss: (0.1242) |  Loss2: (0.0000) | Acc: (95.00%) (36857/38528)
Epoch: 157 | Batch_idx: 310 |  Loss: (0.1240) |  Loss2: (0.0000) | Acc: (95.00%) (38082/39808)
Epoch: 157 | Batch_idx: 320 |  Loss: (0.1236) |  Loss2: (0.0000) | Acc: (95.00%) (39321/41088)
Epoch: 157 | Batch_idx: 330 |  Loss: (0.1238) |  Loss2: (0.0000) | Acc: (95.00%) (40550/42368)
Epoch: 157 | Batch_idx: 340 |  Loss: (0.1233) |  Loss2: (0.0000) | Acc: (95.00%) (41782/43648)
Epoch: 157 | Batch_idx: 350 |  Loss: (0.1230) |  Loss2: (0.0000) | Acc: (95.00%) (43013/44928)
Epoch: 157 | Batch_idx: 360 |  Loss: (0.1227) |  Loss2: (0.0000) | Acc: (95.00%) (44249/46208)
Epoch: 157 | Batch_idx: 370 |  Loss: (0.1224) |  Loss2: (0.0000) | Acc: (95.00%) (45483/47488)
Epoch: 157 | Batch_idx: 380 |  Loss: (0.1220) |  Loss2: (0.0000) | Acc: (95.00%) (46712/48768)
Epoch: 157 | Batch_idx: 390 |  Loss: (0.1212) |  Loss2: (0.0000) | Acc: (95.00%) (47909/50000)
# TEST : Loss: (0.4015) | Acc: (88.00%) (8824/10000)
percent tensor([0.5811, 0.5926, 0.5862, 0.5751, 0.5925, 0.5652, 0.5975, 0.5950, 0.5964,
        0.5911, 0.5902, 0.5921, 0.5879, 0.5912, 0.5820, 0.5759],
       device='cuda:0') torch.Size([16])
percent tensor([0.5454, 0.5313, 0.5153, 0.5381, 0.5317, 0.5303, 0.5377, 0.5540, 0.5338,
        0.5160, 0.5129, 0.5095, 0.5356, 0.5440, 0.5439, 0.5326],
       device='cuda:0') torch.Size([16])
percent tensor([0.5484, 0.4821, 0.4861, 0.5548, 0.4633, 0.5996, 0.5035, 0.5359, 0.5243,
        0.4706, 0.5056, 0.4672, 0.4658, 0.6150, 0.5434, 0.5507],
       device='cuda:0') torch.Size([16])
percent tensor([0.6641, 0.6851, 0.6353, 0.6242, 0.6140, 0.6667, 0.6574, 0.6030, 0.6568,
        0.6870, 0.6761, 0.6636, 0.7119, 0.6614, 0.6656, 0.6778],
       device='cuda:0') torch.Size([16])
percent tensor([0.6716, 0.6088, 0.6783, 0.6793, 0.6865, 0.7624, 0.6824, 0.6436, 0.7416,
        0.6807, 0.7103, 0.6926, 0.6185, 0.7445, 0.6291, 0.7240],
       device='cuda:0') torch.Size([16])
percent tensor([0.8200, 0.8201, 0.8477, 0.8450, 0.8736, 0.8537, 0.8428, 0.8375, 0.8334,
        0.8298, 0.8361, 0.8118, 0.8177, 0.8505, 0.8112, 0.8446],
       device='cuda:0') torch.Size([16])
percent tensor([0.3811, 0.5554, 0.5857, 0.5280, 0.6741, 0.6792, 0.4706, 0.4371, 0.5423,
        0.4712, 0.4763, 0.4536, 0.4697, 0.4511, 0.3421, 0.4170],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9998, 0.9996, 0.9996, 0.9997, 0.9995, 0.9996, 0.9996, 0.9997,
        0.9996, 0.9999, 0.9996, 0.9995, 0.9989, 0.9992, 0.9996],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 158 | Batch_idx: 0 |  Loss: (0.1064) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 158 | Batch_idx: 10 |  Loss: (0.1072) |  Loss2: (0.0000) | Acc: (96.00%) (1356/1408)
Epoch: 158 | Batch_idx: 20 |  Loss: (0.1014) |  Loss2: (0.0000) | Acc: (96.00%) (2592/2688)
Epoch: 158 | Batch_idx: 30 |  Loss: (0.1065) |  Loss2: (0.0000) | Acc: (96.00%) (3825/3968)
Epoch: 158 | Batch_idx: 40 |  Loss: (0.1085) |  Loss2: (0.0000) | Acc: (96.00%) (5052/5248)
Epoch: 158 | Batch_idx: 50 |  Loss: (0.1050) |  Loss2: (0.0000) | Acc: (96.00%) (6291/6528)
Epoch: 158 | Batch_idx: 60 |  Loss: (0.1053) |  Loss2: (0.0000) | Acc: (96.00%) (7522/7808)
Epoch: 158 | Batch_idx: 70 |  Loss: (0.1029) |  Loss2: (0.0000) | Acc: (96.00%) (8766/9088)
Epoch: 158 | Batch_idx: 80 |  Loss: (0.1024) |  Loss2: (0.0000) | Acc: (96.00%) (10006/10368)
Epoch: 158 | Batch_idx: 90 |  Loss: (0.1003) |  Loss2: (0.0000) | Acc: (96.00%) (11246/11648)
Epoch: 158 | Batch_idx: 100 |  Loss: (0.1021) |  Loss2: (0.0000) | Acc: (96.00%) (12473/12928)
Epoch: 158 | Batch_idx: 110 |  Loss: (0.1029) |  Loss2: (0.0000) | Acc: (96.00%) (13705/14208)
Epoch: 158 | Batch_idx: 120 |  Loss: (0.1029) |  Loss2: (0.0000) | Acc: (96.00%) (14939/15488)
Epoch: 158 | Batch_idx: 130 |  Loss: (0.1020) |  Loss2: (0.0000) | Acc: (96.00%) (16180/16768)
Epoch: 158 | Batch_idx: 140 |  Loss: (0.1018) |  Loss2: (0.0000) | Acc: (96.00%) (17420/18048)
Epoch: 158 | Batch_idx: 150 |  Loss: (0.1010) |  Loss2: (0.0000) | Acc: (96.00%) (18662/19328)
Epoch: 158 | Batch_idx: 160 |  Loss: (0.1019) |  Loss2: (0.0000) | Acc: (96.00%) (19890/20608)
Epoch: 158 | Batch_idx: 170 |  Loss: (0.1024) |  Loss2: (0.0000) | Acc: (96.00%) (21122/21888)
Epoch: 158 | Batch_idx: 180 |  Loss: (0.1012) |  Loss2: (0.0000) | Acc: (96.00%) (22365/23168)
Epoch: 158 | Batch_idx: 190 |  Loss: (0.1016) |  Loss2: (0.0000) | Acc: (96.00%) (23594/24448)
Epoch: 158 | Batch_idx: 200 |  Loss: (0.1018) |  Loss2: (0.0000) | Acc: (96.00%) (24831/25728)
Epoch: 158 | Batch_idx: 210 |  Loss: (0.1015) |  Loss2: (0.0000) | Acc: (96.00%) (26069/27008)
Epoch: 158 | Batch_idx: 220 |  Loss: (0.1014) |  Loss2: (0.0000) | Acc: (96.00%) (27306/28288)
Epoch: 158 | Batch_idx: 230 |  Loss: (0.1015) |  Loss2: (0.0000) | Acc: (96.00%) (28542/29568)
Epoch: 158 | Batch_idx: 240 |  Loss: (0.1014) |  Loss2: (0.0000) | Acc: (96.00%) (29775/30848)
Epoch: 158 | Batch_idx: 250 |  Loss: (0.1016) |  Loss2: (0.0000) | Acc: (96.00%) (31009/32128)
Epoch: 158 | Batch_idx: 260 |  Loss: (0.1015) |  Loss2: (0.0000) | Acc: (96.00%) (32245/33408)
Epoch: 158 | Batch_idx: 270 |  Loss: (0.1020) |  Loss2: (0.0000) | Acc: (96.00%) (33483/34688)
Epoch: 158 | Batch_idx: 280 |  Loss: (0.1035) |  Loss2: (0.0000) | Acc: (96.00%) (34699/35968)
Epoch: 158 | Batch_idx: 290 |  Loss: (0.1032) |  Loss2: (0.0000) | Acc: (96.00%) (35934/37248)
Epoch: 158 | Batch_idx: 300 |  Loss: (0.1033) |  Loss2: (0.0000) | Acc: (96.00%) (37165/38528)
Epoch: 158 | Batch_idx: 310 |  Loss: (0.1037) |  Loss2: (0.0000) | Acc: (96.00%) (38395/39808)
Epoch: 158 | Batch_idx: 320 |  Loss: (0.1038) |  Loss2: (0.0000) | Acc: (96.00%) (39633/41088)
Epoch: 158 | Batch_idx: 330 |  Loss: (0.1036) |  Loss2: (0.0000) | Acc: (96.00%) (40872/42368)
Epoch: 158 | Batch_idx: 340 |  Loss: (0.1037) |  Loss2: (0.0000) | Acc: (96.00%) (42105/43648)
Epoch: 158 | Batch_idx: 350 |  Loss: (0.1038) |  Loss2: (0.0000) | Acc: (96.00%) (43334/44928)
Epoch: 158 | Batch_idx: 360 |  Loss: (0.1042) |  Loss2: (0.0000) | Acc: (96.00%) (44565/46208)
Epoch: 158 | Batch_idx: 370 |  Loss: (0.1043) |  Loss2: (0.0000) | Acc: (96.00%) (45796/47488)
Epoch: 158 | Batch_idx: 380 |  Loss: (0.1044) |  Loss2: (0.0000) | Acc: (96.00%) (47033/48768)
Epoch: 158 | Batch_idx: 390 |  Loss: (0.1042) |  Loss2: (0.0000) | Acc: (96.00%) (48224/50000)
# TEST : Loss: (0.3912) | Acc: (88.00%) (8840/10000)
percent tensor([0.5814, 0.5929, 0.5865, 0.5744, 0.5923, 0.5655, 0.5976, 0.5943, 0.5962,
        0.5913, 0.5908, 0.5922, 0.5881, 0.5914, 0.5821, 0.5759],
       device='cuda:0') torch.Size([16])
percent tensor([0.5444, 0.5330, 0.5152, 0.5380, 0.5294, 0.5240, 0.5380, 0.5545, 0.5333,
        0.5172, 0.5130, 0.5090, 0.5370, 0.5484, 0.5425, 0.5331],
       device='cuda:0') torch.Size([16])
percent tensor([0.5390, 0.4665, 0.4881, 0.5564, 0.4619, 0.5913, 0.4944, 0.5380, 0.5247,
        0.4571, 0.4983, 0.4713, 0.4560, 0.6119, 0.5286, 0.5408],
       device='cuda:0') torch.Size([16])
percent tensor([0.6618, 0.6828, 0.6391, 0.6253, 0.6184, 0.6678, 0.6579, 0.6032, 0.6563,
        0.6850, 0.6749, 0.6646, 0.7094, 0.6606, 0.6632, 0.6756],
       device='cuda:0') torch.Size([16])
percent tensor([0.6663, 0.5965, 0.6815, 0.6894, 0.6882, 0.7422, 0.6696, 0.6498, 0.7457,
        0.6686, 0.7046, 0.6864, 0.6292, 0.7424, 0.6218, 0.7099],
       device='cuda:0') torch.Size([16])
percent tensor([0.8227, 0.8224, 0.8535, 0.8391, 0.8722, 0.8473, 0.8400, 0.8441, 0.8335,
        0.8323, 0.8360, 0.8138, 0.8191, 0.8520, 0.8164, 0.8368],
       device='cuda:0') torch.Size([16])
percent tensor([0.3810, 0.5552, 0.5652, 0.5010, 0.6424, 0.6455, 0.4894, 0.4116, 0.5403,
        0.4804, 0.4726, 0.4440, 0.4626, 0.4763, 0.3393, 0.3802],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9997, 0.9992, 0.9997, 0.9993, 0.9992, 0.9996, 0.9996, 0.9996,
        0.9997, 0.9999, 0.9998, 0.9994, 0.9993, 0.9991, 0.9996],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 159 | Batch_idx: 0 |  Loss: (0.0746) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 159 | Batch_idx: 10 |  Loss: (0.1008) |  Loss2: (0.0000) | Acc: (96.00%) (1365/1408)
Epoch: 159 | Batch_idx: 20 |  Loss: (0.1103) |  Loss2: (0.0000) | Acc: (96.00%) (2589/2688)
Epoch: 159 | Batch_idx: 30 |  Loss: (0.1198) |  Loss2: (0.0000) | Acc: (95.00%) (3801/3968)
Epoch: 159 | Batch_idx: 40 |  Loss: (0.1208) |  Loss2: (0.0000) | Acc: (95.00%) (5025/5248)
Epoch: 159 | Batch_idx: 50 |  Loss: (0.1205) |  Loss2: (0.0000) | Acc: (95.00%) (6254/6528)
Epoch: 159 | Batch_idx: 60 |  Loss: (0.1212) |  Loss2: (0.0000) | Acc: (95.00%) (7483/7808)
Epoch: 159 | Batch_idx: 70 |  Loss: (0.1240) |  Loss2: (0.0000) | Acc: (95.00%) (8697/9088)
Epoch: 159 | Batch_idx: 80 |  Loss: (0.1245) |  Loss2: (0.0000) | Acc: (95.00%) (9920/10368)
Epoch: 159 | Batch_idx: 90 |  Loss: (0.1245) |  Loss2: (0.0000) | Acc: (95.00%) (11146/11648)
Epoch: 159 | Batch_idx: 100 |  Loss: (0.1244) |  Loss2: (0.0000) | Acc: (95.00%) (12365/12928)
Epoch: 159 | Batch_idx: 110 |  Loss: (0.1234) |  Loss2: (0.0000) | Acc: (95.00%) (13589/14208)
Epoch: 159 | Batch_idx: 120 |  Loss: (0.1248) |  Loss2: (0.0000) | Acc: (95.00%) (14806/15488)
Epoch: 159 | Batch_idx: 130 |  Loss: (0.1242) |  Loss2: (0.0000) | Acc: (95.00%) (16022/16768)
Epoch: 159 | Batch_idx: 140 |  Loss: (0.1224) |  Loss2: (0.0000) | Acc: (95.00%) (17260/18048)
Epoch: 159 | Batch_idx: 150 |  Loss: (0.1221) |  Loss2: (0.0000) | Acc: (95.00%) (18485/19328)
Epoch: 159 | Batch_idx: 160 |  Loss: (0.1206) |  Loss2: (0.0000) | Acc: (95.00%) (19726/20608)
Epoch: 159 | Batch_idx: 170 |  Loss: (0.1231) |  Loss2: (0.0000) | Acc: (95.00%) (20943/21888)
Epoch: 159 | Batch_idx: 180 |  Loss: (0.1232) |  Loss2: (0.0000) | Acc: (95.00%) (22171/23168)
Epoch: 159 | Batch_idx: 190 |  Loss: (0.1243) |  Loss2: (0.0000) | Acc: (95.00%) (23383/24448)
Epoch: 159 | Batch_idx: 200 |  Loss: (0.1245) |  Loss2: (0.0000) | Acc: (95.00%) (24606/25728)
Epoch: 159 | Batch_idx: 210 |  Loss: (0.1242) |  Loss2: (0.0000) | Acc: (95.00%) (25832/27008)
Epoch: 159 | Batch_idx: 220 |  Loss: (0.1231) |  Loss2: (0.0000) | Acc: (95.00%) (27072/28288)
Epoch: 159 | Batch_idx: 230 |  Loss: (0.1229) |  Loss2: (0.0000) | Acc: (95.00%) (28302/29568)
Epoch: 159 | Batch_idx: 240 |  Loss: (0.1217) |  Loss2: (0.0000) | Acc: (95.00%) (29544/30848)
Epoch: 159 | Batch_idx: 250 |  Loss: (0.1210) |  Loss2: (0.0000) | Acc: (95.00%) (30778/32128)
Epoch: 159 | Batch_idx: 260 |  Loss: (0.1204) |  Loss2: (0.0000) | Acc: (95.00%) (32017/33408)
Epoch: 159 | Batch_idx: 270 |  Loss: (0.1207) |  Loss2: (0.0000) | Acc: (95.00%) (33234/34688)
Epoch: 159 | Batch_idx: 280 |  Loss: (0.1215) |  Loss2: (0.0000) | Acc: (95.00%) (34456/35968)
Epoch: 159 | Batch_idx: 290 |  Loss: (0.1208) |  Loss2: (0.0000) | Acc: (95.00%) (35696/37248)
Epoch: 159 | Batch_idx: 300 |  Loss: (0.1209) |  Loss2: (0.0000) | Acc: (95.00%) (36923/38528)
Epoch: 159 | Batch_idx: 310 |  Loss: (0.1210) |  Loss2: (0.0000) | Acc: (95.00%) (38150/39808)
Epoch: 159 | Batch_idx: 320 |  Loss: (0.1203) |  Loss2: (0.0000) | Acc: (95.00%) (39389/41088)
Epoch: 159 | Batch_idx: 330 |  Loss: (0.1200) |  Loss2: (0.0000) | Acc: (95.00%) (40616/42368)
Epoch: 159 | Batch_idx: 340 |  Loss: (0.1196) |  Loss2: (0.0000) | Acc: (95.00%) (41849/43648)
Epoch: 159 | Batch_idx: 350 |  Loss: (0.1197) |  Loss2: (0.0000) | Acc: (95.00%) (43080/44928)
Epoch: 159 | Batch_idx: 360 |  Loss: (0.1195) |  Loss2: (0.0000) | Acc: (95.00%) (44309/46208)
Epoch: 159 | Batch_idx: 370 |  Loss: (0.1192) |  Loss2: (0.0000) | Acc: (95.00%) (45540/47488)
Epoch: 159 | Batch_idx: 380 |  Loss: (0.1188) |  Loss2: (0.0000) | Acc: (95.00%) (46770/48768)
Epoch: 159 | Batch_idx: 390 |  Loss: (0.1184) |  Loss2: (0.0000) | Acc: (95.00%) (47972/50000)
# TEST : Loss: (0.4002) | Acc: (88.00%) (8830/10000)
percent tensor([0.5846, 0.5969, 0.5907, 0.5775, 0.5965, 0.5676, 0.6026, 0.5986, 0.5997,
        0.5958, 0.5944, 0.5976, 0.5917, 0.5951, 0.5856, 0.5784],
       device='cuda:0') torch.Size([16])
percent tensor([0.5480, 0.5351, 0.5209, 0.5407, 0.5332, 0.5271, 0.5408, 0.5575, 0.5363,
        0.5223, 0.5154, 0.5162, 0.5399, 0.5458, 0.5448, 0.5357],
       device='cuda:0') torch.Size([16])
percent tensor([0.5486, 0.4747, 0.4959, 0.5652, 0.4819, 0.5922, 0.5055, 0.5538, 0.5311,
        0.4674, 0.4986, 0.4767, 0.4643, 0.6046, 0.5377, 0.5484],
       device='cuda:0') torch.Size([16])
percent tensor([0.6559, 0.6781, 0.6297, 0.6207, 0.6072, 0.6609, 0.6496, 0.5930, 0.6514,
        0.6803, 0.6733, 0.6539, 0.7076, 0.6554, 0.6579, 0.6711],
       device='cuda:0') torch.Size([16])
percent tensor([0.6626, 0.5926, 0.6752, 0.6940, 0.6796, 0.7433, 0.6620, 0.6441, 0.7506,
        0.6708, 0.7110, 0.6872, 0.6316, 0.7527, 0.6212, 0.7112],
       device='cuda:0') torch.Size([16])
percent tensor([0.8278, 0.8256, 0.8553, 0.8406, 0.8750, 0.8506, 0.8421, 0.8460, 0.8348,
        0.8380, 0.8389, 0.8171, 0.8263, 0.8547, 0.8217, 0.8416],
       device='cuda:0') torch.Size([16])
percent tensor([0.4132, 0.5764, 0.5812, 0.5219, 0.6496, 0.6851, 0.5126, 0.4282, 0.5433,
        0.5009, 0.4757, 0.4663, 0.4787, 0.4826, 0.3544, 0.4272],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9997, 0.9993, 0.9998, 0.9993, 0.9993, 0.9997, 0.9996, 0.9996,
        0.9996, 0.9999, 0.9997, 0.9995, 0.9993, 0.9990, 0.9996],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 160 | Batch_idx: 0 |  Loss: (0.1231) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 160 | Batch_idx: 10 |  Loss: (0.1022) |  Loss2: (0.0000) | Acc: (96.00%) (1354/1408)
Epoch: 160 | Batch_idx: 20 |  Loss: (0.0959) |  Loss2: (0.0000) | Acc: (96.00%) (2591/2688)
Epoch: 160 | Batch_idx: 30 |  Loss: (0.0926) |  Loss2: (0.0000) | Acc: (96.00%) (3834/3968)
Epoch: 160 | Batch_idx: 40 |  Loss: (0.0920) |  Loss2: (0.0000) | Acc: (96.00%) (5071/5248)
Epoch: 160 | Batch_idx: 50 |  Loss: (0.0927) |  Loss2: (0.0000) | Acc: (96.00%) (6310/6528)
Epoch: 160 | Batch_idx: 60 |  Loss: (0.0938) |  Loss2: (0.0000) | Acc: (96.00%) (7550/7808)
Epoch: 160 | Batch_idx: 70 |  Loss: (0.0943) |  Loss2: (0.0000) | Acc: (96.00%) (8794/9088)
Epoch: 160 | Batch_idx: 80 |  Loss: (0.0967) |  Loss2: (0.0000) | Acc: (96.00%) (10014/10368)
Epoch: 160 | Batch_idx: 90 |  Loss: (0.0948) |  Loss2: (0.0000) | Acc: (96.00%) (11264/11648)
Epoch: 160 | Batch_idx: 100 |  Loss: (0.0945) |  Loss2: (0.0000) | Acc: (96.00%) (12506/12928)
Epoch: 160 | Batch_idx: 110 |  Loss: (0.0962) |  Loss2: (0.0000) | Acc: (96.00%) (13733/14208)
Epoch: 160 | Batch_idx: 120 |  Loss: (0.0959) |  Loss2: (0.0000) | Acc: (96.00%) (14976/15488)
Epoch: 160 | Batch_idx: 130 |  Loss: (0.0954) |  Loss2: (0.0000) | Acc: (96.00%) (16223/16768)
Epoch: 160 | Batch_idx: 140 |  Loss: (0.0987) |  Loss2: (0.0000) | Acc: (96.00%) (17433/18048)
Epoch: 160 | Batch_idx: 150 |  Loss: (0.1004) |  Loss2: (0.0000) | Acc: (96.00%) (18662/19328)
Epoch: 160 | Batch_idx: 160 |  Loss: (0.0999) |  Loss2: (0.0000) | Acc: (96.00%) (19906/20608)
Epoch: 160 | Batch_idx: 170 |  Loss: (0.1005) |  Loss2: (0.0000) | Acc: (96.00%) (21133/21888)
Epoch: 160 | Batch_idx: 180 |  Loss: (0.1008) |  Loss2: (0.0000) | Acc: (96.00%) (22361/23168)
Epoch: 160 | Batch_idx: 190 |  Loss: (0.1012) |  Loss2: (0.0000) | Acc: (96.00%) (23597/24448)
Epoch: 160 | Batch_idx: 200 |  Loss: (0.1008) |  Loss2: (0.0000) | Acc: (96.00%) (24839/25728)
Epoch: 160 | Batch_idx: 210 |  Loss: (0.1012) |  Loss2: (0.0000) | Acc: (96.00%) (26074/27008)
Epoch: 160 | Batch_idx: 220 |  Loss: (0.1009) |  Loss2: (0.0000) | Acc: (96.00%) (27311/28288)
Epoch: 160 | Batch_idx: 230 |  Loss: (0.1015) |  Loss2: (0.0000) | Acc: (96.00%) (28540/29568)
Epoch: 160 | Batch_idx: 240 |  Loss: (0.1021) |  Loss2: (0.0000) | Acc: (96.00%) (29768/30848)
Epoch: 160 | Batch_idx: 250 |  Loss: (0.1027) |  Loss2: (0.0000) | Acc: (96.00%) (31002/32128)
Epoch: 160 | Batch_idx: 260 |  Loss: (0.1027) |  Loss2: (0.0000) | Acc: (96.00%) (32246/33408)
Epoch: 160 | Batch_idx: 270 |  Loss: (0.1023) |  Loss2: (0.0000) | Acc: (96.00%) (33485/34688)
Epoch: 160 | Batch_idx: 280 |  Loss: (0.1023) |  Loss2: (0.0000) | Acc: (96.00%) (34724/35968)
Epoch: 160 | Batch_idx: 290 |  Loss: (0.1023) |  Loss2: (0.0000) | Acc: (96.00%) (35958/37248)
Epoch: 160 | Batch_idx: 300 |  Loss: (0.1026) |  Loss2: (0.0000) | Acc: (96.00%) (37182/38528)
Epoch: 160 | Batch_idx: 310 |  Loss: (0.1022) |  Loss2: (0.0000) | Acc: (96.00%) (38424/39808)
Epoch: 160 | Batch_idx: 320 |  Loss: (0.1021) |  Loss2: (0.0000) | Acc: (96.00%) (39664/41088)
Epoch: 160 | Batch_idx: 330 |  Loss: (0.1021) |  Loss2: (0.0000) | Acc: (96.00%) (40898/42368)
Epoch: 160 | Batch_idx: 340 |  Loss: (0.1023) |  Loss2: (0.0000) | Acc: (96.00%) (42133/43648)
Epoch: 160 | Batch_idx: 350 |  Loss: (0.1028) |  Loss2: (0.0000) | Acc: (96.00%) (43365/44928)
Epoch: 160 | Batch_idx: 360 |  Loss: (0.1026) |  Loss2: (0.0000) | Acc: (96.00%) (44607/46208)
Epoch: 160 | Batch_idx: 370 |  Loss: (0.1023) |  Loss2: (0.0000) | Acc: (96.00%) (45844/47488)
Epoch: 160 | Batch_idx: 380 |  Loss: (0.1027) |  Loss2: (0.0000) | Acc: (96.00%) (47073/48768)
Epoch: 160 | Batch_idx: 390 |  Loss: (0.1028) |  Loss2: (0.0000) | Acc: (96.00%) (48265/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_160.pth.tar'
# TEST : Loss: (0.4315) | Acc: (87.00%) (8770/10000)
percent tensor([0.5831, 0.5982, 0.5859, 0.5763, 0.5927, 0.5669, 0.6014, 0.5965, 0.6000,
        0.5945, 0.5947, 0.5930, 0.5909, 0.5984, 0.5857, 0.5792],
       device='cuda:0') torch.Size([16])
percent tensor([0.5412, 0.5374, 0.5091, 0.5378, 0.5263, 0.5208, 0.5380, 0.5519, 0.5318,
        0.5167, 0.5145, 0.5041, 0.5354, 0.5518, 0.5439, 0.5324],
       device='cuda:0') torch.Size([16])
percent tensor([0.5342, 0.4788, 0.4850, 0.5552, 0.4718, 0.5769, 0.5043, 0.5440, 0.5168,
        0.4617, 0.4914, 0.4677, 0.4599, 0.6100, 0.5333, 0.5390],
       device='cuda:0') torch.Size([16])
percent tensor([0.6584, 0.6770, 0.6270, 0.6124, 0.6064, 0.6714, 0.6503, 0.5882, 0.6551,
        0.6777, 0.6726, 0.6574, 0.7056, 0.6581, 0.6563, 0.6708],
       device='cuda:0') torch.Size([16])
percent tensor([0.6679, 0.5990, 0.6713, 0.6725, 0.6823, 0.7588, 0.6783, 0.6303, 0.7428,
        0.6782, 0.7203, 0.6817, 0.6179, 0.7525, 0.6243, 0.7150],
       device='cuda:0') torch.Size([16])
percent tensor([0.8241, 0.8253, 0.8499, 0.8449, 0.8737, 0.8565, 0.8464, 0.8442, 0.8345,
        0.8355, 0.8432, 0.8182, 0.8167, 0.8580, 0.8190, 0.8487],
       device='cuda:0') torch.Size([16])
percent tensor([0.4305, 0.5957, 0.5887, 0.5304, 0.6550, 0.6839, 0.5228, 0.4620, 0.5684,
        0.5366, 0.4978, 0.4746, 0.4769, 0.5020, 0.3779, 0.4595],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9996, 0.9993, 0.9997, 0.9994, 0.9995, 0.9995, 0.9997, 0.9997,
        0.9995, 0.9999, 0.9997, 0.9996, 0.9992, 0.9991, 0.9996],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(180.8198, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(821.4667, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(808.7940, device='cuda:0')
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1502.0043, device='cuda:0')
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(485.1147, device='cuda:0')
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2273.0750, device='cuda:0')
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4288.2266, device='cuda:0')
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1368.6105, device='cuda:0')
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6264.6655, device='cuda:0')
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11652.8164, device='cuda:0')
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3845.0295, device='cuda:0')
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16221.7637, device='cuda:0')
Epoch: 161 | Batch_idx: 0 |  Loss: (0.0709) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 161 | Batch_idx: 10 |  Loss: (0.1016) |  Loss2: (0.0000) | Acc: (96.00%) (1357/1408)
Epoch: 161 | Batch_idx: 20 |  Loss: (0.1212) |  Loss2: (0.0000) | Acc: (95.00%) (2568/2688)
Epoch: 161 | Batch_idx: 30 |  Loss: (0.1297) |  Loss2: (0.0000) | Acc: (95.00%) (3783/3968)
Epoch: 161 | Batch_idx: 40 |  Loss: (0.1286) |  Loss2: (0.0000) | Acc: (95.00%) (5005/5248)
Epoch: 161 | Batch_idx: 50 |  Loss: (0.1328) |  Loss2: (0.0000) | Acc: (95.00%) (6213/6528)
Epoch: 161 | Batch_idx: 60 |  Loss: (0.1380) |  Loss2: (0.0000) | Acc: (95.00%) (7422/7808)
Epoch: 161 | Batch_idx: 70 |  Loss: (0.1348) |  Loss2: (0.0000) | Acc: (95.00%) (8652/9088)
Epoch: 161 | Batch_idx: 80 |  Loss: (0.1348) |  Loss2: (0.0000) | Acc: (95.00%) (9865/10368)
Epoch: 161 | Batch_idx: 90 |  Loss: (0.1369) |  Loss2: (0.0000) | Acc: (95.00%) (11075/11648)
Epoch: 161 | Batch_idx: 100 |  Loss: (0.1351) |  Loss2: (0.0000) | Acc: (95.00%) (12305/12928)
Epoch: 161 | Batch_idx: 110 |  Loss: (0.1377) |  Loss2: (0.0000) | Acc: (95.00%) (13512/14208)
Epoch: 161 | Batch_idx: 120 |  Loss: (0.1376) |  Loss2: (0.0000) | Acc: (95.00%) (14728/15488)
Epoch: 161 | Batch_idx: 130 |  Loss: (0.1363) |  Loss2: (0.0000) | Acc: (95.00%) (15956/16768)
Epoch: 161 | Batch_idx: 140 |  Loss: (0.1348) |  Loss2: (0.0000) | Acc: (95.00%) (17189/18048)
Epoch: 161 | Batch_idx: 150 |  Loss: (0.1351) |  Loss2: (0.0000) | Acc: (95.00%) (18402/19328)
Epoch: 161 | Batch_idx: 160 |  Loss: (0.1345) |  Loss2: (0.0000) | Acc: (95.00%) (19627/20608)
Epoch: 161 | Batch_idx: 170 |  Loss: (0.1347) |  Loss2: (0.0000) | Acc: (95.00%) (20841/21888)
Epoch: 161 | Batch_idx: 180 |  Loss: (0.1344) |  Loss2: (0.0000) | Acc: (95.00%) (22064/23168)
Epoch: 161 | Batch_idx: 190 |  Loss: (0.1346) |  Loss2: (0.0000) | Acc: (95.00%) (23279/24448)
Epoch: 161 | Batch_idx: 200 |  Loss: (0.1342) |  Loss2: (0.0000) | Acc: (95.00%) (24508/25728)
Epoch: 161 | Batch_idx: 210 |  Loss: (0.1344) |  Loss2: (0.0000) | Acc: (95.00%) (25722/27008)
Epoch: 161 | Batch_idx: 220 |  Loss: (0.1340) |  Loss2: (0.0000) | Acc: (95.00%) (26947/28288)
Epoch: 161 | Batch_idx: 230 |  Loss: (0.1343) |  Loss2: (0.0000) | Acc: (95.00%) (28166/29568)
Epoch: 161 | Batch_idx: 240 |  Loss: (0.1342) |  Loss2: (0.0000) | Acc: (95.00%) (29393/30848)
Epoch: 161 | Batch_idx: 250 |  Loss: (0.1334) |  Loss2: (0.0000) | Acc: (95.00%) (30620/32128)
Epoch: 161 | Batch_idx: 260 |  Loss: (0.1321) |  Loss2: (0.0000) | Acc: (95.00%) (31864/33408)
Epoch: 161 | Batch_idx: 270 |  Loss: (0.1307) |  Loss2: (0.0000) | Acc: (95.00%) (33105/34688)
Epoch: 161 | Batch_idx: 280 |  Loss: (0.1305) |  Loss2: (0.0000) | Acc: (95.00%) (34328/35968)
Epoch: 161 | Batch_idx: 290 |  Loss: (0.1295) |  Loss2: (0.0000) | Acc: (95.00%) (35564/37248)
Epoch: 161 | Batch_idx: 300 |  Loss: (0.1292) |  Loss2: (0.0000) | Acc: (95.00%) (36791/38528)
Epoch: 161 | Batch_idx: 310 |  Loss: (0.1284) |  Loss2: (0.0000) | Acc: (95.00%) (38029/39808)
Epoch: 161 | Batch_idx: 320 |  Loss: (0.1273) |  Loss2: (0.0000) | Acc: (95.00%) (39267/41088)
Epoch: 161 | Batch_idx: 330 |  Loss: (0.1273) |  Loss2: (0.0000) | Acc: (95.00%) (40498/42368)
Epoch: 161 | Batch_idx: 340 |  Loss: (0.1267) |  Loss2: (0.0000) | Acc: (95.00%) (41732/43648)
Epoch: 161 | Batch_idx: 350 |  Loss: (0.1263) |  Loss2: (0.0000) | Acc: (95.00%) (42956/44928)
Epoch: 161 | Batch_idx: 360 |  Loss: (0.1255) |  Loss2: (0.0000) | Acc: (95.00%) (44198/46208)
Epoch: 161 | Batch_idx: 370 |  Loss: (0.1251) |  Loss2: (0.0000) | Acc: (95.00%) (45430/47488)
Epoch: 161 | Batch_idx: 380 |  Loss: (0.1249) |  Loss2: (0.0000) | Acc: (95.00%) (46656/48768)
Epoch: 161 | Batch_idx: 390 |  Loss: (0.1246) |  Loss2: (0.0000) | Acc: (95.00%) (47834/50000)
# TEST : Loss: (0.4236) | Acc: (87.00%) (8780/10000)
percent tensor([0.5838, 0.6019, 0.5864, 0.5786, 0.5938, 0.5681, 0.6038, 0.5995, 0.6027,
        0.5970, 0.5968, 0.5942, 0.5927, 0.6027, 0.5881, 0.5811],
       device='cuda:0') torch.Size([16])
percent tensor([0.5430, 0.5372, 0.5166, 0.5438, 0.5347, 0.5252, 0.5411, 0.5593, 0.5344,
        0.5175, 0.5134, 0.5085, 0.5358, 0.5491, 0.5467, 0.5335],
       device='cuda:0') torch.Size([16])
percent tensor([0.5500, 0.5027, 0.4955, 0.5673, 0.4803, 0.5845, 0.5221, 0.5532, 0.5347,
        0.4858, 0.5150, 0.4890, 0.4817, 0.6259, 0.5481, 0.5527],
       device='cuda:0') torch.Size([16])
percent tensor([0.6778, 0.6933, 0.6509, 0.6376, 0.6315, 0.6905, 0.6716, 0.6131, 0.6707,
        0.6950, 0.6882, 0.6787, 0.7205, 0.6750, 0.6787, 0.6908],
       device='cuda:0') torch.Size([16])
percent tensor([0.6813, 0.5959, 0.7028, 0.7110, 0.7293, 0.7922, 0.6978, 0.6765, 0.7583,
        0.6816, 0.7277, 0.7029, 0.5975, 0.7679, 0.6442, 0.7412],
       device='cuda:0') torch.Size([16])
percent tensor([0.8140, 0.8142, 0.8471, 0.8366, 0.8704, 0.8471, 0.8400, 0.8349, 0.8290,
        0.8288, 0.8368, 0.8094, 0.8070, 0.8496, 0.8079, 0.8376],
       device='cuda:0') torch.Size([16])
percent tensor([0.4260, 0.6024, 0.5839, 0.5245, 0.6354, 0.6723, 0.5290, 0.4144, 0.5923,
        0.5583, 0.5458, 0.4854, 0.5275, 0.5212, 0.3616, 0.4299],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9995, 0.9993, 0.9997, 0.9993, 0.9992, 0.9995, 0.9996, 0.9997,
        0.9996, 0.9999, 0.9998, 0.9997, 0.9993, 0.9991, 0.9996],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 162 | Batch_idx: 0 |  Loss: (0.0912) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 162 | Batch_idx: 10 |  Loss: (0.0969) |  Loss2: (0.0000) | Acc: (96.00%) (1359/1408)
Epoch: 162 | Batch_idx: 20 |  Loss: (0.0895) |  Loss2: (0.0000) | Acc: (96.00%) (2605/2688)
Epoch: 162 | Batch_idx: 30 |  Loss: (0.0881) |  Loss2: (0.0000) | Acc: (97.00%) (3853/3968)
Epoch: 162 | Batch_idx: 40 |  Loss: (0.0879) |  Loss2: (0.0000) | Acc: (97.00%) (5092/5248)
Epoch: 162 | Batch_idx: 50 |  Loss: (0.0873) |  Loss2: (0.0000) | Acc: (97.00%) (6335/6528)
Epoch: 162 | Batch_idx: 60 |  Loss: (0.0900) |  Loss2: (0.0000) | Acc: (96.00%) (7573/7808)
Epoch: 162 | Batch_idx: 70 |  Loss: (0.0882) |  Loss2: (0.0000) | Acc: (97.00%) (8820/9088)
Epoch: 162 | Batch_idx: 80 |  Loss: (0.0887) |  Loss2: (0.0000) | Acc: (97.00%) (10065/10368)
Epoch: 162 | Batch_idx: 90 |  Loss: (0.0890) |  Loss2: (0.0000) | Acc: (97.00%) (11303/11648)
Epoch: 162 | Batch_idx: 100 |  Loss: (0.0906) |  Loss2: (0.0000) | Acc: (96.00%) (12540/12928)
Epoch: 162 | Batch_idx: 110 |  Loss: (0.0925) |  Loss2: (0.0000) | Acc: (96.00%) (13769/14208)
Epoch: 162 | Batch_idx: 120 |  Loss: (0.0953) |  Loss2: (0.0000) | Acc: (96.00%) (14993/15488)
Epoch: 162 | Batch_idx: 130 |  Loss: (0.0956) |  Loss2: (0.0000) | Acc: (96.00%) (16220/16768)
Epoch: 162 | Batch_idx: 140 |  Loss: (0.0957) |  Loss2: (0.0000) | Acc: (96.00%) (17455/18048)
Epoch: 162 | Batch_idx: 150 |  Loss: (0.0966) |  Loss2: (0.0000) | Acc: (96.00%) (18691/19328)
Epoch: 162 | Batch_idx: 160 |  Loss: (0.0966) |  Loss2: (0.0000) | Acc: (96.00%) (19922/20608)
Epoch: 162 | Batch_idx: 170 |  Loss: (0.0967) |  Loss2: (0.0000) | Acc: (96.00%) (21157/21888)
Epoch: 162 | Batch_idx: 180 |  Loss: (0.0970) |  Loss2: (0.0000) | Acc: (96.00%) (22397/23168)
Epoch: 162 | Batch_idx: 190 |  Loss: (0.0974) |  Loss2: (0.0000) | Acc: (96.00%) (23634/24448)
Epoch: 162 | Batch_idx: 200 |  Loss: (0.0971) |  Loss2: (0.0000) | Acc: (96.00%) (24875/25728)
Epoch: 162 | Batch_idx: 210 |  Loss: (0.0975) |  Loss2: (0.0000) | Acc: (96.00%) (26108/27008)
Epoch: 162 | Batch_idx: 220 |  Loss: (0.0977) |  Loss2: (0.0000) | Acc: (96.00%) (27346/28288)
Epoch: 162 | Batch_idx: 230 |  Loss: (0.0979) |  Loss2: (0.0000) | Acc: (96.00%) (28583/29568)
Epoch: 162 | Batch_idx: 240 |  Loss: (0.0984) |  Loss2: (0.0000) | Acc: (96.00%) (29814/30848)
Epoch: 162 | Batch_idx: 250 |  Loss: (0.0982) |  Loss2: (0.0000) | Acc: (96.00%) (31057/32128)
Epoch: 162 | Batch_idx: 260 |  Loss: (0.0988) |  Loss2: (0.0000) | Acc: (96.00%) (32283/33408)
Epoch: 162 | Batch_idx: 270 |  Loss: (0.0990) |  Loss2: (0.0000) | Acc: (96.00%) (33519/34688)
Epoch: 162 | Batch_idx: 280 |  Loss: (0.0988) |  Loss2: (0.0000) | Acc: (96.00%) (34755/35968)
Epoch: 162 | Batch_idx: 290 |  Loss: (0.0991) |  Loss2: (0.0000) | Acc: (96.00%) (35986/37248)
Epoch: 162 | Batch_idx: 300 |  Loss: (0.0996) |  Loss2: (0.0000) | Acc: (96.00%) (37217/38528)
Epoch: 162 | Batch_idx: 310 |  Loss: (0.1002) |  Loss2: (0.0000) | Acc: (96.00%) (38443/39808)
Epoch: 162 | Batch_idx: 320 |  Loss: (0.1001) |  Loss2: (0.0000) | Acc: (96.00%) (39682/41088)
Epoch: 162 | Batch_idx: 330 |  Loss: (0.1003) |  Loss2: (0.0000) | Acc: (96.00%) (40910/42368)
Epoch: 162 | Batch_idx: 340 |  Loss: (0.1004) |  Loss2: (0.0000) | Acc: (96.00%) (42150/43648)
Epoch: 162 | Batch_idx: 350 |  Loss: (0.1005) |  Loss2: (0.0000) | Acc: (96.00%) (43385/44928)
Epoch: 162 | Batch_idx: 360 |  Loss: (0.1010) |  Loss2: (0.0000) | Acc: (96.00%) (44614/46208)
Epoch: 162 | Batch_idx: 370 |  Loss: (0.1007) |  Loss2: (0.0000) | Acc: (96.00%) (45857/47488)
Epoch: 162 | Batch_idx: 380 |  Loss: (0.1009) |  Loss2: (0.0000) | Acc: (96.00%) (47090/48768)
Epoch: 162 | Batch_idx: 390 |  Loss: (0.1009) |  Loss2: (0.0000) | Acc: (96.00%) (48280/50000)
# TEST : Loss: (0.3961) | Acc: (88.00%) (8806/10000)
percent tensor([0.5856, 0.6019, 0.5931, 0.5805, 0.5989, 0.5694, 0.6063, 0.6013, 0.6027,
        0.5990, 0.5965, 0.6002, 0.5933, 0.6014, 0.5886, 0.5814],
       device='cuda:0') torch.Size([16])
percent tensor([0.5459, 0.5371, 0.5149, 0.5398, 0.5354, 0.5271, 0.5440, 0.5580, 0.5356,
        0.5183, 0.5128, 0.5113, 0.5372, 0.5494, 0.5476, 0.5338],
       device='cuda:0') torch.Size([16])
percent tensor([0.5558, 0.5054, 0.4846, 0.5555, 0.4781, 0.5891, 0.5263, 0.5472, 0.5439,
        0.4880, 0.5206, 0.4851, 0.4852, 0.6329, 0.5497, 0.5560],
       device='cuda:0') torch.Size([16])
percent tensor([0.6782, 0.6974, 0.6516, 0.6385, 0.6346, 0.6890, 0.6761, 0.6164, 0.6769,
        0.6988, 0.6935, 0.6781, 0.7272, 0.6761, 0.6824, 0.6914],
       device='cuda:0') torch.Size([16])
percent tensor([0.6810, 0.5957, 0.7127, 0.7221, 0.7358, 0.7728, 0.6840, 0.6886, 0.7673,
        0.6833, 0.7250, 0.7059, 0.6297, 0.7531, 0.6423, 0.7409],
       device='cuda:0') torch.Size([16])
percent tensor([0.8146, 0.8085, 0.8463, 0.8347, 0.8696, 0.8479, 0.8293, 0.8345, 0.8251,
        0.8218, 0.8323, 0.8041, 0.8121, 0.8385, 0.8106, 0.8330],
       device='cuda:0') torch.Size([16])
percent tensor([0.4169, 0.5709, 0.6026, 0.5361, 0.6639, 0.6942, 0.5070, 0.4416, 0.5957,
        0.5393, 0.5576, 0.4782, 0.4967, 0.4936, 0.3460, 0.4074],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9997, 0.9992, 0.9997, 0.9994, 0.9993, 0.9996, 0.9997, 0.9996,
        0.9996, 0.9999, 0.9997, 0.9995, 0.9995, 0.9989, 0.9994],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 163 | Batch_idx: 0 |  Loss: (0.0731) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 163 | Batch_idx: 10 |  Loss: (0.0811) |  Loss2: (0.0000) | Acc: (97.00%) (1369/1408)
Epoch: 163 | Batch_idx: 20 |  Loss: (0.1041) |  Loss2: (0.0000) | Acc: (96.00%) (2593/2688)
Epoch: 163 | Batch_idx: 30 |  Loss: (0.1076) |  Loss2: (0.0000) | Acc: (96.00%) (3822/3968)
Epoch: 163 | Batch_idx: 40 |  Loss: (0.1067) |  Loss2: (0.0000) | Acc: (96.00%) (5056/5248)
Epoch: 163 | Batch_idx: 50 |  Loss: (0.1121) |  Loss2: (0.0000) | Acc: (96.00%) (6274/6528)
Epoch: 163 | Batch_idx: 60 |  Loss: (0.1150) |  Loss2: (0.0000) | Acc: (96.00%) (7498/7808)
Epoch: 163 | Batch_idx: 70 |  Loss: (0.1168) |  Loss2: (0.0000) | Acc: (95.00%) (8722/9088)
Epoch: 163 | Batch_idx: 80 |  Loss: (0.1197) |  Loss2: (0.0000) | Acc: (95.00%) (9939/10368)
Epoch: 163 | Batch_idx: 90 |  Loss: (0.1204) |  Loss2: (0.0000) | Acc: (95.00%) (11168/11648)
Epoch: 163 | Batch_idx: 100 |  Loss: (0.1181) |  Loss2: (0.0000) | Acc: (95.00%) (12408/12928)
Epoch: 163 | Batch_idx: 110 |  Loss: (0.1179) |  Loss2: (0.0000) | Acc: (95.00%) (13636/14208)
Epoch: 163 | Batch_idx: 120 |  Loss: (0.1174) |  Loss2: (0.0000) | Acc: (96.00%) (14870/15488)
Epoch: 163 | Batch_idx: 130 |  Loss: (0.1168) |  Loss2: (0.0000) | Acc: (96.00%) (16105/16768)
Epoch: 163 | Batch_idx: 140 |  Loss: (0.1156) |  Loss2: (0.0000) | Acc: (96.00%) (17333/18048)
Epoch: 163 | Batch_idx: 150 |  Loss: (0.1156) |  Loss2: (0.0000) | Acc: (96.00%) (18564/19328)
Epoch: 163 | Batch_idx: 160 |  Loss: (0.1157) |  Loss2: (0.0000) | Acc: (96.00%) (19792/20608)
Epoch: 163 | Batch_idx: 170 |  Loss: (0.1145) |  Loss2: (0.0000) | Acc: (96.00%) (21029/21888)
Epoch: 163 | Batch_idx: 180 |  Loss: (0.1137) |  Loss2: (0.0000) | Acc: (96.00%) (22269/23168)
Epoch: 163 | Batch_idx: 190 |  Loss: (0.1139) |  Loss2: (0.0000) | Acc: (96.00%) (23495/24448)
Epoch: 163 | Batch_idx: 200 |  Loss: (0.1137) |  Loss2: (0.0000) | Acc: (96.00%) (24725/25728)
Epoch: 163 | Batch_idx: 210 |  Loss: (0.1129) |  Loss2: (0.0000) | Acc: (96.00%) (25963/27008)
Epoch: 163 | Batch_idx: 220 |  Loss: (0.1131) |  Loss2: (0.0000) | Acc: (96.00%) (27194/28288)
Epoch: 163 | Batch_idx: 230 |  Loss: (0.1122) |  Loss2: (0.0000) | Acc: (96.00%) (28433/29568)
Epoch: 163 | Batch_idx: 240 |  Loss: (0.1115) |  Loss2: (0.0000) | Acc: (96.00%) (29666/30848)
Epoch: 163 | Batch_idx: 250 |  Loss: (0.1116) |  Loss2: (0.0000) | Acc: (96.00%) (30899/32128)
Epoch: 163 | Batch_idx: 260 |  Loss: (0.1116) |  Loss2: (0.0000) | Acc: (96.00%) (32130/33408)
Epoch: 163 | Batch_idx: 270 |  Loss: (0.1111) |  Loss2: (0.0000) | Acc: (96.00%) (33371/34688)
Epoch: 163 | Batch_idx: 280 |  Loss: (0.1106) |  Loss2: (0.0000) | Acc: (96.00%) (34612/35968)
Epoch: 163 | Batch_idx: 290 |  Loss: (0.1115) |  Loss2: (0.0000) | Acc: (96.00%) (35829/37248)
Epoch: 163 | Batch_idx: 300 |  Loss: (0.1111) |  Loss2: (0.0000) | Acc: (96.00%) (37065/38528)
Epoch: 163 | Batch_idx: 310 |  Loss: (0.1111) |  Loss2: (0.0000) | Acc: (96.00%) (38292/39808)
Epoch: 163 | Batch_idx: 320 |  Loss: (0.1102) |  Loss2: (0.0000) | Acc: (96.00%) (39533/41088)
Epoch: 163 | Batch_idx: 330 |  Loss: (0.1104) |  Loss2: (0.0000) | Acc: (96.00%) (40761/42368)
Epoch: 163 | Batch_idx: 340 |  Loss: (0.1101) |  Loss2: (0.0000) | Acc: (96.00%) (42000/43648)
Epoch: 163 | Batch_idx: 350 |  Loss: (0.1095) |  Loss2: (0.0000) | Acc: (96.00%) (43246/44928)
Epoch: 163 | Batch_idx: 360 |  Loss: (0.1099) |  Loss2: (0.0000) | Acc: (96.00%) (44474/46208)
Epoch: 163 | Batch_idx: 370 |  Loss: (0.1099) |  Loss2: (0.0000) | Acc: (96.00%) (45702/47488)
Epoch: 163 | Batch_idx: 380 |  Loss: (0.1102) |  Loss2: (0.0000) | Acc: (96.00%) (46919/48768)
Epoch: 163 | Batch_idx: 390 |  Loss: (0.1102) |  Loss2: (0.0000) | Acc: (96.00%) (48105/50000)
# TEST : Loss: (0.3910) | Acc: (88.00%) (8850/10000)
percent tensor([0.5754, 0.5894, 0.5802, 0.5696, 0.5861, 0.5594, 0.5938, 0.5888, 0.5909,
        0.5868, 0.5853, 0.5875, 0.5822, 0.5898, 0.5772, 0.5711],
       device='cuda:0') torch.Size([16])
percent tensor([0.5526, 0.5451, 0.5232, 0.5484, 0.5424, 0.5358, 0.5502, 0.5638, 0.5422,
        0.5257, 0.5206, 0.5185, 0.5441, 0.5552, 0.5569, 0.5414],
       device='cuda:0') torch.Size([16])
percent tensor([0.5678, 0.5226, 0.4890, 0.5649, 0.4844, 0.5977, 0.5393, 0.5542, 0.5498,
        0.5015, 0.5346, 0.4935, 0.4989, 0.6378, 0.5659, 0.5694],
       device='cuda:0') torch.Size([16])
percent tensor([0.6844, 0.7047, 0.6569, 0.6425, 0.6401, 0.6893, 0.6843, 0.6230, 0.6843,
        0.7068, 0.7012, 0.6867, 0.7359, 0.6804, 0.6891, 0.6968],
       device='cuda:0') torch.Size([16])
percent tensor([0.6901, 0.6059, 0.7264, 0.7266, 0.7470, 0.7748, 0.6959, 0.7001, 0.7606,
        0.6873, 0.7197, 0.7184, 0.6265, 0.7513, 0.6536, 0.7387],
       device='cuda:0') torch.Size([16])
percent tensor([0.8325, 0.8265, 0.8604, 0.8552, 0.8829, 0.8674, 0.8470, 0.8520, 0.8375,
        0.8381, 0.8472, 0.8214, 0.8280, 0.8513, 0.8274, 0.8516],
       device='cuda:0') torch.Size([16])
percent tensor([0.4414, 0.5976, 0.6495, 0.5793, 0.6878, 0.7069, 0.5374, 0.4781, 0.6128,
        0.5690, 0.5767, 0.5177, 0.5123, 0.5333, 0.3781, 0.4309],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9997, 0.9994, 0.9997, 0.9996, 0.9990, 0.9997, 0.9997, 0.9997,
        0.9997, 0.9999, 0.9998, 0.9994, 0.9995, 0.9990, 0.9995],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 164 | Batch_idx: 0 |  Loss: (0.0903) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 164 | Batch_idx: 10 |  Loss: (0.1064) |  Loss2: (0.0000) | Acc: (96.00%) (1352/1408)
Epoch: 164 | Batch_idx: 20 |  Loss: (0.0982) |  Loss2: (0.0000) | Acc: (96.00%) (2594/2688)
Epoch: 164 | Batch_idx: 30 |  Loss: (0.0966) |  Loss2: (0.0000) | Acc: (96.00%) (3828/3968)
Epoch: 164 | Batch_idx: 40 |  Loss: (0.0964) |  Loss2: (0.0000) | Acc: (96.00%) (5063/5248)
Epoch: 164 | Batch_idx: 50 |  Loss: (0.0922) |  Loss2: (0.0000) | Acc: (96.00%) (6315/6528)
Epoch: 164 | Batch_idx: 60 |  Loss: (0.0909) |  Loss2: (0.0000) | Acc: (96.00%) (7561/7808)
Epoch: 164 | Batch_idx: 70 |  Loss: (0.0917) |  Loss2: (0.0000) | Acc: (96.00%) (8796/9088)
Epoch: 164 | Batch_idx: 80 |  Loss: (0.0923) |  Loss2: (0.0000) | Acc: (96.00%) (10037/10368)
Epoch: 164 | Batch_idx: 90 |  Loss: (0.0917) |  Loss2: (0.0000) | Acc: (96.00%) (11278/11648)
Epoch: 164 | Batch_idx: 100 |  Loss: (0.0918) |  Loss2: (0.0000) | Acc: (96.00%) (12517/12928)
Epoch: 164 | Batch_idx: 110 |  Loss: (0.0928) |  Loss2: (0.0000) | Acc: (96.00%) (13752/14208)
Epoch: 164 | Batch_idx: 120 |  Loss: (0.0925) |  Loss2: (0.0000) | Acc: (96.00%) (14990/15488)
Epoch: 164 | Batch_idx: 130 |  Loss: (0.0942) |  Loss2: (0.0000) | Acc: (96.00%) (16211/16768)
Epoch: 164 | Batch_idx: 140 |  Loss: (0.0947) |  Loss2: (0.0000) | Acc: (96.00%) (17449/18048)
Epoch: 164 | Batch_idx: 150 |  Loss: (0.0955) |  Loss2: (0.0000) | Acc: (96.00%) (18682/19328)
Epoch: 164 | Batch_idx: 160 |  Loss: (0.0953) |  Loss2: (0.0000) | Acc: (96.00%) (19922/20608)
Epoch: 164 | Batch_idx: 170 |  Loss: (0.0947) |  Loss2: (0.0000) | Acc: (96.00%) (21168/21888)
Epoch: 164 | Batch_idx: 180 |  Loss: (0.0951) |  Loss2: (0.0000) | Acc: (96.00%) (22405/23168)
Epoch: 164 | Batch_idx: 190 |  Loss: (0.0956) |  Loss2: (0.0000) | Acc: (96.00%) (23642/24448)
Epoch: 164 | Batch_idx: 200 |  Loss: (0.0954) |  Loss2: (0.0000) | Acc: (96.00%) (24880/25728)
Epoch: 164 | Batch_idx: 210 |  Loss: (0.0956) |  Loss2: (0.0000) | Acc: (96.00%) (26114/27008)
Epoch: 164 | Batch_idx: 220 |  Loss: (0.0958) |  Loss2: (0.0000) | Acc: (96.00%) (27351/28288)
Epoch: 164 | Batch_idx: 230 |  Loss: (0.0957) |  Loss2: (0.0000) | Acc: (96.00%) (28587/29568)
Epoch: 164 | Batch_idx: 240 |  Loss: (0.0956) |  Loss2: (0.0000) | Acc: (96.00%) (29824/30848)
Epoch: 164 | Batch_idx: 250 |  Loss: (0.0957) |  Loss2: (0.0000) | Acc: (96.00%) (31061/32128)
Epoch: 164 | Batch_idx: 260 |  Loss: (0.0958) |  Loss2: (0.0000) | Acc: (96.00%) (32301/33408)
Epoch: 164 | Batch_idx: 270 |  Loss: (0.0956) |  Loss2: (0.0000) | Acc: (96.00%) (33536/34688)
Epoch: 164 | Batch_idx: 280 |  Loss: (0.0958) |  Loss2: (0.0000) | Acc: (96.00%) (34774/35968)
Epoch: 164 | Batch_idx: 290 |  Loss: (0.0954) |  Loss2: (0.0000) | Acc: (96.00%) (36013/37248)
Epoch: 164 | Batch_idx: 300 |  Loss: (0.0958) |  Loss2: (0.0000) | Acc: (96.00%) (37242/38528)
Epoch: 164 | Batch_idx: 310 |  Loss: (0.0957) |  Loss2: (0.0000) | Acc: (96.00%) (38486/39808)
Epoch: 164 | Batch_idx: 320 |  Loss: (0.0954) |  Loss2: (0.0000) | Acc: (96.00%) (39725/41088)
Epoch: 164 | Batch_idx: 330 |  Loss: (0.0955) |  Loss2: (0.0000) | Acc: (96.00%) (40961/42368)
Epoch: 164 | Batch_idx: 340 |  Loss: (0.0962) |  Loss2: (0.0000) | Acc: (96.00%) (42185/43648)
Epoch: 164 | Batch_idx: 350 |  Loss: (0.0964) |  Loss2: (0.0000) | Acc: (96.00%) (43421/44928)
Epoch: 164 | Batch_idx: 360 |  Loss: (0.0967) |  Loss2: (0.0000) | Acc: (96.00%) (44655/46208)
Epoch: 164 | Batch_idx: 370 |  Loss: (0.0970) |  Loss2: (0.0000) | Acc: (96.00%) (45895/47488)
Epoch: 164 | Batch_idx: 380 |  Loss: (0.0974) |  Loss2: (0.0000) | Acc: (96.00%) (47124/48768)
Epoch: 164 | Batch_idx: 390 |  Loss: (0.0974) |  Loss2: (0.0000) | Acc: (96.00%) (48315/50000)
# TEST : Loss: (0.4246) | Acc: (87.00%) (8741/10000)
percent tensor([0.5745, 0.5884, 0.5784, 0.5688, 0.5839, 0.5594, 0.5918, 0.5885, 0.5899,
        0.5856, 0.5849, 0.5854, 0.5819, 0.5880, 0.5770, 0.5702],
       device='cuda:0') torch.Size([16])
percent tensor([0.5506, 0.5460, 0.5215, 0.5469, 0.5401, 0.5329, 0.5494, 0.5642, 0.5431,
        0.5255, 0.5213, 0.5167, 0.5440, 0.5548, 0.5552, 0.5409],
       device='cuda:0') torch.Size([16])
percent tensor([0.5652, 0.5127, 0.4963, 0.5713, 0.4861, 0.5972, 0.5346, 0.5528, 0.5497,
        0.4963, 0.5309, 0.4955, 0.4979, 0.6377, 0.5617, 0.5688],
       device='cuda:0') torch.Size([16])
percent tensor([0.6889, 0.7059, 0.6541, 0.6429, 0.6404, 0.6912, 0.6828, 0.6246, 0.6832,
        0.7075, 0.6999, 0.6858, 0.7358, 0.6820, 0.6887, 0.6987],
       device='cuda:0') torch.Size([16])
percent tensor([0.6853, 0.6077, 0.7140, 0.7245, 0.7374, 0.7822, 0.7002, 0.6982, 0.7434,
        0.6843, 0.7168, 0.7139, 0.6243, 0.7548, 0.6611, 0.7388],
       device='cuda:0') torch.Size([16])
percent tensor([0.8254, 0.8202, 0.8575, 0.8448, 0.8786, 0.8628, 0.8442, 0.8481, 0.8303,
        0.8296, 0.8406, 0.8149, 0.8204, 0.8494, 0.8198, 0.8483],
       device='cuda:0') torch.Size([16])
percent tensor([0.4548, 0.6146, 0.6492, 0.5698, 0.6721, 0.7310, 0.5497, 0.4735, 0.5979,
        0.5471, 0.5639, 0.5189, 0.5252, 0.5087, 0.3801, 0.4724],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9997, 0.9994, 0.9998, 0.9995, 0.9994, 0.9997, 0.9995, 0.9996,
        0.9996, 1.0000, 0.9998, 0.9997, 0.9991, 0.9990, 0.9992],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 165 | Batch_idx: 0 |  Loss: (0.0808) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 165 | Batch_idx: 10 |  Loss: (0.1268) |  Loss2: (0.0000) | Acc: (95.00%) (1344/1408)
Epoch: 165 | Batch_idx: 20 |  Loss: (0.1223) |  Loss2: (0.0000) | Acc: (95.00%) (2568/2688)
Epoch: 165 | Batch_idx: 30 |  Loss: (0.1414) |  Loss2: (0.0000) | Acc: (94.00%) (3768/3968)
Epoch: 165 | Batch_idx: 40 |  Loss: (0.1480) |  Loss2: (0.0000) | Acc: (94.00%) (4968/5248)
Epoch: 165 | Batch_idx: 50 |  Loss: (0.1530) |  Loss2: (0.0000) | Acc: (94.00%) (6178/6528)
Epoch: 165 | Batch_idx: 60 |  Loss: (0.1580) |  Loss2: (0.0000) | Acc: (94.00%) (7377/7808)
Epoch: 165 | Batch_idx: 70 |  Loss: (0.1583) |  Loss2: (0.0000) | Acc: (94.00%) (8580/9088)
Epoch: 165 | Batch_idx: 80 |  Loss: (0.1604) |  Loss2: (0.0000) | Acc: (94.00%) (9782/10368)
Epoch: 165 | Batch_idx: 90 |  Loss: (0.1587) |  Loss2: (0.0000) | Acc: (94.00%) (10995/11648)
Epoch: 165 | Batch_idx: 100 |  Loss: (0.1583) |  Loss2: (0.0000) | Acc: (94.00%) (12200/12928)
Epoch: 165 | Batch_idx: 110 |  Loss: (0.1607) |  Loss2: (0.0000) | Acc: (94.00%) (13396/14208)
Epoch: 165 | Batch_idx: 120 |  Loss: (0.1615) |  Loss2: (0.0000) | Acc: (94.00%) (14592/15488)
Epoch: 165 | Batch_idx: 130 |  Loss: (0.1624) |  Loss2: (0.0000) | Acc: (94.00%) (15794/16768)
Epoch: 165 | Batch_idx: 140 |  Loss: (0.1616) |  Loss2: (0.0000) | Acc: (94.00%) (17012/18048)
Epoch: 165 | Batch_idx: 150 |  Loss: (0.1611) |  Loss2: (0.0000) | Acc: (94.00%) (18220/19328)
Epoch: 165 | Batch_idx: 160 |  Loss: (0.1620) |  Loss2: (0.0000) | Acc: (94.00%) (19416/20608)
Epoch: 165 | Batch_idx: 170 |  Loss: (0.1608) |  Loss2: (0.0000) | Acc: (94.00%) (20632/21888)
Epoch: 165 | Batch_idx: 180 |  Loss: (0.1604) |  Loss2: (0.0000) | Acc: (94.00%) (21849/23168)
Epoch: 165 | Batch_idx: 190 |  Loss: (0.1604) |  Loss2: (0.0000) | Acc: (94.00%) (23046/24448)
Epoch: 165 | Batch_idx: 200 |  Loss: (0.1604) |  Loss2: (0.0000) | Acc: (94.00%) (24260/25728)
Epoch: 165 | Batch_idx: 210 |  Loss: (0.1580) |  Loss2: (0.0000) | Acc: (94.00%) (25496/27008)
Epoch: 165 | Batch_idx: 220 |  Loss: (0.1574) |  Loss2: (0.0000) | Acc: (94.00%) (26719/28288)
Epoch: 165 | Batch_idx: 230 |  Loss: (0.1567) |  Loss2: (0.0000) | Acc: (94.00%) (27924/29568)
Epoch: 165 | Batch_idx: 240 |  Loss: (0.1568) |  Loss2: (0.0000) | Acc: (94.00%) (29130/30848)
Epoch: 165 | Batch_idx: 250 |  Loss: (0.1558) |  Loss2: (0.0000) | Acc: (94.00%) (30352/32128)
Epoch: 165 | Batch_idx: 260 |  Loss: (0.1549) |  Loss2: (0.0000) | Acc: (94.00%) (31572/33408)
Epoch: 165 | Batch_idx: 270 |  Loss: (0.1545) |  Loss2: (0.0000) | Acc: (94.00%) (32784/34688)
Epoch: 165 | Batch_idx: 280 |  Loss: (0.1544) |  Loss2: (0.0000) | Acc: (94.00%) (33992/35968)
Epoch: 165 | Batch_idx: 290 |  Loss: (0.1544) |  Loss2: (0.0000) | Acc: (94.00%) (35204/37248)
Epoch: 165 | Batch_idx: 300 |  Loss: (0.1534) |  Loss2: (0.0000) | Acc: (94.00%) (36437/38528)
Epoch: 165 | Batch_idx: 310 |  Loss: (0.1533) |  Loss2: (0.0000) | Acc: (94.00%) (37652/39808)
Epoch: 165 | Batch_idx: 320 |  Loss: (0.1530) |  Loss2: (0.0000) | Acc: (94.00%) (38867/41088)
Epoch: 165 | Batch_idx: 330 |  Loss: (0.1527) |  Loss2: (0.0000) | Acc: (94.00%) (40076/42368)
Epoch: 165 | Batch_idx: 340 |  Loss: (0.1523) |  Loss2: (0.0000) | Acc: (94.00%) (41289/43648)
Epoch: 165 | Batch_idx: 350 |  Loss: (0.1510) |  Loss2: (0.0000) | Acc: (94.00%) (42519/44928)
Epoch: 165 | Batch_idx: 360 |  Loss: (0.1503) |  Loss2: (0.0000) | Acc: (94.00%) (43738/46208)
Epoch: 165 | Batch_idx: 370 |  Loss: (0.1494) |  Loss2: (0.0000) | Acc: (94.00%) (44973/47488)
Epoch: 165 | Batch_idx: 380 |  Loss: (0.1487) |  Loss2: (0.0000) | Acc: (94.00%) (46204/48768)
Epoch: 165 | Batch_idx: 390 |  Loss: (0.1479) |  Loss2: (0.0000) | Acc: (94.00%) (47384/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_165.pth.tar'
# TEST : Loss: (0.4054) | Acc: (87.00%) (8784/10000)
percent tensor([0.5877, 0.6025, 0.5951, 0.5831, 0.6018, 0.5705, 0.6074, 0.6045, 0.6041,
        0.6007, 0.5977, 0.6023, 0.5956, 0.6003, 0.5907, 0.5824],
       device='cuda:0') torch.Size([16])
percent tensor([0.5406, 0.5310, 0.5223, 0.5407, 0.5389, 0.5211, 0.5388, 0.5636, 0.5335,
        0.5136, 0.5058, 0.5103, 0.5326, 0.5405, 0.5425, 0.5278],
       device='cuda:0') torch.Size([16])
percent tensor([0.5536, 0.5151, 0.4918, 0.5627, 0.4745, 0.5866, 0.5291, 0.5459, 0.5439,
        0.4991, 0.5291, 0.4964, 0.4969, 0.6296, 0.5575, 0.5586],
       device='cuda:0') torch.Size([16])
percent tensor([0.6448, 0.6582, 0.6140, 0.6026, 0.6019, 0.6522, 0.6389, 0.5848, 0.6407,
        0.6585, 0.6519, 0.6431, 0.6876, 0.6350, 0.6447, 0.6521],
       device='cuda:0') torch.Size([16])
percent tensor([0.6806, 0.5812, 0.7286, 0.7293, 0.7527, 0.7766, 0.6962, 0.7176, 0.7386,
        0.6662, 0.7059, 0.6981, 0.5788, 0.7503, 0.6476, 0.7258],
       device='cuda:0') torch.Size([16])
percent tensor([0.8400, 0.8321, 0.8728, 0.8621, 0.8921, 0.8778, 0.8589, 0.8633, 0.8441,
        0.8395, 0.8538, 0.8297, 0.8316, 0.8660, 0.8344, 0.8614],
       device='cuda:0') torch.Size([16])
percent tensor([0.4370, 0.5822, 0.6020, 0.5195, 0.6297, 0.7152, 0.5183, 0.4077, 0.5599,
        0.5140, 0.5286, 0.4730, 0.5137, 0.4751, 0.3527, 0.4607],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9997, 0.9993, 0.9996, 0.9994, 0.9993, 0.9997, 0.9995, 0.9997,
        0.9997, 1.0000, 0.9999, 0.9998, 0.9993, 0.9990, 0.9995],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 166 | Batch_idx: 0 |  Loss: (0.1740) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 166 | Batch_idx: 10 |  Loss: (0.1148) |  Loss2: (0.0000) | Acc: (95.00%) (1344/1408)
Epoch: 166 | Batch_idx: 20 |  Loss: (0.1030) |  Loss2: (0.0000) | Acc: (96.00%) (2582/2688)
Epoch: 166 | Batch_idx: 30 |  Loss: (0.0967) |  Loss2: (0.0000) | Acc: (96.00%) (3826/3968)
Epoch: 166 | Batch_idx: 40 |  Loss: (0.0970) |  Loss2: (0.0000) | Acc: (96.00%) (5069/5248)
Epoch: 166 | Batch_idx: 50 |  Loss: (0.0979) |  Loss2: (0.0000) | Acc: (96.00%) (6301/6528)
Epoch: 166 | Batch_idx: 60 |  Loss: (0.0980) |  Loss2: (0.0000) | Acc: (96.00%) (7536/7808)
Epoch: 166 | Batch_idx: 70 |  Loss: (0.0996) |  Loss2: (0.0000) | Acc: (96.00%) (8763/9088)
Epoch: 166 | Batch_idx: 80 |  Loss: (0.0976) |  Loss2: (0.0000) | Acc: (96.00%) (10014/10368)
Epoch: 166 | Batch_idx: 90 |  Loss: (0.0959) |  Loss2: (0.0000) | Acc: (96.00%) (11262/11648)
Epoch: 166 | Batch_idx: 100 |  Loss: (0.0948) |  Loss2: (0.0000) | Acc: (96.00%) (12508/12928)
Epoch: 166 | Batch_idx: 110 |  Loss: (0.0950) |  Loss2: (0.0000) | Acc: (96.00%) (13743/14208)
Epoch: 166 | Batch_idx: 120 |  Loss: (0.0944) |  Loss2: (0.0000) | Acc: (96.00%) (14982/15488)
Epoch: 166 | Batch_idx: 130 |  Loss: (0.0957) |  Loss2: (0.0000) | Acc: (96.00%) (16216/16768)
Epoch: 166 | Batch_idx: 140 |  Loss: (0.0953) |  Loss2: (0.0000) | Acc: (96.00%) (17453/18048)
Epoch: 166 | Batch_idx: 150 |  Loss: (0.0957) |  Loss2: (0.0000) | Acc: (96.00%) (18691/19328)
Epoch: 166 | Batch_idx: 160 |  Loss: (0.0953) |  Loss2: (0.0000) | Acc: (96.00%) (19933/20608)
Epoch: 166 | Batch_idx: 170 |  Loss: (0.0942) |  Loss2: (0.0000) | Acc: (96.00%) (21182/21888)
Epoch: 166 | Batch_idx: 180 |  Loss: (0.0943) |  Loss2: (0.0000) | Acc: (96.00%) (22422/23168)
Epoch: 166 | Batch_idx: 190 |  Loss: (0.0935) |  Loss2: (0.0000) | Acc: (96.00%) (23670/24448)
Epoch: 166 | Batch_idx: 200 |  Loss: (0.0931) |  Loss2: (0.0000) | Acc: (96.00%) (24912/25728)
Epoch: 166 | Batch_idx: 210 |  Loss: (0.0931) |  Loss2: (0.0000) | Acc: (96.00%) (26148/27008)
Epoch: 166 | Batch_idx: 220 |  Loss: (0.0929) |  Loss2: (0.0000) | Acc: (96.00%) (27391/28288)
Epoch: 166 | Batch_idx: 230 |  Loss: (0.0930) |  Loss2: (0.0000) | Acc: (96.00%) (28626/29568)
Epoch: 166 | Batch_idx: 240 |  Loss: (0.0927) |  Loss2: (0.0000) | Acc: (96.00%) (29867/30848)
Epoch: 166 | Batch_idx: 250 |  Loss: (0.0928) |  Loss2: (0.0000) | Acc: (96.00%) (31104/32128)
Epoch: 166 | Batch_idx: 260 |  Loss: (0.0939) |  Loss2: (0.0000) | Acc: (96.00%) (32333/33408)
Epoch: 166 | Batch_idx: 270 |  Loss: (0.0941) |  Loss2: (0.0000) | Acc: (96.00%) (33568/34688)
Epoch: 166 | Batch_idx: 280 |  Loss: (0.0944) |  Loss2: (0.0000) | Acc: (96.00%) (34801/35968)
Epoch: 166 | Batch_idx: 290 |  Loss: (0.0944) |  Loss2: (0.0000) | Acc: (96.00%) (36037/37248)
Epoch: 166 | Batch_idx: 300 |  Loss: (0.0948) |  Loss2: (0.0000) | Acc: (96.00%) (37267/38528)
Epoch: 166 | Batch_idx: 310 |  Loss: (0.0945) |  Loss2: (0.0000) | Acc: (96.00%) (38512/39808)
Epoch: 166 | Batch_idx: 320 |  Loss: (0.0946) |  Loss2: (0.0000) | Acc: (96.00%) (39749/41088)
Epoch: 166 | Batch_idx: 330 |  Loss: (0.0943) |  Loss2: (0.0000) | Acc: (96.00%) (40991/42368)
Epoch: 166 | Batch_idx: 340 |  Loss: (0.0940) |  Loss2: (0.0000) | Acc: (96.00%) (42234/43648)
Epoch: 166 | Batch_idx: 350 |  Loss: (0.0940) |  Loss2: (0.0000) | Acc: (96.00%) (43467/44928)
Epoch: 166 | Batch_idx: 360 |  Loss: (0.0942) |  Loss2: (0.0000) | Acc: (96.00%) (44707/46208)
Epoch: 166 | Batch_idx: 370 |  Loss: (0.0944) |  Loss2: (0.0000) | Acc: (96.00%) (45943/47488)
Epoch: 166 | Batch_idx: 380 |  Loss: (0.0946) |  Loss2: (0.0000) | Acc: (96.00%) (47179/48768)
Epoch: 166 | Batch_idx: 390 |  Loss: (0.0944) |  Loss2: (0.0000) | Acc: (96.00%) (48378/50000)
# TEST : Loss: (0.3867) | Acc: (88.00%) (8880/10000)
percent tensor([0.5868, 0.6026, 0.5959, 0.5817, 0.6026, 0.5703, 0.6075, 0.6044, 0.6052,
        0.6006, 0.5975, 0.6022, 0.5952, 0.6011, 0.5899, 0.5819],
       device='cuda:0') torch.Size([16])
percent tensor([0.5404, 0.5321, 0.5179, 0.5417, 0.5390, 0.5175, 0.5407, 0.5632, 0.5352,
        0.5141, 0.5088, 0.5087, 0.5335, 0.5444, 0.5441, 0.5294],
       device='cuda:0') torch.Size([16])
percent tensor([0.5502, 0.5159, 0.4951, 0.5644, 0.4737, 0.5827, 0.5283, 0.5489, 0.5397,
        0.4973, 0.5296, 0.4975, 0.4891, 0.6378, 0.5569, 0.5569],
       device='cuda:0') torch.Size([16])
percent tensor([0.6403, 0.6602, 0.6158, 0.6022, 0.5997, 0.6514, 0.6414, 0.5846, 0.6320,
        0.6585, 0.6479, 0.6419, 0.6822, 0.6384, 0.6430, 0.6513],
       device='cuda:0') torch.Size([16])
percent tensor([0.6916, 0.5940, 0.7403, 0.7365, 0.7680, 0.7746, 0.7070, 0.7152, 0.7654,
        0.6866, 0.7146, 0.7246, 0.6080, 0.7599, 0.6466, 0.7330],
       device='cuda:0') torch.Size([16])
percent tensor([0.8430, 0.8357, 0.8669, 0.8594, 0.8926, 0.8733, 0.8604, 0.8605, 0.8515,
        0.8482, 0.8599, 0.8318, 0.8334, 0.8664, 0.8365, 0.8651],
       device='cuda:0') torch.Size([16])
percent tensor([0.4282, 0.5702, 0.5996, 0.5184, 0.6563, 0.7092, 0.4903, 0.4162, 0.5686,
        0.5271, 0.5336, 0.4651, 0.5155, 0.4763, 0.3651, 0.4391],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9996, 0.9995, 0.9998, 0.9994, 0.9985, 0.9997, 0.9997, 0.9997,
        0.9996, 0.9999, 0.9997, 0.9996, 0.9991, 0.9988, 0.9993],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 167 | Batch_idx: 0 |  Loss: (0.0409) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 167 | Batch_idx: 10 |  Loss: (0.0945) |  Loss2: (0.0000) | Acc: (96.00%) (1360/1408)
Epoch: 167 | Batch_idx: 20 |  Loss: (0.1073) |  Loss2: (0.0000) | Acc: (95.00%) (2580/2688)
Epoch: 167 | Batch_idx: 30 |  Loss: (0.1082) |  Loss2: (0.0000) | Acc: (95.00%) (3807/3968)
Epoch: 167 | Batch_idx: 40 |  Loss: (0.1169) |  Loss2: (0.0000) | Acc: (95.00%) (5024/5248)
Epoch: 167 | Batch_idx: 50 |  Loss: (0.1155) |  Loss2: (0.0000) | Acc: (95.00%) (6252/6528)
Epoch: 167 | Batch_idx: 60 |  Loss: (0.1169) |  Loss2: (0.0000) | Acc: (95.00%) (7480/7808)
Epoch: 167 | Batch_idx: 70 |  Loss: (0.1188) |  Loss2: (0.0000) | Acc: (95.00%) (8699/9088)
Epoch: 167 | Batch_idx: 80 |  Loss: (0.1190) |  Loss2: (0.0000) | Acc: (95.00%) (9926/10368)
Epoch: 167 | Batch_idx: 90 |  Loss: (0.1188) |  Loss2: (0.0000) | Acc: (95.00%) (11149/11648)
Epoch: 167 | Batch_idx: 100 |  Loss: (0.1175) |  Loss2: (0.0000) | Acc: (95.00%) (12385/12928)
Epoch: 167 | Batch_idx: 110 |  Loss: (0.1204) |  Loss2: (0.0000) | Acc: (95.00%) (13603/14208)
Epoch: 167 | Batch_idx: 120 |  Loss: (0.1193) |  Loss2: (0.0000) | Acc: (95.00%) (14834/15488)
Epoch: 167 | Batch_idx: 130 |  Loss: (0.1175) |  Loss2: (0.0000) | Acc: (95.00%) (16074/16768)
Epoch: 167 | Batch_idx: 140 |  Loss: (0.1181) |  Loss2: (0.0000) | Acc: (95.00%) (17294/18048)
Epoch: 167 | Batch_idx: 150 |  Loss: (0.1179) |  Loss2: (0.0000) | Acc: (95.00%) (18524/19328)
Epoch: 167 | Batch_idx: 160 |  Loss: (0.1175) |  Loss2: (0.0000) | Acc: (95.00%) (19753/20608)
Epoch: 167 | Batch_idx: 170 |  Loss: (0.1170) |  Loss2: (0.0000) | Acc: (95.00%) (20985/21888)
Epoch: 167 | Batch_idx: 180 |  Loss: (0.1171) |  Loss2: (0.0000) | Acc: (95.00%) (22210/23168)
Epoch: 167 | Batch_idx: 190 |  Loss: (0.1174) |  Loss2: (0.0000) | Acc: (95.00%) (23436/24448)
Epoch: 167 | Batch_idx: 200 |  Loss: (0.1168) |  Loss2: (0.0000) | Acc: (95.00%) (24668/25728)
Epoch: 167 | Batch_idx: 210 |  Loss: (0.1167) |  Loss2: (0.0000) | Acc: (95.00%) (25895/27008)
Epoch: 167 | Batch_idx: 220 |  Loss: (0.1152) |  Loss2: (0.0000) | Acc: (95.00%) (27137/28288)
Epoch: 167 | Batch_idx: 230 |  Loss: (0.1152) |  Loss2: (0.0000) | Acc: (95.00%) (28362/29568)
Epoch: 167 | Batch_idx: 240 |  Loss: (0.1153) |  Loss2: (0.0000) | Acc: (95.00%) (29593/30848)
Epoch: 167 | Batch_idx: 250 |  Loss: (0.1145) |  Loss2: (0.0000) | Acc: (95.00%) (30832/32128)
Epoch: 167 | Batch_idx: 260 |  Loss: (0.1142) |  Loss2: (0.0000) | Acc: (95.00%) (32059/33408)
Epoch: 167 | Batch_idx: 270 |  Loss: (0.1139) |  Loss2: (0.0000) | Acc: (95.00%) (33295/34688)
Epoch: 167 | Batch_idx: 280 |  Loss: (0.1135) |  Loss2: (0.0000) | Acc: (95.00%) (34525/35968)
Epoch: 167 | Batch_idx: 290 |  Loss: (0.1129) |  Loss2: (0.0000) | Acc: (96.00%) (35762/37248)
Epoch: 167 | Batch_idx: 300 |  Loss: (0.1122) |  Loss2: (0.0000) | Acc: (96.00%) (37001/38528)
Epoch: 167 | Batch_idx: 310 |  Loss: (0.1119) |  Loss2: (0.0000) | Acc: (96.00%) (38231/39808)
Epoch: 167 | Batch_idx: 320 |  Loss: (0.1112) |  Loss2: (0.0000) | Acc: (96.00%) (39469/41088)
Epoch: 167 | Batch_idx: 330 |  Loss: (0.1109) |  Loss2: (0.0000) | Acc: (96.00%) (40706/42368)
Epoch: 167 | Batch_idx: 340 |  Loss: (0.1105) |  Loss2: (0.0000) | Acc: (96.00%) (41945/43648)
Epoch: 167 | Batch_idx: 350 |  Loss: (0.1102) |  Loss2: (0.0000) | Acc: (96.00%) (43173/44928)
Epoch: 167 | Batch_idx: 360 |  Loss: (0.1094) |  Loss2: (0.0000) | Acc: (96.00%) (44416/46208)
Epoch: 167 | Batch_idx: 370 |  Loss: (0.1089) |  Loss2: (0.0000) | Acc: (96.00%) (45655/47488)
Epoch: 167 | Batch_idx: 380 |  Loss: (0.1079) |  Loss2: (0.0000) | Acc: (96.00%) (46907/48768)
Epoch: 167 | Batch_idx: 390 |  Loss: (0.1072) |  Loss2: (0.0000) | Acc: (96.00%) (48106/50000)
# TEST : Loss: (0.4019) | Acc: (88.00%) (8832/10000)
percent tensor([0.5860, 0.6021, 0.5925, 0.5796, 0.5998, 0.5690, 0.6063, 0.6021, 0.6039,
        0.5990, 0.5968, 0.5997, 0.5942, 0.6008, 0.5891, 0.5810],
       device='cuda:0') torch.Size([16])
percent tensor([0.5312, 0.5237, 0.5068, 0.5292, 0.5268, 0.5064, 0.5318, 0.5544, 0.5245,
        0.5056, 0.4997, 0.4986, 0.5264, 0.5354, 0.5346, 0.5199],
       device='cuda:0') torch.Size([16])
percent tensor([0.5570, 0.5166, 0.4968, 0.5640, 0.4801, 0.5827, 0.5310, 0.5550, 0.5441,
        0.5005, 0.5327, 0.4962, 0.4942, 0.6342, 0.5598, 0.5607],
       device='cuda:0') torch.Size([16])
percent tensor([0.6693, 0.6929, 0.6351, 0.6202, 0.6197, 0.6744, 0.6739, 0.6059, 0.6584,
        0.6908, 0.6793, 0.6678, 0.7161, 0.6649, 0.6718, 0.6808],
       device='cuda:0') torch.Size([16])
percent tensor([0.6895, 0.6092, 0.7243, 0.7215, 0.7443, 0.7680, 0.7053, 0.6890, 0.7613,
        0.6962, 0.7220, 0.7349, 0.6323, 0.7661, 0.6446, 0.7334],
       device='cuda:0') torch.Size([16])
percent tensor([0.8329, 0.8301, 0.8571, 0.8466, 0.8824, 0.8672, 0.8494, 0.8471, 0.8420,
        0.8426, 0.8516, 0.8200, 0.8298, 0.8600, 0.8264, 0.8570],
       device='cuda:0') torch.Size([16])
percent tensor([0.4207, 0.5739, 0.5996, 0.5088, 0.6424, 0.7232, 0.4713, 0.4002, 0.5704,
        0.5208, 0.5382, 0.4487, 0.5211, 0.4605, 0.3459, 0.4306],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9997, 0.9995, 0.9998, 0.9995, 0.9987, 0.9996, 0.9997, 0.9998,
        0.9996, 0.9999, 0.9997, 0.9996, 0.9993, 0.9989, 0.9993],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 168 | Batch_idx: 0 |  Loss: (0.1928) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 168 | Batch_idx: 10 |  Loss: (0.1172) |  Loss2: (0.0000) | Acc: (96.00%) (1352/1408)
Epoch: 168 | Batch_idx: 20 |  Loss: (0.0985) |  Loss2: (0.0000) | Acc: (96.00%) (2595/2688)
Epoch: 168 | Batch_idx: 30 |  Loss: (0.0936) |  Loss2: (0.0000) | Acc: (96.00%) (3840/3968)
Epoch: 168 | Batch_idx: 40 |  Loss: (0.0925) |  Loss2: (0.0000) | Acc: (96.00%) (5083/5248)
Epoch: 168 | Batch_idx: 50 |  Loss: (0.0927) |  Loss2: (0.0000) | Acc: (96.00%) (6319/6528)
Epoch: 168 | Batch_idx: 60 |  Loss: (0.0904) |  Loss2: (0.0000) | Acc: (96.00%) (7568/7808)
Epoch: 168 | Batch_idx: 70 |  Loss: (0.0924) |  Loss2: (0.0000) | Acc: (96.00%) (8796/9088)
Epoch: 168 | Batch_idx: 80 |  Loss: (0.0933) |  Loss2: (0.0000) | Acc: (96.00%) (10035/10368)
Epoch: 168 | Batch_idx: 90 |  Loss: (0.0921) |  Loss2: (0.0000) | Acc: (96.00%) (11281/11648)
Epoch: 168 | Batch_idx: 100 |  Loss: (0.0926) |  Loss2: (0.0000) | Acc: (96.00%) (12513/12928)
Epoch: 168 | Batch_idx: 110 |  Loss: (0.0923) |  Loss2: (0.0000) | Acc: (96.00%) (13752/14208)
Epoch: 168 | Batch_idx: 120 |  Loss: (0.0921) |  Loss2: (0.0000) | Acc: (96.00%) (14996/15488)
Epoch: 168 | Batch_idx: 130 |  Loss: (0.0920) |  Loss2: (0.0000) | Acc: (96.00%) (16231/16768)
Epoch: 168 | Batch_idx: 140 |  Loss: (0.0903) |  Loss2: (0.0000) | Acc: (96.00%) (17484/18048)
Epoch: 168 | Batch_idx: 150 |  Loss: (0.0905) |  Loss2: (0.0000) | Acc: (96.00%) (18726/19328)
Epoch: 168 | Batch_idx: 160 |  Loss: (0.0905) |  Loss2: (0.0000) | Acc: (96.00%) (19970/20608)
Epoch: 168 | Batch_idx: 170 |  Loss: (0.0903) |  Loss2: (0.0000) | Acc: (96.00%) (21216/21888)
Epoch: 168 | Batch_idx: 180 |  Loss: (0.0901) |  Loss2: (0.0000) | Acc: (96.00%) (22461/23168)
Epoch: 168 | Batch_idx: 190 |  Loss: (0.0917) |  Loss2: (0.0000) | Acc: (96.00%) (23684/24448)
Epoch: 168 | Batch_idx: 200 |  Loss: (0.0916) |  Loss2: (0.0000) | Acc: (96.00%) (24922/25728)
Epoch: 168 | Batch_idx: 210 |  Loss: (0.0925) |  Loss2: (0.0000) | Acc: (96.00%) (26152/27008)
Epoch: 168 | Batch_idx: 220 |  Loss: (0.0921) |  Loss2: (0.0000) | Acc: (96.00%) (27396/28288)
Epoch: 168 | Batch_idx: 230 |  Loss: (0.0928) |  Loss2: (0.0000) | Acc: (96.00%) (28627/29568)
Epoch: 168 | Batch_idx: 240 |  Loss: (0.0926) |  Loss2: (0.0000) | Acc: (96.00%) (29867/30848)
Epoch: 168 | Batch_idx: 250 |  Loss: (0.0923) |  Loss2: (0.0000) | Acc: (96.00%) (31109/32128)
Epoch: 168 | Batch_idx: 260 |  Loss: (0.0928) |  Loss2: (0.0000) | Acc: (96.00%) (32338/33408)
Epoch: 168 | Batch_idx: 270 |  Loss: (0.0935) |  Loss2: (0.0000) | Acc: (96.00%) (33572/34688)
Epoch: 168 | Batch_idx: 280 |  Loss: (0.0939) |  Loss2: (0.0000) | Acc: (96.00%) (34800/35968)
Epoch: 168 | Batch_idx: 290 |  Loss: (0.0942) |  Loss2: (0.0000) | Acc: (96.00%) (36035/37248)
Epoch: 168 | Batch_idx: 300 |  Loss: (0.0942) |  Loss2: (0.0000) | Acc: (96.00%) (37276/38528)
Epoch: 168 | Batch_idx: 310 |  Loss: (0.0944) |  Loss2: (0.0000) | Acc: (96.00%) (38508/39808)
Epoch: 168 | Batch_idx: 320 |  Loss: (0.0946) |  Loss2: (0.0000) | Acc: (96.00%) (39745/41088)
Epoch: 168 | Batch_idx: 330 |  Loss: (0.0943) |  Loss2: (0.0000) | Acc: (96.00%) (40983/42368)
Epoch: 168 | Batch_idx: 340 |  Loss: (0.0944) |  Loss2: (0.0000) | Acc: (96.00%) (42220/43648)
Epoch: 168 | Batch_idx: 350 |  Loss: (0.0944) |  Loss2: (0.0000) | Acc: (96.00%) (43463/44928)
Epoch: 168 | Batch_idx: 360 |  Loss: (0.0945) |  Loss2: (0.0000) | Acc: (96.00%) (44692/46208)
Epoch: 168 | Batch_idx: 370 |  Loss: (0.0944) |  Loss2: (0.0000) | Acc: (96.00%) (45935/47488)
Epoch: 168 | Batch_idx: 380 |  Loss: (0.0945) |  Loss2: (0.0000) | Acc: (96.00%) (47169/48768)
Epoch: 168 | Batch_idx: 390 |  Loss: (0.0951) |  Loss2: (0.0000) | Acc: (96.00%) (48346/50000)
# TEST : Loss: (0.4117) | Acc: (87.00%) (8799/10000)
percent tensor([0.5858, 0.6038, 0.5918, 0.5799, 0.5990, 0.5691, 0.6071, 0.6023, 0.6036,
        0.5992, 0.5975, 0.5989, 0.5942, 0.6036, 0.5899, 0.5816],
       device='cuda:0') torch.Size([16])
percent tensor([0.5336, 0.5275, 0.5032, 0.5286, 0.5248, 0.5109, 0.5333, 0.5532, 0.5256,
        0.5056, 0.5017, 0.4966, 0.5279, 0.5408, 0.5368, 0.5229],
       device='cuda:0') torch.Size([16])
percent tensor([0.5584, 0.5165, 0.4835, 0.5619, 0.4727, 0.5889, 0.5281, 0.5507, 0.5369,
        0.4984, 0.5297, 0.4901, 0.4899, 0.6307, 0.5649, 0.5618],
       device='cuda:0') torch.Size([16])
percent tensor([0.6680, 0.6933, 0.6316, 0.6253, 0.6145, 0.6752, 0.6684, 0.6036, 0.6602,
        0.6925, 0.6782, 0.6688, 0.7170, 0.6647, 0.6770, 0.6799],
       device='cuda:0') torch.Size([16])
percent tensor([0.6773, 0.6002, 0.7149, 0.7186, 0.7367, 0.7695, 0.6960, 0.6901, 0.7575,
        0.6877, 0.7184, 0.7218, 0.6218, 0.7581, 0.6509, 0.7261],
       device='cuda:0') torch.Size([16])
percent tensor([0.8315, 0.8251, 0.8575, 0.8411, 0.8802, 0.8634, 0.8458, 0.8454, 0.8400,
        0.8374, 0.8501, 0.8175, 0.8292, 0.8477, 0.8241, 0.8524],
       device='cuda:0') torch.Size([16])
percent tensor([0.4217, 0.5433, 0.6315, 0.5141, 0.6234, 0.6982, 0.4913, 0.4092, 0.5527,
        0.4880, 0.5112, 0.4788, 0.5284, 0.4207, 0.3374, 0.4172],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9997, 0.9996, 0.9999, 0.9996, 0.9991, 0.9996, 0.9997, 0.9996,
        0.9996, 0.9999, 0.9998, 0.9993, 0.9993, 0.9989, 0.9994],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 169 | Batch_idx: 0 |  Loss: (0.0504) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 169 | Batch_idx: 10 |  Loss: (0.1246) |  Loss2: (0.0000) | Acc: (95.00%) (1345/1408)
Epoch: 169 | Batch_idx: 20 |  Loss: (0.1391) |  Loss2: (0.0000) | Acc: (94.00%) (2552/2688)
Epoch: 169 | Batch_idx: 30 |  Loss: (0.1640) |  Loss2: (0.0000) | Acc: (94.00%) (3745/3968)
Epoch: 169 | Batch_idx: 40 |  Loss: (0.1671) |  Loss2: (0.0000) | Acc: (94.00%) (4951/5248)
Epoch: 169 | Batch_idx: 50 |  Loss: (0.1695) |  Loss2: (0.0000) | Acc: (94.00%) (6147/6528)
Epoch: 169 | Batch_idx: 60 |  Loss: (0.1708) |  Loss2: (0.0000) | Acc: (94.00%) (7345/7808)
Epoch: 169 | Batch_idx: 70 |  Loss: (0.1727) |  Loss2: (0.0000) | Acc: (93.00%) (8541/9088)
Epoch: 169 | Batch_idx: 80 |  Loss: (0.1722) |  Loss2: (0.0000) | Acc: (94.00%) (9746/10368)
Epoch: 169 | Batch_idx: 90 |  Loss: (0.1718) |  Loss2: (0.0000) | Acc: (93.00%) (10949/11648)
Epoch: 169 | Batch_idx: 100 |  Loss: (0.1707) |  Loss2: (0.0000) | Acc: (94.00%) (12159/12928)
Epoch: 169 | Batch_idx: 110 |  Loss: (0.1708) |  Loss2: (0.0000) | Acc: (93.00%) (13353/14208)
Epoch: 169 | Batch_idx: 120 |  Loss: (0.1676) |  Loss2: (0.0000) | Acc: (94.00%) (14577/15488)
Epoch: 169 | Batch_idx: 130 |  Loss: (0.1660) |  Loss2: (0.0000) | Acc: (94.00%) (15794/16768)
Epoch: 169 | Batch_idx: 140 |  Loss: (0.1642) |  Loss2: (0.0000) | Acc: (94.00%) (17011/18048)
Epoch: 169 | Batch_idx: 150 |  Loss: (0.1627) |  Loss2: (0.0000) | Acc: (94.00%) (18231/19328)
Epoch: 169 | Batch_idx: 160 |  Loss: (0.1619) |  Loss2: (0.0000) | Acc: (94.00%) (19434/20608)
Epoch: 169 | Batch_idx: 170 |  Loss: (0.1613) |  Loss2: (0.0000) | Acc: (94.00%) (20653/21888)
Epoch: 169 | Batch_idx: 180 |  Loss: (0.1612) |  Loss2: (0.0000) | Acc: (94.00%) (21867/23168)
Epoch: 169 | Batch_idx: 190 |  Loss: (0.1604) |  Loss2: (0.0000) | Acc: (94.00%) (23086/24448)
Epoch: 169 | Batch_idx: 200 |  Loss: (0.1598) |  Loss2: (0.0000) | Acc: (94.00%) (24298/25728)
Epoch: 169 | Batch_idx: 210 |  Loss: (0.1589) |  Loss2: (0.0000) | Acc: (94.00%) (25517/27008)
Epoch: 169 | Batch_idx: 220 |  Loss: (0.1582) |  Loss2: (0.0000) | Acc: (94.00%) (26725/28288)
Epoch: 169 | Batch_idx: 230 |  Loss: (0.1561) |  Loss2: (0.0000) | Acc: (94.00%) (27961/29568)
Epoch: 169 | Batch_idx: 240 |  Loss: (0.1546) |  Loss2: (0.0000) | Acc: (94.00%) (29180/30848)
Epoch: 169 | Batch_idx: 250 |  Loss: (0.1530) |  Loss2: (0.0000) | Acc: (94.00%) (30413/32128)
Epoch: 169 | Batch_idx: 260 |  Loss: (0.1518) |  Loss2: (0.0000) | Acc: (94.00%) (31642/33408)
Epoch: 169 | Batch_idx: 270 |  Loss: (0.1507) |  Loss2: (0.0000) | Acc: (94.00%) (32874/34688)
Epoch: 169 | Batch_idx: 280 |  Loss: (0.1497) |  Loss2: (0.0000) | Acc: (94.00%) (34099/35968)
Epoch: 169 | Batch_idx: 290 |  Loss: (0.1494) |  Loss2: (0.0000) | Acc: (94.00%) (35319/37248)
Epoch: 169 | Batch_idx: 300 |  Loss: (0.1491) |  Loss2: (0.0000) | Acc: (94.00%) (36538/38528)
Epoch: 169 | Batch_idx: 310 |  Loss: (0.1479) |  Loss2: (0.0000) | Acc: (94.00%) (37766/39808)
Epoch: 169 | Batch_idx: 320 |  Loss: (0.1482) |  Loss2: (0.0000) | Acc: (94.00%) (38979/41088)
Epoch: 169 | Batch_idx: 330 |  Loss: (0.1473) |  Loss2: (0.0000) | Acc: (94.00%) (40202/42368)
Epoch: 169 | Batch_idx: 340 |  Loss: (0.1464) |  Loss2: (0.0000) | Acc: (94.00%) (41429/43648)
Epoch: 169 | Batch_idx: 350 |  Loss: (0.1457) |  Loss2: (0.0000) | Acc: (94.00%) (42652/44928)
Epoch: 169 | Batch_idx: 360 |  Loss: (0.1451) |  Loss2: (0.0000) | Acc: (94.00%) (43872/46208)
Epoch: 169 | Batch_idx: 370 |  Loss: (0.1450) |  Loss2: (0.0000) | Acc: (94.00%) (45094/47488)
Epoch: 169 | Batch_idx: 380 |  Loss: (0.1441) |  Loss2: (0.0000) | Acc: (94.00%) (46327/48768)
Epoch: 169 | Batch_idx: 390 |  Loss: (0.1438) |  Loss2: (0.0000) | Acc: (95.00%) (47503/50000)
# TEST : Loss: (0.4353) | Acc: (87.00%) (8747/10000)
percent tensor([0.5858, 0.6035, 0.5933, 0.5816, 0.6008, 0.5694, 0.6073, 0.6028, 0.6037,
        0.5995, 0.5968, 0.6002, 0.5938, 0.6038, 0.5898, 0.5814],
       device='cuda:0') torch.Size([16])
percent tensor([0.5521, 0.5486, 0.5185, 0.5455, 0.5429, 0.5269, 0.5545, 0.5709, 0.5454,
        0.5248, 0.5213, 0.5144, 0.5463, 0.5613, 0.5560, 0.5406],
       device='cuda:0') torch.Size([16])
percent tensor([0.5686, 0.5297, 0.4973, 0.5783, 0.4932, 0.5914, 0.5456, 0.5669, 0.5504,
        0.5149, 0.5426, 0.5094, 0.5013, 0.6357, 0.5774, 0.5713],
       device='cuda:0') torch.Size([16])
percent tensor([0.6425, 0.6640, 0.6064, 0.5982, 0.5909, 0.6411, 0.6408, 0.5820, 0.6346,
        0.6640, 0.6504, 0.6390, 0.6884, 0.6361, 0.6456, 0.6495],
       device='cuda:0') torch.Size([16])
percent tensor([0.6775, 0.5873, 0.7195, 0.7245, 0.7485, 0.7531, 0.7050, 0.7079, 0.7548,
        0.6799, 0.7185, 0.7132, 0.5874, 0.7538, 0.6464, 0.7227],
       device='cuda:0') torch.Size([16])
percent tensor([0.7675, 0.7629, 0.8027, 0.7868, 0.8279, 0.8131, 0.7866, 0.7806, 0.7803,
        0.7737, 0.7892, 0.7567, 0.7663, 0.7878, 0.7580, 0.7933],
       device='cuda:0') torch.Size([16])
percent tensor([0.4664, 0.5627, 0.6354, 0.5040, 0.6235, 0.7488, 0.5010, 0.3767, 0.5837,
        0.5068, 0.5196, 0.4800, 0.5648, 0.4645, 0.3318, 0.4641],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9996, 0.9997, 0.9999, 0.9997, 0.9993, 0.9995, 0.9996, 0.9997,
        0.9997, 0.9999, 0.9998, 0.9994, 0.9993, 0.9988, 0.9995],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 170 | Batch_idx: 0 |  Loss: (0.1310) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 170 | Batch_idx: 10 |  Loss: (0.1172) |  Loss2: (0.0000) | Acc: (95.00%) (1350/1408)
Epoch: 170 | Batch_idx: 20 |  Loss: (0.1074) |  Loss2: (0.0000) | Acc: (96.00%) (2585/2688)
Epoch: 170 | Batch_idx: 30 |  Loss: (0.1059) |  Loss2: (0.0000) | Acc: (96.00%) (3827/3968)
Epoch: 170 | Batch_idx: 40 |  Loss: (0.1009) |  Loss2: (0.0000) | Acc: (96.00%) (5074/5248)
Epoch: 170 | Batch_idx: 50 |  Loss: (0.0984) |  Loss2: (0.0000) | Acc: (96.00%) (6316/6528)
Epoch: 170 | Batch_idx: 60 |  Loss: (0.0982) |  Loss2: (0.0000) | Acc: (96.00%) (7552/7808)
Epoch: 170 | Batch_idx: 70 |  Loss: (0.0990) |  Loss2: (0.0000) | Acc: (96.00%) (8786/9088)
Epoch: 170 | Batch_idx: 80 |  Loss: (0.0956) |  Loss2: (0.0000) | Acc: (96.00%) (10037/10368)
Epoch: 170 | Batch_idx: 90 |  Loss: (0.0962) |  Loss2: (0.0000) | Acc: (96.00%) (11276/11648)
Epoch: 170 | Batch_idx: 100 |  Loss: (0.0964) |  Loss2: (0.0000) | Acc: (96.00%) (12513/12928)
Epoch: 170 | Batch_idx: 110 |  Loss: (0.0954) |  Loss2: (0.0000) | Acc: (96.00%) (13754/14208)
Epoch: 170 | Batch_idx: 120 |  Loss: (0.0952) |  Loss2: (0.0000) | Acc: (96.00%) (14990/15488)
Epoch: 170 | Batch_idx: 130 |  Loss: (0.0939) |  Loss2: (0.0000) | Acc: (96.00%) (16240/16768)
Epoch: 170 | Batch_idx: 140 |  Loss: (0.0950) |  Loss2: (0.0000) | Acc: (96.00%) (17470/18048)
Epoch: 170 | Batch_idx: 150 |  Loss: (0.0944) |  Loss2: (0.0000) | Acc: (96.00%) (18718/19328)
Epoch: 170 | Batch_idx: 160 |  Loss: (0.0941) |  Loss2: (0.0000) | Acc: (96.00%) (19958/20608)
Epoch: 170 | Batch_idx: 170 |  Loss: (0.0944) |  Loss2: (0.0000) | Acc: (96.00%) (21197/21888)
Epoch: 170 | Batch_idx: 180 |  Loss: (0.0947) |  Loss2: (0.0000) | Acc: (96.00%) (22436/23168)
Epoch: 170 | Batch_idx: 190 |  Loss: (0.0947) |  Loss2: (0.0000) | Acc: (96.00%) (23672/24448)
Epoch: 170 | Batch_idx: 200 |  Loss: (0.0949) |  Loss2: (0.0000) | Acc: (96.00%) (24912/25728)
Epoch: 170 | Batch_idx: 210 |  Loss: (0.0951) |  Loss2: (0.0000) | Acc: (96.00%) (26152/27008)
Epoch: 170 | Batch_idx: 220 |  Loss: (0.0966) |  Loss2: (0.0000) | Acc: (96.00%) (27375/28288)
Epoch: 170 | Batch_idx: 230 |  Loss: (0.0966) |  Loss2: (0.0000) | Acc: (96.00%) (28608/29568)
Epoch: 170 | Batch_idx: 240 |  Loss: (0.0965) |  Loss2: (0.0000) | Acc: (96.00%) (29849/30848)
Epoch: 170 | Batch_idx: 250 |  Loss: (0.0958) |  Loss2: (0.0000) | Acc: (96.00%) (31096/32128)
Epoch: 170 | Batch_idx: 260 |  Loss: (0.0960) |  Loss2: (0.0000) | Acc: (96.00%) (32340/33408)
Epoch: 170 | Batch_idx: 270 |  Loss: (0.0954) |  Loss2: (0.0000) | Acc: (96.00%) (33587/34688)
Epoch: 170 | Batch_idx: 280 |  Loss: (0.0951) |  Loss2: (0.0000) | Acc: (96.00%) (34827/35968)
Epoch: 170 | Batch_idx: 290 |  Loss: (0.0956) |  Loss2: (0.0000) | Acc: (96.00%) (36054/37248)
Epoch: 170 | Batch_idx: 300 |  Loss: (0.0957) |  Loss2: (0.0000) | Acc: (96.00%) (37284/38528)
Epoch: 170 | Batch_idx: 310 |  Loss: (0.0953) |  Loss2: (0.0000) | Acc: (96.00%) (38532/39808)
Epoch: 170 | Batch_idx: 320 |  Loss: (0.0950) |  Loss2: (0.0000) | Acc: (96.00%) (39778/41088)
Epoch: 170 | Batch_idx: 330 |  Loss: (0.0946) |  Loss2: (0.0000) | Acc: (96.00%) (41015/42368)
Epoch: 170 | Batch_idx: 340 |  Loss: (0.0947) |  Loss2: (0.0000) | Acc: (96.00%) (42253/43648)
Epoch: 170 | Batch_idx: 350 |  Loss: (0.0946) |  Loss2: (0.0000) | Acc: (96.00%) (43489/44928)
Epoch: 170 | Batch_idx: 360 |  Loss: (0.0946) |  Loss2: (0.0000) | Acc: (96.00%) (44733/46208)
Epoch: 170 | Batch_idx: 370 |  Loss: (0.0943) |  Loss2: (0.0000) | Acc: (96.00%) (45976/47488)
Epoch: 170 | Batch_idx: 380 |  Loss: (0.0947) |  Loss2: (0.0000) | Acc: (96.00%) (47206/48768)
Epoch: 170 | Batch_idx: 390 |  Loss: (0.0948) |  Loss2: (0.0000) | Acc: (96.00%) (48397/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_170.pth.tar'
# TEST : Loss: (0.4071) | Acc: (88.00%) (8824/10000)
percent tensor([0.5851, 0.6029, 0.5909, 0.5803, 0.5978, 0.5691, 0.6056, 0.6014, 0.6028,
        0.5982, 0.5961, 0.5979, 0.5932, 0.6023, 0.5895, 0.5806],
       device='cuda:0') torch.Size([16])
percent tensor([0.5509, 0.5460, 0.5147, 0.5412, 0.5381, 0.5226, 0.5498, 0.5680, 0.5416,
        0.5241, 0.5186, 0.5097, 0.5450, 0.5589, 0.5526, 0.5385],
       device='cuda:0') torch.Size([16])
percent tensor([0.5732, 0.5200, 0.5051, 0.5713, 0.4936, 0.5927, 0.5426, 0.5574, 0.5573,
        0.5118, 0.5441, 0.5073, 0.5021, 0.6364, 0.5711, 0.5710],
       device='cuda:0') torch.Size([16])
percent tensor([0.6433, 0.6606, 0.6193, 0.5994, 0.6013, 0.6417, 0.6438, 0.5820, 0.6376,
        0.6612, 0.6537, 0.6497, 0.6893, 0.6356, 0.6447, 0.6485],
       device='cuda:0') torch.Size([16])
percent tensor([0.6833, 0.6062, 0.7165, 0.7239, 0.7475, 0.7439, 0.7094, 0.7081, 0.7485,
        0.6938, 0.7231, 0.7162, 0.5892, 0.7562, 0.6477, 0.7213],
       device='cuda:0') torch.Size([16])
percent tensor([0.7648, 0.7622, 0.7972, 0.7816, 0.8245, 0.8042, 0.7876, 0.7805, 0.7861,
        0.7747, 0.7880, 0.7541, 0.7626, 0.8006, 0.7542, 0.7932],
       device='cuda:0') torch.Size([16])
percent tensor([0.4234, 0.5533, 0.5901, 0.4972, 0.6237, 0.7053, 0.5209, 0.3676, 0.5642,
        0.5016, 0.5171, 0.4295, 0.5336, 0.5179, 0.3259, 0.4581],
       device='cuda:0') torch.Size([16])
percent tensor([0.9996, 0.9997, 0.9994, 0.9998, 0.9995, 0.9987, 0.9997, 0.9995, 0.9998,
        0.9997, 0.9999, 0.9997, 0.9996, 0.9988, 0.9991, 0.9996],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(181.0983, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(822.6047, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(809.8035, device='cuda:0')
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1500.1825, device='cuda:0')
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(483.4477, device='cuda:0')
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2277.8364, device='cuda:0')
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4286.6763, device='cuda:0')
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1363.6049, device='cuda:0')
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6277.8652, device='cuda:0')
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11618.4922, device='cuda:0')
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3830.1458, device='cuda:0')
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16156.6963, device='cuda:0')
Epoch: 171 | Batch_idx: 0 |  Loss: (0.0878) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 171 | Batch_idx: 10 |  Loss: (0.0872) |  Loss2: (0.0000) | Acc: (97.00%) (1366/1408)
Epoch: 171 | Batch_idx: 20 |  Loss: (0.1036) |  Loss2: (0.0000) | Acc: (96.00%) (2596/2688)
Epoch: 171 | Batch_idx: 30 |  Loss: (0.1076) |  Loss2: (0.0000) | Acc: (96.00%) (3823/3968)
Epoch: 171 | Batch_idx: 40 |  Loss: (0.1094) |  Loss2: (0.0000) | Acc: (96.00%) (5057/5248)
Epoch: 171 | Batch_idx: 50 |  Loss: (0.1101) |  Loss2: (0.0000) | Acc: (96.00%) (6289/6528)
Epoch: 171 | Batch_idx: 60 |  Loss: (0.1157) |  Loss2: (0.0000) | Acc: (96.00%) (7506/7808)
Epoch: 171 | Batch_idx: 70 |  Loss: (0.1151) |  Loss2: (0.0000) | Acc: (96.00%) (8743/9088)
Epoch: 171 | Batch_idx: 80 |  Loss: (0.1154) |  Loss2: (0.0000) | Acc: (96.00%) (9971/10368)
Epoch: 171 | Batch_idx: 90 |  Loss: (0.1166) |  Loss2: (0.0000) | Acc: (96.00%) (11188/11648)
Epoch: 171 | Batch_idx: 100 |  Loss: (0.1164) |  Loss2: (0.0000) | Acc: (96.00%) (12413/12928)
Epoch: 171 | Batch_idx: 110 |  Loss: (0.1175) |  Loss2: (0.0000) | Acc: (95.00%) (13636/14208)
Epoch: 171 | Batch_idx: 120 |  Loss: (0.1187) |  Loss2: (0.0000) | Acc: (95.00%) (14851/15488)
Epoch: 171 | Batch_idx: 130 |  Loss: (0.1192) |  Loss2: (0.0000) | Acc: (95.00%) (16071/16768)
Epoch: 171 | Batch_idx: 140 |  Loss: (0.1179) |  Loss2: (0.0000) | Acc: (95.00%) (17310/18048)
Epoch: 171 | Batch_idx: 150 |  Loss: (0.1166) |  Loss2: (0.0000) | Acc: (95.00%) (18541/19328)
Epoch: 171 | Batch_idx: 160 |  Loss: (0.1166) |  Loss2: (0.0000) | Acc: (95.00%) (19766/20608)
Epoch: 171 | Batch_idx: 170 |  Loss: (0.1163) |  Loss2: (0.0000) | Acc: (95.00%) (20995/21888)
Epoch: 171 | Batch_idx: 180 |  Loss: (0.1156) |  Loss2: (0.0000) | Acc: (95.00%) (22232/23168)
Epoch: 171 | Batch_idx: 190 |  Loss: (0.1153) |  Loss2: (0.0000) | Acc: (95.00%) (23462/24448)
Epoch: 171 | Batch_idx: 200 |  Loss: (0.1137) |  Loss2: (0.0000) | Acc: (96.00%) (24709/25728)
Epoch: 171 | Batch_idx: 210 |  Loss: (0.1133) |  Loss2: (0.0000) | Acc: (96.00%) (25944/27008)
Epoch: 171 | Batch_idx: 220 |  Loss: (0.1125) |  Loss2: (0.0000) | Acc: (96.00%) (27183/28288)
Epoch: 171 | Batch_idx: 230 |  Loss: (0.1122) |  Loss2: (0.0000) | Acc: (96.00%) (28415/29568)
Epoch: 171 | Batch_idx: 240 |  Loss: (0.1117) |  Loss2: (0.0000) | Acc: (96.00%) (29654/30848)
Epoch: 171 | Batch_idx: 250 |  Loss: (0.1114) |  Loss2: (0.0000) | Acc: (96.00%) (30888/32128)
Epoch: 171 | Batch_idx: 260 |  Loss: (0.1109) |  Loss2: (0.0000) | Acc: (96.00%) (32122/33408)
Epoch: 171 | Batch_idx: 270 |  Loss: (0.1108) |  Loss2: (0.0000) | Acc: (96.00%) (33355/34688)
Epoch: 171 | Batch_idx: 280 |  Loss: (0.1106) |  Loss2: (0.0000) | Acc: (96.00%) (34583/35968)
Epoch: 171 | Batch_idx: 290 |  Loss: (0.1110) |  Loss2: (0.0000) | Acc: (96.00%) (35804/37248)
Epoch: 171 | Batch_idx: 300 |  Loss: (0.1109) |  Loss2: (0.0000) | Acc: (96.00%) (37037/38528)
Epoch: 171 | Batch_idx: 310 |  Loss: (0.1109) |  Loss2: (0.0000) | Acc: (96.00%) (38263/39808)
Epoch: 171 | Batch_idx: 320 |  Loss: (0.1108) |  Loss2: (0.0000) | Acc: (96.00%) (39493/41088)
Epoch: 171 | Batch_idx: 330 |  Loss: (0.1104) |  Loss2: (0.0000) | Acc: (96.00%) (40723/42368)
Epoch: 171 | Batch_idx: 340 |  Loss: (0.1106) |  Loss2: (0.0000) | Acc: (96.00%) (41954/43648)
Epoch: 171 | Batch_idx: 350 |  Loss: (0.1102) |  Loss2: (0.0000) | Acc: (96.00%) (43197/44928)
Epoch: 171 | Batch_idx: 360 |  Loss: (0.1096) |  Loss2: (0.0000) | Acc: (96.00%) (44443/46208)
Epoch: 171 | Batch_idx: 370 |  Loss: (0.1094) |  Loss2: (0.0000) | Acc: (96.00%) (45676/47488)
Epoch: 171 | Batch_idx: 380 |  Loss: (0.1093) |  Loss2: (0.0000) | Acc: (96.00%) (46906/48768)
Epoch: 171 | Batch_idx: 390 |  Loss: (0.1095) |  Loss2: (0.0000) | Acc: (96.00%) (48089/50000)
# TEST : Loss: (0.4011) | Acc: (88.00%) (8836/10000)
percent tensor([0.5843, 0.6019, 0.5907, 0.5796, 0.5983, 0.5691, 0.6048, 0.6009, 0.6022,
        0.5974, 0.5950, 0.5976, 0.5923, 0.6012, 0.5884, 0.5799],
       device='cuda:0') torch.Size([16])
percent tensor([0.5532, 0.5467, 0.5142, 0.5429, 0.5386, 0.5263, 0.5509, 0.5697, 0.5425,
        0.5245, 0.5200, 0.5095, 0.5464, 0.5602, 0.5546, 0.5411],
       device='cuda:0') torch.Size([16])
percent tensor([0.5749, 0.5285, 0.4945, 0.5685, 0.4854, 0.5996, 0.5439, 0.5545, 0.5546,
        0.5155, 0.5498, 0.5025, 0.5050, 0.6457, 0.5768, 0.5778],
       device='cuda:0') torch.Size([16])
percent tensor([0.6479, 0.6644, 0.6253, 0.6040, 0.6081, 0.6508, 0.6465, 0.5856, 0.6376,
        0.6652, 0.6565, 0.6504, 0.6934, 0.6351, 0.6500, 0.6550],
       device='cuda:0') torch.Size([16])
percent tensor([0.6867, 0.5983, 0.7171, 0.7345, 0.7514, 0.7625, 0.7112, 0.7084, 0.7638,
        0.6843, 0.7257, 0.7252, 0.5982, 0.7629, 0.6486, 0.7344],
       device='cuda:0') torch.Size([16])
percent tensor([0.7876, 0.7821, 0.8138, 0.7991, 0.8404, 0.8209, 0.8068, 0.8002, 0.8082,
        0.7957, 0.8097, 0.7745, 0.7854, 0.8184, 0.7754, 0.8130],
       device='cuda:0') torch.Size([16])
percent tensor([0.4081, 0.5630, 0.5630, 0.4634, 0.5904, 0.7058, 0.5063, 0.3123, 0.5878,
        0.5143, 0.5321, 0.4305, 0.5604, 0.5231, 0.2979, 0.4140],
       device='cuda:0') torch.Size([16])
percent tensor([0.9996, 0.9997, 0.9995, 0.9998, 0.9995, 0.9991, 0.9997, 0.9996, 0.9998,
        0.9997, 0.9999, 0.9998, 0.9996, 0.9988, 0.9991, 0.9995],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 172 | Batch_idx: 0 |  Loss: (0.1164) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 172 | Batch_idx: 10 |  Loss: (0.0841) |  Loss2: (0.0000) | Acc: (97.00%) (1366/1408)
Epoch: 172 | Batch_idx: 20 |  Loss: (0.0811) |  Loss2: (0.0000) | Acc: (97.00%) (2615/2688)
Epoch: 172 | Batch_idx: 30 |  Loss: (0.0852) |  Loss2: (0.0000) | Acc: (96.00%) (3847/3968)
Epoch: 172 | Batch_idx: 40 |  Loss: (0.0844) |  Loss2: (0.0000) | Acc: (97.00%) (5091/5248)
Epoch: 172 | Batch_idx: 50 |  Loss: (0.0841) |  Loss2: (0.0000) | Acc: (97.00%) (6341/6528)
Epoch: 172 | Batch_idx: 60 |  Loss: (0.0822) |  Loss2: (0.0000) | Acc: (97.00%) (7591/7808)
Epoch: 172 | Batch_idx: 70 |  Loss: (0.0822) |  Loss2: (0.0000) | Acc: (97.00%) (8835/9088)
Epoch: 172 | Batch_idx: 80 |  Loss: (0.0832) |  Loss2: (0.0000) | Acc: (97.00%) (10072/10368)
Epoch: 172 | Batch_idx: 90 |  Loss: (0.0847) |  Loss2: (0.0000) | Acc: (97.00%) (11303/11648)
Epoch: 172 | Batch_idx: 100 |  Loss: (0.0859) |  Loss2: (0.0000) | Acc: (96.00%) (12538/12928)
Epoch: 172 | Batch_idx: 110 |  Loss: (0.0852) |  Loss2: (0.0000) | Acc: (97.00%) (13785/14208)
Epoch: 172 | Batch_idx: 120 |  Loss: (0.0851) |  Loss2: (0.0000) | Acc: (96.00%) (15019/15488)
Epoch: 172 | Batch_idx: 130 |  Loss: (0.0853) |  Loss2: (0.0000) | Acc: (96.00%) (16255/16768)
Epoch: 172 | Batch_idx: 140 |  Loss: (0.0870) |  Loss2: (0.0000) | Acc: (96.00%) (17487/18048)
Epoch: 172 | Batch_idx: 150 |  Loss: (0.0885) |  Loss2: (0.0000) | Acc: (96.00%) (18722/19328)
Epoch: 172 | Batch_idx: 160 |  Loss: (0.0890) |  Loss2: (0.0000) | Acc: (96.00%) (19960/20608)
Epoch: 172 | Batch_idx: 170 |  Loss: (0.0896) |  Loss2: (0.0000) | Acc: (96.00%) (21195/21888)
Epoch: 172 | Batch_idx: 180 |  Loss: (0.0896) |  Loss2: (0.0000) | Acc: (96.00%) (22434/23168)
Epoch: 172 | Batch_idx: 190 |  Loss: (0.0896) |  Loss2: (0.0000) | Acc: (96.00%) (23675/24448)
Epoch: 172 | Batch_idx: 200 |  Loss: (0.0902) |  Loss2: (0.0000) | Acc: (96.00%) (24909/25728)
Epoch: 172 | Batch_idx: 210 |  Loss: (0.0908) |  Loss2: (0.0000) | Acc: (96.00%) (26146/27008)
Epoch: 172 | Batch_idx: 220 |  Loss: (0.0904) |  Loss2: (0.0000) | Acc: (96.00%) (27394/28288)
Epoch: 172 | Batch_idx: 230 |  Loss: (0.0908) |  Loss2: (0.0000) | Acc: (96.00%) (28631/29568)
Epoch: 172 | Batch_idx: 240 |  Loss: (0.0913) |  Loss2: (0.0000) | Acc: (96.00%) (29863/30848)
Epoch: 172 | Batch_idx: 250 |  Loss: (0.0913) |  Loss2: (0.0000) | Acc: (96.00%) (31109/32128)
Epoch: 172 | Batch_idx: 260 |  Loss: (0.0923) |  Loss2: (0.0000) | Acc: (96.00%) (32335/33408)
Epoch: 172 | Batch_idx: 270 |  Loss: (0.0924) |  Loss2: (0.0000) | Acc: (96.00%) (33576/34688)
Epoch: 172 | Batch_idx: 280 |  Loss: (0.0916) |  Loss2: (0.0000) | Acc: (96.00%) (34829/35968)
Epoch: 172 | Batch_idx: 290 |  Loss: (0.0914) |  Loss2: (0.0000) | Acc: (96.00%) (36072/37248)
Epoch: 172 | Batch_idx: 300 |  Loss: (0.0911) |  Loss2: (0.0000) | Acc: (96.00%) (37317/38528)
Epoch: 172 | Batch_idx: 310 |  Loss: (0.0908) |  Loss2: (0.0000) | Acc: (96.00%) (38561/39808)
Epoch: 172 | Batch_idx: 320 |  Loss: (0.0905) |  Loss2: (0.0000) | Acc: (96.00%) (39807/41088)
Epoch: 172 | Batch_idx: 330 |  Loss: (0.0910) |  Loss2: (0.0000) | Acc: (96.00%) (41038/42368)
Epoch: 172 | Batch_idx: 340 |  Loss: (0.0909) |  Loss2: (0.0000) | Acc: (96.00%) (42282/43648)
Epoch: 172 | Batch_idx: 350 |  Loss: (0.0908) |  Loss2: (0.0000) | Acc: (96.00%) (43522/44928)
Epoch: 172 | Batch_idx: 360 |  Loss: (0.0905) |  Loss2: (0.0000) | Acc: (96.00%) (44768/46208)
Epoch: 172 | Batch_idx: 370 |  Loss: (0.0904) |  Loss2: (0.0000) | Acc: (96.00%) (46007/47488)
Epoch: 172 | Batch_idx: 380 |  Loss: (0.0909) |  Loss2: (0.0000) | Acc: (96.00%) (47247/48768)
Epoch: 172 | Batch_idx: 390 |  Loss: (0.0905) |  Loss2: (0.0000) | Acc: (96.00%) (48444/50000)
# TEST : Loss: (0.4131) | Acc: (88.00%) (8826/10000)
percent tensor([0.5845, 0.6030, 0.5908, 0.5802, 0.5972, 0.5686, 0.6059, 0.6023, 0.6030,
        0.5985, 0.5965, 0.5981, 0.5930, 0.6038, 0.5893, 0.5807],
       device='cuda:0') torch.Size([16])
percent tensor([0.5516, 0.5450, 0.5176, 0.5428, 0.5389, 0.5244, 0.5506, 0.5696, 0.5433,
        0.5243, 0.5207, 0.5108, 0.5454, 0.5562, 0.5531, 0.5400],
       device='cuda:0') torch.Size([16])
percent tensor([0.5677, 0.5249, 0.4998, 0.5701, 0.4825, 0.5920, 0.5386, 0.5530, 0.5471,
        0.5122, 0.5441, 0.5047, 0.5003, 0.6409, 0.5704, 0.5689],
       device='cuda:0') torch.Size([16])
percent tensor([0.6465, 0.6692, 0.6137, 0.6001, 0.5966, 0.6454, 0.6479, 0.5893, 0.6363,
        0.6656, 0.6572, 0.6447, 0.6926, 0.6368, 0.6477, 0.6570],
       device='cuda:0') torch.Size([16])
percent tensor([0.6848, 0.5850, 0.7253, 0.7280, 0.7531, 0.7675, 0.7208, 0.6988, 0.7661,
        0.6801, 0.7189, 0.7287, 0.5924, 0.7775, 0.6502, 0.7365],
       device='cuda:0') torch.Size([16])
percent tensor([0.7882, 0.7800, 0.8230, 0.8050, 0.8490, 0.8287, 0.8086, 0.8042, 0.8044,
        0.7949, 0.8059, 0.7762, 0.7865, 0.8183, 0.7749, 0.8124],
       device='cuda:0') torch.Size([16])
percent tensor([0.4084, 0.5699, 0.5650, 0.5061, 0.5953, 0.6702, 0.4659, 0.3616, 0.5713,
        0.4943, 0.5250, 0.4387, 0.5420, 0.4627, 0.3113, 0.3922],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9995, 0.9992, 0.9998, 0.9997, 0.9988, 0.9997, 0.9995, 0.9997,
        0.9996, 0.9999, 0.9997, 0.9994, 0.9991, 0.9992, 0.9995],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 173 | Batch_idx: 0 |  Loss: (0.1434) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 173 | Batch_idx: 10 |  Loss: (0.0958) |  Loss2: (0.0000) | Acc: (96.00%) (1359/1408)
Epoch: 173 | Batch_idx: 20 |  Loss: (0.1034) |  Loss2: (0.0000) | Acc: (96.00%) (2587/2688)
Epoch: 173 | Batch_idx: 30 |  Loss: (0.1054) |  Loss2: (0.0000) | Acc: (96.00%) (3819/3968)
Epoch: 173 | Batch_idx: 40 |  Loss: (0.1153) |  Loss2: (0.0000) | Acc: (95.00%) (5031/5248)
Epoch: 173 | Batch_idx: 50 |  Loss: (0.1171) |  Loss2: (0.0000) | Acc: (95.00%) (6260/6528)
Epoch: 173 | Batch_idx: 60 |  Loss: (0.1138) |  Loss2: (0.0000) | Acc: (96.00%) (7500/7808)
Epoch: 173 | Batch_idx: 70 |  Loss: (0.1115) |  Loss2: (0.0000) | Acc: (96.00%) (8731/9088)
Epoch: 173 | Batch_idx: 80 |  Loss: (0.1128) |  Loss2: (0.0000) | Acc: (96.00%) (9955/10368)
Epoch: 173 | Batch_idx: 90 |  Loss: (0.1110) |  Loss2: (0.0000) | Acc: (96.00%) (11193/11648)
Epoch: 173 | Batch_idx: 100 |  Loss: (0.1130) |  Loss2: (0.0000) | Acc: (95.00%) (12405/12928)
Epoch: 173 | Batch_idx: 110 |  Loss: (0.1144) |  Loss2: (0.0000) | Acc: (95.00%) (13624/14208)
Epoch: 173 | Batch_idx: 120 |  Loss: (0.1131) |  Loss2: (0.0000) | Acc: (95.00%) (14863/15488)
Epoch: 173 | Batch_idx: 130 |  Loss: (0.1126) |  Loss2: (0.0000) | Acc: (95.00%) (16094/16768)
Epoch: 173 | Batch_idx: 140 |  Loss: (0.1118) |  Loss2: (0.0000) | Acc: (96.00%) (17332/18048)
Epoch: 173 | Batch_idx: 150 |  Loss: (0.1112) |  Loss2: (0.0000) | Acc: (96.00%) (18566/19328)
Epoch: 173 | Batch_idx: 160 |  Loss: (0.1116) |  Loss2: (0.0000) | Acc: (96.00%) (19797/20608)
Epoch: 173 | Batch_idx: 170 |  Loss: (0.1106) |  Loss2: (0.0000) | Acc: (96.00%) (21033/21888)
Epoch: 173 | Batch_idx: 180 |  Loss: (0.1105) |  Loss2: (0.0000) | Acc: (96.00%) (22256/23168)
Epoch: 173 | Batch_idx: 190 |  Loss: (0.1099) |  Loss2: (0.0000) | Acc: (96.00%) (23489/24448)
Epoch: 173 | Batch_idx: 200 |  Loss: (0.1101) |  Loss2: (0.0000) | Acc: (96.00%) (24711/25728)
Epoch: 173 | Batch_idx: 210 |  Loss: (0.1095) |  Loss2: (0.0000) | Acc: (96.00%) (25949/27008)
Epoch: 173 | Batch_idx: 220 |  Loss: (0.1087) |  Loss2: (0.0000) | Acc: (96.00%) (27187/28288)
Epoch: 173 | Batch_idx: 230 |  Loss: (0.1093) |  Loss2: (0.0000) | Acc: (96.00%) (28419/29568)
Epoch: 173 | Batch_idx: 240 |  Loss: (0.1083) |  Loss2: (0.0000) | Acc: (96.00%) (29661/30848)
Epoch: 173 | Batch_idx: 250 |  Loss: (0.1086) |  Loss2: (0.0000) | Acc: (96.00%) (30886/32128)
Epoch: 173 | Batch_idx: 260 |  Loss: (0.1093) |  Loss2: (0.0000) | Acc: (96.00%) (32102/33408)
Epoch: 173 | Batch_idx: 270 |  Loss: (0.1090) |  Loss2: (0.0000) | Acc: (96.00%) (33334/34688)
Epoch: 173 | Batch_idx: 280 |  Loss: (0.1083) |  Loss2: (0.0000) | Acc: (96.00%) (34573/35968)
Epoch: 173 | Batch_idx: 290 |  Loss: (0.1079) |  Loss2: (0.0000) | Acc: (96.00%) (35814/37248)
Epoch: 173 | Batch_idx: 300 |  Loss: (0.1070) |  Loss2: (0.0000) | Acc: (96.00%) (37059/38528)
Epoch: 173 | Batch_idx: 310 |  Loss: (0.1068) |  Loss2: (0.0000) | Acc: (96.00%) (38291/39808)
Epoch: 173 | Batch_idx: 320 |  Loss: (0.1068) |  Loss2: (0.0000) | Acc: (96.00%) (39526/41088)
Epoch: 173 | Batch_idx: 330 |  Loss: (0.1068) |  Loss2: (0.0000) | Acc: (96.00%) (40751/42368)
Epoch: 173 | Batch_idx: 340 |  Loss: (0.1061) |  Loss2: (0.0000) | Acc: (96.00%) (41999/43648)
Epoch: 173 | Batch_idx: 350 |  Loss: (0.1058) |  Loss2: (0.0000) | Acc: (96.00%) (43237/44928)
Epoch: 173 | Batch_idx: 360 |  Loss: (0.1062) |  Loss2: (0.0000) | Acc: (96.00%) (44462/46208)
Epoch: 173 | Batch_idx: 370 |  Loss: (0.1055) |  Loss2: (0.0000) | Acc: (96.00%) (45705/47488)
Epoch: 173 | Batch_idx: 380 |  Loss: (0.1057) |  Loss2: (0.0000) | Acc: (96.00%) (46935/48768)
Epoch: 173 | Batch_idx: 390 |  Loss: (0.1055) |  Loss2: (0.0000) | Acc: (96.00%) (48125/50000)
# TEST : Loss: (0.3993) | Acc: (88.00%) (8862/10000)
percent tensor([0.5788, 0.5967, 0.5853, 0.5752, 0.5912, 0.5639, 0.5998, 0.5970, 0.5968,
        0.5927, 0.5905, 0.5928, 0.5871, 0.5978, 0.5839, 0.5755],
       device='cuda:0') torch.Size([16])
percent tensor([0.5390, 0.5333, 0.5008, 0.5282, 0.5237, 0.5131, 0.5372, 0.5576, 0.5287,
        0.5097, 0.5070, 0.4919, 0.5330, 0.5430, 0.5414, 0.5286],
       device='cuda:0') torch.Size([16])
percent tensor([0.5598, 0.5171, 0.4847, 0.5633, 0.4630, 0.5846, 0.5257, 0.5371, 0.5365,
        0.5012, 0.5358, 0.4897, 0.4926, 0.6372, 0.5588, 0.5623],
       device='cuda:0') torch.Size([16])
percent tensor([0.6403, 0.6603, 0.6086, 0.5971, 0.5927, 0.6455, 0.6386, 0.5824, 0.6274,
        0.6552, 0.6462, 0.6343, 0.6860, 0.6259, 0.6415, 0.6512],
       device='cuda:0') torch.Size([16])
percent tensor([0.6925, 0.5830, 0.7269, 0.7213, 0.7570, 0.7862, 0.7148, 0.7089, 0.7589,
        0.6681, 0.7030, 0.7272, 0.6018, 0.7590, 0.6549, 0.7482],
       device='cuda:0') torch.Size([16])
percent tensor([0.8107, 0.8007, 0.8439, 0.8278, 0.8682, 0.8521, 0.8291, 0.8306, 0.8264,
        0.8137, 0.8272, 0.8036, 0.8094, 0.8389, 0.8005, 0.8325],
       device='cuda:0') torch.Size([16])
percent tensor([0.3972, 0.5655, 0.5748, 0.5089, 0.5846, 0.6703, 0.4609, 0.3590, 0.5759,
        0.4913, 0.5365, 0.4669, 0.5448, 0.4653, 0.3168, 0.3675],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9997, 0.9993, 0.9999, 0.9995, 0.9988, 0.9998, 0.9995, 0.9998,
        0.9996, 0.9999, 0.9998, 0.9995, 0.9992, 0.9992, 0.9994],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 174 | Batch_idx: 0 |  Loss: (0.1017) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 174 | Batch_idx: 10 |  Loss: (0.0859) |  Loss2: (0.0000) | Acc: (97.00%) (1370/1408)
Epoch: 174 | Batch_idx: 20 |  Loss: (0.0862) |  Loss2: (0.0000) | Acc: (96.00%) (2607/2688)
Epoch: 174 | Batch_idx: 30 |  Loss: (0.0864) |  Loss2: (0.0000) | Acc: (96.00%) (3847/3968)
Epoch: 174 | Batch_idx: 40 |  Loss: (0.0844) |  Loss2: (0.0000) | Acc: (97.00%) (5098/5248)
Epoch: 174 | Batch_idx: 50 |  Loss: (0.0820) |  Loss2: (0.0000) | Acc: (97.00%) (6352/6528)
Epoch: 174 | Batch_idx: 60 |  Loss: (0.0812) |  Loss2: (0.0000) | Acc: (97.00%) (7605/7808)
Epoch: 174 | Batch_idx: 70 |  Loss: (0.0819) |  Loss2: (0.0000) | Acc: (97.00%) (8849/9088)
Epoch: 174 | Batch_idx: 80 |  Loss: (0.0823) |  Loss2: (0.0000) | Acc: (97.00%) (10094/10368)
Epoch: 174 | Batch_idx: 90 |  Loss: (0.0838) |  Loss2: (0.0000) | Acc: (97.00%) (11332/11648)
Epoch: 174 | Batch_idx: 100 |  Loss: (0.0827) |  Loss2: (0.0000) | Acc: (97.00%) (12579/12928)
Epoch: 174 | Batch_idx: 110 |  Loss: (0.0824) |  Loss2: (0.0000) | Acc: (97.00%) (13822/14208)
Epoch: 174 | Batch_idx: 120 |  Loss: (0.0824) |  Loss2: (0.0000) | Acc: (97.00%) (15061/15488)
Epoch: 174 | Batch_idx: 130 |  Loss: (0.0822) |  Loss2: (0.0000) | Acc: (97.00%) (16306/16768)
Epoch: 174 | Batch_idx: 140 |  Loss: (0.0811) |  Loss2: (0.0000) | Acc: (97.00%) (17559/18048)
Epoch: 174 | Batch_idx: 150 |  Loss: (0.0836) |  Loss2: (0.0000) | Acc: (97.00%) (18790/19328)
Epoch: 174 | Batch_idx: 160 |  Loss: (0.0846) |  Loss2: (0.0000) | Acc: (97.00%) (20031/20608)
Epoch: 174 | Batch_idx: 170 |  Loss: (0.0845) |  Loss2: (0.0000) | Acc: (97.00%) (21273/21888)
Epoch: 174 | Batch_idx: 180 |  Loss: (0.0841) |  Loss2: (0.0000) | Acc: (97.00%) (22521/23168)
Epoch: 174 | Batch_idx: 190 |  Loss: (0.0851) |  Loss2: (0.0000) | Acc: (97.00%) (23753/24448)
Epoch: 174 | Batch_idx: 200 |  Loss: (0.0848) |  Loss2: (0.0000) | Acc: (97.00%) (24996/25728)
Epoch: 174 | Batch_idx: 210 |  Loss: (0.0852) |  Loss2: (0.0000) | Acc: (97.00%) (26238/27008)
Epoch: 174 | Batch_idx: 220 |  Loss: (0.0857) |  Loss2: (0.0000) | Acc: (97.00%) (27471/28288)
Epoch: 174 | Batch_idx: 230 |  Loss: (0.0873) |  Loss2: (0.0000) | Acc: (97.00%) (28699/29568)
Epoch: 174 | Batch_idx: 240 |  Loss: (0.0878) |  Loss2: (0.0000) | Acc: (97.00%) (29928/30848)
Epoch: 174 | Batch_idx: 250 |  Loss: (0.0880) |  Loss2: (0.0000) | Acc: (97.00%) (31165/32128)
Epoch: 174 | Batch_idx: 260 |  Loss: (0.0876) |  Loss2: (0.0000) | Acc: (97.00%) (32413/33408)
Epoch: 174 | Batch_idx: 270 |  Loss: (0.0873) |  Loss2: (0.0000) | Acc: (97.00%) (33661/34688)
Epoch: 174 | Batch_idx: 280 |  Loss: (0.0869) |  Loss2: (0.0000) | Acc: (97.00%) (34911/35968)
Epoch: 174 | Batch_idx: 290 |  Loss: (0.0865) |  Loss2: (0.0000) | Acc: (97.00%) (36156/37248)
Epoch: 174 | Batch_idx: 300 |  Loss: (0.0862) |  Loss2: (0.0000) | Acc: (97.00%) (37404/38528)
Epoch: 174 | Batch_idx: 310 |  Loss: (0.0859) |  Loss2: (0.0000) | Acc: (97.00%) (38652/39808)
Epoch: 174 | Batch_idx: 320 |  Loss: (0.0859) |  Loss2: (0.0000) | Acc: (97.00%) (39896/41088)
Epoch: 174 | Batch_idx: 330 |  Loss: (0.0856) |  Loss2: (0.0000) | Acc: (97.00%) (41143/42368)
Epoch: 174 | Batch_idx: 340 |  Loss: (0.0858) |  Loss2: (0.0000) | Acc: (97.00%) (42387/43648)
Epoch: 174 | Batch_idx: 350 |  Loss: (0.0857) |  Loss2: (0.0000) | Acc: (97.00%) (43629/44928)
Epoch: 174 | Batch_idx: 360 |  Loss: (0.0857) |  Loss2: (0.0000) | Acc: (97.00%) (44872/46208)
Epoch: 174 | Batch_idx: 370 |  Loss: (0.0862) |  Loss2: (0.0000) | Acc: (97.00%) (46108/47488)
Epoch: 174 | Batch_idx: 380 |  Loss: (0.0867) |  Loss2: (0.0000) | Acc: (97.00%) (47339/48768)
Epoch: 174 | Batch_idx: 390 |  Loss: (0.0869) |  Loss2: (0.0000) | Acc: (97.00%) (48528/50000)
# TEST : Loss: (0.3950) | Acc: (88.00%) (8873/10000)
percent tensor([0.5788, 0.5954, 0.5846, 0.5750, 0.5909, 0.5642, 0.5981, 0.5953, 0.5958,
        0.5916, 0.5896, 0.5907, 0.5868, 0.5956, 0.5830, 0.5749],
       device='cuda:0') torch.Size([16])
percent tensor([0.5411, 0.5333, 0.5163, 0.5374, 0.5360, 0.5160, 0.5408, 0.5628, 0.5302,
        0.5141, 0.5047, 0.5048, 0.5339, 0.5440, 0.5436, 0.5292],
       device='cuda:0') torch.Size([16])
percent tensor([0.5598, 0.5147, 0.4981, 0.5621, 0.4798, 0.5918, 0.5268, 0.5485, 0.5388,
        0.5014, 0.5355, 0.4942, 0.4936, 0.6319, 0.5632, 0.5619],
       device='cuda:0') torch.Size([16])
percent tensor([0.6393, 0.6541, 0.6137, 0.5931, 0.5903, 0.6429, 0.6342, 0.5807, 0.6267,
        0.6492, 0.6419, 0.6301, 0.6845, 0.6190, 0.6380, 0.6474],
       device='cuda:0') torch.Size([16])
percent tensor([0.6977, 0.5961, 0.7223, 0.7281, 0.7488, 0.7947, 0.7151, 0.7048, 0.7698,
        0.6713, 0.7276, 0.7131, 0.6033, 0.7669, 0.6606, 0.7502],
       device='cuda:0') torch.Size([16])
percent tensor([0.8107, 0.8097, 0.8491, 0.8295, 0.8664, 0.8447, 0.8334, 0.8267, 0.8269,
        0.8209, 0.8351, 0.8065, 0.8112, 0.8451, 0.8063, 0.8342],
       device='cuda:0') torch.Size([16])
percent tensor([0.3938, 0.5900, 0.5847, 0.4960, 0.5830, 0.6587, 0.4950, 0.3628, 0.5886,
        0.5300, 0.5594, 0.4722, 0.5653, 0.4891, 0.3231, 0.3673],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9997, 0.9992, 0.9998, 0.9997, 0.9987, 0.9998, 0.9997, 0.9995,
        0.9996, 0.9999, 0.9998, 0.9995, 0.9990, 0.9991, 0.9994],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 175 | Batch_idx: 0 |  Loss: (0.0585) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 175 | Batch_idx: 10 |  Loss: (0.0952) |  Loss2: (0.0000) | Acc: (96.00%) (1359/1408)
Epoch: 175 | Batch_idx: 20 |  Loss: (0.1098) |  Loss2: (0.0000) | Acc: (96.00%) (2584/2688)
Epoch: 175 | Batch_idx: 30 |  Loss: (0.1075) |  Loss2: (0.0000) | Acc: (96.00%) (3818/3968)
Epoch: 175 | Batch_idx: 40 |  Loss: (0.1102) |  Loss2: (0.0000) | Acc: (96.00%) (5045/5248)
Epoch: 175 | Batch_idx: 50 |  Loss: (0.1095) |  Loss2: (0.0000) | Acc: (96.00%) (6274/6528)
Epoch: 175 | Batch_idx: 60 |  Loss: (0.1082) |  Loss2: (0.0000) | Acc: (96.00%) (7513/7808)
Epoch: 175 | Batch_idx: 70 |  Loss: (0.1081) |  Loss2: (0.0000) | Acc: (96.00%) (8742/9088)
Epoch: 175 | Batch_idx: 80 |  Loss: (0.1078) |  Loss2: (0.0000) | Acc: (96.00%) (9971/10368)
Epoch: 175 | Batch_idx: 90 |  Loss: (0.1066) |  Loss2: (0.0000) | Acc: (96.00%) (11213/11648)
Epoch: 175 | Batch_idx: 100 |  Loss: (0.1072) |  Loss2: (0.0000) | Acc: (96.00%) (12439/12928)
Epoch: 175 | Batch_idx: 110 |  Loss: (0.1067) |  Loss2: (0.0000) | Acc: (96.00%) (13675/14208)
Epoch: 175 | Batch_idx: 120 |  Loss: (0.1072) |  Loss2: (0.0000) | Acc: (96.00%) (14900/15488)
Epoch: 175 | Batch_idx: 130 |  Loss: (0.1097) |  Loss2: (0.0000) | Acc: (96.00%) (16114/16768)
Epoch: 175 | Batch_idx: 140 |  Loss: (0.1094) |  Loss2: (0.0000) | Acc: (96.00%) (17346/18048)
Epoch: 175 | Batch_idx: 150 |  Loss: (0.1102) |  Loss2: (0.0000) | Acc: (96.00%) (18574/19328)
Epoch: 175 | Batch_idx: 160 |  Loss: (0.1102) |  Loss2: (0.0000) | Acc: (96.00%) (19801/20608)
Epoch: 175 | Batch_idx: 170 |  Loss: (0.1101) |  Loss2: (0.0000) | Acc: (96.00%) (21038/21888)
Epoch: 175 | Batch_idx: 180 |  Loss: (0.1097) |  Loss2: (0.0000) | Acc: (96.00%) (22267/23168)
Epoch: 175 | Batch_idx: 190 |  Loss: (0.1097) |  Loss2: (0.0000) | Acc: (96.00%) (23494/24448)
Epoch: 175 | Batch_idx: 200 |  Loss: (0.1107) |  Loss2: (0.0000) | Acc: (96.00%) (24714/25728)
Epoch: 175 | Batch_idx: 210 |  Loss: (0.1099) |  Loss2: (0.0000) | Acc: (96.00%) (25951/27008)
Epoch: 175 | Batch_idx: 220 |  Loss: (0.1103) |  Loss2: (0.0000) | Acc: (96.00%) (27180/28288)
Epoch: 175 | Batch_idx: 230 |  Loss: (0.1103) |  Loss2: (0.0000) | Acc: (96.00%) (28407/29568)
Epoch: 175 | Batch_idx: 240 |  Loss: (0.1104) |  Loss2: (0.0000) | Acc: (96.00%) (29637/30848)
Epoch: 175 | Batch_idx: 250 |  Loss: (0.1111) |  Loss2: (0.0000) | Acc: (96.00%) (30854/32128)
Epoch: 175 | Batch_idx: 260 |  Loss: (0.1108) |  Loss2: (0.0000) | Acc: (96.00%) (32089/33408)
Epoch: 175 | Batch_idx: 270 |  Loss: (0.1104) |  Loss2: (0.0000) | Acc: (96.00%) (33326/34688)
Epoch: 175 | Batch_idx: 280 |  Loss: (0.1112) |  Loss2: (0.0000) | Acc: (96.00%) (34548/35968)
Epoch: 175 | Batch_idx: 290 |  Loss: (0.1107) |  Loss2: (0.0000) | Acc: (96.00%) (35788/37248)
Epoch: 175 | Batch_idx: 300 |  Loss: (0.1107) |  Loss2: (0.0000) | Acc: (96.00%) (37014/38528)
Epoch: 175 | Batch_idx: 310 |  Loss: (0.1096) |  Loss2: (0.0000) | Acc: (96.00%) (38262/39808)
Epoch: 175 | Batch_idx: 320 |  Loss: (0.1091) |  Loss2: (0.0000) | Acc: (96.00%) (39500/41088)
Epoch: 175 | Batch_idx: 330 |  Loss: (0.1089) |  Loss2: (0.0000) | Acc: (96.00%) (40734/42368)
Epoch: 175 | Batch_idx: 340 |  Loss: (0.1083) |  Loss2: (0.0000) | Acc: (96.00%) (41977/43648)
Epoch: 175 | Batch_idx: 350 |  Loss: (0.1080) |  Loss2: (0.0000) | Acc: (96.00%) (43217/44928)
Epoch: 175 | Batch_idx: 360 |  Loss: (0.1078) |  Loss2: (0.0000) | Acc: (96.00%) (44452/46208)
Epoch: 175 | Batch_idx: 370 |  Loss: (0.1075) |  Loss2: (0.0000) | Acc: (96.00%) (45687/47488)
Epoch: 175 | Batch_idx: 380 |  Loss: (0.1070) |  Loss2: (0.0000) | Acc: (96.00%) (46927/48768)
Epoch: 175 | Batch_idx: 390 |  Loss: (0.1068) |  Loss2: (0.0000) | Acc: (96.00%) (48114/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_175.pth.tar'
# TEST : Loss: (0.3920) | Acc: (89.00%) (8904/10000)
percent tensor([0.5900, 0.6073, 0.5968, 0.5850, 0.6045, 0.5757, 0.6113, 0.6073, 0.6083,
        0.6037, 0.6019, 0.6043, 0.5986, 0.6064, 0.5949, 0.5854],
       device='cuda:0') torch.Size([16])
percent tensor([0.5309, 0.5242, 0.5102, 0.5293, 0.5277, 0.5085, 0.5315, 0.5534, 0.5202,
        0.5060, 0.4949, 0.4984, 0.5244, 0.5353, 0.5343, 0.5196],
       device='cuda:0') torch.Size([16])
percent tensor([0.5753, 0.5326, 0.5154, 0.5796, 0.4937, 0.6053, 0.5403, 0.5619, 0.5535,
        0.5186, 0.5519, 0.5155, 0.5117, 0.6440, 0.5804, 0.5773],
       device='cuda:0') torch.Size([16])
percent tensor([0.6629, 0.6789, 0.6326, 0.6179, 0.6131, 0.6654, 0.6595, 0.6022, 0.6492,
        0.6743, 0.6676, 0.6547, 0.7066, 0.6433, 0.6631, 0.6718],
       device='cuda:0') torch.Size([16])
percent tensor([0.6990, 0.5843, 0.7241, 0.7372, 0.7543, 0.8017, 0.7076, 0.7067, 0.7754,
        0.6745, 0.7311, 0.7138, 0.6044, 0.7662, 0.6519, 0.7593],
       device='cuda:0') torch.Size([16])
percent tensor([0.8140, 0.8111, 0.8516, 0.8320, 0.8693, 0.8473, 0.8340, 0.8321, 0.8318,
        0.8204, 0.8377, 0.8098, 0.8162, 0.8468, 0.8091, 0.8352],
       device='cuda:0') torch.Size([16])
percent tensor([0.4240, 0.5940, 0.6044, 0.5085, 0.5950, 0.6732, 0.4958, 0.3736, 0.6084,
        0.5208, 0.5632, 0.4590, 0.5971, 0.4949, 0.3219, 0.3717],
       device='cuda:0') torch.Size([16])
percent tensor([0.9996, 0.9997, 0.9994, 0.9999, 0.9998, 0.9985, 0.9997, 0.9997, 0.9996,
        0.9996, 0.9999, 0.9999, 0.9996, 0.9990, 0.9991, 0.9994],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 176 | Batch_idx: 0 |  Loss: (0.0856) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 176 | Batch_idx: 10 |  Loss: (0.0916) |  Loss2: (0.0000) | Acc: (96.00%) (1365/1408)
Epoch: 176 | Batch_idx: 20 |  Loss: (0.0859) |  Loss2: (0.0000) | Acc: (97.00%) (2611/2688)
Epoch: 176 | Batch_idx: 30 |  Loss: (0.0855) |  Loss2: (0.0000) | Acc: (97.00%) (3849/3968)
Epoch: 176 | Batch_idx: 40 |  Loss: (0.0814) |  Loss2: (0.0000) | Acc: (97.00%) (5103/5248)
Epoch: 176 | Batch_idx: 50 |  Loss: (0.0851) |  Loss2: (0.0000) | Acc: (97.00%) (6342/6528)
Epoch: 176 | Batch_idx: 60 |  Loss: (0.0832) |  Loss2: (0.0000) | Acc: (97.00%) (7595/7808)
Epoch: 176 | Batch_idx: 70 |  Loss: (0.0826) |  Loss2: (0.0000) | Acc: (97.00%) (8847/9088)
Epoch: 176 | Batch_idx: 80 |  Loss: (0.0834) |  Loss2: (0.0000) | Acc: (97.00%) (10083/10368)
Epoch: 176 | Batch_idx: 90 |  Loss: (0.0820) |  Loss2: (0.0000) | Acc: (97.00%) (11331/11648)
Epoch: 176 | Batch_idx: 100 |  Loss: (0.0838) |  Loss2: (0.0000) | Acc: (97.00%) (12562/12928)
Epoch: 176 | Batch_idx: 110 |  Loss: (0.0833) |  Loss2: (0.0000) | Acc: (97.00%) (13811/14208)
Epoch: 176 | Batch_idx: 120 |  Loss: (0.0834) |  Loss2: (0.0000) | Acc: (97.00%) (15058/15488)
Epoch: 176 | Batch_idx: 130 |  Loss: (0.0825) |  Loss2: (0.0000) | Acc: (97.00%) (16305/16768)
Epoch: 176 | Batch_idx: 140 |  Loss: (0.0815) |  Loss2: (0.0000) | Acc: (97.00%) (17557/18048)
Epoch: 176 | Batch_idx: 150 |  Loss: (0.0811) |  Loss2: (0.0000) | Acc: (97.00%) (18802/19328)
Epoch: 176 | Batch_idx: 160 |  Loss: (0.0812) |  Loss2: (0.0000) | Acc: (97.00%) (20049/20608)
Epoch: 176 | Batch_idx: 170 |  Loss: (0.0817) |  Loss2: (0.0000) | Acc: (97.00%) (21286/21888)
Epoch: 176 | Batch_idx: 180 |  Loss: (0.0823) |  Loss2: (0.0000) | Acc: (97.00%) (22530/23168)
Epoch: 176 | Batch_idx: 190 |  Loss: (0.0823) |  Loss2: (0.0000) | Acc: (97.00%) (23773/24448)
Epoch: 176 | Batch_idx: 200 |  Loss: (0.0811) |  Loss2: (0.0000) | Acc: (97.00%) (25032/25728)
Epoch: 176 | Batch_idx: 210 |  Loss: (0.0807) |  Loss2: (0.0000) | Acc: (97.00%) (26285/27008)
Epoch: 176 | Batch_idx: 220 |  Loss: (0.0804) |  Loss2: (0.0000) | Acc: (97.00%) (27533/28288)
Epoch: 176 | Batch_idx: 230 |  Loss: (0.0805) |  Loss2: (0.0000) | Acc: (97.00%) (28774/29568)
Epoch: 176 | Batch_idx: 240 |  Loss: (0.0810) |  Loss2: (0.0000) | Acc: (97.00%) (30016/30848)
Epoch: 176 | Batch_idx: 250 |  Loss: (0.0813) |  Loss2: (0.0000) | Acc: (97.00%) (31261/32128)
Epoch: 176 | Batch_idx: 260 |  Loss: (0.0812) |  Loss2: (0.0000) | Acc: (97.00%) (32507/33408)
Epoch: 176 | Batch_idx: 270 |  Loss: (0.0824) |  Loss2: (0.0000) | Acc: (97.00%) (33733/34688)
Epoch: 176 | Batch_idx: 280 |  Loss: (0.0826) |  Loss2: (0.0000) | Acc: (97.00%) (34972/35968)
Epoch: 176 | Batch_idx: 290 |  Loss: (0.0824) |  Loss2: (0.0000) | Acc: (97.00%) (36228/37248)
Epoch: 176 | Batch_idx: 300 |  Loss: (0.0826) |  Loss2: (0.0000) | Acc: (97.00%) (37463/38528)
Epoch: 176 | Batch_idx: 310 |  Loss: (0.0828) |  Loss2: (0.0000) | Acc: (97.00%) (38696/39808)
Epoch: 176 | Batch_idx: 320 |  Loss: (0.0834) |  Loss2: (0.0000) | Acc: (97.00%) (39929/41088)
Epoch: 176 | Batch_idx: 330 |  Loss: (0.0833) |  Loss2: (0.0000) | Acc: (97.00%) (41178/42368)
Epoch: 176 | Batch_idx: 340 |  Loss: (0.0838) |  Loss2: (0.0000) | Acc: (97.00%) (42414/43648)
Epoch: 176 | Batch_idx: 350 |  Loss: (0.0844) |  Loss2: (0.0000) | Acc: (97.00%) (43644/44928)
Epoch: 176 | Batch_idx: 360 |  Loss: (0.0847) |  Loss2: (0.0000) | Acc: (97.00%) (44887/46208)
Epoch: 176 | Batch_idx: 370 |  Loss: (0.0846) |  Loss2: (0.0000) | Acc: (97.00%) (46133/47488)
Epoch: 176 | Batch_idx: 380 |  Loss: (0.0846) |  Loss2: (0.0000) | Acc: (97.00%) (47372/48768)
Epoch: 176 | Batch_idx: 390 |  Loss: (0.0850) |  Loss2: (0.0000) | Acc: (97.00%) (48568/50000)
# TEST : Loss: (0.3951) | Acc: (88.00%) (8886/10000)
percent tensor([0.5904, 0.6071, 0.5994, 0.5849, 0.6060, 0.5757, 0.6115, 0.6077, 0.6084,
        0.6035, 0.6018, 0.6054, 0.5985, 0.6060, 0.5942, 0.5851],
       device='cuda:0') torch.Size([16])
percent tensor([0.5354, 0.5265, 0.5035, 0.5290, 0.5271, 0.5142, 0.5328, 0.5491, 0.5230,
        0.5053, 0.5010, 0.4944, 0.5283, 0.5390, 0.5366, 0.5235],
       device='cuda:0') torch.Size([16])
percent tensor([0.5737, 0.5475, 0.4891, 0.5761, 0.4762, 0.6087, 0.5485, 0.5490, 0.5591,
        0.5176, 0.5634, 0.5013, 0.5172, 0.6558, 0.5874, 0.5839],
       device='cuda:0') torch.Size([16])
percent tensor([0.6600, 0.6792, 0.6249, 0.6153, 0.6081, 0.6609, 0.6572, 0.5975, 0.6455,
        0.6779, 0.6678, 0.6538, 0.7048, 0.6442, 0.6626, 0.6723],
       device='cuda:0') torch.Size([16])
percent tensor([0.7078, 0.6060, 0.7340, 0.7449, 0.7508, 0.7879, 0.7108, 0.7169, 0.7686,
        0.6927, 0.7225, 0.7334, 0.6342, 0.7577, 0.6433, 0.7588],
       device='cuda:0') torch.Size([16])
percent tensor([0.8156, 0.8133, 0.8441, 0.8377, 0.8642, 0.8514, 0.8359, 0.8305, 0.8292,
        0.8261, 0.8371, 0.8111, 0.8170, 0.8531, 0.8116, 0.8402],
       device='cuda:0') torch.Size([16])
percent tensor([0.3891, 0.5629, 0.6047, 0.5404, 0.6222, 0.7047, 0.4759, 0.3675, 0.5650,
        0.5277, 0.5416, 0.4723, 0.5511, 0.4799, 0.3188, 0.3881],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9997, 0.9989, 0.9997, 0.9995, 0.9993, 0.9997, 0.9995, 0.9998,
        0.9997, 0.9999, 0.9997, 0.9996, 0.9993, 0.9993, 0.9995],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 177 | Batch_idx: 0 |  Loss: (0.0508) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 177 | Batch_idx: 10 |  Loss: (0.0853) |  Loss2: (0.0000) | Acc: (97.00%) (1368/1408)
Epoch: 177 | Batch_idx: 20 |  Loss: (0.0931) |  Loss2: (0.0000) | Acc: (96.00%) (2603/2688)
Epoch: 177 | Batch_idx: 30 |  Loss: (0.1104) |  Loss2: (0.0000) | Acc: (96.00%) (3811/3968)
Epoch: 177 | Batch_idx: 40 |  Loss: (0.1139) |  Loss2: (0.0000) | Acc: (96.00%) (5043/5248)
Epoch: 177 | Batch_idx: 50 |  Loss: (0.1152) |  Loss2: (0.0000) | Acc: (95.00%) (6265/6528)
Epoch: 177 | Batch_idx: 60 |  Loss: (0.1146) |  Loss2: (0.0000) | Acc: (95.00%) (7494/7808)
Epoch: 177 | Batch_idx: 70 |  Loss: (0.1135) |  Loss2: (0.0000) | Acc: (96.00%) (8729/9088)
Epoch: 177 | Batch_idx: 80 |  Loss: (0.1136) |  Loss2: (0.0000) | Acc: (96.00%) (9954/10368)
Epoch: 177 | Batch_idx: 90 |  Loss: (0.1127) |  Loss2: (0.0000) | Acc: (96.00%) (11185/11648)
Epoch: 177 | Batch_idx: 100 |  Loss: (0.1117) |  Loss2: (0.0000) | Acc: (96.00%) (12418/12928)
Epoch: 177 | Batch_idx: 110 |  Loss: (0.1118) |  Loss2: (0.0000) | Acc: (96.00%) (13648/14208)
Epoch: 177 | Batch_idx: 120 |  Loss: (0.1135) |  Loss2: (0.0000) | Acc: (95.00%) (14868/15488)
Epoch: 177 | Batch_idx: 130 |  Loss: (0.1131) |  Loss2: (0.0000) | Acc: (95.00%) (16092/16768)
Epoch: 177 | Batch_idx: 140 |  Loss: (0.1119) |  Loss2: (0.0000) | Acc: (95.00%) (17323/18048)
Epoch: 177 | Batch_idx: 150 |  Loss: (0.1117) |  Loss2: (0.0000) | Acc: (96.00%) (18555/19328)
Epoch: 177 | Batch_idx: 160 |  Loss: (0.1115) |  Loss2: (0.0000) | Acc: (96.00%) (19788/20608)
Epoch: 177 | Batch_idx: 170 |  Loss: (0.1100) |  Loss2: (0.0000) | Acc: (96.00%) (21034/21888)
Epoch: 177 | Batch_idx: 180 |  Loss: (0.1094) |  Loss2: (0.0000) | Acc: (96.00%) (22268/23168)
Epoch: 177 | Batch_idx: 190 |  Loss: (0.1090) |  Loss2: (0.0000) | Acc: (96.00%) (23501/24448)
Epoch: 177 | Batch_idx: 200 |  Loss: (0.1083) |  Loss2: (0.0000) | Acc: (96.00%) (24747/25728)
Epoch: 177 | Batch_idx: 210 |  Loss: (0.1075) |  Loss2: (0.0000) | Acc: (96.00%) (25993/27008)
Epoch: 177 | Batch_idx: 220 |  Loss: (0.1066) |  Loss2: (0.0000) | Acc: (96.00%) (27228/28288)
Epoch: 177 | Batch_idx: 230 |  Loss: (0.1069) |  Loss2: (0.0000) | Acc: (96.00%) (28453/29568)
Epoch: 177 | Batch_idx: 240 |  Loss: (0.1067) |  Loss2: (0.0000) | Acc: (96.00%) (29690/30848)
Epoch: 177 | Batch_idx: 250 |  Loss: (0.1063) |  Loss2: (0.0000) | Acc: (96.00%) (30931/32128)
Epoch: 177 | Batch_idx: 260 |  Loss: (0.1053) |  Loss2: (0.0000) | Acc: (96.00%) (32175/33408)
Epoch: 177 | Batch_idx: 270 |  Loss: (0.1050) |  Loss2: (0.0000) | Acc: (96.00%) (33417/34688)
Epoch: 177 | Batch_idx: 280 |  Loss: (0.1047) |  Loss2: (0.0000) | Acc: (96.00%) (34656/35968)
Epoch: 177 | Batch_idx: 290 |  Loss: (0.1044) |  Loss2: (0.0000) | Acc: (96.00%) (35896/37248)
Epoch: 177 | Batch_idx: 300 |  Loss: (0.1040) |  Loss2: (0.0000) | Acc: (96.00%) (37136/38528)
Epoch: 177 | Batch_idx: 310 |  Loss: (0.1038) |  Loss2: (0.0000) | Acc: (96.00%) (38369/39808)
Epoch: 177 | Batch_idx: 320 |  Loss: (0.1036) |  Loss2: (0.0000) | Acc: (96.00%) (39604/41088)
Epoch: 177 | Batch_idx: 330 |  Loss: (0.1032) |  Loss2: (0.0000) | Acc: (96.00%) (40846/42368)
Epoch: 177 | Batch_idx: 340 |  Loss: (0.1033) |  Loss2: (0.0000) | Acc: (96.00%) (42083/43648)
Epoch: 177 | Batch_idx: 350 |  Loss: (0.1031) |  Loss2: (0.0000) | Acc: (96.00%) (43324/44928)
Epoch: 177 | Batch_idx: 360 |  Loss: (0.1027) |  Loss2: (0.0000) | Acc: (96.00%) (44570/46208)
Epoch: 177 | Batch_idx: 370 |  Loss: (0.1029) |  Loss2: (0.0000) | Acc: (96.00%) (45802/47488)
Epoch: 177 | Batch_idx: 380 |  Loss: (0.1028) |  Loss2: (0.0000) | Acc: (96.00%) (47043/48768)
Epoch: 177 | Batch_idx: 390 |  Loss: (0.1033) |  Loss2: (0.0000) | Acc: (96.00%) (48219/50000)
# TEST : Loss: (0.4028) | Acc: (88.00%) (8846/10000)
percent tensor([0.5843, 0.5990, 0.5932, 0.5782, 0.5991, 0.5696, 0.6040, 0.6000, 0.6015,
        0.5962, 0.5950, 0.5983, 0.5917, 0.5986, 0.5870, 0.5788],
       device='cuda:0') torch.Size([16])
percent tensor([0.5245, 0.5160, 0.4893, 0.5177, 0.5173, 0.5024, 0.5220, 0.5393, 0.5100,
        0.4919, 0.4876, 0.4786, 0.5165, 0.5281, 0.5271, 0.5126],
       device='cuda:0') torch.Size([16])
percent tensor([0.5556, 0.5265, 0.4825, 0.5706, 0.4634, 0.5970, 0.5243, 0.5371, 0.5439,
        0.5014, 0.5455, 0.4873, 0.4999, 0.6460, 0.5659, 0.5686],
       device='cuda:0') torch.Size([16])
percent tensor([0.6546, 0.6727, 0.6181, 0.6098, 0.6035, 0.6556, 0.6518, 0.5935, 0.6418,
        0.6712, 0.6619, 0.6473, 0.6966, 0.6407, 0.6569, 0.6660],
       device='cuda:0') torch.Size([16])
percent tensor([0.7346, 0.6410, 0.7512, 0.7701, 0.7737, 0.8023, 0.7460, 0.7585, 0.7953,
        0.7132, 0.7518, 0.7486, 0.6544, 0.7887, 0.6837, 0.7853],
       device='cuda:0') torch.Size([16])
percent tensor([0.8240, 0.8222, 0.8517, 0.8477, 0.8721, 0.8604, 0.8442, 0.8398, 0.8388,
        0.8342, 0.8457, 0.8199, 0.8249, 0.8654, 0.8181, 0.8483],
       device='cuda:0') torch.Size([16])
percent tensor([0.3840, 0.5795, 0.6209, 0.5389, 0.6355, 0.7290, 0.4893, 0.3525, 0.5799,
        0.5443, 0.5628, 0.4853, 0.5663, 0.5126, 0.3133, 0.3835],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9998, 0.9990, 0.9997, 0.9994, 0.9991, 0.9996, 0.9995, 0.9999,
        0.9997, 0.9999, 0.9998, 0.9997, 0.9993, 0.9992, 0.9995],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 178 | Batch_idx: 0 |  Loss: (0.1523) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 178 | Batch_idx: 10 |  Loss: (0.0825) |  Loss2: (0.0000) | Acc: (97.00%) (1373/1408)
Epoch: 178 | Batch_idx: 20 |  Loss: (0.0694) |  Loss2: (0.0000) | Acc: (98.00%) (2636/2688)
Epoch: 178 | Batch_idx: 30 |  Loss: (0.0700) |  Loss2: (0.0000) | Acc: (97.00%) (3883/3968)
Epoch: 178 | Batch_idx: 40 |  Loss: (0.0762) |  Loss2: (0.0000) | Acc: (97.00%) (5122/5248)
Epoch: 178 | Batch_idx: 50 |  Loss: (0.0743) |  Loss2: (0.0000) | Acc: (97.00%) (6370/6528)
Epoch: 178 | Batch_idx: 60 |  Loss: (0.0751) |  Loss2: (0.0000) | Acc: (97.00%) (7616/7808)
Epoch: 178 | Batch_idx: 70 |  Loss: (0.0751) |  Loss2: (0.0000) | Acc: (97.00%) (8863/9088)
Epoch: 178 | Batch_idx: 80 |  Loss: (0.0764) |  Loss2: (0.0000) | Acc: (97.00%) (10109/10368)
Epoch: 178 | Batch_idx: 90 |  Loss: (0.0791) |  Loss2: (0.0000) | Acc: (97.00%) (11344/11648)
Epoch: 178 | Batch_idx: 100 |  Loss: (0.0792) |  Loss2: (0.0000) | Acc: (97.00%) (12588/12928)
Epoch: 178 | Batch_idx: 110 |  Loss: (0.0781) |  Loss2: (0.0000) | Acc: (97.00%) (13842/14208)
Epoch: 178 | Batch_idx: 120 |  Loss: (0.0784) |  Loss2: (0.0000) | Acc: (97.00%) (15089/15488)
Epoch: 178 | Batch_idx: 130 |  Loss: (0.0808) |  Loss2: (0.0000) | Acc: (97.00%) (16320/16768)
Epoch: 178 | Batch_idx: 140 |  Loss: (0.0808) |  Loss2: (0.0000) | Acc: (97.00%) (17569/18048)
Epoch: 178 | Batch_idx: 150 |  Loss: (0.0802) |  Loss2: (0.0000) | Acc: (97.00%) (18819/19328)
Epoch: 178 | Batch_idx: 160 |  Loss: (0.0795) |  Loss2: (0.0000) | Acc: (97.00%) (20072/20608)
Epoch: 178 | Batch_idx: 170 |  Loss: (0.0794) |  Loss2: (0.0000) | Acc: (97.00%) (21313/21888)
Epoch: 178 | Batch_idx: 180 |  Loss: (0.0792) |  Loss2: (0.0000) | Acc: (97.00%) (22565/23168)
Epoch: 178 | Batch_idx: 190 |  Loss: (0.0789) |  Loss2: (0.0000) | Acc: (97.00%) (23815/24448)
Epoch: 178 | Batch_idx: 200 |  Loss: (0.0803) |  Loss2: (0.0000) | Acc: (97.00%) (25048/25728)
Epoch: 178 | Batch_idx: 210 |  Loss: (0.0808) |  Loss2: (0.0000) | Acc: (97.00%) (26284/27008)
Epoch: 178 | Batch_idx: 220 |  Loss: (0.0818) |  Loss2: (0.0000) | Acc: (97.00%) (27521/28288)
Epoch: 178 | Batch_idx: 230 |  Loss: (0.0820) |  Loss2: (0.0000) | Acc: (97.00%) (28764/29568)
Epoch: 178 | Batch_idx: 240 |  Loss: (0.0829) |  Loss2: (0.0000) | Acc: (97.00%) (29999/30848)
Epoch: 178 | Batch_idx: 250 |  Loss: (0.0828) |  Loss2: (0.0000) | Acc: (97.00%) (31235/32128)
Epoch: 178 | Batch_idx: 260 |  Loss: (0.0834) |  Loss2: (0.0000) | Acc: (97.00%) (32477/33408)
Epoch: 178 | Batch_idx: 270 |  Loss: (0.0832) |  Loss2: (0.0000) | Acc: (97.00%) (33723/34688)
Epoch: 178 | Batch_idx: 280 |  Loss: (0.0837) |  Loss2: (0.0000) | Acc: (97.00%) (34962/35968)
Epoch: 178 | Batch_idx: 290 |  Loss: (0.0841) |  Loss2: (0.0000) | Acc: (97.00%) (36204/37248)
Epoch: 178 | Batch_idx: 300 |  Loss: (0.0839) |  Loss2: (0.0000) | Acc: (97.00%) (37447/38528)
Epoch: 178 | Batch_idx: 310 |  Loss: (0.0835) |  Loss2: (0.0000) | Acc: (97.00%) (38701/39808)
Epoch: 178 | Batch_idx: 320 |  Loss: (0.0839) |  Loss2: (0.0000) | Acc: (97.00%) (39942/41088)
Epoch: 178 | Batch_idx: 330 |  Loss: (0.0838) |  Loss2: (0.0000) | Acc: (97.00%) (41189/42368)
Epoch: 178 | Batch_idx: 340 |  Loss: (0.0837) |  Loss2: (0.0000) | Acc: (97.00%) (42427/43648)
Epoch: 178 | Batch_idx: 350 |  Loss: (0.0834) |  Loss2: (0.0000) | Acc: (97.00%) (43677/44928)
Epoch: 178 | Batch_idx: 360 |  Loss: (0.0837) |  Loss2: (0.0000) | Acc: (97.00%) (44915/46208)
Epoch: 178 | Batch_idx: 370 |  Loss: (0.0838) |  Loss2: (0.0000) | Acc: (97.00%) (46154/47488)
Epoch: 178 | Batch_idx: 380 |  Loss: (0.0843) |  Loss2: (0.0000) | Acc: (97.00%) (47390/48768)
Epoch: 178 | Batch_idx: 390 |  Loss: (0.0843) |  Loss2: (0.0000) | Acc: (97.00%) (48582/50000)
# TEST : Loss: (0.3869) | Acc: (89.00%) (8913/10000)
percent tensor([0.5836, 0.5980, 0.5915, 0.5779, 0.5978, 0.5678, 0.6024, 0.5996, 0.6012,
        0.5954, 0.5941, 0.5971, 0.5911, 0.5969, 0.5863, 0.5783],
       device='cuda:0') torch.Size([16])
percent tensor([0.5263, 0.5192, 0.4948, 0.5176, 0.5198, 0.5030, 0.5255, 0.5439, 0.5158,
        0.4956, 0.4906, 0.4823, 0.5183, 0.5280, 0.5288, 0.5151],
       device='cuda:0') torch.Size([16])
percent tensor([0.5677, 0.5308, 0.4976, 0.5699, 0.4761, 0.6042, 0.5303, 0.5424, 0.5469,
        0.5127, 0.5512, 0.5054, 0.5037, 0.6434, 0.5711, 0.5750],
       device='cuda:0') torch.Size([16])
percent tensor([0.6475, 0.6651, 0.6220, 0.6026, 0.6068, 0.6474, 0.6468, 0.5913, 0.6343,
        0.6620, 0.6529, 0.6409, 0.6888, 0.6308, 0.6503, 0.6567],
       device='cuda:0') torch.Size([16])
percent tensor([0.7449, 0.6347, 0.7649, 0.7682, 0.7924, 0.8294, 0.7503, 0.7507, 0.8063,
        0.7196, 0.7646, 0.7613, 0.6611, 0.7936, 0.6987, 0.7874],
       device='cuda:0') torch.Size([16])
percent tensor([0.8266, 0.8224, 0.8584, 0.8457, 0.8775, 0.8552, 0.8470, 0.8434, 0.8459,
        0.8371, 0.8521, 0.8187, 0.8230, 0.8611, 0.8156, 0.8475],
       device='cuda:0') torch.Size([16])
percent tensor([0.3890, 0.5860, 0.6272, 0.5309, 0.6354, 0.7051, 0.4861, 0.3877, 0.5888,
        0.5095, 0.5536, 0.4876, 0.5466, 0.4647, 0.3082, 0.3517],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9996, 0.9995, 0.9999, 0.9996, 0.9995, 0.9996, 0.9996, 0.9996,
        0.9996, 0.9999, 0.9998, 0.9996, 0.9990, 0.9991, 0.9994],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 179 | Batch_idx: 0 |  Loss: (0.0822) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 179 | Batch_idx: 10 |  Loss: (0.0721) |  Loss2: (0.0000) | Acc: (97.00%) (1373/1408)
Epoch: 179 | Batch_idx: 20 |  Loss: (0.0877) |  Loss2: (0.0000) | Acc: (96.00%) (2601/2688)
Epoch: 179 | Batch_idx: 30 |  Loss: (0.0981) |  Loss2: (0.0000) | Acc: (96.00%) (3824/3968)
Epoch: 179 | Batch_idx: 40 |  Loss: (0.1024) |  Loss2: (0.0000) | Acc: (96.00%) (5047/5248)
Epoch: 179 | Batch_idx: 50 |  Loss: (0.1011) |  Loss2: (0.0000) | Acc: (96.00%) (6283/6528)
Epoch: 179 | Batch_idx: 60 |  Loss: (0.1010) |  Loss2: (0.0000) | Acc: (96.00%) (7522/7808)
Epoch: 179 | Batch_idx: 70 |  Loss: (0.1008) |  Loss2: (0.0000) | Acc: (96.00%) (8759/9088)
Epoch: 179 | Batch_idx: 80 |  Loss: (0.0993) |  Loss2: (0.0000) | Acc: (96.00%) (10000/10368)
Epoch: 179 | Batch_idx: 90 |  Loss: (0.0998) |  Loss2: (0.0000) | Acc: (96.00%) (11232/11648)
Epoch: 179 | Batch_idx: 100 |  Loss: (0.0983) |  Loss2: (0.0000) | Acc: (96.00%) (12469/12928)
Epoch: 179 | Batch_idx: 110 |  Loss: (0.0995) |  Loss2: (0.0000) | Acc: (96.00%) (13702/14208)
Epoch: 179 | Batch_idx: 120 |  Loss: (0.0984) |  Loss2: (0.0000) | Acc: (96.00%) (14942/15488)
Epoch: 179 | Batch_idx: 130 |  Loss: (0.0972) |  Loss2: (0.0000) | Acc: (96.00%) (16190/16768)
Epoch: 179 | Batch_idx: 140 |  Loss: (0.0963) |  Loss2: (0.0000) | Acc: (96.00%) (17432/18048)
Epoch: 179 | Batch_idx: 150 |  Loss: (0.0963) |  Loss2: (0.0000) | Acc: (96.00%) (18674/19328)
Epoch: 179 | Batch_idx: 160 |  Loss: (0.0966) |  Loss2: (0.0000) | Acc: (96.00%) (19908/20608)
Epoch: 179 | Batch_idx: 170 |  Loss: (0.0955) |  Loss2: (0.0000) | Acc: (96.00%) (21160/21888)
Epoch: 179 | Batch_idx: 180 |  Loss: (0.0953) |  Loss2: (0.0000) | Acc: (96.00%) (22398/23168)
Epoch: 179 | Batch_idx: 190 |  Loss: (0.0958) |  Loss2: (0.0000) | Acc: (96.00%) (23630/24448)
Epoch: 179 | Batch_idx: 200 |  Loss: (0.0943) |  Loss2: (0.0000) | Acc: (96.00%) (24881/25728)
Epoch: 179 | Batch_idx: 210 |  Loss: (0.0937) |  Loss2: (0.0000) | Acc: (96.00%) (26126/27008)
Epoch: 179 | Batch_idx: 220 |  Loss: (0.0931) |  Loss2: (0.0000) | Acc: (96.00%) (27364/28288)
Epoch: 179 | Batch_idx: 230 |  Loss: (0.0925) |  Loss2: (0.0000) | Acc: (96.00%) (28608/29568)
Epoch: 179 | Batch_idx: 240 |  Loss: (0.0921) |  Loss2: (0.0000) | Acc: (96.00%) (29852/30848)
Epoch: 179 | Batch_idx: 250 |  Loss: (0.0918) |  Loss2: (0.0000) | Acc: (96.00%) (31087/32128)
Epoch: 179 | Batch_idx: 260 |  Loss: (0.0912) |  Loss2: (0.0000) | Acc: (96.00%) (32331/33408)
Epoch: 179 | Batch_idx: 270 |  Loss: (0.0915) |  Loss2: (0.0000) | Acc: (96.00%) (33567/34688)
Epoch: 179 | Batch_idx: 280 |  Loss: (0.0908) |  Loss2: (0.0000) | Acc: (96.00%) (34819/35968)
Epoch: 179 | Batch_idx: 290 |  Loss: (0.0907) |  Loss2: (0.0000) | Acc: (96.00%) (36058/37248)
Epoch: 179 | Batch_idx: 300 |  Loss: (0.0907) |  Loss2: (0.0000) | Acc: (96.00%) (37299/38528)
Epoch: 179 | Batch_idx: 310 |  Loss: (0.0900) |  Loss2: (0.0000) | Acc: (96.00%) (38551/39808)
Epoch: 179 | Batch_idx: 320 |  Loss: (0.0899) |  Loss2: (0.0000) | Acc: (96.00%) (39794/41088)
Epoch: 179 | Batch_idx: 330 |  Loss: (0.0896) |  Loss2: (0.0000) | Acc: (96.00%) (41046/42368)
Epoch: 179 | Batch_idx: 340 |  Loss: (0.0899) |  Loss2: (0.0000) | Acc: (96.00%) (42283/43648)
Epoch: 179 | Batch_idx: 350 |  Loss: (0.0898) |  Loss2: (0.0000) | Acc: (96.00%) (43526/44928)
Epoch: 179 | Batch_idx: 360 |  Loss: (0.0894) |  Loss2: (0.0000) | Acc: (96.00%) (44774/46208)
Epoch: 179 | Batch_idx: 370 |  Loss: (0.0893) |  Loss2: (0.0000) | Acc: (96.00%) (46018/47488)
Epoch: 179 | Batch_idx: 380 |  Loss: (0.0887) |  Loss2: (0.0000) | Acc: (96.00%) (47275/48768)
Epoch: 179 | Batch_idx: 390 |  Loss: (0.0889) |  Loss2: (0.0000) | Acc: (96.00%) (48467/50000)
# TEST : Loss: (0.3984) | Acc: (88.00%) (8869/10000)
percent tensor([0.5855, 0.5998, 0.5943, 0.5802, 0.6013, 0.5698, 0.6045, 0.6019, 0.6032,
        0.5973, 0.5958, 0.5995, 0.5933, 0.5983, 0.5881, 0.5801],
       device='cuda:0') torch.Size([16])
percent tensor([0.5431, 0.5342, 0.5132, 0.5360, 0.5376, 0.5208, 0.5410, 0.5625, 0.5324,
        0.5114, 0.5067, 0.4994, 0.5329, 0.5435, 0.5460, 0.5312],
       device='cuda:0') torch.Size([16])
percent tensor([0.5771, 0.5478, 0.5141, 0.5829, 0.4995, 0.6099, 0.5476, 0.5586, 0.5585,
        0.5266, 0.5592, 0.5226, 0.5147, 0.6514, 0.5873, 0.5850],
       device='cuda:0') torch.Size([16])
percent tensor([0.6513, 0.6703, 0.6255, 0.6103, 0.6102, 0.6527, 0.6503, 0.5934, 0.6410,
        0.6682, 0.6600, 0.6463, 0.6943, 0.6401, 0.6539, 0.6626],
       device='cuda:0') torch.Size([16])
percent tensor([0.6911, 0.5888, 0.7142, 0.7253, 0.7477, 0.7951, 0.7040, 0.7047, 0.7711,
        0.6819, 0.7228, 0.7116, 0.6051, 0.7705, 0.6412, 0.7478],
       device='cuda:0') torch.Size([16])
percent tensor([0.8042, 0.8033, 0.8389, 0.8237, 0.8601, 0.8356, 0.8257, 0.8248, 0.8250,
        0.8184, 0.8294, 0.8010, 0.8037, 0.8409, 0.7986, 0.8265],
       device='cuda:0') torch.Size([16])
percent tensor([0.4309, 0.6220, 0.6373, 0.5405, 0.6337, 0.7094, 0.5257, 0.4183, 0.6103,
        0.5454, 0.5795, 0.5291, 0.6012, 0.4926, 0.3394, 0.4010],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9996, 0.9993, 0.9999, 0.9995, 0.9994, 0.9997, 0.9995, 0.9997,
        0.9996, 0.9999, 0.9998, 0.9997, 0.9992, 0.9991, 0.9994],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 180 | Batch_idx: 0 |  Loss: (0.0888) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 180 | Batch_idx: 10 |  Loss: (0.0666) |  Loss2: (0.0000) | Acc: (97.00%) (1375/1408)
Epoch: 180 | Batch_idx: 20 |  Loss: (0.0731) |  Loss2: (0.0000) | Acc: (97.00%) (2624/2688)
Epoch: 180 | Batch_idx: 30 |  Loss: (0.0742) |  Loss2: (0.0000) | Acc: (97.00%) (3874/3968)
Epoch: 180 | Batch_idx: 40 |  Loss: (0.0723) |  Loss2: (0.0000) | Acc: (97.00%) (5125/5248)
Epoch: 180 | Batch_idx: 50 |  Loss: (0.0697) |  Loss2: (0.0000) | Acc: (97.00%) (6384/6528)
Epoch: 180 | Batch_idx: 60 |  Loss: (0.0700) |  Loss2: (0.0000) | Acc: (97.00%) (7628/7808)
Epoch: 180 | Batch_idx: 70 |  Loss: (0.0703) |  Loss2: (0.0000) | Acc: (97.00%) (8879/9088)
Epoch: 180 | Batch_idx: 80 |  Loss: (0.0705) |  Loss2: (0.0000) | Acc: (97.00%) (10134/10368)
Epoch: 180 | Batch_idx: 90 |  Loss: (0.0723) |  Loss2: (0.0000) | Acc: (97.00%) (11378/11648)
Epoch: 180 | Batch_idx: 100 |  Loss: (0.0739) |  Loss2: (0.0000) | Acc: (97.00%) (12619/12928)
Epoch: 180 | Batch_idx: 110 |  Loss: (0.0750) |  Loss2: (0.0000) | Acc: (97.00%) (13864/14208)
Epoch: 180 | Batch_idx: 120 |  Loss: (0.0747) |  Loss2: (0.0000) | Acc: (97.00%) (15117/15488)
Epoch: 180 | Batch_idx: 130 |  Loss: (0.0765) |  Loss2: (0.0000) | Acc: (97.00%) (16358/16768)
Epoch: 180 | Batch_idx: 140 |  Loss: (0.0767) |  Loss2: (0.0000) | Acc: (97.00%) (17603/18048)
Epoch: 180 | Batch_idx: 150 |  Loss: (0.0762) |  Loss2: (0.0000) | Acc: (97.00%) (18855/19328)
Epoch: 180 | Batch_idx: 160 |  Loss: (0.0758) |  Loss2: (0.0000) | Acc: (97.00%) (20100/20608)
Epoch: 180 | Batch_idx: 170 |  Loss: (0.0757) |  Loss2: (0.0000) | Acc: (97.00%) (21351/21888)
Epoch: 180 | Batch_idx: 180 |  Loss: (0.0763) |  Loss2: (0.0000) | Acc: (97.00%) (22594/23168)
Epoch: 180 | Batch_idx: 190 |  Loss: (0.0762) |  Loss2: (0.0000) | Acc: (97.00%) (23840/24448)
Epoch: 180 | Batch_idx: 200 |  Loss: (0.0766) |  Loss2: (0.0000) | Acc: (97.00%) (25080/25728)
Epoch: 180 | Batch_idx: 210 |  Loss: (0.0777) |  Loss2: (0.0000) | Acc: (97.00%) (26314/27008)
Epoch: 180 | Batch_idx: 220 |  Loss: (0.0787) |  Loss2: (0.0000) | Acc: (97.00%) (27547/28288)
Epoch: 180 | Batch_idx: 230 |  Loss: (0.0788) |  Loss2: (0.0000) | Acc: (97.00%) (28792/29568)
Epoch: 180 | Batch_idx: 240 |  Loss: (0.0791) |  Loss2: (0.0000) | Acc: (97.00%) (30036/30848)
Epoch: 180 | Batch_idx: 250 |  Loss: (0.0795) |  Loss2: (0.0000) | Acc: (97.00%) (31280/32128)
Epoch: 180 | Batch_idx: 260 |  Loss: (0.0799) |  Loss2: (0.0000) | Acc: (97.00%) (32525/33408)
Epoch: 180 | Batch_idx: 270 |  Loss: (0.0799) |  Loss2: (0.0000) | Acc: (97.00%) (33775/34688)
Epoch: 180 | Batch_idx: 280 |  Loss: (0.0802) |  Loss2: (0.0000) | Acc: (97.00%) (35015/35968)
Epoch: 180 | Batch_idx: 290 |  Loss: (0.0808) |  Loss2: (0.0000) | Acc: (97.00%) (36253/37248)
Epoch: 180 | Batch_idx: 300 |  Loss: (0.0810) |  Loss2: (0.0000) | Acc: (97.00%) (37495/38528)
Epoch: 180 | Batch_idx: 310 |  Loss: (0.0816) |  Loss2: (0.0000) | Acc: (97.00%) (38728/39808)
Epoch: 180 | Batch_idx: 320 |  Loss: (0.0819) |  Loss2: (0.0000) | Acc: (97.00%) (39970/41088)
Epoch: 180 | Batch_idx: 330 |  Loss: (0.0822) |  Loss2: (0.0000) | Acc: (97.00%) (41209/42368)
Epoch: 180 | Batch_idx: 340 |  Loss: (0.0822) |  Loss2: (0.0000) | Acc: (97.00%) (42457/43648)
Epoch: 180 | Batch_idx: 350 |  Loss: (0.0825) |  Loss2: (0.0000) | Acc: (97.00%) (43695/44928)
Epoch: 180 | Batch_idx: 360 |  Loss: (0.0826) |  Loss2: (0.0000) | Acc: (97.00%) (44934/46208)
Epoch: 180 | Batch_idx: 370 |  Loss: (0.0828) |  Loss2: (0.0000) | Acc: (97.00%) (46169/47488)
Epoch: 180 | Batch_idx: 380 |  Loss: (0.0830) |  Loss2: (0.0000) | Acc: (97.00%) (47410/48768)
Epoch: 180 | Batch_idx: 390 |  Loss: (0.0837) |  Loss2: (0.0000) | Acc: (97.00%) (48598/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_180.pth.tar'
# TEST : Loss: (0.4277) | Acc: (88.00%) (8822/10000)
percent tensor([0.5840, 0.6012, 0.5916, 0.5802, 0.5984, 0.5683, 0.6047, 0.6022, 0.6019,
        0.5972, 0.5951, 0.5973, 0.5923, 0.6018, 0.5885, 0.5802],
       device='cuda:0') torch.Size([16])
percent tensor([0.5411, 0.5340, 0.5114, 0.5394, 0.5356, 0.5164, 0.5399, 0.5642, 0.5330,
        0.5113, 0.5053, 0.5006, 0.5334, 0.5459, 0.5450, 0.5302],
       device='cuda:0') torch.Size([16])
percent tensor([0.5672, 0.5377, 0.4989, 0.5865, 0.4885, 0.6080, 0.5403, 0.5509, 0.5486,
        0.5172, 0.5518, 0.5115, 0.5061, 0.6422, 0.5810, 0.5764],
       device='cuda:0') torch.Size([16])
percent tensor([0.6552, 0.6711, 0.6292, 0.6148, 0.6121, 0.6555, 0.6532, 0.6004, 0.6444,
        0.6739, 0.6633, 0.6518, 0.6976, 0.6375, 0.6565, 0.6646],
       device='cuda:0') torch.Size([16])
percent tensor([0.6876, 0.5873, 0.7037, 0.7191, 0.7348, 0.7735, 0.7069, 0.7084, 0.7696,
        0.6716, 0.7152, 0.7022, 0.5906, 0.7693, 0.6361, 0.7468],
       device='cuda:0') torch.Size([16])
percent tensor([0.8088, 0.8012, 0.8354, 0.8292, 0.8571, 0.8374, 0.8255, 0.8211, 0.8248,
        0.8151, 0.8275, 0.8011, 0.8018, 0.8446, 0.8003, 0.8295],
       device='cuda:0') torch.Size([16])
percent tensor([0.4350, 0.6094, 0.6572, 0.5653, 0.6613, 0.7131, 0.5356, 0.3985, 0.6213,
        0.5611, 0.5747, 0.5194, 0.5911, 0.5088, 0.3277, 0.4298],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9997, 0.9996, 0.9999, 0.9998, 0.9990, 0.9996, 0.9997, 0.9997,
        0.9997, 0.9999, 0.9999, 0.9996, 0.9991, 0.9991, 0.9995],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(181.2317, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(823.2168, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(810.3001, device='cuda:0')
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1498.2430, device='cuda:0')
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(481.7758, device='cuda:0')
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2281.3040, device='cuda:0')
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4284.1797, device='cuda:0')
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1358.4506, device='cuda:0')
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6289.3755, device='cuda:0')
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11583.6885, device='cuda:0')
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3815.4250, device='cuda:0')
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16091.6406, device='cuda:0')
Epoch: 181 | Batch_idx: 0 |  Loss: (0.0667) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 181 | Batch_idx: 10 |  Loss: (0.0801) |  Loss2: (0.0000) | Acc: (97.00%) (1367/1408)
Epoch: 181 | Batch_idx: 20 |  Loss: (0.0910) |  Loss2: (0.0000) | Acc: (96.00%) (2601/2688)
Epoch: 181 | Batch_idx: 30 |  Loss: (0.0952) |  Loss2: (0.0000) | Acc: (96.00%) (3835/3968)
Epoch: 181 | Batch_idx: 40 |  Loss: (0.1000) |  Loss2: (0.0000) | Acc: (96.00%) (5069/5248)
Epoch: 181 | Batch_idx: 50 |  Loss: (0.1058) |  Loss2: (0.0000) | Acc: (96.00%) (6290/6528)
Epoch: 181 | Batch_idx: 60 |  Loss: (0.1110) |  Loss2: (0.0000) | Acc: (96.00%) (7507/7808)
Epoch: 181 | Batch_idx: 70 |  Loss: (0.1081) |  Loss2: (0.0000) | Acc: (96.00%) (8750/9088)
Epoch: 181 | Batch_idx: 80 |  Loss: (0.1092) |  Loss2: (0.0000) | Acc: (96.00%) (9974/10368)
Epoch: 181 | Batch_idx: 90 |  Loss: (0.1082) |  Loss2: (0.0000) | Acc: (96.00%) (11206/11648)
Epoch: 181 | Batch_idx: 100 |  Loss: (0.1085) |  Loss2: (0.0000) | Acc: (96.00%) (12440/12928)
Epoch: 181 | Batch_idx: 110 |  Loss: (0.1079) |  Loss2: (0.0000) | Acc: (96.00%) (13673/14208)
Epoch: 181 | Batch_idx: 120 |  Loss: (0.1075) |  Loss2: (0.0000) | Acc: (96.00%) (14906/15488)
Epoch: 181 | Batch_idx: 130 |  Loss: (0.1060) |  Loss2: (0.0000) | Acc: (96.00%) (16149/16768)
Epoch: 181 | Batch_idx: 140 |  Loss: (0.1064) |  Loss2: (0.0000) | Acc: (96.00%) (17380/18048)
Epoch: 181 | Batch_idx: 150 |  Loss: (0.1068) |  Loss2: (0.0000) | Acc: (96.00%) (18610/19328)
Epoch: 181 | Batch_idx: 160 |  Loss: (0.1060) |  Loss2: (0.0000) | Acc: (96.00%) (19851/20608)
Epoch: 181 | Batch_idx: 170 |  Loss: (0.1057) |  Loss2: (0.0000) | Acc: (96.00%) (21082/21888)
Epoch: 181 | Batch_idx: 180 |  Loss: (0.1051) |  Loss2: (0.0000) | Acc: (96.00%) (22319/23168)
Epoch: 181 | Batch_idx: 190 |  Loss: (0.1047) |  Loss2: (0.0000) | Acc: (96.00%) (23550/24448)
Epoch: 181 | Batch_idx: 200 |  Loss: (0.1043) |  Loss2: (0.0000) | Acc: (96.00%) (24780/25728)
Epoch: 181 | Batch_idx: 210 |  Loss: (0.1043) |  Loss2: (0.0000) | Acc: (96.00%) (26011/27008)
Epoch: 181 | Batch_idx: 220 |  Loss: (0.1042) |  Loss2: (0.0000) | Acc: (96.00%) (27249/28288)
Epoch: 181 | Batch_idx: 230 |  Loss: (0.1035) |  Loss2: (0.0000) | Acc: (96.00%) (28486/29568)
Epoch: 181 | Batch_idx: 240 |  Loss: (0.1031) |  Loss2: (0.0000) | Acc: (96.00%) (29725/30848)
Epoch: 181 | Batch_idx: 250 |  Loss: (0.1033) |  Loss2: (0.0000) | Acc: (96.00%) (30951/32128)
Epoch: 181 | Batch_idx: 260 |  Loss: (0.1029) |  Loss2: (0.0000) | Acc: (96.00%) (32193/33408)
Epoch: 181 | Batch_idx: 270 |  Loss: (0.1021) |  Loss2: (0.0000) | Acc: (96.00%) (33443/34688)
Epoch: 181 | Batch_idx: 280 |  Loss: (0.1016) |  Loss2: (0.0000) | Acc: (96.00%) (34691/35968)
Epoch: 181 | Batch_idx: 290 |  Loss: (0.1009) |  Loss2: (0.0000) | Acc: (96.00%) (35935/37248)
Epoch: 181 | Batch_idx: 300 |  Loss: (0.1010) |  Loss2: (0.0000) | Acc: (96.00%) (37165/38528)
Epoch: 181 | Batch_idx: 310 |  Loss: (0.1013) |  Loss2: (0.0000) | Acc: (96.00%) (38385/39808)
Epoch: 181 | Batch_idx: 320 |  Loss: (0.1012) |  Loss2: (0.0000) | Acc: (96.00%) (39622/41088)
Epoch: 181 | Batch_idx: 330 |  Loss: (0.1012) |  Loss2: (0.0000) | Acc: (96.00%) (40853/42368)
Epoch: 181 | Batch_idx: 340 |  Loss: (0.1007) |  Loss2: (0.0000) | Acc: (96.00%) (42097/43648)
Epoch: 181 | Batch_idx: 350 |  Loss: (0.1011) |  Loss2: (0.0000) | Acc: (96.00%) (43327/44928)
Epoch: 181 | Batch_idx: 360 |  Loss: (0.1010) |  Loss2: (0.0000) | Acc: (96.00%) (44564/46208)
Epoch: 181 | Batch_idx: 370 |  Loss: (0.1009) |  Loss2: (0.0000) | Acc: (96.00%) (45798/47488)
Epoch: 181 | Batch_idx: 380 |  Loss: (0.1005) |  Loss2: (0.0000) | Acc: (96.00%) (47050/48768)
Epoch: 181 | Batch_idx: 390 |  Loss: (0.0999) |  Loss2: (0.0000) | Acc: (96.00%) (48252/50000)
# TEST : Loss: (0.4087) | Acc: (88.00%) (8872/10000)
percent tensor([0.5869, 0.6090, 0.5941, 0.5836, 0.6008, 0.5707, 0.6106, 0.6073, 0.6077,
        0.6025, 0.6007, 0.6004, 0.5965, 0.6119, 0.5934, 0.5848],
       device='cuda:0') torch.Size([16])
percent tensor([0.5428, 0.5357, 0.5157, 0.5418, 0.5366, 0.5177, 0.5429, 0.5679, 0.5355,
        0.5147, 0.5093, 0.5045, 0.5354, 0.5497, 0.5466, 0.5322],
       device='cuda:0') torch.Size([16])
percent tensor([0.5773, 0.5408, 0.5074, 0.5959, 0.4966, 0.6138, 0.5493, 0.5575, 0.5577,
        0.5221, 0.5578, 0.5155, 0.5120, 0.6509, 0.5840, 0.5866],
       device='cuda:0') torch.Size([16])
percent tensor([0.6716, 0.6908, 0.6389, 0.6296, 0.6242, 0.6650, 0.6734, 0.6126, 0.6648,
        0.6907, 0.6850, 0.6690, 0.7168, 0.6591, 0.6736, 0.6800],
       device='cuda:0') torch.Size([16])
percent tensor([0.6747, 0.5853, 0.6915, 0.7063, 0.7179, 0.7541, 0.6910, 0.6893, 0.7473,
        0.6682, 0.7041, 0.7015, 0.5925, 0.7524, 0.6309, 0.7251],
       device='cuda:0') torch.Size([16])
percent tensor([0.7941, 0.7888, 0.8273, 0.8173, 0.8470, 0.8260, 0.8129, 0.8091, 0.8093,
        0.8026, 0.8126, 0.7924, 0.7905, 0.8309, 0.7894, 0.8144],
       device='cuda:0') torch.Size([16])
percent tensor([0.4523, 0.5925, 0.6350, 0.5452, 0.6597, 0.7064, 0.5242, 0.4267, 0.5760,
        0.5553, 0.5408, 0.5100, 0.5653, 0.4801, 0.3384, 0.4722],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9998, 0.9995, 0.9999, 0.9996, 0.9994, 0.9997, 0.9997, 0.9997,
        0.9997, 0.9999, 0.9999, 0.9996, 0.9989, 0.9991, 0.9994],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 182 | Batch_idx: 0 |  Loss: (0.1091) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 182 | Batch_idx: 10 |  Loss: (0.0663) |  Loss2: (0.0000) | Acc: (97.00%) (1377/1408)
Epoch: 182 | Batch_idx: 20 |  Loss: (0.0755) |  Loss2: (0.0000) | Acc: (97.00%) (2618/2688)
Epoch: 182 | Batch_idx: 30 |  Loss: (0.0730) |  Loss2: (0.0000) | Acc: (97.00%) (3874/3968)
Epoch: 182 | Batch_idx: 40 |  Loss: (0.0734) |  Loss2: (0.0000) | Acc: (97.00%) (5124/5248)
Epoch: 182 | Batch_idx: 50 |  Loss: (0.0716) |  Loss2: (0.0000) | Acc: (97.00%) (6381/6528)
Epoch: 182 | Batch_idx: 60 |  Loss: (0.0724) |  Loss2: (0.0000) | Acc: (97.00%) (7629/7808)
Epoch: 182 | Batch_idx: 70 |  Loss: (0.0723) |  Loss2: (0.0000) | Acc: (97.00%) (8880/9088)
Epoch: 182 | Batch_idx: 80 |  Loss: (0.0720) |  Loss2: (0.0000) | Acc: (97.00%) (10132/10368)
Epoch: 182 | Batch_idx: 90 |  Loss: (0.0731) |  Loss2: (0.0000) | Acc: (97.00%) (11377/11648)
Epoch: 182 | Batch_idx: 100 |  Loss: (0.0739) |  Loss2: (0.0000) | Acc: (97.00%) (12624/12928)
Epoch: 182 | Batch_idx: 110 |  Loss: (0.0747) |  Loss2: (0.0000) | Acc: (97.00%) (13864/14208)
Epoch: 182 | Batch_idx: 120 |  Loss: (0.0742) |  Loss2: (0.0000) | Acc: (97.00%) (15115/15488)
Epoch: 182 | Batch_idx: 130 |  Loss: (0.0746) |  Loss2: (0.0000) | Acc: (97.00%) (16363/16768)
Epoch: 182 | Batch_idx: 140 |  Loss: (0.0746) |  Loss2: (0.0000) | Acc: (97.00%) (17615/18048)
Epoch: 182 | Batch_idx: 150 |  Loss: (0.0747) |  Loss2: (0.0000) | Acc: (97.00%) (18860/19328)
Epoch: 182 | Batch_idx: 160 |  Loss: (0.0753) |  Loss2: (0.0000) | Acc: (97.00%) (20105/20608)
Epoch: 182 | Batch_idx: 170 |  Loss: (0.0746) |  Loss2: (0.0000) | Acc: (97.00%) (21359/21888)
Epoch: 182 | Batch_idx: 180 |  Loss: (0.0744) |  Loss2: (0.0000) | Acc: (97.00%) (22611/23168)
Epoch: 182 | Batch_idx: 190 |  Loss: (0.0751) |  Loss2: (0.0000) | Acc: (97.00%) (23856/24448)
Epoch: 182 | Batch_idx: 200 |  Loss: (0.0750) |  Loss2: (0.0000) | Acc: (97.00%) (25102/25728)
Epoch: 182 | Batch_idx: 210 |  Loss: (0.0755) |  Loss2: (0.0000) | Acc: (97.00%) (26343/27008)
Epoch: 182 | Batch_idx: 220 |  Loss: (0.0754) |  Loss2: (0.0000) | Acc: (97.00%) (27592/28288)
Epoch: 182 | Batch_idx: 230 |  Loss: (0.0760) |  Loss2: (0.0000) | Acc: (97.00%) (28835/29568)
Epoch: 182 | Batch_idx: 240 |  Loss: (0.0769) |  Loss2: (0.0000) | Acc: (97.00%) (30069/30848)
Epoch: 182 | Batch_idx: 250 |  Loss: (0.0769) |  Loss2: (0.0000) | Acc: (97.00%) (31311/32128)
Epoch: 182 | Batch_idx: 260 |  Loss: (0.0772) |  Loss2: (0.0000) | Acc: (97.00%) (32555/33408)
Epoch: 182 | Batch_idx: 270 |  Loss: (0.0769) |  Loss2: (0.0000) | Acc: (97.00%) (33804/34688)
Epoch: 182 | Batch_idx: 280 |  Loss: (0.0767) |  Loss2: (0.0000) | Acc: (97.00%) (35059/35968)
Epoch: 182 | Batch_idx: 290 |  Loss: (0.0774) |  Loss2: (0.0000) | Acc: (97.00%) (36293/37248)
Epoch: 182 | Batch_idx: 300 |  Loss: (0.0776) |  Loss2: (0.0000) | Acc: (97.00%) (37540/38528)
Epoch: 182 | Batch_idx: 310 |  Loss: (0.0781) |  Loss2: (0.0000) | Acc: (97.00%) (38783/39808)
Epoch: 182 | Batch_idx: 320 |  Loss: (0.0785) |  Loss2: (0.0000) | Acc: (97.00%) (40020/41088)
Epoch: 182 | Batch_idx: 330 |  Loss: (0.0779) |  Loss2: (0.0000) | Acc: (97.00%) (41282/42368)
Epoch: 182 | Batch_idx: 340 |  Loss: (0.0779) |  Loss2: (0.0000) | Acc: (97.00%) (42529/43648)
Epoch: 182 | Batch_idx: 350 |  Loss: (0.0783) |  Loss2: (0.0000) | Acc: (97.00%) (43772/44928)
Epoch: 182 | Batch_idx: 360 |  Loss: (0.0781) |  Loss2: (0.0000) | Acc: (97.00%) (45026/46208)
Epoch: 182 | Batch_idx: 370 |  Loss: (0.0782) |  Loss2: (0.0000) | Acc: (97.00%) (46262/47488)
Epoch: 182 | Batch_idx: 380 |  Loss: (0.0784) |  Loss2: (0.0000) | Acc: (97.00%) (47503/48768)
Epoch: 182 | Batch_idx: 390 |  Loss: (0.0787) |  Loss2: (0.0000) | Acc: (97.00%) (48695/50000)
# TEST : Loss: (0.4075) | Acc: (88.00%) (8866/10000)
percent tensor([0.5879, 0.6075, 0.5926, 0.5823, 0.5994, 0.5707, 0.6100, 0.6061, 0.6068,
        0.6021, 0.6008, 0.6005, 0.5970, 0.6087, 0.5927, 0.5842],
       device='cuda:0') torch.Size([16])
percent tensor([0.5396, 0.5330, 0.5114, 0.5348, 0.5318, 0.5145, 0.5399, 0.5616, 0.5321,
        0.5127, 0.5066, 0.4997, 0.5320, 0.5466, 0.5443, 0.5289],
       device='cuda:0') torch.Size([16])
percent tensor([0.5696, 0.5378, 0.5113, 0.5898, 0.4948, 0.6066, 0.5430, 0.5557, 0.5599,
        0.5195, 0.5531, 0.5168, 0.5089, 0.6523, 0.5801, 0.5800],
       device='cuda:0') torch.Size([16])
percent tensor([0.6731, 0.6962, 0.6352, 0.6265, 0.6249, 0.6722, 0.6790, 0.6092, 0.6661,
        0.6927, 0.6896, 0.6685, 0.7188, 0.6707, 0.6770, 0.6830],
       device='cuda:0') torch.Size([16])
percent tensor([0.6870, 0.5922, 0.6939, 0.7091, 0.7257, 0.7703, 0.6825, 0.6815, 0.7509,
        0.6700, 0.7141, 0.6962, 0.6112, 0.7523, 0.6456, 0.7336],
       device='cuda:0') torch.Size([16])
percent tensor([0.7928, 0.7873, 0.8319, 0.8249, 0.8493, 0.8211, 0.8116, 0.8124, 0.8105,
        0.8027, 0.8128, 0.7960, 0.7934, 0.8257, 0.7853, 0.8157],
       device='cuda:0') torch.Size([16])
percent tensor([0.4310, 0.5789, 0.6240, 0.5199, 0.6386, 0.7029, 0.5281, 0.4027, 0.5766,
        0.5080, 0.5288, 0.4983, 0.5366, 0.4589, 0.3440, 0.4373],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9998, 0.9995, 0.9998, 0.9993, 0.9994, 0.9997, 0.9997, 0.9997,
        0.9996, 1.0000, 0.9999, 0.9996, 0.9993, 0.9992, 0.9994],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 183 | Batch_idx: 0 |  Loss: (0.0872) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 183 | Batch_idx: 10 |  Loss: (0.0819) |  Loss2: (0.0000) | Acc: (96.00%) (1363/1408)
Epoch: 183 | Batch_idx: 20 |  Loss: (0.0948) |  Loss2: (0.0000) | Acc: (96.00%) (2590/2688)
Epoch: 183 | Batch_idx: 30 |  Loss: (0.0998) |  Loss2: (0.0000) | Acc: (96.00%) (3821/3968)
Epoch: 183 | Batch_idx: 40 |  Loss: (0.1014) |  Loss2: (0.0000) | Acc: (96.00%) (5059/5248)
Epoch: 183 | Batch_idx: 50 |  Loss: (0.1011) |  Loss2: (0.0000) | Acc: (96.00%) (6299/6528)
Epoch: 183 | Batch_idx: 60 |  Loss: (0.1001) |  Loss2: (0.0000) | Acc: (96.00%) (7534/7808)
Epoch: 183 | Batch_idx: 70 |  Loss: (0.0972) |  Loss2: (0.0000) | Acc: (96.00%) (8773/9088)
Epoch: 183 | Batch_idx: 80 |  Loss: (0.0969) |  Loss2: (0.0000) | Acc: (96.00%) (10015/10368)
Epoch: 183 | Batch_idx: 90 |  Loss: (0.0969) |  Loss2: (0.0000) | Acc: (96.00%) (11251/11648)
Epoch: 183 | Batch_idx: 100 |  Loss: (0.0974) |  Loss2: (0.0000) | Acc: (96.00%) (12487/12928)
Epoch: 183 | Batch_idx: 110 |  Loss: (0.0969) |  Loss2: (0.0000) | Acc: (96.00%) (13728/14208)
Epoch: 183 | Batch_idx: 120 |  Loss: (0.0975) |  Loss2: (0.0000) | Acc: (96.00%) (14963/15488)
Epoch: 183 | Batch_idx: 130 |  Loss: (0.0972) |  Loss2: (0.0000) | Acc: (96.00%) (16201/16768)
Epoch: 183 | Batch_idx: 140 |  Loss: (0.0974) |  Loss2: (0.0000) | Acc: (96.00%) (17439/18048)
Epoch: 183 | Batch_idx: 150 |  Loss: (0.0978) |  Loss2: (0.0000) | Acc: (96.00%) (18676/19328)
Epoch: 183 | Batch_idx: 160 |  Loss: (0.0975) |  Loss2: (0.0000) | Acc: (96.00%) (19918/20608)
Epoch: 183 | Batch_idx: 170 |  Loss: (0.0969) |  Loss2: (0.0000) | Acc: (96.00%) (21164/21888)
Epoch: 183 | Batch_idx: 180 |  Loss: (0.0954) |  Loss2: (0.0000) | Acc: (96.00%) (22410/23168)
Epoch: 183 | Batch_idx: 190 |  Loss: (0.0949) |  Loss2: (0.0000) | Acc: (96.00%) (23657/24448)
Epoch: 183 | Batch_idx: 200 |  Loss: (0.0951) |  Loss2: (0.0000) | Acc: (96.00%) (24887/25728)
Epoch: 183 | Batch_idx: 210 |  Loss: (0.0952) |  Loss2: (0.0000) | Acc: (96.00%) (26119/27008)
Epoch: 183 | Batch_idx: 220 |  Loss: (0.0950) |  Loss2: (0.0000) | Acc: (96.00%) (27363/28288)
Epoch: 183 | Batch_idx: 230 |  Loss: (0.0948) |  Loss2: (0.0000) | Acc: (96.00%) (28605/29568)
Epoch: 183 | Batch_idx: 240 |  Loss: (0.0942) |  Loss2: (0.0000) | Acc: (96.00%) (29852/30848)
Epoch: 183 | Batch_idx: 250 |  Loss: (0.0936) |  Loss2: (0.0000) | Acc: (96.00%) (31099/32128)
Epoch: 183 | Batch_idx: 260 |  Loss: (0.0939) |  Loss2: (0.0000) | Acc: (96.00%) (32335/33408)
Epoch: 183 | Batch_idx: 270 |  Loss: (0.0945) |  Loss2: (0.0000) | Acc: (96.00%) (33566/34688)
Epoch: 183 | Batch_idx: 280 |  Loss: (0.0940) |  Loss2: (0.0000) | Acc: (96.00%) (34814/35968)
Epoch: 183 | Batch_idx: 290 |  Loss: (0.0940) |  Loss2: (0.0000) | Acc: (96.00%) (36052/37248)
Epoch: 183 | Batch_idx: 300 |  Loss: (0.0937) |  Loss2: (0.0000) | Acc: (96.00%) (37297/38528)
Epoch: 183 | Batch_idx: 310 |  Loss: (0.0933) |  Loss2: (0.0000) | Acc: (96.00%) (38545/39808)
Epoch: 183 | Batch_idx: 320 |  Loss: (0.0928) |  Loss2: (0.0000) | Acc: (96.00%) (39789/41088)
Epoch: 183 | Batch_idx: 330 |  Loss: (0.0921) |  Loss2: (0.0000) | Acc: (96.00%) (41043/42368)
Epoch: 183 | Batch_idx: 340 |  Loss: (0.0921) |  Loss2: (0.0000) | Acc: (96.00%) (42280/43648)
Epoch: 183 | Batch_idx: 350 |  Loss: (0.0915) |  Loss2: (0.0000) | Acc: (96.00%) (43528/44928)
Epoch: 183 | Batch_idx: 360 |  Loss: (0.0917) |  Loss2: (0.0000) | Acc: (96.00%) (44766/46208)
Epoch: 183 | Batch_idx: 370 |  Loss: (0.0916) |  Loss2: (0.0000) | Acc: (96.00%) (46001/47488)
Epoch: 183 | Batch_idx: 380 |  Loss: (0.0914) |  Loss2: (0.0000) | Acc: (96.00%) (47237/48768)
Epoch: 183 | Batch_idx: 390 |  Loss: (0.0911) |  Loss2: (0.0000) | Acc: (96.00%) (48436/50000)
# TEST : Loss: (0.4035) | Acc: (88.00%) (8848/10000)
percent tensor([0.5865, 0.6039, 0.5917, 0.5804, 0.5979, 0.5696, 0.6072, 0.6039, 0.6049,
        0.5997, 0.5984, 0.5988, 0.5949, 0.6048, 0.5904, 0.5822],
       device='cuda:0') torch.Size([16])
percent tensor([0.5549, 0.5423, 0.5325, 0.5524, 0.5508, 0.5306, 0.5532, 0.5762, 0.5479,
        0.5259, 0.5196, 0.5175, 0.5434, 0.5572, 0.5569, 0.5429],
       device='cuda:0') torch.Size([16])
percent tensor([0.5646, 0.5200, 0.5088, 0.5869, 0.4942, 0.6017, 0.5355, 0.5566, 0.5542,
        0.5054, 0.5403, 0.5071, 0.4925, 0.6440, 0.5709, 0.5712],
       device='cuda:0') torch.Size([16])
percent tensor([0.6670, 0.6880, 0.6340, 0.6251, 0.6227, 0.6679, 0.6725, 0.6060, 0.6597,
        0.6872, 0.6837, 0.6656, 0.7126, 0.6646, 0.6699, 0.6778],
       device='cuda:0') torch.Size([16])
percent tensor([0.6700, 0.5623, 0.6995, 0.7102, 0.7240, 0.7615, 0.6630, 0.6792, 0.7328,
        0.6526, 0.6925, 0.6940, 0.5811, 0.7314, 0.6232, 0.7121],
       device='cuda:0') torch.Size([16])
percent tensor([0.7552, 0.7536, 0.8022, 0.7901, 0.8183, 0.7873, 0.7776, 0.7750, 0.7788,
        0.7693, 0.7789, 0.7620, 0.7590, 0.7908, 0.7471, 0.7785],
       device='cuda:0') torch.Size([16])
percent tensor([0.4582, 0.5993, 0.5992, 0.5013, 0.6188, 0.6762, 0.5397, 0.3850, 0.5925,
        0.5356, 0.5331, 0.5071, 0.5609, 0.4673, 0.3608, 0.4597],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9997, 0.9995, 0.9998, 0.9993, 0.9992, 0.9997, 0.9996, 0.9997,
        0.9997, 0.9999, 0.9999, 0.9997, 0.9993, 0.9992, 0.9995],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 184 | Batch_idx: 0 |  Loss: (0.0487) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 184 | Batch_idx: 10 |  Loss: (0.0598) |  Loss2: (0.0000) | Acc: (98.00%) (1384/1408)
Epoch: 184 | Batch_idx: 20 |  Loss: (0.0713) |  Loss2: (0.0000) | Acc: (97.00%) (2626/2688)
Epoch: 184 | Batch_idx: 30 |  Loss: (0.0705) |  Loss2: (0.0000) | Acc: (97.00%) (3882/3968)
Epoch: 184 | Batch_idx: 40 |  Loss: (0.0701) |  Loss2: (0.0000) | Acc: (97.00%) (5134/5248)
Epoch: 184 | Batch_idx: 50 |  Loss: (0.0723) |  Loss2: (0.0000) | Acc: (97.00%) (6379/6528)
Epoch: 184 | Batch_idx: 60 |  Loss: (0.0723) |  Loss2: (0.0000) | Acc: (97.00%) (7634/7808)
Epoch: 184 | Batch_idx: 70 |  Loss: (0.0760) |  Loss2: (0.0000) | Acc: (97.00%) (8866/9088)
Epoch: 184 | Batch_idx: 80 |  Loss: (0.0765) |  Loss2: (0.0000) | Acc: (97.00%) (10112/10368)
Epoch: 184 | Batch_idx: 90 |  Loss: (0.0744) |  Loss2: (0.0000) | Acc: (97.00%) (11370/11648)
Epoch: 184 | Batch_idx: 100 |  Loss: (0.0751) |  Loss2: (0.0000) | Acc: (97.00%) (12617/12928)
Epoch: 184 | Batch_idx: 110 |  Loss: (0.0752) |  Loss2: (0.0000) | Acc: (97.00%) (13862/14208)
Epoch: 184 | Batch_idx: 120 |  Loss: (0.0756) |  Loss2: (0.0000) | Acc: (97.00%) (15106/15488)
Epoch: 184 | Batch_idx: 130 |  Loss: (0.0755) |  Loss2: (0.0000) | Acc: (97.00%) (16349/16768)
Epoch: 184 | Batch_idx: 140 |  Loss: (0.0753) |  Loss2: (0.0000) | Acc: (97.00%) (17604/18048)
Epoch: 184 | Batch_idx: 150 |  Loss: (0.0757) |  Loss2: (0.0000) | Acc: (97.00%) (18851/19328)
Epoch: 184 | Batch_idx: 160 |  Loss: (0.0757) |  Loss2: (0.0000) | Acc: (97.00%) (20099/20608)
Epoch: 184 | Batch_idx: 170 |  Loss: (0.0755) |  Loss2: (0.0000) | Acc: (97.00%) (21350/21888)
Epoch: 184 | Batch_idx: 180 |  Loss: (0.0759) |  Loss2: (0.0000) | Acc: (97.00%) (22593/23168)
Epoch: 184 | Batch_idx: 190 |  Loss: (0.0762) |  Loss2: (0.0000) | Acc: (97.00%) (23838/24448)
Epoch: 184 | Batch_idx: 200 |  Loss: (0.0765) |  Loss2: (0.0000) | Acc: (97.00%) (25087/25728)
Epoch: 184 | Batch_idx: 210 |  Loss: (0.0765) |  Loss2: (0.0000) | Acc: (97.00%) (26335/27008)
Epoch: 184 | Batch_idx: 220 |  Loss: (0.0763) |  Loss2: (0.0000) | Acc: (97.00%) (27585/28288)
Epoch: 184 | Batch_idx: 230 |  Loss: (0.0758) |  Loss2: (0.0000) | Acc: (97.00%) (28840/29568)
Epoch: 184 | Batch_idx: 240 |  Loss: (0.0760) |  Loss2: (0.0000) | Acc: (97.00%) (30086/30848)
Epoch: 184 | Batch_idx: 250 |  Loss: (0.0760) |  Loss2: (0.0000) | Acc: (97.00%) (31332/32128)
Epoch: 184 | Batch_idx: 260 |  Loss: (0.0763) |  Loss2: (0.0000) | Acc: (97.00%) (32574/33408)
Epoch: 184 | Batch_idx: 270 |  Loss: (0.0772) |  Loss2: (0.0000) | Acc: (97.00%) (33816/34688)
Epoch: 184 | Batch_idx: 280 |  Loss: (0.0776) |  Loss2: (0.0000) | Acc: (97.00%) (35059/35968)
Epoch: 184 | Batch_idx: 290 |  Loss: (0.0774) |  Loss2: (0.0000) | Acc: (97.00%) (36303/37248)
Epoch: 184 | Batch_idx: 300 |  Loss: (0.0772) |  Loss2: (0.0000) | Acc: (97.00%) (37545/38528)
Epoch: 184 | Batch_idx: 310 |  Loss: (0.0767) |  Loss2: (0.0000) | Acc: (97.00%) (38801/39808)
Epoch: 184 | Batch_idx: 320 |  Loss: (0.0767) |  Loss2: (0.0000) | Acc: (97.00%) (40048/41088)
Epoch: 184 | Batch_idx: 330 |  Loss: (0.0768) |  Loss2: (0.0000) | Acc: (97.00%) (41288/42368)
Epoch: 184 | Batch_idx: 340 |  Loss: (0.0767) |  Loss2: (0.0000) | Acc: (97.00%) (42540/43648)
Epoch: 184 | Batch_idx: 350 |  Loss: (0.0767) |  Loss2: (0.0000) | Acc: (97.00%) (43789/44928)
Epoch: 184 | Batch_idx: 360 |  Loss: (0.0769) |  Loss2: (0.0000) | Acc: (97.00%) (45037/46208)
Epoch: 184 | Batch_idx: 370 |  Loss: (0.0767) |  Loss2: (0.0000) | Acc: (97.00%) (46293/47488)
Epoch: 184 | Batch_idx: 380 |  Loss: (0.0767) |  Loss2: (0.0000) | Acc: (97.00%) (47539/48768)
Epoch: 184 | Batch_idx: 390 |  Loss: (0.0767) |  Loss2: (0.0000) | Acc: (97.00%) (48738/50000)
# TEST : Loss: (0.4850) | Acc: (86.00%) (8690/10000)
percent tensor([0.5862, 0.6029, 0.5937, 0.5812, 0.5995, 0.5700, 0.6065, 0.6034, 0.6044,
        0.5991, 0.5980, 0.5989, 0.5942, 0.6034, 0.5897, 0.5819],
       device='cuda:0') torch.Size([16])
percent tensor([0.5562, 0.5431, 0.5282, 0.5555, 0.5514, 0.5347, 0.5520, 0.5778, 0.5468,
        0.5234, 0.5185, 0.5149, 0.5440, 0.5599, 0.5586, 0.5443],
       device='cuda:0') torch.Size([16])
percent tensor([0.5643, 0.5255, 0.5016, 0.5919, 0.4949, 0.6005, 0.5334, 0.5630, 0.5537,
        0.5071, 0.5390, 0.5002, 0.4906, 0.6478, 0.5758, 0.5689],
       device='cuda:0') torch.Size([16])
percent tensor([0.6659, 0.6855, 0.6405, 0.6285, 0.6266, 0.6709, 0.6705, 0.6064, 0.6603,
        0.6857, 0.6802, 0.6703, 0.7119, 0.6568, 0.6701, 0.6760],
       device='cuda:0') torch.Size([16])
percent tensor([0.6705, 0.5773, 0.6920, 0.6988, 0.7134, 0.7560, 0.6821, 0.6819, 0.7526,
        0.6649, 0.7010, 0.6865, 0.5877, 0.7518, 0.6181, 0.7149],
       device='cuda:0') torch.Size([16])
percent tensor([0.7543, 0.7538, 0.7939, 0.7815, 0.8146, 0.7956, 0.7795, 0.7709, 0.7792,
        0.7667, 0.7791, 0.7588, 0.7579, 0.7916, 0.7479, 0.7777],
       device='cuda:0') torch.Size([16])
percent tensor([0.4862, 0.5759, 0.6027, 0.5176, 0.6131, 0.7100, 0.5112, 0.3632, 0.6087,
        0.5547, 0.5415, 0.4977, 0.5927, 0.5067, 0.3266, 0.4652],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9997, 0.9995, 0.9998, 0.9994, 0.9993, 0.9998, 0.9996, 0.9996,
        0.9997, 0.9999, 0.9998, 0.9995, 0.9988, 0.9993, 0.9995],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 185 | Batch_idx: 0 |  Loss: (0.0481) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 185 | Batch_idx: 10 |  Loss: (0.0680) |  Loss2: (0.0000) | Acc: (97.00%) (1374/1408)
Epoch: 185 | Batch_idx: 20 |  Loss: (0.0807) |  Loss2: (0.0000) | Acc: (97.00%) (2612/2688)
Epoch: 185 | Batch_idx: 30 |  Loss: (0.0865) |  Loss2: (0.0000) | Acc: (97.00%) (3850/3968)
Epoch: 185 | Batch_idx: 40 |  Loss: (0.0883) |  Loss2: (0.0000) | Acc: (96.00%) (5089/5248)
Epoch: 185 | Batch_idx: 50 |  Loss: (0.0879) |  Loss2: (0.0000) | Acc: (96.00%) (6328/6528)
Epoch: 185 | Batch_idx: 60 |  Loss: (0.0925) |  Loss2: (0.0000) | Acc: (96.00%) (7556/7808)
Epoch: 185 | Batch_idx: 70 |  Loss: (0.0923) |  Loss2: (0.0000) | Acc: (96.00%) (8804/9088)
Epoch: 185 | Batch_idx: 80 |  Loss: (0.0936) |  Loss2: (0.0000) | Acc: (96.00%) (10028/10368)
Epoch: 185 | Batch_idx: 90 |  Loss: (0.0924) |  Loss2: (0.0000) | Acc: (96.00%) (11275/11648)
Epoch: 185 | Batch_idx: 100 |  Loss: (0.0918) |  Loss2: (0.0000) | Acc: (96.00%) (12523/12928)
Epoch: 185 | Batch_idx: 110 |  Loss: (0.0901) |  Loss2: (0.0000) | Acc: (96.00%) (13768/14208)
Epoch: 185 | Batch_idx: 120 |  Loss: (0.0901) |  Loss2: (0.0000) | Acc: (96.00%) (15010/15488)
Epoch: 185 | Batch_idx: 130 |  Loss: (0.0903) |  Loss2: (0.0000) | Acc: (96.00%) (16251/16768)
Epoch: 185 | Batch_idx: 140 |  Loss: (0.0908) |  Loss2: (0.0000) | Acc: (96.00%) (17490/18048)
Epoch: 185 | Batch_idx: 150 |  Loss: (0.0906) |  Loss2: (0.0000) | Acc: (96.00%) (18733/19328)
Epoch: 185 | Batch_idx: 160 |  Loss: (0.0907) |  Loss2: (0.0000) | Acc: (96.00%) (19970/20608)
Epoch: 185 | Batch_idx: 170 |  Loss: (0.0899) |  Loss2: (0.0000) | Acc: (96.00%) (21217/21888)
Epoch: 185 | Batch_idx: 180 |  Loss: (0.0895) |  Loss2: (0.0000) | Acc: (96.00%) (22467/23168)
Epoch: 185 | Batch_idx: 190 |  Loss: (0.0891) |  Loss2: (0.0000) | Acc: (96.00%) (23711/24448)
Epoch: 185 | Batch_idx: 200 |  Loss: (0.0884) |  Loss2: (0.0000) | Acc: (97.00%) (24960/25728)
Epoch: 185 | Batch_idx: 210 |  Loss: (0.0885) |  Loss2: (0.0000) | Acc: (97.00%) (26201/27008)
Epoch: 185 | Batch_idx: 220 |  Loss: (0.0890) |  Loss2: (0.0000) | Acc: (97.00%) (27442/28288)
Epoch: 185 | Batch_idx: 230 |  Loss: (0.0888) |  Loss2: (0.0000) | Acc: (97.00%) (28688/29568)
Epoch: 185 | Batch_idx: 240 |  Loss: (0.0889) |  Loss2: (0.0000) | Acc: (97.00%) (29930/30848)
Epoch: 185 | Batch_idx: 250 |  Loss: (0.0890) |  Loss2: (0.0000) | Acc: (97.00%) (31167/32128)
Epoch: 185 | Batch_idx: 260 |  Loss: (0.0886) |  Loss2: (0.0000) | Acc: (97.00%) (32412/33408)
Epoch: 185 | Batch_idx: 270 |  Loss: (0.0885) |  Loss2: (0.0000) | Acc: (97.00%) (33654/34688)
Epoch: 185 | Batch_idx: 280 |  Loss: (0.0884) |  Loss2: (0.0000) | Acc: (97.00%) (34906/35968)
Epoch: 185 | Batch_idx: 290 |  Loss: (0.0883) |  Loss2: (0.0000) | Acc: (97.00%) (36147/37248)
Epoch: 185 | Batch_idx: 300 |  Loss: (0.0884) |  Loss2: (0.0000) | Acc: (97.00%) (37392/38528)
Epoch: 185 | Batch_idx: 310 |  Loss: (0.0881) |  Loss2: (0.0000) | Acc: (97.00%) (38638/39808)
Epoch: 185 | Batch_idx: 320 |  Loss: (0.0875) |  Loss2: (0.0000) | Acc: (97.00%) (39893/41088)
Epoch: 185 | Batch_idx: 330 |  Loss: (0.0873) |  Loss2: (0.0000) | Acc: (97.00%) (41138/42368)
Epoch: 185 | Batch_idx: 340 |  Loss: (0.0873) |  Loss2: (0.0000) | Acc: (97.00%) (42377/43648)
Epoch: 185 | Batch_idx: 350 |  Loss: (0.0870) |  Loss2: (0.0000) | Acc: (97.00%) (43627/44928)
Epoch: 185 | Batch_idx: 360 |  Loss: (0.0868) |  Loss2: (0.0000) | Acc: (97.00%) (44874/46208)
Epoch: 185 | Batch_idx: 370 |  Loss: (0.0867) |  Loss2: (0.0000) | Acc: (97.00%) (46120/47488)
Epoch: 185 | Batch_idx: 380 |  Loss: (0.0862) |  Loss2: (0.0000) | Acc: (97.00%) (47365/48768)
Epoch: 185 | Batch_idx: 390 |  Loss: (0.0862) |  Loss2: (0.0000) | Acc: (97.00%) (48561/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_185.pth.tar'
# TEST : Loss: (0.3981) | Acc: (88.00%) (8852/10000)
percent tensor([0.5813, 0.5968, 0.5887, 0.5768, 0.5937, 0.5651, 0.6005, 0.5976, 0.5986,
        0.5937, 0.5922, 0.5938, 0.5887, 0.5980, 0.5839, 0.5770],
       device='cuda:0') torch.Size([16])
percent tensor([0.5592, 0.5486, 0.5304, 0.5580, 0.5533, 0.5373, 0.5552, 0.5782, 0.5497,
        0.5293, 0.5249, 0.5185, 0.5488, 0.5622, 0.5619, 0.5477],
       device='cuda:0') torch.Size([16])
percent tensor([0.5612, 0.5186, 0.4888, 0.5815, 0.4856, 0.5961, 0.5265, 0.5497, 0.5464,
        0.5013, 0.5357, 0.4870, 0.4858, 0.6403, 0.5717, 0.5650],
       device='cuda:0') torch.Size([16])
percent tensor([0.6648, 0.6827, 0.6386, 0.6260, 0.6247, 0.6678, 0.6693, 0.6069, 0.6591,
        0.6844, 0.6792, 0.6683, 0.7081, 0.6542, 0.6678, 0.6741],
       device='cuda:0') torch.Size([16])
percent tensor([0.6863, 0.5889, 0.7163, 0.7283, 0.7434, 0.7809, 0.7004, 0.7090, 0.7688,
        0.6752, 0.7135, 0.7018, 0.5854, 0.7748, 0.6289, 0.7399],
       device='cuda:0') torch.Size([16])
percent tensor([0.7614, 0.7601, 0.8003, 0.7868, 0.8232, 0.8059, 0.7858, 0.7766, 0.7847,
        0.7734, 0.7818, 0.7651, 0.7641, 0.7979, 0.7532, 0.7872],
       device='cuda:0') torch.Size([16])
percent tensor([0.4589, 0.5640, 0.5789, 0.4981, 0.6079, 0.7076, 0.4945, 0.3676, 0.5757,
        0.5318, 0.5147, 0.4704, 0.5724, 0.4742, 0.3260, 0.4386],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9997, 0.9994, 0.9998, 0.9994, 0.9990, 0.9997, 0.9996, 0.9997,
        0.9998, 0.9999, 0.9998, 0.9996, 0.9991, 0.9993, 0.9995],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 186 | Batch_idx: 0 |  Loss: (0.0556) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 186 | Batch_idx: 10 |  Loss: (0.0684) |  Loss2: (0.0000) | Acc: (97.00%) (1378/1408)
Epoch: 186 | Batch_idx: 20 |  Loss: (0.0689) |  Loss2: (0.0000) | Acc: (97.00%) (2628/2688)
Epoch: 186 | Batch_idx: 30 |  Loss: (0.0674) |  Loss2: (0.0000) | Acc: (97.00%) (3879/3968)
Epoch: 186 | Batch_idx: 40 |  Loss: (0.0658) |  Loss2: (0.0000) | Acc: (97.00%) (5135/5248)
Epoch: 186 | Batch_idx: 50 |  Loss: (0.0681) |  Loss2: (0.0000) | Acc: (97.00%) (6386/6528)
Epoch: 186 | Batch_idx: 60 |  Loss: (0.0665) |  Loss2: (0.0000) | Acc: (97.00%) (7647/7808)
Epoch: 186 | Batch_idx: 70 |  Loss: (0.0675) |  Loss2: (0.0000) | Acc: (97.00%) (8899/9088)
Epoch: 186 | Batch_idx: 80 |  Loss: (0.0690) |  Loss2: (0.0000) | Acc: (97.00%) (10147/10368)
Epoch: 186 | Batch_idx: 90 |  Loss: (0.0705) |  Loss2: (0.0000) | Acc: (97.00%) (11397/11648)
Epoch: 186 | Batch_idx: 100 |  Loss: (0.0699) |  Loss2: (0.0000) | Acc: (97.00%) (12653/12928)
Epoch: 186 | Batch_idx: 110 |  Loss: (0.0706) |  Loss2: (0.0000) | Acc: (97.00%) (13894/14208)
Epoch: 186 | Batch_idx: 120 |  Loss: (0.0714) |  Loss2: (0.0000) | Acc: (97.00%) (15138/15488)
Epoch: 186 | Batch_idx: 130 |  Loss: (0.0719) |  Loss2: (0.0000) | Acc: (97.00%) (16382/16768)
Epoch: 186 | Batch_idx: 140 |  Loss: (0.0722) |  Loss2: (0.0000) | Acc: (97.00%) (17628/18048)
Epoch: 186 | Batch_idx: 150 |  Loss: (0.0726) |  Loss2: (0.0000) | Acc: (97.00%) (18876/19328)
Epoch: 186 | Batch_idx: 160 |  Loss: (0.0726) |  Loss2: (0.0000) | Acc: (97.00%) (20126/20608)
Epoch: 186 | Batch_idx: 170 |  Loss: (0.0730) |  Loss2: (0.0000) | Acc: (97.00%) (21374/21888)
Epoch: 186 | Batch_idx: 180 |  Loss: (0.0726) |  Loss2: (0.0000) | Acc: (97.00%) (22630/23168)
Epoch: 186 | Batch_idx: 190 |  Loss: (0.0729) |  Loss2: (0.0000) | Acc: (97.00%) (23878/24448)
Epoch: 186 | Batch_idx: 200 |  Loss: (0.0729) |  Loss2: (0.0000) | Acc: (97.00%) (25122/25728)
Epoch: 186 | Batch_idx: 210 |  Loss: (0.0733) |  Loss2: (0.0000) | Acc: (97.00%) (26367/27008)
Epoch: 186 | Batch_idx: 220 |  Loss: (0.0735) |  Loss2: (0.0000) | Acc: (97.00%) (27613/28288)
Epoch: 186 | Batch_idx: 230 |  Loss: (0.0729) |  Loss2: (0.0000) | Acc: (97.00%) (28866/29568)
Epoch: 186 | Batch_idx: 240 |  Loss: (0.0734) |  Loss2: (0.0000) | Acc: (97.00%) (30112/30848)
Epoch: 186 | Batch_idx: 250 |  Loss: (0.0741) |  Loss2: (0.0000) | Acc: (97.00%) (31349/32128)
Epoch: 186 | Batch_idx: 260 |  Loss: (0.0743) |  Loss2: (0.0000) | Acc: (97.00%) (32592/33408)
Epoch: 186 | Batch_idx: 270 |  Loss: (0.0744) |  Loss2: (0.0000) | Acc: (97.00%) (33834/34688)
Epoch: 186 | Batch_idx: 280 |  Loss: (0.0748) |  Loss2: (0.0000) | Acc: (97.00%) (35075/35968)
Epoch: 186 | Batch_idx: 290 |  Loss: (0.0750) |  Loss2: (0.0000) | Acc: (97.00%) (36324/37248)
Epoch: 186 | Batch_idx: 300 |  Loss: (0.0749) |  Loss2: (0.0000) | Acc: (97.00%) (37575/38528)
Epoch: 186 | Batch_idx: 310 |  Loss: (0.0752) |  Loss2: (0.0000) | Acc: (97.00%) (38819/39808)
Epoch: 186 | Batch_idx: 320 |  Loss: (0.0752) |  Loss2: (0.0000) | Acc: (97.00%) (40063/41088)
Epoch: 186 | Batch_idx: 330 |  Loss: (0.0754) |  Loss2: (0.0000) | Acc: (97.00%) (41307/42368)
Epoch: 186 | Batch_idx: 340 |  Loss: (0.0752) |  Loss2: (0.0000) | Acc: (97.00%) (42556/43648)
Epoch: 186 | Batch_idx: 350 |  Loss: (0.0753) |  Loss2: (0.0000) | Acc: (97.00%) (43798/44928)
Epoch: 186 | Batch_idx: 360 |  Loss: (0.0753) |  Loss2: (0.0000) | Acc: (97.00%) (45044/46208)
Epoch: 186 | Batch_idx: 370 |  Loss: (0.0748) |  Loss2: (0.0000) | Acc: (97.00%) (46300/47488)
Epoch: 186 | Batch_idx: 380 |  Loss: (0.0749) |  Loss2: (0.0000) | Acc: (97.00%) (47553/48768)
Epoch: 186 | Batch_idx: 390 |  Loss: (0.0749) |  Loss2: (0.0000) | Acc: (97.00%) (48755/50000)
# TEST : Loss: (0.3964) | Acc: (88.00%) (8872/10000)
percent tensor([0.5811, 0.5980, 0.5857, 0.5763, 0.5924, 0.5661, 0.6009, 0.5972, 0.5979,
        0.5937, 0.5921, 0.5921, 0.5886, 0.5992, 0.5848, 0.5769],
       device='cuda:0') torch.Size([16])
percent tensor([0.5599, 0.5500, 0.5338, 0.5546, 0.5559, 0.5387, 0.5605, 0.5785, 0.5528,
        0.5307, 0.5274, 0.5243, 0.5487, 0.5625, 0.5611, 0.5473],
       device='cuda:0') torch.Size([16])
percent tensor([0.5619, 0.5266, 0.4861, 0.5686, 0.4782, 0.5970, 0.5372, 0.5514, 0.5481,
        0.5044, 0.5407, 0.4875, 0.4887, 0.6488, 0.5720, 0.5711],
       device='cuda:0') torch.Size([16])
percent tensor([0.6651, 0.6818, 0.6402, 0.6305, 0.6243, 0.6631, 0.6672, 0.6130, 0.6588,
        0.6833, 0.6783, 0.6690, 0.7079, 0.6534, 0.6710, 0.6724],
       device='cuda:0') torch.Size([16])
percent tensor([0.6788, 0.5830, 0.7301, 0.7338, 0.7491, 0.7708, 0.6792, 0.7030, 0.7651,
        0.6681, 0.7096, 0.7045, 0.5825, 0.7584, 0.6292, 0.7276],
       device='cuda:0') torch.Size([16])
percent tensor([0.7583, 0.7623, 0.7982, 0.7853, 0.8177, 0.7988, 0.7820, 0.7716, 0.7793,
        0.7732, 0.7830, 0.7593, 0.7636, 0.7986, 0.7523, 0.7808],
       device='cuda:0') torch.Size([16])
percent tensor([0.4179, 0.5689, 0.5868, 0.5194, 0.5986, 0.6867, 0.5232, 0.3771, 0.5784,
        0.5101, 0.5161, 0.4750, 0.5529, 0.4684, 0.3465, 0.4330],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9997, 0.9994, 0.9999, 0.9995, 0.9984, 0.9998, 0.9996, 0.9998,
        0.9997, 1.0000, 0.9999, 0.9996, 0.9989, 0.9992, 0.9996],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 187 | Batch_idx: 0 |  Loss: (0.0283) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 187 | Batch_idx: 10 |  Loss: (0.0698) |  Loss2: (0.0000) | Acc: (97.00%) (1370/1408)
Epoch: 187 | Batch_idx: 20 |  Loss: (0.0825) |  Loss2: (0.0000) | Acc: (97.00%) (2609/2688)
Epoch: 187 | Batch_idx: 30 |  Loss: (0.0883) |  Loss2: (0.0000) | Acc: (96.00%) (3846/3968)
Epoch: 187 | Batch_idx: 40 |  Loss: (0.0867) |  Loss2: (0.0000) | Acc: (97.00%) (5093/5248)
Epoch: 187 | Batch_idx: 50 |  Loss: (0.0926) |  Loss2: (0.0000) | Acc: (96.00%) (6321/6528)
Epoch: 187 | Batch_idx: 60 |  Loss: (0.0993) |  Loss2: (0.0000) | Acc: (96.00%) (7539/7808)
Epoch: 187 | Batch_idx: 70 |  Loss: (0.0975) |  Loss2: (0.0000) | Acc: (96.00%) (8784/9088)
Epoch: 187 | Batch_idx: 80 |  Loss: (0.0967) |  Loss2: (0.0000) | Acc: (96.00%) (10018/10368)
Epoch: 187 | Batch_idx: 90 |  Loss: (0.0961) |  Loss2: (0.0000) | Acc: (96.00%) (11257/11648)
Epoch: 187 | Batch_idx: 100 |  Loss: (0.0959) |  Loss2: (0.0000) | Acc: (96.00%) (12496/12928)
Epoch: 187 | Batch_idx: 110 |  Loss: (0.0966) |  Loss2: (0.0000) | Acc: (96.00%) (13738/14208)
Epoch: 187 | Batch_idx: 120 |  Loss: (0.0966) |  Loss2: (0.0000) | Acc: (96.00%) (14974/15488)
Epoch: 187 | Batch_idx: 130 |  Loss: (0.0956) |  Loss2: (0.0000) | Acc: (96.00%) (16210/16768)
Epoch: 187 | Batch_idx: 140 |  Loss: (0.0958) |  Loss2: (0.0000) | Acc: (96.00%) (17437/18048)
Epoch: 187 | Batch_idx: 150 |  Loss: (0.0952) |  Loss2: (0.0000) | Acc: (96.00%) (18678/19328)
Epoch: 187 | Batch_idx: 160 |  Loss: (0.0942) |  Loss2: (0.0000) | Acc: (96.00%) (19921/20608)
Epoch: 187 | Batch_idx: 170 |  Loss: (0.0943) |  Loss2: (0.0000) | Acc: (96.00%) (21153/21888)
Epoch: 187 | Batch_idx: 180 |  Loss: (0.0931) |  Loss2: (0.0000) | Acc: (96.00%) (22401/23168)
Epoch: 187 | Batch_idx: 190 |  Loss: (0.0924) |  Loss2: (0.0000) | Acc: (96.00%) (23647/24448)
Epoch: 187 | Batch_idx: 200 |  Loss: (0.0926) |  Loss2: (0.0000) | Acc: (96.00%) (24891/25728)
Epoch: 187 | Batch_idx: 210 |  Loss: (0.0923) |  Loss2: (0.0000) | Acc: (96.00%) (26129/27008)
Epoch: 187 | Batch_idx: 220 |  Loss: (0.0923) |  Loss2: (0.0000) | Acc: (96.00%) (27366/28288)
Epoch: 187 | Batch_idx: 230 |  Loss: (0.0918) |  Loss2: (0.0000) | Acc: (96.00%) (28613/29568)
Epoch: 187 | Batch_idx: 240 |  Loss: (0.0917) |  Loss2: (0.0000) | Acc: (96.00%) (29847/30848)
Epoch: 187 | Batch_idx: 250 |  Loss: (0.0916) |  Loss2: (0.0000) | Acc: (96.00%) (31097/32128)
Epoch: 187 | Batch_idx: 260 |  Loss: (0.0917) |  Loss2: (0.0000) | Acc: (96.00%) (32334/33408)
Epoch: 187 | Batch_idx: 270 |  Loss: (0.0921) |  Loss2: (0.0000) | Acc: (96.00%) (33571/34688)
Epoch: 187 | Batch_idx: 280 |  Loss: (0.0912) |  Loss2: (0.0000) | Acc: (96.00%) (34821/35968)
Epoch: 187 | Batch_idx: 290 |  Loss: (0.0906) |  Loss2: (0.0000) | Acc: (96.00%) (36072/37248)
Epoch: 187 | Batch_idx: 300 |  Loss: (0.0906) |  Loss2: (0.0000) | Acc: (96.00%) (37311/38528)
Epoch: 187 | Batch_idx: 310 |  Loss: (0.0905) |  Loss2: (0.0000) | Acc: (96.00%) (38552/39808)
Epoch: 187 | Batch_idx: 320 |  Loss: (0.0906) |  Loss2: (0.0000) | Acc: (96.00%) (39786/41088)
Epoch: 187 | Batch_idx: 330 |  Loss: (0.0910) |  Loss2: (0.0000) | Acc: (96.00%) (41016/42368)
Epoch: 187 | Batch_idx: 340 |  Loss: (0.0906) |  Loss2: (0.0000) | Acc: (96.00%) (42261/43648)
Epoch: 187 | Batch_idx: 350 |  Loss: (0.0906) |  Loss2: (0.0000) | Acc: (96.00%) (43502/44928)
Epoch: 187 | Batch_idx: 360 |  Loss: (0.0902) |  Loss2: (0.0000) | Acc: (96.00%) (44751/46208)
Epoch: 187 | Batch_idx: 370 |  Loss: (0.0900) |  Loss2: (0.0000) | Acc: (96.00%) (45994/47488)
Epoch: 187 | Batch_idx: 380 |  Loss: (0.0894) |  Loss2: (0.0000) | Acc: (96.00%) (47248/48768)
Epoch: 187 | Batch_idx: 390 |  Loss: (0.0896) |  Loss2: (0.0000) | Acc: (96.00%) (48438/50000)
# TEST : Loss: (0.4193) | Acc: (88.00%) (8833/10000)
percent tensor([0.5782, 0.5953, 0.5795, 0.5718, 0.5871, 0.5629, 0.5973, 0.5932, 0.5949,
        0.5898, 0.5894, 0.5867, 0.5856, 0.5976, 0.5817, 0.5742],
       device='cuda:0') torch.Size([16])
percent tensor([0.5489, 0.5427, 0.5193, 0.5384, 0.5456, 0.5259, 0.5539, 0.5696, 0.5406,
        0.5201, 0.5159, 0.5116, 0.5407, 0.5513, 0.5509, 0.5360],
       device='cuda:0') torch.Size([16])
percent tensor([0.5434, 0.5101, 0.4652, 0.5447, 0.4568, 0.5845, 0.5182, 0.5244, 0.5256,
        0.4853, 0.5237, 0.4649, 0.4711, 0.6334, 0.5529, 0.5541],
       device='cuda:0') torch.Size([16])
percent tensor([0.6810, 0.7010, 0.6511, 0.6382, 0.6343, 0.6804, 0.6831, 0.6212, 0.6704,
        0.6996, 0.6959, 0.6808, 0.7273, 0.6646, 0.6903, 0.6886],
       device='cuda:0') torch.Size([16])
percent tensor([0.6975, 0.6148, 0.7353, 0.7368, 0.7477, 0.7841, 0.6975, 0.7094, 0.7731,
        0.6944, 0.7299, 0.7111, 0.6238, 0.7741, 0.6453, 0.7482],
       device='cuda:0') torch.Size([16])
percent tensor([0.7813, 0.7872, 0.8182, 0.8053, 0.8363, 0.8192, 0.8059, 0.7921, 0.8018,
        0.7997, 0.8082, 0.7820, 0.7867, 0.8266, 0.7760, 0.8051],
       device='cuda:0') torch.Size([16])
percent tensor([0.4225, 0.5909, 0.5972, 0.5406, 0.6041, 0.6964, 0.5275, 0.3780, 0.5979,
        0.5443, 0.5421, 0.5030, 0.5622, 0.5188, 0.3485, 0.4359],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9997, 0.9993, 0.9998, 0.9994, 0.9985, 0.9998, 0.9995, 0.9998,
        0.9997, 0.9999, 0.9999, 0.9995, 0.9988, 0.9992, 0.9995],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 188 | Batch_idx: 0 |  Loss: (0.0738) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 188 | Batch_idx: 10 |  Loss: (0.0619) |  Loss2: (0.0000) | Acc: (98.00%) (1383/1408)
Epoch: 188 | Batch_idx: 20 |  Loss: (0.0685) |  Loss2: (0.0000) | Acc: (97.00%) (2627/2688)
Epoch: 188 | Batch_idx: 30 |  Loss: (0.0689) |  Loss2: (0.0000) | Acc: (97.00%) (3880/3968)
Epoch: 188 | Batch_idx: 40 |  Loss: (0.0692) |  Loss2: (0.0000) | Acc: (97.00%) (5131/5248)
Epoch: 188 | Batch_idx: 50 |  Loss: (0.0694) |  Loss2: (0.0000) | Acc: (97.00%) (6377/6528)
Epoch: 188 | Batch_idx: 60 |  Loss: (0.0698) |  Loss2: (0.0000) | Acc: (97.00%) (7625/7808)
Epoch: 188 | Batch_idx: 70 |  Loss: (0.0694) |  Loss2: (0.0000) | Acc: (97.00%) (8881/9088)
Epoch: 188 | Batch_idx: 80 |  Loss: (0.0690) |  Loss2: (0.0000) | Acc: (97.00%) (10134/10368)
Epoch: 188 | Batch_idx: 90 |  Loss: (0.0699) |  Loss2: (0.0000) | Acc: (97.00%) (11382/11648)
Epoch: 188 | Batch_idx: 100 |  Loss: (0.0716) |  Loss2: (0.0000) | Acc: (97.00%) (12624/12928)
Epoch: 188 | Batch_idx: 110 |  Loss: (0.0719) |  Loss2: (0.0000) | Acc: (97.00%) (13876/14208)
Epoch: 188 | Batch_idx: 120 |  Loss: (0.0712) |  Loss2: (0.0000) | Acc: (97.00%) (15132/15488)
Epoch: 188 | Batch_idx: 130 |  Loss: (0.0709) |  Loss2: (0.0000) | Acc: (97.00%) (16385/16768)
Epoch: 188 | Batch_idx: 140 |  Loss: (0.0712) |  Loss2: (0.0000) | Acc: (97.00%) (17633/18048)
Epoch: 188 | Batch_idx: 150 |  Loss: (0.0713) |  Loss2: (0.0000) | Acc: (97.00%) (18883/19328)
Epoch: 188 | Batch_idx: 160 |  Loss: (0.0720) |  Loss2: (0.0000) | Acc: (97.00%) (20128/20608)
Epoch: 188 | Batch_idx: 170 |  Loss: (0.0720) |  Loss2: (0.0000) | Acc: (97.00%) (21379/21888)
Epoch: 188 | Batch_idx: 180 |  Loss: (0.0725) |  Loss2: (0.0000) | Acc: (97.00%) (22626/23168)
Epoch: 188 | Batch_idx: 190 |  Loss: (0.0729) |  Loss2: (0.0000) | Acc: (97.00%) (23876/24448)
Epoch: 188 | Batch_idx: 200 |  Loss: (0.0728) |  Loss2: (0.0000) | Acc: (97.00%) (25123/25728)
Epoch: 188 | Batch_idx: 210 |  Loss: (0.0728) |  Loss2: (0.0000) | Acc: (97.00%) (26378/27008)
Epoch: 188 | Batch_idx: 220 |  Loss: (0.0722) |  Loss2: (0.0000) | Acc: (97.00%) (27636/28288)
Epoch: 188 | Batch_idx: 230 |  Loss: (0.0724) |  Loss2: (0.0000) | Acc: (97.00%) (28890/29568)
Epoch: 188 | Batch_idx: 240 |  Loss: (0.0719) |  Loss2: (0.0000) | Acc: (97.00%) (30143/30848)
Epoch: 188 | Batch_idx: 250 |  Loss: (0.0725) |  Loss2: (0.0000) | Acc: (97.00%) (31388/32128)
Epoch: 188 | Batch_idx: 260 |  Loss: (0.0724) |  Loss2: (0.0000) | Acc: (97.00%) (32636/33408)
Epoch: 188 | Batch_idx: 270 |  Loss: (0.0726) |  Loss2: (0.0000) | Acc: (97.00%) (33887/34688)
Epoch: 188 | Batch_idx: 280 |  Loss: (0.0729) |  Loss2: (0.0000) | Acc: (97.00%) (35134/35968)
Epoch: 188 | Batch_idx: 290 |  Loss: (0.0726) |  Loss2: (0.0000) | Acc: (97.00%) (36394/37248)
Epoch: 188 | Batch_idx: 300 |  Loss: (0.0727) |  Loss2: (0.0000) | Acc: (97.00%) (37638/38528)
Epoch: 188 | Batch_idx: 310 |  Loss: (0.0726) |  Loss2: (0.0000) | Acc: (97.00%) (38892/39808)
Epoch: 188 | Batch_idx: 320 |  Loss: (0.0728) |  Loss2: (0.0000) | Acc: (97.00%) (40135/41088)
Epoch: 188 | Batch_idx: 330 |  Loss: (0.0732) |  Loss2: (0.0000) | Acc: (97.00%) (41377/42368)
Epoch: 188 | Batch_idx: 340 |  Loss: (0.0731) |  Loss2: (0.0000) | Acc: (97.00%) (42630/43648)
Epoch: 188 | Batch_idx: 350 |  Loss: (0.0734) |  Loss2: (0.0000) | Acc: (97.00%) (43876/44928)
Epoch: 188 | Batch_idx: 360 |  Loss: (0.0734) |  Loss2: (0.0000) | Acc: (97.00%) (45124/46208)
Epoch: 188 | Batch_idx: 370 |  Loss: (0.0735) |  Loss2: (0.0000) | Acc: (97.00%) (46370/47488)
Epoch: 188 | Batch_idx: 380 |  Loss: (0.0737) |  Loss2: (0.0000) | Acc: (97.00%) (47614/48768)
Epoch: 188 | Batch_idx: 390 |  Loss: (0.0736) |  Loss2: (0.0000) | Acc: (97.00%) (48819/50000)
# TEST : Loss: (0.4091) | Acc: (88.00%) (8893/10000)
percent tensor([0.5778, 0.5955, 0.5819, 0.5728, 0.5876, 0.5598, 0.5974, 0.5938, 0.5958,
        0.5907, 0.5898, 0.5879, 0.5861, 0.5979, 0.5812, 0.5743],
       device='cuda:0') torch.Size([16])
percent tensor([0.5478, 0.5412, 0.5112, 0.5375, 0.5374, 0.5214, 0.5465, 0.5657, 0.5360,
        0.5171, 0.5125, 0.5021, 0.5401, 0.5502, 0.5490, 0.5350],
       device='cuda:0') torch.Size([16])
percent tensor([0.5497, 0.5077, 0.4746, 0.5509, 0.4626, 0.5900, 0.5172, 0.5245, 0.5262,
        0.4867, 0.5254, 0.4733, 0.4722, 0.6320, 0.5582, 0.5570],
       device='cuda:0') torch.Size([16])
percent tensor([0.6828, 0.7051, 0.6469, 0.6363, 0.6307, 0.6752, 0.6867, 0.6220, 0.6715,
        0.7024, 0.6972, 0.6823, 0.7301, 0.6708, 0.6879, 0.6916],
       device='cuda:0') torch.Size([16])
percent tensor([0.6921, 0.6154, 0.7140, 0.7328, 0.7387, 0.7931, 0.7064, 0.7196, 0.7666,
        0.6868, 0.7284, 0.6977, 0.6106, 0.7668, 0.6611, 0.7489],
       device='cuda:0') torch.Size([16])
percent tensor([0.7808, 0.7927, 0.8051, 0.7997, 0.8335, 0.8191, 0.8160, 0.7943, 0.8035,
        0.8029, 0.8149, 0.7794, 0.7840, 0.8266, 0.7774, 0.8087],
       device='cuda:0') torch.Size([16])
percent tensor([0.4122, 0.5933, 0.5770, 0.5110, 0.5972, 0.6735, 0.5176, 0.3868, 0.5730,
        0.5437, 0.5312, 0.4553, 0.5615, 0.5158, 0.3411, 0.4311],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9997, 0.9997, 0.9999, 0.9996, 0.9990, 0.9997, 0.9996, 0.9997,
        0.9998, 0.9999, 0.9998, 0.9995, 0.9989, 0.9991, 0.9995],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 189 | Batch_idx: 0 |  Loss: (0.0672) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 189 | Batch_idx: 10 |  Loss: (0.0748) |  Loss2: (0.0000) | Acc: (97.00%) (1374/1408)
Epoch: 189 | Batch_idx: 20 |  Loss: (0.0771) |  Loss2: (0.0000) | Acc: (97.00%) (2616/2688)
Epoch: 189 | Batch_idx: 30 |  Loss: (0.0858) |  Loss2: (0.0000) | Acc: (96.00%) (3845/3968)
Epoch: 189 | Batch_idx: 40 |  Loss: (0.0924) |  Loss2: (0.0000) | Acc: (96.00%) (5074/5248)
Epoch: 189 | Batch_idx: 50 |  Loss: (0.0940) |  Loss2: (0.0000) | Acc: (96.00%) (6312/6528)
Epoch: 189 | Batch_idx: 60 |  Loss: (0.0923) |  Loss2: (0.0000) | Acc: (96.00%) (7556/7808)
Epoch: 189 | Batch_idx: 70 |  Loss: (0.0944) |  Loss2: (0.0000) | Acc: (96.00%) (8784/9088)
Epoch: 189 | Batch_idx: 80 |  Loss: (0.0932) |  Loss2: (0.0000) | Acc: (96.00%) (10024/10368)
Epoch: 189 | Batch_idx: 90 |  Loss: (0.0937) |  Loss2: (0.0000) | Acc: (96.00%) (11253/11648)
Epoch: 189 | Batch_idx: 100 |  Loss: (0.0943) |  Loss2: (0.0000) | Acc: (96.00%) (12489/12928)
Epoch: 189 | Batch_idx: 110 |  Loss: (0.0937) |  Loss2: (0.0000) | Acc: (96.00%) (13735/14208)
Epoch: 189 | Batch_idx: 120 |  Loss: (0.0935) |  Loss2: (0.0000) | Acc: (96.00%) (14969/15488)
Epoch: 189 | Batch_idx: 130 |  Loss: (0.0938) |  Loss2: (0.0000) | Acc: (96.00%) (16202/16768)
Epoch: 189 | Batch_idx: 140 |  Loss: (0.0927) |  Loss2: (0.0000) | Acc: (96.00%) (17445/18048)
Epoch: 189 | Batch_idx: 150 |  Loss: (0.0931) |  Loss2: (0.0000) | Acc: (96.00%) (18676/19328)
Epoch: 189 | Batch_idx: 160 |  Loss: (0.0930) |  Loss2: (0.0000) | Acc: (96.00%) (19921/20608)
Epoch: 189 | Batch_idx: 170 |  Loss: (0.0931) |  Loss2: (0.0000) | Acc: (96.00%) (21157/21888)
Epoch: 189 | Batch_idx: 180 |  Loss: (0.0931) |  Loss2: (0.0000) | Acc: (96.00%) (22397/23168)
Epoch: 189 | Batch_idx: 190 |  Loss: (0.0947) |  Loss2: (0.0000) | Acc: (96.00%) (23625/24448)
Epoch: 189 | Batch_idx: 200 |  Loss: (0.0942) |  Loss2: (0.0000) | Acc: (96.00%) (24869/25728)
Epoch: 189 | Batch_idx: 210 |  Loss: (0.0941) |  Loss2: (0.0000) | Acc: (96.00%) (26104/27008)
Epoch: 189 | Batch_idx: 220 |  Loss: (0.0939) |  Loss2: (0.0000) | Acc: (96.00%) (27345/28288)
Epoch: 189 | Batch_idx: 230 |  Loss: (0.0936) |  Loss2: (0.0000) | Acc: (96.00%) (28584/29568)
Epoch: 189 | Batch_idx: 240 |  Loss: (0.0935) |  Loss2: (0.0000) | Acc: (96.00%) (29827/30848)
Epoch: 189 | Batch_idx: 250 |  Loss: (0.0931) |  Loss2: (0.0000) | Acc: (96.00%) (31080/32128)
Epoch: 189 | Batch_idx: 260 |  Loss: (0.0931) |  Loss2: (0.0000) | Acc: (96.00%) (32317/33408)
Epoch: 189 | Batch_idx: 270 |  Loss: (0.0926) |  Loss2: (0.0000) | Acc: (96.00%) (33558/34688)
Epoch: 189 | Batch_idx: 280 |  Loss: (0.0925) |  Loss2: (0.0000) | Acc: (96.00%) (34803/35968)
Epoch: 189 | Batch_idx: 290 |  Loss: (0.0922) |  Loss2: (0.0000) | Acc: (96.00%) (36048/37248)
Epoch: 189 | Batch_idx: 300 |  Loss: (0.0924) |  Loss2: (0.0000) | Acc: (96.00%) (37282/38528)
Epoch: 189 | Batch_idx: 310 |  Loss: (0.0917) |  Loss2: (0.0000) | Acc: (96.00%) (38531/39808)
Epoch: 189 | Batch_idx: 320 |  Loss: (0.0913) |  Loss2: (0.0000) | Acc: (96.00%) (39782/41088)
Epoch: 189 | Batch_idx: 330 |  Loss: (0.0917) |  Loss2: (0.0000) | Acc: (96.00%) (41008/42368)
Epoch: 189 | Batch_idx: 340 |  Loss: (0.0919) |  Loss2: (0.0000) | Acc: (96.00%) (42248/43648)
Epoch: 189 | Batch_idx: 350 |  Loss: (0.0918) |  Loss2: (0.0000) | Acc: (96.00%) (43492/44928)
Epoch: 189 | Batch_idx: 360 |  Loss: (0.0912) |  Loss2: (0.0000) | Acc: (96.00%) (44736/46208)
Epoch: 189 | Batch_idx: 370 |  Loss: (0.0913) |  Loss2: (0.0000) | Acc: (96.00%) (45976/47488)
Epoch: 189 | Batch_idx: 380 |  Loss: (0.0908) |  Loss2: (0.0000) | Acc: (96.00%) (47230/48768)
Epoch: 189 | Batch_idx: 390 |  Loss: (0.0906) |  Loss2: (0.0000) | Acc: (96.00%) (48419/50000)
# TEST : Loss: (0.4056) | Acc: (88.00%) (8857/10000)
percent tensor([0.5830, 0.6036, 0.5873, 0.5786, 0.5934, 0.5644, 0.6047, 0.6010, 0.6022,
        0.5977, 0.5962, 0.5940, 0.5921, 0.6063, 0.5878, 0.5803],
       device='cuda:0') torch.Size([16])
percent tensor([0.5547, 0.5457, 0.5218, 0.5475, 0.5458, 0.5304, 0.5526, 0.5741, 0.5433,
        0.5237, 0.5194, 0.5116, 0.5463, 0.5573, 0.5564, 0.5421],
       device='cuda:0') torch.Size([16])
percent tensor([0.5564, 0.5095, 0.4933, 0.5662, 0.4801, 0.5971, 0.5246, 0.5419, 0.5368,
        0.4913, 0.5300, 0.4851, 0.4781, 0.6372, 0.5636, 0.5602],
       device='cuda:0') torch.Size([16])
percent tensor([0.6797, 0.7024, 0.6455, 0.6353, 0.6305, 0.6739, 0.6821, 0.6202, 0.6692,
        0.6983, 0.6940, 0.6774, 0.7253, 0.6688, 0.6840, 0.6895],
       device='cuda:0') torch.Size([16])
percent tensor([0.6599, 0.5704, 0.6870, 0.7135, 0.7321, 0.7639, 0.6838, 0.7000, 0.7445,
        0.6479, 0.6932, 0.6636, 0.5548, 0.7586, 0.6286, 0.7162],
       device='cuda:0') torch.Size([16])
percent tensor([0.7895, 0.7975, 0.8162, 0.8087, 0.8448, 0.8255, 0.8244, 0.8046, 0.8072,
        0.8078, 0.8189, 0.7877, 0.7916, 0.8301, 0.7872, 0.8158],
       device='cuda:0') torch.Size([16])
percent tensor([0.4486, 0.6038, 0.5594, 0.4962, 0.5873, 0.6688, 0.5260, 0.3935, 0.5570,
        0.5637, 0.5279, 0.4744, 0.5850, 0.5036, 0.3548, 0.4503],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9998, 0.9996, 0.9999, 0.9996, 0.9993, 0.9997, 0.9996, 0.9998,
        0.9998, 0.9999, 0.9998, 0.9996, 0.9987, 0.9992, 0.9995],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 190 | Batch_idx: 0 |  Loss: (0.0471) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 190 | Batch_idx: 10 |  Loss: (0.0613) |  Loss2: (0.0000) | Acc: (98.00%) (1380/1408)
Epoch: 190 | Batch_idx: 20 |  Loss: (0.0694) |  Loss2: (0.0000) | Acc: (97.00%) (2629/2688)
Epoch: 190 | Batch_idx: 30 |  Loss: (0.0693) |  Loss2: (0.0000) | Acc: (97.00%) (3884/3968)
Epoch: 190 | Batch_idx: 40 |  Loss: (0.0663) |  Loss2: (0.0000) | Acc: (98.00%) (5147/5248)
Epoch: 190 | Batch_idx: 50 |  Loss: (0.0635) |  Loss2: (0.0000) | Acc: (98.00%) (6407/6528)
Epoch: 190 | Batch_idx: 60 |  Loss: (0.0607) |  Loss2: (0.0000) | Acc: (98.00%) (7673/7808)
Epoch: 190 | Batch_idx: 70 |  Loss: (0.0609) |  Loss2: (0.0000) | Acc: (98.00%) (8929/9088)
Epoch: 190 | Batch_idx: 80 |  Loss: (0.0604) |  Loss2: (0.0000) | Acc: (98.00%) (10190/10368)
Epoch: 190 | Batch_idx: 90 |  Loss: (0.0622) |  Loss2: (0.0000) | Acc: (98.00%) (11440/11648)
Epoch: 190 | Batch_idx: 100 |  Loss: (0.0624) |  Loss2: (0.0000) | Acc: (98.00%) (12695/12928)
Epoch: 190 | Batch_idx: 110 |  Loss: (0.0638) |  Loss2: (0.0000) | Acc: (98.00%) (13952/14208)
Epoch: 190 | Batch_idx: 120 |  Loss: (0.0646) |  Loss2: (0.0000) | Acc: (98.00%) (15195/15488)
Epoch: 190 | Batch_idx: 130 |  Loss: (0.0648) |  Loss2: (0.0000) | Acc: (98.00%) (16446/16768)
Epoch: 190 | Batch_idx: 140 |  Loss: (0.0639) |  Loss2: (0.0000) | Acc: (98.00%) (17706/18048)
Epoch: 190 | Batch_idx: 150 |  Loss: (0.0652) |  Loss2: (0.0000) | Acc: (98.00%) (18952/19328)
Epoch: 190 | Batch_idx: 160 |  Loss: (0.0649) |  Loss2: (0.0000) | Acc: (98.00%) (20209/20608)
Epoch: 190 | Batch_idx: 170 |  Loss: (0.0659) |  Loss2: (0.0000) | Acc: (97.00%) (21450/21888)
Epoch: 190 | Batch_idx: 180 |  Loss: (0.0660) |  Loss2: (0.0000) | Acc: (97.00%) (22697/23168)
Epoch: 190 | Batch_idx: 190 |  Loss: (0.0659) |  Loss2: (0.0000) | Acc: (97.00%) (23949/24448)
Epoch: 190 | Batch_idx: 200 |  Loss: (0.0659) |  Loss2: (0.0000) | Acc: (97.00%) (25200/25728)
Epoch: 190 | Batch_idx: 210 |  Loss: (0.0660) |  Loss2: (0.0000) | Acc: (97.00%) (26450/27008)
Epoch: 190 | Batch_idx: 220 |  Loss: (0.0664) |  Loss2: (0.0000) | Acc: (97.00%) (27701/28288)
Epoch: 190 | Batch_idx: 230 |  Loss: (0.0675) |  Loss2: (0.0000) | Acc: (97.00%) (28946/29568)
Epoch: 190 | Batch_idx: 240 |  Loss: (0.0677) |  Loss2: (0.0000) | Acc: (97.00%) (30196/30848)
Epoch: 190 | Batch_idx: 250 |  Loss: (0.0681) |  Loss2: (0.0000) | Acc: (97.00%) (31443/32128)
Epoch: 190 | Batch_idx: 260 |  Loss: (0.0679) |  Loss2: (0.0000) | Acc: (97.00%) (32695/33408)
Epoch: 190 | Batch_idx: 270 |  Loss: (0.0676) |  Loss2: (0.0000) | Acc: (97.00%) (33957/34688)
Epoch: 190 | Batch_idx: 280 |  Loss: (0.0683) |  Loss2: (0.0000) | Acc: (97.00%) (35198/35968)
Epoch: 190 | Batch_idx: 290 |  Loss: (0.0684) |  Loss2: (0.0000) | Acc: (97.00%) (36451/37248)
Epoch: 190 | Batch_idx: 300 |  Loss: (0.0684) |  Loss2: (0.0000) | Acc: (97.00%) (37703/38528)
Epoch: 190 | Batch_idx: 310 |  Loss: (0.0685) |  Loss2: (0.0000) | Acc: (97.00%) (38953/39808)
Epoch: 190 | Batch_idx: 320 |  Loss: (0.0688) |  Loss2: (0.0000) | Acc: (97.00%) (40201/41088)
Epoch: 190 | Batch_idx: 330 |  Loss: (0.0687) |  Loss2: (0.0000) | Acc: (97.00%) (41451/42368)
Epoch: 190 | Batch_idx: 340 |  Loss: (0.0685) |  Loss2: (0.0000) | Acc: (97.00%) (42702/43648)
Epoch: 190 | Batch_idx: 350 |  Loss: (0.0690) |  Loss2: (0.0000) | Acc: (97.00%) (43947/44928)
Epoch: 190 | Batch_idx: 360 |  Loss: (0.0694) |  Loss2: (0.0000) | Acc: (97.00%) (45191/46208)
Epoch: 190 | Batch_idx: 370 |  Loss: (0.0696) |  Loss2: (0.0000) | Acc: (97.00%) (46445/47488)
Epoch: 190 | Batch_idx: 380 |  Loss: (0.0699) |  Loss2: (0.0000) | Acc: (97.00%) (47687/48768)
Epoch: 190 | Batch_idx: 390 |  Loss: (0.0700) |  Loss2: (0.0000) | Acc: (97.00%) (48888/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_190.pth.tar'
# TEST : Loss: (0.4049) | Acc: (88.00%) (8858/10000)
percent tensor([0.5856, 0.6036, 0.5910, 0.5803, 0.5982, 0.5672, 0.6074, 0.6021, 0.6035,
        0.5992, 0.5972, 0.5982, 0.5939, 0.6055, 0.5890, 0.5809],
       device='cuda:0') torch.Size([16])
percent tensor([0.5565, 0.5429, 0.5278, 0.5485, 0.5525, 0.5355, 0.5540, 0.5752, 0.5469,
        0.5245, 0.5210, 0.5157, 0.5466, 0.5519, 0.5568, 0.5427],
       device='cuda:0') torch.Size([16])
percent tensor([0.5571, 0.5107, 0.4829, 0.5663, 0.4814, 0.6044, 0.5236, 0.5416, 0.5414,
        0.4906, 0.5324, 0.4819, 0.4806, 0.6320, 0.5678, 0.5686],
       device='cuda:0') torch.Size([16])
percent tensor([0.6777, 0.6976, 0.6448, 0.6354, 0.6301, 0.6714, 0.6785, 0.6237, 0.6680,
        0.6973, 0.6893, 0.6776, 0.7218, 0.6634, 0.6829, 0.6893],
       device='cuda:0') torch.Size([16])
percent tensor([0.6837, 0.5794, 0.6996, 0.7141, 0.7232, 0.7847, 0.6727, 0.6851, 0.7527,
        0.6507, 0.7064, 0.6864, 0.5758, 0.7629, 0.6358, 0.7216],
       device='cuda:0') torch.Size([16])
percent tensor([0.7934, 0.8033, 0.8183, 0.8093, 0.8409, 0.8271, 0.8150, 0.8030, 0.8061,
        0.8082, 0.8186, 0.7849, 0.7942, 0.8350, 0.7868, 0.8166],
       device='cuda:0') torch.Size([16])
percent tensor([0.4415, 0.6312, 0.5858, 0.5229, 0.5894, 0.7062, 0.5238, 0.3693, 0.5534,
        0.5583, 0.5308, 0.4951, 0.5733, 0.5393, 0.3641, 0.4677],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9997, 0.9995, 0.9998, 0.9996, 0.9994, 0.9997, 0.9995, 0.9996,
        0.9997, 0.9999, 0.9999, 0.9996, 0.9991, 0.9991, 0.9993],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(181.3590, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(823.5388, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(810.6650, device='cuda:0')
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1495.6409, device='cuda:0')
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(480.0189, device='cuda:0')
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2283.7949, device='cuda:0')
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4279.7979, device='cuda:0')
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1353.3796, device='cuda:0')
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6297.4609, device='cuda:0')
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11547.9375, device='cuda:0')
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3800.7454, device='cuda:0')
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16026.5850, device='cuda:0')
Epoch: 191 | Batch_idx: 0 |  Loss: (0.0560) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 191 | Batch_idx: 10 |  Loss: (0.0678) |  Loss2: (0.0000) | Acc: (97.00%) (1377/1408)
Epoch: 191 | Batch_idx: 20 |  Loss: (0.0714) |  Loss2: (0.0000) | Acc: (97.00%) (2630/2688)
Epoch: 191 | Batch_idx: 30 |  Loss: (0.0728) |  Loss2: (0.0000) | Acc: (97.00%) (3878/3968)
Epoch: 191 | Batch_idx: 40 |  Loss: (0.0745) |  Loss2: (0.0000) | Acc: (97.00%) (5125/5248)
Epoch: 191 | Batch_idx: 50 |  Loss: (0.0766) |  Loss2: (0.0000) | Acc: (97.00%) (6363/6528)
Epoch: 191 | Batch_idx: 60 |  Loss: (0.0764) |  Loss2: (0.0000) | Acc: (97.00%) (7616/7808)
Epoch: 191 | Batch_idx: 70 |  Loss: (0.0752) |  Loss2: (0.0000) | Acc: (97.00%) (8870/9088)
Epoch: 191 | Batch_idx: 80 |  Loss: (0.0773) |  Loss2: (0.0000) | Acc: (97.00%) (10113/10368)
Epoch: 191 | Batch_idx: 90 |  Loss: (0.0775) |  Loss2: (0.0000) | Acc: (97.00%) (11354/11648)
Epoch: 191 | Batch_idx: 100 |  Loss: (0.0794) |  Loss2: (0.0000) | Acc: (97.00%) (12588/12928)
Epoch: 191 | Batch_idx: 110 |  Loss: (0.0800) |  Loss2: (0.0000) | Acc: (97.00%) (13830/14208)
Epoch: 191 | Batch_idx: 120 |  Loss: (0.0802) |  Loss2: (0.0000) | Acc: (97.00%) (15078/15488)
Epoch: 191 | Batch_idx: 130 |  Loss: (0.0804) |  Loss2: (0.0000) | Acc: (97.00%) (16323/16768)
Epoch: 191 | Batch_idx: 140 |  Loss: (0.0797) |  Loss2: (0.0000) | Acc: (97.00%) (17574/18048)
Epoch: 191 | Batch_idx: 150 |  Loss: (0.0791) |  Loss2: (0.0000) | Acc: (97.00%) (18827/19328)
Epoch: 191 | Batch_idx: 160 |  Loss: (0.0795) |  Loss2: (0.0000) | Acc: (97.00%) (20073/20608)
Epoch: 191 | Batch_idx: 170 |  Loss: (0.0793) |  Loss2: (0.0000) | Acc: (97.00%) (21319/21888)
Epoch: 191 | Batch_idx: 180 |  Loss: (0.0796) |  Loss2: (0.0000) | Acc: (97.00%) (22556/23168)
Epoch: 191 | Batch_idx: 190 |  Loss: (0.0791) |  Loss2: (0.0000) | Acc: (97.00%) (23802/24448)
Epoch: 191 | Batch_idx: 200 |  Loss: (0.0788) |  Loss2: (0.0000) | Acc: (97.00%) (25055/25728)
Epoch: 191 | Batch_idx: 210 |  Loss: (0.0789) |  Loss2: (0.0000) | Acc: (97.00%) (26299/27008)
Epoch: 191 | Batch_idx: 220 |  Loss: (0.0790) |  Loss2: (0.0000) | Acc: (97.00%) (27545/28288)
Epoch: 191 | Batch_idx: 230 |  Loss: (0.0789) |  Loss2: (0.0000) | Acc: (97.00%) (28792/29568)
Epoch: 191 | Batch_idx: 240 |  Loss: (0.0784) |  Loss2: (0.0000) | Acc: (97.00%) (30037/30848)
Epoch: 191 | Batch_idx: 250 |  Loss: (0.0788) |  Loss2: (0.0000) | Acc: (97.00%) (31280/32128)
Epoch: 191 | Batch_idx: 260 |  Loss: (0.0781) |  Loss2: (0.0000) | Acc: (97.00%) (32538/33408)
Epoch: 191 | Batch_idx: 270 |  Loss: (0.0786) |  Loss2: (0.0000) | Acc: (97.00%) (33775/34688)
Epoch: 191 | Batch_idx: 280 |  Loss: (0.0781) |  Loss2: (0.0000) | Acc: (97.00%) (35026/35968)
Epoch: 191 | Batch_idx: 290 |  Loss: (0.0780) |  Loss2: (0.0000) | Acc: (97.00%) (36278/37248)
Epoch: 191 | Batch_idx: 300 |  Loss: (0.0782) |  Loss2: (0.0000) | Acc: (97.00%) (37524/38528)
Epoch: 191 | Batch_idx: 310 |  Loss: (0.0781) |  Loss2: (0.0000) | Acc: (97.00%) (38773/39808)
Epoch: 191 | Batch_idx: 320 |  Loss: (0.0776) |  Loss2: (0.0000) | Acc: (97.00%) (40025/41088)
Epoch: 191 | Batch_idx: 330 |  Loss: (0.0776) |  Loss2: (0.0000) | Acc: (97.00%) (41276/42368)
Epoch: 191 | Batch_idx: 340 |  Loss: (0.0769) |  Loss2: (0.0000) | Acc: (97.00%) (42533/43648)
Epoch: 191 | Batch_idx: 350 |  Loss: (0.0767) |  Loss2: (0.0000) | Acc: (97.00%) (43781/44928)
Epoch: 191 | Batch_idx: 360 |  Loss: (0.0766) |  Loss2: (0.0000) | Acc: (97.00%) (45031/46208)
Epoch: 191 | Batch_idx: 370 |  Loss: (0.0769) |  Loss2: (0.0000) | Acc: (97.00%) (46273/47488)
Epoch: 191 | Batch_idx: 380 |  Loss: (0.0772) |  Loss2: (0.0000) | Acc: (97.00%) (47513/48768)
Epoch: 191 | Batch_idx: 390 |  Loss: (0.0767) |  Loss2: (0.0000) | Acc: (97.00%) (48723/50000)
# TEST : Loss: (0.3944) | Acc: (88.00%) (8887/10000)
percent tensor([0.5843, 0.6034, 0.5884, 0.5781, 0.5953, 0.5651, 0.6069, 0.6016, 0.6025,
        0.5985, 0.5964, 0.5963, 0.5929, 0.6056, 0.5883, 0.5801],
       device='cuda:0') torch.Size([16])
percent tensor([0.5579, 0.5475, 0.5284, 0.5472, 0.5544, 0.5332, 0.5580, 0.5784, 0.5478,
        0.5276, 0.5222, 0.5160, 0.5496, 0.5518, 0.5584, 0.5433],
       device='cuda:0') torch.Size([16])
percent tensor([0.5558, 0.5114, 0.4807, 0.5608, 0.4735, 0.5987, 0.5222, 0.5364, 0.5387,
        0.4922, 0.5326, 0.4810, 0.4803, 0.6337, 0.5635, 0.5646],
       device='cuda:0') torch.Size([16])
percent tensor([0.6768, 0.7020, 0.6422, 0.6307, 0.6271, 0.6689, 0.6802, 0.6186, 0.6671,
        0.6997, 0.6938, 0.6748, 0.7251, 0.6655, 0.6836, 0.6903],
       device='cuda:0') torch.Size([16])
percent tensor([0.7073, 0.5984, 0.7242, 0.7347, 0.7459, 0.8121, 0.6948, 0.6998, 0.7676,
        0.6767, 0.7253, 0.7092, 0.6014, 0.7796, 0.6501, 0.7537],
       device='cuda:0') torch.Size([16])
percent tensor([0.8026, 0.8112, 0.8264, 0.8192, 0.8505, 0.8353, 0.8253, 0.8126, 0.8155,
        0.8184, 0.8282, 0.7965, 0.8062, 0.8421, 0.7965, 0.8245],
       device='cuda:0') torch.Size([16])
percent tensor([0.3985, 0.6079, 0.5799, 0.5061, 0.5751, 0.7011, 0.4874, 0.3341, 0.5516,
        0.5303, 0.5255, 0.4810, 0.5630, 0.5040, 0.3503, 0.4049],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9998, 0.9996, 0.9998, 0.9996, 0.9991, 0.9997, 0.9995, 0.9998,
        0.9997, 1.0000, 0.9999, 0.9997, 0.9991, 0.9990, 0.9993],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 192 | Batch_idx: 0 |  Loss: (0.0475) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 192 | Batch_idx: 10 |  Loss: (0.0615) |  Loss2: (0.0000) | Acc: (97.00%) (1377/1408)
Epoch: 192 | Batch_idx: 20 |  Loss: (0.0587) |  Loss2: (0.0000) | Acc: (97.00%) (2634/2688)
Epoch: 192 | Batch_idx: 30 |  Loss: (0.0616) |  Loss2: (0.0000) | Acc: (98.00%) (3892/3968)
Epoch: 192 | Batch_idx: 40 |  Loss: (0.0630) |  Loss2: (0.0000) | Acc: (97.00%) (5143/5248)
Epoch: 192 | Batch_idx: 50 |  Loss: (0.0628) |  Loss2: (0.0000) | Acc: (98.00%) (6402/6528)
Epoch: 192 | Batch_idx: 60 |  Loss: (0.0641) |  Loss2: (0.0000) | Acc: (98.00%) (7652/7808)
Epoch: 192 | Batch_idx: 70 |  Loss: (0.0636) |  Loss2: (0.0000) | Acc: (97.00%) (8905/9088)
Epoch: 192 | Batch_idx: 80 |  Loss: (0.0643) |  Loss2: (0.0000) | Acc: (97.00%) (10155/10368)
Epoch: 192 | Batch_idx: 90 |  Loss: (0.0653) |  Loss2: (0.0000) | Acc: (97.00%) (11404/11648)
Epoch: 192 | Batch_idx: 100 |  Loss: (0.0649) |  Loss2: (0.0000) | Acc: (97.00%) (12661/12928)
Epoch: 192 | Batch_idx: 110 |  Loss: (0.0642) |  Loss2: (0.0000) | Acc: (97.00%) (13917/14208)
Epoch: 192 | Batch_idx: 120 |  Loss: (0.0639) |  Loss2: (0.0000) | Acc: (97.00%) (15173/15488)
Epoch: 192 | Batch_idx: 130 |  Loss: (0.0641) |  Loss2: (0.0000) | Acc: (97.00%) (16422/16768)
Epoch: 192 | Batch_idx: 140 |  Loss: (0.0643) |  Loss2: (0.0000) | Acc: (97.00%) (17672/18048)
Epoch: 192 | Batch_idx: 150 |  Loss: (0.0641) |  Loss2: (0.0000) | Acc: (97.00%) (18922/19328)
Epoch: 192 | Batch_idx: 160 |  Loss: (0.0640) |  Loss2: (0.0000) | Acc: (97.00%) (20176/20608)
Epoch: 192 | Batch_idx: 170 |  Loss: (0.0647) |  Loss2: (0.0000) | Acc: (97.00%) (21421/21888)
Epoch: 192 | Batch_idx: 180 |  Loss: (0.0650) |  Loss2: (0.0000) | Acc: (97.00%) (22672/23168)
Epoch: 192 | Batch_idx: 190 |  Loss: (0.0652) |  Loss2: (0.0000) | Acc: (97.00%) (23928/24448)
Epoch: 192 | Batch_idx: 200 |  Loss: (0.0647) |  Loss2: (0.0000) | Acc: (97.00%) (25187/25728)
Epoch: 192 | Batch_idx: 210 |  Loss: (0.0653) |  Loss2: (0.0000) | Acc: (97.00%) (26435/27008)
Epoch: 192 | Batch_idx: 220 |  Loss: (0.0657) |  Loss2: (0.0000) | Acc: (97.00%) (27688/28288)
Epoch: 192 | Batch_idx: 230 |  Loss: (0.0661) |  Loss2: (0.0000) | Acc: (97.00%) (28935/29568)
Epoch: 192 | Batch_idx: 240 |  Loss: (0.0664) |  Loss2: (0.0000) | Acc: (97.00%) (30182/30848)
Epoch: 192 | Batch_idx: 250 |  Loss: (0.0666) |  Loss2: (0.0000) | Acc: (97.00%) (31431/32128)
Epoch: 192 | Batch_idx: 260 |  Loss: (0.0668) |  Loss2: (0.0000) | Acc: (97.00%) (32681/33408)
Epoch: 192 | Batch_idx: 270 |  Loss: (0.0671) |  Loss2: (0.0000) | Acc: (97.00%) (33929/34688)
Epoch: 192 | Batch_idx: 280 |  Loss: (0.0671) |  Loss2: (0.0000) | Acc: (97.00%) (35173/35968)
Epoch: 192 | Batch_idx: 290 |  Loss: (0.0668) |  Loss2: (0.0000) | Acc: (97.00%) (36422/37248)
Epoch: 192 | Batch_idx: 300 |  Loss: (0.0669) |  Loss2: (0.0000) | Acc: (97.00%) (37673/38528)
Epoch: 192 | Batch_idx: 310 |  Loss: (0.0672) |  Loss2: (0.0000) | Acc: (97.00%) (38921/39808)
Epoch: 192 | Batch_idx: 320 |  Loss: (0.0674) |  Loss2: (0.0000) | Acc: (97.00%) (40172/41088)
Epoch: 192 | Batch_idx: 330 |  Loss: (0.0675) |  Loss2: (0.0000) | Acc: (97.00%) (41412/42368)
Epoch: 192 | Batch_idx: 340 |  Loss: (0.0674) |  Loss2: (0.0000) | Acc: (97.00%) (42667/43648)
Epoch: 192 | Batch_idx: 350 |  Loss: (0.0676) |  Loss2: (0.0000) | Acc: (97.00%) (43915/44928)
Epoch: 192 | Batch_idx: 360 |  Loss: (0.0679) |  Loss2: (0.0000) | Acc: (97.00%) (45160/46208)
Epoch: 192 | Batch_idx: 370 |  Loss: (0.0681) |  Loss2: (0.0000) | Acc: (97.00%) (46410/47488)
Epoch: 192 | Batch_idx: 380 |  Loss: (0.0687) |  Loss2: (0.0000) | Acc: (97.00%) (47647/48768)
Epoch: 192 | Batch_idx: 390 |  Loss: (0.0686) |  Loss2: (0.0000) | Acc: (97.00%) (48855/50000)
# TEST : Loss: (0.4096) | Acc: (88.00%) (8845/10000)
percent tensor([0.5840, 0.6027, 0.5894, 0.5781, 0.5970, 0.5653, 0.6060, 0.6013, 0.6021,
        0.5981, 0.5955, 0.5968, 0.5924, 0.6048, 0.5874, 0.5797],
       device='cuda:0') torch.Size([16])
percent tensor([0.5567, 0.5486, 0.5254, 0.5485, 0.5500, 0.5281, 0.5575, 0.5788, 0.5487,
        0.5264, 0.5225, 0.5134, 0.5498, 0.5569, 0.5564, 0.5442],
       device='cuda:0') torch.Size([16])
percent tensor([0.5533, 0.5161, 0.4923, 0.5697, 0.4795, 0.5939, 0.5261, 0.5455, 0.5377,
        0.4965, 0.5342, 0.4886, 0.4806, 0.6365, 0.5625, 0.5615],
       device='cuda:0') torch.Size([16])
percent tensor([0.6789, 0.7023, 0.6510, 0.6314, 0.6271, 0.6691, 0.6781, 0.6217, 0.6716,
        0.7007, 0.6967, 0.6779, 0.7280, 0.6667, 0.6830, 0.6873],
       device='cuda:0') torch.Size([16])
percent tensor([0.6957, 0.5988, 0.7079, 0.7375, 0.7434, 0.8000, 0.7099, 0.7040, 0.7633,
        0.6876, 0.7191, 0.7062, 0.5924, 0.7938, 0.6494, 0.7575],
       device='cuda:0') torch.Size([16])
percent tensor([0.7976, 0.8030, 0.8313, 0.8182, 0.8524, 0.8221, 0.8270, 0.8136, 0.8131,
        0.8196, 0.8259, 0.7993, 0.8048, 0.8408, 0.7953, 0.8232],
       device='cuda:0') torch.Size([16])
percent tensor([0.4091, 0.5800, 0.5807, 0.4964, 0.5809, 0.6628, 0.4940, 0.3337, 0.5645,
        0.5352, 0.5305, 0.4739, 0.5788, 0.4650, 0.3417, 0.3913],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9998, 0.9996, 0.9999, 0.9998, 0.9986, 0.9997, 0.9995, 0.9998,
        0.9997, 1.0000, 0.9998, 0.9997, 0.9992, 0.9991, 0.9993],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 193 | Batch_idx: 0 |  Loss: (0.1114) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 193 | Batch_idx: 10 |  Loss: (0.0794) |  Loss2: (0.0000) | Acc: (96.00%) (1365/1408)
Epoch: 193 | Batch_idx: 20 |  Loss: (0.0789) |  Loss2: (0.0000) | Acc: (96.00%) (2607/2688)
Epoch: 193 | Batch_idx: 30 |  Loss: (0.0834) |  Loss2: (0.0000) | Acc: (96.00%) (3841/3968)
Epoch: 193 | Batch_idx: 40 |  Loss: (0.0888) |  Loss2: (0.0000) | Acc: (96.00%) (5079/5248)
Epoch: 193 | Batch_idx: 50 |  Loss: (0.0893) |  Loss2: (0.0000) | Acc: (96.00%) (6318/6528)
Epoch: 193 | Batch_idx: 60 |  Loss: (0.0917) |  Loss2: (0.0000) | Acc: (96.00%) (7544/7808)
Epoch: 193 | Batch_idx: 70 |  Loss: (0.0915) |  Loss2: (0.0000) | Acc: (96.00%) (8776/9088)
Epoch: 193 | Batch_idx: 80 |  Loss: (0.0933) |  Loss2: (0.0000) | Acc: (96.00%) (10007/10368)
Epoch: 193 | Batch_idx: 90 |  Loss: (0.0933) |  Loss2: (0.0000) | Acc: (96.00%) (11244/11648)
Epoch: 193 | Batch_idx: 100 |  Loss: (0.0940) |  Loss2: (0.0000) | Acc: (96.00%) (12477/12928)
Epoch: 193 | Batch_idx: 110 |  Loss: (0.0940) |  Loss2: (0.0000) | Acc: (96.00%) (13711/14208)
Epoch: 193 | Batch_idx: 120 |  Loss: (0.0939) |  Loss2: (0.0000) | Acc: (96.00%) (14946/15488)
Epoch: 193 | Batch_idx: 130 |  Loss: (0.0933) |  Loss2: (0.0000) | Acc: (96.00%) (16189/16768)
Epoch: 193 | Batch_idx: 140 |  Loss: (0.0937) |  Loss2: (0.0000) | Acc: (96.00%) (17420/18048)
Epoch: 193 | Batch_idx: 150 |  Loss: (0.0933) |  Loss2: (0.0000) | Acc: (96.00%) (18663/19328)
Epoch: 193 | Batch_idx: 160 |  Loss: (0.0936) |  Loss2: (0.0000) | Acc: (96.00%) (19901/20608)
Epoch: 193 | Batch_idx: 170 |  Loss: (0.0935) |  Loss2: (0.0000) | Acc: (96.00%) (21135/21888)
Epoch: 193 | Batch_idx: 180 |  Loss: (0.0947) |  Loss2: (0.0000) | Acc: (96.00%) (22356/23168)
Epoch: 193 | Batch_idx: 190 |  Loss: (0.0944) |  Loss2: (0.0000) | Acc: (96.00%) (23594/24448)
Epoch: 193 | Batch_idx: 200 |  Loss: (0.0937) |  Loss2: (0.0000) | Acc: (96.00%) (24838/25728)
Epoch: 193 | Batch_idx: 210 |  Loss: (0.0935) |  Loss2: (0.0000) | Acc: (96.00%) (26080/27008)
Epoch: 193 | Batch_idx: 220 |  Loss: (0.0940) |  Loss2: (0.0000) | Acc: (96.00%) (27315/28288)
Epoch: 193 | Batch_idx: 230 |  Loss: (0.0946) |  Loss2: (0.0000) | Acc: (96.00%) (28542/29568)
Epoch: 193 | Batch_idx: 240 |  Loss: (0.0947) |  Loss2: (0.0000) | Acc: (96.00%) (29773/30848)
Epoch: 193 | Batch_idx: 250 |  Loss: (0.0938) |  Loss2: (0.0000) | Acc: (96.00%) (31022/32128)
Epoch: 193 | Batch_idx: 260 |  Loss: (0.0935) |  Loss2: (0.0000) | Acc: (96.00%) (32265/33408)
Epoch: 193 | Batch_idx: 270 |  Loss: (0.0929) |  Loss2: (0.0000) | Acc: (96.00%) (33509/34688)
Epoch: 193 | Batch_idx: 280 |  Loss: (0.0931) |  Loss2: (0.0000) | Acc: (96.00%) (34744/35968)
Epoch: 193 | Batch_idx: 290 |  Loss: (0.0934) |  Loss2: (0.0000) | Acc: (96.00%) (35981/37248)
Epoch: 193 | Batch_idx: 300 |  Loss: (0.0936) |  Loss2: (0.0000) | Acc: (96.00%) (37224/38528)
Epoch: 193 | Batch_idx: 310 |  Loss: (0.0932) |  Loss2: (0.0000) | Acc: (96.00%) (38469/39808)
Epoch: 193 | Batch_idx: 320 |  Loss: (0.0934) |  Loss2: (0.0000) | Acc: (96.00%) (39703/41088)
Epoch: 193 | Batch_idx: 330 |  Loss: (0.0929) |  Loss2: (0.0000) | Acc: (96.00%) (40951/42368)
Epoch: 193 | Batch_idx: 340 |  Loss: (0.0925) |  Loss2: (0.0000) | Acc: (96.00%) (42199/43648)
Epoch: 193 | Batch_idx: 350 |  Loss: (0.0921) |  Loss2: (0.0000) | Acc: (96.00%) (43445/44928)
Epoch: 193 | Batch_idx: 360 |  Loss: (0.0915) |  Loss2: (0.0000) | Acc: (96.00%) (44691/46208)
Epoch: 193 | Batch_idx: 370 |  Loss: (0.0914) |  Loss2: (0.0000) | Acc: (96.00%) (45928/47488)
Epoch: 193 | Batch_idx: 380 |  Loss: (0.0909) |  Loss2: (0.0000) | Acc: (96.00%) (47177/48768)
Epoch: 193 | Batch_idx: 390 |  Loss: (0.0909) |  Loss2: (0.0000) | Acc: (96.00%) (48366/50000)
# TEST : Loss: (0.4189) | Acc: (88.00%) (8847/10000)
percent tensor([0.5787, 0.5946, 0.5855, 0.5732, 0.5919, 0.5601, 0.5986, 0.5952, 0.5954,
        0.5918, 0.5886, 0.5915, 0.5864, 0.5960, 0.5807, 0.5737],
       device='cuda:0') torch.Size([16])
percent tensor([0.5709, 0.5616, 0.5357, 0.5628, 0.5599, 0.5410, 0.5701, 0.5917, 0.5619,
        0.5397, 0.5371, 0.5248, 0.5633, 0.5739, 0.5701, 0.5567],
       device='cuda:0') torch.Size([16])
percent tensor([0.5465, 0.5020, 0.4901, 0.5692, 0.4754, 0.5874, 0.5173, 0.5437, 0.5323,
        0.4860, 0.5248, 0.4778, 0.4678, 0.6321, 0.5557, 0.5516],
       device='cuda:0') torch.Size([16])
percent tensor([0.6742, 0.6994, 0.6421, 0.6209, 0.6199, 0.6594, 0.6743, 0.6128, 0.6683,
        0.6979, 0.6937, 0.6692, 0.7265, 0.6613, 0.6746, 0.6818],
       device='cuda:0') torch.Size([16])
percent tensor([0.6800, 0.5935, 0.7004, 0.7220, 0.7274, 0.7715, 0.6994, 0.6880, 0.7468,
        0.6777, 0.7006, 0.6844, 0.5842, 0.7786, 0.6273, 0.7393],
       device='cuda:0') torch.Size([16])
percent tensor([0.7938, 0.7997, 0.8266, 0.8169, 0.8466, 0.8217, 0.8239, 0.8088, 0.8082,
        0.8151, 0.8207, 0.7975, 0.8023, 0.8339, 0.7913, 0.8202],
       device='cuda:0') torch.Size([16])
percent tensor([0.4034, 0.5574, 0.5475, 0.4625, 0.5517, 0.6688, 0.4793, 0.3265, 0.5422,
        0.5231, 0.5156, 0.4741, 0.5616, 0.4624, 0.3475, 0.4006],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9998, 0.9997, 0.9999, 0.9998, 0.9990, 0.9997, 0.9995, 0.9998,
        0.9997, 0.9999, 0.9998, 0.9997, 0.9994, 0.9992, 0.9993],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 194 | Batch_idx: 0 |  Loss: (0.0901) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 194 | Batch_idx: 10 |  Loss: (0.0594) |  Loss2: (0.0000) | Acc: (98.00%) (1381/1408)
Epoch: 194 | Batch_idx: 20 |  Loss: (0.0743) |  Loss2: (0.0000) | Acc: (97.00%) (2621/2688)
Epoch: 194 | Batch_idx: 30 |  Loss: (0.0715) |  Loss2: (0.0000) | Acc: (97.00%) (3873/3968)
Epoch: 194 | Batch_idx: 40 |  Loss: (0.0716) |  Loss2: (0.0000) | Acc: (97.00%) (5118/5248)
Epoch: 194 | Batch_idx: 50 |  Loss: (0.0683) |  Loss2: (0.0000) | Acc: (97.00%) (6379/6528)
Epoch: 194 | Batch_idx: 60 |  Loss: (0.0670) |  Loss2: (0.0000) | Acc: (97.00%) (7632/7808)
Epoch: 194 | Batch_idx: 70 |  Loss: (0.0665) |  Loss2: (0.0000) | Acc: (97.00%) (8887/9088)
Epoch: 194 | Batch_idx: 80 |  Loss: (0.0638) |  Loss2: (0.0000) | Acc: (97.00%) (10151/10368)
Epoch: 194 | Batch_idx: 90 |  Loss: (0.0638) |  Loss2: (0.0000) | Acc: (97.00%) (11401/11648)
Epoch: 194 | Batch_idx: 100 |  Loss: (0.0643) |  Loss2: (0.0000) | Acc: (97.00%) (12652/12928)
Epoch: 194 | Batch_idx: 110 |  Loss: (0.0652) |  Loss2: (0.0000) | Acc: (97.00%) (13898/14208)
Epoch: 194 | Batch_idx: 120 |  Loss: (0.0655) |  Loss2: (0.0000) | Acc: (97.00%) (15146/15488)
Epoch: 194 | Batch_idx: 130 |  Loss: (0.0657) |  Loss2: (0.0000) | Acc: (97.00%) (16401/16768)
Epoch: 194 | Batch_idx: 140 |  Loss: (0.0654) |  Loss2: (0.0000) | Acc: (97.00%) (17653/18048)
Epoch: 194 | Batch_idx: 150 |  Loss: (0.0651) |  Loss2: (0.0000) | Acc: (97.00%) (18911/19328)
Epoch: 194 | Batch_idx: 160 |  Loss: (0.0661) |  Loss2: (0.0000) | Acc: (97.00%) (20158/20608)
Epoch: 194 | Batch_idx: 170 |  Loss: (0.0661) |  Loss2: (0.0000) | Acc: (97.00%) (21410/21888)
Epoch: 194 | Batch_idx: 180 |  Loss: (0.0655) |  Loss2: (0.0000) | Acc: (97.00%) (22667/23168)
Epoch: 194 | Batch_idx: 190 |  Loss: (0.0647) |  Loss2: (0.0000) | Acc: (97.00%) (23927/24448)
Epoch: 194 | Batch_idx: 200 |  Loss: (0.0654) |  Loss2: (0.0000) | Acc: (97.00%) (25173/25728)
Epoch: 194 | Batch_idx: 210 |  Loss: (0.0652) |  Loss2: (0.0000) | Acc: (97.00%) (26428/27008)
Epoch: 194 | Batch_idx: 220 |  Loss: (0.0651) |  Loss2: (0.0000) | Acc: (97.00%) (27683/28288)
Epoch: 194 | Batch_idx: 230 |  Loss: (0.0655) |  Loss2: (0.0000) | Acc: (97.00%) (28926/29568)
Epoch: 194 | Batch_idx: 240 |  Loss: (0.0652) |  Loss2: (0.0000) | Acc: (97.00%) (30184/30848)
Epoch: 194 | Batch_idx: 250 |  Loss: (0.0656) |  Loss2: (0.0000) | Acc: (97.00%) (31435/32128)
Epoch: 194 | Batch_idx: 260 |  Loss: (0.0654) |  Loss2: (0.0000) | Acc: (97.00%) (32692/33408)
Epoch: 194 | Batch_idx: 270 |  Loss: (0.0661) |  Loss2: (0.0000) | Acc: (97.00%) (33941/34688)
Epoch: 194 | Batch_idx: 280 |  Loss: (0.0663) |  Loss2: (0.0000) | Acc: (97.00%) (35191/35968)
Epoch: 194 | Batch_idx: 290 |  Loss: (0.0664) |  Loss2: (0.0000) | Acc: (97.00%) (36443/37248)
Epoch: 194 | Batch_idx: 300 |  Loss: (0.0666) |  Loss2: (0.0000) | Acc: (97.00%) (37689/38528)
Epoch: 194 | Batch_idx: 310 |  Loss: (0.0667) |  Loss2: (0.0000) | Acc: (97.00%) (38941/39808)
Epoch: 194 | Batch_idx: 320 |  Loss: (0.0673) |  Loss2: (0.0000) | Acc: (97.00%) (40180/41088)
Epoch: 194 | Batch_idx: 330 |  Loss: (0.0673) |  Loss2: (0.0000) | Acc: (97.00%) (41433/42368)
Epoch: 194 | Batch_idx: 340 |  Loss: (0.0674) |  Loss2: (0.0000) | Acc: (97.00%) (42689/43648)
Epoch: 194 | Batch_idx: 350 |  Loss: (0.0678) |  Loss2: (0.0000) | Acc: (97.00%) (43933/44928)
Epoch: 194 | Batch_idx: 360 |  Loss: (0.0677) |  Loss2: (0.0000) | Acc: (97.00%) (45185/46208)
Epoch: 194 | Batch_idx: 370 |  Loss: (0.0678) |  Loss2: (0.0000) | Acc: (97.00%) (46434/47488)
Epoch: 194 | Batch_idx: 380 |  Loss: (0.0677) |  Loss2: (0.0000) | Acc: (97.00%) (47692/48768)
Epoch: 194 | Batch_idx: 390 |  Loss: (0.0677) |  Loss2: (0.0000) | Acc: (97.00%) (48896/50000)
# TEST : Loss: (0.4099) | Acc: (88.00%) (8849/10000)
percent tensor([0.5780, 0.5931, 0.5842, 0.5721, 0.5899, 0.5592, 0.5971, 0.5953, 0.5955,
        0.5905, 0.5888, 0.5896, 0.5856, 0.5944, 0.5799, 0.5733],
       device='cuda:0') torch.Size([16])
percent tensor([0.5687, 0.5555, 0.5448, 0.5647, 0.5662, 0.5379, 0.5688, 0.5940, 0.5576,
        0.5382, 0.5290, 0.5316, 0.5608, 0.5658, 0.5654, 0.5536],
       device='cuda:0') torch.Size([16])
percent tensor([0.5446, 0.4987, 0.4879, 0.5605, 0.4770, 0.5786, 0.5128, 0.5465, 0.5323,
        0.4856, 0.5198, 0.4764, 0.4704, 0.6279, 0.5466, 0.5499],
       device='cuda:0') torch.Size([16])
percent tensor([0.6714, 0.6962, 0.6398, 0.6190, 0.6221, 0.6618, 0.6762, 0.6064, 0.6640,
        0.6946, 0.6894, 0.6712, 0.7211, 0.6601, 0.6717, 0.6795],
       device='cuda:0') torch.Size([16])
percent tensor([0.6895, 0.5988, 0.7016, 0.7105, 0.7248, 0.7696, 0.6900, 0.6930, 0.7549,
        0.6801, 0.7117, 0.6834, 0.6025, 0.7699, 0.6242, 0.7329],
       device='cuda:0') torch.Size([16])
percent tensor([0.7904, 0.7972, 0.8181, 0.8161, 0.8434, 0.8190, 0.8188, 0.8101, 0.8077,
        0.8112, 0.8201, 0.7950, 0.7968, 0.8327, 0.7879, 0.8124],
       device='cuda:0') torch.Size([16])
percent tensor([0.3819, 0.5576, 0.5677, 0.4768, 0.5878, 0.6553, 0.4695, 0.3361, 0.5557,
        0.5093, 0.5095, 0.4766, 0.5447, 0.4599, 0.3514, 0.3851],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9998, 0.9996, 0.9999, 0.9996, 0.9993, 0.9998, 0.9995, 0.9999,
        0.9997, 1.0000, 0.9998, 0.9998, 0.9994, 0.9993, 0.9994],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 195 | Batch_idx: 0 |  Loss: (0.0332) |  Loss2: (0.0000) | Acc: (100.00%) (128/128)
Epoch: 195 | Batch_idx: 10 |  Loss: (0.0616) |  Loss2: (0.0000) | Acc: (98.00%) (1382/1408)
Epoch: 195 | Batch_idx: 20 |  Loss: (0.0698) |  Loss2: (0.0000) | Acc: (97.00%) (2628/2688)
Epoch: 195 | Batch_idx: 30 |  Loss: (0.0734) |  Loss2: (0.0000) | Acc: (97.00%) (3873/3968)
Epoch: 195 | Batch_idx: 40 |  Loss: (0.0730) |  Loss2: (0.0000) | Acc: (97.00%) (5127/5248)
Epoch: 195 | Batch_idx: 50 |  Loss: (0.0772) |  Loss2: (0.0000) | Acc: (97.00%) (6365/6528)
Epoch: 195 | Batch_idx: 60 |  Loss: (0.0759) |  Loss2: (0.0000) | Acc: (97.00%) (7617/7808)
Epoch: 195 | Batch_idx: 70 |  Loss: (0.0764) |  Loss2: (0.0000) | Acc: (97.00%) (8866/9088)
Epoch: 195 | Batch_idx: 80 |  Loss: (0.0779) |  Loss2: (0.0000) | Acc: (97.00%) (10108/10368)
Epoch: 195 | Batch_idx: 90 |  Loss: (0.0777) |  Loss2: (0.0000) | Acc: (97.00%) (11353/11648)
Epoch: 195 | Batch_idx: 100 |  Loss: (0.0786) |  Loss2: (0.0000) | Acc: (97.00%) (12595/12928)
Epoch: 195 | Batch_idx: 110 |  Loss: (0.0786) |  Loss2: (0.0000) | Acc: (97.00%) (13845/14208)
Epoch: 195 | Batch_idx: 120 |  Loss: (0.0794) |  Loss2: (0.0000) | Acc: (97.00%) (15087/15488)
Epoch: 195 | Batch_idx: 130 |  Loss: (0.0795) |  Loss2: (0.0000) | Acc: (97.00%) (16334/16768)
Epoch: 195 | Batch_idx: 140 |  Loss: (0.0789) |  Loss2: (0.0000) | Acc: (97.00%) (17589/18048)
Epoch: 195 | Batch_idx: 150 |  Loss: (0.0792) |  Loss2: (0.0000) | Acc: (97.00%) (18835/19328)
Epoch: 195 | Batch_idx: 160 |  Loss: (0.0785) |  Loss2: (0.0000) | Acc: (97.00%) (20087/20608)
Epoch: 195 | Batch_idx: 170 |  Loss: (0.0794) |  Loss2: (0.0000) | Acc: (97.00%) (21328/21888)
Epoch: 195 | Batch_idx: 180 |  Loss: (0.0791) |  Loss2: (0.0000) | Acc: (97.00%) (22573/23168)
Epoch: 195 | Batch_idx: 190 |  Loss: (0.0801) |  Loss2: (0.0000) | Acc: (97.00%) (23807/24448)
Epoch: 195 | Batch_idx: 200 |  Loss: (0.0799) |  Loss2: (0.0000) | Acc: (97.00%) (25055/25728)
Epoch: 195 | Batch_idx: 210 |  Loss: (0.0797) |  Loss2: (0.0000) | Acc: (97.00%) (26302/27008)
Epoch: 195 | Batch_idx: 220 |  Loss: (0.0794) |  Loss2: (0.0000) | Acc: (97.00%) (27554/28288)
Epoch: 195 | Batch_idx: 230 |  Loss: (0.0791) |  Loss2: (0.0000) | Acc: (97.00%) (28802/29568)
Epoch: 195 | Batch_idx: 240 |  Loss: (0.0794) |  Loss2: (0.0000) | Acc: (97.00%) (30046/30848)
Epoch: 195 | Batch_idx: 250 |  Loss: (0.0788) |  Loss2: (0.0000) | Acc: (97.00%) (31300/32128)
Epoch: 195 | Batch_idx: 260 |  Loss: (0.0785) |  Loss2: (0.0000) | Acc: (97.00%) (32547/33408)
Epoch: 195 | Batch_idx: 270 |  Loss: (0.0778) |  Loss2: (0.0000) | Acc: (97.00%) (33799/34688)
Epoch: 195 | Batch_idx: 280 |  Loss: (0.0777) |  Loss2: (0.0000) | Acc: (97.00%) (35047/35968)
Epoch: 195 | Batch_idx: 290 |  Loss: (0.0774) |  Loss2: (0.0000) | Acc: (97.00%) (36293/37248)
Epoch: 195 | Batch_idx: 300 |  Loss: (0.0776) |  Loss2: (0.0000) | Acc: (97.00%) (37541/38528)
Epoch: 195 | Batch_idx: 310 |  Loss: (0.0776) |  Loss2: (0.0000) | Acc: (97.00%) (38781/39808)
Epoch: 195 | Batch_idx: 320 |  Loss: (0.0774) |  Loss2: (0.0000) | Acc: (97.00%) (40038/41088)
Epoch: 195 | Batch_idx: 330 |  Loss: (0.0774) |  Loss2: (0.0000) | Acc: (97.00%) (41280/42368)
Epoch: 195 | Batch_idx: 340 |  Loss: (0.0771) |  Loss2: (0.0000) | Acc: (97.00%) (42533/43648)
Epoch: 195 | Batch_idx: 350 |  Loss: (0.0770) |  Loss2: (0.0000) | Acc: (97.00%) (43785/44928)
Epoch: 195 | Batch_idx: 360 |  Loss: (0.0767) |  Loss2: (0.0000) | Acc: (97.00%) (45040/46208)
Epoch: 195 | Batch_idx: 370 |  Loss: (0.0762) |  Loss2: (0.0000) | Acc: (97.00%) (46295/47488)
Epoch: 195 | Batch_idx: 380 |  Loss: (0.0762) |  Loss2: (0.0000) | Acc: (97.00%) (47541/48768)
Epoch: 195 | Batch_idx: 390 |  Loss: (0.0759) |  Loss2: (0.0000) | Acc: (97.00%) (48750/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_195.pth.tar'
# TEST : Loss: (0.3996) | Acc: (88.00%) (8880/10000)
percent tensor([0.5829, 0.5990, 0.5895, 0.5779, 0.5954, 0.5643, 0.6029, 0.6019, 0.6007,
        0.5960, 0.5938, 0.5952, 0.5909, 0.6001, 0.5856, 0.5782],
       device='cuda:0') torch.Size([16])
percent tensor([0.5699, 0.5547, 0.5440, 0.5666, 0.5646, 0.5406, 0.5678, 0.5964, 0.5586,
        0.5381, 0.5299, 0.5298, 0.5602, 0.5693, 0.5660, 0.5547],
       device='cuda:0') torch.Size([16])
percent tensor([0.5425, 0.4964, 0.4853, 0.5620, 0.4711, 0.5762, 0.5071, 0.5399, 0.5306,
        0.4856, 0.5220, 0.4742, 0.4675, 0.6272, 0.5461, 0.5501],
       device='cuda:0') torch.Size([16])
percent tensor([0.6864, 0.7109, 0.6537, 0.6366, 0.6387, 0.6761, 0.6928, 0.6208, 0.6803,
        0.7106, 0.7058, 0.6878, 0.7348, 0.6763, 0.6862, 0.6954],
       device='cuda:0') torch.Size([16])
percent tensor([0.6898, 0.5947, 0.7200, 0.7392, 0.7446, 0.7737, 0.6936, 0.7145, 0.7694,
        0.6837, 0.7206, 0.6998, 0.5917, 0.7915, 0.6246, 0.7410],
       device='cuda:0') torch.Size([16])
percent tensor([0.8059, 0.8134, 0.8333, 0.8302, 0.8565, 0.8294, 0.8326, 0.8268, 0.8239,
        0.8274, 0.8359, 0.8130, 0.8143, 0.8504, 0.8034, 0.8265],
       device='cuda:0') torch.Size([16])
percent tensor([0.3881, 0.5795, 0.5724, 0.4753, 0.5745, 0.6503, 0.4768, 0.3165, 0.5734,
        0.5384, 0.5447, 0.5097, 0.5853, 0.4794, 0.3563, 0.3882],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9998, 0.9995, 0.9999, 0.9995, 0.9994, 0.9998, 0.9995, 0.9998,
        0.9997, 1.0000, 0.9998, 0.9998, 0.9993, 0.9994, 0.9994],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 196 | Batch_idx: 0 |  Loss: (0.0713) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 196 | Batch_idx: 10 |  Loss: (0.0626) |  Loss2: (0.0000) | Acc: (97.00%) (1379/1408)
Epoch: 196 | Batch_idx: 20 |  Loss: (0.0645) |  Loss2: (0.0000) | Acc: (97.00%) (2630/2688)
Epoch: 196 | Batch_idx: 30 |  Loss: (0.0626) |  Loss2: (0.0000) | Acc: (97.00%) (3888/3968)
Epoch: 196 | Batch_idx: 40 |  Loss: (0.0620) |  Loss2: (0.0000) | Acc: (97.00%) (5142/5248)
Epoch: 196 | Batch_idx: 50 |  Loss: (0.0638) |  Loss2: (0.0000) | Acc: (97.00%) (6394/6528)
Epoch: 196 | Batch_idx: 60 |  Loss: (0.0631) |  Loss2: (0.0000) | Acc: (98.00%) (7653/7808)
Epoch: 196 | Batch_idx: 70 |  Loss: (0.0636) |  Loss2: (0.0000) | Acc: (98.00%) (8907/9088)
Epoch: 196 | Batch_idx: 80 |  Loss: (0.0640) |  Loss2: (0.0000) | Acc: (97.00%) (10159/10368)
Epoch: 196 | Batch_idx: 90 |  Loss: (0.0653) |  Loss2: (0.0000) | Acc: (97.00%) (11411/11648)
Epoch: 196 | Batch_idx: 100 |  Loss: (0.0652) |  Loss2: (0.0000) | Acc: (97.00%) (12669/12928)
Epoch: 196 | Batch_idx: 110 |  Loss: (0.0655) |  Loss2: (0.0000) | Acc: (97.00%) (13919/14208)
Epoch: 196 | Batch_idx: 120 |  Loss: (0.0657) |  Loss2: (0.0000) | Acc: (97.00%) (15166/15488)
Epoch: 196 | Batch_idx: 130 |  Loss: (0.0643) |  Loss2: (0.0000) | Acc: (97.00%) (16425/16768)
Epoch: 196 | Batch_idx: 140 |  Loss: (0.0636) |  Loss2: (0.0000) | Acc: (97.00%) (17685/18048)
Epoch: 196 | Batch_idx: 150 |  Loss: (0.0641) |  Loss2: (0.0000) | Acc: (97.00%) (18938/19328)
Epoch: 196 | Batch_idx: 160 |  Loss: (0.0643) |  Loss2: (0.0000) | Acc: (97.00%) (20188/20608)
Epoch: 196 | Batch_idx: 170 |  Loss: (0.0637) |  Loss2: (0.0000) | Acc: (97.00%) (21444/21888)
Epoch: 196 | Batch_idx: 180 |  Loss: (0.0637) |  Loss2: (0.0000) | Acc: (97.00%) (22702/23168)
Epoch: 196 | Batch_idx: 190 |  Loss: (0.0644) |  Loss2: (0.0000) | Acc: (97.00%) (23953/24448)
Epoch: 196 | Batch_idx: 200 |  Loss: (0.0636) |  Loss2: (0.0000) | Acc: (97.00%) (25213/25728)
Epoch: 196 | Batch_idx: 210 |  Loss: (0.0634) |  Loss2: (0.0000) | Acc: (98.00%) (26468/27008)
Epoch: 196 | Batch_idx: 220 |  Loss: (0.0637) |  Loss2: (0.0000) | Acc: (97.00%) (27717/28288)
Epoch: 196 | Batch_idx: 230 |  Loss: (0.0637) |  Loss2: (0.0000) | Acc: (97.00%) (28969/29568)
Epoch: 196 | Batch_idx: 240 |  Loss: (0.0639) |  Loss2: (0.0000) | Acc: (97.00%) (30221/30848)
Epoch: 196 | Batch_idx: 250 |  Loss: (0.0633) |  Loss2: (0.0000) | Acc: (97.00%) (31485/32128)
Epoch: 196 | Batch_idx: 260 |  Loss: (0.0636) |  Loss2: (0.0000) | Acc: (97.00%) (32734/33408)
Epoch: 196 | Batch_idx: 270 |  Loss: (0.0639) |  Loss2: (0.0000) | Acc: (97.00%) (33989/34688)
Epoch: 196 | Batch_idx: 280 |  Loss: (0.0641) |  Loss2: (0.0000) | Acc: (97.00%) (35236/35968)
Epoch: 196 | Batch_idx: 290 |  Loss: (0.0641) |  Loss2: (0.0000) | Acc: (97.00%) (36491/37248)
Epoch: 196 | Batch_idx: 300 |  Loss: (0.0642) |  Loss2: (0.0000) | Acc: (97.00%) (37742/38528)
Epoch: 196 | Batch_idx: 310 |  Loss: (0.0647) |  Loss2: (0.0000) | Acc: (97.00%) (38981/39808)
Epoch: 196 | Batch_idx: 320 |  Loss: (0.0651) |  Loss2: (0.0000) | Acc: (97.00%) (40230/41088)
Epoch: 196 | Batch_idx: 330 |  Loss: (0.0655) |  Loss2: (0.0000) | Acc: (97.00%) (41478/42368)
Epoch: 196 | Batch_idx: 340 |  Loss: (0.0657) |  Loss2: (0.0000) | Acc: (97.00%) (42730/43648)
Epoch: 196 | Batch_idx: 350 |  Loss: (0.0662) |  Loss2: (0.0000) | Acc: (97.00%) (43974/44928)
Epoch: 196 | Batch_idx: 360 |  Loss: (0.0663) |  Loss2: (0.0000) | Acc: (97.00%) (45227/46208)
Epoch: 196 | Batch_idx: 370 |  Loss: (0.0665) |  Loss2: (0.0000) | Acc: (97.00%) (46478/47488)
Epoch: 196 | Batch_idx: 380 |  Loss: (0.0667) |  Loss2: (0.0000) | Acc: (97.00%) (47725/48768)
Epoch: 196 | Batch_idx: 390 |  Loss: (0.0673) |  Loss2: (0.0000) | Acc: (97.00%) (48914/50000)
# TEST : Loss: (0.4325) | Acc: (87.00%) (8795/10000)
percent tensor([0.5825, 0.5976, 0.5897, 0.5783, 0.5957, 0.5641, 0.6022, 0.6007, 0.5997,
        0.5956, 0.5931, 0.5956, 0.5903, 0.5992, 0.5850, 0.5776],
       device='cuda:0') torch.Size([16])
percent tensor([0.5700, 0.5576, 0.5418, 0.5619, 0.5661, 0.5387, 0.5703, 0.5926, 0.5626,
        0.5396, 0.5342, 0.5314, 0.5613, 0.5669, 0.5663, 0.5535],
       device='cuda:0') torch.Size([16])
percent tensor([0.5380, 0.4961, 0.4653, 0.5414, 0.4585, 0.5746, 0.5046, 0.5235, 0.5339,
        0.4808, 0.5226, 0.4647, 0.4675, 0.6244, 0.5426, 0.5443],
       device='cuda:0') torch.Size([16])
percent tensor([0.6907, 0.7155, 0.6599, 0.6441, 0.6388, 0.6774, 0.6963, 0.6253, 0.6843,
        0.7148, 0.7087, 0.6917, 0.7398, 0.6830, 0.6920, 0.6996],
       device='cuda:0') torch.Size([16])
percent tensor([0.7015, 0.5831, 0.7334, 0.7486, 0.7601, 0.8014, 0.6930, 0.7086, 0.7607,
        0.6742, 0.7033, 0.7123, 0.5946, 0.7641, 0.6334, 0.7497],
       device='cuda:0') torch.Size([16])
percent tensor([0.8115, 0.8099, 0.8417, 0.8304, 0.8604, 0.8456, 0.8275, 0.8215, 0.8217,
        0.8201, 0.8312, 0.8075, 0.8161, 0.8481, 0.8035, 0.8296],
       device='cuda:0') torch.Size([16])
percent tensor([0.3974, 0.5815, 0.5707, 0.4962, 0.5673, 0.6828, 0.4899, 0.3353, 0.5473,
        0.5245, 0.5456, 0.4811, 0.5747, 0.4734, 0.3269, 0.3975],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9997, 0.9993, 0.9998, 0.9996, 0.9991, 0.9998, 0.9995, 0.9997,
        0.9996, 0.9999, 0.9998, 0.9996, 0.9993, 0.9991, 0.9994],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 197 | Batch_idx: 0 |  Loss: (0.0830) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 197 | Batch_idx: 10 |  Loss: (0.0703) |  Loss2: (0.0000) | Acc: (97.00%) (1370/1408)
Epoch: 197 | Batch_idx: 20 |  Loss: (0.0729) |  Loss2: (0.0000) | Acc: (97.00%) (2615/2688)
Epoch: 197 | Batch_idx: 30 |  Loss: (0.0789) |  Loss2: (0.0000) | Acc: (97.00%) (3855/3968)
Epoch: 197 | Batch_idx: 40 |  Loss: (0.0805) |  Loss2: (0.0000) | Acc: (97.00%) (5101/5248)
Epoch: 197 | Batch_idx: 50 |  Loss: (0.0798) |  Loss2: (0.0000) | Acc: (97.00%) (6353/6528)
Epoch: 197 | Batch_idx: 60 |  Loss: (0.0815) |  Loss2: (0.0000) | Acc: (97.00%) (7590/7808)
Epoch: 197 | Batch_idx: 70 |  Loss: (0.0811) |  Loss2: (0.0000) | Acc: (97.00%) (8836/9088)
Epoch: 197 | Batch_idx: 80 |  Loss: (0.0836) |  Loss2: (0.0000) | Acc: (97.00%) (10069/10368)
Epoch: 197 | Batch_idx: 90 |  Loss: (0.0840) |  Loss2: (0.0000) | Acc: (97.00%) (11314/11648)
Epoch: 197 | Batch_idx: 100 |  Loss: (0.0864) |  Loss2: (0.0000) | Acc: (97.00%) (12542/12928)
Epoch: 197 | Batch_idx: 110 |  Loss: (0.0859) |  Loss2: (0.0000) | Acc: (97.00%) (13786/14208)
Epoch: 197 | Batch_idx: 120 |  Loss: (0.0862) |  Loss2: (0.0000) | Acc: (96.00%) (15023/15488)
Epoch: 197 | Batch_idx: 130 |  Loss: (0.0873) |  Loss2: (0.0000) | Acc: (96.00%) (16256/16768)
Epoch: 197 | Batch_idx: 140 |  Loss: (0.0874) |  Loss2: (0.0000) | Acc: (96.00%) (17493/18048)
Epoch: 197 | Batch_idx: 150 |  Loss: (0.0869) |  Loss2: (0.0000) | Acc: (96.00%) (18731/19328)
Epoch: 197 | Batch_idx: 160 |  Loss: (0.0869) |  Loss2: (0.0000) | Acc: (96.00%) (19973/20608)
Epoch: 197 | Batch_idx: 170 |  Loss: (0.0872) |  Loss2: (0.0000) | Acc: (96.00%) (21208/21888)
Epoch: 197 | Batch_idx: 180 |  Loss: (0.0863) |  Loss2: (0.0000) | Acc: (96.00%) (22450/23168)
Epoch: 197 | Batch_idx: 190 |  Loss: (0.0860) |  Loss2: (0.0000) | Acc: (96.00%) (23703/24448)
Epoch: 197 | Batch_idx: 200 |  Loss: (0.0856) |  Loss2: (0.0000) | Acc: (96.00%) (24953/25728)
Epoch: 197 | Batch_idx: 210 |  Loss: (0.0869) |  Loss2: (0.0000) | Acc: (96.00%) (26184/27008)
Epoch: 197 | Batch_idx: 220 |  Loss: (0.0868) |  Loss2: (0.0000) | Acc: (96.00%) (27430/28288)
Epoch: 197 | Batch_idx: 230 |  Loss: (0.0879) |  Loss2: (0.0000) | Acc: (96.00%) (28660/29568)
Epoch: 197 | Batch_idx: 240 |  Loss: (0.0873) |  Loss2: (0.0000) | Acc: (96.00%) (29909/30848)
Epoch: 197 | Batch_idx: 250 |  Loss: (0.0872) |  Loss2: (0.0000) | Acc: (96.00%) (31157/32128)
Epoch: 197 | Batch_idx: 260 |  Loss: (0.0876) |  Loss2: (0.0000) | Acc: (96.00%) (32394/33408)
Epoch: 197 | Batch_idx: 270 |  Loss: (0.0879) |  Loss2: (0.0000) | Acc: (96.00%) (33631/34688)
Epoch: 197 | Batch_idx: 280 |  Loss: (0.0880) |  Loss2: (0.0000) | Acc: (96.00%) (34864/35968)
Epoch: 197 | Batch_idx: 290 |  Loss: (0.0875) |  Loss2: (0.0000) | Acc: (96.00%) (36109/37248)
Epoch: 197 | Batch_idx: 300 |  Loss: (0.0878) |  Loss2: (0.0000) | Acc: (96.00%) (37346/38528)
Epoch: 197 | Batch_idx: 310 |  Loss: (0.0873) |  Loss2: (0.0000) | Acc: (96.00%) (38598/39808)
Epoch: 197 | Batch_idx: 320 |  Loss: (0.0872) |  Loss2: (0.0000) | Acc: (96.00%) (39832/41088)
Epoch: 197 | Batch_idx: 330 |  Loss: (0.0872) |  Loss2: (0.0000) | Acc: (96.00%) (41074/42368)
Epoch: 197 | Batch_idx: 340 |  Loss: (0.0871) |  Loss2: (0.0000) | Acc: (96.00%) (42321/43648)
Epoch: 197 | Batch_idx: 350 |  Loss: (0.0874) |  Loss2: (0.0000) | Acc: (96.00%) (43552/44928)
Epoch: 197 | Batch_idx: 360 |  Loss: (0.0872) |  Loss2: (0.0000) | Acc: (96.00%) (44798/46208)
Epoch: 197 | Batch_idx: 370 |  Loss: (0.0873) |  Loss2: (0.0000) | Acc: (96.00%) (46043/47488)
Epoch: 197 | Batch_idx: 380 |  Loss: (0.0874) |  Loss2: (0.0000) | Acc: (96.00%) (47288/48768)
Epoch: 197 | Batch_idx: 390 |  Loss: (0.0875) |  Loss2: (0.0000) | Acc: (96.00%) (48484/50000)
# TEST : Loss: (0.4119) | Acc: (88.00%) (8827/10000)
percent tensor([0.5765, 0.5907, 0.5818, 0.5719, 0.5878, 0.5585, 0.5950, 0.5937, 0.5925,
        0.5890, 0.5866, 0.5884, 0.5840, 0.5923, 0.5789, 0.5718],
       device='cuda:0') torch.Size([16])
percent tensor([0.5704, 0.5590, 0.5496, 0.5642, 0.5697, 0.5395, 0.5737, 0.5930, 0.5638,
        0.5452, 0.5354, 0.5401, 0.5615, 0.5654, 0.5656, 0.5536],
       device='cuda:0') torch.Size([16])
percent tensor([0.5520, 0.5144, 0.4811, 0.5649, 0.4744, 0.5836, 0.5219, 0.5384, 0.5461,
        0.4977, 0.5327, 0.4792, 0.4829, 0.6352, 0.5535, 0.5588],
       device='cuda:0') torch.Size([16])
percent tensor([0.6950, 0.7214, 0.6630, 0.6436, 0.6441, 0.6765, 0.7005, 0.6300, 0.6900,
        0.7206, 0.7161, 0.6995, 0.7467, 0.6873, 0.6974, 0.7011],
       device='cuda:0') torch.Size([16])
percent tensor([0.6899, 0.5775, 0.7169, 0.7343, 0.7446, 0.7854, 0.6849, 0.6919, 0.7520,
        0.6699, 0.7093, 0.7132, 0.5916, 0.7555, 0.6287, 0.7337],
       device='cuda:0') torch.Size([16])
percent tensor([0.8075, 0.8069, 0.8364, 0.8313, 0.8575, 0.8443, 0.8240, 0.8167, 0.8166,
        0.8186, 0.8320, 0.8067, 0.8135, 0.8443, 0.7997, 0.8263],
       device='cuda:0') torch.Size([16])
percent tensor([0.4180, 0.5761, 0.5875, 0.4974, 0.5765, 0.6785, 0.5041, 0.3655, 0.5469,
        0.5393, 0.5476, 0.5022, 0.5612, 0.4765, 0.3562, 0.4092],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9996, 0.9995, 0.9998, 0.9997, 0.9989, 0.9998, 0.9996, 0.9998,
        0.9997, 1.0000, 0.9998, 0.9996, 0.9994, 0.9991, 0.9994],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 198 | Batch_idx: 0 |  Loss: (0.0808) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 198 | Batch_idx: 10 |  Loss: (0.0782) |  Loss2: (0.0000) | Acc: (96.00%) (1364/1408)
Epoch: 198 | Batch_idx: 20 |  Loss: (0.0708) |  Loss2: (0.0000) | Acc: (97.00%) (2616/2688)
Epoch: 198 | Batch_idx: 30 |  Loss: (0.0661) |  Loss2: (0.0000) | Acc: (97.00%) (3876/3968)
Epoch: 198 | Batch_idx: 40 |  Loss: (0.0643) |  Loss2: (0.0000) | Acc: (97.00%) (5132/5248)
Epoch: 198 | Batch_idx: 50 |  Loss: (0.0661) |  Loss2: (0.0000) | Acc: (97.00%) (6379/6528)
Epoch: 198 | Batch_idx: 60 |  Loss: (0.0674) |  Loss2: (0.0000) | Acc: (97.00%) (7624/7808)
Epoch: 198 | Batch_idx: 70 |  Loss: (0.0666) |  Loss2: (0.0000) | Acc: (97.00%) (8880/9088)
Epoch: 198 | Batch_idx: 80 |  Loss: (0.0666) |  Loss2: (0.0000) | Acc: (97.00%) (10134/10368)
Epoch: 198 | Batch_idx: 90 |  Loss: (0.0655) |  Loss2: (0.0000) | Acc: (97.00%) (11391/11648)
Epoch: 198 | Batch_idx: 100 |  Loss: (0.0654) |  Loss2: (0.0000) | Acc: (97.00%) (12649/12928)
Epoch: 198 | Batch_idx: 110 |  Loss: (0.0652) |  Loss2: (0.0000) | Acc: (97.00%) (13903/14208)
Epoch: 198 | Batch_idx: 120 |  Loss: (0.0652) |  Loss2: (0.0000) | Acc: (97.00%) (15156/15488)
Epoch: 198 | Batch_idx: 130 |  Loss: (0.0650) |  Loss2: (0.0000) | Acc: (97.00%) (16412/16768)
Epoch: 198 | Batch_idx: 140 |  Loss: (0.0657) |  Loss2: (0.0000) | Acc: (97.00%) (17658/18048)
Epoch: 198 | Batch_idx: 150 |  Loss: (0.0652) |  Loss2: (0.0000) | Acc: (97.00%) (18918/19328)
Epoch: 198 | Batch_idx: 160 |  Loss: (0.0649) |  Loss2: (0.0000) | Acc: (97.00%) (20178/20608)
Epoch: 198 | Batch_idx: 170 |  Loss: (0.0657) |  Loss2: (0.0000) | Acc: (97.00%) (21428/21888)
Epoch: 198 | Batch_idx: 180 |  Loss: (0.0663) |  Loss2: (0.0000) | Acc: (97.00%) (22676/23168)
Epoch: 198 | Batch_idx: 190 |  Loss: (0.0666) |  Loss2: (0.0000) | Acc: (97.00%) (23927/24448)
Epoch: 198 | Batch_idx: 200 |  Loss: (0.0655) |  Loss2: (0.0000) | Acc: (97.00%) (25198/25728)
Epoch: 198 | Batch_idx: 210 |  Loss: (0.0655) |  Loss2: (0.0000) | Acc: (97.00%) (26446/27008)
Epoch: 198 | Batch_idx: 220 |  Loss: (0.0655) |  Loss2: (0.0000) | Acc: (97.00%) (27701/28288)
Epoch: 198 | Batch_idx: 230 |  Loss: (0.0656) |  Loss2: (0.0000) | Acc: (97.00%) (28952/29568)
Epoch: 198 | Batch_idx: 240 |  Loss: (0.0657) |  Loss2: (0.0000) | Acc: (97.00%) (30196/30848)
Epoch: 198 | Batch_idx: 250 |  Loss: (0.0657) |  Loss2: (0.0000) | Acc: (97.00%) (31444/32128)
Epoch: 198 | Batch_idx: 260 |  Loss: (0.0661) |  Loss2: (0.0000) | Acc: (97.00%) (32693/33408)
Epoch: 198 | Batch_idx: 270 |  Loss: (0.0667) |  Loss2: (0.0000) | Acc: (97.00%) (33935/34688)
Epoch: 198 | Batch_idx: 280 |  Loss: (0.0670) |  Loss2: (0.0000) | Acc: (97.00%) (35185/35968)
Epoch: 198 | Batch_idx: 290 |  Loss: (0.0673) |  Loss2: (0.0000) | Acc: (97.00%) (36433/37248)
Epoch: 198 | Batch_idx: 300 |  Loss: (0.0680) |  Loss2: (0.0000) | Acc: (97.00%) (37671/38528)
Epoch: 198 | Batch_idx: 310 |  Loss: (0.0679) |  Loss2: (0.0000) | Acc: (97.00%) (38924/39808)
Epoch: 198 | Batch_idx: 320 |  Loss: (0.0677) |  Loss2: (0.0000) | Acc: (97.00%) (40176/41088)
Epoch: 198 | Batch_idx: 330 |  Loss: (0.0679) |  Loss2: (0.0000) | Acc: (97.00%) (41424/42368)
Epoch: 198 | Batch_idx: 340 |  Loss: (0.0679) |  Loss2: (0.0000) | Acc: (97.00%) (42676/43648)
Epoch: 198 | Batch_idx: 350 |  Loss: (0.0677) |  Loss2: (0.0000) | Acc: (97.00%) (43926/44928)
Epoch: 198 | Batch_idx: 360 |  Loss: (0.0676) |  Loss2: (0.0000) | Acc: (97.00%) (45179/46208)
Epoch: 198 | Batch_idx: 370 |  Loss: (0.0678) |  Loss2: (0.0000) | Acc: (97.00%) (46426/47488)
Epoch: 198 | Batch_idx: 380 |  Loss: (0.0678) |  Loss2: (0.0000) | Acc: (97.00%) (47675/48768)
Epoch: 198 | Batch_idx: 390 |  Loss: (0.0682) |  Loss2: (0.0000) | Acc: (97.00%) (48869/50000)
# TEST : Loss: (0.4258) | Acc: (88.00%) (8826/10000)
percent tensor([0.5769, 0.5922, 0.5830, 0.5701, 0.5889, 0.5593, 0.5961, 0.5930, 0.5955,
        0.5890, 0.5886, 0.5883, 0.5848, 0.5947, 0.5785, 0.5718],
       device='cuda:0') torch.Size([16])
percent tensor([0.5690, 0.5600, 0.5298, 0.5619, 0.5565, 0.5400, 0.5670, 0.5888, 0.5589,
        0.5376, 0.5351, 0.5223, 0.5606, 0.5727, 0.5691, 0.5553],
       device='cuda:0') torch.Size([16])
percent tensor([0.5546, 0.5129, 0.4884, 0.5692, 0.4807, 0.5941, 0.5213, 0.5402, 0.5375,
        0.5006, 0.5339, 0.4834, 0.4817, 0.6333, 0.5616, 0.5594],
       device='cuda:0') torch.Size([16])
percent tensor([0.6932, 0.7271, 0.6569, 0.6475, 0.6423, 0.6799, 0.7049, 0.6346, 0.6867,
        0.7217, 0.7154, 0.6984, 0.7452, 0.6953, 0.7021, 0.7054],
       device='cuda:0') torch.Size([16])
percent tensor([0.6815, 0.5937, 0.6996, 0.7063, 0.7310, 0.7681, 0.6867, 0.6799, 0.7664,
        0.6842, 0.7182, 0.6920, 0.5975, 0.7783, 0.6268, 0.7316],
       device='cuda:0') torch.Size([16])
percent tensor([0.8046, 0.8113, 0.8274, 0.8193, 0.8535, 0.8385, 0.8278, 0.8108, 0.8263,
        0.8258, 0.8354, 0.8056, 0.8089, 0.8554, 0.7983, 0.8227],
       device='cuda:0') torch.Size([16])
percent tensor([0.4186, 0.5873, 0.5606, 0.4932, 0.5693, 0.6660, 0.5070, 0.3653, 0.5875,
        0.5420, 0.5553, 0.4924, 0.5635, 0.5029, 0.3485, 0.4188],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9998, 0.9996, 0.9999, 0.9996, 0.9997, 0.9998, 0.9996, 0.9998,
        0.9997, 1.0000, 0.9998, 0.9995, 0.9994, 0.9993, 0.9996],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 199 | Batch_idx: 0 |  Loss: (0.0230) |  Loss2: (0.0000) | Acc: (100.00%) (128/128)
Epoch: 199 | Batch_idx: 10 |  Loss: (0.0478) |  Loss2: (0.0000) | Acc: (98.00%) (1387/1408)
Epoch: 199 | Batch_idx: 20 |  Loss: (0.0634) |  Loss2: (0.0000) | Acc: (97.00%) (2633/2688)
Epoch: 199 | Batch_idx: 30 |  Loss: (0.0686) |  Loss2: (0.0000) | Acc: (97.00%) (3882/3968)
Epoch: 199 | Batch_idx: 40 |  Loss: (0.0758) |  Loss2: (0.0000) | Acc: (97.00%) (5123/5248)
Epoch: 199 | Batch_idx: 50 |  Loss: (0.0789) |  Loss2: (0.0000) | Acc: (97.00%) (6358/6528)
Epoch: 199 | Batch_idx: 60 |  Loss: (0.0796) |  Loss2: (0.0000) | Acc: (97.00%) (7601/7808)
Epoch: 199 | Batch_idx: 70 |  Loss: (0.0812) |  Loss2: (0.0000) | Acc: (97.00%) (8842/9088)
Epoch: 199 | Batch_idx: 80 |  Loss: (0.0830) |  Loss2: (0.0000) | Acc: (97.00%) (10087/10368)
Epoch: 199 | Batch_idx: 90 |  Loss: (0.0844) |  Loss2: (0.0000) | Acc: (97.00%) (11324/11648)
Epoch: 199 | Batch_idx: 100 |  Loss: (0.0851) |  Loss2: (0.0000) | Acc: (97.00%) (12561/12928)
Epoch: 199 | Batch_idx: 110 |  Loss: (0.0869) |  Loss2: (0.0000) | Acc: (97.00%) (13796/14208)
Epoch: 199 | Batch_idx: 120 |  Loss: (0.0865) |  Loss2: (0.0000) | Acc: (97.00%) (15030/15488)
Epoch: 199 | Batch_idx: 130 |  Loss: (0.0869) |  Loss2: (0.0000) | Acc: (97.00%) (16267/16768)
Epoch: 199 | Batch_idx: 140 |  Loss: (0.0863) |  Loss2: (0.0000) | Acc: (97.00%) (17509/18048)
Epoch: 199 | Batch_idx: 150 |  Loss: (0.0863) |  Loss2: (0.0000) | Acc: (97.00%) (18755/19328)
Epoch: 199 | Batch_idx: 160 |  Loss: (0.0855) |  Loss2: (0.0000) | Acc: (97.00%) (20005/20608)
Epoch: 199 | Batch_idx: 170 |  Loss: (0.0847) |  Loss2: (0.0000) | Acc: (97.00%) (21254/21888)
Epoch: 199 | Batch_idx: 180 |  Loss: (0.0842) |  Loss2: (0.0000) | Acc: (97.00%) (22502/23168)
Epoch: 199 | Batch_idx: 190 |  Loss: (0.0843) |  Loss2: (0.0000) | Acc: (97.00%) (23740/24448)
Epoch: 199 | Batch_idx: 200 |  Loss: (0.0834) |  Loss2: (0.0000) | Acc: (97.00%) (24991/25728)
Epoch: 199 | Batch_idx: 210 |  Loss: (0.0837) |  Loss2: (0.0000) | Acc: (97.00%) (26240/27008)
Epoch: 199 | Batch_idx: 220 |  Loss: (0.0832) |  Loss2: (0.0000) | Acc: (97.00%) (27492/28288)
Epoch: 199 | Batch_idx: 230 |  Loss: (0.0835) |  Loss2: (0.0000) | Acc: (97.00%) (28730/29568)
Epoch: 199 | Batch_idx: 240 |  Loss: (0.0836) |  Loss2: (0.0000) | Acc: (97.00%) (29976/30848)
Epoch: 199 | Batch_idx: 250 |  Loss: (0.0837) |  Loss2: (0.0000) | Acc: (97.00%) (31216/32128)
Epoch: 199 | Batch_idx: 260 |  Loss: (0.0837) |  Loss2: (0.0000) | Acc: (97.00%) (32462/33408)
Epoch: 199 | Batch_idx: 270 |  Loss: (0.0835) |  Loss2: (0.0000) | Acc: (97.00%) (33709/34688)
Epoch: 199 | Batch_idx: 280 |  Loss: (0.0832) |  Loss2: (0.0000) | Acc: (97.00%) (34948/35968)
Epoch: 199 | Batch_idx: 290 |  Loss: (0.0827) |  Loss2: (0.0000) | Acc: (97.00%) (36199/37248)
Epoch: 199 | Batch_idx: 300 |  Loss: (0.0830) |  Loss2: (0.0000) | Acc: (97.00%) (37441/38528)
Epoch: 199 | Batch_idx: 310 |  Loss: (0.0827) |  Loss2: (0.0000) | Acc: (97.00%) (38692/39808)
Epoch: 199 | Batch_idx: 320 |  Loss: (0.0826) |  Loss2: (0.0000) | Acc: (97.00%) (39939/41088)
Epoch: 199 | Batch_idx: 330 |  Loss: (0.0826) |  Loss2: (0.0000) | Acc: (97.00%) (41178/42368)
Epoch: 199 | Batch_idx: 340 |  Loss: (0.0831) |  Loss2: (0.0000) | Acc: (97.00%) (42419/43648)
Epoch: 199 | Batch_idx: 350 |  Loss: (0.0829) |  Loss2: (0.0000) | Acc: (97.00%) (43667/44928)
Epoch: 199 | Batch_idx: 360 |  Loss: (0.0829) |  Loss2: (0.0000) | Acc: (97.00%) (44912/46208)
Epoch: 199 | Batch_idx: 370 |  Loss: (0.0828) |  Loss2: (0.0000) | Acc: (97.00%) (46158/47488)
Epoch: 199 | Batch_idx: 380 |  Loss: (0.0824) |  Loss2: (0.0000) | Acc: (97.00%) (47405/48768)
Epoch: 199 | Batch_idx: 390 |  Loss: (0.0824) |  Loss2: (0.0000) | Acc: (97.00%) (48605/50000)
# TEST : Loss: (0.4050) | Acc: (88.00%) (8853/10000)
percent tensor([0.5761, 0.5914, 0.5817, 0.5687, 0.5879, 0.5581, 0.5951, 0.5915, 0.5944,
        0.5880, 0.5877, 0.5870, 0.5838, 0.5940, 0.5774, 0.5709],
       device='cuda:0') torch.Size([16])
percent tensor([0.5664, 0.5555, 0.5236, 0.5549, 0.5494, 0.5343, 0.5620, 0.5819, 0.5544,
        0.5335, 0.5322, 0.5176, 0.5598, 0.5681, 0.5635, 0.5508],
       device='cuda:0') torch.Size([16])
percent tensor([0.5478, 0.5024, 0.4797, 0.5613, 0.4684, 0.5806, 0.5091, 0.5321, 0.5304,
        0.4911, 0.5257, 0.4754, 0.4778, 0.6245, 0.5491, 0.5484],
       device='cuda:0') torch.Size([16])
percent tensor([0.6765, 0.7118, 0.6411, 0.6299, 0.6274, 0.6630, 0.6905, 0.6202, 0.6681,
        0.7049, 0.6969, 0.6812, 0.7294, 0.6755, 0.6866, 0.6895],
       device='cuda:0') torch.Size([16])
percent tensor([0.6985, 0.6071, 0.7005, 0.7076, 0.7247, 0.7716, 0.6995, 0.6848, 0.7662,
        0.6922, 0.7195, 0.6931, 0.6156, 0.7723, 0.6382, 0.7402],
       device='cuda:0') torch.Size([16])
percent tensor([0.8184, 0.8218, 0.8408, 0.8348, 0.8647, 0.8511, 0.8412, 0.8244, 0.8377,
        0.8346, 0.8441, 0.8175, 0.8181, 0.8644, 0.8110, 0.8348],
       device='cuda:0') torch.Size([16])
percent tensor([0.4062, 0.5783, 0.5674, 0.5055, 0.5949, 0.6791, 0.5130, 0.3809, 0.5852,
        0.5240, 0.5521, 0.4819, 0.5425, 0.4875, 0.3442, 0.4089],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9998, 0.9994, 0.9998, 0.9997, 0.9995, 0.9997, 0.9996, 0.9997,
        0.9996, 0.9999, 0.9998, 0.9996, 0.9994, 0.9992, 0.9995],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 200 | Batch_idx: 0 |  Loss: (0.0746) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 200 | Batch_idx: 10 |  Loss: (0.0710) |  Loss2: (0.0000) | Acc: (97.00%) (1378/1408)
Epoch: 200 | Batch_idx: 20 |  Loss: (0.0631) |  Loss2: (0.0000) | Acc: (98.00%) (2635/2688)
Epoch: 200 | Batch_idx: 30 |  Loss: (0.0637) |  Loss2: (0.0000) | Acc: (97.00%) (3885/3968)
Epoch: 200 | Batch_idx: 40 |  Loss: (0.0643) |  Loss2: (0.0000) | Acc: (97.00%) (5136/5248)
Epoch: 200 | Batch_idx: 50 |  Loss: (0.0650) |  Loss2: (0.0000) | Acc: (97.00%) (6392/6528)
Epoch: 200 | Batch_idx: 60 |  Loss: (0.0658) |  Loss2: (0.0000) | Acc: (97.00%) (7640/7808)
Epoch: 200 | Batch_idx: 70 |  Loss: (0.0651) |  Loss2: (0.0000) | Acc: (97.00%) (8891/9088)
Epoch: 200 | Batch_idx: 80 |  Loss: (0.0647) |  Loss2: (0.0000) | Acc: (97.00%) (10145/10368)
Epoch: 200 | Batch_idx: 90 |  Loss: (0.0653) |  Loss2: (0.0000) | Acc: (97.00%) (11396/11648)
Epoch: 200 | Batch_idx: 100 |  Loss: (0.0655) |  Loss2: (0.0000) | Acc: (97.00%) (12651/12928)
Epoch: 200 | Batch_idx: 110 |  Loss: (0.0661) |  Loss2: (0.0000) | Acc: (97.00%) (13895/14208)
Epoch: 200 | Batch_idx: 120 |  Loss: (0.0658) |  Loss2: (0.0000) | Acc: (97.00%) (15143/15488)
Epoch: 200 | Batch_idx: 130 |  Loss: (0.0663) |  Loss2: (0.0000) | Acc: (97.00%) (16391/16768)
Epoch: 200 | Batch_idx: 140 |  Loss: (0.0663) |  Loss2: (0.0000) | Acc: (97.00%) (17640/18048)
Epoch: 200 | Batch_idx: 150 |  Loss: (0.0655) |  Loss2: (0.0000) | Acc: (97.00%) (18899/19328)
Epoch: 200 | Batch_idx: 160 |  Loss: (0.0652) |  Loss2: (0.0000) | Acc: (97.00%) (20150/20608)
Epoch: 200 | Batch_idx: 170 |  Loss: (0.0646) |  Loss2: (0.0000) | Acc: (97.00%) (21408/21888)
Epoch: 200 | Batch_idx: 180 |  Loss: (0.0650) |  Loss2: (0.0000) | Acc: (97.00%) (22656/23168)
Epoch: 200 | Batch_idx: 190 |  Loss: (0.0651) |  Loss2: (0.0000) | Acc: (97.00%) (23909/24448)
Epoch: 200 | Batch_idx: 200 |  Loss: (0.0648) |  Loss2: (0.0000) | Acc: (97.00%) (25171/25728)
Epoch: 200 | Batch_idx: 210 |  Loss: (0.0653) |  Loss2: (0.0000) | Acc: (97.00%) (26415/27008)
Epoch: 200 | Batch_idx: 220 |  Loss: (0.0648) |  Loss2: (0.0000) | Acc: (97.00%) (27674/28288)
Epoch: 200 | Batch_idx: 230 |  Loss: (0.0644) |  Loss2: (0.0000) | Acc: (97.00%) (28935/29568)
Epoch: 200 | Batch_idx: 240 |  Loss: (0.0642) |  Loss2: (0.0000) | Acc: (97.00%) (30188/30848)
Epoch: 200 | Batch_idx: 250 |  Loss: (0.0641) |  Loss2: (0.0000) | Acc: (97.00%) (31440/32128)
Epoch: 200 | Batch_idx: 260 |  Loss: (0.0646) |  Loss2: (0.0000) | Acc: (97.00%) (32681/33408)
Epoch: 200 | Batch_idx: 270 |  Loss: (0.0651) |  Loss2: (0.0000) | Acc: (97.00%) (33929/34688)
Epoch: 200 | Batch_idx: 280 |  Loss: (0.0658) |  Loss2: (0.0000) | Acc: (97.00%) (35168/35968)
Epoch: 200 | Batch_idx: 290 |  Loss: (0.0657) |  Loss2: (0.0000) | Acc: (97.00%) (36415/37248)
Epoch: 200 | Batch_idx: 300 |  Loss: (0.0662) |  Loss2: (0.0000) | Acc: (97.00%) (37656/38528)
Epoch: 200 | Batch_idx: 310 |  Loss: (0.0664) |  Loss2: (0.0000) | Acc: (97.00%) (38904/39808)
Epoch: 200 | Batch_idx: 320 |  Loss: (0.0664) |  Loss2: (0.0000) | Acc: (97.00%) (40151/41088)
Epoch: 200 | Batch_idx: 330 |  Loss: (0.0665) |  Loss2: (0.0000) | Acc: (97.00%) (41401/42368)
Epoch: 200 | Batch_idx: 340 |  Loss: (0.0669) |  Loss2: (0.0000) | Acc: (97.00%) (42650/43648)
Epoch: 200 | Batch_idx: 350 |  Loss: (0.0673) |  Loss2: (0.0000) | Acc: (97.00%) (43898/44928)
Epoch: 200 | Batch_idx: 360 |  Loss: (0.0675) |  Loss2: (0.0000) | Acc: (97.00%) (45144/46208)
Epoch: 200 | Batch_idx: 370 |  Loss: (0.0680) |  Loss2: (0.0000) | Acc: (97.00%) (46385/47488)
Epoch: 200 | Batch_idx: 380 |  Loss: (0.0681) |  Loss2: (0.0000) | Acc: (97.00%) (47634/48768)
Epoch: 200 | Batch_idx: 390 |  Loss: (0.0680) |  Loss2: (0.0000) | Acc: (97.00%) (48841/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_200.pth.tar'
# TEST : Loss: (0.4346) | Acc: (87.00%) (8795/10000)
percent tensor([0.5760, 0.5929, 0.5815, 0.5703, 0.5873, 0.5572, 0.5958, 0.5932, 0.5947,
        0.5891, 0.5880, 0.5872, 0.5841, 0.5965, 0.5786, 0.5717],
       device='cuda:0') torch.Size([16])
percent tensor([0.5588, 0.5533, 0.5170, 0.5420, 0.5412, 0.5213, 0.5607, 0.5762, 0.5520,
        0.5290, 0.5262, 0.5118, 0.5546, 0.5697, 0.5558, 0.5424],
       device='cuda:0') torch.Size([16])
percent tensor([0.5436, 0.5070, 0.4867, 0.5609, 0.4709, 0.5783, 0.5218, 0.5389, 0.5363,
        0.4947, 0.5274, 0.4883, 0.4757, 0.6404, 0.5446, 0.5467],
       device='cuda:0') torch.Size([16])
percent tensor([0.6806, 0.7124, 0.6555, 0.6371, 0.6369, 0.6703, 0.6911, 0.6227, 0.6704,
        0.7087, 0.6989, 0.6880, 0.7321, 0.6695, 0.6871, 0.6893],
       device='cuda:0') torch.Size([16])
percent tensor([0.6981, 0.6049, 0.7204, 0.7300, 0.7428, 0.7673, 0.7042, 0.7020, 0.7634,
        0.6915, 0.7300, 0.6985, 0.6213, 0.7701, 0.6456, 0.7429],
       device='cuda:0') torch.Size([16])
percent tensor([0.8206, 0.8197, 0.8522, 0.8375, 0.8712, 0.8448, 0.8434, 0.8290, 0.8374,
        0.8339, 0.8508, 0.8211, 0.8249, 0.8610, 0.8112, 0.8353],
       device='cuda:0') torch.Size([16])
percent tensor([0.4169, 0.5860, 0.6249, 0.5279, 0.5991, 0.6823, 0.5232, 0.3913, 0.5756,
        0.5392, 0.5495, 0.5001, 0.5549, 0.4932, 0.3614, 0.4133],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9998, 0.9996, 0.9998, 0.9998, 0.9992, 0.9997, 0.9997, 0.9998,
        0.9997, 1.0000, 0.9998, 0.9996, 0.9992, 0.9989, 0.9995],
       device='cuda:0') torch.Size([16])
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(181.4935, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(823.4806, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(810.7863, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1493.0728, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(478.1948, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2286.2280, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4276.2100, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1348.2588, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6305.8574, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11512.5879, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3786.0769, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(15962.0127, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 201 | Batch_idx: 0 |  Loss: (0.0556) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 201 | Batch_idx: 10 |  Loss: (0.0541) |  Loss2: (0.0000) | Acc: (98.00%) (1380/1408)
Epoch: 201 | Batch_idx: 20 |  Loss: (0.0530) |  Loss2: (0.0000) | Acc: (98.00%) (2639/2688)
Epoch: 201 | Batch_idx: 30 |  Loss: (0.0553) |  Loss2: (0.0000) | Acc: (98.00%) (3889/3968)
Epoch: 201 | Batch_idx: 40 |  Loss: (0.0587) |  Loss2: (0.0000) | Acc: (97.00%) (5137/5248)
Epoch: 201 | Batch_idx: 50 |  Loss: (0.0565) |  Loss2: (0.0000) | Acc: (98.00%) (6403/6528)
Epoch: 201 | Batch_idx: 60 |  Loss: (0.0595) |  Loss2: (0.0000) | Acc: (97.00%) (7651/7808)
Epoch: 201 | Batch_idx: 70 |  Loss: (0.0609) |  Loss2: (0.0000) | Acc: (97.00%) (8902/9088)
Epoch: 201 | Batch_idx: 80 |  Loss: (0.0616) |  Loss2: (0.0000) | Acc: (97.00%) (10150/10368)
Epoch: 201 | Batch_idx: 90 |  Loss: (0.0616) |  Loss2: (0.0000) | Acc: (97.00%) (11400/11648)
Epoch: 201 | Batch_idx: 100 |  Loss: (0.0612) |  Loss2: (0.0000) | Acc: (97.00%) (12656/12928)
Epoch: 201 | Batch_idx: 110 |  Loss: (0.0617) |  Loss2: (0.0000) | Acc: (97.00%) (13907/14208)
Epoch: 201 | Batch_idx: 120 |  Loss: (0.0616) |  Loss2: (0.0000) | Acc: (97.00%) (15153/15488)
Epoch: 201 | Batch_idx: 130 |  Loss: (0.0617) |  Loss2: (0.0000) | Acc: (97.00%) (16407/16768)
Epoch: 201 | Batch_idx: 140 |  Loss: (0.0625) |  Loss2: (0.0000) | Acc: (97.00%) (17653/18048)
Epoch: 201 | Batch_idx: 150 |  Loss: (0.0625) |  Loss2: (0.0000) | Acc: (97.00%) (18908/19328)
Epoch: 201 | Batch_idx: 160 |  Loss: (0.0630) |  Loss2: (0.0000) | Acc: (97.00%) (20158/20608)
Epoch: 201 | Batch_idx: 170 |  Loss: (0.0625) |  Loss2: (0.0000) | Acc: (97.00%) (21417/21888)
Epoch: 201 | Batch_idx: 180 |  Loss: (0.0629) |  Loss2: (0.0000) | Acc: (97.00%) (22667/23168)
Epoch: 201 | Batch_idx: 190 |  Loss: (0.0625) |  Loss2: (0.0000) | Acc: (97.00%) (23925/24448)
Epoch: 201 | Batch_idx: 200 |  Loss: (0.0625) |  Loss2: (0.0000) | Acc: (97.00%) (25178/25728)
Epoch: 201 | Batch_idx: 210 |  Loss: (0.0625) |  Loss2: (0.0000) | Acc: (97.00%) (26431/27008)
Epoch: 201 | Batch_idx: 220 |  Loss: (0.0625) |  Loss2: (0.0000) | Acc: (97.00%) (27679/28288)
Epoch: 201 | Batch_idx: 230 |  Loss: (0.0624) |  Loss2: (0.0000) | Acc: (97.00%) (28936/29568)
Epoch: 201 | Batch_idx: 240 |  Loss: (0.0629) |  Loss2: (0.0000) | Acc: (97.00%) (30187/30848)
Epoch: 201 | Batch_idx: 250 |  Loss: (0.0628) |  Loss2: (0.0000) | Acc: (97.00%) (31445/32128)
Epoch: 201 | Batch_idx: 260 |  Loss: (0.0629) |  Loss2: (0.0000) | Acc: (97.00%) (32697/33408)
Epoch: 201 | Batch_idx: 270 |  Loss: (0.0627) |  Loss2: (0.0000) | Acc: (97.00%) (33956/34688)
Epoch: 201 | Batch_idx: 280 |  Loss: (0.0626) |  Loss2: (0.0000) | Acc: (97.00%) (35207/35968)
Epoch: 201 | Batch_idx: 290 |  Loss: (0.0628) |  Loss2: (0.0000) | Acc: (97.00%) (36456/37248)
Epoch: 201 | Batch_idx: 300 |  Loss: (0.0630) |  Loss2: (0.0000) | Acc: (97.00%) (37708/38528)
Epoch: 201 | Batch_idx: 310 |  Loss: (0.0634) |  Loss2: (0.0000) | Acc: (97.00%) (38953/39808)
Epoch: 201 | Batch_idx: 320 |  Loss: (0.0634) |  Loss2: (0.0000) | Acc: (97.00%) (40207/41088)
Epoch: 201 | Batch_idx: 330 |  Loss: (0.0640) |  Loss2: (0.0000) | Acc: (97.00%) (41456/42368)
Epoch: 201 | Batch_idx: 340 |  Loss: (0.0637) |  Loss2: (0.0000) | Acc: (97.00%) (42716/43648)
Epoch: 201 | Batch_idx: 350 |  Loss: (0.0639) |  Loss2: (0.0000) | Acc: (97.00%) (43960/44928)
Epoch: 201 | Batch_idx: 360 |  Loss: (0.0640) |  Loss2: (0.0000) | Acc: (97.00%) (45207/46208)
Epoch: 201 | Batch_idx: 370 |  Loss: (0.0640) |  Loss2: (0.0000) | Acc: (97.00%) (46460/47488)
Epoch: 201 | Batch_idx: 380 |  Loss: (0.0638) |  Loss2: (0.0000) | Acc: (97.00%) (47720/48768)
Epoch: 201 | Batch_idx: 390 |  Loss: (0.0641) |  Loss2: (0.0000) | Acc: (97.00%) (48916/50000)
# TEST : Loss: (0.4122) | Acc: (88.00%) (8882/10000)
percent tensor([0.5804, 0.5966, 0.5855, 0.5742, 0.5912, 0.5611, 0.6000, 0.5963, 0.5975,
        0.5930, 0.5916, 0.5915, 0.5883, 0.5992, 0.5826, 0.5754],
       device='cuda:0') torch.Size([16])
percent tensor([0.5635, 0.5559, 0.5283, 0.5510, 0.5527, 0.5305, 0.5661, 0.5801, 0.5544,
        0.5352, 0.5302, 0.5231, 0.5575, 0.5690, 0.5631, 0.5482],
       device='cuda:0') torch.Size([16])
percent tensor([0.5451, 0.5192, 0.4860, 0.5621, 0.4761, 0.5856, 0.5310, 0.5400, 0.5429,
        0.5019, 0.5331, 0.4901, 0.4778, 0.6422, 0.5593, 0.5529],
       device='cuda:0') torch.Size([16])
percent tensor([0.6815, 0.7128, 0.6499, 0.6326, 0.6347, 0.6737, 0.6910, 0.6208, 0.6718,
        0.7116, 0.7004, 0.6852, 0.7338, 0.6724, 0.6871, 0.6949],
       device='cuda:0') torch.Size([16])
percent tensor([0.6803, 0.5950, 0.7088, 0.7226, 0.7330, 0.7609, 0.6902, 0.6939, 0.7547,
        0.6858, 0.7112, 0.6979, 0.6044, 0.7651, 0.6372, 0.7297],
       device='cuda:0') torch.Size([16])
percent tensor([0.8205, 0.8235, 0.8467, 0.8377, 0.8669, 0.8458, 0.8406, 0.8276, 0.8331,
        0.8372, 0.8456, 0.8173, 0.8296, 0.8563, 0.8161, 0.8388],
       device='cuda:0') torch.Size([16])
percent tensor([0.4370, 0.6067, 0.6076, 0.5259, 0.6180, 0.6844, 0.5517, 0.4030, 0.5907,
        0.5488, 0.5530, 0.5005, 0.5895, 0.4842, 0.3705, 0.4227],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9998, 0.9995, 0.9998, 0.9998, 0.9991, 0.9998, 0.9994, 0.9998,
        0.9997, 1.0000, 0.9999, 0.9997, 0.9992, 0.9992, 0.9994],
       device='cuda:0') torch.Size([16])
Epoch: 202 | Batch_idx: 0 |  Loss: (0.0567) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 202 | Batch_idx: 10 |  Loss: (0.0647) |  Loss2: (0.0000) | Acc: (97.00%) (1373/1408)
Epoch: 202 | Batch_idx: 20 |  Loss: (0.0575) |  Loss2: (0.0000) | Acc: (98.00%) (2637/2688)
Epoch: 202 | Batch_idx: 30 |  Loss: (0.0599) |  Loss2: (0.0000) | Acc: (98.00%) (3894/3968)
Epoch: 202 | Batch_idx: 40 |  Loss: (0.0585) |  Loss2: (0.0000) | Acc: (98.00%) (5153/5248)
Epoch: 202 | Batch_idx: 50 |  Loss: (0.0548) |  Loss2: (0.0000) | Acc: (98.00%) (6422/6528)
Epoch: 202 | Batch_idx: 60 |  Loss: (0.0556) |  Loss2: (0.0000) | Acc: (98.00%) (7677/7808)
Epoch: 202 | Batch_idx: 70 |  Loss: (0.0530) |  Loss2: (0.0000) | Acc: (98.00%) (8944/9088)
Epoch: 202 | Batch_idx: 80 |  Loss: (0.0530) |  Loss2: (0.0000) | Acc: (98.00%) (10202/10368)
Epoch: 202 | Batch_idx: 90 |  Loss: (0.0533) |  Loss2: (0.0000) | Acc: (98.00%) (11458/11648)
Epoch: 202 | Batch_idx: 100 |  Loss: (0.0535) |  Loss2: (0.0000) | Acc: (98.00%) (12713/12928)
Epoch: 202 | Batch_idx: 110 |  Loss: (0.0538) |  Loss2: (0.0000) | Acc: (98.00%) (13970/14208)
Epoch: 202 | Batch_idx: 120 |  Loss: (0.0546) |  Loss2: (0.0000) | Acc: (98.00%) (15220/15488)
Epoch: 202 | Batch_idx: 130 |  Loss: (0.0547) |  Loss2: (0.0000) | Acc: (98.00%) (16480/16768)
Epoch: 202 | Batch_idx: 140 |  Loss: (0.0553) |  Loss2: (0.0000) | Acc: (98.00%) (17727/18048)
Epoch: 202 | Batch_idx: 150 |  Loss: (0.0566) |  Loss2: (0.0000) | Acc: (98.00%) (18977/19328)
Epoch: 202 | Batch_idx: 160 |  Loss: (0.0565) |  Loss2: (0.0000) | Acc: (98.00%) (20233/20608)
Epoch: 202 | Batch_idx: 170 |  Loss: (0.0565) |  Loss2: (0.0000) | Acc: (98.00%) (21491/21888)
Epoch: 202 | Batch_idx: 180 |  Loss: (0.0568) |  Loss2: (0.0000) | Acc: (98.00%) (22746/23168)
Epoch: 202 | Batch_idx: 190 |  Loss: (0.0574) |  Loss2: (0.0000) | Acc: (98.00%) (24000/24448)
Epoch: 202 | Batch_idx: 200 |  Loss: (0.0576) |  Loss2: (0.0000) | Acc: (98.00%) (25253/25728)
Epoch: 202 | Batch_idx: 210 |  Loss: (0.0577) |  Loss2: (0.0000) | Acc: (98.00%) (26511/27008)
Epoch: 202 | Batch_idx: 220 |  Loss: (0.0582) |  Loss2: (0.0000) | Acc: (98.00%) (27761/28288)
Epoch: 202 | Batch_idx: 230 |  Loss: (0.0582) |  Loss2: (0.0000) | Acc: (98.00%) (29019/29568)
Epoch: 202 | Batch_idx: 240 |  Loss: (0.0583) |  Loss2: (0.0000) | Acc: (98.00%) (30278/30848)
Epoch: 202 | Batch_idx: 250 |  Loss: (0.0587) |  Loss2: (0.0000) | Acc: (98.00%) (31533/32128)
Epoch: 202 | Batch_idx: 260 |  Loss: (0.0586) |  Loss2: (0.0000) | Acc: (98.00%) (32788/33408)
Epoch: 202 | Batch_idx: 270 |  Loss: (0.0587) |  Loss2: (0.0000) | Acc: (98.00%) (34045/34688)
Epoch: 202 | Batch_idx: 280 |  Loss: (0.0586) |  Loss2: (0.0000) | Acc: (98.00%) (35305/35968)
Epoch: 202 | Batch_idx: 290 |  Loss: (0.0591) |  Loss2: (0.0000) | Acc: (98.00%) (36548/37248)
Epoch: 202 | Batch_idx: 300 |  Loss: (0.0590) |  Loss2: (0.0000) | Acc: (98.00%) (37811/38528)
Epoch: 202 | Batch_idx: 310 |  Loss: (0.0593) |  Loss2: (0.0000) | Acc: (98.00%) (39066/39808)
Epoch: 202 | Batch_idx: 320 |  Loss: (0.0594) |  Loss2: (0.0000) | Acc: (98.00%) (40320/41088)
Epoch: 202 | Batch_idx: 330 |  Loss: (0.0597) |  Loss2: (0.0000) | Acc: (98.00%) (41572/42368)
Epoch: 202 | Batch_idx: 340 |  Loss: (0.0603) |  Loss2: (0.0000) | Acc: (98.00%) (42818/43648)
Epoch: 202 | Batch_idx: 350 |  Loss: (0.0606) |  Loss2: (0.0000) | Acc: (98.00%) (44076/44928)
Epoch: 202 | Batch_idx: 360 |  Loss: (0.0608) |  Loss2: (0.0000) | Acc: (98.00%) (45325/46208)
Epoch: 202 | Batch_idx: 370 |  Loss: (0.0608) |  Loss2: (0.0000) | Acc: (98.00%) (46585/47488)
Epoch: 202 | Batch_idx: 380 |  Loss: (0.0607) |  Loss2: (0.0000) | Acc: (98.00%) (47841/48768)
Epoch: 202 | Batch_idx: 390 |  Loss: (0.0609) |  Loss2: (0.0000) | Acc: (98.00%) (49044/50000)
# TEST : Loss: (0.4184) | Acc: (88.00%) (8882/10000)
percent tensor([0.5828, 0.5989, 0.5897, 0.5770, 0.5963, 0.5639, 0.6033, 0.5997, 0.6002,
        0.5957, 0.5938, 0.5956, 0.5908, 0.6010, 0.5852, 0.5775],
       device='cuda:0') torch.Size([16])
percent tensor([0.5636, 0.5536, 0.5214, 0.5443, 0.5463, 0.5306, 0.5622, 0.5797, 0.5527,
        0.5319, 0.5280, 0.5166, 0.5581, 0.5685, 0.5589, 0.5465],
       device='cuda:0') torch.Size([16])
percent tensor([0.5617, 0.5181, 0.5052, 0.5752, 0.4878, 0.5998, 0.5319, 0.5560, 0.5528,
        0.5071, 0.5376, 0.4995, 0.4908, 0.6438, 0.5655, 0.5633],
       device='cuda:0') torch.Size([16])
percent tensor([0.6832, 0.7169, 0.6568, 0.6393, 0.6339, 0.6686, 0.6958, 0.6266, 0.6788,
        0.7113, 0.7031, 0.6923, 0.7396, 0.6839, 0.6892, 0.6927],
       device='cuda:0') torch.Size([16])
percent tensor([0.6874, 0.5993, 0.7172, 0.7381, 0.7444, 0.7632, 0.7042, 0.7065, 0.7627,
        0.6857, 0.7206, 0.7072, 0.5987, 0.7809, 0.6477, 0.7374],
       device='cuda:0') torch.Size([16])
percent tensor([0.8166, 0.8162, 0.8469, 0.8314, 0.8648, 0.8446, 0.8375, 0.8275, 0.8302,
        0.8288, 0.8459, 0.8140, 0.8218, 0.8523, 0.8122, 0.8331],
       device='cuda:0') torch.Size([16])
percent tensor([0.4238, 0.5780, 0.6007, 0.5538, 0.6343, 0.7064, 0.5346, 0.3874, 0.5449,
        0.5210, 0.5322, 0.4986, 0.5695, 0.4307, 0.3677, 0.4157],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9998, 0.9993, 0.9999, 0.9997, 0.9992, 0.9997, 0.9996, 0.9997,
        0.9996, 0.9999, 0.9997, 0.9996, 0.9994, 0.9992, 0.9994],
       device='cuda:0') torch.Size([16])
Epoch: 203 | Batch_idx: 0 |  Loss: (0.1177) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 203 | Batch_idx: 10 |  Loss: (0.0747) |  Loss2: (0.0000) | Acc: (97.00%) (1374/1408)
Epoch: 203 | Batch_idx: 20 |  Loss: (0.0698) |  Loss2: (0.0000) | Acc: (97.00%) (2629/2688)
Epoch: 203 | Batch_idx: 30 |  Loss: (0.0697) |  Loss2: (0.0000) | Acc: (97.00%) (3884/3968)
Epoch: 203 | Batch_idx: 40 |  Loss: (0.0660) |  Loss2: (0.0000) | Acc: (98.00%) (5144/5248)
Epoch: 203 | Batch_idx: 50 |  Loss: (0.0633) |  Loss2: (0.0000) | Acc: (98.00%) (6400/6528)
Epoch: 203 | Batch_idx: 60 |  Loss: (0.0616) |  Loss2: (0.0000) | Acc: (98.00%) (7655/7808)
Epoch: 203 | Batch_idx: 70 |  Loss: (0.0620) |  Loss2: (0.0000) | Acc: (98.00%) (8908/9088)
Epoch: 203 | Batch_idx: 80 |  Loss: (0.0614) |  Loss2: (0.0000) | Acc: (98.00%) (10168/10368)
Epoch: 203 | Batch_idx: 90 |  Loss: (0.0623) |  Loss2: (0.0000) | Acc: (98.00%) (11423/11648)
Epoch: 203 | Batch_idx: 100 |  Loss: (0.0621) |  Loss2: (0.0000) | Acc: (98.00%) (12674/12928)
Epoch: 203 | Batch_idx: 110 |  Loss: (0.0610) |  Loss2: (0.0000) | Acc: (98.00%) (13935/14208)
Epoch: 203 | Batch_idx: 120 |  Loss: (0.0605) |  Loss2: (0.0000) | Acc: (98.00%) (15191/15488)
Epoch: 203 | Batch_idx: 130 |  Loss: (0.0608) |  Loss2: (0.0000) | Acc: (98.00%) (16442/16768)
Epoch: 203 | Batch_idx: 140 |  Loss: (0.0611) |  Loss2: (0.0000) | Acc: (98.00%) (17694/18048)
Epoch: 203 | Batch_idx: 150 |  Loss: (0.0617) |  Loss2: (0.0000) | Acc: (98.00%) (18943/19328)
Epoch: 203 | Batch_idx: 160 |  Loss: (0.0614) |  Loss2: (0.0000) | Acc: (98.00%) (20199/20608)
Epoch: 203 | Batch_idx: 170 |  Loss: (0.0618) |  Loss2: (0.0000) | Acc: (97.00%) (21450/21888)
Epoch: 203 | Batch_idx: 180 |  Loss: (0.0624) |  Loss2: (0.0000) | Acc: (97.00%) (22700/23168)
Epoch: 203 | Batch_idx: 190 |  Loss: (0.0616) |  Loss2: (0.0000) | Acc: (98.00%) (23963/24448)
Epoch: 203 | Batch_idx: 200 |  Loss: (0.0617) |  Loss2: (0.0000) | Acc: (98.00%) (25214/25728)
Epoch: 203 | Batch_idx: 210 |  Loss: (0.0623) |  Loss2: (0.0000) | Acc: (97.00%) (26463/27008)
Epoch: 203 | Batch_idx: 220 |  Loss: (0.0628) |  Loss2: (0.0000) | Acc: (97.00%) (27711/28288)
Epoch: 203 | Batch_idx: 230 |  Loss: (0.0621) |  Loss2: (0.0000) | Acc: (97.00%) (28972/29568)
Epoch: 203 | Batch_idx: 240 |  Loss: (0.0617) |  Loss2: (0.0000) | Acc: (97.00%) (30226/30848)
Epoch: 203 | Batch_idx: 250 |  Loss: (0.0616) |  Loss2: (0.0000) | Acc: (97.00%) (31479/32128)
Epoch: 203 | Batch_idx: 260 |  Loss: (0.0615) |  Loss2: (0.0000) | Acc: (98.00%) (32740/33408)
Epoch: 203 | Batch_idx: 270 |  Loss: (0.0616) |  Loss2: (0.0000) | Acc: (97.00%) (33991/34688)
Epoch: 203 | Batch_idx: 280 |  Loss: (0.0617) |  Loss2: (0.0000) | Acc: (97.00%) (35243/35968)
Epoch: 203 | Batch_idx: 290 |  Loss: (0.0620) |  Loss2: (0.0000) | Acc: (97.00%) (36497/37248)
Epoch: 203 | Batch_idx: 300 |  Loss: (0.0621) |  Loss2: (0.0000) | Acc: (97.00%) (37750/38528)
Epoch: 203 | Batch_idx: 310 |  Loss: (0.0628) |  Loss2: (0.0000) | Acc: (97.00%) (38995/39808)
Epoch: 203 | Batch_idx: 320 |  Loss: (0.0628) |  Loss2: (0.0000) | Acc: (97.00%) (40250/41088)
Epoch: 203 | Batch_idx: 330 |  Loss: (0.0631) |  Loss2: (0.0000) | Acc: (97.00%) (41496/42368)
Epoch: 203 | Batch_idx: 340 |  Loss: (0.0633) |  Loss2: (0.0000) | Acc: (97.00%) (42744/43648)
Epoch: 203 | Batch_idx: 350 |  Loss: (0.0631) |  Loss2: (0.0000) | Acc: (97.00%) (44001/44928)
Epoch: 203 | Batch_idx: 360 |  Loss: (0.0627) |  Loss2: (0.0000) | Acc: (97.00%) (45268/46208)
Epoch: 203 | Batch_idx: 370 |  Loss: (0.0625) |  Loss2: (0.0000) | Acc: (97.00%) (46528/47488)
Epoch: 203 | Batch_idx: 380 |  Loss: (0.0622) |  Loss2: (0.0000) | Acc: (97.00%) (47786/48768)
Epoch: 203 | Batch_idx: 390 |  Loss: (0.0621) |  Loss2: (0.0000) | Acc: (97.00%) (48995/50000)
# TEST : Loss: (0.3842) | Acc: (89.00%) (8955/10000)
percent tensor([0.5800, 0.5965, 0.5854, 0.5750, 0.5912, 0.5618, 0.5998, 0.5973, 0.5976,
        0.5931, 0.5915, 0.5913, 0.5882, 0.5990, 0.5831, 0.5759],
       device='cuda:0') torch.Size([16])
percent tensor([0.5566, 0.5485, 0.5262, 0.5495, 0.5457, 0.5248, 0.5581, 0.5792, 0.5469,
        0.5307, 0.5217, 0.5170, 0.5512, 0.5605, 0.5541, 0.5426],
       device='cuda:0') torch.Size([16])
percent tensor([0.5485, 0.5141, 0.4927, 0.5687, 0.4781, 0.5916, 0.5229, 0.5465, 0.5372,
        0.4975, 0.5297, 0.4860, 0.4768, 0.6406, 0.5533, 0.5573],
       device='cuda:0') torch.Size([16])
percent tensor([0.6864, 0.7144, 0.6586, 0.6425, 0.6377, 0.6723, 0.6951, 0.6281, 0.6805,
        0.7128, 0.7057, 0.6897, 0.7386, 0.6826, 0.6887, 0.6949],
       device='cuda:0') torch.Size([16])
percent tensor([0.7024, 0.6209, 0.7186, 0.7282, 0.7501, 0.7823, 0.7178, 0.6994, 0.7712,
        0.7020, 0.7428, 0.7126, 0.6202, 0.7931, 0.6581, 0.7490],
       device='cuda:0') torch.Size([16])
percent tensor([0.8155, 0.8183, 0.8456, 0.8330, 0.8683, 0.8474, 0.8389, 0.8300, 0.8262,
        0.8301, 0.8395, 0.8087, 0.8170, 0.8501, 0.8113, 0.8368],
       device='cuda:0') torch.Size([16])
percent tensor([0.4174, 0.6007, 0.5847, 0.5191, 0.6267, 0.7023, 0.5294, 0.3919, 0.5519,
        0.5398, 0.5411, 0.4896, 0.5721, 0.4614, 0.3667, 0.4560],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9998, 0.9994, 0.9999, 0.9997, 0.9990, 0.9996, 0.9993, 0.9998,
        0.9997, 1.0000, 0.9998, 0.9997, 0.9994, 0.9991, 0.9995],
       device='cuda:0') torch.Size([16])
Epoch: 204 | Batch_idx: 0 |  Loss: (0.0944) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 204 | Batch_idx: 10 |  Loss: (0.0510) |  Loss2: (0.0000) | Acc: (98.00%) (1381/1408)
Epoch: 204 | Batch_idx: 20 |  Loss: (0.0534) |  Loss2: (0.0000) | Acc: (98.00%) (2640/2688)
Epoch: 204 | Batch_idx: 30 |  Loss: (0.0590) |  Loss2: (0.0000) | Acc: (98.00%) (3889/3968)
Epoch: 204 | Batch_idx: 40 |  Loss: (0.0611) |  Loss2: (0.0000) | Acc: (97.00%) (5140/5248)
Epoch: 204 | Batch_idx: 50 |  Loss: (0.0613) |  Loss2: (0.0000) | Acc: (97.00%) (6394/6528)
Epoch: 204 | Batch_idx: 60 |  Loss: (0.0600) |  Loss2: (0.0000) | Acc: (97.00%) (7644/7808)
Epoch: 204 | Batch_idx: 70 |  Loss: (0.0612) |  Loss2: (0.0000) | Acc: (97.00%) (8890/9088)
Epoch: 204 | Batch_idx: 80 |  Loss: (0.0602) |  Loss2: (0.0000) | Acc: (97.00%) (10148/10368)
Epoch: 204 | Batch_idx: 90 |  Loss: (0.0599) |  Loss2: (0.0000) | Acc: (97.00%) (11401/11648)
Epoch: 204 | Batch_idx: 100 |  Loss: (0.0600) |  Loss2: (0.0000) | Acc: (97.00%) (12650/12928)
Epoch: 204 | Batch_idx: 110 |  Loss: (0.0591) |  Loss2: (0.0000) | Acc: (97.00%) (13911/14208)
Epoch: 204 | Batch_idx: 120 |  Loss: (0.0586) |  Loss2: (0.0000) | Acc: (97.00%) (15164/15488)
Epoch: 204 | Batch_idx: 130 |  Loss: (0.0574) |  Loss2: (0.0000) | Acc: (97.00%) (16429/16768)
Epoch: 204 | Batch_idx: 140 |  Loss: (0.0575) |  Loss2: (0.0000) | Acc: (97.00%) (17684/18048)
Epoch: 204 | Batch_idx: 150 |  Loss: (0.0580) |  Loss2: (0.0000) | Acc: (97.00%) (18936/19328)
Epoch: 204 | Batch_idx: 160 |  Loss: (0.0578) |  Loss2: (0.0000) | Acc: (97.00%) (20192/20608)
Epoch: 204 | Batch_idx: 170 |  Loss: (0.0580) |  Loss2: (0.0000) | Acc: (97.00%) (21447/21888)
Epoch: 204 | Batch_idx: 180 |  Loss: (0.0575) |  Loss2: (0.0000) | Acc: (98.00%) (22706/23168)
Epoch: 204 | Batch_idx: 190 |  Loss: (0.0571) |  Loss2: (0.0000) | Acc: (98.00%) (23968/24448)
Epoch: 204 | Batch_idx: 200 |  Loss: (0.0569) |  Loss2: (0.0000) | Acc: (98.00%) (25226/25728)
Epoch: 204 | Batch_idx: 210 |  Loss: (0.0571) |  Loss2: (0.0000) | Acc: (98.00%) (26477/27008)
Epoch: 204 | Batch_idx: 220 |  Loss: (0.0570) |  Loss2: (0.0000) | Acc: (98.00%) (27738/28288)
Epoch: 204 | Batch_idx: 230 |  Loss: (0.0569) |  Loss2: (0.0000) | Acc: (98.00%) (28990/29568)
Epoch: 204 | Batch_idx: 240 |  Loss: (0.0574) |  Loss2: (0.0000) | Acc: (98.00%) (30238/30848)
Epoch: 204 | Batch_idx: 250 |  Loss: (0.0569) |  Loss2: (0.0000) | Acc: (98.00%) (31502/32128)
Epoch: 204 | Batch_idx: 260 |  Loss: (0.0571) |  Loss2: (0.0000) | Acc: (98.00%) (32756/33408)
Epoch: 204 | Batch_idx: 270 |  Loss: (0.0572) |  Loss2: (0.0000) | Acc: (98.00%) (34014/34688)
Epoch: 204 | Batch_idx: 280 |  Loss: (0.0571) |  Loss2: (0.0000) | Acc: (98.00%) (35272/35968)
Epoch: 204 | Batch_idx: 290 |  Loss: (0.0570) |  Loss2: (0.0000) | Acc: (98.00%) (36525/37248)
Epoch: 204 | Batch_idx: 300 |  Loss: (0.0575) |  Loss2: (0.0000) | Acc: (98.00%) (37773/38528)
Epoch: 204 | Batch_idx: 310 |  Loss: (0.0576) |  Loss2: (0.0000) | Acc: (98.00%) (39025/39808)
Epoch: 204 | Batch_idx: 320 |  Loss: (0.0575) |  Loss2: (0.0000) | Acc: (98.00%) (40281/41088)
Epoch: 204 | Batch_idx: 330 |  Loss: (0.0577) |  Loss2: (0.0000) | Acc: (98.00%) (41535/42368)
Epoch: 204 | Batch_idx: 340 |  Loss: (0.0581) |  Loss2: (0.0000) | Acc: (98.00%) (42786/43648)
Epoch: 204 | Batch_idx: 350 |  Loss: (0.0584) |  Loss2: (0.0000) | Acc: (98.00%) (44038/44928)
Epoch: 204 | Batch_idx: 360 |  Loss: (0.0586) |  Loss2: (0.0000) | Acc: (98.00%) (45294/46208)
Epoch: 204 | Batch_idx: 370 |  Loss: (0.0589) |  Loss2: (0.0000) | Acc: (98.00%) (46541/47488)
Epoch: 204 | Batch_idx: 380 |  Loss: (0.0589) |  Loss2: (0.0000) | Acc: (98.00%) (47797/48768)
Epoch: 204 | Batch_idx: 390 |  Loss: (0.0592) |  Loss2: (0.0000) | Acc: (97.00%) (48999/50000)
# TEST : Loss: (0.4188) | Acc: (88.00%) (8847/10000)
percent tensor([0.5803, 0.5967, 0.5869, 0.5751, 0.5940, 0.5623, 0.6003, 0.5971, 0.5976,
        0.5929, 0.5913, 0.5923, 0.5879, 0.5984, 0.5831, 0.5756],
       device='cuda:0') torch.Size([16])
percent tensor([0.5545, 0.5491, 0.5244, 0.5480, 0.5470, 0.5219, 0.5582, 0.5767, 0.5465,
        0.5296, 0.5216, 0.5190, 0.5497, 0.5609, 0.5553, 0.5406],
       device='cuda:0') torch.Size([16])
percent tensor([0.5615, 0.5178, 0.4932, 0.5761, 0.4785, 0.6003, 0.5283, 0.5516, 0.5500,
        0.5050, 0.5392, 0.4957, 0.4912, 0.6451, 0.5642, 0.5644],
       device='cuda:0') torch.Size([16])
percent tensor([0.6846, 0.7145, 0.6544, 0.6409, 0.6339, 0.6694, 0.6924, 0.6284, 0.6772,
        0.7119, 0.7054, 0.6869, 0.7380, 0.6803, 0.6876, 0.6936],
       device='cuda:0') torch.Size([16])
percent tensor([0.6804, 0.6061, 0.7083, 0.7099, 0.7312, 0.7712, 0.6960, 0.6964, 0.7606,
        0.6862, 0.7241, 0.6888, 0.6005, 0.7712, 0.6346, 0.7284],
       device='cuda:0') torch.Size([16])
percent tensor([0.8101, 0.8175, 0.8380, 0.8281, 0.8572, 0.8369, 0.8349, 0.8221, 0.8262,
        0.8290, 0.8423, 0.8015, 0.8174, 0.8483, 0.8063, 0.8314],
       device='cuda:0') torch.Size([16])
percent tensor([0.4167, 0.6206, 0.5714, 0.5053, 0.6294, 0.6744, 0.5269, 0.4007, 0.5632,
        0.5381, 0.5585, 0.4794, 0.5759, 0.4939, 0.3403, 0.4249],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9998, 0.9994, 0.9998, 0.9995, 0.9993, 0.9997, 0.9995, 0.9996,
        0.9997, 0.9999, 0.9998, 0.9996, 0.9992, 0.9992, 0.9995],
       device='cuda:0') torch.Size([16])
Epoch: 205 | Batch_idx: 0 |  Loss: (0.0224) |  Loss2: (0.0000) | Acc: (100.00%) (128/128)
Epoch: 205 | Batch_idx: 10 |  Loss: (0.0580) |  Loss2: (0.0000) | Acc: (98.00%) (1383/1408)
Epoch: 205 | Batch_idx: 20 |  Loss: (0.0628) |  Loss2: (0.0000) | Acc: (98.00%) (2637/2688)
Epoch: 205 | Batch_idx: 30 |  Loss: (0.0657) |  Loss2: (0.0000) | Acc: (98.00%) (3890/3968)
Epoch: 205 | Batch_idx: 40 |  Loss: (0.0651) |  Loss2: (0.0000) | Acc: (97.00%) (5142/5248)
Epoch: 205 | Batch_idx: 50 |  Loss: (0.0623) |  Loss2: (0.0000) | Acc: (98.00%) (6403/6528)
Epoch: 205 | Batch_idx: 60 |  Loss: (0.0621) |  Loss2: (0.0000) | Acc: (98.00%) (7659/7808)
Epoch: 205 | Batch_idx: 70 |  Loss: (0.0627) |  Loss2: (0.0000) | Acc: (98.00%) (8910/9088)
Epoch: 205 | Batch_idx: 80 |  Loss: (0.0628) |  Loss2: (0.0000) | Acc: (98.00%) (10163/10368)
Epoch: 205 | Batch_idx: 90 |  Loss: (0.0624) |  Loss2: (0.0000) | Acc: (97.00%) (11414/11648)
Epoch: 205 | Batch_idx: 100 |  Loss: (0.0609) |  Loss2: (0.0000) | Acc: (98.00%) (12675/12928)
Epoch: 205 | Batch_idx: 110 |  Loss: (0.0604) |  Loss2: (0.0000) | Acc: (98.00%) (13935/14208)
Epoch: 205 | Batch_idx: 120 |  Loss: (0.0607) |  Loss2: (0.0000) | Acc: (98.00%) (15186/15488)
Epoch: 205 | Batch_idx: 130 |  Loss: (0.0602) |  Loss2: (0.0000) | Acc: (98.00%) (16448/16768)
Epoch: 205 | Batch_idx: 140 |  Loss: (0.0589) |  Loss2: (0.0000) | Acc: (98.00%) (17711/18048)
Epoch: 205 | Batch_idx: 150 |  Loss: (0.0583) |  Loss2: (0.0000) | Acc: (98.00%) (18969/19328)
Epoch: 205 | Batch_idx: 160 |  Loss: (0.0593) |  Loss2: (0.0000) | Acc: (98.00%) (20212/20608)
Epoch: 205 | Batch_idx: 170 |  Loss: (0.0599) |  Loss2: (0.0000) | Acc: (98.00%) (21462/21888)
Epoch: 205 | Batch_idx: 180 |  Loss: (0.0601) |  Loss2: (0.0000) | Acc: (98.00%) (22718/23168)
Epoch: 205 | Batch_idx: 190 |  Loss: (0.0605) |  Loss2: (0.0000) | Acc: (98.00%) (23965/24448)
Epoch: 205 | Batch_idx: 200 |  Loss: (0.0599) |  Loss2: (0.0000) | Acc: (98.00%) (25227/25728)
Epoch: 205 | Batch_idx: 210 |  Loss: (0.0599) |  Loss2: (0.0000) | Acc: (98.00%) (26480/27008)
Epoch: 205 | Batch_idx: 220 |  Loss: (0.0592) |  Loss2: (0.0000) | Acc: (98.00%) (27742/28288)
Epoch: 205 | Batch_idx: 230 |  Loss: (0.0589) |  Loss2: (0.0000) | Acc: (98.00%) (29004/29568)
Epoch: 205 | Batch_idx: 240 |  Loss: (0.0585) |  Loss2: (0.0000) | Acc: (98.00%) (30259/30848)
Epoch: 205 | Batch_idx: 250 |  Loss: (0.0585) |  Loss2: (0.0000) | Acc: (98.00%) (31515/32128)
Epoch: 205 | Batch_idx: 260 |  Loss: (0.0591) |  Loss2: (0.0000) | Acc: (98.00%) (32756/33408)
Epoch: 205 | Batch_idx: 270 |  Loss: (0.0590) |  Loss2: (0.0000) | Acc: (98.00%) (34009/34688)
Epoch: 205 | Batch_idx: 280 |  Loss: (0.0593) |  Loss2: (0.0000) | Acc: (98.00%) (35259/35968)
Epoch: 205 | Batch_idx: 290 |  Loss: (0.0594) |  Loss2: (0.0000) | Acc: (98.00%) (36513/37248)
Epoch: 205 | Batch_idx: 300 |  Loss: (0.0595) |  Loss2: (0.0000) | Acc: (98.00%) (37766/38528)
Epoch: 205 | Batch_idx: 310 |  Loss: (0.0594) |  Loss2: (0.0000) | Acc: (98.00%) (39025/39808)
Epoch: 205 | Batch_idx: 320 |  Loss: (0.0589) |  Loss2: (0.0000) | Acc: (98.00%) (40289/41088)
Epoch: 205 | Batch_idx: 330 |  Loss: (0.0591) |  Loss2: (0.0000) | Acc: (98.00%) (41541/42368)
Epoch: 205 | Batch_idx: 340 |  Loss: (0.0587) |  Loss2: (0.0000) | Acc: (98.00%) (42800/43648)
Epoch: 205 | Batch_idx: 350 |  Loss: (0.0590) |  Loss2: (0.0000) | Acc: (98.00%) (44047/44928)
Epoch: 205 | Batch_idx: 360 |  Loss: (0.0592) |  Loss2: (0.0000) | Acc: (98.00%) (45297/46208)
Epoch: 205 | Batch_idx: 370 |  Loss: (0.0594) |  Loss2: (0.0000) | Acc: (98.00%) (46550/47488)
Epoch: 205 | Batch_idx: 380 |  Loss: (0.0596) |  Loss2: (0.0000) | Acc: (98.00%) (47804/48768)
Epoch: 205 | Batch_idx: 390 |  Loss: (0.0593) |  Loss2: (0.0000) | Acc: (98.00%) (49020/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_205.pth.tar'
# TEST : Loss: (0.4209) | Acc: (88.00%) (8869/10000)
percent tensor([0.5837, 0.5990, 0.5890, 0.5777, 0.5959, 0.5669, 0.6028, 0.5999, 0.6005,
        0.5956, 0.5951, 0.5950, 0.5913, 0.5998, 0.5860, 0.5784],
       device='cuda:0') torch.Size([16])
percent tensor([0.5568, 0.5474, 0.5184, 0.5435, 0.5430, 0.5268, 0.5566, 0.5727, 0.5456,
        0.5281, 0.5233, 0.5130, 0.5513, 0.5606, 0.5552, 0.5432],
       device='cuda:0') torch.Size([16])
percent tensor([0.5585, 0.5204, 0.4871, 0.5715, 0.4824, 0.5973, 0.5304, 0.5457, 0.5458,
        0.5077, 0.5367, 0.4904, 0.4894, 0.6401, 0.5673, 0.5673],
       device='cuda:0') torch.Size([16])
percent tensor([0.6881, 0.7158, 0.6650, 0.6404, 0.6406, 0.6703, 0.6969, 0.6279, 0.6792,
        0.7148, 0.7062, 0.6926, 0.7388, 0.6849, 0.6910, 0.6944],
       device='cuda:0') torch.Size([16])
percent tensor([0.7001, 0.6225, 0.7087, 0.7296, 0.7451, 0.7762, 0.7119, 0.7123, 0.7774,
        0.6951, 0.7466, 0.6969, 0.6280, 0.7876, 0.6552, 0.7569],
       device='cuda:0') torch.Size([16])
percent tensor([0.8151, 0.8195, 0.8434, 0.8316, 0.8644, 0.8416, 0.8390, 0.8260, 0.8303,
        0.8349, 0.8487, 0.8102, 0.8214, 0.8510, 0.8119, 0.8375],
       device='cuda:0') torch.Size([16])
percent tensor([0.4022, 0.6077, 0.5790, 0.5002, 0.5859, 0.6938, 0.5003, 0.3774, 0.5757,
        0.5472, 0.5707, 0.5048, 0.5733, 0.5050, 0.3739, 0.4014],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9998, 0.9995, 0.9998, 0.9997, 0.9993, 0.9998, 0.9996, 0.9997,
        0.9998, 1.0000, 0.9999, 0.9996, 0.9993, 0.9990, 0.9995],
       device='cuda:0') torch.Size([16])
Epoch: 206 | Batch_idx: 0 |  Loss: (0.0395) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 206 | Batch_idx: 10 |  Loss: (0.0445) |  Loss2: (0.0000) | Acc: (98.00%) (1389/1408)
Epoch: 206 | Batch_idx: 20 |  Loss: (0.0486) |  Loss2: (0.0000) | Acc: (98.00%) (2644/2688)
Epoch: 206 | Batch_idx: 30 |  Loss: (0.0515) |  Loss2: (0.0000) | Acc: (98.00%) (3898/3968)
Epoch: 206 | Batch_idx: 40 |  Loss: (0.0550) |  Loss2: (0.0000) | Acc: (98.00%) (5145/5248)
Epoch: 206 | Batch_idx: 50 |  Loss: (0.0544) |  Loss2: (0.0000) | Acc: (98.00%) (6407/6528)
Epoch: 206 | Batch_idx: 60 |  Loss: (0.0553) |  Loss2: (0.0000) | Acc: (98.00%) (7665/7808)
Epoch: 206 | Batch_idx: 70 |  Loss: (0.0565) |  Loss2: (0.0000) | Acc: (98.00%) (8916/9088)
Epoch: 206 | Batch_idx: 80 |  Loss: (0.0554) |  Loss2: (0.0000) | Acc: (98.00%) (10173/10368)
Epoch: 206 | Batch_idx: 90 |  Loss: (0.0536) |  Loss2: (0.0000) | Acc: (98.00%) (11438/11648)
Epoch: 206 | Batch_idx: 100 |  Loss: (0.0533) |  Loss2: (0.0000) | Acc: (98.00%) (12699/12928)
Epoch: 206 | Batch_idx: 110 |  Loss: (0.0541) |  Loss2: (0.0000) | Acc: (98.00%) (13952/14208)
Epoch: 206 | Batch_idx: 120 |  Loss: (0.0559) |  Loss2: (0.0000) | Acc: (98.00%) (15196/15488)
Epoch: 206 | Batch_idx: 130 |  Loss: (0.0561) |  Loss2: (0.0000) | Acc: (98.00%) (16449/16768)
Epoch: 206 | Batch_idx: 140 |  Loss: (0.0561) |  Loss2: (0.0000) | Acc: (98.00%) (17702/18048)
Epoch: 206 | Batch_idx: 150 |  Loss: (0.0572) |  Loss2: (0.0000) | Acc: (98.00%) (18950/19328)
Epoch: 206 | Batch_idx: 160 |  Loss: (0.0568) |  Loss2: (0.0000) | Acc: (98.00%) (20208/20608)
Epoch: 206 | Batch_idx: 170 |  Loss: (0.0564) |  Loss2: (0.0000) | Acc: (98.00%) (21464/21888)
Epoch: 206 | Batch_idx: 180 |  Loss: (0.0568) |  Loss2: (0.0000) | Acc: (98.00%) (22716/23168)
Epoch: 206 | Batch_idx: 190 |  Loss: (0.0565) |  Loss2: (0.0000) | Acc: (98.00%) (23970/24448)
Epoch: 206 | Batch_idx: 200 |  Loss: (0.0565) |  Loss2: (0.0000) | Acc: (98.00%) (25227/25728)
Epoch: 206 | Batch_idx: 210 |  Loss: (0.0560) |  Loss2: (0.0000) | Acc: (98.00%) (26493/27008)
Epoch: 206 | Batch_idx: 220 |  Loss: (0.0566) |  Loss2: (0.0000) | Acc: (98.00%) (27742/28288)
Epoch: 206 | Batch_idx: 230 |  Loss: (0.0567) |  Loss2: (0.0000) | Acc: (98.00%) (28995/29568)
Epoch: 206 | Batch_idx: 240 |  Loss: (0.0568) |  Loss2: (0.0000) | Acc: (98.00%) (30252/30848)
Epoch: 206 | Batch_idx: 250 |  Loss: (0.0565) |  Loss2: (0.0000) | Acc: (98.00%) (31514/32128)
Epoch: 206 | Batch_idx: 260 |  Loss: (0.0567) |  Loss2: (0.0000) | Acc: (98.00%) (32767/33408)
Epoch: 206 | Batch_idx: 270 |  Loss: (0.0562) |  Loss2: (0.0000) | Acc: (98.00%) (34033/34688)
Epoch: 206 | Batch_idx: 280 |  Loss: (0.0562) |  Loss2: (0.0000) | Acc: (98.00%) (35294/35968)
Epoch: 206 | Batch_idx: 290 |  Loss: (0.0560) |  Loss2: (0.0000) | Acc: (98.00%) (36557/37248)
Epoch: 206 | Batch_idx: 300 |  Loss: (0.0558) |  Loss2: (0.0000) | Acc: (98.00%) (37818/38528)
Epoch: 206 | Batch_idx: 310 |  Loss: (0.0557) |  Loss2: (0.0000) | Acc: (98.00%) (39076/39808)
Epoch: 206 | Batch_idx: 320 |  Loss: (0.0557) |  Loss2: (0.0000) | Acc: (98.00%) (40331/41088)
Epoch: 206 | Batch_idx: 330 |  Loss: (0.0559) |  Loss2: (0.0000) | Acc: (98.00%) (41585/42368)
Epoch: 206 | Batch_idx: 340 |  Loss: (0.0563) |  Loss2: (0.0000) | Acc: (98.00%) (42830/43648)
Epoch: 206 | Batch_idx: 350 |  Loss: (0.0566) |  Loss2: (0.0000) | Acc: (98.00%) (44080/44928)
Epoch: 206 | Batch_idx: 360 |  Loss: (0.0566) |  Loss2: (0.0000) | Acc: (98.00%) (45330/46208)
Epoch: 206 | Batch_idx: 370 |  Loss: (0.0570) |  Loss2: (0.0000) | Acc: (98.00%) (46580/47488)
Epoch: 206 | Batch_idx: 380 |  Loss: (0.0570) |  Loss2: (0.0000) | Acc: (98.00%) (47837/48768)
Epoch: 206 | Batch_idx: 390 |  Loss: (0.0568) |  Loss2: (0.0000) | Acc: (98.00%) (49045/50000)
# TEST : Loss: (0.4135) | Acc: (88.00%) (8889/10000)
percent tensor([0.5858, 0.6022, 0.5910, 0.5808, 0.5975, 0.5673, 0.6053, 0.6033, 0.6028,
        0.5984, 0.5968, 0.5969, 0.5941, 0.6031, 0.5883, 0.5807],
       device='cuda:0') torch.Size([16])
percent tensor([0.5622, 0.5499, 0.5314, 0.5516, 0.5524, 0.5304, 0.5604, 0.5800, 0.5500,
        0.5332, 0.5248, 0.5237, 0.5547, 0.5606, 0.5587, 0.5462],
       device='cuda:0') torch.Size([16])
percent tensor([0.5578, 0.5242, 0.5008, 0.5750, 0.4919, 0.5946, 0.5311, 0.5556, 0.5441,
        0.5112, 0.5358, 0.4988, 0.4869, 0.6412, 0.5738, 0.5660],
       device='cuda:0') torch.Size([16])
percent tensor([0.6889, 0.7200, 0.6540, 0.6427, 0.6363, 0.6776, 0.6990, 0.6264, 0.6799,
        0.7149, 0.7090, 0.6909, 0.7430, 0.6855, 0.6950, 0.6964],
       device='cuda:0') torch.Size([16])
percent tensor([0.6997, 0.5947, 0.7122, 0.7242, 0.7552, 0.7847, 0.7057, 0.7058, 0.7831,
        0.6846, 0.7359, 0.7082, 0.6082, 0.7767, 0.6528, 0.7397],
       device='cuda:0') torch.Size([16])
percent tensor([0.8137, 0.8145, 0.8395, 0.8294, 0.8667, 0.8439, 0.8336, 0.8275, 0.8299,
        0.8248, 0.8401, 0.8091, 0.8160, 0.8528, 0.8123, 0.8350],
       device='cuda:0') torch.Size([16])
percent tensor([0.4107, 0.5978, 0.5708, 0.5122, 0.6337, 0.7024, 0.5313, 0.3919, 0.5836,
        0.5266, 0.5494, 0.5017, 0.5697, 0.5174, 0.3939, 0.4324],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9998, 0.9995, 0.9998, 0.9997, 0.9994, 0.9998, 0.9993, 0.9998,
        0.9998, 1.0000, 0.9998, 0.9996, 0.9996, 0.9990, 0.9995],
       device='cuda:0') torch.Size([16])
Epoch: 207 | Batch_idx: 0 |  Loss: (0.1433) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 207 | Batch_idx: 10 |  Loss: (0.0656) |  Loss2: (0.0000) | Acc: (97.00%) (1378/1408)
Epoch: 207 | Batch_idx: 20 |  Loss: (0.0572) |  Loss2: (0.0000) | Acc: (98.00%) (2640/2688)
Epoch: 207 | Batch_idx: 30 |  Loss: (0.0539) |  Loss2: (0.0000) | Acc: (98.00%) (3899/3968)
Epoch: 207 | Batch_idx: 40 |  Loss: (0.0513) |  Loss2: (0.0000) | Acc: (98.00%) (5165/5248)
Epoch: 207 | Batch_idx: 50 |  Loss: (0.0523) |  Loss2: (0.0000) | Acc: (98.00%) (6421/6528)
Epoch: 207 | Batch_idx: 60 |  Loss: (0.0524) |  Loss2: (0.0000) | Acc: (98.00%) (7681/7808)
Epoch: 207 | Batch_idx: 70 |  Loss: (0.0509) |  Loss2: (0.0000) | Acc: (98.00%) (8946/9088)
Epoch: 207 | Batch_idx: 80 |  Loss: (0.0511) |  Loss2: (0.0000) | Acc: (98.00%) (10200/10368)
Epoch: 207 | Batch_idx: 90 |  Loss: (0.0502) |  Loss2: (0.0000) | Acc: (98.00%) (11462/11648)
Epoch: 207 | Batch_idx: 100 |  Loss: (0.0493) |  Loss2: (0.0000) | Acc: (98.00%) (12729/12928)
Epoch: 207 | Batch_idx: 110 |  Loss: (0.0502) |  Loss2: (0.0000) | Acc: (98.00%) (13988/14208)
Epoch: 207 | Batch_idx: 120 |  Loss: (0.0514) |  Loss2: (0.0000) | Acc: (98.00%) (15241/15488)
Epoch: 207 | Batch_idx: 130 |  Loss: (0.0515) |  Loss2: (0.0000) | Acc: (98.00%) (16498/16768)
Epoch: 207 | Batch_idx: 140 |  Loss: (0.0517) |  Loss2: (0.0000) | Acc: (98.00%) (17760/18048)
Epoch: 207 | Batch_idx: 150 |  Loss: (0.0515) |  Loss2: (0.0000) | Acc: (98.00%) (19023/19328)
Epoch: 207 | Batch_idx: 160 |  Loss: (0.0513) |  Loss2: (0.0000) | Acc: (98.00%) (20283/20608)
Epoch: 207 | Batch_idx: 170 |  Loss: (0.0520) |  Loss2: (0.0000) | Acc: (98.00%) (21539/21888)
Epoch: 207 | Batch_idx: 180 |  Loss: (0.0524) |  Loss2: (0.0000) | Acc: (98.00%) (22792/23168)
Epoch: 207 | Batch_idx: 190 |  Loss: (0.0527) |  Loss2: (0.0000) | Acc: (98.00%) (24042/24448)
Epoch: 207 | Batch_idx: 200 |  Loss: (0.0529) |  Loss2: (0.0000) | Acc: (98.00%) (25297/25728)
Epoch: 207 | Batch_idx: 210 |  Loss: (0.0529) |  Loss2: (0.0000) | Acc: (98.00%) (26555/27008)
Epoch: 207 | Batch_idx: 220 |  Loss: (0.0528) |  Loss2: (0.0000) | Acc: (98.00%) (27814/28288)
Epoch: 207 | Batch_idx: 230 |  Loss: (0.0534) |  Loss2: (0.0000) | Acc: (98.00%) (29063/29568)
Epoch: 207 | Batch_idx: 240 |  Loss: (0.0533) |  Loss2: (0.0000) | Acc: (98.00%) (30321/30848)
Epoch: 207 | Batch_idx: 250 |  Loss: (0.0533) |  Loss2: (0.0000) | Acc: (98.00%) (31582/32128)
Epoch: 207 | Batch_idx: 260 |  Loss: (0.0531) |  Loss2: (0.0000) | Acc: (98.00%) (32837/33408)
Epoch: 207 | Batch_idx: 270 |  Loss: (0.0538) |  Loss2: (0.0000) | Acc: (98.00%) (34086/34688)
Epoch: 207 | Batch_idx: 280 |  Loss: (0.0538) |  Loss2: (0.0000) | Acc: (98.00%) (35347/35968)
Epoch: 207 | Batch_idx: 290 |  Loss: (0.0539) |  Loss2: (0.0000) | Acc: (98.00%) (36602/37248)
Epoch: 207 | Batch_idx: 300 |  Loss: (0.0541) |  Loss2: (0.0000) | Acc: (98.00%) (37860/38528)
Epoch: 207 | Batch_idx: 310 |  Loss: (0.0540) |  Loss2: (0.0000) | Acc: (98.00%) (39120/39808)
Epoch: 207 | Batch_idx: 320 |  Loss: (0.0542) |  Loss2: (0.0000) | Acc: (98.00%) (40376/41088)
Epoch: 207 | Batch_idx: 330 |  Loss: (0.0538) |  Loss2: (0.0000) | Acc: (98.00%) (41640/42368)
Epoch: 207 | Batch_idx: 340 |  Loss: (0.0539) |  Loss2: (0.0000) | Acc: (98.00%) (42891/43648)
Epoch: 207 | Batch_idx: 350 |  Loss: (0.0543) |  Loss2: (0.0000) | Acc: (98.00%) (44144/44928)
Epoch: 207 | Batch_idx: 360 |  Loss: (0.0545) |  Loss2: (0.0000) | Acc: (98.00%) (45397/46208)
Epoch: 207 | Batch_idx: 370 |  Loss: (0.0549) |  Loss2: (0.0000) | Acc: (98.00%) (46647/47488)
Epoch: 207 | Batch_idx: 380 |  Loss: (0.0553) |  Loss2: (0.0000) | Acc: (98.00%) (47899/48768)
Epoch: 207 | Batch_idx: 390 |  Loss: (0.0551) |  Loss2: (0.0000) | Acc: (98.00%) (49117/50000)
# TEST : Loss: (0.4421) | Acc: (88.00%) (8821/10000)
percent tensor([0.5829, 0.5994, 0.5898, 0.5776, 0.5952, 0.5648, 0.6028, 0.6005, 0.6004,
        0.5956, 0.5944, 0.5948, 0.5909, 0.6004, 0.5857, 0.5780],
       device='cuda:0') torch.Size([16])
percent tensor([0.5571, 0.5465, 0.5231, 0.5447, 0.5449, 0.5232, 0.5555, 0.5748, 0.5466,
        0.5281, 0.5219, 0.5165, 0.5524, 0.5592, 0.5551, 0.5403],
       device='cuda:0') torch.Size([16])
percent tensor([0.5567, 0.5153, 0.4872, 0.5744, 0.4813, 0.5955, 0.5288, 0.5498, 0.5428,
        0.5018, 0.5326, 0.4873, 0.4846, 0.6436, 0.5662, 0.5650],
       device='cuda:0') torch.Size([16])
percent tensor([0.6902, 0.7170, 0.6618, 0.6487, 0.6397, 0.6735, 0.6982, 0.6308, 0.6835,
        0.7169, 0.7109, 0.6917, 0.7417, 0.6863, 0.6910, 0.6972],
       device='cuda:0') torch.Size([16])
percent tensor([0.6918, 0.6066, 0.7153, 0.7252, 0.7571, 0.7841, 0.7042, 0.7012, 0.7757,
        0.6926, 0.7309, 0.7053, 0.6174, 0.7834, 0.6503, 0.7456],
       device='cuda:0') torch.Size([16])
percent tensor([0.8136, 0.8269, 0.8416, 0.8303, 0.8675, 0.8407, 0.8395, 0.8274, 0.8312,
        0.8344, 0.8445, 0.8128, 0.8217, 0.8556, 0.8143, 0.8361],
       device='cuda:0') torch.Size([16])
percent tensor([0.4207, 0.6214, 0.5573, 0.5137, 0.6248, 0.6646, 0.5500, 0.3946, 0.5383,
        0.5489, 0.5475, 0.4734, 0.5640, 0.4906, 0.3554, 0.4227],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9998, 0.9994, 0.9998, 0.9995, 0.9994, 0.9997, 0.9994, 0.9997,
        0.9997, 0.9999, 0.9998, 0.9995, 0.9997, 0.9990, 0.9993],
       device='cuda:0') torch.Size([16])
Epoch: 208 | Batch_idx: 0 |  Loss: (0.0130) |  Loss2: (0.0000) | Acc: (100.00%) (128/128)
Epoch: 208 | Batch_idx: 10 |  Loss: (0.0477) |  Loss2: (0.0000) | Acc: (98.00%) (1388/1408)
Epoch: 208 | Batch_idx: 20 |  Loss: (0.0483) |  Loss2: (0.0000) | Acc: (98.00%) (2646/2688)
Epoch: 208 | Batch_idx: 30 |  Loss: (0.0499) |  Loss2: (0.0000) | Acc: (98.00%) (3900/3968)
Epoch: 208 | Batch_idx: 40 |  Loss: (0.0502) |  Loss2: (0.0000) | Acc: (98.00%) (5158/5248)
Epoch: 208 | Batch_idx: 50 |  Loss: (0.0501) |  Loss2: (0.0000) | Acc: (98.00%) (6418/6528)
Epoch: 208 | Batch_idx: 60 |  Loss: (0.0509) |  Loss2: (0.0000) | Acc: (98.00%) (7678/7808)
Epoch: 208 | Batch_idx: 70 |  Loss: (0.0510) |  Loss2: (0.0000) | Acc: (98.00%) (8941/9088)
Epoch: 208 | Batch_idx: 80 |  Loss: (0.0516) |  Loss2: (0.0000) | Acc: (98.00%) (10193/10368)
Epoch: 208 | Batch_idx: 90 |  Loss: (0.0527) |  Loss2: (0.0000) | Acc: (98.00%) (11446/11648)
Epoch: 208 | Batch_idx: 100 |  Loss: (0.0540) |  Loss2: (0.0000) | Acc: (98.00%) (12698/12928)
Epoch: 208 | Batch_idx: 110 |  Loss: (0.0554) |  Loss2: (0.0000) | Acc: (98.00%) (13949/14208)
Epoch: 208 | Batch_idx: 120 |  Loss: (0.0552) |  Loss2: (0.0000) | Acc: (98.00%) (15208/15488)
Epoch: 208 | Batch_idx: 130 |  Loss: (0.0548) |  Loss2: (0.0000) | Acc: (98.00%) (16467/16768)
Epoch: 208 | Batch_idx: 140 |  Loss: (0.0547) |  Loss2: (0.0000) | Acc: (98.00%) (17725/18048)
Epoch: 208 | Batch_idx: 150 |  Loss: (0.0550) |  Loss2: (0.0000) | Acc: (98.00%) (18985/19328)
Epoch: 208 | Batch_idx: 160 |  Loss: (0.0547) |  Loss2: (0.0000) | Acc: (98.00%) (20245/20608)
Epoch: 208 | Batch_idx: 170 |  Loss: (0.0540) |  Loss2: (0.0000) | Acc: (98.00%) (21507/21888)
Epoch: 208 | Batch_idx: 180 |  Loss: (0.0535) |  Loss2: (0.0000) | Acc: (98.00%) (22769/23168)
Epoch: 208 | Batch_idx: 190 |  Loss: (0.0537) |  Loss2: (0.0000) | Acc: (98.00%) (24018/24448)
Epoch: 208 | Batch_idx: 200 |  Loss: (0.0536) |  Loss2: (0.0000) | Acc: (98.00%) (25282/25728)
Epoch: 208 | Batch_idx: 210 |  Loss: (0.0535) |  Loss2: (0.0000) | Acc: (98.00%) (26539/27008)
Epoch: 208 | Batch_idx: 220 |  Loss: (0.0541) |  Loss2: (0.0000) | Acc: (98.00%) (27793/28288)
Epoch: 208 | Batch_idx: 230 |  Loss: (0.0542) |  Loss2: (0.0000) | Acc: (98.00%) (29049/29568)
Epoch: 208 | Batch_idx: 240 |  Loss: (0.0552) |  Loss2: (0.0000) | Acc: (98.00%) (30295/30848)
Epoch: 208 | Batch_idx: 250 |  Loss: (0.0559) |  Loss2: (0.0000) | Acc: (98.00%) (31543/32128)
Epoch: 208 | Batch_idx: 260 |  Loss: (0.0555) |  Loss2: (0.0000) | Acc: (98.00%) (32802/33408)
Epoch: 208 | Batch_idx: 270 |  Loss: (0.0558) |  Loss2: (0.0000) | Acc: (98.00%) (34054/34688)
Epoch: 208 | Batch_idx: 280 |  Loss: (0.0558) |  Loss2: (0.0000) | Acc: (98.00%) (35312/35968)
Epoch: 208 | Batch_idx: 290 |  Loss: (0.0558) |  Loss2: (0.0000) | Acc: (98.00%) (36567/37248)
Epoch: 208 | Batch_idx: 300 |  Loss: (0.0558) |  Loss2: (0.0000) | Acc: (98.00%) (37824/38528)
Epoch: 208 | Batch_idx: 310 |  Loss: (0.0554) |  Loss2: (0.0000) | Acc: (98.00%) (39086/39808)
Epoch: 208 | Batch_idx: 320 |  Loss: (0.0555) |  Loss2: (0.0000) | Acc: (98.00%) (40341/41088)
Epoch: 208 | Batch_idx: 330 |  Loss: (0.0557) |  Loss2: (0.0000) | Acc: (98.00%) (41596/42368)
Epoch: 208 | Batch_idx: 340 |  Loss: (0.0560) |  Loss2: (0.0000) | Acc: (98.00%) (42850/43648)
Epoch: 208 | Batch_idx: 350 |  Loss: (0.0562) |  Loss2: (0.0000) | Acc: (98.00%) (44101/44928)
Epoch: 208 | Batch_idx: 360 |  Loss: (0.0561) |  Loss2: (0.0000) | Acc: (98.00%) (45359/46208)
Epoch: 208 | Batch_idx: 370 |  Loss: (0.0562) |  Loss2: (0.0000) | Acc: (98.00%) (46608/47488)
Epoch: 208 | Batch_idx: 380 |  Loss: (0.0562) |  Loss2: (0.0000) | Acc: (98.00%) (47864/48768)
Epoch: 208 | Batch_idx: 390 |  Loss: (0.0562) |  Loss2: (0.0000) | Acc: (98.00%) (49074/50000)
# TEST : Loss: (0.4079) | Acc: (88.00%) (8886/10000)
percent tensor([0.5853, 0.6050, 0.5904, 0.5802, 0.5968, 0.5677, 0.6069, 0.6034, 0.6042,
        0.5993, 0.5980, 0.5966, 0.5940, 0.6079, 0.5897, 0.5814],
       device='cuda:0') torch.Size([16])
percent tensor([0.5555, 0.5470, 0.5202, 0.5439, 0.5434, 0.5242, 0.5564, 0.5736, 0.5471,
        0.5272, 0.5228, 0.5133, 0.5496, 0.5618, 0.5537, 0.5414],
       device='cuda:0') torch.Size([16])
percent tensor([0.5613, 0.5169, 0.4968, 0.5793, 0.4891, 0.6017, 0.5363, 0.5519, 0.5503,
        0.5043, 0.5392, 0.4995, 0.4886, 0.6467, 0.5666, 0.5689],
       device='cuda:0') torch.Size([16])
percent tensor([0.6923, 0.7208, 0.6650, 0.6502, 0.6417, 0.6739, 0.7025, 0.6353, 0.6854,
        0.7201, 0.7111, 0.6988, 0.7440, 0.6879, 0.6952, 0.6981],
       device='cuda:0') torch.Size([16])
percent tensor([0.6901, 0.6205, 0.7055, 0.7263, 0.7494, 0.7758, 0.7091, 0.6967, 0.7705,
        0.6933, 0.7286, 0.6937, 0.6247, 0.7876, 0.6467, 0.7476],
       device='cuda:0') torch.Size([16])
percent tensor([0.8162, 0.8242, 0.8471, 0.8358, 0.8672, 0.8449, 0.8438, 0.8263, 0.8352,
        0.8334, 0.8425, 0.8173, 0.8204, 0.8619, 0.8159, 0.8370],
       device='cuda:0') torch.Size([16])
percent tensor([0.4252, 0.6136, 0.6089, 0.5393, 0.6456, 0.7000, 0.5421, 0.4210, 0.5761,
        0.5341, 0.5659, 0.5124, 0.5701, 0.4925, 0.3867, 0.4117],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9998, 0.9994, 0.9998, 0.9995, 0.9991, 0.9997, 0.9994, 0.9998,
        0.9997, 1.0000, 0.9998, 0.9998, 0.9994, 0.9991, 0.9995],
       device='cuda:0') torch.Size([16])
Epoch: 209 | Batch_idx: 0 |  Loss: (0.0286) |  Loss2: (0.0000) | Acc: (100.00%) (128/128)
Epoch: 209 | Batch_idx: 10 |  Loss: (0.0418) |  Loss2: (0.0000) | Acc: (98.00%) (1393/1408)
Epoch: 209 | Batch_idx: 20 |  Loss: (0.0474) |  Loss2: (0.0000) | Acc: (98.00%) (2655/2688)
Epoch: 209 | Batch_idx: 30 |  Loss: (0.0498) |  Loss2: (0.0000) | Acc: (98.00%) (3908/3968)
Epoch: 209 | Batch_idx: 40 |  Loss: (0.0481) |  Loss2: (0.0000) | Acc: (98.00%) (5172/5248)
Epoch: 209 | Batch_idx: 50 |  Loss: (0.0478) |  Loss2: (0.0000) | Acc: (98.00%) (6434/6528)
Epoch: 209 | Batch_idx: 60 |  Loss: (0.0464) |  Loss2: (0.0000) | Acc: (98.00%) (7705/7808)
Epoch: 209 | Batch_idx: 70 |  Loss: (0.0452) |  Loss2: (0.0000) | Acc: (98.00%) (8968/9088)
Epoch: 209 | Batch_idx: 80 |  Loss: (0.0452) |  Loss2: (0.0000) | Acc: (98.00%) (10233/10368)
Epoch: 209 | Batch_idx: 90 |  Loss: (0.0463) |  Loss2: (0.0000) | Acc: (98.00%) (11490/11648)
Epoch: 209 | Batch_idx: 100 |  Loss: (0.0462) |  Loss2: (0.0000) | Acc: (98.00%) (12754/12928)
Epoch: 209 | Batch_idx: 110 |  Loss: (0.0472) |  Loss2: (0.0000) | Acc: (98.00%) (14006/14208)
Epoch: 209 | Batch_idx: 120 |  Loss: (0.0471) |  Loss2: (0.0000) | Acc: (98.00%) (15272/15488)
Epoch: 209 | Batch_idx: 130 |  Loss: (0.0467) |  Loss2: (0.0000) | Acc: (98.00%) (16535/16768)
Epoch: 209 | Batch_idx: 140 |  Loss: (0.0469) |  Loss2: (0.0000) | Acc: (98.00%) (17798/18048)
Epoch: 209 | Batch_idx: 150 |  Loss: (0.0467) |  Loss2: (0.0000) | Acc: (98.00%) (19065/19328)
Epoch: 209 | Batch_idx: 160 |  Loss: (0.0468) |  Loss2: (0.0000) | Acc: (98.00%) (20325/20608)
Epoch: 209 | Batch_idx: 170 |  Loss: (0.0472) |  Loss2: (0.0000) | Acc: (98.00%) (21582/21888)
Epoch: 209 | Batch_idx: 180 |  Loss: (0.0468) |  Loss2: (0.0000) | Acc: (98.00%) (22846/23168)
Epoch: 209 | Batch_idx: 190 |  Loss: (0.0468) |  Loss2: (0.0000) | Acc: (98.00%) (24105/24448)
Epoch: 209 | Batch_idx: 200 |  Loss: (0.0467) |  Loss2: (0.0000) | Acc: (98.00%) (25368/25728)
Epoch: 209 | Batch_idx: 210 |  Loss: (0.0471) |  Loss2: (0.0000) | Acc: (98.00%) (26621/27008)
Epoch: 209 | Batch_idx: 220 |  Loss: (0.0475) |  Loss2: (0.0000) | Acc: (98.00%) (27880/28288)
Epoch: 209 | Batch_idx: 230 |  Loss: (0.0477) |  Loss2: (0.0000) | Acc: (98.00%) (29134/29568)
Epoch: 209 | Batch_idx: 240 |  Loss: (0.0479) |  Loss2: (0.0000) | Acc: (98.00%) (30389/30848)
Epoch: 209 | Batch_idx: 250 |  Loss: (0.0484) |  Loss2: (0.0000) | Acc: (98.00%) (31648/32128)
Epoch: 209 | Batch_idx: 260 |  Loss: (0.0485) |  Loss2: (0.0000) | Acc: (98.00%) (32910/33408)
Epoch: 209 | Batch_idx: 270 |  Loss: (0.0489) |  Loss2: (0.0000) | Acc: (98.00%) (34164/34688)
Epoch: 209 | Batch_idx: 280 |  Loss: (0.0490) |  Loss2: (0.0000) | Acc: (98.00%) (35424/35968)
Epoch: 209 | Batch_idx: 290 |  Loss: (0.0491) |  Loss2: (0.0000) | Acc: (98.00%) (36682/37248)
Epoch: 209 | Batch_idx: 300 |  Loss: (0.0491) |  Loss2: (0.0000) | Acc: (98.00%) (37938/38528)
Epoch: 209 | Batch_idx: 310 |  Loss: (0.0493) |  Loss2: (0.0000) | Acc: (98.00%) (39197/39808)
Epoch: 209 | Batch_idx: 320 |  Loss: (0.0490) |  Loss2: (0.0000) | Acc: (98.00%) (40468/41088)
Epoch: 209 | Batch_idx: 330 |  Loss: (0.0491) |  Loss2: (0.0000) | Acc: (98.00%) (41728/42368)
Epoch: 209 | Batch_idx: 340 |  Loss: (0.0492) |  Loss2: (0.0000) | Acc: (98.00%) (42984/43648)
Epoch: 209 | Batch_idx: 350 |  Loss: (0.0492) |  Loss2: (0.0000) | Acc: (98.00%) (44241/44928)
Epoch: 209 | Batch_idx: 360 |  Loss: (0.0493) |  Loss2: (0.0000) | Acc: (98.00%) (45503/46208)
Epoch: 209 | Batch_idx: 370 |  Loss: (0.0496) |  Loss2: (0.0000) | Acc: (98.00%) (46754/47488)
Epoch: 209 | Batch_idx: 380 |  Loss: (0.0500) |  Loss2: (0.0000) | Acc: (98.00%) (48010/48768)
Epoch: 209 | Batch_idx: 390 |  Loss: (0.0504) |  Loss2: (0.0000) | Acc: (98.00%) (49216/50000)
# TEST : Loss: (0.4382) | Acc: (88.00%) (8862/10000)
percent tensor([0.5859, 0.6001, 0.5928, 0.5802, 0.5990, 0.5681, 0.6047, 0.6026, 0.6028,
        0.5977, 0.5965, 0.5977, 0.5935, 0.6010, 0.5874, 0.5804],
       device='cuda:0') torch.Size([16])
percent tensor([0.5536, 0.5427, 0.5225, 0.5421, 0.5427, 0.5191, 0.5511, 0.5739, 0.5425,
        0.5255, 0.5179, 0.5150, 0.5476, 0.5526, 0.5506, 0.5369],
       device='cuda:0') torch.Size([16])
percent tensor([0.5637, 0.5246, 0.4980, 0.5738, 0.4840, 0.5995, 0.5350, 0.5586, 0.5536,
        0.5107, 0.5406, 0.4974, 0.4933, 0.6445, 0.5735, 0.5718],
       device='cuda:0') torch.Size([16])
percent tensor([0.6913, 0.7180, 0.6635, 0.6425, 0.6420, 0.6827, 0.7001, 0.6324, 0.6818,
        0.7193, 0.7092, 0.6970, 0.7454, 0.6816, 0.6931, 0.7021],
       device='cuda:0') torch.Size([16])
percent tensor([0.6906, 0.6127, 0.7154, 0.7257, 0.7459, 0.7764, 0.7106, 0.7147, 0.7789,
        0.6920, 0.7295, 0.6903, 0.6256, 0.7860, 0.6485, 0.7427],
       device='cuda:0') torch.Size([16])
percent tensor([0.8256, 0.8306, 0.8474, 0.8335, 0.8670, 0.8487, 0.8487, 0.8321, 0.8436,
        0.8389, 0.8482, 0.8168, 0.8313, 0.8600, 0.8234, 0.8440],
       device='cuda:0') torch.Size([16])
percent tensor([0.4152, 0.6068, 0.5797, 0.5153, 0.6335, 0.6858, 0.5348, 0.3921, 0.5890,
        0.5570, 0.5697, 0.4831, 0.5812, 0.4910, 0.3922, 0.4174],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9998, 0.9996, 0.9999, 0.9998, 0.9991, 0.9999, 0.9994, 0.9998,
        0.9998, 1.0000, 0.9999, 0.9998, 0.9993, 0.9985, 0.9995],
       device='cuda:0') torch.Size([16])
Epoch: 210 | Batch_idx: 0 |  Loss: (0.0687) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 210 | Batch_idx: 10 |  Loss: (0.0568) |  Loss2: (0.0000) | Acc: (98.00%) (1380/1408)
Epoch: 210 | Batch_idx: 20 |  Loss: (0.0518) |  Loss2: (0.0000) | Acc: (98.00%) (2644/2688)
Epoch: 210 | Batch_idx: 30 |  Loss: (0.0506) |  Loss2: (0.0000) | Acc: (98.00%) (3905/3968)
Epoch: 210 | Batch_idx: 40 |  Loss: (0.0495) |  Loss2: (0.0000) | Acc: (98.00%) (5164/5248)
Epoch: 210 | Batch_idx: 50 |  Loss: (0.0513) |  Loss2: (0.0000) | Acc: (98.00%) (6420/6528)
Epoch: 210 | Batch_idx: 60 |  Loss: (0.0525) |  Loss2: (0.0000) | Acc: (98.00%) (7680/7808)
Epoch: 210 | Batch_idx: 70 |  Loss: (0.0523) |  Loss2: (0.0000) | Acc: (98.00%) (8940/9088)
Epoch: 210 | Batch_idx: 80 |  Loss: (0.0512) |  Loss2: (0.0000) | Acc: (98.00%) (10205/10368)
Epoch: 210 | Batch_idx: 90 |  Loss: (0.0506) |  Loss2: (0.0000) | Acc: (98.00%) (11464/11648)
Epoch: 210 | Batch_idx: 100 |  Loss: (0.0499) |  Loss2: (0.0000) | Acc: (98.00%) (12728/12928)
Epoch: 210 | Batch_idx: 110 |  Loss: (0.0504) |  Loss2: (0.0000) | Acc: (98.00%) (13988/14208)
Epoch: 210 | Batch_idx: 120 |  Loss: (0.0504) |  Loss2: (0.0000) | Acc: (98.00%) (15250/15488)
Epoch: 210 | Batch_idx: 130 |  Loss: (0.0522) |  Loss2: (0.0000) | Acc: (98.00%) (16491/16768)
Epoch: 210 | Batch_idx: 140 |  Loss: (0.0519) |  Loss2: (0.0000) | Acc: (98.00%) (17755/18048)
Epoch: 210 | Batch_idx: 150 |  Loss: (0.0515) |  Loss2: (0.0000) | Acc: (98.00%) (19014/19328)
Epoch: 210 | Batch_idx: 160 |  Loss: (0.0514) |  Loss2: (0.0000) | Acc: (98.00%) (20273/20608)
Epoch: 210 | Batch_idx: 170 |  Loss: (0.0511) |  Loss2: (0.0000) | Acc: (98.00%) (21536/21888)
Epoch: 210 | Batch_idx: 180 |  Loss: (0.0506) |  Loss2: (0.0000) | Acc: (98.00%) (22803/23168)
Epoch: 210 | Batch_idx: 190 |  Loss: (0.0498) |  Loss2: (0.0000) | Acc: (98.00%) (24070/24448)
Epoch: 210 | Batch_idx: 200 |  Loss: (0.0496) |  Loss2: (0.0000) | Acc: (98.00%) (25331/25728)
Epoch: 210 | Batch_idx: 210 |  Loss: (0.0495) |  Loss2: (0.0000) | Acc: (98.00%) (26590/27008)
Epoch: 210 | Batch_idx: 220 |  Loss: (0.0489) |  Loss2: (0.0000) | Acc: (98.00%) (27857/28288)
Epoch: 210 | Batch_idx: 230 |  Loss: (0.0489) |  Loss2: (0.0000) | Acc: (98.00%) (29118/29568)
Epoch: 210 | Batch_idx: 240 |  Loss: (0.0493) |  Loss2: (0.0000) | Acc: (98.00%) (30374/30848)
Epoch: 210 | Batch_idx: 250 |  Loss: (0.0497) |  Loss2: (0.0000) | Acc: (98.00%) (31632/32128)
Epoch: 210 | Batch_idx: 260 |  Loss: (0.0502) |  Loss2: (0.0000) | Acc: (98.00%) (32880/33408)
Epoch: 210 | Batch_idx: 270 |  Loss: (0.0502) |  Loss2: (0.0000) | Acc: (98.00%) (34138/34688)
Epoch: 210 | Batch_idx: 280 |  Loss: (0.0511) |  Loss2: (0.0000) | Acc: (98.00%) (35383/35968)
Epoch: 210 | Batch_idx: 290 |  Loss: (0.0514) |  Loss2: (0.0000) | Acc: (98.00%) (36641/37248)
Epoch: 210 | Batch_idx: 300 |  Loss: (0.0517) |  Loss2: (0.0000) | Acc: (98.00%) (37894/38528)
Epoch: 210 | Batch_idx: 310 |  Loss: (0.0516) |  Loss2: (0.0000) | Acc: (98.00%) (39156/39808)
Epoch: 210 | Batch_idx: 320 |  Loss: (0.0519) |  Loss2: (0.0000) | Acc: (98.00%) (40410/41088)
Epoch: 210 | Batch_idx: 330 |  Loss: (0.0518) |  Loss2: (0.0000) | Acc: (98.00%) (41664/42368)
Epoch: 210 | Batch_idx: 340 |  Loss: (0.0517) |  Loss2: (0.0000) | Acc: (98.00%) (42923/43648)
Epoch: 210 | Batch_idx: 350 |  Loss: (0.0520) |  Loss2: (0.0000) | Acc: (98.00%) (44179/44928)
Epoch: 210 | Batch_idx: 360 |  Loss: (0.0519) |  Loss2: (0.0000) | Acc: (98.00%) (45439/46208)
Epoch: 210 | Batch_idx: 370 |  Loss: (0.0521) |  Loss2: (0.0000) | Acc: (98.00%) (46691/47488)
Epoch: 210 | Batch_idx: 380 |  Loss: (0.0530) |  Loss2: (0.0000) | Acc: (98.00%) (47930/48768)
Epoch: 210 | Batch_idx: 390 |  Loss: (0.0529) |  Loss2: (0.0000) | Acc: (98.00%) (49143/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_210.pth.tar'
# TEST : Loss: (0.4328) | Acc: (88.00%) (8857/10000)
percent tensor([0.5873, 0.6057, 0.5934, 0.5818, 0.5997, 0.5692, 0.6081, 0.6057, 0.6058,
        0.6005, 0.5996, 0.5988, 0.5957, 0.6077, 0.5907, 0.5829],
       device='cuda:0') torch.Size([16])
percent tensor([0.5547, 0.5454, 0.5251, 0.5442, 0.5472, 0.5202, 0.5565, 0.5753, 0.5451,
        0.5259, 0.5188, 0.5162, 0.5479, 0.5574, 0.5504, 0.5390],
       device='cuda:0') torch.Size([16])
percent tensor([0.5507, 0.5224, 0.4939, 0.5736, 0.4847, 0.5958, 0.5348, 0.5524, 0.5442,
        0.5050, 0.5359, 0.4932, 0.4814, 0.6468, 0.5621, 0.5651],
       device='cuda:0') torch.Size([16])
percent tensor([0.6925, 0.7245, 0.6669, 0.6440, 0.6400, 0.6697, 0.7008, 0.6355, 0.6897,
        0.7260, 0.7153, 0.7055, 0.7493, 0.6910, 0.6922, 0.7004],
       device='cuda:0') torch.Size([16])
percent tensor([0.6875, 0.5956, 0.6909, 0.7107, 0.7345, 0.7675, 0.6834, 0.6852, 0.7676,
        0.6789, 0.7308, 0.6833, 0.6243, 0.7511, 0.6369, 0.7336],
       device='cuda:0') torch.Size([16])
percent tensor([0.8206, 0.8250, 0.8486, 0.8393, 0.8651, 0.8461, 0.8443, 0.8255, 0.8372,
        0.8364, 0.8457, 0.8163, 0.8277, 0.8589, 0.8195, 0.8367],
       device='cuda:0') torch.Size([16])
percent tensor([0.4132, 0.5918, 0.5806, 0.5060, 0.6128, 0.6730, 0.5259, 0.3719, 0.5367,
        0.5220, 0.5370, 0.4574, 0.5423, 0.4532, 0.3609, 0.3838],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9998, 0.9995, 0.9999, 0.9997, 0.9994, 0.9998, 0.9995, 0.9998,
        0.9997, 1.0000, 0.9998, 0.9998, 0.9992, 0.9992, 0.9994],
       device='cuda:0') torch.Size([16])
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(182.2009, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(826.7021, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(813.8647, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1493.3715, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(476.7452, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2298.5347, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4282.8110, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1343.3999, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6343.0435, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11485.4219, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3771.5579, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(15896.5420, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 211 | Batch_idx: 0 |  Loss: (0.0356) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 211 | Batch_idx: 10 |  Loss: (0.0428) |  Loss2: (0.0000) | Acc: (98.00%) (1392/1408)
Epoch: 211 | Batch_idx: 20 |  Loss: (0.0458) |  Loss2: (0.0000) | Acc: (98.00%) (2650/2688)
Epoch: 211 | Batch_idx: 30 |  Loss: (0.0455) |  Loss2: (0.0000) | Acc: (98.00%) (3913/3968)
Epoch: 211 | Batch_idx: 40 |  Loss: (0.0457) |  Loss2: (0.0000) | Acc: (98.00%) (5175/5248)
Epoch: 211 | Batch_idx: 50 |  Loss: (0.0450) |  Loss2: (0.0000) | Acc: (98.00%) (6438/6528)
Epoch: 211 | Batch_idx: 60 |  Loss: (0.0456) |  Loss2: (0.0000) | Acc: (98.00%) (7699/7808)
Epoch: 211 | Batch_idx: 70 |  Loss: (0.0447) |  Loss2: (0.0000) | Acc: (98.00%) (8966/9088)
Epoch: 211 | Batch_idx: 80 |  Loss: (0.0443) |  Loss2: (0.0000) | Acc: (98.00%) (10229/10368)
Epoch: 211 | Batch_idx: 90 |  Loss: (0.0443) |  Loss2: (0.0000) | Acc: (98.00%) (11490/11648)
Epoch: 211 | Batch_idx: 100 |  Loss: (0.0442) |  Loss2: (0.0000) | Acc: (98.00%) (12753/12928)
Epoch: 211 | Batch_idx: 110 |  Loss: (0.0448) |  Loss2: (0.0000) | Acc: (98.00%) (14014/14208)
Epoch: 211 | Batch_idx: 120 |  Loss: (0.0461) |  Loss2: (0.0000) | Acc: (98.00%) (15262/15488)
Epoch: 211 | Batch_idx: 130 |  Loss: (0.0469) |  Loss2: (0.0000) | Acc: (98.00%) (16514/16768)
Epoch: 211 | Batch_idx: 140 |  Loss: (0.0478) |  Loss2: (0.0000) | Acc: (98.00%) (17766/18048)
Epoch: 211 | Batch_idx: 150 |  Loss: (0.0481) |  Loss2: (0.0000) | Acc: (98.00%) (19024/19328)
Epoch: 211 | Batch_idx: 160 |  Loss: (0.0484) |  Loss2: (0.0000) | Acc: (98.00%) (20278/20608)
Epoch: 211 | Batch_idx: 170 |  Loss: (0.0485) |  Loss2: (0.0000) | Acc: (98.00%) (21537/21888)
Epoch: 211 | Batch_idx: 180 |  Loss: (0.0491) |  Loss2: (0.0000) | Acc: (98.00%) (22793/23168)
Epoch: 211 | Batch_idx: 190 |  Loss: (0.0493) |  Loss2: (0.0000) | Acc: (98.00%) (24055/24448)
Epoch: 211 | Batch_idx: 200 |  Loss: (0.0494) |  Loss2: (0.0000) | Acc: (98.00%) (25310/25728)
Epoch: 211 | Batch_idx: 210 |  Loss: (0.0496) |  Loss2: (0.0000) | Acc: (98.00%) (26571/27008)
Epoch: 211 | Batch_idx: 220 |  Loss: (0.0496) |  Loss2: (0.0000) | Acc: (98.00%) (27832/28288)
Epoch: 211 | Batch_idx: 230 |  Loss: (0.0494) |  Loss2: (0.0000) | Acc: (98.00%) (29088/29568)
Epoch: 211 | Batch_idx: 240 |  Loss: (0.0489) |  Loss2: (0.0000) | Acc: (98.00%) (30352/30848)
Epoch: 211 | Batch_idx: 250 |  Loss: (0.0493) |  Loss2: (0.0000) | Acc: (98.00%) (31605/32128)
Epoch: 211 | Batch_idx: 260 |  Loss: (0.0492) |  Loss2: (0.0000) | Acc: (98.00%) (32866/33408)
Epoch: 211 | Batch_idx: 270 |  Loss: (0.0491) |  Loss2: (0.0000) | Acc: (98.00%) (34127/34688)
Epoch: 211 | Batch_idx: 280 |  Loss: (0.0488) |  Loss2: (0.0000) | Acc: (98.00%) (35391/35968)
Epoch: 211 | Batch_idx: 290 |  Loss: (0.0489) |  Loss2: (0.0000) | Acc: (98.00%) (36647/37248)
Epoch: 211 | Batch_idx: 300 |  Loss: (0.0488) |  Loss2: (0.0000) | Acc: (98.00%) (37908/38528)
Epoch: 211 | Batch_idx: 310 |  Loss: (0.0494) |  Loss2: (0.0000) | Acc: (98.00%) (39161/39808)
Epoch: 211 | Batch_idx: 320 |  Loss: (0.0492) |  Loss2: (0.0000) | Acc: (98.00%) (40424/41088)
Epoch: 211 | Batch_idx: 330 |  Loss: (0.0495) |  Loss2: (0.0000) | Acc: (98.00%) (41678/42368)
Epoch: 211 | Batch_idx: 340 |  Loss: (0.0494) |  Loss2: (0.0000) | Acc: (98.00%) (42945/43648)
Epoch: 211 | Batch_idx: 350 |  Loss: (0.0497) |  Loss2: (0.0000) | Acc: (98.00%) (44197/44928)
Epoch: 211 | Batch_idx: 360 |  Loss: (0.0498) |  Loss2: (0.0000) | Acc: (98.00%) (45457/46208)
Epoch: 211 | Batch_idx: 370 |  Loss: (0.0497) |  Loss2: (0.0000) | Acc: (98.00%) (46720/47488)
Epoch: 211 | Batch_idx: 380 |  Loss: (0.0500) |  Loss2: (0.0000) | Acc: (98.00%) (47979/48768)
Epoch: 211 | Batch_idx: 390 |  Loss: (0.0499) |  Loss2: (0.0000) | Acc: (98.00%) (49190/50000)
# TEST : Loss: (0.4229) | Acc: (88.00%) (8897/10000)
percent tensor([0.5849, 0.6021, 0.5914, 0.5789, 0.5961, 0.5657, 0.6051, 0.6030, 0.6027,
        0.5979, 0.5965, 0.5969, 0.5931, 0.6040, 0.5875, 0.5798],
       device='cuda:0') torch.Size([16])
percent tensor([0.5607, 0.5504, 0.5281, 0.5490, 0.5486, 0.5246, 0.5601, 0.5802, 0.5481,
        0.5323, 0.5245, 0.5222, 0.5553, 0.5621, 0.5571, 0.5454],
       device='cuda:0') torch.Size([16])
percent tensor([0.5520, 0.5188, 0.4948, 0.5765, 0.4832, 0.5896, 0.5306, 0.5494, 0.5401,
        0.5054, 0.5314, 0.4898, 0.4837, 0.6410, 0.5631, 0.5667],
       device='cuda:0') torch.Size([16])
percent tensor([0.6932, 0.7195, 0.6613, 0.6402, 0.6395, 0.6735, 0.6994, 0.6321, 0.6908,
        0.7208, 0.7153, 0.6988, 0.7475, 0.6884, 0.6954, 0.7000],
       device='cuda:0') torch.Size([16])
percent tensor([0.6924, 0.6122, 0.7031, 0.7264, 0.7437, 0.7752, 0.6990, 0.6956, 0.7781,
        0.6917, 0.7445, 0.7081, 0.6261, 0.7849, 0.6500, 0.7478],
       device='cuda:0') torch.Size([16])
percent tensor([0.8181, 0.8240, 0.8372, 0.8368, 0.8622, 0.8452, 0.8409, 0.8264, 0.8378,
        0.8355, 0.8497, 0.8136, 0.8252, 0.8637, 0.8166, 0.8400],
       device='cuda:0') torch.Size([16])
percent tensor([0.4398, 0.6290, 0.5963, 0.5556, 0.6675, 0.7098, 0.5627, 0.4315, 0.5985,
        0.5566, 0.5795, 0.4679, 0.5706, 0.5375, 0.3819, 0.4211],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9998, 0.9994, 0.9998, 0.9996, 0.9995, 0.9998, 0.9995, 0.9998,
        0.9998, 1.0000, 0.9999, 0.9998, 0.9992, 0.9992, 0.9994],
       device='cuda:0') torch.Size([16])
Epoch: 212 | Batch_idx: 0 |  Loss: (0.0237) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 212 | Batch_idx: 10 |  Loss: (0.0490) |  Loss2: (0.0000) | Acc: (98.00%) (1382/1408)
Epoch: 212 | Batch_idx: 20 |  Loss: (0.0481) |  Loss2: (0.0000) | Acc: (98.00%) (2641/2688)
Epoch: 212 | Batch_idx: 30 |  Loss: (0.0467) |  Loss2: (0.0000) | Acc: (98.00%) (3902/3968)
Epoch: 212 | Batch_idx: 40 |  Loss: (0.0445) |  Loss2: (0.0000) | Acc: (98.00%) (5168/5248)
Epoch: 212 | Batch_idx: 50 |  Loss: (0.0425) |  Loss2: (0.0000) | Acc: (98.00%) (6437/6528)
Epoch: 212 | Batch_idx: 60 |  Loss: (0.0437) |  Loss2: (0.0000) | Acc: (98.00%) (7696/7808)
Epoch: 212 | Batch_idx: 70 |  Loss: (0.0444) |  Loss2: (0.0000) | Acc: (98.00%) (8957/9088)
Epoch: 212 | Batch_idx: 80 |  Loss: (0.0445) |  Loss2: (0.0000) | Acc: (98.00%) (10222/10368)
Epoch: 212 | Batch_idx: 90 |  Loss: (0.0446) |  Loss2: (0.0000) | Acc: (98.00%) (11481/11648)
Epoch: 212 | Batch_idx: 100 |  Loss: (0.0447) |  Loss2: (0.0000) | Acc: (98.00%) (12742/12928)
Epoch: 212 | Batch_idx: 110 |  Loss: (0.0445) |  Loss2: (0.0000) | Acc: (98.00%) (14008/14208)
Epoch: 212 | Batch_idx: 120 |  Loss: (0.0446) |  Loss2: (0.0000) | Acc: (98.00%) (15270/15488)
Epoch: 212 | Batch_idx: 130 |  Loss: (0.0443) |  Loss2: (0.0000) | Acc: (98.00%) (16535/16768)
Epoch: 212 | Batch_idx: 140 |  Loss: (0.0440) |  Loss2: (0.0000) | Acc: (98.00%) (17796/18048)
Epoch: 212 | Batch_idx: 150 |  Loss: (0.0435) |  Loss2: (0.0000) | Acc: (98.00%) (19064/19328)
Epoch: 212 | Batch_idx: 160 |  Loss: (0.0438) |  Loss2: (0.0000) | Acc: (98.00%) (20326/20608)
Epoch: 212 | Batch_idx: 170 |  Loss: (0.0445) |  Loss2: (0.0000) | Acc: (98.00%) (21587/21888)
Epoch: 212 | Batch_idx: 180 |  Loss: (0.0445) |  Loss2: (0.0000) | Acc: (98.00%) (22849/23168)
Epoch: 212 | Batch_idx: 190 |  Loss: (0.0452) |  Loss2: (0.0000) | Acc: (98.00%) (24103/24448)
Epoch: 212 | Batch_idx: 200 |  Loss: (0.0454) |  Loss2: (0.0000) | Acc: (98.00%) (25366/25728)
Epoch: 212 | Batch_idx: 210 |  Loss: (0.0458) |  Loss2: (0.0000) | Acc: (98.00%) (26622/27008)
Epoch: 212 | Batch_idx: 220 |  Loss: (0.0460) |  Loss2: (0.0000) | Acc: (98.00%) (27880/28288)
Epoch: 212 | Batch_idx: 230 |  Loss: (0.0454) |  Loss2: (0.0000) | Acc: (98.00%) (29149/29568)
Epoch: 212 | Batch_idx: 240 |  Loss: (0.0462) |  Loss2: (0.0000) | Acc: (98.00%) (30397/30848)
Epoch: 212 | Batch_idx: 250 |  Loss: (0.0471) |  Loss2: (0.0000) | Acc: (98.00%) (31650/32128)
Epoch: 212 | Batch_idx: 260 |  Loss: (0.0474) |  Loss2: (0.0000) | Acc: (98.00%) (32910/33408)
Epoch: 212 | Batch_idx: 270 |  Loss: (0.0481) |  Loss2: (0.0000) | Acc: (98.00%) (34160/34688)
Epoch: 212 | Batch_idx: 280 |  Loss: (0.0484) |  Loss2: (0.0000) | Acc: (98.00%) (35413/35968)
Epoch: 212 | Batch_idx: 290 |  Loss: (0.0484) |  Loss2: (0.0000) | Acc: (98.00%) (36676/37248)
Epoch: 212 | Batch_idx: 300 |  Loss: (0.0487) |  Loss2: (0.0000) | Acc: (98.00%) (37933/38528)
Epoch: 212 | Batch_idx: 310 |  Loss: (0.0488) |  Loss2: (0.0000) | Acc: (98.00%) (39197/39808)
Epoch: 212 | Batch_idx: 320 |  Loss: (0.0491) |  Loss2: (0.0000) | Acc: (98.00%) (40454/41088)
Epoch: 212 | Batch_idx: 330 |  Loss: (0.0497) |  Loss2: (0.0000) | Acc: (98.00%) (41708/42368)
Epoch: 212 | Batch_idx: 340 |  Loss: (0.0497) |  Loss2: (0.0000) | Acc: (98.00%) (42965/43648)
Epoch: 212 | Batch_idx: 350 |  Loss: (0.0498) |  Loss2: (0.0000) | Acc: (98.00%) (44221/44928)
Epoch: 212 | Batch_idx: 360 |  Loss: (0.0494) |  Loss2: (0.0000) | Acc: (98.00%) (45485/46208)
Epoch: 212 | Batch_idx: 370 |  Loss: (0.0494) |  Loss2: (0.0000) | Acc: (98.00%) (46741/47488)
Epoch: 212 | Batch_idx: 380 |  Loss: (0.0496) |  Loss2: (0.0000) | Acc: (98.00%) (47996/48768)
Epoch: 212 | Batch_idx: 390 |  Loss: (0.0498) |  Loss2: (0.0000) | Acc: (98.00%) (49207/50000)
# TEST : Loss: (0.4199) | Acc: (89.00%) (8901/10000)
percent tensor([0.5871, 0.6039, 0.5916, 0.5809, 0.5975, 0.5669, 0.6070, 0.6053, 0.6053,
        0.6001, 0.5986, 0.5988, 0.5959, 0.6055, 0.5894, 0.5817],
       device='cuda:0') torch.Size([16])
percent tensor([0.5567, 0.5466, 0.5236, 0.5424, 0.5441, 0.5207, 0.5549, 0.5758, 0.5459,
        0.5272, 0.5208, 0.5159, 0.5511, 0.5565, 0.5521, 0.5396],
       device='cuda:0') torch.Size([16])
percent tensor([0.5532, 0.5200, 0.4853, 0.5622, 0.4817, 0.5904, 0.5310, 0.5389, 0.5462,
        0.5016, 0.5379, 0.4825, 0.4795, 0.6463, 0.5599, 0.5617],
       device='cuda:0') torch.Size([16])
percent tensor([0.6973, 0.7260, 0.6649, 0.6546, 0.6442, 0.6809, 0.7054, 0.6370, 0.6892,
        0.7265, 0.7170, 0.7002, 0.7475, 0.6970, 0.6985, 0.7068],
       device='cuda:0') torch.Size([16])
percent tensor([0.6866, 0.5985, 0.7318, 0.7370, 0.7573, 0.7744, 0.7038, 0.6942, 0.7717,
        0.7035, 0.7256, 0.7249, 0.6189, 0.7708, 0.6338, 0.7402],
       device='cuda:0') torch.Size([16])
percent tensor([0.8190, 0.8258, 0.8501, 0.8403, 0.8705, 0.8423, 0.8430, 0.8326, 0.8374,
        0.8380, 0.8437, 0.8195, 0.8250, 0.8595, 0.8166, 0.8372],
       device='cuda:0') torch.Size([16])
percent tensor([0.4233, 0.6395, 0.6037, 0.5771, 0.6526, 0.6756, 0.5629, 0.4336, 0.5814,
        0.5348, 0.5530, 0.5155, 0.5964, 0.4813, 0.3653, 0.4018],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9998, 0.9996, 0.9999, 0.9997, 0.9995, 0.9998, 0.9994, 0.9998,
        0.9997, 1.0000, 0.9999, 0.9998, 0.9995, 0.9993, 0.9994],
       device='cuda:0') torch.Size([16])
Epoch: 213 | Batch_idx: 0 |  Loss: (0.0503) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 213 | Batch_idx: 10 |  Loss: (0.0428) |  Loss2: (0.0000) | Acc: (98.00%) (1391/1408)
Epoch: 213 | Batch_idx: 20 |  Loss: (0.0412) |  Loss2: (0.0000) | Acc: (98.00%) (2653/2688)
Epoch: 213 | Batch_idx: 30 |  Loss: (0.0439) |  Loss2: (0.0000) | Acc: (98.00%) (3915/3968)
Epoch: 213 | Batch_idx: 40 |  Loss: (0.0467) |  Loss2: (0.0000) | Acc: (98.00%) (5174/5248)
Epoch: 213 | Batch_idx: 50 |  Loss: (0.0489) |  Loss2: (0.0000) | Acc: (98.00%) (6427/6528)
Epoch: 213 | Batch_idx: 60 |  Loss: (0.0482) |  Loss2: (0.0000) | Acc: (98.00%) (7691/7808)
Epoch: 213 | Batch_idx: 70 |  Loss: (0.0475) |  Loss2: (0.0000) | Acc: (98.00%) (8954/9088)
Epoch: 213 | Batch_idx: 80 |  Loss: (0.0482) |  Loss2: (0.0000) | Acc: (98.00%) (10211/10368)
Epoch: 213 | Batch_idx: 90 |  Loss: (0.0498) |  Loss2: (0.0000) | Acc: (98.00%) (11466/11648)
Epoch: 213 | Batch_idx: 100 |  Loss: (0.0496) |  Loss2: (0.0000) | Acc: (98.00%) (12722/12928)
Epoch: 213 | Batch_idx: 110 |  Loss: (0.0492) |  Loss2: (0.0000) | Acc: (98.00%) (13989/14208)
Epoch: 213 | Batch_idx: 120 |  Loss: (0.0500) |  Loss2: (0.0000) | Acc: (98.00%) (15248/15488)
Epoch: 213 | Batch_idx: 130 |  Loss: (0.0503) |  Loss2: (0.0000) | Acc: (98.00%) (16506/16768)
Epoch: 213 | Batch_idx: 140 |  Loss: (0.0500) |  Loss2: (0.0000) | Acc: (98.00%) (17769/18048)
Epoch: 213 | Batch_idx: 150 |  Loss: (0.0515) |  Loss2: (0.0000) | Acc: (98.00%) (19020/19328)
Epoch: 213 | Batch_idx: 160 |  Loss: (0.0518) |  Loss2: (0.0000) | Acc: (98.00%) (20275/20608)
Epoch: 213 | Batch_idx: 170 |  Loss: (0.0510) |  Loss2: (0.0000) | Acc: (98.00%) (21540/21888)
Epoch: 213 | Batch_idx: 180 |  Loss: (0.0506) |  Loss2: (0.0000) | Acc: (98.00%) (22804/23168)
Epoch: 213 | Batch_idx: 190 |  Loss: (0.0505) |  Loss2: (0.0000) | Acc: (98.00%) (24065/24448)
Epoch: 213 | Batch_idx: 200 |  Loss: (0.0500) |  Loss2: (0.0000) | Acc: (98.00%) (25329/25728)
Epoch: 213 | Batch_idx: 210 |  Loss: (0.0502) |  Loss2: (0.0000) | Acc: (98.00%) (26586/27008)
Epoch: 213 | Batch_idx: 220 |  Loss: (0.0502) |  Loss2: (0.0000) | Acc: (98.00%) (27846/28288)
Epoch: 213 | Batch_idx: 230 |  Loss: (0.0500) |  Loss2: (0.0000) | Acc: (98.00%) (29109/29568)
Epoch: 213 | Batch_idx: 240 |  Loss: (0.0499) |  Loss2: (0.0000) | Acc: (98.00%) (30374/30848)
Epoch: 213 | Batch_idx: 250 |  Loss: (0.0498) |  Loss2: (0.0000) | Acc: (98.00%) (31633/32128)
Epoch: 213 | Batch_idx: 260 |  Loss: (0.0499) |  Loss2: (0.0000) | Acc: (98.00%) (32890/33408)
Epoch: 213 | Batch_idx: 270 |  Loss: (0.0501) |  Loss2: (0.0000) | Acc: (98.00%) (34148/34688)
Epoch: 213 | Batch_idx: 280 |  Loss: (0.0497) |  Loss2: (0.0000) | Acc: (98.00%) (35411/35968)
Epoch: 213 | Batch_idx: 290 |  Loss: (0.0498) |  Loss2: (0.0000) | Acc: (98.00%) (36666/37248)
Epoch: 213 | Batch_idx: 300 |  Loss: (0.0501) |  Loss2: (0.0000) | Acc: (98.00%) (37917/38528)
Epoch: 213 | Batch_idx: 310 |  Loss: (0.0499) |  Loss2: (0.0000) | Acc: (98.00%) (39178/39808)
Epoch: 213 | Batch_idx: 320 |  Loss: (0.0497) |  Loss2: (0.0000) | Acc: (98.00%) (40440/41088)
Epoch: 213 | Batch_idx: 330 |  Loss: (0.0497) |  Loss2: (0.0000) | Acc: (98.00%) (41699/42368)
Epoch: 213 | Batch_idx: 340 |  Loss: (0.0500) |  Loss2: (0.0000) | Acc: (98.00%) (42955/43648)
Epoch: 213 | Batch_idx: 350 |  Loss: (0.0506) |  Loss2: (0.0000) | Acc: (98.00%) (44201/44928)
Epoch: 213 | Batch_idx: 360 |  Loss: (0.0509) |  Loss2: (0.0000) | Acc: (98.00%) (45454/46208)
Epoch: 213 | Batch_idx: 370 |  Loss: (0.0510) |  Loss2: (0.0000) | Acc: (98.00%) (46710/47488)
Epoch: 213 | Batch_idx: 380 |  Loss: (0.0510) |  Loss2: (0.0000) | Acc: (98.00%) (47971/48768)
Epoch: 213 | Batch_idx: 390 |  Loss: (0.0506) |  Loss2: (0.0000) | Acc: (98.00%) (49191/50000)
# TEST : Loss: (0.4270) | Acc: (88.00%) (8891/10000)
percent tensor([0.5874, 0.6045, 0.5940, 0.5819, 0.5997, 0.5679, 0.6078, 0.6058, 0.6065,
        0.6006, 0.5992, 0.5993, 0.5959, 0.6067, 0.5900, 0.5826],
       device='cuda:0') torch.Size([16])
percent tensor([0.5563, 0.5475, 0.5236, 0.5454, 0.5441, 0.5232, 0.5549, 0.5771, 0.5449,
        0.5282, 0.5224, 0.5157, 0.5521, 0.5577, 0.5552, 0.5427],
       device='cuda:0') torch.Size([16])
percent tensor([0.5564, 0.5273, 0.5021, 0.5775, 0.4919, 0.5921, 0.5393, 0.5546, 0.5517,
        0.5140, 0.5435, 0.5010, 0.4867, 0.6508, 0.5678, 0.5675],
       device='cuda:0') torch.Size([16])
percent tensor([0.6939, 0.7230, 0.6666, 0.6498, 0.6428, 0.6820, 0.7039, 0.6321, 0.6868,
        0.7221, 0.7129, 0.6992, 0.7440, 0.6915, 0.6973, 0.7050],
       device='cuda:0') torch.Size([16])
percent tensor([0.7001, 0.6150, 0.7158, 0.7238, 0.7463, 0.7856, 0.7031, 0.7018, 0.7648,
        0.7041, 0.7308, 0.7193, 0.6176, 0.7755, 0.6527, 0.7472],
       device='cuda:0') torch.Size([16])
percent tensor([0.8214, 0.8266, 0.8485, 0.8366, 0.8681, 0.8457, 0.8482, 0.8348, 0.8429,
        0.8372, 0.8471, 0.8220, 0.8253, 0.8616, 0.8193, 0.8408],
       device='cuda:0') torch.Size([16])
percent tensor([0.4440, 0.6327, 0.5965, 0.5491, 0.6522, 0.6839, 0.5657, 0.4001, 0.6084,
        0.5382, 0.5935, 0.4846, 0.5960, 0.4942, 0.3647, 0.4058],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9998, 0.9996, 0.9999, 0.9997, 0.9994, 0.9998, 0.9995, 0.9999,
        0.9998, 1.0000, 0.9998, 0.9998, 0.9996, 0.9992, 0.9995],
       device='cuda:0') torch.Size([16])
Epoch: 214 | Batch_idx: 0 |  Loss: (0.0767) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 214 | Batch_idx: 10 |  Loss: (0.0499) |  Loss2: (0.0000) | Acc: (98.00%) (1388/1408)
Epoch: 214 | Batch_idx: 20 |  Loss: (0.0463) |  Loss2: (0.0000) | Acc: (98.00%) (2649/2688)
Epoch: 214 | Batch_idx: 30 |  Loss: (0.0459) |  Loss2: (0.0000) | Acc: (98.00%) (3914/3968)
Epoch: 214 | Batch_idx: 40 |  Loss: (0.0444) |  Loss2: (0.0000) | Acc: (98.00%) (5181/5248)
Epoch: 214 | Batch_idx: 50 |  Loss: (0.0427) |  Loss2: (0.0000) | Acc: (98.00%) (6452/6528)
Epoch: 214 | Batch_idx: 60 |  Loss: (0.0431) |  Loss2: (0.0000) | Acc: (98.00%) (7712/7808)
Epoch: 214 | Batch_idx: 70 |  Loss: (0.0439) |  Loss2: (0.0000) | Acc: (98.00%) (8974/9088)
Epoch: 214 | Batch_idx: 80 |  Loss: (0.0443) |  Loss2: (0.0000) | Acc: (98.00%) (10236/10368)
Epoch: 214 | Batch_idx: 90 |  Loss: (0.0439) |  Loss2: (0.0000) | Acc: (98.00%) (11500/11648)
Epoch: 214 | Batch_idx: 100 |  Loss: (0.0428) |  Loss2: (0.0000) | Acc: (98.00%) (12770/12928)
Epoch: 214 | Batch_idx: 110 |  Loss: (0.0436) |  Loss2: (0.0000) | Acc: (98.00%) (14030/14208)
Epoch: 214 | Batch_idx: 120 |  Loss: (0.0431) |  Loss2: (0.0000) | Acc: (98.00%) (15294/15488)
Epoch: 214 | Batch_idx: 130 |  Loss: (0.0430) |  Loss2: (0.0000) | Acc: (98.00%) (16553/16768)
Epoch: 214 | Batch_idx: 140 |  Loss: (0.0423) |  Loss2: (0.0000) | Acc: (98.00%) (17821/18048)
Epoch: 214 | Batch_idx: 150 |  Loss: (0.0422) |  Loss2: (0.0000) | Acc: (98.00%) (19083/19328)
Epoch: 214 | Batch_idx: 160 |  Loss: (0.0422) |  Loss2: (0.0000) | Acc: (98.00%) (20344/20608)
Epoch: 214 | Batch_idx: 170 |  Loss: (0.0432) |  Loss2: (0.0000) | Acc: (98.00%) (21592/21888)
Epoch: 214 | Batch_idx: 180 |  Loss: (0.0435) |  Loss2: (0.0000) | Acc: (98.00%) (22852/23168)
Epoch: 214 | Batch_idx: 190 |  Loss: (0.0443) |  Loss2: (0.0000) | Acc: (98.00%) (24103/24448)
Epoch: 214 | Batch_idx: 200 |  Loss: (0.0445) |  Loss2: (0.0000) | Acc: (98.00%) (25361/25728)
Epoch: 214 | Batch_idx: 210 |  Loss: (0.0453) |  Loss2: (0.0000) | Acc: (98.00%) (26614/27008)
Epoch: 214 | Batch_idx: 220 |  Loss: (0.0457) |  Loss2: (0.0000) | Acc: (98.00%) (27869/28288)
Epoch: 214 | Batch_idx: 230 |  Loss: (0.0459) |  Loss2: (0.0000) | Acc: (98.00%) (29126/29568)
Epoch: 214 | Batch_idx: 240 |  Loss: (0.0465) |  Loss2: (0.0000) | Acc: (98.00%) (30379/30848)
Epoch: 214 | Batch_idx: 250 |  Loss: (0.0462) |  Loss2: (0.0000) | Acc: (98.00%) (31643/32128)
Epoch: 214 | Batch_idx: 260 |  Loss: (0.0470) |  Loss2: (0.0000) | Acc: (98.00%) (32896/33408)
Epoch: 214 | Batch_idx: 270 |  Loss: (0.0473) |  Loss2: (0.0000) | Acc: (98.00%) (34154/34688)
Epoch: 214 | Batch_idx: 280 |  Loss: (0.0474) |  Loss2: (0.0000) | Acc: (98.00%) (35409/35968)
Epoch: 214 | Batch_idx: 290 |  Loss: (0.0474) |  Loss2: (0.0000) | Acc: (98.00%) (36671/37248)
Epoch: 214 | Batch_idx: 300 |  Loss: (0.0472) |  Loss2: (0.0000) | Acc: (98.00%) (37937/38528)
Epoch: 214 | Batch_idx: 310 |  Loss: (0.0470) |  Loss2: (0.0000) | Acc: (98.00%) (39200/39808)
Epoch: 214 | Batch_idx: 320 |  Loss: (0.0470) |  Loss2: (0.0000) | Acc: (98.00%) (40458/41088)
Epoch: 214 | Batch_idx: 330 |  Loss: (0.0472) |  Loss2: (0.0000) | Acc: (98.00%) (41711/42368)
Epoch: 214 | Batch_idx: 340 |  Loss: (0.0477) |  Loss2: (0.0000) | Acc: (98.00%) (42965/43648)
Epoch: 214 | Batch_idx: 350 |  Loss: (0.0484) |  Loss2: (0.0000) | Acc: (98.00%) (44211/44928)
Epoch: 214 | Batch_idx: 360 |  Loss: (0.0483) |  Loss2: (0.0000) | Acc: (98.00%) (45480/46208)
Epoch: 214 | Batch_idx: 370 |  Loss: (0.0480) |  Loss2: (0.0000) | Acc: (98.00%) (46746/47488)
Epoch: 214 | Batch_idx: 380 |  Loss: (0.0479) |  Loss2: (0.0000) | Acc: (98.00%) (48006/48768)
Epoch: 214 | Batch_idx: 390 |  Loss: (0.0480) |  Loss2: (0.0000) | Acc: (98.00%) (49222/50000)
# TEST : Loss: (0.4382) | Acc: (88.00%) (8850/10000)
percent tensor([0.5864, 0.6025, 0.5954, 0.5812, 0.6012, 0.5678, 0.6069, 0.6051, 0.6047,
        0.5997, 0.5971, 0.6003, 0.5946, 0.6041, 0.5887, 0.5810],
       device='cuda:0') torch.Size([16])
percent tensor([0.5603, 0.5501, 0.5282, 0.5493, 0.5484, 0.5279, 0.5565, 0.5793, 0.5483,
        0.5311, 0.5252, 0.5210, 0.5561, 0.5602, 0.5590, 0.5450],
       device='cuda:0') torch.Size([16])
percent tensor([0.5540, 0.5118, 0.4967, 0.5679, 0.4850, 0.5945, 0.5266, 0.5463, 0.5431,
        0.5031, 0.5334, 0.4942, 0.4836, 0.6355, 0.5625, 0.5624],
       device='cuda:0') torch.Size([16])
percent tensor([0.6974, 0.7246, 0.6709, 0.6503, 0.6475, 0.6849, 0.7091, 0.6371, 0.6889,
        0.7251, 0.7159, 0.7021, 0.7463, 0.6877, 0.7026, 0.7063],
       device='cuda:0') torch.Size([16])
percent tensor([0.7053, 0.6192, 0.7278, 0.7255, 0.7567, 0.7842, 0.7197, 0.7111, 0.7792,
        0.6998, 0.7382, 0.7112, 0.6343, 0.7847, 0.6518, 0.7511],
       device='cuda:0') torch.Size([16])
percent tensor([0.8269, 0.8290, 0.8525, 0.8376, 0.8714, 0.8532, 0.8515, 0.8400, 0.8412,
        0.8383, 0.8500, 0.8211, 0.8302, 0.8586, 0.8216, 0.8459],
       device='cuda:0') torch.Size([16])
percent tensor([0.4467, 0.6573, 0.5811, 0.5602, 0.6453, 0.6990, 0.5549, 0.4024, 0.6267,
        0.5584, 0.5999, 0.4995, 0.6115, 0.5041, 0.3712, 0.4161],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9998, 0.9994, 0.9999, 0.9998, 0.9993, 0.9998, 0.9993, 0.9998,
        0.9997, 1.0000, 0.9999, 0.9998, 0.9994, 0.9992, 0.9993],
       device='cuda:0') torch.Size([16])
Epoch: 215 | Batch_idx: 0 |  Loss: (0.0130) |  Loss2: (0.0000) | Acc: (100.00%) (128/128)
Epoch: 215 | Batch_idx: 10 |  Loss: (0.0415) |  Loss2: (0.0000) | Acc: (98.00%) (1392/1408)
Epoch: 215 | Batch_idx: 20 |  Loss: (0.0476) |  Loss2: (0.0000) | Acc: (98.00%) (2655/2688)
Epoch: 215 | Batch_idx: 30 |  Loss: (0.0463) |  Loss2: (0.0000) | Acc: (98.00%) (3917/3968)
Epoch: 215 | Batch_idx: 40 |  Loss: (0.0442) |  Loss2: (0.0000) | Acc: (98.00%) (5186/5248)
Epoch: 215 | Batch_idx: 50 |  Loss: (0.0453) |  Loss2: (0.0000) | Acc: (98.00%) (6444/6528)
Epoch: 215 | Batch_idx: 60 |  Loss: (0.0471) |  Loss2: (0.0000) | Acc: (98.00%) (7700/7808)
Epoch: 215 | Batch_idx: 70 |  Loss: (0.0458) |  Loss2: (0.0000) | Acc: (98.00%) (8970/9088)
Epoch: 215 | Batch_idx: 80 |  Loss: (0.0461) |  Loss2: (0.0000) | Acc: (98.00%) (10231/10368)
Epoch: 215 | Batch_idx: 90 |  Loss: (0.0470) |  Loss2: (0.0000) | Acc: (98.00%) (11479/11648)
Epoch: 215 | Batch_idx: 100 |  Loss: (0.0469) |  Loss2: (0.0000) | Acc: (98.00%) (12738/12928)
Epoch: 215 | Batch_idx: 110 |  Loss: (0.0480) |  Loss2: (0.0000) | Acc: (98.00%) (13992/14208)
Epoch: 215 | Batch_idx: 120 |  Loss: (0.0485) |  Loss2: (0.0000) | Acc: (98.00%) (15245/15488)
Epoch: 215 | Batch_idx: 130 |  Loss: (0.0482) |  Loss2: (0.0000) | Acc: (98.00%) (16509/16768)
Epoch: 215 | Batch_idx: 140 |  Loss: (0.0484) |  Loss2: (0.0000) | Acc: (98.00%) (17772/18048)
Epoch: 215 | Batch_idx: 150 |  Loss: (0.0477) |  Loss2: (0.0000) | Acc: (98.00%) (19039/19328)
Epoch: 215 | Batch_idx: 160 |  Loss: (0.0484) |  Loss2: (0.0000) | Acc: (98.00%) (20294/20608)
Epoch: 215 | Batch_idx: 170 |  Loss: (0.0485) |  Loss2: (0.0000) | Acc: (98.00%) (21549/21888)
Epoch: 215 | Batch_idx: 180 |  Loss: (0.0496) |  Loss2: (0.0000) | Acc: (98.00%) (22802/23168)
Epoch: 215 | Batch_idx: 190 |  Loss: (0.0499) |  Loss2: (0.0000) | Acc: (98.00%) (24062/24448)
Epoch: 215 | Batch_idx: 200 |  Loss: (0.0497) |  Loss2: (0.0000) | Acc: (98.00%) (25325/25728)
Epoch: 215 | Batch_idx: 210 |  Loss: (0.0494) |  Loss2: (0.0000) | Acc: (98.00%) (26585/27008)
Epoch: 215 | Batch_idx: 220 |  Loss: (0.0490) |  Loss2: (0.0000) | Acc: (98.00%) (27848/28288)
Epoch: 215 | Batch_idx: 230 |  Loss: (0.0489) |  Loss2: (0.0000) | Acc: (98.00%) (29110/29568)
Epoch: 215 | Batch_idx: 240 |  Loss: (0.0487) |  Loss2: (0.0000) | Acc: (98.00%) (30372/30848)
Epoch: 215 | Batch_idx: 250 |  Loss: (0.0488) |  Loss2: (0.0000) | Acc: (98.00%) (31627/32128)
Epoch: 215 | Batch_idx: 260 |  Loss: (0.0490) |  Loss2: (0.0000) | Acc: (98.00%) (32885/33408)
Epoch: 215 | Batch_idx: 270 |  Loss: (0.0489) |  Loss2: (0.0000) | Acc: (98.00%) (34144/34688)
Epoch: 215 | Batch_idx: 280 |  Loss: (0.0487) |  Loss2: (0.0000) | Acc: (98.00%) (35408/35968)
Epoch: 215 | Batch_idx: 290 |  Loss: (0.0484) |  Loss2: (0.0000) | Acc: (98.00%) (36672/37248)
Epoch: 215 | Batch_idx: 300 |  Loss: (0.0485) |  Loss2: (0.0000) | Acc: (98.00%) (37930/38528)
Epoch: 215 | Batch_idx: 310 |  Loss: (0.0481) |  Loss2: (0.0000) | Acc: (98.00%) (39198/39808)
Epoch: 215 | Batch_idx: 320 |  Loss: (0.0480) |  Loss2: (0.0000) | Acc: (98.00%) (40460/41088)
Epoch: 215 | Batch_idx: 330 |  Loss: (0.0477) |  Loss2: (0.0000) | Acc: (98.00%) (41726/42368)
Epoch: 215 | Batch_idx: 340 |  Loss: (0.0478) |  Loss2: (0.0000) | Acc: (98.00%) (42989/43648)
Epoch: 215 | Batch_idx: 350 |  Loss: (0.0477) |  Loss2: (0.0000) | Acc: (98.00%) (44252/44928)
Epoch: 215 | Batch_idx: 360 |  Loss: (0.0479) |  Loss2: (0.0000) | Acc: (98.00%) (45509/46208)
Epoch: 215 | Batch_idx: 370 |  Loss: (0.0477) |  Loss2: (0.0000) | Acc: (98.00%) (46777/47488)
Epoch: 215 | Batch_idx: 380 |  Loss: (0.0480) |  Loss2: (0.0000) | Acc: (98.00%) (48031/48768)
Epoch: 215 | Batch_idx: 390 |  Loss: (0.0479) |  Loss2: (0.0000) | Acc: (98.00%) (49248/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_215.pth.tar'
# TEST : Loss: (0.4000) | Acc: (89.00%) (8945/10000)
percent tensor([0.5874, 0.6059, 0.5925, 0.5811, 0.5994, 0.5691, 0.6085, 0.6058, 0.6064,
        0.6008, 0.6001, 0.5987, 0.5963, 0.6077, 0.5910, 0.5825],
       device='cuda:0') torch.Size([16])
percent tensor([0.5592, 0.5481, 0.5268, 0.5422, 0.5460, 0.5241, 0.5575, 0.5763, 0.5481,
        0.5300, 0.5246, 0.5200, 0.5537, 0.5575, 0.5561, 0.5424],
       device='cuda:0') torch.Size([16])
percent tensor([0.5625, 0.5262, 0.5037, 0.5766, 0.4954, 0.5977, 0.5368, 0.5533, 0.5505,
        0.5132, 0.5396, 0.5030, 0.4921, 0.6437, 0.5738, 0.5705],
       device='cuda:0') torch.Size([16])
percent tensor([0.7020, 0.7271, 0.6752, 0.6488, 0.6498, 0.6904, 0.7107, 0.6384, 0.6948,
        0.7279, 0.7191, 0.7084, 0.7499, 0.6919, 0.7043, 0.7073],
       device='cuda:0') torch.Size([16])
percent tensor([0.6861, 0.5939, 0.7032, 0.7337, 0.7437, 0.7720, 0.6890, 0.6915, 0.7672,
        0.6871, 0.7094, 0.6953, 0.6098, 0.7877, 0.6252, 0.7324],
       device='cuda:0') torch.Size([16])
percent tensor([0.8175, 0.8251, 0.8462, 0.8368, 0.8659, 0.8455, 0.8434, 0.8299, 0.8428,
        0.8344, 0.8425, 0.8169, 0.8274, 0.8614, 0.8155, 0.8371],
       device='cuda:0') torch.Size([16])
percent tensor([0.4251, 0.6364, 0.5977, 0.5389, 0.6295, 0.7076, 0.5554, 0.3924, 0.6088,
        0.5598, 0.5861, 0.5037, 0.6112, 0.4977, 0.3912, 0.4138],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9998, 0.9996, 0.9999, 0.9997, 0.9994, 0.9998, 0.9995, 0.9998,
        0.9997, 1.0000, 0.9998, 0.9998, 0.9995, 0.9992, 0.9994],
       device='cuda:0') torch.Size([16])
Epoch: 216 | Batch_idx: 0 |  Loss: (0.0183) |  Loss2: (0.0000) | Acc: (100.00%) (128/128)
Epoch: 216 | Batch_idx: 10 |  Loss: (0.0394) |  Loss2: (0.0000) | Acc: (98.00%) (1391/1408)
Epoch: 216 | Batch_idx: 20 |  Loss: (0.0393) |  Loss2: (0.0000) | Acc: (98.00%) (2655/2688)
Epoch: 216 | Batch_idx: 30 |  Loss: (0.0398) |  Loss2: (0.0000) | Acc: (98.00%) (3919/3968)
Epoch: 216 | Batch_idx: 40 |  Loss: (0.0419) |  Loss2: (0.0000) | Acc: (98.00%) (5178/5248)
Epoch: 216 | Batch_idx: 50 |  Loss: (0.0409) |  Loss2: (0.0000) | Acc: (98.00%) (6447/6528)
Epoch: 216 | Batch_idx: 60 |  Loss: (0.0406) |  Loss2: (0.0000) | Acc: (98.00%) (7715/7808)
Epoch: 216 | Batch_idx: 70 |  Loss: (0.0396) |  Loss2: (0.0000) | Acc: (98.00%) (8981/9088)
Epoch: 216 | Batch_idx: 80 |  Loss: (0.0405) |  Loss2: (0.0000) | Acc: (98.00%) (10243/10368)
Epoch: 216 | Batch_idx: 90 |  Loss: (0.0399) |  Loss2: (0.0000) | Acc: (98.00%) (11509/11648)
Epoch: 216 | Batch_idx: 100 |  Loss: (0.0397) |  Loss2: (0.0000) | Acc: (98.00%) (12774/12928)
Epoch: 216 | Batch_idx: 110 |  Loss: (0.0405) |  Loss2: (0.0000) | Acc: (98.00%) (14035/14208)
Epoch: 216 | Batch_idx: 120 |  Loss: (0.0400) |  Loss2: (0.0000) | Acc: (98.00%) (15304/15488)
Epoch: 216 | Batch_idx: 130 |  Loss: (0.0402) |  Loss2: (0.0000) | Acc: (98.00%) (16572/16768)
Epoch: 216 | Batch_idx: 140 |  Loss: (0.0414) |  Loss2: (0.0000) | Acc: (98.00%) (17827/18048)
Epoch: 216 | Batch_idx: 150 |  Loss: (0.0420) |  Loss2: (0.0000) | Acc: (98.00%) (19083/19328)
Epoch: 216 | Batch_idx: 160 |  Loss: (0.0423) |  Loss2: (0.0000) | Acc: (98.00%) (20347/20608)
Epoch: 216 | Batch_idx: 170 |  Loss: (0.0435) |  Loss2: (0.0000) | Acc: (98.00%) (21602/21888)
Epoch: 216 | Batch_idx: 180 |  Loss: (0.0436) |  Loss2: (0.0000) | Acc: (98.00%) (22866/23168)
Epoch: 216 | Batch_idx: 190 |  Loss: (0.0437) |  Loss2: (0.0000) | Acc: (98.00%) (24127/24448)
Epoch: 216 | Batch_idx: 200 |  Loss: (0.0437) |  Loss2: (0.0000) | Acc: (98.00%) (25388/25728)
Epoch: 216 | Batch_idx: 210 |  Loss: (0.0446) |  Loss2: (0.0000) | Acc: (98.00%) (26641/27008)
Epoch: 216 | Batch_idx: 220 |  Loss: (0.0444) |  Loss2: (0.0000) | Acc: (98.00%) (27907/28288)
Epoch: 216 | Batch_idx: 230 |  Loss: (0.0446) |  Loss2: (0.0000) | Acc: (98.00%) (29167/29568)
Epoch: 216 | Batch_idx: 240 |  Loss: (0.0449) |  Loss2: (0.0000) | Acc: (98.00%) (30426/30848)
Epoch: 216 | Batch_idx: 250 |  Loss: (0.0449) |  Loss2: (0.0000) | Acc: (98.00%) (31687/32128)
Epoch: 216 | Batch_idx: 260 |  Loss: (0.0452) |  Loss2: (0.0000) | Acc: (98.00%) (32947/33408)
Epoch: 216 | Batch_idx: 270 |  Loss: (0.0452) |  Loss2: (0.0000) | Acc: (98.00%) (34209/34688)
Epoch: 216 | Batch_idx: 280 |  Loss: (0.0452) |  Loss2: (0.0000) | Acc: (98.00%) (35471/35968)
Epoch: 216 | Batch_idx: 290 |  Loss: (0.0452) |  Loss2: (0.0000) | Acc: (98.00%) (36736/37248)
Epoch: 216 | Batch_idx: 300 |  Loss: (0.0456) |  Loss2: (0.0000) | Acc: (98.00%) (37994/38528)
Epoch: 216 | Batch_idx: 310 |  Loss: (0.0457) |  Loss2: (0.0000) | Acc: (98.00%) (39257/39808)
Epoch: 216 | Batch_idx: 320 |  Loss: (0.0455) |  Loss2: (0.0000) | Acc: (98.00%) (40523/41088)
Epoch: 216 | Batch_idx: 330 |  Loss: (0.0458) |  Loss2: (0.0000) | Acc: (98.00%) (41777/42368)
Epoch: 216 | Batch_idx: 340 |  Loss: (0.0458) |  Loss2: (0.0000) | Acc: (98.00%) (43037/43648)
Epoch: 216 | Batch_idx: 350 |  Loss: (0.0459) |  Loss2: (0.0000) | Acc: (98.00%) (44298/44928)
Epoch: 216 | Batch_idx: 360 |  Loss: (0.0456) |  Loss2: (0.0000) | Acc: (98.00%) (45564/46208)
Epoch: 216 | Batch_idx: 370 |  Loss: (0.0460) |  Loss2: (0.0000) | Acc: (98.00%) (46820/47488)
Epoch: 216 | Batch_idx: 380 |  Loss: (0.0459) |  Loss2: (0.0000) | Acc: (98.00%) (48081/48768)
Epoch: 216 | Batch_idx: 390 |  Loss: (0.0461) |  Loss2: (0.0000) | Acc: (98.00%) (49291/50000)
# TEST : Loss: (0.4227) | Acc: (89.00%) (8935/10000)
percent tensor([0.5872, 0.6047, 0.5940, 0.5807, 0.5999, 0.5680, 0.6078, 0.6060, 0.6064,
        0.6007, 0.5994, 0.5995, 0.5961, 0.6061, 0.5898, 0.5822],
       device='cuda:0') torch.Size([16])
percent tensor([0.5579, 0.5475, 0.5269, 0.5438, 0.5475, 0.5289, 0.5562, 0.5742, 0.5463,
        0.5295, 0.5234, 0.5203, 0.5515, 0.5571, 0.5554, 0.5426],
       device='cuda:0') torch.Size([16])
percent tensor([0.5623, 0.5303, 0.4937, 0.5802, 0.4884, 0.6046, 0.5348, 0.5520, 0.5540,
        0.5098, 0.5454, 0.4939, 0.4915, 0.6559, 0.5764, 0.5750],
       device='cuda:0') torch.Size([16])
percent tensor([0.7014, 0.7318, 0.6750, 0.6484, 0.6503, 0.6824, 0.7141, 0.6387, 0.6934,
        0.7311, 0.7178, 0.7093, 0.7537, 0.6935, 0.7036, 0.7084],
       device='cuda:0') torch.Size([16])
percent tensor([0.7029, 0.6153, 0.7281, 0.7361, 0.7542, 0.7798, 0.7114, 0.7101, 0.7875,
        0.7115, 0.7387, 0.7235, 0.6493, 0.7790, 0.6470, 0.7494],
       device='cuda:0') torch.Size([16])
percent tensor([0.8214, 0.8259, 0.8459, 0.8404, 0.8659, 0.8450, 0.8459, 0.8323, 0.8420,
        0.8381, 0.8522, 0.8185, 0.8322, 0.8619, 0.8168, 0.8411],
       device='cuda:0') torch.Size([16])
percent tensor([0.4363, 0.6367, 0.5967, 0.5332, 0.6176, 0.6768, 0.5476, 0.3891, 0.6033,
        0.5719, 0.6139, 0.4660, 0.6064, 0.5338, 0.3731, 0.3981],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9999, 0.9997, 0.9999, 0.9997, 0.9991, 0.9999, 0.9996, 0.9999,
        0.9998, 1.0000, 0.9998, 0.9997, 0.9994, 0.9992, 0.9994],
       device='cuda:0') torch.Size([16])
Epoch: 217 | Batch_idx: 0 |  Loss: (0.0356) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 217 | Batch_idx: 10 |  Loss: (0.0417) |  Loss2: (0.0000) | Acc: (98.00%) (1390/1408)
Epoch: 217 | Batch_idx: 20 |  Loss: (0.0419) |  Loss2: (0.0000) | Acc: (98.00%) (2655/2688)
Epoch: 217 | Batch_idx: 30 |  Loss: (0.0427) |  Loss2: (0.0000) | Acc: (98.00%) (3915/3968)
Epoch: 217 | Batch_idx: 40 |  Loss: (0.0427) |  Loss2: (0.0000) | Acc: (98.00%) (5178/5248)
Epoch: 217 | Batch_idx: 50 |  Loss: (0.0428) |  Loss2: (0.0000) | Acc: (98.00%) (6444/6528)
Epoch: 217 | Batch_idx: 60 |  Loss: (0.0432) |  Loss2: (0.0000) | Acc: (98.00%) (7704/7808)
Epoch: 217 | Batch_idx: 70 |  Loss: (0.0435) |  Loss2: (0.0000) | Acc: (98.00%) (8966/9088)
Epoch: 217 | Batch_idx: 80 |  Loss: (0.0434) |  Loss2: (0.0000) | Acc: (98.00%) (10231/10368)
Epoch: 217 | Batch_idx: 90 |  Loss: (0.0426) |  Loss2: (0.0000) | Acc: (98.00%) (11498/11648)
Epoch: 217 | Batch_idx: 100 |  Loss: (0.0426) |  Loss2: (0.0000) | Acc: (98.00%) (12761/12928)
Epoch: 217 | Batch_idx: 110 |  Loss: (0.0421) |  Loss2: (0.0000) | Acc: (98.00%) (14027/14208)
Epoch: 217 | Batch_idx: 120 |  Loss: (0.0418) |  Loss2: (0.0000) | Acc: (98.00%) (15289/15488)
Epoch: 217 | Batch_idx: 130 |  Loss: (0.0422) |  Loss2: (0.0000) | Acc: (98.00%) (16554/16768)
Epoch: 217 | Batch_idx: 140 |  Loss: (0.0421) |  Loss2: (0.0000) | Acc: (98.00%) (17818/18048)
Epoch: 217 | Batch_idx: 150 |  Loss: (0.0436) |  Loss2: (0.0000) | Acc: (98.00%) (19070/19328)
Epoch: 217 | Batch_idx: 160 |  Loss: (0.0436) |  Loss2: (0.0000) | Acc: (98.00%) (20337/20608)
Epoch: 217 | Batch_idx: 170 |  Loss: (0.0444) |  Loss2: (0.0000) | Acc: (98.00%) (21590/21888)
Epoch: 217 | Batch_idx: 180 |  Loss: (0.0447) |  Loss2: (0.0000) | Acc: (98.00%) (22849/23168)
Epoch: 217 | Batch_idx: 190 |  Loss: (0.0447) |  Loss2: (0.0000) | Acc: (98.00%) (24111/24448)
Epoch: 217 | Batch_idx: 200 |  Loss: (0.0446) |  Loss2: (0.0000) | Acc: (98.00%) (25368/25728)
Epoch: 217 | Batch_idx: 210 |  Loss: (0.0448) |  Loss2: (0.0000) | Acc: (98.00%) (26627/27008)
Epoch: 217 | Batch_idx: 220 |  Loss: (0.0448) |  Loss2: (0.0000) | Acc: (98.00%) (27883/28288)
Epoch: 217 | Batch_idx: 230 |  Loss: (0.0447) |  Loss2: (0.0000) | Acc: (98.00%) (29146/29568)
Epoch: 217 | Batch_idx: 240 |  Loss: (0.0446) |  Loss2: (0.0000) | Acc: (98.00%) (30410/30848)
Epoch: 217 | Batch_idx: 250 |  Loss: (0.0448) |  Loss2: (0.0000) | Acc: (98.00%) (31665/32128)
Epoch: 217 | Batch_idx: 260 |  Loss: (0.0450) |  Loss2: (0.0000) | Acc: (98.00%) (32927/33408)
Epoch: 217 | Batch_idx: 270 |  Loss: (0.0453) |  Loss2: (0.0000) | Acc: (98.00%) (34183/34688)
Epoch: 217 | Batch_idx: 280 |  Loss: (0.0452) |  Loss2: (0.0000) | Acc: (98.00%) (35447/35968)
Epoch: 217 | Batch_idx: 290 |  Loss: (0.0454) |  Loss2: (0.0000) | Acc: (98.00%) (36705/37248)
Epoch: 217 | Batch_idx: 300 |  Loss: (0.0456) |  Loss2: (0.0000) | Acc: (98.00%) (37963/38528)
Epoch: 217 | Batch_idx: 310 |  Loss: (0.0458) |  Loss2: (0.0000) | Acc: (98.00%) (39219/39808)
Epoch: 217 | Batch_idx: 320 |  Loss: (0.0460) |  Loss2: (0.0000) | Acc: (98.00%) (40477/41088)
Epoch: 217 | Batch_idx: 330 |  Loss: (0.0461) |  Loss2: (0.0000) | Acc: (98.00%) (41738/42368)
Epoch: 217 | Batch_idx: 340 |  Loss: (0.0461) |  Loss2: (0.0000) | Acc: (98.00%) (42996/43648)
Epoch: 217 | Batch_idx: 350 |  Loss: (0.0462) |  Loss2: (0.0000) | Acc: (98.00%) (44251/44928)
Epoch: 217 | Batch_idx: 360 |  Loss: (0.0463) |  Loss2: (0.0000) | Acc: (98.00%) (45512/46208)
Epoch: 217 | Batch_idx: 370 |  Loss: (0.0465) |  Loss2: (0.0000) | Acc: (98.00%) (46772/47488)
Epoch: 217 | Batch_idx: 380 |  Loss: (0.0465) |  Loss2: (0.0000) | Acc: (98.00%) (48033/48768)
Epoch: 217 | Batch_idx: 390 |  Loss: (0.0470) |  Loss2: (0.0000) | Acc: (98.00%) (49236/50000)
# TEST : Loss: (0.4367) | Acc: (88.00%) (8859/10000)
percent tensor([0.5890, 0.6044, 0.5961, 0.5826, 0.6024, 0.5700, 0.6091, 0.6067, 0.6074,
        0.6017, 0.6005, 0.6019, 0.5975, 0.6056, 0.5905, 0.5830],
       device='cuda:0') torch.Size([16])
percent tensor([0.5588, 0.5460, 0.5255, 0.5440, 0.5455, 0.5303, 0.5552, 0.5759, 0.5474,
        0.5268, 0.5227, 0.5168, 0.5517, 0.5569, 0.5552, 0.5424],
       device='cuda:0') torch.Size([16])
percent tensor([0.5668, 0.5206, 0.5089, 0.5857, 0.4971, 0.6073, 0.5334, 0.5605, 0.5573,
        0.5097, 0.5411, 0.5016, 0.4877, 0.6487, 0.5725, 0.5740],
       device='cuda:0') torch.Size([16])
percent tensor([0.6928, 0.7224, 0.6664, 0.6471, 0.6457, 0.6777, 0.7033, 0.6316, 0.6818,
        0.7234, 0.7095, 0.7022, 0.7454, 0.6879, 0.6979, 0.7020],
       device='cuda:0') torch.Size([16])
percent tensor([0.7113, 0.6267, 0.7385, 0.7456, 0.7611, 0.7897, 0.7171, 0.7172, 0.7817,
        0.7213, 0.7482, 0.7347, 0.6497, 0.7804, 0.6653, 0.7620],
       device='cuda:0') torch.Size([16])
percent tensor([0.8230, 0.8307, 0.8547, 0.8465, 0.8751, 0.8502, 0.8475, 0.8367, 0.8476,
        0.8377, 0.8536, 0.8254, 0.8309, 0.8661, 0.8211, 0.8443],
       device='cuda:0') torch.Size([16])
percent tensor([0.4166, 0.6479, 0.5965, 0.5405, 0.6199, 0.6948, 0.5630, 0.3686, 0.6335,
        0.5559, 0.5934, 0.5092, 0.6072, 0.5347, 0.3699, 0.3921],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9997, 0.9995, 0.9999, 0.9997, 0.9993, 0.9999, 0.9996, 0.9997,
        0.9997, 1.0000, 0.9999, 0.9997, 0.9994, 0.9992, 0.9993],
       device='cuda:0') torch.Size([16])
Epoch: 218 | Batch_idx: 0 |  Loss: (0.0221) |  Loss2: (0.0000) | Acc: (100.00%) (128/128)
Epoch: 218 | Batch_idx: 10 |  Loss: (0.0427) |  Loss2: (0.0000) | Acc: (98.00%) (1392/1408)
Epoch: 218 | Batch_idx: 20 |  Loss: (0.0476) |  Loss2: (0.0000) | Acc: (98.00%) (2650/2688)
Epoch: 218 | Batch_idx: 30 |  Loss: (0.0506) |  Loss2: (0.0000) | Acc: (98.00%) (3900/3968)
Epoch: 218 | Batch_idx: 40 |  Loss: (0.0508) |  Loss2: (0.0000) | Acc: (98.00%) (5161/5248)
Epoch: 218 | Batch_idx: 50 |  Loss: (0.0512) |  Loss2: (0.0000) | Acc: (98.00%) (6416/6528)
Epoch: 218 | Batch_idx: 60 |  Loss: (0.0492) |  Loss2: (0.0000) | Acc: (98.00%) (7682/7808)
Epoch: 218 | Batch_idx: 70 |  Loss: (0.0481) |  Loss2: (0.0000) | Acc: (98.00%) (8944/9088)
Epoch: 218 | Batch_idx: 80 |  Loss: (0.0472) |  Loss2: (0.0000) | Acc: (98.00%) (10207/10368)
Epoch: 218 | Batch_idx: 90 |  Loss: (0.0467) |  Loss2: (0.0000) | Acc: (98.00%) (11470/11648)
Epoch: 218 | Batch_idx: 100 |  Loss: (0.0466) |  Loss2: (0.0000) | Acc: (98.00%) (12730/12928)
Epoch: 218 | Batch_idx: 110 |  Loss: (0.0458) |  Loss2: (0.0000) | Acc: (98.00%) (13995/14208)
Epoch: 218 | Batch_idx: 120 |  Loss: (0.0453) |  Loss2: (0.0000) | Acc: (98.00%) (15254/15488)
Epoch: 218 | Batch_idx: 130 |  Loss: (0.0444) |  Loss2: (0.0000) | Acc: (98.00%) (16522/16768)
Epoch: 218 | Batch_idx: 140 |  Loss: (0.0456) |  Loss2: (0.0000) | Acc: (98.00%) (17774/18048)
Epoch: 218 | Batch_idx: 150 |  Loss: (0.0456) |  Loss2: (0.0000) | Acc: (98.00%) (19034/19328)
Epoch: 218 | Batch_idx: 160 |  Loss: (0.0455) |  Loss2: (0.0000) | Acc: (98.00%) (20299/20608)
Epoch: 218 | Batch_idx: 170 |  Loss: (0.0453) |  Loss2: (0.0000) | Acc: (98.00%) (21563/21888)
Epoch: 218 | Batch_idx: 180 |  Loss: (0.0453) |  Loss2: (0.0000) | Acc: (98.00%) (22826/23168)
Epoch: 218 | Batch_idx: 190 |  Loss: (0.0452) |  Loss2: (0.0000) | Acc: (98.00%) (24088/24448)
Epoch: 218 | Batch_idx: 200 |  Loss: (0.0452) |  Loss2: (0.0000) | Acc: (98.00%) (25345/25728)
Epoch: 218 | Batch_idx: 210 |  Loss: (0.0450) |  Loss2: (0.0000) | Acc: (98.00%) (26612/27008)
Epoch: 218 | Batch_idx: 220 |  Loss: (0.0450) |  Loss2: (0.0000) | Acc: (98.00%) (27871/28288)
Epoch: 218 | Batch_idx: 230 |  Loss: (0.0449) |  Loss2: (0.0000) | Acc: (98.00%) (29131/29568)
Epoch: 218 | Batch_idx: 240 |  Loss: (0.0448) |  Loss2: (0.0000) | Acc: (98.00%) (30396/30848)
Epoch: 218 | Batch_idx: 250 |  Loss: (0.0450) |  Loss2: (0.0000) | Acc: (98.00%) (31662/32128)
Epoch: 218 | Batch_idx: 260 |  Loss: (0.0452) |  Loss2: (0.0000) | Acc: (98.00%) (32920/33408)
Epoch: 218 | Batch_idx: 270 |  Loss: (0.0453) |  Loss2: (0.0000) | Acc: (98.00%) (34181/34688)
Epoch: 218 | Batch_idx: 280 |  Loss: (0.0452) |  Loss2: (0.0000) | Acc: (98.00%) (35442/35968)
Epoch: 218 | Batch_idx: 290 |  Loss: (0.0456) |  Loss2: (0.0000) | Acc: (98.00%) (36695/37248)
Epoch: 218 | Batch_idx: 300 |  Loss: (0.0453) |  Loss2: (0.0000) | Acc: (98.00%) (37961/38528)
Epoch: 218 | Batch_idx: 310 |  Loss: (0.0452) |  Loss2: (0.0000) | Acc: (98.00%) (39223/39808)
Epoch: 218 | Batch_idx: 320 |  Loss: (0.0450) |  Loss2: (0.0000) | Acc: (98.00%) (40489/41088)
Epoch: 218 | Batch_idx: 330 |  Loss: (0.0455) |  Loss2: (0.0000) | Acc: (98.00%) (41741/42368)
Epoch: 218 | Batch_idx: 340 |  Loss: (0.0458) |  Loss2: (0.0000) | Acc: (98.00%) (43006/43648)
Epoch: 218 | Batch_idx: 350 |  Loss: (0.0459) |  Loss2: (0.0000) | Acc: (98.00%) (44264/44928)
Epoch: 218 | Batch_idx: 360 |  Loss: (0.0463) |  Loss2: (0.0000) | Acc: (98.00%) (45521/46208)
Epoch: 218 | Batch_idx: 370 |  Loss: (0.0463) |  Loss2: (0.0000) | Acc: (98.00%) (46781/47488)
Epoch: 218 | Batch_idx: 380 |  Loss: (0.0469) |  Loss2: (0.0000) | Acc: (98.00%) (48033/48768)
Epoch: 218 | Batch_idx: 390 |  Loss: (0.0468) |  Loss2: (0.0000) | Acc: (98.00%) (49249/50000)
# TEST : Loss: (0.4394) | Acc: (88.00%) (8878/10000)
percent tensor([0.5885, 0.6054, 0.5965, 0.5818, 0.6025, 0.5684, 0.6095, 0.6076, 0.6080,
        0.6020, 0.6008, 0.6021, 0.5972, 0.6072, 0.5909, 0.5831],
       device='cuda:0') torch.Size([16])
percent tensor([0.5568, 0.5480, 0.5267, 0.5414, 0.5453, 0.5238, 0.5567, 0.5765, 0.5478,
        0.5297, 0.5224, 0.5191, 0.5530, 0.5559, 0.5549, 0.5410],
       device='cuda:0') torch.Size([16])
percent tensor([0.5627, 0.5225, 0.4958, 0.5780, 0.4927, 0.6063, 0.5347, 0.5495, 0.5501,
        0.5049, 0.5405, 0.4980, 0.4902, 0.6440, 0.5696, 0.5743],
       device='cuda:0') torch.Size([16])
percent tensor([0.6934, 0.7213, 0.6621, 0.6486, 0.6422, 0.6761, 0.7023, 0.6300, 0.6846,
        0.7221, 0.7118, 0.6982, 0.7448, 0.6874, 0.6993, 0.7038],
       device='cuda:0') torch.Size([16])
percent tensor([0.7151, 0.6217, 0.7432, 0.7538, 0.7660, 0.7964, 0.7145, 0.7106, 0.7819,
        0.7128, 0.7454, 0.7504, 0.6456, 0.7756, 0.6627, 0.7547],
       device='cuda:0') torch.Size([16])
percent tensor([0.8239, 0.8256, 0.8499, 0.8389, 0.8695, 0.8492, 0.8429, 0.8307, 0.8426,
        0.8362, 0.8524, 0.8201, 0.8270, 0.8600, 0.8177, 0.8447],
       device='cuda:0') torch.Size([16])
percent tensor([0.4038, 0.6146, 0.5879, 0.5399, 0.6047, 0.6882, 0.5223, 0.3688, 0.5800,
        0.5320, 0.5703, 0.4592, 0.5871, 0.4751, 0.3482, 0.3909],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9999, 0.9996, 0.9999, 0.9997, 0.9996, 0.9998, 0.9996, 0.9998,
        0.9998, 1.0000, 0.9999, 0.9997, 0.9991, 0.9994, 0.9992],
       device='cuda:0') torch.Size([16])
Epoch: 219 | Batch_idx: 0 |  Loss: (0.0480) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 219 | Batch_idx: 10 |  Loss: (0.0666) |  Loss2: (0.0000) | Acc: (97.00%) (1376/1408)
Epoch: 219 | Batch_idx: 20 |  Loss: (0.0523) |  Loss2: (0.0000) | Acc: (98.00%) (2642/2688)
Epoch: 219 | Batch_idx: 30 |  Loss: (0.0496) |  Loss2: (0.0000) | Acc: (98.00%) (3904/3968)
Epoch: 219 | Batch_idx: 40 |  Loss: (0.0497) |  Loss2: (0.0000) | Acc: (98.00%) (5165/5248)
Epoch: 219 | Batch_idx: 50 |  Loss: (0.0496) |  Loss2: (0.0000) | Acc: (98.00%) (6422/6528)
Epoch: 219 | Batch_idx: 60 |  Loss: (0.0483) |  Loss2: (0.0000) | Acc: (98.00%) (7684/7808)
Epoch: 219 | Batch_idx: 70 |  Loss: (0.0470) |  Loss2: (0.0000) | Acc: (98.00%) (8949/9088)
Epoch: 219 | Batch_idx: 80 |  Loss: (0.0470) |  Loss2: (0.0000) | Acc: (98.00%) (10208/10368)
Epoch: 219 | Batch_idx: 90 |  Loss: (0.0483) |  Loss2: (0.0000) | Acc: (98.00%) (11465/11648)
Epoch: 219 | Batch_idx: 100 |  Loss: (0.0485) |  Loss2: (0.0000) | Acc: (98.00%) (12721/12928)
Epoch: 219 | Batch_idx: 110 |  Loss: (0.0482) |  Loss2: (0.0000) | Acc: (98.00%) (13980/14208)
Epoch: 219 | Batch_idx: 120 |  Loss: (0.0479) |  Loss2: (0.0000) | Acc: (98.00%) (15243/15488)
Epoch: 219 | Batch_idx: 130 |  Loss: (0.0481) |  Loss2: (0.0000) | Acc: (98.00%) (16501/16768)
Epoch: 219 | Batch_idx: 140 |  Loss: (0.0480) |  Loss2: (0.0000) | Acc: (98.00%) (17763/18048)
Epoch: 219 | Batch_idx: 150 |  Loss: (0.0485) |  Loss2: (0.0000) | Acc: (98.00%) (19020/19328)
Epoch: 219 | Batch_idx: 160 |  Loss: (0.0487) |  Loss2: (0.0000) | Acc: (98.00%) (20279/20608)
Epoch: 219 | Batch_idx: 170 |  Loss: (0.0485) |  Loss2: (0.0000) | Acc: (98.00%) (21541/21888)
Epoch: 219 | Batch_idx: 180 |  Loss: (0.0488) |  Loss2: (0.0000) | Acc: (98.00%) (22795/23168)
Epoch: 219 | Batch_idx: 190 |  Loss: (0.0482) |  Loss2: (0.0000) | Acc: (98.00%) (24061/24448)
Epoch: 219 | Batch_idx: 200 |  Loss: (0.0474) |  Loss2: (0.0000) | Acc: (98.00%) (25329/25728)
Epoch: 219 | Batch_idx: 210 |  Loss: (0.0475) |  Loss2: (0.0000) | Acc: (98.00%) (26593/27008)
Epoch: 219 | Batch_idx: 220 |  Loss: (0.0472) |  Loss2: (0.0000) | Acc: (98.00%) (27852/28288)
Epoch: 219 | Batch_idx: 230 |  Loss: (0.0472) |  Loss2: (0.0000) | Acc: (98.00%) (29111/29568)
Epoch: 219 | Batch_idx: 240 |  Loss: (0.0471) |  Loss2: (0.0000) | Acc: (98.00%) (30374/30848)
Epoch: 219 | Batch_idx: 250 |  Loss: (0.0467) |  Loss2: (0.0000) | Acc: (98.00%) (31641/32128)
Epoch: 219 | Batch_idx: 260 |  Loss: (0.0466) |  Loss2: (0.0000) | Acc: (98.00%) (32907/33408)
Epoch: 219 | Batch_idx: 270 |  Loss: (0.0465) |  Loss2: (0.0000) | Acc: (98.00%) (34170/34688)
Epoch: 219 | Batch_idx: 280 |  Loss: (0.0465) |  Loss2: (0.0000) | Acc: (98.00%) (35431/35968)
Epoch: 219 | Batch_idx: 290 |  Loss: (0.0465) |  Loss2: (0.0000) | Acc: (98.00%) (36692/37248)
Epoch: 219 | Batch_idx: 300 |  Loss: (0.0464) |  Loss2: (0.0000) | Acc: (98.00%) (37954/38528)
Epoch: 219 | Batch_idx: 310 |  Loss: (0.0467) |  Loss2: (0.0000) | Acc: (98.00%) (39213/39808)
Epoch: 219 | Batch_idx: 320 |  Loss: (0.0466) |  Loss2: (0.0000) | Acc: (98.00%) (40477/41088)
Epoch: 219 | Batch_idx: 330 |  Loss: (0.0465) |  Loss2: (0.0000) | Acc: (98.00%) (41742/42368)
Epoch: 219 | Batch_idx: 340 |  Loss: (0.0464) |  Loss2: (0.0000) | Acc: (98.00%) (43002/43648)
Epoch: 219 | Batch_idx: 350 |  Loss: (0.0461) |  Loss2: (0.0000) | Acc: (98.00%) (44267/44928)
Epoch: 219 | Batch_idx: 360 |  Loss: (0.0461) |  Loss2: (0.0000) | Acc: (98.00%) (45525/46208)
Epoch: 219 | Batch_idx: 370 |  Loss: (0.0462) |  Loss2: (0.0000) | Acc: (98.00%) (46782/47488)
Epoch: 219 | Batch_idx: 380 |  Loss: (0.0463) |  Loss2: (0.0000) | Acc: (98.00%) (48037/48768)
Epoch: 219 | Batch_idx: 390 |  Loss: (0.0464) |  Loss2: (0.0000) | Acc: (98.00%) (49245/50000)
# TEST : Loss: (0.4239) | Acc: (89.00%) (8905/10000)
percent tensor([0.5855, 0.6029, 0.5888, 0.5788, 0.5957, 0.5667, 0.6054, 0.6024, 0.6038,
        0.5983, 0.5978, 0.5952, 0.5941, 0.6055, 0.5882, 0.5809],
       device='cuda:0') torch.Size([16])
percent tensor([0.5594, 0.5504, 0.5260, 0.5454, 0.5455, 0.5242, 0.5585, 0.5769, 0.5503,
        0.5307, 0.5248, 0.5199, 0.5564, 0.5611, 0.5558, 0.5426],
       device='cuda:0') torch.Size([16])
percent tensor([0.5626, 0.5236, 0.5043, 0.5786, 0.4957, 0.6033, 0.5362, 0.5566, 0.5570,
        0.5100, 0.5436, 0.5035, 0.4920, 0.6508, 0.5681, 0.5723],
       device='cuda:0') torch.Size([16])
percent tensor([0.6982, 0.7245, 0.6693, 0.6497, 0.6470, 0.6893, 0.7046, 0.6347, 0.6864,
        0.7224, 0.7150, 0.7006, 0.7484, 0.6920, 0.7022, 0.7085],
       device='cuda:0') torch.Size([16])
percent tensor([0.7033, 0.6038, 0.7310, 0.7375, 0.7552, 0.7798, 0.6969, 0.7053, 0.7698,
        0.7026, 0.7306, 0.7136, 0.6293, 0.7708, 0.6381, 0.7552],
       device='cuda:0') torch.Size([16])
percent tensor([0.8259, 0.8313, 0.8542, 0.8492, 0.8732, 0.8558, 0.8449, 0.8368, 0.8440,
        0.8400, 0.8501, 0.8274, 0.8310, 0.8613, 0.8195, 0.8484],
       device='cuda:0') torch.Size([16])
percent tensor([0.4033, 0.6443, 0.5922, 0.5533, 0.5943, 0.7142, 0.5459, 0.3676, 0.6094,
        0.5731, 0.5839, 0.5037, 0.5895, 0.5249, 0.3895, 0.3937],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9998, 0.9997, 0.9998, 0.9996, 0.9990, 0.9999, 0.9996, 0.9998,
        0.9998, 1.0000, 0.9999, 0.9998, 0.9994, 0.9993, 0.9993],
       device='cuda:0') torch.Size([16])
Epoch: 220 | Batch_idx: 0 |  Loss: (0.0382) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 220 | Batch_idx: 10 |  Loss: (0.0379) |  Loss2: (0.0000) | Acc: (98.00%) (1388/1408)
Epoch: 220 | Batch_idx: 20 |  Loss: (0.0459) |  Loss2: (0.0000) | Acc: (98.00%) (2652/2688)
Epoch: 220 | Batch_idx: 30 |  Loss: (0.0434) |  Loss2: (0.0000) | Acc: (98.00%) (3921/3968)
Epoch: 220 | Batch_idx: 40 |  Loss: (0.0411) |  Loss2: (0.0000) | Acc: (98.00%) (5188/5248)
Epoch: 220 | Batch_idx: 50 |  Loss: (0.0402) |  Loss2: (0.0000) | Acc: (98.00%) (6456/6528)
Epoch: 220 | Batch_idx: 60 |  Loss: (0.0409) |  Loss2: (0.0000) | Acc: (98.00%) (7714/7808)
Epoch: 220 | Batch_idx: 70 |  Loss: (0.0425) |  Loss2: (0.0000) | Acc: (98.00%) (8966/9088)
Epoch: 220 | Batch_idx: 80 |  Loss: (0.0416) |  Loss2: (0.0000) | Acc: (98.00%) (10235/10368)
Epoch: 220 | Batch_idx: 90 |  Loss: (0.0413) |  Loss2: (0.0000) | Acc: (98.00%) (11499/11648)
Epoch: 220 | Batch_idx: 100 |  Loss: (0.0414) |  Loss2: (0.0000) | Acc: (98.00%) (12760/12928)
Epoch: 220 | Batch_idx: 110 |  Loss: (0.0411) |  Loss2: (0.0000) | Acc: (98.00%) (14025/14208)
Epoch: 220 | Batch_idx: 120 |  Loss: (0.0416) |  Loss2: (0.0000) | Acc: (98.00%) (15286/15488)
Epoch: 220 | Batch_idx: 130 |  Loss: (0.0407) |  Loss2: (0.0000) | Acc: (98.00%) (16555/16768)
Epoch: 220 | Batch_idx: 140 |  Loss: (0.0406) |  Loss2: (0.0000) | Acc: (98.00%) (17819/18048)
Epoch: 220 | Batch_idx: 150 |  Loss: (0.0407) |  Loss2: (0.0000) | Acc: (98.00%) (19084/19328)
Epoch: 220 | Batch_idx: 160 |  Loss: (0.0407) |  Loss2: (0.0000) | Acc: (98.00%) (20348/20608)
Epoch: 220 | Batch_idx: 170 |  Loss: (0.0413) |  Loss2: (0.0000) | Acc: (98.00%) (21611/21888)
Epoch: 220 | Batch_idx: 180 |  Loss: (0.0410) |  Loss2: (0.0000) | Acc: (98.00%) (22877/23168)
Epoch: 220 | Batch_idx: 190 |  Loss: (0.0410) |  Loss2: (0.0000) | Acc: (98.00%) (24139/24448)
Epoch: 220 | Batch_idx: 200 |  Loss: (0.0410) |  Loss2: (0.0000) | Acc: (98.00%) (25402/25728)
Epoch: 220 | Batch_idx: 210 |  Loss: (0.0406) |  Loss2: (0.0000) | Acc: (98.00%) (26673/27008)
Epoch: 220 | Batch_idx: 220 |  Loss: (0.0407) |  Loss2: (0.0000) | Acc: (98.00%) (27934/28288)
Epoch: 220 | Batch_idx: 230 |  Loss: (0.0409) |  Loss2: (0.0000) | Acc: (98.00%) (29192/29568)
Epoch: 220 | Batch_idx: 240 |  Loss: (0.0408) |  Loss2: (0.0000) | Acc: (98.00%) (30456/30848)
Epoch: 220 | Batch_idx: 250 |  Loss: (0.0408) |  Loss2: (0.0000) | Acc: (98.00%) (31719/32128)
Epoch: 220 | Batch_idx: 260 |  Loss: (0.0406) |  Loss2: (0.0000) | Acc: (98.00%) (32982/33408)
Epoch: 220 | Batch_idx: 270 |  Loss: (0.0411) |  Loss2: (0.0000) | Acc: (98.00%) (34237/34688)
Epoch: 220 | Batch_idx: 280 |  Loss: (0.0408) |  Loss2: (0.0000) | Acc: (98.00%) (35506/35968)
Epoch: 220 | Batch_idx: 290 |  Loss: (0.0410) |  Loss2: (0.0000) | Acc: (98.00%) (36768/37248)
Epoch: 220 | Batch_idx: 300 |  Loss: (0.0414) |  Loss2: (0.0000) | Acc: (98.00%) (38020/38528)
Epoch: 220 | Batch_idx: 310 |  Loss: (0.0417) |  Loss2: (0.0000) | Acc: (98.00%) (39280/39808)
Epoch: 220 | Batch_idx: 320 |  Loss: (0.0419) |  Loss2: (0.0000) | Acc: (98.00%) (40538/41088)
Epoch: 220 | Batch_idx: 330 |  Loss: (0.0421) |  Loss2: (0.0000) | Acc: (98.00%) (41793/42368)
Epoch: 220 | Batch_idx: 340 |  Loss: (0.0423) |  Loss2: (0.0000) | Acc: (98.00%) (43055/43648)
Epoch: 220 | Batch_idx: 350 |  Loss: (0.0424) |  Loss2: (0.0000) | Acc: (98.00%) (44315/44928)
Epoch: 220 | Batch_idx: 360 |  Loss: (0.0423) |  Loss2: (0.0000) | Acc: (98.00%) (45578/46208)
Epoch: 220 | Batch_idx: 370 |  Loss: (0.0427) |  Loss2: (0.0000) | Acc: (98.00%) (46836/47488)
Epoch: 220 | Batch_idx: 380 |  Loss: (0.0425) |  Loss2: (0.0000) | Acc: (98.00%) (48097/48768)
Epoch: 220 | Batch_idx: 390 |  Loss: (0.0427) |  Loss2: (0.0000) | Acc: (98.00%) (49310/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_220.pth.tar'
# TEST : Loss: (0.4299) | Acc: (88.00%) (8876/10000)
percent tensor([0.5879, 0.6046, 0.5954, 0.5815, 0.6011, 0.5690, 0.6085, 0.6058, 0.6072,
        0.6007, 0.6001, 0.6005, 0.5962, 0.6067, 0.5903, 0.5824],
       device='cuda:0') torch.Size([16])
percent tensor([0.5637, 0.5546, 0.5267, 0.5498, 0.5501, 0.5359, 0.5613, 0.5784, 0.5531,
        0.5329, 0.5304, 0.5213, 0.5581, 0.5639, 0.5640, 0.5495],
       device='cuda:0') torch.Size([16])
percent tensor([0.5665, 0.5218, 0.4980, 0.5753, 0.4947, 0.6142, 0.5373, 0.5536, 0.5549,
        0.5065, 0.5469, 0.4967, 0.4904, 0.6492, 0.5774, 0.5758],
       device='cuda:0') torch.Size([16])
percent tensor([0.6996, 0.7302, 0.6766, 0.6551, 0.6541, 0.6882, 0.7112, 0.6399, 0.6927,
        0.7305, 0.7198, 0.7100, 0.7540, 0.6922, 0.7072, 0.7128],
       device='cuda:0') torch.Size([16])
percent tensor([0.6977, 0.6131, 0.7323, 0.7293, 0.7528, 0.7954, 0.6956, 0.7047, 0.7568,
        0.7002, 0.7232, 0.7269, 0.6297, 0.7616, 0.6565, 0.7535],
       device='cuda:0') torch.Size([16])
percent tensor([0.8248, 0.8291, 0.8572, 0.8452, 0.8705, 0.8510, 0.8451, 0.8386, 0.8456,
        0.8382, 0.8481, 0.8208, 0.8251, 0.8629, 0.8216, 0.8458],
       device='cuda:0') torch.Size([16])
percent tensor([0.4328, 0.6618, 0.6074, 0.5258, 0.6242, 0.6686, 0.5743, 0.3828, 0.6250,
        0.5695, 0.5846, 0.4888, 0.6052, 0.5158, 0.3735, 0.3880],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9998, 0.9995, 0.9999, 0.9996, 0.9990, 0.9999, 0.9995, 0.9999,
        0.9998, 1.0000, 0.9998, 0.9999, 0.9995, 0.9991, 0.9993],
       device='cuda:0') torch.Size([16])
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(182.7646, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(829.1328, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(816.0197, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1493.5150, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(475.2010, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2308.1465, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4288.1401, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1338.5004, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6372.9883, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11457.3604, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3757.0254, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(15831.6338, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 221 | Batch_idx: 0 |  Loss: (0.0371) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 221 | Batch_idx: 10 |  Loss: (0.0344) |  Loss2: (0.0000) | Acc: (99.00%) (1396/1408)
Epoch: 221 | Batch_idx: 20 |  Loss: (0.0355) |  Loss2: (0.0000) | Acc: (99.00%) (2664/2688)
Epoch: 221 | Batch_idx: 30 |  Loss: (0.0342) |  Loss2: (0.0000) | Acc: (99.00%) (3936/3968)
Epoch: 221 | Batch_idx: 40 |  Loss: (0.0349) |  Loss2: (0.0000) | Acc: (99.00%) (5200/5248)
Epoch: 221 | Batch_idx: 50 |  Loss: (0.0354) |  Loss2: (0.0000) | Acc: (98.00%) (6459/6528)
Epoch: 221 | Batch_idx: 60 |  Loss: (0.0367) |  Loss2: (0.0000) | Acc: (98.00%) (7723/7808)
Epoch: 221 | Batch_idx: 70 |  Loss: (0.0383) |  Loss2: (0.0000) | Acc: (98.00%) (8977/9088)
Epoch: 221 | Batch_idx: 80 |  Loss: (0.0389) |  Loss2: (0.0000) | Acc: (98.00%) (10241/10368)
Epoch: 221 | Batch_idx: 90 |  Loss: (0.0381) |  Loss2: (0.0000) | Acc: (98.00%) (11508/11648)
Epoch: 221 | Batch_idx: 100 |  Loss: (0.0393) |  Loss2: (0.0000) | Acc: (98.00%) (12766/12928)
Epoch: 221 | Batch_idx: 110 |  Loss: (0.0398) |  Loss2: (0.0000) | Acc: (98.00%) (14032/14208)
Epoch: 221 | Batch_idx: 120 |  Loss: (0.0403) |  Loss2: (0.0000) | Acc: (98.00%) (15290/15488)
Epoch: 221 | Batch_idx: 130 |  Loss: (0.0408) |  Loss2: (0.0000) | Acc: (98.00%) (16546/16768)
Epoch: 221 | Batch_idx: 140 |  Loss: (0.0410) |  Loss2: (0.0000) | Acc: (98.00%) (17804/18048)
Epoch: 221 | Batch_idx: 150 |  Loss: (0.0411) |  Loss2: (0.0000) | Acc: (98.00%) (19068/19328)
Epoch: 221 | Batch_idx: 160 |  Loss: (0.0415) |  Loss2: (0.0000) | Acc: (98.00%) (20331/20608)
Epoch: 221 | Batch_idx: 170 |  Loss: (0.0426) |  Loss2: (0.0000) | Acc: (98.00%) (21584/21888)
Epoch: 221 | Batch_idx: 180 |  Loss: (0.0434) |  Loss2: (0.0000) | Acc: (98.00%) (22834/23168)
Epoch: 221 | Batch_idx: 190 |  Loss: (0.0431) |  Loss2: (0.0000) | Acc: (98.00%) (24099/24448)
Epoch: 221 | Batch_idx: 200 |  Loss: (0.0433) |  Loss2: (0.0000) | Acc: (98.00%) (25357/25728)
Epoch: 221 | Batch_idx: 210 |  Loss: (0.0436) |  Loss2: (0.0000) | Acc: (98.00%) (26617/27008)
Epoch: 221 | Batch_idx: 220 |  Loss: (0.0433) |  Loss2: (0.0000) | Acc: (98.00%) (27880/28288)
Epoch: 221 | Batch_idx: 230 |  Loss: (0.0434) |  Loss2: (0.0000) | Acc: (98.00%) (29140/29568)
Epoch: 221 | Batch_idx: 240 |  Loss: (0.0433) |  Loss2: (0.0000) | Acc: (98.00%) (30407/30848)
Epoch: 221 | Batch_idx: 250 |  Loss: (0.0430) |  Loss2: (0.0000) | Acc: (98.00%) (31670/32128)
Epoch: 221 | Batch_idx: 260 |  Loss: (0.0429) |  Loss2: (0.0000) | Acc: (98.00%) (32933/33408)
Epoch: 221 | Batch_idx: 270 |  Loss: (0.0430) |  Loss2: (0.0000) | Acc: (98.00%) (34194/34688)
Epoch: 221 | Batch_idx: 280 |  Loss: (0.0432) |  Loss2: (0.0000) | Acc: (98.00%) (35458/35968)
Epoch: 221 | Batch_idx: 290 |  Loss: (0.0428) |  Loss2: (0.0000) | Acc: (98.00%) (36725/37248)
Epoch: 221 | Batch_idx: 300 |  Loss: (0.0428) |  Loss2: (0.0000) | Acc: (98.00%) (37987/38528)
Epoch: 221 | Batch_idx: 310 |  Loss: (0.0426) |  Loss2: (0.0000) | Acc: (98.00%) (39250/39808)
Epoch: 221 | Batch_idx: 320 |  Loss: (0.0429) |  Loss2: (0.0000) | Acc: (98.00%) (40504/41088)
Epoch: 221 | Batch_idx: 330 |  Loss: (0.0426) |  Loss2: (0.0000) | Acc: (98.00%) (41773/42368)
Epoch: 221 | Batch_idx: 340 |  Loss: (0.0428) |  Loss2: (0.0000) | Acc: (98.00%) (43031/43648)
Epoch: 221 | Batch_idx: 350 |  Loss: (0.0433) |  Loss2: (0.0000) | Acc: (98.00%) (44285/44928)
Epoch: 221 | Batch_idx: 360 |  Loss: (0.0433) |  Loss2: (0.0000) | Acc: (98.00%) (45547/46208)
Epoch: 221 | Batch_idx: 370 |  Loss: (0.0433) |  Loss2: (0.0000) | Acc: (98.00%) (46806/47488)
Epoch: 221 | Batch_idx: 380 |  Loss: (0.0434) |  Loss2: (0.0000) | Acc: (98.00%) (48068/48768)
Epoch: 221 | Batch_idx: 390 |  Loss: (0.0434) |  Loss2: (0.0000) | Acc: (98.00%) (49282/50000)
# TEST : Loss: (0.4467) | Acc: (88.00%) (8871/10000)
percent tensor([0.5898, 0.6055, 0.5970, 0.5830, 0.6028, 0.5699, 0.6099, 0.6076, 0.6092,
        0.6019, 0.6021, 0.6020, 0.5982, 0.6068, 0.5916, 0.5842],
       device='cuda:0') torch.Size([16])
percent tensor([0.5600, 0.5493, 0.5255, 0.5501, 0.5486, 0.5349, 0.5574, 0.5782, 0.5471,
        0.5312, 0.5243, 0.5194, 0.5536, 0.5596, 0.5616, 0.5473],
       device='cuda:0') torch.Size([16])
percent tensor([0.5646, 0.5238, 0.5023, 0.5851, 0.4940, 0.6145, 0.5353, 0.5587, 0.5495,
        0.5133, 0.5396, 0.5008, 0.4874, 0.6549, 0.5797, 0.5754],
       device='cuda:0') torch.Size([16])
percent tensor([0.6987, 0.7294, 0.6663, 0.6481, 0.6469, 0.6833, 0.7082, 0.6351, 0.6880,
        0.7286, 0.7194, 0.7001, 0.7529, 0.6864, 0.7056, 0.7096],
       device='cuda:0') torch.Size([16])
percent tensor([0.7192, 0.6181, 0.7317, 0.7221, 0.7559, 0.7984, 0.7161, 0.7091, 0.7903,
        0.7159, 0.7491, 0.7242, 0.6529, 0.7927, 0.6637, 0.7653],
       device='cuda:0') torch.Size([16])
percent tensor([0.8228, 0.8273, 0.8511, 0.8386, 0.8692, 0.8505, 0.8456, 0.8372, 0.8410,
        0.8368, 0.8478, 0.8217, 0.8256, 0.8582, 0.8181, 0.8450],
       device='cuda:0') torch.Size([16])
percent tensor([0.4375, 0.6524, 0.6137, 0.5541, 0.6412, 0.6906, 0.5754, 0.3922, 0.6189,
        0.5721, 0.5757, 0.4940, 0.6025, 0.5392, 0.3680, 0.4068],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9998, 0.9996, 0.9999, 0.9997, 0.9991, 0.9999, 0.9995, 0.9998,
        0.9998, 0.9999, 0.9999, 0.9999, 0.9995, 0.9993, 0.9994],
       device='cuda:0') torch.Size([16])
Epoch: 222 | Batch_idx: 0 |  Loss: (0.0322) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 222 | Batch_idx: 10 |  Loss: (0.0282) |  Loss2: (0.0000) | Acc: (99.00%) (1397/1408)
Epoch: 222 | Batch_idx: 20 |  Loss: (0.0312) |  Loss2: (0.0000) | Acc: (99.00%) (2665/2688)
Epoch: 222 | Batch_idx: 30 |  Loss: (0.0352) |  Loss2: (0.0000) | Acc: (98.00%) (3925/3968)
Epoch: 222 | Batch_idx: 40 |  Loss: (0.0385) |  Loss2: (0.0000) | Acc: (98.00%) (5185/5248)
Epoch: 222 | Batch_idx: 50 |  Loss: (0.0405) |  Loss2: (0.0000) | Acc: (98.00%) (6442/6528)
Epoch: 222 | Batch_idx: 60 |  Loss: (0.0415) |  Loss2: (0.0000) | Acc: (98.00%) (7702/7808)
Epoch: 222 | Batch_idx: 70 |  Loss: (0.0413) |  Loss2: (0.0000) | Acc: (98.00%) (8960/9088)
Epoch: 222 | Batch_idx: 80 |  Loss: (0.0399) |  Loss2: (0.0000) | Acc: (98.00%) (10231/10368)
Epoch: 222 | Batch_idx: 90 |  Loss: (0.0382) |  Loss2: (0.0000) | Acc: (98.00%) (11502/11648)
Epoch: 222 | Batch_idx: 100 |  Loss: (0.0380) |  Loss2: (0.0000) | Acc: (98.00%) (12768/12928)
Epoch: 222 | Batch_idx: 110 |  Loss: (0.0375) |  Loss2: (0.0000) | Acc: (98.00%) (14034/14208)
Epoch: 222 | Batch_idx: 120 |  Loss: (0.0379) |  Loss2: (0.0000) | Acc: (98.00%) (15297/15488)
Epoch: 222 | Batch_idx: 130 |  Loss: (0.0388) |  Loss2: (0.0000) | Acc: (98.00%) (16558/16768)
Epoch: 222 | Batch_idx: 140 |  Loss: (0.0384) |  Loss2: (0.0000) | Acc: (98.00%) (17828/18048)
Epoch: 222 | Batch_idx: 150 |  Loss: (0.0383) |  Loss2: (0.0000) | Acc: (98.00%) (19097/19328)
Epoch: 222 | Batch_idx: 160 |  Loss: (0.0384) |  Loss2: (0.0000) | Acc: (98.00%) (20362/20608)
Epoch: 222 | Batch_idx: 170 |  Loss: (0.0382) |  Loss2: (0.0000) | Acc: (98.00%) (21630/21888)
Epoch: 222 | Batch_idx: 180 |  Loss: (0.0388) |  Loss2: (0.0000) | Acc: (98.00%) (22889/23168)
Epoch: 222 | Batch_idx: 190 |  Loss: (0.0387) |  Loss2: (0.0000) | Acc: (98.00%) (24153/24448)
Epoch: 222 | Batch_idx: 200 |  Loss: (0.0389) |  Loss2: (0.0000) | Acc: (98.00%) (25413/25728)
Epoch: 222 | Batch_idx: 210 |  Loss: (0.0394) |  Loss2: (0.0000) | Acc: (98.00%) (26671/27008)
Epoch: 222 | Batch_idx: 220 |  Loss: (0.0395) |  Loss2: (0.0000) | Acc: (98.00%) (27936/28288)
Epoch: 222 | Batch_idx: 230 |  Loss: (0.0397) |  Loss2: (0.0000) | Acc: (98.00%) (29196/29568)
Epoch: 222 | Batch_idx: 240 |  Loss: (0.0396) |  Loss2: (0.0000) | Acc: (98.00%) (30464/30848)
Epoch: 222 | Batch_idx: 250 |  Loss: (0.0398) |  Loss2: (0.0000) | Acc: (98.00%) (31728/32128)
Epoch: 222 | Batch_idx: 260 |  Loss: (0.0397) |  Loss2: (0.0000) | Acc: (98.00%) (32994/33408)
Epoch: 222 | Batch_idx: 270 |  Loss: (0.0393) |  Loss2: (0.0000) | Acc: (98.00%) (34260/34688)
Epoch: 222 | Batch_idx: 280 |  Loss: (0.0392) |  Loss2: (0.0000) | Acc: (98.00%) (35523/35968)
Epoch: 222 | Batch_idx: 290 |  Loss: (0.0394) |  Loss2: (0.0000) | Acc: (98.00%) (36783/37248)
Epoch: 222 | Batch_idx: 300 |  Loss: (0.0393) |  Loss2: (0.0000) | Acc: (98.00%) (38048/38528)
Epoch: 222 | Batch_idx: 310 |  Loss: (0.0394) |  Loss2: (0.0000) | Acc: (98.00%) (39309/39808)
Epoch: 222 | Batch_idx: 320 |  Loss: (0.0398) |  Loss2: (0.0000) | Acc: (98.00%) (40566/41088)
Epoch: 222 | Batch_idx: 330 |  Loss: (0.0396) |  Loss2: (0.0000) | Acc: (98.00%) (41833/42368)
Epoch: 222 | Batch_idx: 340 |  Loss: (0.0398) |  Loss2: (0.0000) | Acc: (98.00%) (43090/43648)
Epoch: 222 | Batch_idx: 350 |  Loss: (0.0398) |  Loss2: (0.0000) | Acc: (98.00%) (44353/44928)
Epoch: 222 | Batch_idx: 360 |  Loss: (0.0399) |  Loss2: (0.0000) | Acc: (98.00%) (45620/46208)
Epoch: 222 | Batch_idx: 370 |  Loss: (0.0401) |  Loss2: (0.0000) | Acc: (98.00%) (46879/47488)
Epoch: 222 | Batch_idx: 380 |  Loss: (0.0404) |  Loss2: (0.0000) | Acc: (98.00%) (48135/48768)
Epoch: 222 | Batch_idx: 390 |  Loss: (0.0406) |  Loss2: (0.0000) | Acc: (98.00%) (49346/50000)
# TEST : Loss: (0.4366) | Acc: (89.00%) (8909/10000)
percent tensor([0.5894, 0.6056, 0.5956, 0.5829, 0.6027, 0.5705, 0.6097, 0.6068, 0.6082,
        0.6018, 0.6015, 0.6015, 0.5978, 0.6065, 0.5919, 0.5842],
       device='cuda:0') torch.Size([16])
percent tensor([0.5615, 0.5517, 0.5286, 0.5491, 0.5487, 0.5330, 0.5595, 0.5783, 0.5513,
        0.5341, 0.5279, 0.5218, 0.5569, 0.5607, 0.5624, 0.5473],
       device='cuda:0') torch.Size([16])
percent tensor([0.5686, 0.5247, 0.5149, 0.5909, 0.4973, 0.6161, 0.5357, 0.5638, 0.5515,
        0.5159, 0.5409, 0.5102, 0.4871, 0.6569, 0.5774, 0.5777],
       device='cuda:0') torch.Size([16])
percent tensor([0.7028, 0.7309, 0.6688, 0.6576, 0.6477, 0.6912, 0.7080, 0.6377, 0.6939,
        0.7310, 0.7212, 0.7041, 0.7531, 0.6957, 0.7097, 0.7143],
       device='cuda:0') torch.Size([16])
percent tensor([0.7050, 0.6114, 0.7140, 0.7256, 0.7502, 0.7858, 0.7127, 0.7059, 0.7735,
        0.7063, 0.7287, 0.7104, 0.6327, 0.7786, 0.6511, 0.7587],
       device='cuda:0') torch.Size([16])
percent tensor([0.8191, 0.8287, 0.8467, 0.8434, 0.8668, 0.8476, 0.8476, 0.8366, 0.8463,
        0.8361, 0.8483, 0.8230, 0.8296, 0.8634, 0.8191, 0.8462],
       device='cuda:0') torch.Size([16])
percent tensor([0.4263, 0.6121, 0.5716, 0.5076, 0.6103, 0.6984, 0.5456, 0.3794, 0.6002,
        0.5294, 0.5737, 0.4587, 0.5997, 0.4839, 0.3538, 0.4134],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9998, 0.9997, 0.9999, 0.9997, 0.9997, 0.9999, 0.9995, 0.9998,
        0.9997, 0.9999, 0.9999, 0.9998, 0.9994, 0.9993, 0.9993],
       device='cuda:0') torch.Size([16])
Epoch: 223 | Batch_idx: 0 |  Loss: (0.0448) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 223 | Batch_idx: 10 |  Loss: (0.0457) |  Loss2: (0.0000) | Acc: (98.00%) (1387/1408)
Epoch: 223 | Batch_idx: 20 |  Loss: (0.0460) |  Loss2: (0.0000) | Acc: (98.00%) (2648/2688)
Epoch: 223 | Batch_idx: 30 |  Loss: (0.0413) |  Loss2: (0.0000) | Acc: (98.00%) (3916/3968)
Epoch: 223 | Batch_idx: 40 |  Loss: (0.0397) |  Loss2: (0.0000) | Acc: (98.00%) (5183/5248)
Epoch: 223 | Batch_idx: 50 |  Loss: (0.0386) |  Loss2: (0.0000) | Acc: (98.00%) (6450/6528)
Epoch: 223 | Batch_idx: 60 |  Loss: (0.0387) |  Loss2: (0.0000) | Acc: (98.00%) (7716/7808)
Epoch: 223 | Batch_idx: 70 |  Loss: (0.0385) |  Loss2: (0.0000) | Acc: (98.00%) (8984/9088)
Epoch: 223 | Batch_idx: 80 |  Loss: (0.0384) |  Loss2: (0.0000) | Acc: (98.00%) (10250/10368)
Epoch: 223 | Batch_idx: 90 |  Loss: (0.0388) |  Loss2: (0.0000) | Acc: (98.00%) (11509/11648)
Epoch: 223 | Batch_idx: 100 |  Loss: (0.0397) |  Loss2: (0.0000) | Acc: (98.00%) (12764/12928)
Epoch: 223 | Batch_idx: 110 |  Loss: (0.0394) |  Loss2: (0.0000) | Acc: (98.00%) (14032/14208)
Epoch: 223 | Batch_idx: 120 |  Loss: (0.0394) |  Loss2: (0.0000) | Acc: (98.00%) (15293/15488)
Epoch: 223 | Batch_idx: 130 |  Loss: (0.0401) |  Loss2: (0.0000) | Acc: (98.00%) (16555/16768)
Epoch: 223 | Batch_idx: 140 |  Loss: (0.0404) |  Loss2: (0.0000) | Acc: (98.00%) (17815/18048)
Epoch: 223 | Batch_idx: 150 |  Loss: (0.0400) |  Loss2: (0.0000) | Acc: (98.00%) (19084/19328)
Epoch: 223 | Batch_idx: 160 |  Loss: (0.0402) |  Loss2: (0.0000) | Acc: (98.00%) (20346/20608)
Epoch: 223 | Batch_idx: 170 |  Loss: (0.0402) |  Loss2: (0.0000) | Acc: (98.00%) (21611/21888)
Epoch: 223 | Batch_idx: 180 |  Loss: (0.0398) |  Loss2: (0.0000) | Acc: (98.00%) (22876/23168)
Epoch: 223 | Batch_idx: 190 |  Loss: (0.0402) |  Loss2: (0.0000) | Acc: (98.00%) (24135/24448)
Epoch: 223 | Batch_idx: 200 |  Loss: (0.0403) |  Loss2: (0.0000) | Acc: (98.00%) (25393/25728)
Epoch: 223 | Batch_idx: 210 |  Loss: (0.0403) |  Loss2: (0.0000) | Acc: (98.00%) (26654/27008)
Epoch: 223 | Batch_idx: 220 |  Loss: (0.0404) |  Loss2: (0.0000) | Acc: (98.00%) (27917/28288)
Epoch: 223 | Batch_idx: 230 |  Loss: (0.0404) |  Loss2: (0.0000) | Acc: (98.00%) (29174/29568)
Epoch: 223 | Batch_idx: 240 |  Loss: (0.0408) |  Loss2: (0.0000) | Acc: (98.00%) (30440/30848)
Epoch: 223 | Batch_idx: 250 |  Loss: (0.0405) |  Loss2: (0.0000) | Acc: (98.00%) (31707/32128)
Epoch: 223 | Batch_idx: 260 |  Loss: (0.0409) |  Loss2: (0.0000) | Acc: (98.00%) (32967/33408)
Epoch: 223 | Batch_idx: 270 |  Loss: (0.0406) |  Loss2: (0.0000) | Acc: (98.00%) (34234/34688)
Epoch: 223 | Batch_idx: 280 |  Loss: (0.0408) |  Loss2: (0.0000) | Acc: (98.00%) (35494/35968)
Epoch: 223 | Batch_idx: 290 |  Loss: (0.0405) |  Loss2: (0.0000) | Acc: (98.00%) (36764/37248)
Epoch: 223 | Batch_idx: 300 |  Loss: (0.0408) |  Loss2: (0.0000) | Acc: (98.00%) (38017/38528)
Epoch: 223 | Batch_idx: 310 |  Loss: (0.0406) |  Loss2: (0.0000) | Acc: (98.00%) (39283/39808)
Epoch: 223 | Batch_idx: 320 |  Loss: (0.0406) |  Loss2: (0.0000) | Acc: (98.00%) (40549/41088)
Epoch: 223 | Batch_idx: 330 |  Loss: (0.0404) |  Loss2: (0.0000) | Acc: (98.00%) (41816/42368)
Epoch: 223 | Batch_idx: 340 |  Loss: (0.0403) |  Loss2: (0.0000) | Acc: (98.00%) (43079/43648)
Epoch: 223 | Batch_idx: 350 |  Loss: (0.0404) |  Loss2: (0.0000) | Acc: (98.00%) (44345/44928)
Epoch: 223 | Batch_idx: 360 |  Loss: (0.0406) |  Loss2: (0.0000) | Acc: (98.00%) (45603/46208)
Epoch: 223 | Batch_idx: 370 |  Loss: (0.0408) |  Loss2: (0.0000) | Acc: (98.00%) (46859/47488)
Epoch: 223 | Batch_idx: 380 |  Loss: (0.0406) |  Loss2: (0.0000) | Acc: (98.00%) (48128/48768)
Epoch: 223 | Batch_idx: 390 |  Loss: (0.0410) |  Loss2: (0.0000) | Acc: (98.00%) (49341/50000)
# TEST : Loss: (0.4229) | Acc: (88.00%) (8882/10000)
percent tensor([0.5903, 0.6066, 0.5955, 0.5843, 0.6033, 0.5708, 0.6100, 0.6072, 0.6083,
        0.6023, 0.6019, 0.6009, 0.5986, 0.6082, 0.5925, 0.5850],
       device='cuda:0') torch.Size([16])
percent tensor([0.5614, 0.5505, 0.5251, 0.5476, 0.5474, 0.5315, 0.5582, 0.5761, 0.5493,
        0.5324, 0.5263, 0.5195, 0.5568, 0.5601, 0.5596, 0.5472],
       device='cuda:0') torch.Size([16])
percent tensor([0.5640, 0.5289, 0.4956, 0.5812, 0.4906, 0.6130, 0.5401, 0.5583, 0.5595,
        0.5151, 0.5449, 0.4996, 0.4898, 0.6611, 0.5744, 0.5783],
       device='cuda:0') torch.Size([16])
percent tensor([0.6992, 0.7309, 0.6710, 0.6559, 0.6518, 0.6860, 0.7118, 0.6409, 0.6888,
        0.7300, 0.7175, 0.7079, 0.7528, 0.6963, 0.7061, 0.7107],
       device='cuda:0') torch.Size([16])
percent tensor([0.7087, 0.6208, 0.7404, 0.7390, 0.7645, 0.7938, 0.7036, 0.7150, 0.7850,
        0.7109, 0.7415, 0.7217, 0.6440, 0.7830, 0.6656, 0.7575],
       device='cuda:0') torch.Size([16])
percent tensor([0.8233, 0.8295, 0.8544, 0.8445, 0.8742, 0.8516, 0.8462, 0.8379, 0.8409,
        0.8381, 0.8499, 0.8248, 0.8298, 0.8574, 0.8235, 0.8459],
       device='cuda:0') torch.Size([16])
percent tensor([0.4132, 0.6279, 0.5857, 0.5421, 0.6236, 0.6950, 0.5678, 0.3715, 0.6188,
        0.5400, 0.5915, 0.4904, 0.6116, 0.4935, 0.3545, 0.3791],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9999, 0.9995, 0.9998, 0.9997, 0.9995, 0.9999, 0.9993, 0.9999,
        0.9998, 1.0000, 0.9999, 0.9998, 0.9994, 0.9994, 0.9994],
       device='cuda:0') torch.Size([16])
Epoch: 224 | Batch_idx: 0 |  Loss: (0.0484) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 224 | Batch_idx: 10 |  Loss: (0.0301) |  Loss2: (0.0000) | Acc: (99.00%) (1400/1408)
Epoch: 224 | Batch_idx: 20 |  Loss: (0.0374) |  Loss2: (0.0000) | Acc: (99.00%) (2666/2688)
Epoch: 224 | Batch_idx: 30 |  Loss: (0.0373) |  Loss2: (0.0000) | Acc: (99.00%) (3930/3968)
Epoch: 224 | Batch_idx: 40 |  Loss: (0.0380) |  Loss2: (0.0000) | Acc: (99.00%) (5196/5248)
Epoch: 224 | Batch_idx: 50 |  Loss: (0.0387) |  Loss2: (0.0000) | Acc: (98.00%) (6462/6528)
Epoch: 224 | Batch_idx: 60 |  Loss: (0.0376) |  Loss2: (0.0000) | Acc: (98.00%) (7728/7808)
Epoch: 224 | Batch_idx: 70 |  Loss: (0.0365) |  Loss2: (0.0000) | Acc: (98.00%) (8992/9088)
Epoch: 224 | Batch_idx: 80 |  Loss: (0.0351) |  Loss2: (0.0000) | Acc: (99.00%) (10265/10368)
Epoch: 224 | Batch_idx: 90 |  Loss: (0.0356) |  Loss2: (0.0000) | Acc: (98.00%) (11526/11648)
Epoch: 224 | Batch_idx: 100 |  Loss: (0.0373) |  Loss2: (0.0000) | Acc: (98.00%) (12787/12928)
Epoch: 224 | Batch_idx: 110 |  Loss: (0.0374) |  Loss2: (0.0000) | Acc: (98.00%) (14051/14208)
Epoch: 224 | Batch_idx: 120 |  Loss: (0.0376) |  Loss2: (0.0000) | Acc: (98.00%) (15320/15488)
Epoch: 224 | Batch_idx: 130 |  Loss: (0.0380) |  Loss2: (0.0000) | Acc: (98.00%) (16585/16768)
Epoch: 224 | Batch_idx: 140 |  Loss: (0.0381) |  Loss2: (0.0000) | Acc: (98.00%) (17850/18048)
Epoch: 224 | Batch_idx: 150 |  Loss: (0.0375) |  Loss2: (0.0000) | Acc: (98.00%) (19118/19328)
Epoch: 224 | Batch_idx: 160 |  Loss: (0.0373) |  Loss2: (0.0000) | Acc: (98.00%) (20383/20608)
Epoch: 224 | Batch_idx: 170 |  Loss: (0.0380) |  Loss2: (0.0000) | Acc: (98.00%) (21643/21888)
Epoch: 224 | Batch_idx: 180 |  Loss: (0.0378) |  Loss2: (0.0000) | Acc: (98.00%) (22910/23168)
Epoch: 224 | Batch_idx: 190 |  Loss: (0.0380) |  Loss2: (0.0000) | Acc: (98.00%) (24170/24448)
Epoch: 224 | Batch_idx: 200 |  Loss: (0.0377) |  Loss2: (0.0000) | Acc: (98.00%) (25437/25728)
Epoch: 224 | Batch_idx: 210 |  Loss: (0.0378) |  Loss2: (0.0000) | Acc: (98.00%) (26698/27008)
Epoch: 224 | Batch_idx: 220 |  Loss: (0.0379) |  Loss2: (0.0000) | Acc: (98.00%) (27962/28288)
Epoch: 224 | Batch_idx: 230 |  Loss: (0.0377) |  Loss2: (0.0000) | Acc: (98.00%) (29226/29568)
Epoch: 224 | Batch_idx: 240 |  Loss: (0.0380) |  Loss2: (0.0000) | Acc: (98.00%) (30488/30848)
Epoch: 224 | Batch_idx: 250 |  Loss: (0.0384) |  Loss2: (0.0000) | Acc: (98.00%) (31748/32128)
Epoch: 224 | Batch_idx: 260 |  Loss: (0.0386) |  Loss2: (0.0000) | Acc: (98.00%) (33009/33408)
Epoch: 224 | Batch_idx: 270 |  Loss: (0.0385) |  Loss2: (0.0000) | Acc: (98.00%) (34276/34688)
Epoch: 224 | Batch_idx: 280 |  Loss: (0.0388) |  Loss2: (0.0000) | Acc: (98.00%) (35537/35968)
Epoch: 224 | Batch_idx: 290 |  Loss: (0.0391) |  Loss2: (0.0000) | Acc: (98.00%) (36799/37248)
Epoch: 224 | Batch_idx: 300 |  Loss: (0.0387) |  Loss2: (0.0000) | Acc: (98.00%) (38070/38528)
Epoch: 224 | Batch_idx: 310 |  Loss: (0.0388) |  Loss2: (0.0000) | Acc: (98.00%) (39332/39808)
Epoch: 224 | Batch_idx: 320 |  Loss: (0.0387) |  Loss2: (0.0000) | Acc: (98.00%) (40596/41088)
Epoch: 224 | Batch_idx: 330 |  Loss: (0.0390) |  Loss2: (0.0000) | Acc: (98.00%) (41855/42368)
Epoch: 224 | Batch_idx: 340 |  Loss: (0.0390) |  Loss2: (0.0000) | Acc: (98.00%) (43118/43648)
Epoch: 224 | Batch_idx: 350 |  Loss: (0.0392) |  Loss2: (0.0000) | Acc: (98.00%) (44379/44928)
Epoch: 224 | Batch_idx: 360 |  Loss: (0.0392) |  Loss2: (0.0000) | Acc: (98.00%) (45641/46208)
Epoch: 224 | Batch_idx: 370 |  Loss: (0.0391) |  Loss2: (0.0000) | Acc: (98.00%) (46910/47488)
Epoch: 224 | Batch_idx: 380 |  Loss: (0.0389) |  Loss2: (0.0000) | Acc: (98.00%) (48177/48768)
Epoch: 224 | Batch_idx: 390 |  Loss: (0.0391) |  Loss2: (0.0000) | Acc: (98.00%) (49394/50000)
# TEST : Loss: (0.4417) | Acc: (88.00%) (8881/10000)
percent tensor([0.5913, 0.6112, 0.5953, 0.5852, 0.6019, 0.5711, 0.6129, 0.6101, 0.6106,
        0.6052, 0.6048, 0.6014, 0.6006, 0.6142, 0.5951, 0.5873],
       device='cuda:0') torch.Size([16])
percent tensor([0.5633, 0.5519, 0.5277, 0.5491, 0.5495, 0.5324, 0.5600, 0.5783, 0.5519,
        0.5321, 0.5276, 0.5215, 0.5566, 0.5619, 0.5598, 0.5474],
       device='cuda:0') torch.Size([16])
percent tensor([0.5690, 0.5275, 0.5091, 0.5831, 0.4992, 0.6130, 0.5442, 0.5623, 0.5588,
        0.5144, 0.5448, 0.5078, 0.4920, 0.6545, 0.5770, 0.5742],
       device='cuda:0') torch.Size([16])
percent tensor([0.6977, 0.7329, 0.6690, 0.6460, 0.6476, 0.6841, 0.7093, 0.6364, 0.6888,
        0.7294, 0.7180, 0.7045, 0.7513, 0.6978, 0.7016, 0.7074],
       device='cuda:0') torch.Size([16])
percent tensor([0.6961, 0.6049, 0.7146, 0.7280, 0.7536, 0.7860, 0.6980, 0.7029, 0.7641,
        0.6954, 0.7189, 0.7074, 0.6335, 0.7674, 0.6483, 0.7540],
       device='cuda:0') torch.Size([16])
percent tensor([0.8261, 0.8365, 0.8559, 0.8484, 0.8741, 0.8466, 0.8494, 0.8377, 0.8441,
        0.8435, 0.8543, 0.8284, 0.8367, 0.8628, 0.8234, 0.8504],
       device='cuda:0') torch.Size([16])
percent tensor([0.4175, 0.6189, 0.5908, 0.5459, 0.6316, 0.6838, 0.5620, 0.3884, 0.6081,
        0.5605, 0.5658, 0.4942, 0.6150, 0.4973, 0.3412, 0.3874],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9999, 0.9995, 0.9998, 0.9997, 0.9996, 0.9999, 0.9994, 0.9998,
        0.9998, 0.9999, 0.9999, 0.9997, 0.9995, 0.9993, 0.9992],
       device='cuda:0') torch.Size([16])
Epoch: 225 | Batch_idx: 0 |  Loss: (0.0557) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 225 | Batch_idx: 10 |  Loss: (0.0426) |  Loss2: (0.0000) | Acc: (98.00%) (1391/1408)
Epoch: 225 | Batch_idx: 20 |  Loss: (0.0383) |  Loss2: (0.0000) | Acc: (98.00%) (2656/2688)
Epoch: 225 | Batch_idx: 30 |  Loss: (0.0338) |  Loss2: (0.0000) | Acc: (98.00%) (3926/3968)
Epoch: 225 | Batch_idx: 40 |  Loss: (0.0363) |  Loss2: (0.0000) | Acc: (98.00%) (5187/5248)
Epoch: 225 | Batch_idx: 50 |  Loss: (0.0373) |  Loss2: (0.0000) | Acc: (98.00%) (6451/6528)
Epoch: 225 | Batch_idx: 60 |  Loss: (0.0355) |  Loss2: (0.0000) | Acc: (98.00%) (7721/7808)
Epoch: 225 | Batch_idx: 70 |  Loss: (0.0366) |  Loss2: (0.0000) | Acc: (98.00%) (8984/9088)
Epoch: 225 | Batch_idx: 80 |  Loss: (0.0367) |  Loss2: (0.0000) | Acc: (98.00%) (10251/10368)
Epoch: 225 | Batch_idx: 90 |  Loss: (0.0365) |  Loss2: (0.0000) | Acc: (98.00%) (11514/11648)
Epoch: 225 | Batch_idx: 100 |  Loss: (0.0372) |  Loss2: (0.0000) | Acc: (98.00%) (12777/12928)
Epoch: 225 | Batch_idx: 110 |  Loss: (0.0372) |  Loss2: (0.0000) | Acc: (98.00%) (14044/14208)
Epoch: 225 | Batch_idx: 120 |  Loss: (0.0366) |  Loss2: (0.0000) | Acc: (98.00%) (15314/15488)
Epoch: 225 | Batch_idx: 130 |  Loss: (0.0368) |  Loss2: (0.0000) | Acc: (98.00%) (16578/16768)
Epoch: 225 | Batch_idx: 140 |  Loss: (0.0363) |  Loss2: (0.0000) | Acc: (98.00%) (17848/18048)
Epoch: 225 | Batch_idx: 150 |  Loss: (0.0360) |  Loss2: (0.0000) | Acc: (98.00%) (19120/19328)
Epoch: 225 | Batch_idx: 160 |  Loss: (0.0358) |  Loss2: (0.0000) | Acc: (98.00%) (20390/20608)
Epoch: 225 | Batch_idx: 170 |  Loss: (0.0358) |  Loss2: (0.0000) | Acc: (98.00%) (21657/21888)
Epoch: 225 | Batch_idx: 180 |  Loss: (0.0359) |  Loss2: (0.0000) | Acc: (98.00%) (22925/23168)
Epoch: 225 | Batch_idx: 190 |  Loss: (0.0363) |  Loss2: (0.0000) | Acc: (98.00%) (24186/24448)
Epoch: 225 | Batch_idx: 200 |  Loss: (0.0366) |  Loss2: (0.0000) | Acc: (98.00%) (25449/25728)
Epoch: 225 | Batch_idx: 210 |  Loss: (0.0367) |  Loss2: (0.0000) | Acc: (98.00%) (26713/27008)
Epoch: 225 | Batch_idx: 220 |  Loss: (0.0369) |  Loss2: (0.0000) | Acc: (98.00%) (27971/28288)
Epoch: 225 | Batch_idx: 230 |  Loss: (0.0367) |  Loss2: (0.0000) | Acc: (98.00%) (29242/29568)
Epoch: 225 | Batch_idx: 240 |  Loss: (0.0369) |  Loss2: (0.0000) | Acc: (98.00%) (30506/30848)
Epoch: 225 | Batch_idx: 250 |  Loss: (0.0368) |  Loss2: (0.0000) | Acc: (98.00%) (31772/32128)
Epoch: 225 | Batch_idx: 260 |  Loss: (0.0365) |  Loss2: (0.0000) | Acc: (98.00%) (33041/33408)
Epoch: 225 | Batch_idx: 270 |  Loss: (0.0362) |  Loss2: (0.0000) | Acc: (98.00%) (34312/34688)
Epoch: 225 | Batch_idx: 280 |  Loss: (0.0362) |  Loss2: (0.0000) | Acc: (98.00%) (35579/35968)
Epoch: 225 | Batch_idx: 290 |  Loss: (0.0364) |  Loss2: (0.0000) | Acc: (98.00%) (36843/37248)
Epoch: 225 | Batch_idx: 300 |  Loss: (0.0367) |  Loss2: (0.0000) | Acc: (98.00%) (38101/38528)
Epoch: 225 | Batch_idx: 310 |  Loss: (0.0366) |  Loss2: (0.0000) | Acc: (98.00%) (39368/39808)
Epoch: 225 | Batch_idx: 320 |  Loss: (0.0369) |  Loss2: (0.0000) | Acc: (98.00%) (40632/41088)
Epoch: 225 | Batch_idx: 330 |  Loss: (0.0371) |  Loss2: (0.0000) | Acc: (98.00%) (41895/42368)
Epoch: 225 | Batch_idx: 340 |  Loss: (0.0372) |  Loss2: (0.0000) | Acc: (98.00%) (43159/43648)
Epoch: 225 | Batch_idx: 350 |  Loss: (0.0371) |  Loss2: (0.0000) | Acc: (98.00%) (44426/44928)
Epoch: 225 | Batch_idx: 360 |  Loss: (0.0372) |  Loss2: (0.0000) | Acc: (98.00%) (45689/46208)
Epoch: 225 | Batch_idx: 370 |  Loss: (0.0373) |  Loss2: (0.0000) | Acc: (98.00%) (46952/47488)
Epoch: 225 | Batch_idx: 380 |  Loss: (0.0372) |  Loss2: (0.0000) | Acc: (98.00%) (48217/48768)
Epoch: 225 | Batch_idx: 390 |  Loss: (0.0372) |  Loss2: (0.0000) | Acc: (98.00%) (49433/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_225.pth.tar'
# TEST : Loss: (0.4389) | Acc: (88.00%) (8864/10000)
percent tensor([0.5899, 0.6078, 0.5973, 0.5844, 0.6045, 0.5700, 0.6115, 0.6087, 0.6095,
        0.6040, 0.6023, 0.6031, 0.5988, 0.6103, 0.5929, 0.5853],
       device='cuda:0') torch.Size([16])
percent tensor([0.5580, 0.5488, 0.5240, 0.5447, 0.5444, 0.5280, 0.5566, 0.5754, 0.5482,
        0.5293, 0.5238, 0.5173, 0.5538, 0.5600, 0.5564, 0.5441],
       device='cuda:0') torch.Size([16])
percent tensor([0.5612, 0.5226, 0.5065, 0.5820, 0.4935, 0.6027, 0.5367, 0.5661, 0.5530,
        0.5084, 0.5385, 0.4982, 0.4883, 0.6529, 0.5707, 0.5679],
       device='cuda:0') torch.Size([16])
percent tensor([0.6942, 0.7235, 0.6709, 0.6503, 0.6487, 0.6805, 0.7038, 0.6346, 0.6879,
        0.7236, 0.7141, 0.7036, 0.7462, 0.6870, 0.6971, 0.7026],
       device='cuda:0') torch.Size([16])
percent tensor([0.6964, 0.6103, 0.7099, 0.7217, 0.7501, 0.7796, 0.6990, 0.6896, 0.7710,
        0.6945, 0.7261, 0.7066, 0.6375, 0.7665, 0.6506, 0.7425],
       device='cuda:0') torch.Size([16])
percent tensor([0.8303, 0.8320, 0.8542, 0.8459, 0.8751, 0.8568, 0.8498, 0.8382, 0.8448,
        0.8414, 0.8559, 0.8236, 0.8324, 0.8637, 0.8238, 0.8516],
       device='cuda:0') torch.Size([16])
percent tensor([0.4097, 0.6105, 0.6031, 0.5452, 0.6476, 0.6844, 0.5641, 0.4024, 0.6068,
        0.5362, 0.5699, 0.4724, 0.5837, 0.4666, 0.3551, 0.4070],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9999, 0.9997, 0.9999, 0.9997, 0.9993, 0.9999, 0.9997, 0.9999,
        0.9997, 1.0000, 0.9999, 0.9997, 0.9992, 0.9992, 0.9992],
       device='cuda:0') torch.Size([16])
Epoch: 226 | Batch_idx: 0 |  Loss: (0.0154) |  Loss2: (0.0000) | Acc: (100.00%) (128/128)
Epoch: 226 | Batch_idx: 10 |  Loss: (0.0291) |  Loss2: (0.0000) | Acc: (99.00%) (1395/1408)
Epoch: 226 | Batch_idx: 20 |  Loss: (0.0327) |  Loss2: (0.0000) | Acc: (99.00%) (2662/2688)
Epoch: 226 | Batch_idx: 30 |  Loss: (0.0320) |  Loss2: (0.0000) | Acc: (99.00%) (3929/3968)
Epoch: 226 | Batch_idx: 40 |  Loss: (0.0329) |  Loss2: (0.0000) | Acc: (98.00%) (5195/5248)
Epoch: 226 | Batch_idx: 50 |  Loss: (0.0328) |  Loss2: (0.0000) | Acc: (98.00%) (6462/6528)
Epoch: 226 | Batch_idx: 60 |  Loss: (0.0339) |  Loss2: (0.0000) | Acc: (98.00%) (7724/7808)
Epoch: 226 | Batch_idx: 70 |  Loss: (0.0335) |  Loss2: (0.0000) | Acc: (98.00%) (8992/9088)
Epoch: 226 | Batch_idx: 80 |  Loss: (0.0348) |  Loss2: (0.0000) | Acc: (98.00%) (10253/10368)
Epoch: 226 | Batch_idx: 90 |  Loss: (0.0345) |  Loss2: (0.0000) | Acc: (98.00%) (11522/11648)
Epoch: 226 | Batch_idx: 100 |  Loss: (0.0343) |  Loss2: (0.0000) | Acc: (98.00%) (12793/12928)
Epoch: 226 | Batch_idx: 110 |  Loss: (0.0346) |  Loss2: (0.0000) | Acc: (98.00%) (14053/14208)
Epoch: 226 | Batch_idx: 120 |  Loss: (0.0356) |  Loss2: (0.0000) | Acc: (98.00%) (15307/15488)
Epoch: 226 | Batch_idx: 130 |  Loss: (0.0358) |  Loss2: (0.0000) | Acc: (98.00%) (16568/16768)
Epoch: 226 | Batch_idx: 140 |  Loss: (0.0357) |  Loss2: (0.0000) | Acc: (98.00%) (17834/18048)
Epoch: 226 | Batch_idx: 150 |  Loss: (0.0356) |  Loss2: (0.0000) | Acc: (98.00%) (19100/19328)
Epoch: 226 | Batch_idx: 160 |  Loss: (0.0360) |  Loss2: (0.0000) | Acc: (98.00%) (20364/20608)
Epoch: 226 | Batch_idx: 170 |  Loss: (0.0360) |  Loss2: (0.0000) | Acc: (98.00%) (21630/21888)
Epoch: 226 | Batch_idx: 180 |  Loss: (0.0360) |  Loss2: (0.0000) | Acc: (98.00%) (22896/23168)
Epoch: 226 | Batch_idx: 190 |  Loss: (0.0361) |  Loss2: (0.0000) | Acc: (98.00%) (24161/24448)
Epoch: 226 | Batch_idx: 200 |  Loss: (0.0371) |  Loss2: (0.0000) | Acc: (98.00%) (25415/25728)
Epoch: 226 | Batch_idx: 210 |  Loss: (0.0368) |  Loss2: (0.0000) | Acc: (98.00%) (26684/27008)
Epoch: 226 | Batch_idx: 220 |  Loss: (0.0369) |  Loss2: (0.0000) | Acc: (98.00%) (27947/28288)
Epoch: 226 | Batch_idx: 230 |  Loss: (0.0369) |  Loss2: (0.0000) | Acc: (98.00%) (29212/29568)
Epoch: 226 | Batch_idx: 240 |  Loss: (0.0367) |  Loss2: (0.0000) | Acc: (98.00%) (30482/30848)
Epoch: 226 | Batch_idx: 250 |  Loss: (0.0366) |  Loss2: (0.0000) | Acc: (98.00%) (31747/32128)
Epoch: 226 | Batch_idx: 260 |  Loss: (0.0373) |  Loss2: (0.0000) | Acc: (98.00%) (33003/33408)
Epoch: 226 | Batch_idx: 270 |  Loss: (0.0373) |  Loss2: (0.0000) | Acc: (98.00%) (34268/34688)
Epoch: 226 | Batch_idx: 280 |  Loss: (0.0377) |  Loss2: (0.0000) | Acc: (98.00%) (35522/35968)
Epoch: 226 | Batch_idx: 290 |  Loss: (0.0377) |  Loss2: (0.0000) | Acc: (98.00%) (36787/37248)
Epoch: 226 | Batch_idx: 300 |  Loss: (0.0382) |  Loss2: (0.0000) | Acc: (98.00%) (38046/38528)
Epoch: 226 | Batch_idx: 310 |  Loss: (0.0387) |  Loss2: (0.0000) | Acc: (98.00%) (39303/39808)
Epoch: 226 | Batch_idx: 320 |  Loss: (0.0386) |  Loss2: (0.0000) | Acc: (98.00%) (40572/41088)
Epoch: 226 | Batch_idx: 330 |  Loss: (0.0387) |  Loss2: (0.0000) | Acc: (98.00%) (41838/42368)
Epoch: 226 | Batch_idx: 340 |  Loss: (0.0386) |  Loss2: (0.0000) | Acc: (98.00%) (43107/43648)
Epoch: 226 | Batch_idx: 350 |  Loss: (0.0388) |  Loss2: (0.0000) | Acc: (98.00%) (44367/44928)
Epoch: 226 | Batch_idx: 360 |  Loss: (0.0391) |  Loss2: (0.0000) | Acc: (98.00%) (45625/46208)
Epoch: 226 | Batch_idx: 370 |  Loss: (0.0393) |  Loss2: (0.0000) | Acc: (98.00%) (46883/47488)
Epoch: 226 | Batch_idx: 380 |  Loss: (0.0393) |  Loss2: (0.0000) | Acc: (98.00%) (48147/48768)
Epoch: 226 | Batch_idx: 390 |  Loss: (0.0395) |  Loss2: (0.0000) | Acc: (98.00%) (49357/50000)
# TEST : Loss: (0.5214) | Acc: (87.00%) (8713/10000)
percent tensor([0.5905, 0.6099, 0.5944, 0.5846, 0.6025, 0.5711, 0.6123, 0.6095, 0.6100,
        0.6048, 0.6042, 0.6016, 0.5999, 0.6126, 0.5949, 0.5866],
       device='cuda:0') torch.Size([16])
percent tensor([0.5569, 0.5488, 0.5219, 0.5447, 0.5426, 0.5257, 0.5563, 0.5758, 0.5478,
        0.5295, 0.5227, 0.5145, 0.5533, 0.5601, 0.5551, 0.5433],
       device='cuda:0') torch.Size([16])
percent tensor([0.5590, 0.5166, 0.5005, 0.5758, 0.4928, 0.6035, 0.5340, 0.5596, 0.5557,
        0.5028, 0.5327, 0.4949, 0.4847, 0.6569, 0.5679, 0.5666],
       device='cuda:0') torch.Size([16])
percent tensor([0.7001, 0.7316, 0.6749, 0.6576, 0.6545, 0.6959, 0.7101, 0.6387, 0.6931,
        0.7301, 0.7216, 0.7080, 0.7534, 0.6951, 0.7057, 0.7118],
       device='cuda:0') torch.Size([16])
percent tensor([0.6777, 0.5869, 0.7139, 0.7277, 0.7487, 0.7789, 0.6911, 0.6917, 0.7588,
        0.6778, 0.7080, 0.7064, 0.6231, 0.7541, 0.6296, 0.7403],
       device='cuda:0') torch.Size([16])
percent tensor([0.8216, 0.8319, 0.8514, 0.8415, 0.8706, 0.8476, 0.8471, 0.8333, 0.8413,
        0.8383, 0.8479, 0.8301, 0.8309, 0.8602, 0.8195, 0.8454],
       device='cuda:0') torch.Size([16])
percent tensor([0.4183, 0.6085, 0.5884, 0.5347, 0.6192, 0.6777, 0.5478, 0.3908, 0.5874,
        0.5366, 0.5741, 0.4920, 0.6053, 0.4417, 0.3587, 0.4009],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9999, 0.9994, 0.9998, 0.9994, 0.9993, 0.9998, 0.9995, 0.9998,
        0.9998, 1.0000, 0.9998, 0.9998, 0.9997, 0.9992, 0.9995],
       device='cuda:0') torch.Size([16])
Epoch: 227 | Batch_idx: 0 |  Loss: (0.0507) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 227 | Batch_idx: 10 |  Loss: (0.0410) |  Loss2: (0.0000) | Acc: (98.00%) (1389/1408)
Epoch: 227 | Batch_idx: 20 |  Loss: (0.0400) |  Loss2: (0.0000) | Acc: (98.00%) (2656/2688)
Epoch: 227 | Batch_idx: 30 |  Loss: (0.0396) |  Loss2: (0.0000) | Acc: (98.00%) (3921/3968)
Epoch: 227 | Batch_idx: 40 |  Loss: (0.0421) |  Loss2: (0.0000) | Acc: (98.00%) (5179/5248)
Epoch: 227 | Batch_idx: 50 |  Loss: (0.0400) |  Loss2: (0.0000) | Acc: (98.00%) (6446/6528)
Epoch: 227 | Batch_idx: 60 |  Loss: (0.0398) |  Loss2: (0.0000) | Acc: (98.00%) (7710/7808)
Epoch: 227 | Batch_idx: 70 |  Loss: (0.0388) |  Loss2: (0.0000) | Acc: (98.00%) (8979/9088)
Epoch: 227 | Batch_idx: 80 |  Loss: (0.0388) |  Loss2: (0.0000) | Acc: (98.00%) (10244/10368)
Epoch: 227 | Batch_idx: 90 |  Loss: (0.0379) |  Loss2: (0.0000) | Acc: (98.00%) (11514/11648)
Epoch: 227 | Batch_idx: 100 |  Loss: (0.0372) |  Loss2: (0.0000) | Acc: (98.00%) (12781/12928)
Epoch: 227 | Batch_idx: 110 |  Loss: (0.0370) |  Loss2: (0.0000) | Acc: (98.00%) (14047/14208)
Epoch: 227 | Batch_idx: 120 |  Loss: (0.0364) |  Loss2: (0.0000) | Acc: (98.00%) (15315/15488)
Epoch: 227 | Batch_idx: 130 |  Loss: (0.0369) |  Loss2: (0.0000) | Acc: (98.00%) (16579/16768)
Epoch: 227 | Batch_idx: 140 |  Loss: (0.0365) |  Loss2: (0.0000) | Acc: (98.00%) (17846/18048)
Epoch: 227 | Batch_idx: 150 |  Loss: (0.0365) |  Loss2: (0.0000) | Acc: (98.00%) (19112/19328)
Epoch: 227 | Batch_idx: 160 |  Loss: (0.0367) |  Loss2: (0.0000) | Acc: (98.00%) (20378/20608)
Epoch: 227 | Batch_idx: 170 |  Loss: (0.0369) |  Loss2: (0.0000) | Acc: (98.00%) (21647/21888)
Epoch: 227 | Batch_idx: 180 |  Loss: (0.0366) |  Loss2: (0.0000) | Acc: (98.00%) (22916/23168)
Epoch: 227 | Batch_idx: 190 |  Loss: (0.0366) |  Loss2: (0.0000) | Acc: (98.00%) (24178/24448)
Epoch: 227 | Batch_idx: 200 |  Loss: (0.0369) |  Loss2: (0.0000) | Acc: (98.00%) (25441/25728)
Epoch: 227 | Batch_idx: 210 |  Loss: (0.0371) |  Loss2: (0.0000) | Acc: (98.00%) (26702/27008)
Epoch: 227 | Batch_idx: 220 |  Loss: (0.0366) |  Loss2: (0.0000) | Acc: (98.00%) (27972/28288)
Epoch: 227 | Batch_idx: 230 |  Loss: (0.0369) |  Loss2: (0.0000) | Acc: (98.00%) (29235/29568)
Epoch: 227 | Batch_idx: 240 |  Loss: (0.0371) |  Loss2: (0.0000) | Acc: (98.00%) (30501/30848)
Epoch: 227 | Batch_idx: 250 |  Loss: (0.0374) |  Loss2: (0.0000) | Acc: (98.00%) (31761/32128)
Epoch: 227 | Batch_idx: 260 |  Loss: (0.0383) |  Loss2: (0.0000) | Acc: (98.00%) (33011/33408)
Epoch: 227 | Batch_idx: 270 |  Loss: (0.0381) |  Loss2: (0.0000) | Acc: (98.00%) (34278/34688)
Epoch: 227 | Batch_idx: 280 |  Loss: (0.0380) |  Loss2: (0.0000) | Acc: (98.00%) (35549/35968)
Epoch: 227 | Batch_idx: 290 |  Loss: (0.0377) |  Loss2: (0.0000) | Acc: (98.00%) (36815/37248)
Epoch: 227 | Batch_idx: 300 |  Loss: (0.0378) |  Loss2: (0.0000) | Acc: (98.00%) (38079/38528)
Epoch: 227 | Batch_idx: 310 |  Loss: (0.0379) |  Loss2: (0.0000) | Acc: (98.00%) (39341/39808)
Epoch: 227 | Batch_idx: 320 |  Loss: (0.0378) |  Loss2: (0.0000) | Acc: (98.00%) (40605/41088)
Epoch: 227 | Batch_idx: 330 |  Loss: (0.0379) |  Loss2: (0.0000) | Acc: (98.00%) (41867/42368)
Epoch: 227 | Batch_idx: 340 |  Loss: (0.0378) |  Loss2: (0.0000) | Acc: (98.00%) (43132/43648)
Epoch: 227 | Batch_idx: 350 |  Loss: (0.0380) |  Loss2: (0.0000) | Acc: (98.00%) (44395/44928)
Epoch: 227 | Batch_idx: 360 |  Loss: (0.0381) |  Loss2: (0.0000) | Acc: (98.00%) (45658/46208)
Epoch: 227 | Batch_idx: 370 |  Loss: (0.0380) |  Loss2: (0.0000) | Acc: (98.00%) (46924/47488)
Epoch: 227 | Batch_idx: 380 |  Loss: (0.0380) |  Loss2: (0.0000) | Acc: (98.00%) (48184/48768)
Epoch: 227 | Batch_idx: 390 |  Loss: (0.0378) |  Loss2: (0.0000) | Acc: (98.00%) (49401/50000)
# TEST : Loss: (0.4171) | Acc: (89.00%) (8934/10000)
percent tensor([0.5885, 0.6063, 0.5951, 0.5836, 0.6022, 0.5707, 0.6095, 0.6076, 0.6073,
        0.6023, 0.6005, 0.6005, 0.5970, 0.6083, 0.5920, 0.5844],
       device='cuda:0') torch.Size([16])
percent tensor([0.5593, 0.5464, 0.5261, 0.5457, 0.5455, 0.5266, 0.5556, 0.5753, 0.5480,
        0.5288, 0.5229, 0.5174, 0.5544, 0.5559, 0.5555, 0.5434],
       device='cuda:0') torch.Size([16])
percent tensor([0.5703, 0.5225, 0.5097, 0.5847, 0.4958, 0.6150, 0.5393, 0.5613, 0.5609,
        0.5092, 0.5435, 0.5036, 0.4968, 0.6611, 0.5748, 0.5775],
       device='cuda:0') torch.Size([16])
percent tensor([0.7001, 0.7317, 0.6719, 0.6532, 0.6484, 0.6893, 0.7097, 0.6368, 0.6927,
        0.7297, 0.7202, 0.7069, 0.7532, 0.6982, 0.7047, 0.7128],
       device='cuda:0') torch.Size([16])
percent tensor([0.6826, 0.6135, 0.7141, 0.7126, 0.7487, 0.7819, 0.7092, 0.7022, 0.7629,
        0.7013, 0.7201, 0.7179, 0.6391, 0.7763, 0.6448, 0.7460],
       device='cuda:0') torch.Size([16])
percent tensor([0.8256, 0.8327, 0.8535, 0.8430, 0.8733, 0.8518, 0.8490, 0.8376, 0.8439,
        0.8377, 0.8485, 0.8294, 0.8346, 0.8586, 0.8229, 0.8476],
       device='cuda:0') torch.Size([16])
percent tensor([0.4165, 0.6434, 0.5983, 0.5553, 0.6379, 0.6980, 0.5443, 0.4096, 0.5948,
        0.5612, 0.5837, 0.5060, 0.6079, 0.4765, 0.3678, 0.3978],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9999, 0.9995, 0.9999, 0.9997, 0.9991, 0.9998, 0.9996, 0.9998,
        0.9998, 1.0000, 0.9998, 0.9998, 0.9995, 0.9993, 0.9994],
       device='cuda:0') torch.Size([16])
Epoch: 228 | Batch_idx: 0 |  Loss: (0.0439) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 228 | Batch_idx: 10 |  Loss: (0.0275) |  Loss2: (0.0000) | Acc: (99.00%) (1395/1408)
Epoch: 228 | Batch_idx: 20 |  Loss: (0.0297) |  Loss2: (0.0000) | Acc: (98.00%) (2661/2688)
Epoch: 228 | Batch_idx: 30 |  Loss: (0.0295) |  Loss2: (0.0000) | Acc: (99.00%) (3929/3968)
Epoch: 228 | Batch_idx: 40 |  Loss: (0.0313) |  Loss2: (0.0000) | Acc: (99.00%) (5196/5248)
Epoch: 228 | Batch_idx: 50 |  Loss: (0.0315) |  Loss2: (0.0000) | Acc: (99.00%) (6463/6528)
Epoch: 228 | Batch_idx: 60 |  Loss: (0.0312) |  Loss2: (0.0000) | Acc: (98.00%) (7728/7808)
Epoch: 228 | Batch_idx: 70 |  Loss: (0.0317) |  Loss2: (0.0000) | Acc: (98.00%) (8997/9088)
Epoch: 228 | Batch_idx: 80 |  Loss: (0.0319) |  Loss2: (0.0000) | Acc: (98.00%) (10262/10368)
Epoch: 228 | Batch_idx: 90 |  Loss: (0.0321) |  Loss2: (0.0000) | Acc: (98.00%) (11527/11648)
Epoch: 228 | Batch_idx: 100 |  Loss: (0.0330) |  Loss2: (0.0000) | Acc: (98.00%) (12791/12928)
Epoch: 228 | Batch_idx: 110 |  Loss: (0.0329) |  Loss2: (0.0000) | Acc: (98.00%) (14061/14208)
Epoch: 228 | Batch_idx: 120 |  Loss: (0.0328) |  Loss2: (0.0000) | Acc: (98.00%) (15325/15488)
Epoch: 228 | Batch_idx: 130 |  Loss: (0.0330) |  Loss2: (0.0000) | Acc: (98.00%) (16590/16768)
Epoch: 228 | Batch_idx: 140 |  Loss: (0.0326) |  Loss2: (0.0000) | Acc: (98.00%) (17859/18048)
Epoch: 228 | Batch_idx: 150 |  Loss: (0.0331) |  Loss2: (0.0000) | Acc: (98.00%) (19123/19328)
Epoch: 228 | Batch_idx: 160 |  Loss: (0.0331) |  Loss2: (0.0000) | Acc: (98.00%) (20390/20608)
Epoch: 228 | Batch_idx: 170 |  Loss: (0.0333) |  Loss2: (0.0000) | Acc: (98.00%) (21655/21888)
Epoch: 228 | Batch_idx: 180 |  Loss: (0.0337) |  Loss2: (0.0000) | Acc: (98.00%) (22917/23168)
Epoch: 228 | Batch_idx: 190 |  Loss: (0.0343) |  Loss2: (0.0000) | Acc: (98.00%) (24180/24448)
Epoch: 228 | Batch_idx: 200 |  Loss: (0.0345) |  Loss2: (0.0000) | Acc: (98.00%) (25441/25728)
Epoch: 228 | Batch_idx: 210 |  Loss: (0.0339) |  Loss2: (0.0000) | Acc: (98.00%) (26716/27008)
Epoch: 228 | Batch_idx: 220 |  Loss: (0.0340) |  Loss2: (0.0000) | Acc: (98.00%) (27978/28288)
Epoch: 228 | Batch_idx: 230 |  Loss: (0.0340) |  Loss2: (0.0000) | Acc: (98.00%) (29243/29568)
Epoch: 228 | Batch_idx: 240 |  Loss: (0.0340) |  Loss2: (0.0000) | Acc: (98.00%) (30512/30848)
Epoch: 228 | Batch_idx: 250 |  Loss: (0.0342) |  Loss2: (0.0000) | Acc: (98.00%) (31773/32128)
Epoch: 228 | Batch_idx: 260 |  Loss: (0.0343) |  Loss2: (0.0000) | Acc: (98.00%) (33039/33408)
Epoch: 228 | Batch_idx: 270 |  Loss: (0.0348) |  Loss2: (0.0000) | Acc: (98.00%) (34297/34688)
Epoch: 228 | Batch_idx: 280 |  Loss: (0.0353) |  Loss2: (0.0000) | Acc: (98.00%) (35553/35968)
Epoch: 228 | Batch_idx: 290 |  Loss: (0.0354) |  Loss2: (0.0000) | Acc: (98.00%) (36815/37248)
Epoch: 228 | Batch_idx: 300 |  Loss: (0.0353) |  Loss2: (0.0000) | Acc: (98.00%) (38083/38528)
Epoch: 228 | Batch_idx: 310 |  Loss: (0.0355) |  Loss2: (0.0000) | Acc: (98.00%) (39344/39808)
Epoch: 228 | Batch_idx: 320 |  Loss: (0.0359) |  Loss2: (0.0000) | Acc: (98.00%) (40601/41088)
Epoch: 228 | Batch_idx: 330 |  Loss: (0.0357) |  Loss2: (0.0000) | Acc: (98.00%) (41872/42368)
Epoch: 228 | Batch_idx: 340 |  Loss: (0.0359) |  Loss2: (0.0000) | Acc: (98.00%) (43137/43648)
Epoch: 228 | Batch_idx: 350 |  Loss: (0.0361) |  Loss2: (0.0000) | Acc: (98.00%) (44398/44928)
Epoch: 228 | Batch_idx: 360 |  Loss: (0.0364) |  Loss2: (0.0000) | Acc: (98.00%) (45654/46208)
Epoch: 228 | Batch_idx: 370 |  Loss: (0.0363) |  Loss2: (0.0000) | Acc: (98.00%) (46923/47488)
Epoch: 228 | Batch_idx: 380 |  Loss: (0.0363) |  Loss2: (0.0000) | Acc: (98.00%) (48189/48768)
Epoch: 228 | Batch_idx: 390 |  Loss: (0.0364) |  Loss2: (0.0000) | Acc: (98.00%) (49407/50000)
# TEST : Loss: (0.4322) | Acc: (88.00%) (8886/10000)
percent tensor([0.5888, 0.6048, 0.5963, 0.5823, 0.6026, 0.5702, 0.6091, 0.6069, 0.6073,
        0.6016, 0.6007, 0.6015, 0.5969, 0.6067, 0.5914, 0.5833],
       device='cuda:0') torch.Size([16])
percent tensor([0.5630, 0.5527, 0.5298, 0.5488, 0.5494, 0.5319, 0.5609, 0.5789, 0.5513,
        0.5344, 0.5270, 0.5233, 0.5570, 0.5614, 0.5610, 0.5470],
       device='cuda:0') torch.Size([16])
percent tensor([0.5730, 0.5218, 0.5158, 0.5922, 0.5067, 0.6120, 0.5419, 0.5673, 0.5611,
        0.5162, 0.5416, 0.5093, 0.4978, 0.6486, 0.5772, 0.5792],
       device='cuda:0') torch.Size([16])
percent tensor([0.7016, 0.7250, 0.6690, 0.6555, 0.6495, 0.6909, 0.7046, 0.6396, 0.6902,
        0.7251, 0.7165, 0.7061, 0.7517, 0.6905, 0.7058, 0.7099],
       device='cuda:0') torch.Size([16])
percent tensor([0.6974, 0.6120, 0.7238, 0.7312, 0.7510, 0.7911, 0.7056, 0.7064, 0.7749,
        0.7009, 0.7309, 0.7245, 0.6271, 0.7671, 0.6414, 0.7489],
       device='cuda:0') torch.Size([16])
percent tensor([0.8294, 0.8387, 0.8532, 0.8420, 0.8723, 0.8601, 0.8511, 0.8350, 0.8442,
        0.8429, 0.8559, 0.8300, 0.8345, 0.8678, 0.8273, 0.8515],
       device='cuda:0') torch.Size([16])
percent tensor([0.3918, 0.6549, 0.5836, 0.5468, 0.6081, 0.7041, 0.5669, 0.4061, 0.5767,
        0.5563, 0.5778, 0.4933, 0.5906, 0.4930, 0.3954, 0.4036],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9998, 0.9996, 0.9999, 0.9998, 0.9994, 0.9999, 0.9995, 0.9997,
        0.9998, 1.0000, 0.9999, 0.9998, 0.9994, 0.9993, 0.9993],
       device='cuda:0') torch.Size([16])
Epoch: 229 | Batch_idx: 0 |  Loss: (0.0620) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 229 | Batch_idx: 10 |  Loss: (0.0378) |  Loss2: (0.0000) | Acc: (98.00%) (1392/1408)
Epoch: 229 | Batch_idx: 20 |  Loss: (0.0322) |  Loss2: (0.0000) | Acc: (99.00%) (2663/2688)
Epoch: 229 | Batch_idx: 30 |  Loss: (0.0359) |  Loss2: (0.0000) | Acc: (98.00%) (3924/3968)
Epoch: 229 | Batch_idx: 40 |  Loss: (0.0346) |  Loss2: (0.0000) | Acc: (98.00%) (5193/5248)
Epoch: 229 | Batch_idx: 50 |  Loss: (0.0343) |  Loss2: (0.0000) | Acc: (98.00%) (6460/6528)
Epoch: 229 | Batch_idx: 60 |  Loss: (0.0336) |  Loss2: (0.0000) | Acc: (98.00%) (7728/7808)
Epoch: 229 | Batch_idx: 70 |  Loss: (0.0331) |  Loss2: (0.0000) | Acc: (99.00%) (8998/9088)
Epoch: 229 | Batch_idx: 80 |  Loss: (0.0333) |  Loss2: (0.0000) | Acc: (98.00%) (10261/10368)
Epoch: 229 | Batch_idx: 90 |  Loss: (0.0332) |  Loss2: (0.0000) | Acc: (98.00%) (11526/11648)
Epoch: 229 | Batch_idx: 100 |  Loss: (0.0331) |  Loss2: (0.0000) | Acc: (98.00%) (12791/12928)
Epoch: 229 | Batch_idx: 110 |  Loss: (0.0337) |  Loss2: (0.0000) | Acc: (98.00%) (14048/14208)
Epoch: 229 | Batch_idx: 120 |  Loss: (0.0333) |  Loss2: (0.0000) | Acc: (98.00%) (15313/15488)
Epoch: 229 | Batch_idx: 130 |  Loss: (0.0329) |  Loss2: (0.0000) | Acc: (98.00%) (16583/16768)
Epoch: 229 | Batch_idx: 140 |  Loss: (0.0346) |  Loss2: (0.0000) | Acc: (98.00%) (17839/18048)
Epoch: 229 | Batch_idx: 150 |  Loss: (0.0352) |  Loss2: (0.0000) | Acc: (98.00%) (19102/19328)
Epoch: 229 | Batch_idx: 160 |  Loss: (0.0355) |  Loss2: (0.0000) | Acc: (98.00%) (20367/20608)
Epoch: 229 | Batch_idx: 170 |  Loss: (0.0356) |  Loss2: (0.0000) | Acc: (98.00%) (21634/21888)
Epoch: 229 | Batch_idx: 180 |  Loss: (0.0362) |  Loss2: (0.0000) | Acc: (98.00%) (22895/23168)
Epoch: 229 | Batch_idx: 190 |  Loss: (0.0368) |  Loss2: (0.0000) | Acc: (98.00%) (24151/24448)
Epoch: 229 | Batch_idx: 200 |  Loss: (0.0369) |  Loss2: (0.0000) | Acc: (98.00%) (25416/25728)
Epoch: 229 | Batch_idx: 210 |  Loss: (0.0371) |  Loss2: (0.0000) | Acc: (98.00%) (26679/27008)
Epoch: 229 | Batch_idx: 220 |  Loss: (0.0374) |  Loss2: (0.0000) | Acc: (98.00%) (27937/28288)
Epoch: 229 | Batch_idx: 230 |  Loss: (0.0378) |  Loss2: (0.0000) | Acc: (98.00%) (29198/29568)
Epoch: 229 | Batch_idx: 240 |  Loss: (0.0378) |  Loss2: (0.0000) | Acc: (98.00%) (30465/30848)
Epoch: 229 | Batch_idx: 250 |  Loss: (0.0379) |  Loss2: (0.0000) | Acc: (98.00%) (31727/32128)
Epoch: 229 | Batch_idx: 260 |  Loss: (0.0379) |  Loss2: (0.0000) | Acc: (98.00%) (32992/33408)
Epoch: 229 | Batch_idx: 270 |  Loss: (0.0378) |  Loss2: (0.0000) | Acc: (98.00%) (34256/34688)
Epoch: 229 | Batch_idx: 280 |  Loss: (0.0374) |  Loss2: (0.0000) | Acc: (98.00%) (35526/35968)
Epoch: 229 | Batch_idx: 290 |  Loss: (0.0376) |  Loss2: (0.0000) | Acc: (98.00%) (36784/37248)
Epoch: 229 | Batch_idx: 300 |  Loss: (0.0378) |  Loss2: (0.0000) | Acc: (98.00%) (38046/38528)
Epoch: 229 | Batch_idx: 310 |  Loss: (0.0377) |  Loss2: (0.0000) | Acc: (98.00%) (39314/39808)
Epoch: 229 | Batch_idx: 320 |  Loss: (0.0377) |  Loss2: (0.0000) | Acc: (98.00%) (40577/41088)
Epoch: 229 | Batch_idx: 330 |  Loss: (0.0376) |  Loss2: (0.0000) | Acc: (98.00%) (41846/42368)
Epoch: 229 | Batch_idx: 340 |  Loss: (0.0375) |  Loss2: (0.0000) | Acc: (98.00%) (43115/43648)
Epoch: 229 | Batch_idx: 350 |  Loss: (0.0376) |  Loss2: (0.0000) | Acc: (98.00%) (44378/44928)
Epoch: 229 | Batch_idx: 360 |  Loss: (0.0379) |  Loss2: (0.0000) | Acc: (98.00%) (45637/46208)
Epoch: 229 | Batch_idx: 370 |  Loss: (0.0379) |  Loss2: (0.0000) | Acc: (98.00%) (46901/47488)
Epoch: 229 | Batch_idx: 380 |  Loss: (0.0380) |  Loss2: (0.0000) | Acc: (98.00%) (48161/48768)
Epoch: 229 | Batch_idx: 390 |  Loss: (0.0380) |  Loss2: (0.0000) | Acc: (98.00%) (49377/50000)
# TEST : Loss: (0.4578) | Acc: (88.00%) (8847/10000)
percent tensor([0.5906, 0.6076, 0.5969, 0.5837, 0.6039, 0.5709, 0.6112, 0.6087, 0.6102,
        0.6035, 0.6032, 0.6024, 0.5992, 0.6099, 0.5928, 0.5853],
       device='cuda:0') torch.Size([16])
percent tensor([0.5637, 0.5533, 0.5233, 0.5455, 0.5473, 0.5303, 0.5608, 0.5756, 0.5524,
        0.5321, 0.5284, 0.5206, 0.5590, 0.5644, 0.5600, 0.5467],
       device='cuda:0') torch.Size([16])
percent tensor([0.5655, 0.5197, 0.5063, 0.5807, 0.4986, 0.6066, 0.5414, 0.5601, 0.5609,
        0.5131, 0.5391, 0.5023, 0.4916, 0.6545, 0.5713, 0.5719],
       device='cuda:0') torch.Size([16])
percent tensor([0.6950, 0.7228, 0.6680, 0.6495, 0.6423, 0.6808, 0.7046, 0.6352, 0.6840,
        0.7237, 0.7104, 0.7040, 0.7473, 0.6842, 0.6990, 0.7039],
       device='cuda:0') torch.Size([16])
percent tensor([0.7063, 0.6347, 0.7155, 0.7266, 0.7561, 0.7932, 0.7199, 0.7053, 0.7887,
        0.7143, 0.7447, 0.7147, 0.6566, 0.7942, 0.6665, 0.7587],
       device='cuda:0') torch.Size([16])
percent tensor([0.8298, 0.8344, 0.8567, 0.8458, 0.8757, 0.8549, 0.8508, 0.8392, 0.8476,
        0.8410, 0.8525, 0.8271, 0.8313, 0.8697, 0.8238, 0.8501],
       device='cuda:0') torch.Size([16])
percent tensor([0.3873, 0.6314, 0.6099, 0.5394, 0.6391, 0.6654, 0.5338, 0.4142, 0.5908,
        0.5499, 0.5754, 0.4927, 0.5910, 0.5193, 0.3401, 0.3704],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9999, 0.9996, 0.9999, 0.9997, 0.9991, 0.9998, 0.9997, 0.9997,
        0.9998, 1.0000, 0.9999, 0.9997, 0.9995, 0.9993, 0.9994],
       device='cuda:0') torch.Size([16])
Epoch: 230 | Batch_idx: 0 |  Loss: (0.0179) |  Loss2: (0.0000) | Acc: (100.00%) (128/128)
Epoch: 230 | Batch_idx: 10 |  Loss: (0.0466) |  Loss2: (0.0000) | Acc: (98.00%) (1388/1408)
Epoch: 230 | Batch_idx: 20 |  Loss: (0.0441) |  Loss2: (0.0000) | Acc: (98.00%) (2650/2688)
Epoch: 230 | Batch_idx: 30 |  Loss: (0.0412) |  Loss2: (0.0000) | Acc: (98.00%) (3917/3968)
Epoch: 230 | Batch_idx: 40 |  Loss: (0.0415) |  Loss2: (0.0000) | Acc: (98.00%) (5177/5248)
Epoch: 230 | Batch_idx: 50 |  Loss: (0.0402) |  Loss2: (0.0000) | Acc: (98.00%) (6446/6528)
Epoch: 230 | Batch_idx: 60 |  Loss: (0.0404) |  Loss2: (0.0000) | Acc: (98.00%) (7708/7808)
Epoch: 230 | Batch_idx: 70 |  Loss: (0.0398) |  Loss2: (0.0000) | Acc: (98.00%) (8974/9088)
Epoch: 230 | Batch_idx: 80 |  Loss: (0.0403) |  Loss2: (0.0000) | Acc: (98.00%) (10238/10368)
Epoch: 230 | Batch_idx: 90 |  Loss: (0.0403) |  Loss2: (0.0000) | Acc: (98.00%) (11501/11648)
Epoch: 230 | Batch_idx: 100 |  Loss: (0.0396) |  Loss2: (0.0000) | Acc: (98.00%) (12770/12928)
Epoch: 230 | Batch_idx: 110 |  Loss: (0.0391) |  Loss2: (0.0000) | Acc: (98.00%) (14035/14208)
Epoch: 230 | Batch_idx: 120 |  Loss: (0.0383) |  Loss2: (0.0000) | Acc: (98.00%) (15304/15488)
Epoch: 230 | Batch_idx: 130 |  Loss: (0.0395) |  Loss2: (0.0000) | Acc: (98.00%) (16563/16768)
Epoch: 230 | Batch_idx: 140 |  Loss: (0.0396) |  Loss2: (0.0000) | Acc: (98.00%) (17826/18048)
Epoch: 230 | Batch_idx: 150 |  Loss: (0.0390) |  Loss2: (0.0000) | Acc: (98.00%) (19095/19328)
Epoch: 230 | Batch_idx: 160 |  Loss: (0.0389) |  Loss2: (0.0000) | Acc: (98.00%) (20360/20608)
Epoch: 230 | Batch_idx: 170 |  Loss: (0.0392) |  Loss2: (0.0000) | Acc: (98.00%) (21622/21888)
Epoch: 230 | Batch_idx: 180 |  Loss: (0.0393) |  Loss2: (0.0000) | Acc: (98.00%) (22886/23168)
Epoch: 230 | Batch_idx: 190 |  Loss: (0.0392) |  Loss2: (0.0000) | Acc: (98.00%) (24148/24448)
Epoch: 230 | Batch_idx: 200 |  Loss: (0.0391) |  Loss2: (0.0000) | Acc: (98.00%) (25412/25728)
Epoch: 230 | Batch_idx: 210 |  Loss: (0.0389) |  Loss2: (0.0000) | Acc: (98.00%) (26678/27008)
Epoch: 230 | Batch_idx: 220 |  Loss: (0.0390) |  Loss2: (0.0000) | Acc: (98.00%) (27941/28288)
Epoch: 230 | Batch_idx: 230 |  Loss: (0.0390) |  Loss2: (0.0000) | Acc: (98.00%) (29207/29568)
Epoch: 230 | Batch_idx: 240 |  Loss: (0.0390) |  Loss2: (0.0000) | Acc: (98.00%) (30468/30848)
Epoch: 230 | Batch_idx: 250 |  Loss: (0.0391) |  Loss2: (0.0000) | Acc: (98.00%) (31730/32128)
Epoch: 230 | Batch_idx: 260 |  Loss: (0.0387) |  Loss2: (0.0000) | Acc: (98.00%) (32998/33408)
Epoch: 230 | Batch_idx: 270 |  Loss: (0.0389) |  Loss2: (0.0000) | Acc: (98.00%) (34260/34688)
Epoch: 230 | Batch_idx: 280 |  Loss: (0.0391) |  Loss2: (0.0000) | Acc: (98.00%) (35523/35968)
Epoch: 230 | Batch_idx: 290 |  Loss: (0.0389) |  Loss2: (0.0000) | Acc: (98.00%) (36789/37248)
Epoch: 230 | Batch_idx: 300 |  Loss: (0.0386) |  Loss2: (0.0000) | Acc: (98.00%) (38055/38528)
Epoch: 230 | Batch_idx: 310 |  Loss: (0.0388) |  Loss2: (0.0000) | Acc: (98.00%) (39312/39808)
Epoch: 230 | Batch_idx: 320 |  Loss: (0.0388) |  Loss2: (0.0000) | Acc: (98.00%) (40578/41088)
Epoch: 230 | Batch_idx: 330 |  Loss: (0.0387) |  Loss2: (0.0000) | Acc: (98.00%) (41842/42368)
Epoch: 230 | Batch_idx: 340 |  Loss: (0.0389) |  Loss2: (0.0000) | Acc: (98.00%) (43100/43648)
Epoch: 230 | Batch_idx: 350 |  Loss: (0.0389) |  Loss2: (0.0000) | Acc: (98.00%) (44365/44928)
Epoch: 230 | Batch_idx: 360 |  Loss: (0.0387) |  Loss2: (0.0000) | Acc: (98.00%) (45632/46208)
Epoch: 230 | Batch_idx: 370 |  Loss: (0.0386) |  Loss2: (0.0000) | Acc: (98.00%) (46901/47488)
Epoch: 230 | Batch_idx: 380 |  Loss: (0.0384) |  Loss2: (0.0000) | Acc: (98.00%) (48169/48768)
Epoch: 230 | Batch_idx: 390 |  Loss: (0.0382) |  Loss2: (0.0000) | Acc: (98.00%) (49391/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_230.pth.tar'
# TEST : Loss: (0.4466) | Acc: (88.00%) (8899/10000)
percent tensor([0.5885, 0.6073, 0.5927, 0.5831, 0.6002, 0.5702, 0.6092, 0.6074, 0.6077,
        0.6019, 0.6018, 0.5990, 0.5974, 0.6102, 0.5922, 0.5845],
       device='cuda:0') torch.Size([16])
percent tensor([0.5607, 0.5528, 0.5308, 0.5485, 0.5504, 0.5292, 0.5623, 0.5801, 0.5510,
        0.5344, 0.5267, 0.5248, 0.5562, 0.5612, 0.5598, 0.5453],
       device='cuda:0') torch.Size([16])
percent tensor([0.5730, 0.5216, 0.5089, 0.5853, 0.5067, 0.6098, 0.5432, 0.5674, 0.5695,
        0.5143, 0.5468, 0.5050, 0.4955, 0.6574, 0.5761, 0.5783],
       device='cuda:0') torch.Size([16])
percent tensor([0.7040, 0.7287, 0.6765, 0.6583, 0.6522, 0.6908, 0.7107, 0.6418, 0.6926,
        0.7277, 0.7176, 0.7080, 0.7525, 0.6937, 0.7046, 0.7129],
       device='cuda:0') torch.Size([16])
percent tensor([0.6836, 0.6041, 0.7113, 0.7130, 0.7422, 0.7823, 0.6878, 0.6916, 0.7686,
        0.6986, 0.7270, 0.7022, 0.6297, 0.7876, 0.6363, 0.7476],
       device='cuda:0') torch.Size([16])
percent tensor([0.8263, 0.8299, 0.8555, 0.8425, 0.8744, 0.8521, 0.8469, 0.8371, 0.8425,
        0.8383, 0.8491, 0.8218, 0.8338, 0.8602, 0.8205, 0.8518],
       device='cuda:0') torch.Size([16])
percent tensor([0.3824, 0.6057, 0.6151, 0.5496, 0.6365, 0.6523, 0.5530, 0.4108, 0.5474,
        0.5304, 0.5391, 0.4796, 0.5899, 0.4217, 0.3610, 0.3774],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9999, 0.9996, 0.9999, 0.9994, 0.9992, 0.9998, 0.9996, 0.9997,
        0.9998, 1.0000, 0.9998, 0.9998, 0.9996, 0.9995, 0.9994],
       device='cuda:0') torch.Size([16])
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(183.1884, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(830.7102, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(817.6705, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1492.9847, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(473.6676, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2315.4763, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4290.8276, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1333.5472, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6397.6660, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11427.3242, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3742.4670, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(15767.4570, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 231 | Batch_idx: 0 |  Loss: (0.0499) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 231 | Batch_idx: 10 |  Loss: (0.0306) |  Loss2: (0.0000) | Acc: (99.00%) (1395/1408)
Epoch: 231 | Batch_idx: 20 |  Loss: (0.0326) |  Loss2: (0.0000) | Acc: (99.00%) (2662/2688)
Epoch: 231 | Batch_idx: 30 |  Loss: (0.0296) |  Loss2: (0.0000) | Acc: (99.00%) (3934/3968)
Epoch: 231 | Batch_idx: 40 |  Loss: (0.0313) |  Loss2: (0.0000) | Acc: (99.00%) (5196/5248)
Epoch: 231 | Batch_idx: 50 |  Loss: (0.0316) |  Loss2: (0.0000) | Acc: (99.00%) (6464/6528)
Epoch: 231 | Batch_idx: 60 |  Loss: (0.0317) |  Loss2: (0.0000) | Acc: (99.00%) (7731/7808)
Epoch: 231 | Batch_idx: 70 |  Loss: (0.0323) |  Loss2: (0.0000) | Acc: (98.00%) (8996/9088)
Epoch: 231 | Batch_idx: 80 |  Loss: (0.0326) |  Loss2: (0.0000) | Acc: (98.00%) (10262/10368)
Epoch: 231 | Batch_idx: 90 |  Loss: (0.0329) |  Loss2: (0.0000) | Acc: (98.00%) (11530/11648)
Epoch: 231 | Batch_idx: 100 |  Loss: (0.0335) |  Loss2: (0.0000) | Acc: (98.00%) (12792/12928)
Epoch: 231 | Batch_idx: 110 |  Loss: (0.0335) |  Loss2: (0.0000) | Acc: (98.00%) (14057/14208)
Epoch: 231 | Batch_idx: 120 |  Loss: (0.0338) |  Loss2: (0.0000) | Acc: (98.00%) (15323/15488)
Epoch: 231 | Batch_idx: 130 |  Loss: (0.0346) |  Loss2: (0.0000) | Acc: (98.00%) (16582/16768)
Epoch: 231 | Batch_idx: 140 |  Loss: (0.0347) |  Loss2: (0.0000) | Acc: (98.00%) (17846/18048)
Epoch: 231 | Batch_idx: 150 |  Loss: (0.0344) |  Loss2: (0.0000) | Acc: (98.00%) (19117/19328)
Epoch: 231 | Batch_idx: 160 |  Loss: (0.0344) |  Loss2: (0.0000) | Acc: (98.00%) (20386/20608)
Epoch: 231 | Batch_idx: 170 |  Loss: (0.0340) |  Loss2: (0.0000) | Acc: (98.00%) (21656/21888)
Epoch: 231 | Batch_idx: 180 |  Loss: (0.0340) |  Loss2: (0.0000) | Acc: (98.00%) (22925/23168)
Epoch: 231 | Batch_idx: 190 |  Loss: (0.0340) |  Loss2: (0.0000) | Acc: (98.00%) (24193/24448)
Epoch: 231 | Batch_idx: 200 |  Loss: (0.0343) |  Loss2: (0.0000) | Acc: (98.00%) (25460/25728)
Epoch: 231 | Batch_idx: 210 |  Loss: (0.0345) |  Loss2: (0.0000) | Acc: (98.00%) (26725/27008)
Epoch: 231 | Batch_idx: 220 |  Loss: (0.0341) |  Loss2: (0.0000) | Acc: (98.00%) (27998/28288)
Epoch: 231 | Batch_idx: 230 |  Loss: (0.0343) |  Loss2: (0.0000) | Acc: (98.00%) (29264/29568)
Epoch: 231 | Batch_idx: 240 |  Loss: (0.0340) |  Loss2: (0.0000) | Acc: (98.00%) (30533/30848)
Epoch: 231 | Batch_idx: 250 |  Loss: (0.0338) |  Loss2: (0.0000) | Acc: (98.00%) (31803/32128)
Epoch: 231 | Batch_idx: 260 |  Loss: (0.0334) |  Loss2: (0.0000) | Acc: (98.00%) (33073/33408)
Epoch: 231 | Batch_idx: 270 |  Loss: (0.0333) |  Loss2: (0.0000) | Acc: (98.00%) (34340/34688)
Epoch: 231 | Batch_idx: 280 |  Loss: (0.0332) |  Loss2: (0.0000) | Acc: (98.00%) (35605/35968)
Epoch: 231 | Batch_idx: 290 |  Loss: (0.0331) |  Loss2: (0.0000) | Acc: (98.00%) (36873/37248)
Epoch: 231 | Batch_idx: 300 |  Loss: (0.0331) |  Loss2: (0.0000) | Acc: (98.00%) (38138/38528)
Epoch: 231 | Batch_idx: 310 |  Loss: (0.0330) |  Loss2: (0.0000) | Acc: (98.00%) (39409/39808)
Epoch: 231 | Batch_idx: 320 |  Loss: (0.0331) |  Loss2: (0.0000) | Acc: (98.00%) (40675/41088)
Epoch: 231 | Batch_idx: 330 |  Loss: (0.0332) |  Loss2: (0.0000) | Acc: (98.00%) (41939/42368)
Epoch: 231 | Batch_idx: 340 |  Loss: (0.0330) |  Loss2: (0.0000) | Acc: (98.00%) (43210/43648)
Epoch: 231 | Batch_idx: 350 |  Loss: (0.0333) |  Loss2: (0.0000) | Acc: (98.00%) (44473/44928)
Epoch: 231 | Batch_idx: 360 |  Loss: (0.0332) |  Loss2: (0.0000) | Acc: (98.00%) (45741/46208)
Epoch: 231 | Batch_idx: 370 |  Loss: (0.0334) |  Loss2: (0.0000) | Acc: (98.00%) (47007/47488)
Epoch: 231 | Batch_idx: 380 |  Loss: (0.0334) |  Loss2: (0.0000) | Acc: (98.00%) (48272/48768)
Epoch: 231 | Batch_idx: 390 |  Loss: (0.0336) |  Loss2: (0.0000) | Acc: (98.00%) (49488/50000)
# TEST : Loss: (0.4249) | Acc: (89.00%) (8925/10000)
percent tensor([0.5922, 0.6083, 0.6012, 0.5867, 0.6079, 0.5728, 0.6133, 0.6102, 0.6101,
        0.6055, 0.6032, 0.6068, 0.6003, 0.6092, 0.5941, 0.5861],
       device='cuda:0') torch.Size([16])
percent tensor([0.5612, 0.5504, 0.5253, 0.5473, 0.5468, 0.5282, 0.5589, 0.5769, 0.5485,
        0.5326, 0.5256, 0.5209, 0.5553, 0.5610, 0.5588, 0.5443],
       device='cuda:0') torch.Size([16])
percent tensor([0.5787, 0.5280, 0.5130, 0.5904, 0.5070, 0.6184, 0.5483, 0.5735, 0.5674,
        0.5214, 0.5516, 0.5118, 0.4992, 0.6603, 0.5862, 0.5831],
       device='cuda:0') torch.Size([16])
percent tensor([0.7015, 0.7331, 0.6771, 0.6598, 0.6491, 0.6924, 0.7123, 0.6420, 0.6919,
        0.7322, 0.7215, 0.7098, 0.7532, 0.6942, 0.7086, 0.7119],
       device='cuda:0') torch.Size([16])
percent tensor([0.7003, 0.6309, 0.7070, 0.7139, 0.7497, 0.7817, 0.7117, 0.7079, 0.7866,
        0.7184, 0.7574, 0.6968, 0.6473, 0.8024, 0.6542, 0.7597],
       device='cuda:0') torch.Size([16])
percent tensor([0.8297, 0.8344, 0.8514, 0.8415, 0.8731, 0.8492, 0.8529, 0.8383, 0.8496,
        0.8453, 0.8557, 0.8295, 0.8371, 0.8666, 0.8258, 0.8516],
       device='cuda:0') torch.Size([16])
percent tensor([0.3858, 0.6280, 0.5833, 0.5301, 0.6221, 0.6704, 0.5328, 0.3812, 0.5994,
        0.5455, 0.5596, 0.4733, 0.5969, 0.4840, 0.3271, 0.3832],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9999, 0.9997, 0.9999, 0.9996, 0.9994, 0.9998, 0.9995, 0.9998,
        0.9998, 1.0000, 0.9999, 0.9998, 0.9994, 0.9994, 0.9993],
       device='cuda:0') torch.Size([16])
Epoch: 232 | Batch_idx: 0 |  Loss: (0.0167) |  Loss2: (0.0000) | Acc: (100.00%) (128/128)
Epoch: 232 | Batch_idx: 10 |  Loss: (0.0355) |  Loss2: (0.0000) | Acc: (99.00%) (1394/1408)
Epoch: 232 | Batch_idx: 20 |  Loss: (0.0329) |  Loss2: (0.0000) | Acc: (99.00%) (2663/2688)
Epoch: 232 | Batch_idx: 30 |  Loss: (0.0328) |  Loss2: (0.0000) | Acc: (99.00%) (3931/3968)
Epoch: 232 | Batch_idx: 40 |  Loss: (0.0326) |  Loss2: (0.0000) | Acc: (99.00%) (5197/5248)
Epoch: 232 | Batch_idx: 50 |  Loss: (0.0341) |  Loss2: (0.0000) | Acc: (98.00%) (6461/6528)
Epoch: 232 | Batch_idx: 60 |  Loss: (0.0335) |  Loss2: (0.0000) | Acc: (99.00%) (7730/7808)
Epoch: 232 | Batch_idx: 70 |  Loss: (0.0342) |  Loss2: (0.0000) | Acc: (98.00%) (8990/9088)
Epoch: 232 | Batch_idx: 80 |  Loss: (0.0338) |  Loss2: (0.0000) | Acc: (98.00%) (10260/10368)
Epoch: 232 | Batch_idx: 90 |  Loss: (0.0331) |  Loss2: (0.0000) | Acc: (99.00%) (11532/11648)
Epoch: 232 | Batch_idx: 100 |  Loss: (0.0329) |  Loss2: (0.0000) | Acc: (98.00%) (12797/12928)
Epoch: 232 | Batch_idx: 110 |  Loss: (0.0324) |  Loss2: (0.0000) | Acc: (98.00%) (14064/14208)
Epoch: 232 | Batch_idx: 120 |  Loss: (0.0319) |  Loss2: (0.0000) | Acc: (99.00%) (15334/15488)
Epoch: 232 | Batch_idx: 130 |  Loss: (0.0318) |  Loss2: (0.0000) | Acc: (99.00%) (16602/16768)
Epoch: 232 | Batch_idx: 140 |  Loss: (0.0314) |  Loss2: (0.0000) | Acc: (99.00%) (17869/18048)
Epoch: 232 | Batch_idx: 150 |  Loss: (0.0315) |  Loss2: (0.0000) | Acc: (98.00%) (19134/19328)
Epoch: 232 | Batch_idx: 160 |  Loss: (0.0311) |  Loss2: (0.0000) | Acc: (99.00%) (20407/20608)
Epoch: 232 | Batch_idx: 170 |  Loss: (0.0314) |  Loss2: (0.0000) | Acc: (99.00%) (21673/21888)
Epoch: 232 | Batch_idx: 180 |  Loss: (0.0310) |  Loss2: (0.0000) | Acc: (99.00%) (22943/23168)
Epoch: 232 | Batch_idx: 190 |  Loss: (0.0308) |  Loss2: (0.0000) | Acc: (99.00%) (24214/24448)
Epoch: 232 | Batch_idx: 200 |  Loss: (0.0305) |  Loss2: (0.0000) | Acc: (99.00%) (25489/25728)
Epoch: 232 | Batch_idx: 210 |  Loss: (0.0309) |  Loss2: (0.0000) | Acc: (99.00%) (26754/27008)
Epoch: 232 | Batch_idx: 220 |  Loss: (0.0315) |  Loss2: (0.0000) | Acc: (99.00%) (28020/28288)
Epoch: 232 | Batch_idx: 230 |  Loss: (0.0320) |  Loss2: (0.0000) | Acc: (99.00%) (29284/29568)
Epoch: 232 | Batch_idx: 240 |  Loss: (0.0322) |  Loss2: (0.0000) | Acc: (99.00%) (30543/30848)
Epoch: 232 | Batch_idx: 250 |  Loss: (0.0326) |  Loss2: (0.0000) | Acc: (99.00%) (31807/32128)
Epoch: 232 | Batch_idx: 260 |  Loss: (0.0326) |  Loss2: (0.0000) | Acc: (99.00%) (33077/33408)
Epoch: 232 | Batch_idx: 270 |  Loss: (0.0331) |  Loss2: (0.0000) | Acc: (98.00%) (34336/34688)
Epoch: 232 | Batch_idx: 280 |  Loss: (0.0332) |  Loss2: (0.0000) | Acc: (98.00%) (35600/35968)
Epoch: 232 | Batch_idx: 290 |  Loss: (0.0330) |  Loss2: (0.0000) | Acc: (98.00%) (36873/37248)
Epoch: 232 | Batch_idx: 300 |  Loss: (0.0331) |  Loss2: (0.0000) | Acc: (98.00%) (38138/38528)
Epoch: 232 | Batch_idx: 310 |  Loss: (0.0331) |  Loss2: (0.0000) | Acc: (98.00%) (39404/39808)
Epoch: 232 | Batch_idx: 320 |  Loss: (0.0333) |  Loss2: (0.0000) | Acc: (98.00%) (40668/41088)
Epoch: 232 | Batch_idx: 330 |  Loss: (0.0335) |  Loss2: (0.0000) | Acc: (98.00%) (41932/42368)
Epoch: 232 | Batch_idx: 340 |  Loss: (0.0336) |  Loss2: (0.0000) | Acc: (98.00%) (43198/43648)
Epoch: 232 | Batch_idx: 350 |  Loss: (0.0336) |  Loss2: (0.0000) | Acc: (98.00%) (44465/44928)
Epoch: 232 | Batch_idx: 360 |  Loss: (0.0341) |  Loss2: (0.0000) | Acc: (98.00%) (45722/46208)
Epoch: 232 | Batch_idx: 370 |  Loss: (0.0341) |  Loss2: (0.0000) | Acc: (98.00%) (46988/47488)
Epoch: 232 | Batch_idx: 380 |  Loss: (0.0341) |  Loss2: (0.0000) | Acc: (98.00%) (48255/48768)
Epoch: 232 | Batch_idx: 390 |  Loss: (0.0342) |  Loss2: (0.0000) | Acc: (98.00%) (49472/50000)
# TEST : Loss: (0.4795) | Acc: (88.00%) (8810/10000)
percent tensor([0.5931, 0.6093, 0.6006, 0.5874, 0.6079, 0.5729, 0.6138, 0.6118, 0.6114,
        0.6065, 0.6045, 0.6068, 0.6015, 0.6107, 0.5949, 0.5877],
       device='cuda:0') torch.Size([16])
percent tensor([0.5607, 0.5527, 0.5316, 0.5503, 0.5500, 0.5291, 0.5609, 0.5819, 0.5517,
        0.5350, 0.5263, 0.5261, 0.5572, 0.5607, 0.5597, 0.5458],
       device='cuda:0') torch.Size([16])
percent tensor([0.5646, 0.5225, 0.5043, 0.5825, 0.5005, 0.6132, 0.5396, 0.5665, 0.5548,
        0.5062, 0.5373, 0.4985, 0.4840, 0.6545, 0.5751, 0.5755],
       device='cuda:0') torch.Size([16])
percent tensor([0.7058, 0.7326, 0.6750, 0.6596, 0.6505, 0.6930, 0.7129, 0.6432, 0.6976,
        0.7341, 0.7238, 0.7101, 0.7563, 0.6978, 0.7070, 0.7136],
       device='cuda:0') torch.Size([16])
percent tensor([0.7062, 0.6228, 0.7192, 0.7248, 0.7502, 0.7876, 0.6987, 0.7040, 0.7723,
        0.7055, 0.7403, 0.7051, 0.6435, 0.7764, 0.6540, 0.7624],
       device='cuda:0') torch.Size([16])
percent tensor([0.8321, 0.8348, 0.8554, 0.8419, 0.8719, 0.8482, 0.8525, 0.8440, 0.8441,
        0.8406, 0.8536, 0.8329, 0.8370, 0.8615, 0.8282, 0.8511],
       device='cuda:0') torch.Size([16])
percent tensor([0.4081, 0.6210, 0.6060, 0.5421, 0.6055, 0.6993, 0.5378, 0.3863, 0.6040,
        0.5298, 0.5576, 0.4946, 0.6205, 0.4612, 0.3471, 0.3859],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9999, 0.9997, 0.9999, 0.9995, 0.9996, 0.9999, 0.9996, 0.9999,
        0.9998, 1.0000, 0.9999, 0.9999, 0.9994, 0.9993, 0.9993],
       device='cuda:0') torch.Size([16])
Epoch: 233 | Batch_idx: 0 |  Loss: (0.0252) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 233 | Batch_idx: 10 |  Loss: (0.0399) |  Loss2: (0.0000) | Acc: (98.00%) (1392/1408)
Epoch: 233 | Batch_idx: 20 |  Loss: (0.0312) |  Loss2: (0.0000) | Acc: (99.00%) (2667/2688)
Epoch: 233 | Batch_idx: 30 |  Loss: (0.0331) |  Loss2: (0.0000) | Acc: (99.00%) (3931/3968)
Epoch: 233 | Batch_idx: 40 |  Loss: (0.0333) |  Loss2: (0.0000) | Acc: (99.00%) (5200/5248)
Epoch: 233 | Batch_idx: 50 |  Loss: (0.0333) |  Loss2: (0.0000) | Acc: (99.00%) (6469/6528)
Epoch: 233 | Batch_idx: 60 |  Loss: (0.0332) |  Loss2: (0.0000) | Acc: (99.00%) (7733/7808)
Epoch: 233 | Batch_idx: 70 |  Loss: (0.0336) |  Loss2: (0.0000) | Acc: (98.00%) (8996/9088)
Epoch: 233 | Batch_idx: 80 |  Loss: (0.0334) |  Loss2: (0.0000) | Acc: (98.00%) (10261/10368)
Epoch: 233 | Batch_idx: 90 |  Loss: (0.0338) |  Loss2: (0.0000) | Acc: (98.00%) (11523/11648)
Epoch: 233 | Batch_idx: 100 |  Loss: (0.0327) |  Loss2: (0.0000) | Acc: (98.00%) (12796/12928)
Epoch: 233 | Batch_idx: 110 |  Loss: (0.0325) |  Loss2: (0.0000) | Acc: (98.00%) (14063/14208)
Epoch: 233 | Batch_idx: 120 |  Loss: (0.0327) |  Loss2: (0.0000) | Acc: (98.00%) (15331/15488)
Epoch: 233 | Batch_idx: 130 |  Loss: (0.0329) |  Loss2: (0.0000) | Acc: (98.00%) (16596/16768)
Epoch: 233 | Batch_idx: 140 |  Loss: (0.0328) |  Loss2: (0.0000) | Acc: (98.00%) (17863/18048)
Epoch: 233 | Batch_idx: 150 |  Loss: (0.0332) |  Loss2: (0.0000) | Acc: (98.00%) (19125/19328)
Epoch: 233 | Batch_idx: 160 |  Loss: (0.0330) |  Loss2: (0.0000) | Acc: (98.00%) (20395/20608)
Epoch: 233 | Batch_idx: 170 |  Loss: (0.0327) |  Loss2: (0.0000) | Acc: (98.00%) (21666/21888)
Epoch: 233 | Batch_idx: 180 |  Loss: (0.0326) |  Loss2: (0.0000) | Acc: (99.00%) (22937/23168)
Epoch: 233 | Batch_idx: 190 |  Loss: (0.0329) |  Loss2: (0.0000) | Acc: (98.00%) (24202/24448)
Epoch: 233 | Batch_idx: 200 |  Loss: (0.0329) |  Loss2: (0.0000) | Acc: (98.00%) (25470/25728)
Epoch: 233 | Batch_idx: 210 |  Loss: (0.0333) |  Loss2: (0.0000) | Acc: (98.00%) (26733/27008)
Epoch: 233 | Batch_idx: 220 |  Loss: (0.0334) |  Loss2: (0.0000) | Acc: (98.00%) (27995/28288)
Epoch: 233 | Batch_idx: 230 |  Loss: (0.0337) |  Loss2: (0.0000) | Acc: (98.00%) (29260/29568)
Epoch: 233 | Batch_idx: 240 |  Loss: (0.0335) |  Loss2: (0.0000) | Acc: (98.00%) (30528/30848)
Epoch: 233 | Batch_idx: 250 |  Loss: (0.0334) |  Loss2: (0.0000) | Acc: (98.00%) (31793/32128)
Epoch: 233 | Batch_idx: 260 |  Loss: (0.0335) |  Loss2: (0.0000) | Acc: (98.00%) (33060/33408)
Epoch: 233 | Batch_idx: 270 |  Loss: (0.0335) |  Loss2: (0.0000) | Acc: (98.00%) (34320/34688)
Epoch: 233 | Batch_idx: 280 |  Loss: (0.0336) |  Loss2: (0.0000) | Acc: (98.00%) (35586/35968)
Epoch: 233 | Batch_idx: 290 |  Loss: (0.0337) |  Loss2: (0.0000) | Acc: (98.00%) (36853/37248)
Epoch: 233 | Batch_idx: 300 |  Loss: (0.0337) |  Loss2: (0.0000) | Acc: (98.00%) (38117/38528)
Epoch: 233 | Batch_idx: 310 |  Loss: (0.0337) |  Loss2: (0.0000) | Acc: (98.00%) (39383/39808)
Epoch: 233 | Batch_idx: 320 |  Loss: (0.0340) |  Loss2: (0.0000) | Acc: (98.00%) (40641/41088)
Epoch: 233 | Batch_idx: 330 |  Loss: (0.0340) |  Loss2: (0.0000) | Acc: (98.00%) (41908/42368)
Epoch: 233 | Batch_idx: 340 |  Loss: (0.0341) |  Loss2: (0.0000) | Acc: (98.00%) (43174/43648)
Epoch: 233 | Batch_idx: 350 |  Loss: (0.0340) |  Loss2: (0.0000) | Acc: (98.00%) (44440/44928)
Epoch: 233 | Batch_idx: 360 |  Loss: (0.0340) |  Loss2: (0.0000) | Acc: (98.00%) (45706/46208)
Epoch: 233 | Batch_idx: 370 |  Loss: (0.0341) |  Loss2: (0.0000) | Acc: (98.00%) (46970/47488)
Epoch: 233 | Batch_idx: 380 |  Loss: (0.0341) |  Loss2: (0.0000) | Acc: (98.00%) (48237/48768)
Epoch: 233 | Batch_idx: 390 |  Loss: (0.0343) |  Loss2: (0.0000) | Acc: (98.00%) (49454/50000)
# TEST : Loss: (0.4532) | Acc: (88.00%) (8848/10000)
percent tensor([0.5901, 0.6076, 0.5953, 0.5843, 0.6027, 0.5713, 0.6106, 0.6084, 0.6092,
        0.6032, 0.6026, 0.6017, 0.5988, 0.6101, 0.5933, 0.5856],
       device='cuda:0') torch.Size([16])
percent tensor([0.5613, 0.5507, 0.5302, 0.5496, 0.5498, 0.5321, 0.5607, 0.5796, 0.5508,
        0.5331, 0.5270, 0.5239, 0.5562, 0.5588, 0.5594, 0.5471],
       device='cuda:0') torch.Size([16])
percent tensor([0.5670, 0.5293, 0.5162, 0.5979, 0.5085, 0.6099, 0.5424, 0.5723, 0.5582,
        0.5166, 0.5423, 0.5090, 0.4913, 0.6541, 0.5753, 0.5787],
       device='cuda:0') torch.Size([16])
percent tensor([0.7080, 0.7314, 0.6712, 0.6571, 0.6482, 0.6997, 0.7090, 0.6398, 0.6961,
        0.7341, 0.7240, 0.7089, 0.7554, 0.6995, 0.7077, 0.7158],
       device='cuda:0') torch.Size([16])
percent tensor([0.7165, 0.6249, 0.7193, 0.7368, 0.7477, 0.7825, 0.7160, 0.7105, 0.7713,
        0.7150, 0.7416, 0.7164, 0.6417, 0.7913, 0.6525, 0.7583],
       device='cuda:0') torch.Size([16])
percent tensor([0.8316, 0.8302, 0.8519, 0.8421, 0.8701, 0.8461, 0.8532, 0.8377, 0.8461,
        0.8414, 0.8550, 0.8277, 0.8339, 0.8645, 0.8216, 0.8514],
       device='cuda:0') torch.Size([16])
percent tensor([0.3959, 0.6230, 0.5874, 0.5678, 0.5980, 0.6628, 0.5650, 0.3914, 0.5985,
        0.5199, 0.5491, 0.5204, 0.6136, 0.4761, 0.3435, 0.3910],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9999, 0.9997, 0.9999, 0.9997, 0.9994, 0.9999, 0.9996, 0.9999,
        0.9998, 1.0000, 0.9999, 0.9999, 0.9995, 0.9993, 0.9993],
       device='cuda:0') torch.Size([16])
Epoch: 234 | Batch_idx: 0 |  Loss: (0.0262) |  Loss2: (0.0000) | Acc: (100.00%) (128/128)
Epoch: 234 | Batch_idx: 10 |  Loss: (0.0479) |  Loss2: (0.0000) | Acc: (98.00%) (1387/1408)
Epoch: 234 | Batch_idx: 20 |  Loss: (0.0440) |  Loss2: (0.0000) | Acc: (98.00%) (2653/2688)
Epoch: 234 | Batch_idx: 30 |  Loss: (0.0402) |  Loss2: (0.0000) | Acc: (98.00%) (3922/3968)
Epoch: 234 | Batch_idx: 40 |  Loss: (0.0393) |  Loss2: (0.0000) | Acc: (98.00%) (5186/5248)
Epoch: 234 | Batch_idx: 50 |  Loss: (0.0393) |  Loss2: (0.0000) | Acc: (98.00%) (6451/6528)
Epoch: 234 | Batch_idx: 60 |  Loss: (0.0384) |  Loss2: (0.0000) | Acc: (98.00%) (7719/7808)
Epoch: 234 | Batch_idx: 70 |  Loss: (0.0377) |  Loss2: (0.0000) | Acc: (98.00%) (8984/9088)
Epoch: 234 | Batch_idx: 80 |  Loss: (0.0368) |  Loss2: (0.0000) | Acc: (98.00%) (10253/10368)
Epoch: 234 | Batch_idx: 90 |  Loss: (0.0361) |  Loss2: (0.0000) | Acc: (98.00%) (11520/11648)
Epoch: 234 | Batch_idx: 100 |  Loss: (0.0360) |  Loss2: (0.0000) | Acc: (98.00%) (12786/12928)
Epoch: 234 | Batch_idx: 110 |  Loss: (0.0348) |  Loss2: (0.0000) | Acc: (98.00%) (14061/14208)
Epoch: 234 | Batch_idx: 120 |  Loss: (0.0353) |  Loss2: (0.0000) | Acc: (98.00%) (15323/15488)
Epoch: 234 | Batch_idx: 130 |  Loss: (0.0349) |  Loss2: (0.0000) | Acc: (98.00%) (16594/16768)
Epoch: 234 | Batch_idx: 140 |  Loss: (0.0343) |  Loss2: (0.0000) | Acc: (98.00%) (17863/18048)
Epoch: 234 | Batch_idx: 150 |  Loss: (0.0336) |  Loss2: (0.0000) | Acc: (98.00%) (19134/19328)
Epoch: 234 | Batch_idx: 160 |  Loss: (0.0336) |  Loss2: (0.0000) | Acc: (99.00%) (20402/20608)
Epoch: 234 | Batch_idx: 170 |  Loss: (0.0335) |  Loss2: (0.0000) | Acc: (99.00%) (21672/21888)
Epoch: 234 | Batch_idx: 180 |  Loss: (0.0336) |  Loss2: (0.0000) | Acc: (98.00%) (22936/23168)
Epoch: 234 | Batch_idx: 190 |  Loss: (0.0332) |  Loss2: (0.0000) | Acc: (99.00%) (24206/24448)
Epoch: 234 | Batch_idx: 200 |  Loss: (0.0329) |  Loss2: (0.0000) | Acc: (99.00%) (25473/25728)
Epoch: 234 | Batch_idx: 210 |  Loss: (0.0328) |  Loss2: (0.0000) | Acc: (99.00%) (26741/27008)
Epoch: 234 | Batch_idx: 220 |  Loss: (0.0325) |  Loss2: (0.0000) | Acc: (99.00%) (28012/28288)
Epoch: 234 | Batch_idx: 230 |  Loss: (0.0327) |  Loss2: (0.0000) | Acc: (99.00%) (29280/29568)
Epoch: 234 | Batch_idx: 240 |  Loss: (0.0327) |  Loss2: (0.0000) | Acc: (99.00%) (30543/30848)
Epoch: 234 | Batch_idx: 250 |  Loss: (0.0323) |  Loss2: (0.0000) | Acc: (99.00%) (31817/32128)
Epoch: 234 | Batch_idx: 260 |  Loss: (0.0322) |  Loss2: (0.0000) | Acc: (99.00%) (33089/33408)
Epoch: 234 | Batch_idx: 270 |  Loss: (0.0320) |  Loss2: (0.0000) | Acc: (99.00%) (34360/34688)
Epoch: 234 | Batch_idx: 280 |  Loss: (0.0319) |  Loss2: (0.0000) | Acc: (99.00%) (35631/35968)
Epoch: 234 | Batch_idx: 290 |  Loss: (0.0320) |  Loss2: (0.0000) | Acc: (99.00%) (36896/37248)
Epoch: 234 | Batch_idx: 300 |  Loss: (0.0321) |  Loss2: (0.0000) | Acc: (99.00%) (38160/38528)
Epoch: 234 | Batch_idx: 310 |  Loss: (0.0322) |  Loss2: (0.0000) | Acc: (99.00%) (39429/39808)
Epoch: 234 | Batch_idx: 320 |  Loss: (0.0325) |  Loss2: (0.0000) | Acc: (99.00%) (40695/41088)
Epoch: 234 | Batch_idx: 330 |  Loss: (0.0326) |  Loss2: (0.0000) | Acc: (99.00%) (41960/42368)
Epoch: 234 | Batch_idx: 340 |  Loss: (0.0323) |  Loss2: (0.0000) | Acc: (99.00%) (43232/43648)
Epoch: 234 | Batch_idx: 350 |  Loss: (0.0322) |  Loss2: (0.0000) | Acc: (99.00%) (44498/44928)
Epoch: 234 | Batch_idx: 360 |  Loss: (0.0325) |  Loss2: (0.0000) | Acc: (99.00%) (45759/46208)
Epoch: 234 | Batch_idx: 370 |  Loss: (0.0326) |  Loss2: (0.0000) | Acc: (99.00%) (47024/47488)
Epoch: 234 | Batch_idx: 380 |  Loss: (0.0327) |  Loss2: (0.0000) | Acc: (99.00%) (48289/48768)
Epoch: 234 | Batch_idx: 390 |  Loss: (0.0331) |  Loss2: (0.0000) | Acc: (99.00%) (49504/50000)
# TEST : Loss: (0.4267) | Acc: (89.00%) (8929/10000)
percent tensor([0.5920, 0.6082, 0.5996, 0.5862, 0.6061, 0.5730, 0.6123, 0.6112, 0.6108,
        0.6050, 0.6044, 0.6050, 0.6007, 0.6087, 0.5946, 0.5869],
       device='cuda:0') torch.Size([16])
percent tensor([0.5612, 0.5472, 0.5241, 0.5446, 0.5461, 0.5308, 0.5573, 0.5765, 0.5478,
        0.5291, 0.5247, 0.5178, 0.5544, 0.5573, 0.5578, 0.5458],
       device='cuda:0') torch.Size([16])
percent tensor([0.5631, 0.5222, 0.5038, 0.5747, 0.5009, 0.6058, 0.5378, 0.5525, 0.5539,
        0.5082, 0.5401, 0.4956, 0.4839, 0.6501, 0.5690, 0.5680],
       device='cuda:0') torch.Size([16])
percent tensor([0.7028, 0.7287, 0.6715, 0.6571, 0.6465, 0.6858, 0.7083, 0.6400, 0.6941,
        0.7313, 0.7209, 0.7071, 0.7537, 0.6963, 0.7026, 0.7115],
       device='cuda:0') torch.Size([16])
percent tensor([0.7095, 0.6236, 0.7237, 0.7416, 0.7572, 0.7822, 0.7220, 0.7308, 0.7788,
        0.7071, 0.7304, 0.7138, 0.6435, 0.7806, 0.6597, 0.7587],
       device='cuda:0') torch.Size([16])
percent tensor([0.8279, 0.8308, 0.8518, 0.8414, 0.8740, 0.8490, 0.8533, 0.8428, 0.8453,
        0.8389, 0.8486, 0.8285, 0.8365, 0.8586, 0.8240, 0.8520],
       device='cuda:0') torch.Size([16])
percent tensor([0.4204, 0.6234, 0.6197, 0.5696, 0.6370, 0.6982, 0.5457, 0.4303, 0.6062,
        0.5191, 0.5528, 0.5063, 0.6048, 0.4447, 0.3798, 0.4067],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9999, 0.9997, 0.9999, 0.9997, 0.9994, 0.9998, 0.9996, 0.9998,
        0.9998, 1.0000, 0.9999, 0.9998, 0.9993, 0.9994, 0.9994],
       device='cuda:0') torch.Size([16])
Epoch: 235 | Batch_idx: 0 |  Loss: (0.0184) |  Loss2: (0.0000) | Acc: (100.00%) (128/128)
Epoch: 235 | Batch_idx: 10 |  Loss: (0.0303) |  Loss2: (0.0000) | Acc: (99.00%) (1397/1408)
Epoch: 235 | Batch_idx: 20 |  Loss: (0.0365) |  Loss2: (0.0000) | Acc: (99.00%) (2662/2688)
Epoch: 235 | Batch_idx: 30 |  Loss: (0.0399) |  Loss2: (0.0000) | Acc: (98.00%) (3924/3968)
Epoch: 235 | Batch_idx: 40 |  Loss: (0.0376) |  Loss2: (0.0000) | Acc: (98.00%) (5192/5248)
Epoch: 235 | Batch_idx: 50 |  Loss: (0.0365) |  Loss2: (0.0000) | Acc: (98.00%) (6456/6528)
Epoch: 235 | Batch_idx: 60 |  Loss: (0.0362) |  Loss2: (0.0000) | Acc: (98.00%) (7719/7808)
Epoch: 235 | Batch_idx: 70 |  Loss: (0.0363) |  Loss2: (0.0000) | Acc: (98.00%) (8984/9088)
Epoch: 235 | Batch_idx: 80 |  Loss: (0.0356) |  Loss2: (0.0000) | Acc: (98.00%) (10251/10368)
Epoch: 235 | Batch_idx: 90 |  Loss: (0.0358) |  Loss2: (0.0000) | Acc: (98.00%) (11511/11648)
Epoch: 235 | Batch_idx: 100 |  Loss: (0.0358) |  Loss2: (0.0000) | Acc: (98.00%) (12777/12928)
Epoch: 235 | Batch_idx: 110 |  Loss: (0.0359) |  Loss2: (0.0000) | Acc: (98.00%) (14045/14208)
Epoch: 235 | Batch_idx: 120 |  Loss: (0.0353) |  Loss2: (0.0000) | Acc: (98.00%) (15311/15488)
Epoch: 235 | Batch_idx: 130 |  Loss: (0.0357) |  Loss2: (0.0000) | Acc: (98.00%) (16575/16768)
Epoch: 235 | Batch_idx: 140 |  Loss: (0.0354) |  Loss2: (0.0000) | Acc: (98.00%) (17843/18048)
Epoch: 235 | Batch_idx: 150 |  Loss: (0.0353) |  Loss2: (0.0000) | Acc: (98.00%) (19106/19328)
Epoch: 235 | Batch_idx: 160 |  Loss: (0.0357) |  Loss2: (0.0000) | Acc: (98.00%) (20366/20608)
Epoch: 235 | Batch_idx: 170 |  Loss: (0.0356) |  Loss2: (0.0000) | Acc: (98.00%) (21633/21888)
Epoch: 235 | Batch_idx: 180 |  Loss: (0.0357) |  Loss2: (0.0000) | Acc: (98.00%) (22893/23168)
Epoch: 235 | Batch_idx: 190 |  Loss: (0.0353) |  Loss2: (0.0000) | Acc: (98.00%) (24164/24448)
Epoch: 235 | Batch_idx: 200 |  Loss: (0.0357) |  Loss2: (0.0000) | Acc: (98.00%) (25424/25728)
Epoch: 235 | Batch_idx: 210 |  Loss: (0.0361) |  Loss2: (0.0000) | Acc: (98.00%) (26682/27008)
Epoch: 235 | Batch_idx: 220 |  Loss: (0.0359) |  Loss2: (0.0000) | Acc: (98.00%) (27950/28288)
Epoch: 235 | Batch_idx: 230 |  Loss: (0.0358) |  Loss2: (0.0000) | Acc: (98.00%) (29217/29568)
Epoch: 235 | Batch_idx: 240 |  Loss: (0.0356) |  Loss2: (0.0000) | Acc: (98.00%) (30484/30848)
Epoch: 235 | Batch_idx: 250 |  Loss: (0.0354) |  Loss2: (0.0000) | Acc: (98.00%) (31754/32128)
Epoch: 235 | Batch_idx: 260 |  Loss: (0.0349) |  Loss2: (0.0000) | Acc: (98.00%) (33025/33408)
Epoch: 235 | Batch_idx: 270 |  Loss: (0.0355) |  Loss2: (0.0000) | Acc: (98.00%) (34282/34688)
Epoch: 235 | Batch_idx: 280 |  Loss: (0.0355) |  Loss2: (0.0000) | Acc: (98.00%) (35546/35968)
Epoch: 235 | Batch_idx: 290 |  Loss: (0.0355) |  Loss2: (0.0000) | Acc: (98.00%) (36811/37248)
Epoch: 235 | Batch_idx: 300 |  Loss: (0.0355) |  Loss2: (0.0000) | Acc: (98.00%) (38075/38528)
Epoch: 235 | Batch_idx: 310 |  Loss: (0.0353) |  Loss2: (0.0000) | Acc: (98.00%) (39342/39808)
Epoch: 235 | Batch_idx: 320 |  Loss: (0.0350) |  Loss2: (0.0000) | Acc: (98.00%) (40614/41088)
Epoch: 235 | Batch_idx: 330 |  Loss: (0.0352) |  Loss2: (0.0000) | Acc: (98.00%) (41879/42368)
Epoch: 235 | Batch_idx: 340 |  Loss: (0.0351) |  Loss2: (0.0000) | Acc: (98.00%) (43146/43648)
Epoch: 235 | Batch_idx: 350 |  Loss: (0.0348) |  Loss2: (0.0000) | Acc: (98.00%) (44417/44928)
Epoch: 235 | Batch_idx: 360 |  Loss: (0.0348) |  Loss2: (0.0000) | Acc: (98.00%) (45686/46208)
Epoch: 235 | Batch_idx: 370 |  Loss: (0.0347) |  Loss2: (0.0000) | Acc: (98.00%) (46953/47488)
Epoch: 235 | Batch_idx: 380 |  Loss: (0.0350) |  Loss2: (0.0000) | Acc: (98.00%) (48215/48768)
Epoch: 235 | Batch_idx: 390 |  Loss: (0.0350) |  Loss2: (0.0000) | Acc: (98.00%) (49432/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_235.pth.tar'
# TEST : Loss: (0.4567) | Acc: (88.00%) (8879/10000)
percent tensor([0.5924, 0.6071, 0.6012, 0.5871, 0.6097, 0.5732, 0.6125, 0.6114, 0.6117,
        0.6054, 0.6038, 0.6069, 0.6011, 0.6063, 0.5943, 0.5869],
       device='cuda:0') torch.Size([16])
percent tensor([0.5558, 0.5485, 0.5234, 0.5441, 0.5441, 0.5259, 0.5571, 0.5763, 0.5457,
        0.5277, 0.5222, 0.5154, 0.5510, 0.5603, 0.5551, 0.5427],
       device='cuda:0') torch.Size([16])
percent tensor([0.5667, 0.5274, 0.5020, 0.5821, 0.4997, 0.6097, 0.5457, 0.5617, 0.5570,
        0.5121, 0.5443, 0.4971, 0.4850, 0.6630, 0.5729, 0.5746],
       device='cuda:0') torch.Size([16])
percent tensor([0.7073, 0.7351, 0.6789, 0.6631, 0.6537, 0.6931, 0.7129, 0.6439, 0.6972,
        0.7365, 0.7263, 0.7149, 0.7568, 0.7007, 0.7107, 0.7161],
       device='cuda:0') torch.Size([16])
percent tensor([0.7064, 0.6326, 0.7117, 0.7333, 0.7431, 0.7778, 0.7151, 0.7048, 0.7747,
        0.7175, 0.7426, 0.7227, 0.6565, 0.7754, 0.6652, 0.7567],
       device='cuda:0') torch.Size([16])
percent tensor([0.8303, 0.8321, 0.8595, 0.8462, 0.8760, 0.8509, 0.8571, 0.8446, 0.8496,
        0.8412, 0.8550, 0.8339, 0.8332, 0.8654, 0.8275, 0.8536],
       device='cuda:0') torch.Size([16])
percent tensor([0.4159, 0.6254, 0.6260, 0.5746, 0.6542, 0.7011, 0.5743, 0.4332, 0.6223,
        0.5594, 0.5743, 0.5109, 0.5875, 0.4887, 0.3707, 0.4252],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9999, 0.9997, 0.9999, 0.9998, 0.9993, 0.9998, 0.9997, 0.9999,
        0.9997, 1.0000, 0.9998, 0.9998, 0.9995, 0.9994, 0.9993],
       device='cuda:0') torch.Size([16])
Epoch: 236 | Batch_idx: 0 |  Loss: (0.0270) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 236 | Batch_idx: 10 |  Loss: (0.0280) |  Loss2: (0.0000) | Acc: (99.00%) (1395/1408)
Epoch: 236 | Batch_idx: 20 |  Loss: (0.0313) |  Loss2: (0.0000) | Acc: (98.00%) (2658/2688)
Epoch: 236 | Batch_idx: 30 |  Loss: (0.0310) |  Loss2: (0.0000) | Acc: (98.00%) (3928/3968)
Epoch: 236 | Batch_idx: 40 |  Loss: (0.0321) |  Loss2: (0.0000) | Acc: (98.00%) (5193/5248)
Epoch: 236 | Batch_idx: 50 |  Loss: (0.0303) |  Loss2: (0.0000) | Acc: (99.00%) (6467/6528)
Epoch: 236 | Batch_idx: 60 |  Loss: (0.0300) |  Loss2: (0.0000) | Acc: (99.00%) (7735/7808)
Epoch: 236 | Batch_idx: 70 |  Loss: (0.0306) |  Loss2: (0.0000) | Acc: (99.00%) (9000/9088)
Epoch: 236 | Batch_idx: 80 |  Loss: (0.0310) |  Loss2: (0.0000) | Acc: (99.00%) (10266/10368)
Epoch: 236 | Batch_idx: 90 |  Loss: (0.0304) |  Loss2: (0.0000) | Acc: (99.00%) (11538/11648)
Epoch: 236 | Batch_idx: 100 |  Loss: (0.0295) |  Loss2: (0.0000) | Acc: (99.00%) (12810/12928)
Epoch: 236 | Batch_idx: 110 |  Loss: (0.0291) |  Loss2: (0.0000) | Acc: (99.00%) (14081/14208)
Epoch: 236 | Batch_idx: 120 |  Loss: (0.0294) |  Loss2: (0.0000) | Acc: (99.00%) (15346/15488)
Epoch: 236 | Batch_idx: 130 |  Loss: (0.0292) |  Loss2: (0.0000) | Acc: (99.00%) (16616/16768)
Epoch: 236 | Batch_idx: 140 |  Loss: (0.0295) |  Loss2: (0.0000) | Acc: (99.00%) (17885/18048)
Epoch: 236 | Batch_idx: 150 |  Loss: (0.0294) |  Loss2: (0.0000) | Acc: (99.00%) (19154/19328)
Epoch: 236 | Batch_idx: 160 |  Loss: (0.0296) |  Loss2: (0.0000) | Acc: (99.00%) (20422/20608)
Epoch: 236 | Batch_idx: 170 |  Loss: (0.0308) |  Loss2: (0.0000) | Acc: (99.00%) (21683/21888)
Epoch: 236 | Batch_idx: 180 |  Loss: (0.0312) |  Loss2: (0.0000) | Acc: (99.00%) (22948/23168)
Epoch: 236 | Batch_idx: 190 |  Loss: (0.0316) |  Loss2: (0.0000) | Acc: (99.00%) (24205/24448)
Epoch: 236 | Batch_idx: 200 |  Loss: (0.0318) |  Loss2: (0.0000) | Acc: (98.00%) (25469/25728)
Epoch: 236 | Batch_idx: 210 |  Loss: (0.0315) |  Loss2: (0.0000) | Acc: (99.00%) (26744/27008)
Epoch: 236 | Batch_idx: 220 |  Loss: (0.0315) |  Loss2: (0.0000) | Acc: (99.00%) (28009/28288)
Epoch: 236 | Batch_idx: 230 |  Loss: (0.0310) |  Loss2: (0.0000) | Acc: (99.00%) (29285/29568)
Epoch: 236 | Batch_idx: 240 |  Loss: (0.0308) |  Loss2: (0.0000) | Acc: (99.00%) (30553/30848)
Epoch: 236 | Batch_idx: 250 |  Loss: (0.0309) |  Loss2: (0.0000) | Acc: (99.00%) (31820/32128)
Epoch: 236 | Batch_idx: 260 |  Loss: (0.0307) |  Loss2: (0.0000) | Acc: (99.00%) (33090/33408)
Epoch: 236 | Batch_idx: 270 |  Loss: (0.0305) |  Loss2: (0.0000) | Acc: (99.00%) (34365/34688)
Epoch: 236 | Batch_idx: 280 |  Loss: (0.0304) |  Loss2: (0.0000) | Acc: (99.00%) (35634/35968)
Epoch: 236 | Batch_idx: 290 |  Loss: (0.0305) |  Loss2: (0.0000) | Acc: (99.00%) (36903/37248)
Epoch: 236 | Batch_idx: 300 |  Loss: (0.0308) |  Loss2: (0.0000) | Acc: (99.00%) (38167/38528)
Epoch: 236 | Batch_idx: 310 |  Loss: (0.0308) |  Loss2: (0.0000) | Acc: (99.00%) (39434/39808)
Epoch: 236 | Batch_idx: 320 |  Loss: (0.0310) |  Loss2: (0.0000) | Acc: (99.00%) (40697/41088)
Epoch: 236 | Batch_idx: 330 |  Loss: (0.0309) |  Loss2: (0.0000) | Acc: (99.00%) (41965/42368)
Epoch: 236 | Batch_idx: 340 |  Loss: (0.0310) |  Loss2: (0.0000) | Acc: (99.00%) (43229/43648)
Epoch: 236 | Batch_idx: 350 |  Loss: (0.0313) |  Loss2: (0.0000) | Acc: (99.00%) (44492/44928)
Epoch: 236 | Batch_idx: 360 |  Loss: (0.0313) |  Loss2: (0.0000) | Acc: (99.00%) (45758/46208)
Epoch: 236 | Batch_idx: 370 |  Loss: (0.0318) |  Loss2: (0.0000) | Acc: (99.00%) (47018/47488)
Epoch: 236 | Batch_idx: 380 |  Loss: (0.0317) |  Loss2: (0.0000) | Acc: (99.00%) (48287/48768)
Epoch: 236 | Batch_idx: 390 |  Loss: (0.0318) |  Loss2: (0.0000) | Acc: (99.00%) (49504/50000)
# TEST : Loss: (0.4310) | Acc: (89.00%) (8926/10000)
percent tensor([0.5916, 0.6101, 0.5982, 0.5856, 0.6048, 0.5722, 0.6132, 0.6104, 0.6118,
        0.6052, 0.6049, 0.6039, 0.6009, 0.6117, 0.5950, 0.5871],
       device='cuda:0') torch.Size([16])
percent tensor([0.5576, 0.5477, 0.5242, 0.5460, 0.5449, 0.5290, 0.5552, 0.5757, 0.5448,
        0.5283, 0.5224, 0.5161, 0.5517, 0.5564, 0.5572, 0.5432],
       device='cuda:0') torch.Size([16])
percent tensor([0.5714, 0.5307, 0.5030, 0.5917, 0.4980, 0.6160, 0.5461, 0.5667, 0.5540,
        0.5159, 0.5450, 0.5011, 0.4938, 0.6615, 0.5829, 0.5817],
       device='cuda:0') torch.Size([16])
percent tensor([0.7057, 0.7368, 0.6770, 0.6611, 0.6533, 0.6912, 0.7171, 0.6426, 0.7004,
        0.7348, 0.7281, 0.7103, 0.7578, 0.7090, 0.7108, 0.7160],
       device='cuda:0') torch.Size([16])
percent tensor([0.7024, 0.6279, 0.7199, 0.7290, 0.7494, 0.7831, 0.7101, 0.6986, 0.7758,
        0.7131, 0.7335, 0.7265, 0.6552, 0.7843, 0.6534, 0.7555],
       device='cuda:0') torch.Size([16])
percent tensor([0.8287, 0.8283, 0.8561, 0.8450, 0.8786, 0.8529, 0.8517, 0.8425, 0.8462,
        0.8423, 0.8524, 0.8357, 0.8334, 0.8620, 0.8234, 0.8513],
       device='cuda:0') torch.Size([16])
percent tensor([0.4005, 0.6115, 0.6040, 0.5677, 0.6135, 0.6787, 0.5321, 0.4043, 0.6036,
        0.5759, 0.5667, 0.5298, 0.5903, 0.4940, 0.3620, 0.3941],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9999, 0.9997, 0.9999, 0.9997, 0.9995, 0.9998, 0.9995, 0.9998,
        0.9998, 1.0000, 0.9999, 0.9998, 0.9994, 0.9994, 0.9995],
       device='cuda:0') torch.Size([16])
Epoch: 237 | Batch_idx: 0 |  Loss: (0.0135) |  Loss2: (0.0000) | Acc: (100.00%) (128/128)
Epoch: 237 | Batch_idx: 10 |  Loss: (0.0275) |  Loss2: (0.0000) | Acc: (99.00%) (1394/1408)
Epoch: 237 | Batch_idx: 20 |  Loss: (0.0259) |  Loss2: (0.0000) | Acc: (99.00%) (2667/2688)
Epoch: 237 | Batch_idx: 30 |  Loss: (0.0271) |  Loss2: (0.0000) | Acc: (99.00%) (3936/3968)
Epoch: 237 | Batch_idx: 40 |  Loss: (0.0265) |  Loss2: (0.0000) | Acc: (99.00%) (5206/5248)
Epoch: 237 | Batch_idx: 50 |  Loss: (0.0256) |  Loss2: (0.0000) | Acc: (99.00%) (6481/6528)
Epoch: 237 | Batch_idx: 60 |  Loss: (0.0266) |  Loss2: (0.0000) | Acc: (99.00%) (7747/7808)
Epoch: 237 | Batch_idx: 70 |  Loss: (0.0262) |  Loss2: (0.0000) | Acc: (99.00%) (9020/9088)
Epoch: 237 | Batch_idx: 80 |  Loss: (0.0273) |  Loss2: (0.0000) | Acc: (99.00%) (10288/10368)
Epoch: 237 | Batch_idx: 90 |  Loss: (0.0280) |  Loss2: (0.0000) | Acc: (99.00%) (11555/11648)
Epoch: 237 | Batch_idx: 100 |  Loss: (0.0282) |  Loss2: (0.0000) | Acc: (99.00%) (12822/12928)
Epoch: 237 | Batch_idx: 110 |  Loss: (0.0294) |  Loss2: (0.0000) | Acc: (99.00%) (14084/14208)
Epoch: 237 | Batch_idx: 120 |  Loss: (0.0299) |  Loss2: (0.0000) | Acc: (99.00%) (15349/15488)
Epoch: 237 | Batch_idx: 130 |  Loss: (0.0308) |  Loss2: (0.0000) | Acc: (99.00%) (16614/16768)
Epoch: 237 | Batch_idx: 140 |  Loss: (0.0307) |  Loss2: (0.0000) | Acc: (99.00%) (17881/18048)
Epoch: 237 | Batch_idx: 150 |  Loss: (0.0307) |  Loss2: (0.0000) | Acc: (99.00%) (19150/19328)
Epoch: 237 | Batch_idx: 160 |  Loss: (0.0307) |  Loss2: (0.0000) | Acc: (99.00%) (20418/20608)
Epoch: 237 | Batch_idx: 170 |  Loss: (0.0309) |  Loss2: (0.0000) | Acc: (99.00%) (21686/21888)
Epoch: 237 | Batch_idx: 180 |  Loss: (0.0309) |  Loss2: (0.0000) | Acc: (99.00%) (22955/23168)
Epoch: 237 | Batch_idx: 190 |  Loss: (0.0306) |  Loss2: (0.0000) | Acc: (99.00%) (24225/24448)
Epoch: 237 | Batch_idx: 200 |  Loss: (0.0310) |  Loss2: (0.0000) | Acc: (99.00%) (25486/25728)
Epoch: 237 | Batch_idx: 210 |  Loss: (0.0313) |  Loss2: (0.0000) | Acc: (99.00%) (26752/27008)
Epoch: 237 | Batch_idx: 220 |  Loss: (0.0313) |  Loss2: (0.0000) | Acc: (99.00%) (28022/28288)
Epoch: 237 | Batch_idx: 230 |  Loss: (0.0309) |  Loss2: (0.0000) | Acc: (99.00%) (29296/29568)
Epoch: 237 | Batch_idx: 240 |  Loss: (0.0308) |  Loss2: (0.0000) | Acc: (99.00%) (30564/30848)
Epoch: 237 | Batch_idx: 250 |  Loss: (0.0307) |  Loss2: (0.0000) | Acc: (99.00%) (31834/32128)
Epoch: 237 | Batch_idx: 260 |  Loss: (0.0305) |  Loss2: (0.0000) | Acc: (99.00%) (33105/33408)
Epoch: 237 | Batch_idx: 270 |  Loss: (0.0306) |  Loss2: (0.0000) | Acc: (99.00%) (34373/34688)
Epoch: 237 | Batch_idx: 280 |  Loss: (0.0309) |  Loss2: (0.0000) | Acc: (99.00%) (35640/35968)
Epoch: 237 | Batch_idx: 290 |  Loss: (0.0308) |  Loss2: (0.0000) | Acc: (99.00%) (36911/37248)
Epoch: 237 | Batch_idx: 300 |  Loss: (0.0308) |  Loss2: (0.0000) | Acc: (99.00%) (38181/38528)
Epoch: 237 | Batch_idx: 310 |  Loss: (0.0310) |  Loss2: (0.0000) | Acc: (99.00%) (39443/39808)
Epoch: 237 | Batch_idx: 320 |  Loss: (0.0311) |  Loss2: (0.0000) | Acc: (99.00%) (40710/41088)
Epoch: 237 | Batch_idx: 330 |  Loss: (0.0311) |  Loss2: (0.0000) | Acc: (99.00%) (41976/42368)
Epoch: 237 | Batch_idx: 340 |  Loss: (0.0312) |  Loss2: (0.0000) | Acc: (99.00%) (43243/43648)
Epoch: 237 | Batch_idx: 350 |  Loss: (0.0313) |  Loss2: (0.0000) | Acc: (99.00%) (44513/44928)
Epoch: 237 | Batch_idx: 360 |  Loss: (0.0314) |  Loss2: (0.0000) | Acc: (99.00%) (45779/46208)
Epoch: 237 | Batch_idx: 370 |  Loss: (0.0314) |  Loss2: (0.0000) | Acc: (99.00%) (47046/47488)
Epoch: 237 | Batch_idx: 380 |  Loss: (0.0316) |  Loss2: (0.0000) | Acc: (99.00%) (48311/48768)
Epoch: 237 | Batch_idx: 390 |  Loss: (0.0316) |  Loss2: (0.0000) | Acc: (99.00%) (49529/50000)
# TEST : Loss: (0.4632) | Acc: (88.00%) (8874/10000)
percent tensor([0.5944, 0.6101, 0.6026, 0.5886, 0.6105, 0.5763, 0.6155, 0.6119, 0.6121,
        0.6071, 0.6055, 0.6086, 0.6027, 0.6099, 0.5970, 0.5886],
       device='cuda:0') torch.Size([16])
percent tensor([0.5601, 0.5495, 0.5299, 0.5470, 0.5494, 0.5299, 0.5589, 0.5772, 0.5475,
        0.5303, 0.5233, 0.5226, 0.5539, 0.5577, 0.5584, 0.5439],
       device='cuda:0') torch.Size([16])
percent tensor([0.5689, 0.5256, 0.5033, 0.5878, 0.5028, 0.6128, 0.5434, 0.5648, 0.5607,
        0.5146, 0.5451, 0.5002, 0.4880, 0.6590, 0.5759, 0.5759],
       device='cuda:0') torch.Size([16])
percent tensor([0.7093, 0.7344, 0.6797, 0.6614, 0.6568, 0.6977, 0.7157, 0.6443, 0.7029,
        0.7352, 0.7278, 0.7180, 0.7603, 0.6987, 0.7118, 0.7164],
       device='cuda:0') torch.Size([16])
percent tensor([0.7047, 0.6107, 0.7178, 0.7344, 0.7481, 0.7748, 0.7043, 0.7071, 0.7712,
        0.7079, 0.7224, 0.7195, 0.6297, 0.7805, 0.6356, 0.7496],
       device='cuda:0') torch.Size([16])
percent tensor([0.8285, 0.8258, 0.8583, 0.8415, 0.8755, 0.8541, 0.8472, 0.8368, 0.8435,
        0.8346, 0.8479, 0.8259, 0.8330, 0.8584, 0.8211, 0.8493],
       device='cuda:0') torch.Size([16])
percent tensor([0.4206, 0.6153, 0.6404, 0.5771, 0.6435, 0.7024, 0.5602, 0.4128, 0.5859,
        0.5274, 0.5404, 0.4878, 0.5982, 0.4573, 0.3683, 0.4205],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9999, 0.9997, 0.9999, 0.9998, 0.9994, 0.9999, 0.9997, 0.9998,
        0.9998, 1.0000, 0.9999, 0.9998, 0.9996, 0.9994, 0.9993],
       device='cuda:0') torch.Size([16])
Epoch: 238 | Batch_idx: 0 |  Loss: (0.0342) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 238 | Batch_idx: 10 |  Loss: (0.0271) |  Loss2: (0.0000) | Acc: (99.00%) (1400/1408)
Epoch: 238 | Batch_idx: 20 |  Loss: (0.0275) |  Loss2: (0.0000) | Acc: (99.00%) (2668/2688)
Epoch: 238 | Batch_idx: 30 |  Loss: (0.0296) |  Loss2: (0.0000) | Acc: (99.00%) (3930/3968)
Epoch: 238 | Batch_idx: 40 |  Loss: (0.0290) |  Loss2: (0.0000) | Acc: (99.00%) (5198/5248)
Epoch: 238 | Batch_idx: 50 |  Loss: (0.0297) |  Loss2: (0.0000) | Acc: (98.00%) (6461/6528)
Epoch: 238 | Batch_idx: 60 |  Loss: (0.0281) |  Loss2: (0.0000) | Acc: (99.00%) (7734/7808)
Epoch: 238 | Batch_idx: 70 |  Loss: (0.0278) |  Loss2: (0.0000) | Acc: (99.00%) (9007/9088)
Epoch: 238 | Batch_idx: 80 |  Loss: (0.0294) |  Loss2: (0.0000) | Acc: (99.00%) (10269/10368)
Epoch: 238 | Batch_idx: 90 |  Loss: (0.0289) |  Loss2: (0.0000) | Acc: (99.00%) (11541/11648)
Epoch: 238 | Batch_idx: 100 |  Loss: (0.0296) |  Loss2: (0.0000) | Acc: (99.00%) (12805/12928)
Epoch: 238 | Batch_idx: 110 |  Loss: (0.0298) |  Loss2: (0.0000) | Acc: (99.00%) (14076/14208)
Epoch: 238 | Batch_idx: 120 |  Loss: (0.0296) |  Loss2: (0.0000) | Acc: (99.00%) (15346/15488)
Epoch: 238 | Batch_idx: 130 |  Loss: (0.0292) |  Loss2: (0.0000) | Acc: (99.00%) (16618/16768)
Epoch: 238 | Batch_idx: 140 |  Loss: (0.0288) |  Loss2: (0.0000) | Acc: (99.00%) (17891/18048)
Epoch: 238 | Batch_idx: 150 |  Loss: (0.0288) |  Loss2: (0.0000) | Acc: (99.00%) (19159/19328)
Epoch: 238 | Batch_idx: 160 |  Loss: (0.0284) |  Loss2: (0.0000) | Acc: (99.00%) (20432/20608)
Epoch: 238 | Batch_idx: 170 |  Loss: (0.0284) |  Loss2: (0.0000) | Acc: (99.00%) (21703/21888)
Epoch: 238 | Batch_idx: 180 |  Loss: (0.0284) |  Loss2: (0.0000) | Acc: (99.00%) (22970/23168)
Epoch: 238 | Batch_idx: 190 |  Loss: (0.0289) |  Loss2: (0.0000) | Acc: (99.00%) (24234/24448)
Epoch: 238 | Batch_idx: 200 |  Loss: (0.0295) |  Loss2: (0.0000) | Acc: (99.00%) (25493/25728)
Epoch: 238 | Batch_idx: 210 |  Loss: (0.0297) |  Loss2: (0.0000) | Acc: (99.00%) (26760/27008)
Epoch: 238 | Batch_idx: 220 |  Loss: (0.0302) |  Loss2: (0.0000) | Acc: (99.00%) (28026/28288)
Epoch: 238 | Batch_idx: 230 |  Loss: (0.0301) |  Loss2: (0.0000) | Acc: (99.00%) (29291/29568)
Epoch: 238 | Batch_idx: 240 |  Loss: (0.0302) |  Loss2: (0.0000) | Acc: (99.00%) (30559/30848)
Epoch: 238 | Batch_idx: 250 |  Loss: (0.0302) |  Loss2: (0.0000) | Acc: (99.00%) (31828/32128)
Epoch: 238 | Batch_idx: 260 |  Loss: (0.0303) |  Loss2: (0.0000) | Acc: (99.00%) (33094/33408)
Epoch: 238 | Batch_idx: 270 |  Loss: (0.0303) |  Loss2: (0.0000) | Acc: (99.00%) (34359/34688)
Epoch: 238 | Batch_idx: 280 |  Loss: (0.0306) |  Loss2: (0.0000) | Acc: (99.00%) (35619/35968)
Epoch: 238 | Batch_idx: 290 |  Loss: (0.0308) |  Loss2: (0.0000) | Acc: (99.00%) (36880/37248)
Epoch: 238 | Batch_idx: 300 |  Loss: (0.0309) |  Loss2: (0.0000) | Acc: (99.00%) (38144/38528)
Epoch: 238 | Batch_idx: 310 |  Loss: (0.0315) |  Loss2: (0.0000) | Acc: (98.00%) (39406/39808)
Epoch: 238 | Batch_idx: 320 |  Loss: (0.0318) |  Loss2: (0.0000) | Acc: (98.00%) (40668/41088)
Epoch: 238 | Batch_idx: 330 |  Loss: (0.0318) |  Loss2: (0.0000) | Acc: (98.00%) (41938/42368)
Epoch: 238 | Batch_idx: 340 |  Loss: (0.0316) |  Loss2: (0.0000) | Acc: (98.00%) (43209/43648)
Epoch: 238 | Batch_idx: 350 |  Loss: (0.0315) |  Loss2: (0.0000) | Acc: (98.00%) (44478/44928)
Epoch: 238 | Batch_idx: 360 |  Loss: (0.0317) |  Loss2: (0.0000) | Acc: (98.00%) (45743/46208)
Epoch: 238 | Batch_idx: 370 |  Loss: (0.0317) |  Loss2: (0.0000) | Acc: (99.00%) (47014/47488)
Epoch: 238 | Batch_idx: 380 |  Loss: (0.0316) |  Loss2: (0.0000) | Acc: (99.00%) (48282/48768)
Epoch: 238 | Batch_idx: 390 |  Loss: (0.0318) |  Loss2: (0.0000) | Acc: (98.00%) (49499/50000)
# TEST : Loss: (0.4366) | Acc: (89.00%) (8944/10000)
percent tensor([0.5954, 0.6141, 0.6014, 0.5892, 0.6085, 0.5755, 0.6171, 0.6140, 0.6146,
        0.6092, 0.6084, 0.6078, 0.6044, 0.6152, 0.5989, 0.5907],
       device='cuda:0') torch.Size([16])
percent tensor([0.5586, 0.5465, 0.5296, 0.5453, 0.5473, 0.5277, 0.5570, 0.5774, 0.5483,
        0.5286, 0.5235, 0.5211, 0.5535, 0.5531, 0.5560, 0.5426],
       device='cuda:0') torch.Size([16])
percent tensor([0.5695, 0.5240, 0.5139, 0.5954, 0.5048, 0.6198, 0.5469, 0.5712, 0.5613,
        0.5140, 0.5481, 0.5060, 0.4888, 0.6636, 0.5735, 0.5785],
       device='cuda:0') torch.Size([16])
percent tensor([0.6996, 0.7285, 0.6672, 0.6547, 0.6482, 0.6881, 0.7066, 0.6397, 0.6948,
        0.7282, 0.7186, 0.7045, 0.7503, 0.6916, 0.7030, 0.7103],
       device='cuda:0') torch.Size([16])
percent tensor([0.6995, 0.6226, 0.7205, 0.7377, 0.7549, 0.7942, 0.7107, 0.6997, 0.7679,
        0.7059, 0.7314, 0.7171, 0.6367, 0.7959, 0.6533, 0.7545],
       device='cuda:0') torch.Size([16])
percent tensor([0.8301, 0.8364, 0.8524, 0.8457, 0.8747, 0.8542, 0.8529, 0.8379, 0.8451,
        0.8413, 0.8583, 0.8256, 0.8362, 0.8682, 0.8260, 0.8525],
       device='cuda:0') torch.Size([16])
percent tensor([0.4058, 0.6430, 0.5908, 0.5687, 0.6080, 0.7015, 0.5655, 0.3896, 0.5858,
        0.5703, 0.5807, 0.5007, 0.6142, 0.4708, 0.3743, 0.3951],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9998, 0.9997, 0.9999, 0.9997, 0.9997, 0.9999, 0.9996, 0.9998,
        0.9997, 1.0000, 0.9999, 0.9998, 0.9993, 0.9992, 0.9993],
       device='cuda:0') torch.Size([16])
Epoch: 239 | Batch_idx: 0 |  Loss: (0.0288) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 239 | Batch_idx: 10 |  Loss: (0.0313) |  Loss2: (0.0000) | Acc: (99.00%) (1398/1408)
Epoch: 239 | Batch_idx: 20 |  Loss: (0.0302) |  Loss2: (0.0000) | Acc: (99.00%) (2669/2688)
Epoch: 239 | Batch_idx: 30 |  Loss: (0.0323) |  Loss2: (0.0000) | Acc: (99.00%) (3932/3968)
Epoch: 239 | Batch_idx: 40 |  Loss: (0.0314) |  Loss2: (0.0000) | Acc: (99.00%) (5201/5248)
Epoch: 239 | Batch_idx: 50 |  Loss: (0.0312) |  Loss2: (0.0000) | Acc: (99.00%) (6470/6528)
Epoch: 239 | Batch_idx: 60 |  Loss: (0.0293) |  Loss2: (0.0000) | Acc: (99.00%) (7740/7808)
Epoch: 239 | Batch_idx: 70 |  Loss: (0.0294) |  Loss2: (0.0000) | Acc: (99.00%) (9004/9088)
Epoch: 239 | Batch_idx: 80 |  Loss: (0.0302) |  Loss2: (0.0000) | Acc: (99.00%) (10269/10368)
Epoch: 239 | Batch_idx: 90 |  Loss: (0.0302) |  Loss2: (0.0000) | Acc: (99.00%) (11536/11648)
Epoch: 239 | Batch_idx: 100 |  Loss: (0.0299) |  Loss2: (0.0000) | Acc: (99.00%) (12803/12928)
Epoch: 239 | Batch_idx: 110 |  Loss: (0.0294) |  Loss2: (0.0000) | Acc: (99.00%) (14078/14208)
Epoch: 239 | Batch_idx: 120 |  Loss: (0.0288) |  Loss2: (0.0000) | Acc: (99.00%) (15352/15488)
Epoch: 239 | Batch_idx: 130 |  Loss: (0.0282) |  Loss2: (0.0000) | Acc: (99.00%) (16624/16768)
Epoch: 239 | Batch_idx: 140 |  Loss: (0.0282) |  Loss2: (0.0000) | Acc: (99.00%) (17894/18048)
Epoch: 239 | Batch_idx: 150 |  Loss: (0.0282) |  Loss2: (0.0000) | Acc: (99.00%) (19163/19328)
Epoch: 239 | Batch_idx: 160 |  Loss: (0.0285) |  Loss2: (0.0000) | Acc: (99.00%) (20425/20608)
Epoch: 239 | Batch_idx: 170 |  Loss: (0.0284) |  Loss2: (0.0000) | Acc: (99.00%) (21695/21888)
Epoch: 239 | Batch_idx: 180 |  Loss: (0.0286) |  Loss2: (0.0000) | Acc: (99.00%) (22965/23168)
Epoch: 239 | Batch_idx: 190 |  Loss: (0.0288) |  Loss2: (0.0000) | Acc: (99.00%) (24234/24448)
Epoch: 239 | Batch_idx: 200 |  Loss: (0.0288) |  Loss2: (0.0000) | Acc: (99.00%) (25503/25728)
Epoch: 239 | Batch_idx: 210 |  Loss: (0.0286) |  Loss2: (0.0000) | Acc: (99.00%) (26772/27008)
Epoch: 239 | Batch_idx: 220 |  Loss: (0.0292) |  Loss2: (0.0000) | Acc: (99.00%) (28038/28288)
Epoch: 239 | Batch_idx: 230 |  Loss: (0.0296) |  Loss2: (0.0000) | Acc: (99.00%) (29302/29568)
Epoch: 239 | Batch_idx: 240 |  Loss: (0.0292) |  Loss2: (0.0000) | Acc: (99.00%) (30576/30848)
Epoch: 239 | Batch_idx: 250 |  Loss: (0.0292) |  Loss2: (0.0000) | Acc: (99.00%) (31844/32128)
Epoch: 239 | Batch_idx: 260 |  Loss: (0.0291) |  Loss2: (0.0000) | Acc: (99.00%) (33115/33408)
Epoch: 239 | Batch_idx: 270 |  Loss: (0.0288) |  Loss2: (0.0000) | Acc: (99.00%) (34387/34688)
Epoch: 239 | Batch_idx: 280 |  Loss: (0.0289) |  Loss2: (0.0000) | Acc: (99.00%) (35657/35968)
Epoch: 239 | Batch_idx: 290 |  Loss: (0.0292) |  Loss2: (0.0000) | Acc: (99.00%) (36924/37248)
Epoch: 239 | Batch_idx: 300 |  Loss: (0.0292) |  Loss2: (0.0000) | Acc: (99.00%) (38192/38528)
Epoch: 239 | Batch_idx: 310 |  Loss: (0.0293) |  Loss2: (0.0000) | Acc: (99.00%) (39457/39808)
Epoch: 239 | Batch_idx: 320 |  Loss: (0.0291) |  Loss2: (0.0000) | Acc: (99.00%) (40730/41088)
Epoch: 239 | Batch_idx: 330 |  Loss: (0.0293) |  Loss2: (0.0000) | Acc: (99.00%) (41999/42368)
Epoch: 239 | Batch_idx: 340 |  Loss: (0.0296) |  Loss2: (0.0000) | Acc: (99.00%) (43263/43648)
Epoch: 239 | Batch_idx: 350 |  Loss: (0.0297) |  Loss2: (0.0000) | Acc: (99.00%) (44532/44928)
Epoch: 239 | Batch_idx: 360 |  Loss: (0.0296) |  Loss2: (0.0000) | Acc: (99.00%) (45805/46208)
Epoch: 239 | Batch_idx: 370 |  Loss: (0.0295) |  Loss2: (0.0000) | Acc: (99.00%) (47072/47488)
Epoch: 239 | Batch_idx: 380 |  Loss: (0.0297) |  Loss2: (0.0000) | Acc: (99.00%) (48337/48768)
Epoch: 239 | Batch_idx: 390 |  Loss: (0.0299) |  Loss2: (0.0000) | Acc: (99.00%) (49549/50000)
# TEST : Loss: (0.4461) | Acc: (89.00%) (8947/10000)
percent tensor([0.5920, 0.6086, 0.5974, 0.5858, 0.6047, 0.5729, 0.6117, 0.6098, 0.6100,
        0.6045, 0.6039, 0.6032, 0.6003, 0.6096, 0.5947, 0.5870],
       device='cuda:0') torch.Size([16])
percent tensor([0.5620, 0.5504, 0.5205, 0.5437, 0.5430, 0.5319, 0.5580, 0.5737, 0.5493,
        0.5295, 0.5270, 0.5163, 0.5575, 0.5591, 0.5589, 0.5460],
       device='cuda:0') torch.Size([16])
percent tensor([0.5744, 0.5313, 0.5074, 0.5936, 0.5085, 0.6218, 0.5525, 0.5684, 0.5670,
        0.5206, 0.5559, 0.5058, 0.4952, 0.6613, 0.5865, 0.5845],
       device='cuda:0') torch.Size([16])
percent tensor([0.7049, 0.7366, 0.6745, 0.6608, 0.6551, 0.6920, 0.7160, 0.6432, 0.6942,
        0.7342, 0.7253, 0.7116, 0.7567, 0.7026, 0.7099, 0.7163],
       device='cuda:0') torch.Size([16])
percent tensor([0.7085, 0.6227, 0.7408, 0.7474, 0.7627, 0.7961, 0.7134, 0.7077, 0.7713,
        0.7102, 0.7337, 0.7184, 0.6458, 0.7832, 0.6649, 0.7603],
       device='cuda:0') torch.Size([16])
percent tensor([0.8304, 0.8336, 0.8615, 0.8450, 0.8753, 0.8548, 0.8490, 0.8398, 0.8490,
        0.8435, 0.8572, 0.8284, 0.8361, 0.8655, 0.8259, 0.8531],
       device='cuda:0') torch.Size([16])
percent tensor([0.4055, 0.6159, 0.5987, 0.5451, 0.5990, 0.6845, 0.5412, 0.3921, 0.5838,
        0.5418, 0.5711, 0.5069, 0.6001, 0.4771, 0.3577, 0.3861],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9999, 0.9996, 0.9999, 0.9995, 0.9996, 0.9998, 0.9997, 0.9998,
        0.9996, 1.0000, 0.9999, 0.9998, 0.9996, 0.9994, 0.9993],
       device='cuda:0') torch.Size([16])
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(183.4053, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(831.5322, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(818.7746, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1491.9429, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(472.1978, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2320.2200, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4290.6665, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1329.1774, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6413.9419, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11399.2793, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3729.4856, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(15710.0508, device='cuda:0', grad_fn=<NormBackward0>)
6 hours 9 mins 4 secs for training