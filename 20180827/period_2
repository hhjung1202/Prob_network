Files already downloaded and verified
USE 1 GPUs!
Epoch: 0 | Batch_idx: 0 |  Loss: (2.3217) |  Loss2: (0.0000) | Acc: (8.00%) (11/128)
Epoch: 0 | Batch_idx: 10 |  Loss: (2.2986) |  Loss2: (0.0000) | Acc: (10.00%) (145/1408)
Epoch: 0 | Batch_idx: 20 |  Loss: (2.2920) |  Loss2: (0.0000) | Acc: (11.00%) (313/2688)
Epoch: 0 | Batch_idx: 30 |  Loss: (2.2862) |  Loss2: (0.0000) | Acc: (13.00%) (539/3968)
Epoch: 0 | Batch_idx: 40 |  Loss: (2.2774) |  Loss2: (0.0000) | Acc: (15.00%) (799/5248)
Epoch: 0 | Batch_idx: 50 |  Loss: (2.2684) |  Loss2: (0.0000) | Acc: (16.00%) (1052/6528)
Epoch: 0 | Batch_idx: 60 |  Loss: (2.2594) |  Loss2: (0.0000) | Acc: (17.00%) (1344/7808)
Epoch: 0 | Batch_idx: 70 |  Loss: (2.2505) |  Loss2: (0.0000) | Acc: (17.00%) (1613/9088)
Epoch: 0 | Batch_idx: 80 |  Loss: (2.2417) |  Loss2: (0.0000) | Acc: (18.00%) (1914/10368)
Epoch: 0 | Batch_idx: 90 |  Loss: (2.2311) |  Loss2: (0.0000) | Acc: (19.00%) (2218/11648)
Epoch: 0 | Batch_idx: 100 |  Loss: (2.2214) |  Loss2: (0.0000) | Acc: (19.00%) (2509/12928)
Epoch: 0 | Batch_idx: 110 |  Loss: (2.2120) |  Loss2: (0.0000) | Acc: (19.00%) (2792/14208)
Epoch: 0 | Batch_idx: 120 |  Loss: (2.2038) |  Loss2: (0.0000) | Acc: (20.00%) (3117/15488)
Epoch: 0 | Batch_idx: 130 |  Loss: (2.1953) |  Loss2: (0.0000) | Acc: (20.00%) (3450/16768)
Epoch: 0 | Batch_idx: 140 |  Loss: (2.1878) |  Loss2: (0.0000) | Acc: (20.00%) (3776/18048)
Epoch: 0 | Batch_idx: 150 |  Loss: (2.1795) |  Loss2: (0.0000) | Acc: (21.00%) (4113/19328)
Epoch: 0 | Batch_idx: 160 |  Loss: (2.1713) |  Loss2: (0.0000) | Acc: (21.00%) (4445/20608)
Epoch: 0 | Batch_idx: 170 |  Loss: (2.1653) |  Loss2: (0.0000) | Acc: (21.00%) (4775/21888)
Epoch: 0 | Batch_idx: 180 |  Loss: (2.1569) |  Loss2: (0.0000) | Acc: (22.00%) (5148/23168)
Epoch: 0 | Batch_idx: 190 |  Loss: (2.1510) |  Loss2: (0.0000) | Acc: (22.00%) (5499/24448)
Epoch: 0 | Batch_idx: 200 |  Loss: (2.1437) |  Loss2: (0.0000) | Acc: (22.00%) (5853/25728)
Epoch: 0 | Batch_idx: 210 |  Loss: (2.1360) |  Loss2: (0.0000) | Acc: (22.00%) (6211/27008)
Epoch: 0 | Batch_idx: 220 |  Loss: (2.1286) |  Loss2: (0.0000) | Acc: (23.00%) (6564/28288)
Epoch: 0 | Batch_idx: 230 |  Loss: (2.1216) |  Loss2: (0.0000) | Acc: (23.00%) (6897/29568)
Epoch: 0 | Batch_idx: 240 |  Loss: (2.1138) |  Loss2: (0.0000) | Acc: (23.00%) (7268/30848)
Epoch: 0 | Batch_idx: 250 |  Loss: (2.1078) |  Loss2: (0.0000) | Acc: (23.00%) (7637/32128)
Epoch: 0 | Batch_idx: 260 |  Loss: (2.1018) |  Loss2: (0.0000) | Acc: (23.00%) (7986/33408)
Epoch: 0 | Batch_idx: 270 |  Loss: (2.0953) |  Loss2: (0.0000) | Acc: (24.00%) (8366/34688)
Epoch: 0 | Batch_idx: 280 |  Loss: (2.0891) |  Loss2: (0.0000) | Acc: (24.00%) (8774/35968)
Epoch: 0 | Batch_idx: 290 |  Loss: (2.0826) |  Loss2: (0.0000) | Acc: (24.00%) (9188/37248)
Epoch: 0 | Batch_idx: 300 |  Loss: (2.0764) |  Loss2: (0.0000) | Acc: (24.00%) (9588/38528)
Epoch: 0 | Batch_idx: 310 |  Loss: (2.0709) |  Loss2: (0.0000) | Acc: (24.00%) (9948/39808)
Epoch: 0 | Batch_idx: 320 |  Loss: (2.0647) |  Loss2: (0.0000) | Acc: (25.00%) (10340/41088)
Epoch: 0 | Batch_idx: 330 |  Loss: (2.0583) |  Loss2: (0.0000) | Acc: (25.00%) (10750/42368)
Epoch: 0 | Batch_idx: 340 |  Loss: (2.0525) |  Loss2: (0.0000) | Acc: (25.00%) (11159/43648)
Epoch: 0 | Batch_idx: 350 |  Loss: (2.0468) |  Loss2: (0.0000) | Acc: (25.00%) (11584/44928)
Epoch: 0 | Batch_idx: 360 |  Loss: (2.0414) |  Loss2: (0.0000) | Acc: (25.00%) (12004/46208)
Epoch: 0 | Batch_idx: 370 |  Loss: (2.0358) |  Loss2: (0.0000) | Acc: (26.00%) (12433/47488)
Epoch: 0 | Batch_idx: 380 |  Loss: (2.0300) |  Loss2: (0.0000) | Acc: (26.00%) (12861/48768)
Epoch: 0 | Batch_idx: 390 |  Loss: (2.0240) |  Loss2: (0.0000) | Acc: (26.00%) (13293/50000)
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type ResNet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type BasicBlock. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type _Gate. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_000.pth.tar'
# TEST : Loss: (1.7697) | Acc: (33.00%) (3320/10000)
percent tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0') torch.Size([16])
percent tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0') torch.Size([16])
percent tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0') torch.Size([16])
percent tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0') torch.Size([16])
percent tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0') torch.Size([16])
percent tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0') torch.Size([16])
percent tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0') torch.Size([16])
percent tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0') torch.Size([16])
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(167.3529, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(768.4429, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(766.5044, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1532.4392, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(509.5790, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2171.2039, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4334.3853, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1450.7429, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6145.1377, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12279.3340, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4094.5422, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(17369.9102, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 1 | Batch_idx: 0 |  Loss: (1.8860) |  Loss2: (0.0000) | Acc: (28.00%) (37/128)
Epoch: 1 | Batch_idx: 10 |  Loss: (1.8084) |  Loss2: (0.0000) | Acc: (32.00%) (460/1408)
Epoch: 1 | Batch_idx: 20 |  Loss: (1.8013) |  Loss2: (0.0000) | Acc: (33.00%) (892/2688)
Epoch: 1 | Batch_idx: 30 |  Loss: (1.8058) |  Loss2: (0.0000) | Acc: (32.00%) (1294/3968)
Epoch: 1 | Batch_idx: 40 |  Loss: (1.7999) |  Loss2: (0.0000) | Acc: (32.00%) (1722/5248)
Epoch: 1 | Batch_idx: 50 |  Loss: (1.7997) |  Loss2: (0.0000) | Acc: (33.00%) (2167/6528)
Epoch: 1 | Batch_idx: 60 |  Loss: (1.7952) |  Loss2: (0.0000) | Acc: (33.00%) (2608/7808)
Epoch: 1 | Batch_idx: 70 |  Loss: (1.7919) |  Loss2: (0.0000) | Acc: (33.00%) (3054/9088)
Epoch: 1 | Batch_idx: 80 |  Loss: (1.7883) |  Loss2: (0.0000) | Acc: (33.00%) (3505/10368)
Epoch: 1 | Batch_idx: 90 |  Loss: (1.7821) |  Loss2: (0.0000) | Acc: (34.00%) (3988/11648)
Epoch: 1 | Batch_idx: 100 |  Loss: (1.7768) |  Loss2: (0.0000) | Acc: (34.00%) (4475/12928)
Epoch: 1 | Batch_idx: 110 |  Loss: (1.7731) |  Loss2: (0.0000) | Acc: (34.00%) (4966/14208)
Epoch: 1 | Batch_idx: 120 |  Loss: (1.7723) |  Loss2: (0.0000) | Acc: (34.00%) (5420/15488)
Epoch: 1 | Batch_idx: 130 |  Loss: (1.7688) |  Loss2: (0.0000) | Acc: (35.00%) (5907/16768)
Epoch: 1 | Batch_idx: 140 |  Loss: (1.7665) |  Loss2: (0.0000) | Acc: (35.00%) (6402/18048)
Epoch: 1 | Batch_idx: 150 |  Loss: (1.7633) |  Loss2: (0.0000) | Acc: (35.00%) (6884/19328)
Epoch: 1 | Batch_idx: 160 |  Loss: (1.7590) |  Loss2: (0.0000) | Acc: (35.00%) (7368/20608)
Epoch: 1 | Batch_idx: 170 |  Loss: (1.7549) |  Loss2: (0.0000) | Acc: (35.00%) (7860/21888)
Epoch: 1 | Batch_idx: 180 |  Loss: (1.7498) |  Loss2: (0.0000) | Acc: (36.00%) (8368/23168)
Epoch: 1 | Batch_idx: 190 |  Loss: (1.7464) |  Loss2: (0.0000) | Acc: (36.00%) (8882/24448)
Epoch: 1 | Batch_idx: 200 |  Loss: (1.7432) |  Loss2: (0.0000) | Acc: (36.00%) (9384/25728)
Epoch: 1 | Batch_idx: 210 |  Loss: (1.7398) |  Loss2: (0.0000) | Acc: (36.00%) (9905/27008)
Epoch: 1 | Batch_idx: 220 |  Loss: (1.7357) |  Loss2: (0.0000) | Acc: (36.00%) (10413/28288)
Epoch: 1 | Batch_idx: 230 |  Loss: (1.7318) |  Loss2: (0.0000) | Acc: (36.00%) (10924/29568)
Epoch: 1 | Batch_idx: 240 |  Loss: (1.7278) |  Loss2: (0.0000) | Acc: (37.00%) (11442/30848)
Epoch: 1 | Batch_idx: 250 |  Loss: (1.7237) |  Loss2: (0.0000) | Acc: (37.00%) (11958/32128)
Epoch: 1 | Batch_idx: 260 |  Loss: (1.7200) |  Loss2: (0.0000) | Acc: (37.00%) (12468/33408)
Epoch: 1 | Batch_idx: 270 |  Loss: (1.7171) |  Loss2: (0.0000) | Acc: (37.00%) (12954/34688)
Epoch: 1 | Batch_idx: 280 |  Loss: (1.7139) |  Loss2: (0.0000) | Acc: (37.00%) (13462/35968)
Epoch: 1 | Batch_idx: 290 |  Loss: (1.7115) |  Loss2: (0.0000) | Acc: (37.00%) (13966/37248)
Epoch: 1 | Batch_idx: 300 |  Loss: (1.7085) |  Loss2: (0.0000) | Acc: (37.00%) (14462/38528)
Epoch: 1 | Batch_idx: 310 |  Loss: (1.7053) |  Loss2: (0.0000) | Acc: (37.00%) (14989/39808)
Epoch: 1 | Batch_idx: 320 |  Loss: (1.7033) |  Loss2: (0.0000) | Acc: (37.00%) (15514/41088)
Epoch: 1 | Batch_idx: 330 |  Loss: (1.7006) |  Loss2: (0.0000) | Acc: (37.00%) (16028/42368)
Epoch: 1 | Batch_idx: 340 |  Loss: (1.6982) |  Loss2: (0.0000) | Acc: (37.00%) (16561/43648)
Epoch: 1 | Batch_idx: 350 |  Loss: (1.6953) |  Loss2: (0.0000) | Acc: (38.00%) (17097/44928)
Epoch: 1 | Batch_idx: 360 |  Loss: (1.6924) |  Loss2: (0.0000) | Acc: (38.00%) (17623/46208)
Epoch: 1 | Batch_idx: 370 |  Loss: (1.6895) |  Loss2: (0.0000) | Acc: (38.00%) (18148/47488)
Epoch: 1 | Batch_idx: 380 |  Loss: (1.6868) |  Loss2: (0.0000) | Acc: (38.00%) (18653/48768)
Epoch: 1 | Batch_idx: 390 |  Loss: (1.6850) |  Loss2: (0.0000) | Acc: (38.00%) (19132/50000)
# TEST : Loss: (1.6542) | Acc: (38.00%) (3893/10000)
percent tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0') torch.Size([16])
percent tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0') torch.Size([16])
percent tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0') torch.Size([16])
percent tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0') torch.Size([16])
percent tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0') torch.Size([16])
percent tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0') torch.Size([16])
percent tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0') torch.Size([16])
percent tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 2 | Batch_idx: 0 |  Loss: (1.5760) |  Loss2: (0.0000) | Acc: (39.00%) (50/128)
Epoch: 2 | Batch_idx: 10 |  Loss: (1.5921) |  Loss2: (0.0000) | Acc: (40.00%) (569/1408)
Epoch: 2 | Batch_idx: 20 |  Loss: (1.6091) |  Loss2: (0.0000) | Acc: (39.00%) (1064/2688)
Epoch: 2 | Batch_idx: 30 |  Loss: (1.6189) |  Loss2: (0.0000) | Acc: (39.00%) (1564/3968)
Epoch: 2 | Batch_idx: 40 |  Loss: (1.6149) |  Loss2: (0.0000) | Acc: (39.00%) (2065/5248)
Epoch: 2 | Batch_idx: 50 |  Loss: (1.6213) |  Loss2: (0.0000) | Acc: (38.00%) (2542/6528)
Epoch: 2 | Batch_idx: 60 |  Loss: (1.6221) |  Loss2: (0.0000) | Acc: (39.00%) (3047/7808)
Epoch: 2 | Batch_idx: 70 |  Loss: (1.6209) |  Loss2: (0.0000) | Acc: (39.00%) (3567/9088)
Epoch: 2 | Batch_idx: 80 |  Loss: (1.6218) |  Loss2: (0.0000) | Acc: (39.00%) (4088/10368)
Epoch: 2 | Batch_idx: 90 |  Loss: (1.6198) |  Loss2: (0.0000) | Acc: (39.00%) (4588/11648)
Epoch: 2 | Batch_idx: 100 |  Loss: (1.6212) |  Loss2: (0.0000) | Acc: (39.00%) (5072/12928)
Epoch: 2 | Batch_idx: 110 |  Loss: (1.6184) |  Loss2: (0.0000) | Acc: (39.00%) (5597/14208)
Epoch: 2 | Batch_idx: 120 |  Loss: (1.6225) |  Loss2: (0.0000) | Acc: (39.00%) (6068/15488)
Epoch: 2 | Batch_idx: 130 |  Loss: (1.6243) |  Loss2: (0.0000) | Acc: (39.00%) (6570/16768)
Epoch: 2 | Batch_idx: 140 |  Loss: (1.6213) |  Loss2: (0.0000) | Acc: (39.00%) (7099/18048)
Epoch: 2 | Batch_idx: 150 |  Loss: (1.6228) |  Loss2: (0.0000) | Acc: (39.00%) (7589/19328)
Epoch: 2 | Batch_idx: 160 |  Loss: (1.6219) |  Loss2: (0.0000) | Acc: (39.00%) (8093/20608)
Epoch: 2 | Batch_idx: 170 |  Loss: (1.6218) |  Loss2: (0.0000) | Acc: (39.00%) (8583/21888)
Epoch: 2 | Batch_idx: 180 |  Loss: (1.6213) |  Loss2: (0.0000) | Acc: (39.00%) (9092/23168)
Epoch: 2 | Batch_idx: 190 |  Loss: (1.6202) |  Loss2: (0.0000) | Acc: (39.00%) (9603/24448)
Epoch: 2 | Batch_idx: 200 |  Loss: (1.6199) |  Loss2: (0.0000) | Acc: (39.00%) (10103/25728)
Epoch: 2 | Batch_idx: 210 |  Loss: (1.6181) |  Loss2: (0.0000) | Acc: (39.00%) (10623/27008)
Epoch: 2 | Batch_idx: 220 |  Loss: (1.6166) |  Loss2: (0.0000) | Acc: (39.00%) (11136/28288)
Epoch: 2 | Batch_idx: 230 |  Loss: (1.6154) |  Loss2: (0.0000) | Acc: (39.00%) (11657/29568)
Epoch: 2 | Batch_idx: 240 |  Loss: (1.6137) |  Loss2: (0.0000) | Acc: (39.00%) (12207/30848)
Epoch: 2 | Batch_idx: 250 |  Loss: (1.6135) |  Loss2: (0.0000) | Acc: (39.00%) (12689/32128)
Epoch: 2 | Batch_idx: 260 |  Loss: (1.6123) |  Loss2: (0.0000) | Acc: (39.00%) (13196/33408)
Epoch: 2 | Batch_idx: 270 |  Loss: (1.6127) |  Loss2: (0.0000) | Acc: (39.00%) (13711/34688)
Epoch: 2 | Batch_idx: 280 |  Loss: (1.6114) |  Loss2: (0.0000) | Acc: (39.00%) (14243/35968)
Epoch: 2 | Batch_idx: 290 |  Loss: (1.6090) |  Loss2: (0.0000) | Acc: (39.00%) (14771/37248)
Epoch: 2 | Batch_idx: 300 |  Loss: (1.6076) |  Loss2: (0.0000) | Acc: (39.00%) (15305/38528)
Epoch: 2 | Batch_idx: 310 |  Loss: (1.6076) |  Loss2: (0.0000) | Acc: (39.00%) (15813/39808)
Epoch: 2 | Batch_idx: 320 |  Loss: (1.6070) |  Loss2: (0.0000) | Acc: (39.00%) (16346/41088)
Epoch: 2 | Batch_idx: 330 |  Loss: (1.6064) |  Loss2: (0.0000) | Acc: (39.00%) (16867/42368)
Epoch: 2 | Batch_idx: 340 |  Loss: (1.6065) |  Loss2: (0.0000) | Acc: (39.00%) (17388/43648)
Epoch: 2 | Batch_idx: 350 |  Loss: (1.6052) |  Loss2: (0.0000) | Acc: (39.00%) (17906/44928)
Epoch: 2 | Batch_idx: 360 |  Loss: (1.6044) |  Loss2: (0.0000) | Acc: (39.00%) (18411/46208)
Epoch: 2 | Batch_idx: 370 |  Loss: (1.6044) |  Loss2: (0.0000) | Acc: (39.00%) (18904/47488)
Epoch: 2 | Batch_idx: 380 |  Loss: (1.6045) |  Loss2: (0.0000) | Acc: (39.00%) (19421/48768)
Epoch: 2 | Batch_idx: 390 |  Loss: (1.6039) |  Loss2: (0.0000) | Acc: (39.00%) (19922/50000)
# TEST : Loss: (1.5499) | Acc: (41.00%) (4123/10000)
percent tensor([0.5034, 0.5053, 0.5027, 0.5023, 0.5037, 0.5032, 0.5049, 0.5033, 0.5053,
        0.5045, 0.5050, 0.5030, 0.5040, 0.5050, 0.5039, 0.5031],
       device='cuda:0') torch.Size([16])
percent tensor([0.4996, 0.4996, 0.4993, 0.5006, 0.4989, 0.5002, 0.4992, 0.4998, 0.4981,
        0.4994, 0.4984, 0.4993, 0.4991, 0.5001, 0.4999, 0.4999],
       device='cuda:0') torch.Size([16])
percent tensor([0.4908, 0.4934, 0.4914, 0.4982, 0.4858, 0.4922, 0.4879, 0.4909, 0.4798,
        0.4910, 0.4831, 0.4873, 0.4891, 0.4940, 0.4945, 0.4948],
       device='cuda:0') torch.Size([16])
percent tensor([0.5018, 0.4998, 0.4971, 0.5009, 0.4976, 0.5017, 0.4993, 0.4974, 0.4966,
        0.4993, 0.4987, 0.4975, 0.5011, 0.5007, 0.5012, 0.5005],
       device='cuda:0') torch.Size([16])
percent tensor([0.5021, 0.4981, 0.4938, 0.4996, 0.4901, 0.5023, 0.4966, 0.4933, 0.4962,
        0.4981, 0.4976, 0.4947, 0.5010, 0.5006, 0.5015, 0.5004],
       device='cuda:0') torch.Size([16])
percent tensor([0.4988, 0.4951, 0.4979, 0.4979, 0.4940, 0.4985, 0.4965, 0.4939, 0.4977,
        0.4969, 0.4973, 0.4993, 0.4989, 0.4969, 0.4971, 0.4973],
       device='cuda:0') torch.Size([16])
percent tensor([0.5093, 0.5084, 0.5138, 0.5149, 0.5156, 0.5110, 0.5095, 0.5143, 0.5084,
        0.5104, 0.5099, 0.5113, 0.5117, 0.5110, 0.5066, 0.5083],
       device='cuda:0') torch.Size([16])
percent tensor([0.6188, 0.6113, 0.6669, 0.7213, 0.7237, 0.6579, 0.6035, 0.7089, 0.6117,
        0.6159, 0.6191, 0.6018, 0.6346, 0.6290, 0.5846, 0.6187],
       device='cuda:0') torch.Size([16])
Epoch: 3 | Batch_idx: 0 |  Loss: (1.5769) |  Loss2: (0.0000) | Acc: (39.00%) (50/128)
Epoch: 3 | Batch_idx: 10 |  Loss: (1.5597) |  Loss2: (0.0000) | Acc: (41.00%) (580/1408)
Epoch: 3 | Batch_idx: 20 |  Loss: (1.5517) |  Loss2: (0.0000) | Acc: (41.00%) (1119/2688)
Epoch: 3 | Batch_idx: 30 |  Loss: (1.5683) |  Loss2: (0.0000) | Acc: (41.00%) (1638/3968)
Epoch: 3 | Batch_idx: 40 |  Loss: (1.5663) |  Loss2: (0.0000) | Acc: (41.00%) (2168/5248)
Epoch: 3 | Batch_idx: 50 |  Loss: (1.5662) |  Loss2: (0.0000) | Acc: (41.00%) (2687/6528)
Epoch: 3 | Batch_idx: 60 |  Loss: (1.5714) |  Loss2: (0.0000) | Acc: (40.00%) (3162/7808)
Epoch: 3 | Batch_idx: 70 |  Loss: (1.5754) |  Loss2: (0.0000) | Acc: (40.00%) (3672/9088)
Epoch: 3 | Batch_idx: 80 |  Loss: (1.5756) |  Loss2: (0.0000) | Acc: (40.00%) (4197/10368)
Epoch: 3 | Batch_idx: 90 |  Loss: (1.5717) |  Loss2: (0.0000) | Acc: (40.00%) (4750/11648)
Epoch: 3 | Batch_idx: 100 |  Loss: (1.5711) |  Loss2: (0.0000) | Acc: (40.00%) (5264/12928)
Epoch: 3 | Batch_idx: 110 |  Loss: (1.5763) |  Loss2: (0.0000) | Acc: (40.00%) (5768/14208)
Epoch: 3 | Batch_idx: 120 |  Loss: (1.5766) |  Loss2: (0.0000) | Acc: (40.00%) (6292/15488)
Epoch: 3 | Batch_idx: 130 |  Loss: (1.5771) |  Loss2: (0.0000) | Acc: (40.00%) (6797/16768)
Epoch: 3 | Batch_idx: 140 |  Loss: (1.5777) |  Loss2: (0.0000) | Acc: (40.00%) (7326/18048)
Epoch: 3 | Batch_idx: 150 |  Loss: (1.5772) |  Loss2: (0.0000) | Acc: (40.00%) (7846/19328)
Epoch: 3 | Batch_idx: 160 |  Loss: (1.5749) |  Loss2: (0.0000) | Acc: (40.00%) (8383/20608)
Epoch: 3 | Batch_idx: 170 |  Loss: (1.5729) |  Loss2: (0.0000) | Acc: (40.00%) (8896/21888)
Epoch: 3 | Batch_idx: 180 |  Loss: (1.5724) |  Loss2: (0.0000) | Acc: (40.00%) (9394/23168)
Epoch: 3 | Batch_idx: 190 |  Loss: (1.5710) |  Loss2: (0.0000) | Acc: (40.00%) (9937/24448)
Epoch: 3 | Batch_idx: 200 |  Loss: (1.5677) |  Loss2: (0.0000) | Acc: (40.00%) (10484/25728)
Epoch: 3 | Batch_idx: 210 |  Loss: (1.5664) |  Loss2: (0.0000) | Acc: (40.00%) (11004/27008)
Epoch: 3 | Batch_idx: 220 |  Loss: (1.5651) |  Loss2: (0.0000) | Acc: (40.00%) (11538/28288)
Epoch: 3 | Batch_idx: 230 |  Loss: (1.5647) |  Loss2: (0.0000) | Acc: (40.00%) (12056/29568)
Epoch: 3 | Batch_idx: 240 |  Loss: (1.5642) |  Loss2: (0.0000) | Acc: (40.00%) (12571/30848)
Epoch: 3 | Batch_idx: 250 |  Loss: (1.5639) |  Loss2: (0.0000) | Acc: (40.00%) (13089/32128)
Epoch: 3 | Batch_idx: 260 |  Loss: (1.5610) |  Loss2: (0.0000) | Acc: (40.00%) (13669/33408)
Epoch: 3 | Batch_idx: 270 |  Loss: (1.5611) |  Loss2: (0.0000) | Acc: (40.00%) (14174/34688)
Epoch: 3 | Batch_idx: 280 |  Loss: (1.5613) |  Loss2: (0.0000) | Acc: (40.00%) (14705/35968)
Epoch: 3 | Batch_idx: 290 |  Loss: (1.5608) |  Loss2: (0.0000) | Acc: (40.00%) (15250/37248)
Epoch: 3 | Batch_idx: 300 |  Loss: (1.5615) |  Loss2: (0.0000) | Acc: (40.00%) (15775/38528)
Epoch: 3 | Batch_idx: 310 |  Loss: (1.5620) |  Loss2: (0.0000) | Acc: (40.00%) (16313/39808)
Epoch: 3 | Batch_idx: 320 |  Loss: (1.5607) |  Loss2: (0.0000) | Acc: (41.00%) (16852/41088)
Epoch: 3 | Batch_idx: 330 |  Loss: (1.5598) |  Loss2: (0.0000) | Acc: (41.00%) (17383/42368)
Epoch: 3 | Batch_idx: 340 |  Loss: (1.5598) |  Loss2: (0.0000) | Acc: (40.00%) (17894/43648)
Epoch: 3 | Batch_idx: 350 |  Loss: (1.5601) |  Loss2: (0.0000) | Acc: (40.00%) (18403/44928)
Epoch: 3 | Batch_idx: 360 |  Loss: (1.5603) |  Loss2: (0.0000) | Acc: (40.00%) (18918/46208)
Epoch: 3 | Batch_idx: 370 |  Loss: (1.5611) |  Loss2: (0.0000) | Acc: (40.00%) (19446/47488)
Epoch: 3 | Batch_idx: 380 |  Loss: (1.5620) |  Loss2: (0.0000) | Acc: (40.00%) (19965/48768)
Epoch: 3 | Batch_idx: 390 |  Loss: (1.5615) |  Loss2: (0.0000) | Acc: (40.00%) (20492/50000)
# TEST : Loss: (1.5266) | Acc: (42.00%) (4204/10000)
percent tensor([0.5098, 0.5147, 0.5112, 0.5085, 0.5131, 0.5097, 0.5147, 0.5112, 0.5142,
        0.5136, 0.5130, 0.5119, 0.5112, 0.5138, 0.5113, 0.5094],
       device='cuda:0') torch.Size([16])
percent tensor([0.4995, 0.4995, 0.4981, 0.5007, 0.4977, 0.5002, 0.4985, 0.4992, 0.4969,
        0.4988, 0.4975, 0.4981, 0.4986, 0.5005, 0.4998, 0.5000],
       device='cuda:0') torch.Size([16])
percent tensor([0.4999, 0.4951, 0.4930, 0.5084, 0.4838, 0.5009, 0.4858, 0.4921, 0.4736,
        0.4926, 0.4835, 0.4856, 0.4939, 0.4941, 0.5020, 0.5040],
       device='cuda:0') torch.Size([16])
percent tensor([0.5081, 0.5030, 0.4916, 0.5020, 0.4937, 0.5109, 0.4994, 0.4941, 0.4959,
        0.5001, 0.5027, 0.4937, 0.5057, 0.5041, 0.5072, 0.5057],
       device='cuda:0') torch.Size([16])
percent tensor([0.5076, 0.5012, 0.4886, 0.5011, 0.4826, 0.5090, 0.4962, 0.4897, 0.4974,
        0.4994, 0.5014, 0.4911, 0.5056, 0.5050, 0.5066, 0.5044],
       device='cuda:0') torch.Size([16])
percent tensor([0.4996, 0.4949, 0.4952, 0.4943, 0.4899, 0.5004, 0.4958, 0.4898, 0.4980,
        0.4965, 0.4981, 0.4987, 0.4997, 0.4971, 0.4971, 0.4973],
       device='cuda:0') torch.Size([16])
percent tensor([0.5142, 0.5138, 0.5199, 0.5204, 0.5229, 0.5164, 0.5151, 0.5209, 0.5140,
        0.5166, 0.5163, 0.5174, 0.5185, 0.5178, 0.5099, 0.5131],
       device='cuda:0') torch.Size([16])
percent tensor([0.7951, 0.7852, 0.8327, 0.8943, 0.9009, 0.8580, 0.7637, 0.8877, 0.7964,
        0.7943, 0.8094, 0.7547, 0.8339, 0.8240, 0.7223, 0.7973],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 4 | Batch_idx: 0 |  Loss: (1.6753) |  Loss2: (0.0000) | Acc: (37.00%) (48/128)
Epoch: 4 | Batch_idx: 10 |  Loss: (1.5716) |  Loss2: (0.0000) | Acc: (42.00%) (604/1408)
Epoch: 4 | Batch_idx: 20 |  Loss: (1.5775) |  Loss2: (0.0000) | Acc: (41.00%) (1118/2688)
Epoch: 4 | Batch_idx: 30 |  Loss: (1.5604) |  Loss2: (0.0000) | Acc: (41.00%) (1644/3968)
Epoch: 4 | Batch_idx: 40 |  Loss: (1.5536) |  Loss2: (0.0000) | Acc: (41.00%) (2180/5248)
Epoch: 4 | Batch_idx: 50 |  Loss: (1.5516) |  Loss2: (0.0000) | Acc: (41.00%) (2698/6528)
Epoch: 4 | Batch_idx: 60 |  Loss: (1.5445) |  Loss2: (0.0000) | Acc: (41.00%) (3255/7808)
Epoch: 4 | Batch_idx: 70 |  Loss: (1.5386) |  Loss2: (0.0000) | Acc: (42.00%) (3820/9088)
Epoch: 4 | Batch_idx: 80 |  Loss: (1.5328) |  Loss2: (0.0000) | Acc: (42.00%) (4386/10368)
Epoch: 4 | Batch_idx: 90 |  Loss: (1.5249) |  Loss2: (0.0000) | Acc: (42.00%) (4972/11648)
Epoch: 4 | Batch_idx: 100 |  Loss: (1.5189) |  Loss2: (0.0000) | Acc: (43.00%) (5568/12928)
Epoch: 4 | Batch_idx: 110 |  Loss: (1.5173) |  Loss2: (0.0000) | Acc: (43.00%) (6115/14208)
Epoch: 4 | Batch_idx: 120 |  Loss: (1.5114) |  Loss2: (0.0000) | Acc: (43.00%) (6706/15488)
Epoch: 4 | Batch_idx: 130 |  Loss: (1.5076) |  Loss2: (0.0000) | Acc: (43.00%) (7287/16768)
Epoch: 4 | Batch_idx: 140 |  Loss: (1.5066) |  Loss2: (0.0000) | Acc: (43.00%) (7833/18048)
Epoch: 4 | Batch_idx: 150 |  Loss: (1.5028) |  Loss2: (0.0000) | Acc: (43.00%) (8410/19328)
Epoch: 4 | Batch_idx: 160 |  Loss: (1.5011) |  Loss2: (0.0000) | Acc: (43.00%) (8983/20608)
Epoch: 4 | Batch_idx: 170 |  Loss: (1.4989) |  Loss2: (0.0000) | Acc: (43.00%) (9557/21888)
Epoch: 4 | Batch_idx: 180 |  Loss: (1.4962) |  Loss2: (0.0000) | Acc: (43.00%) (10162/23168)
Epoch: 4 | Batch_idx: 190 |  Loss: (1.4916) |  Loss2: (0.0000) | Acc: (44.00%) (10776/24448)
Epoch: 4 | Batch_idx: 200 |  Loss: (1.4883) |  Loss2: (0.0000) | Acc: (44.00%) (11382/25728)
Epoch: 4 | Batch_idx: 210 |  Loss: (1.4865) |  Loss2: (0.0000) | Acc: (44.00%) (11980/27008)
Epoch: 4 | Batch_idx: 220 |  Loss: (1.4834) |  Loss2: (0.0000) | Acc: (44.00%) (12591/28288)
Epoch: 4 | Batch_idx: 230 |  Loss: (1.4818) |  Loss2: (0.0000) | Acc: (44.00%) (13184/29568)
Epoch: 4 | Batch_idx: 240 |  Loss: (1.4804) |  Loss2: (0.0000) | Acc: (44.00%) (13798/30848)
Epoch: 4 | Batch_idx: 250 |  Loss: (1.4776) |  Loss2: (0.0000) | Acc: (44.00%) (14432/32128)
Epoch: 4 | Batch_idx: 260 |  Loss: (1.4755) |  Loss2: (0.0000) | Acc: (45.00%) (15051/33408)
Epoch: 4 | Batch_idx: 270 |  Loss: (1.4728) |  Loss2: (0.0000) | Acc: (45.00%) (15687/34688)
Epoch: 4 | Batch_idx: 280 |  Loss: (1.4715) |  Loss2: (0.0000) | Acc: (45.00%) (16309/35968)
Epoch: 4 | Batch_idx: 290 |  Loss: (1.4690) |  Loss2: (0.0000) | Acc: (45.00%) (16966/37248)
Epoch: 4 | Batch_idx: 300 |  Loss: (1.4679) |  Loss2: (0.0000) | Acc: (45.00%) (17565/38528)
Epoch: 4 | Batch_idx: 310 |  Loss: (1.4657) |  Loss2: (0.0000) | Acc: (45.00%) (18219/39808)
Epoch: 4 | Batch_idx: 320 |  Loss: (1.4627) |  Loss2: (0.0000) | Acc: (45.00%) (18843/41088)
Epoch: 4 | Batch_idx: 330 |  Loss: (1.4600) |  Loss2: (0.0000) | Acc: (46.00%) (19502/42368)
Epoch: 4 | Batch_idx: 340 |  Loss: (1.4582) |  Loss2: (0.0000) | Acc: (46.00%) (20133/43648)
Epoch: 4 | Batch_idx: 350 |  Loss: (1.4563) |  Loss2: (0.0000) | Acc: (46.00%) (20753/44928)
Epoch: 4 | Batch_idx: 360 |  Loss: (1.4544) |  Loss2: (0.0000) | Acc: (46.00%) (21383/46208)
Epoch: 4 | Batch_idx: 370 |  Loss: (1.4514) |  Loss2: (0.0000) | Acc: (46.00%) (22049/47488)
Epoch: 4 | Batch_idx: 380 |  Loss: (1.4503) |  Loss2: (0.0000) | Acc: (46.00%) (22661/48768)
Epoch: 4 | Batch_idx: 390 |  Loss: (1.4476) |  Loss2: (0.0000) | Acc: (46.00%) (23309/50000)
# TEST : Loss: (1.3782) | Acc: (49.00%) (4915/10000)
percent tensor([0.5103, 0.5143, 0.5112, 0.5090, 0.5132, 0.5105, 0.5144, 0.5112, 0.5132,
        0.5136, 0.5126, 0.5122, 0.5111, 0.5121, 0.5119, 0.5102],
       device='cuda:0') torch.Size([16])
percent tensor([0.4996, 0.4991, 0.4981, 0.5003, 0.4975, 0.5008, 0.4985, 0.4991, 0.4977,
        0.4989, 0.4982, 0.4984, 0.4988, 0.5005, 0.4996, 0.5002],
       device='cuda:0') torch.Size([16])
percent tensor([0.4988, 0.4927, 0.4939, 0.5070, 0.4857, 0.4983, 0.4861, 0.4929, 0.4817,
        0.4920, 0.4884, 0.4864, 0.4943, 0.4929, 0.5003, 0.5030],
       device='cuda:0') torch.Size([16])
percent tensor([0.5070, 0.5020, 0.4937, 0.5018, 0.4963, 0.5083, 0.5004, 0.4942, 0.4982,
        0.5012, 0.5027, 0.4964, 0.5044, 0.5036, 0.5059, 0.5049],
       device='cuda:0') torch.Size([16])
percent tensor([0.5063, 0.5016, 0.4888, 0.5006, 0.4876, 0.5083, 0.4980, 0.4907, 0.4993,
        0.5013, 0.5033, 0.4924, 0.5063, 0.5019, 0.5068, 0.5041],
       device='cuda:0') torch.Size([16])
percent tensor([0.4971, 0.4967, 0.4954, 0.4951, 0.4936, 0.4975, 0.4977, 0.4908, 0.4981,
        0.4972, 0.4982, 0.4981, 0.4994, 0.4980, 0.4967, 0.4949],
       device='cuda:0') torch.Size([16])
percent tensor([0.5130, 0.5158, 0.5173, 0.5184, 0.5196, 0.5150, 0.5153, 0.5182, 0.5176,
        0.5195, 0.5212, 0.5163, 0.5195, 0.5191, 0.5111, 0.5134],
       device='cuda:0') torch.Size([16])
percent tensor([0.7812, 0.7976, 0.7861, 0.8571, 0.8483, 0.8378, 0.7640, 0.8741, 0.8564,
        0.8292, 0.8641, 0.7573, 0.8727, 0.8360, 0.7545, 0.8264],
       device='cuda:0') torch.Size([16])
Epoch: 5 | Batch_idx: 0 |  Loss: (1.4612) |  Loss2: (0.0000) | Acc: (46.00%) (59/128)
Epoch: 5 | Batch_idx: 10 |  Loss: (1.3323) |  Loss2: (0.0000) | Acc: (50.00%) (709/1408)
Epoch: 5 | Batch_idx: 20 |  Loss: (1.3394) |  Loss2: (0.0000) | Acc: (50.00%) (1361/2688)
Epoch: 5 | Batch_idx: 30 |  Loss: (1.3321) |  Loss2: (0.0000) | Acc: (51.00%) (2027/3968)
Epoch: 5 | Batch_idx: 40 |  Loss: (1.3298) |  Loss2: (0.0000) | Acc: (51.00%) (2681/5248)
Epoch: 5 | Batch_idx: 50 |  Loss: (1.3290) |  Loss2: (0.0000) | Acc: (51.00%) (3332/6528)
Epoch: 5 | Batch_idx: 60 |  Loss: (1.3273) |  Loss2: (0.0000) | Acc: (51.00%) (4005/7808)
Epoch: 5 | Batch_idx: 70 |  Loss: (1.3259) |  Loss2: (0.0000) | Acc: (51.00%) (4690/9088)
Epoch: 5 | Batch_idx: 80 |  Loss: (1.3244) |  Loss2: (0.0000) | Acc: (51.00%) (5380/10368)
Epoch: 5 | Batch_idx: 90 |  Loss: (1.3241) |  Loss2: (0.0000) | Acc: (51.00%) (6039/11648)
Epoch: 5 | Batch_idx: 100 |  Loss: (1.3190) |  Loss2: (0.0000) | Acc: (52.00%) (6752/12928)
Epoch: 5 | Batch_idx: 110 |  Loss: (1.3147) |  Loss2: (0.0000) | Acc: (52.00%) (7453/14208)
Epoch: 5 | Batch_idx: 120 |  Loss: (1.3137) |  Loss2: (0.0000) | Acc: (52.00%) (8112/15488)
Epoch: 5 | Batch_idx: 130 |  Loss: (1.3126) |  Loss2: (0.0000) | Acc: (52.00%) (8790/16768)
Epoch: 5 | Batch_idx: 140 |  Loss: (1.3132) |  Loss2: (0.0000) | Acc: (52.00%) (9451/18048)
Epoch: 5 | Batch_idx: 150 |  Loss: (1.3109) |  Loss2: (0.0000) | Acc: (52.00%) (10160/19328)
Epoch: 5 | Batch_idx: 160 |  Loss: (1.3115) |  Loss2: (0.0000) | Acc: (52.00%) (10849/20608)
Epoch: 5 | Batch_idx: 170 |  Loss: (1.3087) |  Loss2: (0.0000) | Acc: (52.00%) (11558/21888)
Epoch: 5 | Batch_idx: 180 |  Loss: (1.3086) |  Loss2: (0.0000) | Acc: (52.00%) (12263/23168)
Epoch: 5 | Batch_idx: 190 |  Loss: (1.3065) |  Loss2: (0.0000) | Acc: (53.00%) (12971/24448)
Epoch: 5 | Batch_idx: 200 |  Loss: (1.3050) |  Loss2: (0.0000) | Acc: (53.00%) (13657/25728)
Epoch: 5 | Batch_idx: 210 |  Loss: (1.3043) |  Loss2: (0.0000) | Acc: (52.00%) (14314/27008)
Epoch: 5 | Batch_idx: 220 |  Loss: (1.3020) |  Loss2: (0.0000) | Acc: (53.00%) (15018/28288)
Epoch: 5 | Batch_idx: 230 |  Loss: (1.2998) |  Loss2: (0.0000) | Acc: (53.00%) (15716/29568)
Epoch: 5 | Batch_idx: 240 |  Loss: (1.2981) |  Loss2: (0.0000) | Acc: (53.00%) (16407/30848)
Epoch: 5 | Batch_idx: 250 |  Loss: (1.2966) |  Loss2: (0.0000) | Acc: (53.00%) (17113/32128)
Epoch: 5 | Batch_idx: 260 |  Loss: (1.2946) |  Loss2: (0.0000) | Acc: (53.00%) (17808/33408)
Epoch: 5 | Batch_idx: 270 |  Loss: (1.2923) |  Loss2: (0.0000) | Acc: (53.00%) (18511/34688)
Epoch: 5 | Batch_idx: 280 |  Loss: (1.2902) |  Loss2: (0.0000) | Acc: (53.00%) (19228/35968)
Epoch: 5 | Batch_idx: 290 |  Loss: (1.2892) |  Loss2: (0.0000) | Acc: (53.00%) (19922/37248)
Epoch: 5 | Batch_idx: 300 |  Loss: (1.2883) |  Loss2: (0.0000) | Acc: (53.00%) (20618/38528)
Epoch: 5 | Batch_idx: 310 |  Loss: (1.2869) |  Loss2: (0.0000) | Acc: (53.00%) (21333/39808)
Epoch: 5 | Batch_idx: 320 |  Loss: (1.2855) |  Loss2: (0.0000) | Acc: (53.00%) (22029/41088)
Epoch: 5 | Batch_idx: 330 |  Loss: (1.2839) |  Loss2: (0.0000) | Acc: (53.00%) (22732/42368)
Epoch: 5 | Batch_idx: 340 |  Loss: (1.2820) |  Loss2: (0.0000) | Acc: (53.00%) (23441/43648)
Epoch: 5 | Batch_idx: 350 |  Loss: (1.2808) |  Loss2: (0.0000) | Acc: (53.00%) (24137/44928)
Epoch: 5 | Batch_idx: 360 |  Loss: (1.2781) |  Loss2: (0.0000) | Acc: (53.00%) (24853/46208)
Epoch: 5 | Batch_idx: 370 |  Loss: (1.2774) |  Loss2: (0.0000) | Acc: (53.00%) (25544/47488)
Epoch: 5 | Batch_idx: 380 |  Loss: (1.2751) |  Loss2: (0.0000) | Acc: (53.00%) (26262/48768)
Epoch: 5 | Batch_idx: 390 |  Loss: (1.2737) |  Loss2: (0.0000) | Acc: (53.00%) (26964/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_005.pth.tar'
# TEST : Loss: (1.2991) | Acc: (53.00%) (5332/10000)
percent tensor([0.5106, 0.5147, 0.5105, 0.5088, 0.5127, 0.5105, 0.5145, 0.5108, 0.5129,
        0.5139, 0.5127, 0.5120, 0.5115, 0.5128, 0.5120, 0.5110],
       device='cuda:0') torch.Size([16])
percent tensor([0.4997, 0.4991, 0.4986, 0.5007, 0.4978, 0.5011, 0.4986, 0.4991, 0.4978,
        0.4990, 0.4984, 0.4986, 0.4987, 0.5002, 0.4998, 0.5004],
       device='cuda:0') torch.Size([16])
percent tensor([0.4972, 0.4920, 0.4950, 0.5048, 0.4872, 0.4982, 0.4872, 0.4903, 0.4817,
        0.4917, 0.4884, 0.4912, 0.4933, 0.4931, 0.4987, 0.5005],
       device='cuda:0') torch.Size([16])
percent tensor([0.5056, 0.5004, 0.4941, 0.4996, 0.4953, 0.5077, 0.4986, 0.4938, 0.4978,
        0.5004, 0.5012, 0.4984, 0.5026, 0.5017, 0.5048, 0.5039],
       device='cuda:0') torch.Size([16])
percent tensor([0.5032, 0.4994, 0.4919, 0.4990, 0.4879, 0.5061, 0.4954, 0.4897, 0.4973,
        0.4990, 0.5006, 0.4962, 0.5039, 0.5004, 0.5055, 0.5017],
       device='cuda:0') torch.Size([16])
percent tensor([0.4947, 0.4934, 0.4948, 0.4948, 0.4934, 0.4962, 0.4951, 0.4884, 0.4971,
        0.4935, 0.4954, 0.4972, 0.4980, 0.4961, 0.4941, 0.4918],
       device='cuda:0') torch.Size([16])
percent tensor([0.5134, 0.5152, 0.5157, 0.5163, 0.5190, 0.5155, 0.5143, 0.5181, 0.5140,
        0.5189, 0.5173, 0.5165, 0.5176, 0.5163, 0.5107, 0.5155],
       device='cuda:0') torch.Size([16])
percent tensor([0.8044, 0.8100, 0.7888, 0.8407, 0.8450, 0.8423, 0.7529, 0.8717, 0.7981,
        0.8547, 0.8063, 0.7684, 0.8317, 0.8073, 0.7663, 0.9009],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 6 | Batch_idx: 0 |  Loss: (1.1792) |  Loss2: (0.0000) | Acc: (59.00%) (76/128)
Epoch: 6 | Batch_idx: 10 |  Loss: (1.2232) |  Loss2: (0.0000) | Acc: (57.00%) (812/1408)
Epoch: 6 | Batch_idx: 20 |  Loss: (1.2246) |  Loss2: (0.0000) | Acc: (56.00%) (1526/2688)
Epoch: 6 | Batch_idx: 30 |  Loss: (1.2458) |  Loss2: (0.0000) | Acc: (55.00%) (2195/3968)
Epoch: 6 | Batch_idx: 40 |  Loss: (1.2657) |  Loss2: (0.0000) | Acc: (54.00%) (2858/5248)
Epoch: 6 | Batch_idx: 50 |  Loss: (1.2677) |  Loss2: (0.0000) | Acc: (54.00%) (3549/6528)
Epoch: 6 | Batch_idx: 60 |  Loss: (1.2722) |  Loss2: (0.0000) | Acc: (54.00%) (4226/7808)
Epoch: 6 | Batch_idx: 70 |  Loss: (1.2739) |  Loss2: (0.0000) | Acc: (53.00%) (4891/9088)
Epoch: 6 | Batch_idx: 80 |  Loss: (1.2752) |  Loss2: (0.0000) | Acc: (53.00%) (5575/10368)
Epoch: 6 | Batch_idx: 90 |  Loss: (1.2748) |  Loss2: (0.0000) | Acc: (53.00%) (6246/11648)
Epoch: 6 | Batch_idx: 100 |  Loss: (1.2741) |  Loss2: (0.0000) | Acc: (53.00%) (6943/12928)
Epoch: 6 | Batch_idx: 110 |  Loss: (1.2780) |  Loss2: (0.0000) | Acc: (53.00%) (7604/14208)
Epoch: 6 | Batch_idx: 120 |  Loss: (1.2775) |  Loss2: (0.0000) | Acc: (53.00%) (8304/15488)
Epoch: 6 | Batch_idx: 130 |  Loss: (1.2763) |  Loss2: (0.0000) | Acc: (53.00%) (8997/16768)
Epoch: 6 | Batch_idx: 140 |  Loss: (1.2726) |  Loss2: (0.0000) | Acc: (53.00%) (9729/18048)
Epoch: 6 | Batch_idx: 150 |  Loss: (1.2718) |  Loss2: (0.0000) | Acc: (53.00%) (10412/19328)
Epoch: 6 | Batch_idx: 160 |  Loss: (1.2711) |  Loss2: (0.0000) | Acc: (53.00%) (11092/20608)
Epoch: 6 | Batch_idx: 170 |  Loss: (1.2688) |  Loss2: (0.0000) | Acc: (53.00%) (11798/21888)
Epoch: 6 | Batch_idx: 180 |  Loss: (1.2715) |  Loss2: (0.0000) | Acc: (53.00%) (12450/23168)
Epoch: 6 | Batch_idx: 190 |  Loss: (1.2689) |  Loss2: (0.0000) | Acc: (53.00%) (13156/24448)
Epoch: 6 | Batch_idx: 200 |  Loss: (1.2679) |  Loss2: (0.0000) | Acc: (53.00%) (13852/25728)
Epoch: 6 | Batch_idx: 210 |  Loss: (1.2663) |  Loss2: (0.0000) | Acc: (53.00%) (14582/27008)
Epoch: 6 | Batch_idx: 220 |  Loss: (1.2664) |  Loss2: (0.0000) | Acc: (53.00%) (15265/28288)
Epoch: 6 | Batch_idx: 230 |  Loss: (1.2651) |  Loss2: (0.0000) | Acc: (53.00%) (15953/29568)
Epoch: 6 | Batch_idx: 240 |  Loss: (1.2632) |  Loss2: (0.0000) | Acc: (53.00%) (16655/30848)
Epoch: 6 | Batch_idx: 250 |  Loss: (1.2633) |  Loss2: (0.0000) | Acc: (54.00%) (17352/32128)
Epoch: 6 | Batch_idx: 260 |  Loss: (1.2609) |  Loss2: (0.0000) | Acc: (54.00%) (18063/33408)
Epoch: 6 | Batch_idx: 270 |  Loss: (1.2595) |  Loss2: (0.0000) | Acc: (54.00%) (18764/34688)
Epoch: 6 | Batch_idx: 280 |  Loss: (1.2584) |  Loss2: (0.0000) | Acc: (54.00%) (19460/35968)
Epoch: 6 | Batch_idx: 290 |  Loss: (1.2570) |  Loss2: (0.0000) | Acc: (54.00%) (20173/37248)
Epoch: 6 | Batch_idx: 300 |  Loss: (1.2568) |  Loss2: (0.0000) | Acc: (54.00%) (20865/38528)
Epoch: 6 | Batch_idx: 310 |  Loss: (1.2539) |  Loss2: (0.0000) | Acc: (54.00%) (21591/39808)
Epoch: 6 | Batch_idx: 320 |  Loss: (1.2529) |  Loss2: (0.0000) | Acc: (54.00%) (22312/41088)
Epoch: 6 | Batch_idx: 330 |  Loss: (1.2515) |  Loss2: (0.0000) | Acc: (54.00%) (23046/42368)
Epoch: 6 | Batch_idx: 340 |  Loss: (1.2484) |  Loss2: (0.0000) | Acc: (54.00%) (23786/43648)
Epoch: 6 | Batch_idx: 350 |  Loss: (1.2473) |  Loss2: (0.0000) | Acc: (54.00%) (24477/44928)
Epoch: 6 | Batch_idx: 360 |  Loss: (1.2473) |  Loss2: (0.0000) | Acc: (54.00%) (25157/46208)
Epoch: 6 | Batch_idx: 370 |  Loss: (1.2459) |  Loss2: (0.0000) | Acc: (54.00%) (25864/47488)
Epoch: 6 | Batch_idx: 380 |  Loss: (1.2446) |  Loss2: (0.0000) | Acc: (54.00%) (26555/48768)
Epoch: 6 | Batch_idx: 390 |  Loss: (1.2434) |  Loss2: (0.0000) | Acc: (54.00%) (27285/50000)
# TEST : Loss: (1.2075) | Acc: (56.00%) (5617/10000)
percent tensor([0.5179, 0.5246, 0.5193, 0.5149, 0.5225, 0.5191, 0.5249, 0.5182, 0.5209,
        0.5233, 0.5204, 0.5215, 0.5188, 0.5217, 0.5202, 0.5186],
       device='cuda:0') torch.Size([16])
percent tensor([0.4992, 0.4989, 0.4961, 0.4999, 0.4957, 0.5004, 0.4978, 0.4975, 0.4971,
        0.4984, 0.4983, 0.4966, 0.4984, 0.5009, 0.4994, 0.5002],
       device='cuda:0') torch.Size([16])
percent tensor([0.4957, 0.4837, 0.4631, 0.4943, 0.4522, 0.4862, 0.4651, 0.4646, 0.4683,
        0.4750, 0.4817, 0.4621, 0.4932, 0.4939, 0.4873, 0.4951],
       device='cuda:0') torch.Size([16])
percent tensor([0.4910, 0.4858, 0.4790, 0.4829, 0.4778, 0.4875, 0.4828, 0.4787, 0.4867,
        0.4854, 0.4877, 0.4854, 0.4914, 0.4933, 0.4878, 0.4867],
       device='cuda:0') torch.Size([16])
percent tensor([0.5102, 0.5058, 0.4944, 0.5028, 0.4907, 0.5079, 0.5013, 0.4945, 0.5044,
        0.5047, 0.5077, 0.5007, 0.5116, 0.5087, 0.5098, 0.5082],
       device='cuda:0') torch.Size([16])
percent tensor([0.5069, 0.5054, 0.5073, 0.5061, 0.5076, 0.5070, 0.5068, 0.5066, 0.5069,
        0.5059, 0.5062, 0.5075, 0.5087, 0.5059, 0.5059, 0.5083],
       device='cuda:0') torch.Size([16])
percent tensor([0.5208, 0.5247, 0.5413, 0.5476, 0.5551, 0.5231, 0.5290, 0.5572, 0.5223,
        0.5354, 0.5293, 0.5329, 0.5277, 0.5249, 0.5191, 0.5264],
       device='cuda:0') torch.Size([16])
percent tensor([0.9042, 0.9195, 0.9252, 0.9632, 0.9623, 0.9440, 0.8949, 0.9724, 0.9318,
        0.9504, 0.9348, 0.9037, 0.9567, 0.9406, 0.8963, 0.9674],
       device='cuda:0') torch.Size([16])
Epoch: 7 | Batch_idx: 0 |  Loss: (1.1012) |  Loss2: (0.0000) | Acc: (56.00%) (72/128)
Epoch: 7 | Batch_idx: 10 |  Loss: (1.2059) |  Loss2: (0.0000) | Acc: (56.00%) (793/1408)
Epoch: 7 | Batch_idx: 20 |  Loss: (1.1827) |  Loss2: (0.0000) | Acc: (57.00%) (1539/2688)
Epoch: 7 | Batch_idx: 30 |  Loss: (1.1993) |  Loss2: (0.0000) | Acc: (56.00%) (2243/3968)
Epoch: 7 | Batch_idx: 40 |  Loss: (1.1935) |  Loss2: (0.0000) | Acc: (56.00%) (2973/5248)
Epoch: 7 | Batch_idx: 50 |  Loss: (1.1956) |  Loss2: (0.0000) | Acc: (56.00%) (3689/6528)
Epoch: 7 | Batch_idx: 60 |  Loss: (1.2031) |  Loss2: (0.0000) | Acc: (56.00%) (4391/7808)
Epoch: 7 | Batch_idx: 70 |  Loss: (1.1989) |  Loss2: (0.0000) | Acc: (56.00%) (5131/9088)
Epoch: 7 | Batch_idx: 80 |  Loss: (1.1959) |  Loss2: (0.0000) | Acc: (56.00%) (5849/10368)
Epoch: 7 | Batch_idx: 90 |  Loss: (1.1970) |  Loss2: (0.0000) | Acc: (56.00%) (6552/11648)
Epoch: 7 | Batch_idx: 100 |  Loss: (1.1964) |  Loss2: (0.0000) | Acc: (56.00%) (7298/12928)
Epoch: 7 | Batch_idx: 110 |  Loss: (1.1968) |  Loss2: (0.0000) | Acc: (56.00%) (8013/14208)
Epoch: 7 | Batch_idx: 120 |  Loss: (1.1978) |  Loss2: (0.0000) | Acc: (56.00%) (8730/15488)
Epoch: 7 | Batch_idx: 130 |  Loss: (1.2012) |  Loss2: (0.0000) | Acc: (56.00%) (9449/16768)
Epoch: 7 | Batch_idx: 140 |  Loss: (1.2026) |  Loss2: (0.0000) | Acc: (56.00%) (10172/18048)
Epoch: 7 | Batch_idx: 150 |  Loss: (1.2024) |  Loss2: (0.0000) | Acc: (56.00%) (10901/19328)
Epoch: 7 | Batch_idx: 160 |  Loss: (1.2025) |  Loss2: (0.0000) | Acc: (56.00%) (11634/20608)
Epoch: 7 | Batch_idx: 170 |  Loss: (1.2003) |  Loss2: (0.0000) | Acc: (56.00%) (12382/21888)
Epoch: 7 | Batch_idx: 180 |  Loss: (1.1983) |  Loss2: (0.0000) | Acc: (56.00%) (13113/23168)
Epoch: 7 | Batch_idx: 190 |  Loss: (1.1980) |  Loss2: (0.0000) | Acc: (56.00%) (13826/24448)
Epoch: 7 | Batch_idx: 200 |  Loss: (1.1967) |  Loss2: (0.0000) | Acc: (56.00%) (14544/25728)
Epoch: 7 | Batch_idx: 210 |  Loss: (1.1954) |  Loss2: (0.0000) | Acc: (56.00%) (15266/27008)
Epoch: 7 | Batch_idx: 220 |  Loss: (1.1944) |  Loss2: (0.0000) | Acc: (56.00%) (16008/28288)
Epoch: 7 | Batch_idx: 230 |  Loss: (1.1948) |  Loss2: (0.0000) | Acc: (56.00%) (16728/29568)
Epoch: 7 | Batch_idx: 240 |  Loss: (1.1939) |  Loss2: (0.0000) | Acc: (56.00%) (17442/30848)
Epoch: 7 | Batch_idx: 250 |  Loss: (1.1929) |  Loss2: (0.0000) | Acc: (56.00%) (18170/32128)
Epoch: 7 | Batch_idx: 260 |  Loss: (1.1920) |  Loss2: (0.0000) | Acc: (56.00%) (18907/33408)
Epoch: 7 | Batch_idx: 270 |  Loss: (1.1921) |  Loss2: (0.0000) | Acc: (56.00%) (19630/34688)
Epoch: 7 | Batch_idx: 280 |  Loss: (1.1933) |  Loss2: (0.0000) | Acc: (56.00%) (20326/35968)
Epoch: 7 | Batch_idx: 290 |  Loss: (1.1941) |  Loss2: (0.0000) | Acc: (56.00%) (21043/37248)
Epoch: 7 | Batch_idx: 300 |  Loss: (1.1943) |  Loss2: (0.0000) | Acc: (56.00%) (21777/38528)
Epoch: 7 | Batch_idx: 310 |  Loss: (1.1934) |  Loss2: (0.0000) | Acc: (56.00%) (22530/39808)
Epoch: 7 | Batch_idx: 320 |  Loss: (1.1938) |  Loss2: (0.0000) | Acc: (56.00%) (23247/41088)
Epoch: 7 | Batch_idx: 330 |  Loss: (1.1934) |  Loss2: (0.0000) | Acc: (56.00%) (23964/42368)
Epoch: 7 | Batch_idx: 340 |  Loss: (1.1939) |  Loss2: (0.0000) | Acc: (56.00%) (24656/43648)
Epoch: 7 | Batch_idx: 350 |  Loss: (1.1945) |  Loss2: (0.0000) | Acc: (56.00%) (25359/44928)
Epoch: 7 | Batch_idx: 360 |  Loss: (1.1944) |  Loss2: (0.0000) | Acc: (56.00%) (26069/46208)
Epoch: 7 | Batch_idx: 370 |  Loss: (1.1938) |  Loss2: (0.0000) | Acc: (56.00%) (26807/47488)
Epoch: 7 | Batch_idx: 380 |  Loss: (1.1933) |  Loss2: (0.0000) | Acc: (56.00%) (27528/48768)
Epoch: 7 | Batch_idx: 390 |  Loss: (1.1923) |  Loss2: (0.0000) | Acc: (56.00%) (28227/50000)
# TEST : Loss: (1.1805) | Acc: (57.00%) (5708/10000)
percent tensor([0.5185, 0.5257, 0.5192, 0.5147, 0.5230, 0.5205, 0.5259, 0.5180, 0.5215,
        0.5240, 0.5212, 0.5219, 0.5192, 0.5228, 0.5212, 0.5193],
       device='cuda:0') torch.Size([16])
percent tensor([0.5000, 0.4998, 0.4970, 0.5010, 0.4965, 0.5013, 0.4986, 0.4984, 0.4979,
        0.4992, 0.4990, 0.4975, 0.4991, 0.5020, 0.5003, 0.5012],
       device='cuda:0') torch.Size([16])
percent tensor([0.5018, 0.4851, 0.4656, 0.4966, 0.4510, 0.4852, 0.4653, 0.4670, 0.4715,
        0.4780, 0.4853, 0.4649, 0.5001, 0.4968, 0.4873, 0.5003],
       device='cuda:0') torch.Size([16])
percent tensor([0.4829, 0.4782, 0.4769, 0.4783, 0.4729, 0.4774, 0.4768, 0.4752, 0.4815,
        0.4785, 0.4798, 0.4824, 0.4841, 0.4882, 0.4790, 0.4778],
       device='cuda:0') torch.Size([16])
percent tensor([0.5102, 0.5063, 0.4983, 0.5049, 0.4949, 0.5084, 0.5034, 0.4985, 0.5051,
        0.5056, 0.5079, 0.5035, 0.5119, 0.5094, 0.5099, 0.5088],
       device='cuda:0') torch.Size([16])
percent tensor([0.5163, 0.5152, 0.5206, 0.5185, 0.5231, 0.5167, 0.5179, 0.5240, 0.5151,
        0.5160, 0.5144, 0.5179, 0.5171, 0.5133, 0.5166, 0.5214],
       device='cuda:0') torch.Size([16])
percent tensor([0.5258, 0.5322, 0.5733, 0.5819, 0.5957, 0.5281, 0.5428, 0.6061, 0.5276,
        0.5519, 0.5370, 0.5512, 0.5323, 0.5294, 0.5266, 0.5402],
       device='cuda:0') torch.Size([16])
percent tensor([0.9493, 0.9551, 0.9638, 0.9846, 0.9847, 0.9744, 0.9386, 0.9915, 0.9639,
        0.9777, 0.9668, 0.9457, 0.9798, 0.9697, 0.9411, 0.9907],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 8 | Batch_idx: 0 |  Loss: (1.0766) |  Loss2: (0.0000) | Acc: (61.00%) (79/128)
Epoch: 8 | Batch_idx: 10 |  Loss: (1.1483) |  Loss2: (0.0000) | Acc: (57.00%) (804/1408)
Epoch: 8 | Batch_idx: 20 |  Loss: (1.1405) |  Loss2: (0.0000) | Acc: (57.00%) (1555/2688)
Epoch: 8 | Batch_idx: 30 |  Loss: (1.1561) |  Loss2: (0.0000) | Acc: (57.00%) (2263/3968)
Epoch: 8 | Batch_idx: 40 |  Loss: (1.1614) |  Loss2: (0.0000) | Acc: (57.00%) (2994/5248)
Epoch: 8 | Batch_idx: 50 |  Loss: (1.1574) |  Loss2: (0.0000) | Acc: (57.00%) (3749/6528)
Epoch: 8 | Batch_idx: 60 |  Loss: (1.1621) |  Loss2: (0.0000) | Acc: (57.00%) (4471/7808)
Epoch: 8 | Batch_idx: 70 |  Loss: (1.1640) |  Loss2: (0.0000) | Acc: (57.00%) (5208/9088)
Epoch: 8 | Batch_idx: 80 |  Loss: (1.1662) |  Loss2: (0.0000) | Acc: (57.00%) (5943/10368)
Epoch: 8 | Batch_idx: 90 |  Loss: (1.1652) |  Loss2: (0.0000) | Acc: (57.00%) (6680/11648)
Epoch: 8 | Batch_idx: 100 |  Loss: (1.1633) |  Loss2: (0.0000) | Acc: (57.00%) (7428/12928)
Epoch: 8 | Batch_idx: 110 |  Loss: (1.1637) |  Loss2: (0.0000) | Acc: (57.00%) (8164/14208)
Epoch: 8 | Batch_idx: 120 |  Loss: (1.1663) |  Loss2: (0.0000) | Acc: (57.00%) (8901/15488)
Epoch: 8 | Batch_idx: 130 |  Loss: (1.1677) |  Loss2: (0.0000) | Acc: (57.00%) (9639/16768)
Epoch: 8 | Batch_idx: 140 |  Loss: (1.1662) |  Loss2: (0.0000) | Acc: (57.00%) (10412/18048)
Epoch: 8 | Batch_idx: 150 |  Loss: (1.1636) |  Loss2: (0.0000) | Acc: (57.00%) (11167/19328)
Epoch: 8 | Batch_idx: 160 |  Loss: (1.1641) |  Loss2: (0.0000) | Acc: (57.00%) (11911/20608)
Epoch: 8 | Batch_idx: 170 |  Loss: (1.1610) |  Loss2: (0.0000) | Acc: (57.00%) (12677/21888)
Epoch: 8 | Batch_idx: 180 |  Loss: (1.1609) |  Loss2: (0.0000) | Acc: (57.00%) (13429/23168)
Epoch: 8 | Batch_idx: 190 |  Loss: (1.1600) |  Loss2: (0.0000) | Acc: (57.00%) (14172/24448)
Epoch: 8 | Batch_idx: 200 |  Loss: (1.1594) |  Loss2: (0.0000) | Acc: (57.00%) (14917/25728)
Epoch: 8 | Batch_idx: 210 |  Loss: (1.1575) |  Loss2: (0.0000) | Acc: (58.00%) (15687/27008)
Epoch: 8 | Batch_idx: 220 |  Loss: (1.1580) |  Loss2: (0.0000) | Acc: (58.00%) (16434/28288)
Epoch: 8 | Batch_idx: 230 |  Loss: (1.1543) |  Loss2: (0.0000) | Acc: (58.00%) (17222/29568)
Epoch: 8 | Batch_idx: 240 |  Loss: (1.1538) |  Loss2: (0.0000) | Acc: (58.00%) (17958/30848)
Epoch: 8 | Batch_idx: 250 |  Loss: (1.1521) |  Loss2: (0.0000) | Acc: (58.00%) (18737/32128)
Epoch: 8 | Batch_idx: 260 |  Loss: (1.1515) |  Loss2: (0.0000) | Acc: (58.00%) (19493/33408)
Epoch: 8 | Batch_idx: 270 |  Loss: (1.1508) |  Loss2: (0.0000) | Acc: (58.00%) (20234/34688)
Epoch: 8 | Batch_idx: 280 |  Loss: (1.1482) |  Loss2: (0.0000) | Acc: (58.00%) (21012/35968)
Epoch: 8 | Batch_idx: 290 |  Loss: (1.1458) |  Loss2: (0.0000) | Acc: (58.00%) (21786/37248)
Epoch: 8 | Batch_idx: 300 |  Loss: (1.1442) |  Loss2: (0.0000) | Acc: (58.00%) (22539/38528)
Epoch: 8 | Batch_idx: 310 |  Loss: (1.1413) |  Loss2: (0.0000) | Acc: (58.00%) (23350/39808)
Epoch: 8 | Batch_idx: 320 |  Loss: (1.1394) |  Loss2: (0.0000) | Acc: (58.00%) (24126/41088)
Epoch: 8 | Batch_idx: 330 |  Loss: (1.1376) |  Loss2: (0.0000) | Acc: (58.00%) (24912/42368)
Epoch: 8 | Batch_idx: 340 |  Loss: (1.1360) |  Loss2: (0.0000) | Acc: (58.00%) (25709/43648)
Epoch: 8 | Batch_idx: 350 |  Loss: (1.1344) |  Loss2: (0.0000) | Acc: (59.00%) (26512/44928)
Epoch: 8 | Batch_idx: 360 |  Loss: (1.1328) |  Loss2: (0.0000) | Acc: (59.00%) (27297/46208)
Epoch: 8 | Batch_idx: 370 |  Loss: (1.1305) |  Loss2: (0.0000) | Acc: (59.00%) (28094/47488)
Epoch: 8 | Batch_idx: 380 |  Loss: (1.1294) |  Loss2: (0.0000) | Acc: (59.00%) (28874/48768)
Epoch: 8 | Batch_idx: 390 |  Loss: (1.1269) |  Loss2: (0.0000) | Acc: (59.00%) (29658/50000)
# TEST : Loss: (1.1832) | Acc: (57.00%) (5748/10000)
percent tensor([0.5175, 0.5252, 0.5197, 0.5150, 0.5228, 0.5196, 0.5257, 0.5179, 0.5211,
        0.5238, 0.5204, 0.5224, 0.5185, 0.5228, 0.5205, 0.5185],
       device='cuda:0') torch.Size([16])
percent tensor([0.5000, 0.4996, 0.4984, 0.5009, 0.4974, 0.5020, 0.4987, 0.4988, 0.4983,
        0.4992, 0.4993, 0.4979, 0.4990, 0.5011, 0.5004, 0.5011],
       device='cuda:0') torch.Size([16])
percent tensor([0.4980, 0.4835, 0.4713, 0.4927, 0.4555, 0.4905, 0.4648, 0.4690, 0.4731,
        0.4778, 0.4854, 0.4672, 0.4964, 0.4913, 0.4860, 0.4973],
       device='cuda:0') torch.Size([16])
percent tensor([0.4826, 0.4777, 0.4754, 0.4779, 0.4744, 0.4805, 0.4766, 0.4732, 0.4803,
        0.4771, 0.4788, 0.4810, 0.4833, 0.4866, 0.4788, 0.4778],
       device='cuda:0') torch.Size([16])
percent tensor([0.5085, 0.5070, 0.4932, 0.5024, 0.4897, 0.5086, 0.5029, 0.4958, 0.5051,
        0.5057, 0.5087, 0.4990, 0.5115, 0.5098, 0.5099, 0.5086],
       device='cuda:0') torch.Size([16])
percent tensor([0.5177, 0.5148, 0.5232, 0.5213, 0.5253, 0.5187, 0.5192, 0.5260, 0.5179,
        0.5169, 0.5165, 0.5207, 0.5181, 0.5159, 0.5180, 0.5214],
       device='cuda:0') torch.Size([16])
percent tensor([0.5280, 0.5325, 0.5776, 0.5740, 0.6024, 0.5266, 0.5451, 0.6149, 0.5284,
        0.5538, 0.5349, 0.5576, 0.5331, 0.5251, 0.5334, 0.5372],
       device='cuda:0') torch.Size([16])
percent tensor([0.9521, 0.9483, 0.9739, 0.9819, 0.9886, 0.9744, 0.9373, 0.9934, 0.9668,
        0.9793, 0.9671, 0.9511, 0.9832, 0.9567, 0.9606, 0.9841],
       device='cuda:0') torch.Size([16])
Epoch: 9 | Batch_idx: 0 |  Loss: (1.0265) |  Loss2: (0.0000) | Acc: (62.00%) (80/128)
Epoch: 9 | Batch_idx: 10 |  Loss: (1.0760) |  Loss2: (0.0000) | Acc: (60.00%) (857/1408)
Epoch: 9 | Batch_idx: 20 |  Loss: (1.0641) |  Loss2: (0.0000) | Acc: (61.00%) (1657/2688)
Epoch: 9 | Batch_idx: 30 |  Loss: (1.0698) |  Loss2: (0.0000) | Acc: (61.00%) (2437/3968)
Epoch: 9 | Batch_idx: 40 |  Loss: (1.0731) |  Loss2: (0.0000) | Acc: (60.00%) (3186/5248)
Epoch: 9 | Batch_idx: 50 |  Loss: (1.0614) |  Loss2: (0.0000) | Acc: (61.00%) (3991/6528)
Epoch: 9 | Batch_idx: 60 |  Loss: (1.0579) |  Loss2: (0.0000) | Acc: (61.00%) (4784/7808)
Epoch: 9 | Batch_idx: 70 |  Loss: (1.0545) |  Loss2: (0.0000) | Acc: (61.00%) (5619/9088)
Epoch: 9 | Batch_idx: 80 |  Loss: (1.0489) |  Loss2: (0.0000) | Acc: (62.00%) (6438/10368)
Epoch: 9 | Batch_idx: 90 |  Loss: (1.0508) |  Loss2: (0.0000) | Acc: (62.00%) (7233/11648)
Epoch: 9 | Batch_idx: 100 |  Loss: (1.0463) |  Loss2: (0.0000) | Acc: (62.00%) (8070/12928)
Epoch: 9 | Batch_idx: 110 |  Loss: (1.0486) |  Loss2: (0.0000) | Acc: (62.00%) (8856/14208)
Epoch: 9 | Batch_idx: 120 |  Loss: (1.0480) |  Loss2: (0.0000) | Acc: (62.00%) (9667/15488)
Epoch: 9 | Batch_idx: 130 |  Loss: (1.0469) |  Loss2: (0.0000) | Acc: (62.00%) (10465/16768)
Epoch: 9 | Batch_idx: 140 |  Loss: (1.0460) |  Loss2: (0.0000) | Acc: (62.00%) (11274/18048)
Epoch: 9 | Batch_idx: 150 |  Loss: (1.0475) |  Loss2: (0.0000) | Acc: (62.00%) (12071/19328)
Epoch: 9 | Batch_idx: 160 |  Loss: (1.0479) |  Loss2: (0.0000) | Acc: (62.00%) (12889/20608)
Epoch: 9 | Batch_idx: 170 |  Loss: (1.0463) |  Loss2: (0.0000) | Acc: (62.00%) (13703/21888)
Epoch: 9 | Batch_idx: 180 |  Loss: (1.0442) |  Loss2: (0.0000) | Acc: (62.00%) (14527/23168)
Epoch: 9 | Batch_idx: 190 |  Loss: (1.0433) |  Loss2: (0.0000) | Acc: (62.00%) (15340/24448)
Epoch: 9 | Batch_idx: 200 |  Loss: (1.0442) |  Loss2: (0.0000) | Acc: (62.00%) (16141/25728)
Epoch: 9 | Batch_idx: 210 |  Loss: (1.0441) |  Loss2: (0.0000) | Acc: (62.00%) (16938/27008)
Epoch: 9 | Batch_idx: 220 |  Loss: (1.0445) |  Loss2: (0.0000) | Acc: (62.00%) (17732/28288)
Epoch: 9 | Batch_idx: 230 |  Loss: (1.0452) |  Loss2: (0.0000) | Acc: (62.00%) (18524/29568)
Epoch: 9 | Batch_idx: 240 |  Loss: (1.0435) |  Loss2: (0.0000) | Acc: (62.00%) (19333/30848)
Epoch: 9 | Batch_idx: 250 |  Loss: (1.0422) |  Loss2: (0.0000) | Acc: (62.00%) (20180/32128)
Epoch: 9 | Batch_idx: 260 |  Loss: (1.0405) |  Loss2: (0.0000) | Acc: (62.00%) (20997/33408)
Epoch: 9 | Batch_idx: 270 |  Loss: (1.0400) |  Loss2: (0.0000) | Acc: (62.00%) (21820/34688)
Epoch: 9 | Batch_idx: 280 |  Loss: (1.0400) |  Loss2: (0.0000) | Acc: (62.00%) (22638/35968)
Epoch: 9 | Batch_idx: 290 |  Loss: (1.0385) |  Loss2: (0.0000) | Acc: (62.00%) (23455/37248)
Epoch: 9 | Batch_idx: 300 |  Loss: (1.0364) |  Loss2: (0.0000) | Acc: (63.00%) (24304/38528)
Epoch: 9 | Batch_idx: 310 |  Loss: (1.0355) |  Loss2: (0.0000) | Acc: (63.00%) (25132/39808)
Epoch: 9 | Batch_idx: 320 |  Loss: (1.0344) |  Loss2: (0.0000) | Acc: (63.00%) (25965/41088)
Epoch: 9 | Batch_idx: 330 |  Loss: (1.0324) |  Loss2: (0.0000) | Acc: (63.00%) (26797/42368)
Epoch: 9 | Batch_idx: 340 |  Loss: (1.0303) |  Loss2: (0.0000) | Acc: (63.00%) (27646/43648)
Epoch: 9 | Batch_idx: 350 |  Loss: (1.0293) |  Loss2: (0.0000) | Acc: (63.00%) (28467/44928)
Epoch: 9 | Batch_idx: 360 |  Loss: (1.0271) |  Loss2: (0.0000) | Acc: (63.00%) (29319/46208)
Epoch: 9 | Batch_idx: 370 |  Loss: (1.0258) |  Loss2: (0.0000) | Acc: (63.00%) (30152/47488)
Epoch: 9 | Batch_idx: 380 |  Loss: (1.0252) |  Loss2: (0.0000) | Acc: (63.00%) (30961/48768)
Epoch: 9 | Batch_idx: 390 |  Loss: (1.0244) |  Loss2: (0.0000) | Acc: (63.00%) (31760/50000)
# TEST : Loss: (1.0378) | Acc: (62.00%) (6247/10000)
percent tensor([0.5175, 0.5243, 0.5189, 0.5137, 0.5220, 0.5193, 0.5251, 0.5172, 0.5204,
        0.5232, 0.5203, 0.5217, 0.5181, 0.5211, 0.5202, 0.5181],
       device='cuda:0') torch.Size([16])
percent tensor([0.5002, 0.4995, 0.4978, 0.5007, 0.4973, 0.5025, 0.4986, 0.4984, 0.4987,
        0.4990, 0.4995, 0.4977, 0.4994, 0.5011, 0.5003, 0.5012],
       device='cuda:0') torch.Size([16])
percent tensor([0.4965, 0.4816, 0.4738, 0.4886, 0.4566, 0.4890, 0.4641, 0.4700, 0.4752,
        0.4772, 0.4835, 0.4704, 0.4961, 0.4924, 0.4840, 0.4945],
       device='cuda:0') torch.Size([16])
percent tensor([0.4830, 0.4781, 0.4782, 0.4781, 0.4752, 0.4792, 0.4768, 0.4770, 0.4819,
        0.4774, 0.4790, 0.4818, 0.4835, 0.4872, 0.4779, 0.4787],
       device='cuda:0') torch.Size([16])
percent tensor([0.5096, 0.5061, 0.5018, 0.5023, 0.4954, 0.5075, 0.5031, 0.4995, 0.5056,
        0.5072, 0.5081, 0.5038, 0.5124, 0.5068, 0.5091, 0.5079],
       device='cuda:0') torch.Size([16])
percent tensor([0.5179, 0.5148, 0.5194, 0.5213, 0.5214, 0.5198, 0.5182, 0.5227, 0.5182,
        0.5160, 0.5160, 0.5186, 0.5182, 0.5179, 0.5187, 0.5207],
       device='cuda:0') torch.Size([16])
percent tensor([0.5299, 0.5340, 0.5581, 0.5628, 0.5733, 0.5271, 0.5415, 0.5854, 0.5286,
        0.5481, 0.5343, 0.5553, 0.5324, 0.5324, 0.5329, 0.5388],
       device='cuda:0') torch.Size([16])
percent tensor([0.9809, 0.9502, 0.9572, 0.9652, 0.9693, 0.9771, 0.9570, 0.9863, 0.9618,
        0.9818, 0.9645, 0.9634, 0.9819, 0.9657, 0.9633, 0.9901],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 10 | Batch_idx: 0 |  Loss: (0.9961) |  Loss2: (0.0000) | Acc: (64.00%) (83/128)
Epoch: 10 | Batch_idx: 10 |  Loss: (1.0323) |  Loss2: (0.0000) | Acc: (63.00%) (893/1408)
Epoch: 10 | Batch_idx: 20 |  Loss: (1.0897) |  Loss2: (0.0000) | Acc: (61.00%) (1646/2688)
Epoch: 10 | Batch_idx: 30 |  Loss: (1.1318) |  Loss2: (0.0000) | Acc: (59.00%) (2374/3968)
Epoch: 10 | Batch_idx: 40 |  Loss: (1.1461) |  Loss2: (0.0000) | Acc: (59.00%) (3113/5248)
Epoch: 10 | Batch_idx: 50 |  Loss: (1.1488) |  Loss2: (0.0000) | Acc: (58.00%) (3832/6528)
Epoch: 10 | Batch_idx: 60 |  Loss: (1.1508) |  Loss2: (0.0000) | Acc: (58.00%) (4599/7808)
Epoch: 10 | Batch_idx: 70 |  Loss: (1.1505) |  Loss2: (0.0000) | Acc: (58.00%) (5346/9088)
Epoch: 10 | Batch_idx: 80 |  Loss: (1.1487) |  Loss2: (0.0000) | Acc: (58.00%) (6114/10368)
Epoch: 10 | Batch_idx: 90 |  Loss: (1.1524) |  Loss2: (0.0000) | Acc: (58.00%) (6862/11648)
Epoch: 10 | Batch_idx: 100 |  Loss: (1.1537) |  Loss2: (0.0000) | Acc: (58.00%) (7601/12928)
Epoch: 10 | Batch_idx: 110 |  Loss: (1.1524) |  Loss2: (0.0000) | Acc: (58.00%) (8352/14208)
Epoch: 10 | Batch_idx: 120 |  Loss: (1.1487) |  Loss2: (0.0000) | Acc: (58.00%) (9130/15488)
Epoch: 10 | Batch_idx: 130 |  Loss: (1.1477) |  Loss2: (0.0000) | Acc: (58.00%) (9892/16768)
Epoch: 10 | Batch_idx: 140 |  Loss: (1.1439) |  Loss2: (0.0000) | Acc: (58.00%) (10628/18048)
Epoch: 10 | Batch_idx: 150 |  Loss: (1.1437) |  Loss2: (0.0000) | Acc: (58.00%) (11390/19328)
Epoch: 10 | Batch_idx: 160 |  Loss: (1.1420) |  Loss2: (0.0000) | Acc: (59.00%) (12175/20608)
Epoch: 10 | Batch_idx: 170 |  Loss: (1.1404) |  Loss2: (0.0000) | Acc: (59.00%) (12948/21888)
Epoch: 10 | Batch_idx: 180 |  Loss: (1.1363) |  Loss2: (0.0000) | Acc: (59.00%) (13752/23168)
Epoch: 10 | Batch_idx: 190 |  Loss: (1.1319) |  Loss2: (0.0000) | Acc: (59.00%) (14563/24448)
Epoch: 10 | Batch_idx: 200 |  Loss: (1.1286) |  Loss2: (0.0000) | Acc: (59.00%) (15337/25728)
Epoch: 10 | Batch_idx: 210 |  Loss: (1.1268) |  Loss2: (0.0000) | Acc: (59.00%) (16100/27008)
Epoch: 10 | Batch_idx: 220 |  Loss: (1.1267) |  Loss2: (0.0000) | Acc: (59.00%) (16859/28288)
Epoch: 10 | Batch_idx: 230 |  Loss: (1.1256) |  Loss2: (0.0000) | Acc: (59.00%) (17644/29568)
Epoch: 10 | Batch_idx: 240 |  Loss: (1.1221) |  Loss2: (0.0000) | Acc: (59.00%) (18437/30848)
Epoch: 10 | Batch_idx: 250 |  Loss: (1.1190) |  Loss2: (0.0000) | Acc: (59.00%) (19234/32128)
Epoch: 10 | Batch_idx: 260 |  Loss: (1.1162) |  Loss2: (0.0000) | Acc: (59.00%) (20029/33408)
Epoch: 10 | Batch_idx: 270 |  Loss: (1.1137) |  Loss2: (0.0000) | Acc: (60.00%) (20820/34688)
Epoch: 10 | Batch_idx: 280 |  Loss: (1.1113) |  Loss2: (0.0000) | Acc: (60.00%) (21629/35968)
Epoch: 10 | Batch_idx: 290 |  Loss: (1.1087) |  Loss2: (0.0000) | Acc: (60.00%) (22436/37248)
Epoch: 10 | Batch_idx: 300 |  Loss: (1.1085) |  Loss2: (0.0000) | Acc: (60.00%) (23188/38528)
Epoch: 10 | Batch_idx: 310 |  Loss: (1.1065) |  Loss2: (0.0000) | Acc: (60.00%) (23981/39808)
Epoch: 10 | Batch_idx: 320 |  Loss: (1.1035) |  Loss2: (0.0000) | Acc: (60.00%) (24785/41088)
Epoch: 10 | Batch_idx: 330 |  Loss: (1.1026) |  Loss2: (0.0000) | Acc: (60.00%) (25556/42368)
Epoch: 10 | Batch_idx: 340 |  Loss: (1.1015) |  Loss2: (0.0000) | Acc: (60.00%) (26340/43648)
Epoch: 10 | Batch_idx: 350 |  Loss: (1.0993) |  Loss2: (0.0000) | Acc: (60.00%) (27167/44928)
Epoch: 10 | Batch_idx: 360 |  Loss: (1.0967) |  Loss2: (0.0000) | Acc: (60.00%) (27982/46208)
Epoch: 10 | Batch_idx: 370 |  Loss: (1.0948) |  Loss2: (0.0000) | Acc: (60.00%) (28792/47488)
Epoch: 10 | Batch_idx: 380 |  Loss: (1.0923) |  Loss2: (0.0000) | Acc: (60.00%) (29606/48768)
Epoch: 10 | Batch_idx: 390 |  Loss: (1.0912) |  Loss2: (0.0000) | Acc: (60.00%) (30362/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_010.pth.tar'
# TEST : Loss: (1.0336) | Acc: (62.00%) (6237/10000)
percent tensor([0.5095, 0.5136, 0.5110, 0.5061, 0.5134, 0.5103, 0.5150, 0.5084, 0.5121,
        0.5134, 0.5120, 0.5134, 0.5100, 0.5108, 0.5108, 0.5089],
       device='cuda:0') torch.Size([16])
percent tensor([0.4983, 0.4977, 0.4964, 0.4981, 0.4957, 0.5001, 0.4970, 0.4966, 0.4973,
        0.4975, 0.4980, 0.4965, 0.4979, 0.4990, 0.4984, 0.4992],
       device='cuda:0') torch.Size([16])
percent tensor([0.4965, 0.4772, 0.4421, 0.4646, 0.4349, 0.5003, 0.4524, 0.4369, 0.4650,
        0.4613, 0.4807, 0.4437, 0.4896, 0.4889, 0.4895, 0.4940],
       device='cuda:0') torch.Size([16])
percent tensor([0.4684, 0.4571, 0.4609, 0.4680, 0.4523, 0.4826, 0.4530, 0.4588, 0.4631,
        0.4523, 0.4556, 0.4552, 0.4643, 0.4728, 0.4655, 0.4650],
       device='cuda:0') torch.Size([16])
percent tensor([0.5005, 0.4923, 0.4820, 0.4843, 0.4770, 0.5052, 0.4868, 0.4736, 0.4934,
        0.4901, 0.4920, 0.4841, 0.4986, 0.4979, 0.4996, 0.4981],
       device='cuda:0') torch.Size([16])
percent tensor([0.5119, 0.5075, 0.5157, 0.5225, 0.5158, 0.5164, 0.5127, 0.5217, 0.5116,
        0.5080, 0.5070, 0.5116, 0.5099, 0.5102, 0.5138, 0.5150],
       device='cuda:0') torch.Size([16])
percent tensor([0.5252, 0.5299, 0.5383, 0.5399, 0.5422, 0.5220, 0.5329, 0.5517, 0.5224,
        0.5397, 0.5309, 0.5450, 0.5285, 0.5280, 0.5241, 0.5284],
       device='cuda:0') torch.Size([16])
percent tensor([0.9869, 0.9675, 0.9738, 0.9798, 0.9749, 0.9812, 0.9701, 0.9940, 0.9765,
        0.9896, 0.9763, 0.9846, 0.9862, 0.9789, 0.9758, 0.9909],
       device='cuda:0') torch.Size([16])
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(168.4715, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(775.4302, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(771.0502, device='cuda:0')
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1529.1675, device='cuda:0')
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(507.6246, device='cuda:0')
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2168.8438, device='cuda:0')
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4317.3564, device='cuda:0')
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1445.0117, device='cuda:0')
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6118.1392, device='cuda:0')
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12227.2568, device='cuda:0')
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4078.1392, device='cuda:0')
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(17281.5703, device='cuda:0')
Epoch: 11 | Batch_idx: 0 |  Loss: (1.0656) |  Loss2: (0.0000) | Acc: (60.00%) (77/128)
Epoch: 11 | Batch_idx: 10 |  Loss: (1.0419) |  Loss2: (0.0000) | Acc: (61.00%) (866/1408)
Epoch: 11 | Batch_idx: 20 |  Loss: (1.0278) |  Loss2: (0.0000) | Acc: (62.00%) (1681/2688)
Epoch: 11 | Batch_idx: 30 |  Loss: (1.0293) |  Loss2: (0.0000) | Acc: (62.00%) (2486/3968)
Epoch: 11 | Batch_idx: 40 |  Loss: (1.0282) |  Loss2: (0.0000) | Acc: (62.00%) (3295/5248)
Epoch: 11 | Batch_idx: 50 |  Loss: (1.0331) |  Loss2: (0.0000) | Acc: (62.00%) (4067/6528)
Epoch: 11 | Batch_idx: 60 |  Loss: (1.0291) |  Loss2: (0.0000) | Acc: (62.00%) (4873/7808)
Epoch: 11 | Batch_idx: 70 |  Loss: (1.0270) |  Loss2: (0.0000) | Acc: (62.00%) (5671/9088)
Epoch: 11 | Batch_idx: 80 |  Loss: (1.0272) |  Loss2: (0.0000) | Acc: (62.00%) (6480/10368)
Epoch: 11 | Batch_idx: 90 |  Loss: (1.0244) |  Loss2: (0.0000) | Acc: (62.00%) (7302/11648)
Epoch: 11 | Batch_idx: 100 |  Loss: (1.0217) |  Loss2: (0.0000) | Acc: (63.00%) (8145/12928)
Epoch: 11 | Batch_idx: 110 |  Loss: (1.0232) |  Loss2: (0.0000) | Acc: (62.00%) (8946/14208)
Epoch: 11 | Batch_idx: 120 |  Loss: (1.0226) |  Loss2: (0.0000) | Acc: (62.00%) (9756/15488)
Epoch: 11 | Batch_idx: 130 |  Loss: (1.0213) |  Loss2: (0.0000) | Acc: (63.00%) (10580/16768)
Epoch: 11 | Batch_idx: 140 |  Loss: (1.0233) |  Loss2: (0.0000) | Acc: (63.00%) (11380/18048)
Epoch: 11 | Batch_idx: 150 |  Loss: (1.0204) |  Loss2: (0.0000) | Acc: (63.00%) (12226/19328)
Epoch: 11 | Batch_idx: 160 |  Loss: (1.0188) |  Loss2: (0.0000) | Acc: (63.00%) (13045/20608)
Epoch: 11 | Batch_idx: 170 |  Loss: (1.0169) |  Loss2: (0.0000) | Acc: (63.00%) (13869/21888)
Epoch: 11 | Batch_idx: 180 |  Loss: (1.0162) |  Loss2: (0.0000) | Acc: (63.00%) (14680/23168)
Epoch: 11 | Batch_idx: 190 |  Loss: (1.0147) |  Loss2: (0.0000) | Acc: (63.00%) (15511/24448)
Epoch: 11 | Batch_idx: 200 |  Loss: (1.0164) |  Loss2: (0.0000) | Acc: (63.00%) (16323/25728)
Epoch: 11 | Batch_idx: 210 |  Loss: (1.0158) |  Loss2: (0.0000) | Acc: (63.00%) (17146/27008)
Epoch: 11 | Batch_idx: 220 |  Loss: (1.0156) |  Loss2: (0.0000) | Acc: (63.00%) (17976/28288)
Epoch: 11 | Batch_idx: 230 |  Loss: (1.0154) |  Loss2: (0.0000) | Acc: (63.00%) (18803/29568)
Epoch: 11 | Batch_idx: 240 |  Loss: (1.0135) |  Loss2: (0.0000) | Acc: (63.00%) (19652/30848)
Epoch: 11 | Batch_idx: 250 |  Loss: (1.0108) |  Loss2: (0.0000) | Acc: (63.00%) (20510/32128)
Epoch: 11 | Batch_idx: 260 |  Loss: (1.0095) |  Loss2: (0.0000) | Acc: (63.00%) (21344/33408)
Epoch: 11 | Batch_idx: 270 |  Loss: (1.0083) |  Loss2: (0.0000) | Acc: (63.00%) (22157/34688)
Epoch: 11 | Batch_idx: 280 |  Loss: (1.0087) |  Loss2: (0.0000) | Acc: (63.00%) (22984/35968)
Epoch: 11 | Batch_idx: 290 |  Loss: (1.0075) |  Loss2: (0.0000) | Acc: (63.00%) (23810/37248)
Epoch: 11 | Batch_idx: 300 |  Loss: (1.0080) |  Loss2: (0.0000) | Acc: (63.00%) (24612/38528)
Epoch: 11 | Batch_idx: 310 |  Loss: (1.0076) |  Loss2: (0.0000) | Acc: (63.00%) (25432/39808)
Epoch: 11 | Batch_idx: 320 |  Loss: (1.0065) |  Loss2: (0.0000) | Acc: (63.00%) (26258/41088)
Epoch: 11 | Batch_idx: 330 |  Loss: (1.0055) |  Loss2: (0.0000) | Acc: (63.00%) (27082/42368)
Epoch: 11 | Batch_idx: 340 |  Loss: (1.0061) |  Loss2: (0.0000) | Acc: (63.00%) (27881/43648)
Epoch: 11 | Batch_idx: 350 |  Loss: (1.0049) |  Loss2: (0.0000) | Acc: (63.00%) (28699/44928)
Epoch: 11 | Batch_idx: 360 |  Loss: (1.0059) |  Loss2: (0.0000) | Acc: (63.00%) (29512/46208)
Epoch: 11 | Batch_idx: 370 |  Loss: (1.0072) |  Loss2: (0.0000) | Acc: (63.00%) (30291/47488)
Epoch: 11 | Batch_idx: 380 |  Loss: (1.0080) |  Loss2: (0.0000) | Acc: (63.00%) (31101/48768)
Epoch: 11 | Batch_idx: 390 |  Loss: (1.0091) |  Loss2: (0.0000) | Acc: (63.00%) (31864/50000)
# TEST : Loss: (0.9943) | Acc: (63.00%) (6384/10000)
percent tensor([0.5186, 0.5254, 0.5200, 0.5126, 0.5238, 0.5198, 0.5274, 0.5169, 0.5225,
        0.5247, 0.5224, 0.5242, 0.5194, 0.5213, 0.5208, 0.5181],
       device='cuda:0') torch.Size([16])
percent tensor([0.4987, 0.4980, 0.4969, 0.4984, 0.4963, 0.5004, 0.4974, 0.4972, 0.4977,
        0.4979, 0.4983, 0.4970, 0.4982, 0.4991, 0.4987, 0.4994],
       device='cuda:0') torch.Size([16])
percent tensor([0.4950, 0.4752, 0.4228, 0.4510, 0.4235, 0.5062, 0.4464, 0.4199, 0.4599,
        0.4535, 0.4804, 0.4302, 0.4862, 0.4876, 0.4929, 0.4934],
       device='cuda:0') torch.Size([16])
percent tensor([0.4738, 0.4550, 0.4620, 0.4711, 0.4520, 0.4969, 0.4508, 0.4585, 0.4616,
        0.4489, 0.4528, 0.4527, 0.4642, 0.4726, 0.4701, 0.4694],
       device='cuda:0') torch.Size([16])
percent tensor([0.5006, 0.4890, 0.4773, 0.4851, 0.4733, 0.5105, 0.4838, 0.4710, 0.4906,
        0.4855, 0.4891, 0.4798, 0.4964, 0.4962, 0.5007, 0.4989],
       device='cuda:0') torch.Size([16])
percent tensor([0.5164, 0.5077, 0.5242, 0.5321, 0.5232, 0.5221, 0.5155, 0.5317, 0.5134,
        0.5086, 0.5072, 0.5160, 0.5107, 0.5106, 0.5188, 0.5208],
       device='cuda:0') torch.Size([16])
percent tensor([0.5335, 0.5377, 0.5427, 0.5435, 0.5468, 0.5307, 0.5398, 0.5545, 0.5291,
        0.5477, 0.5381, 0.5468, 0.5373, 0.5372, 0.5308, 0.5371],
       device='cuda:0') torch.Size([16])
percent tensor([0.9928, 0.9788, 0.9845, 0.9886, 0.9868, 0.9881, 0.9812, 0.9973, 0.9863,
        0.9943, 0.9851, 0.9895, 0.9919, 0.9868, 0.9879, 0.9958],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 12 | Batch_idx: 0 |  Loss: (0.9087) |  Loss2: (0.0000) | Acc: (69.00%) (89/128)
Epoch: 12 | Batch_idx: 10 |  Loss: (0.9199) |  Loss2: (0.0000) | Acc: (67.00%) (951/1408)
Epoch: 12 | Batch_idx: 20 |  Loss: (0.9543) |  Loss2: (0.0000) | Acc: (66.00%) (1777/2688)
Epoch: 12 | Batch_idx: 30 |  Loss: (0.9494) |  Loss2: (0.0000) | Acc: (66.00%) (2644/3968)
Epoch: 12 | Batch_idx: 40 |  Loss: (0.9506) |  Loss2: (0.0000) | Acc: (66.00%) (3478/5248)
Epoch: 12 | Batch_idx: 50 |  Loss: (0.9505) |  Loss2: (0.0000) | Acc: (66.00%) (4309/6528)
Epoch: 12 | Batch_idx: 60 |  Loss: (0.9590) |  Loss2: (0.0000) | Acc: (65.00%) (5134/7808)
Epoch: 12 | Batch_idx: 70 |  Loss: (0.9644) |  Loss2: (0.0000) | Acc: (65.00%) (5970/9088)
Epoch: 12 | Batch_idx: 80 |  Loss: (0.9669) |  Loss2: (0.0000) | Acc: (65.00%) (6791/10368)
Epoch: 12 | Batch_idx: 90 |  Loss: (0.9658) |  Loss2: (0.0000) | Acc: (65.00%) (7642/11648)
Epoch: 12 | Batch_idx: 100 |  Loss: (0.9685) |  Loss2: (0.0000) | Acc: (65.00%) (8457/12928)
Epoch: 12 | Batch_idx: 110 |  Loss: (0.9649) |  Loss2: (0.0000) | Acc: (65.00%) (9315/14208)
Epoch: 12 | Batch_idx: 120 |  Loss: (0.9620) |  Loss2: (0.0000) | Acc: (65.00%) (10163/15488)
Epoch: 12 | Batch_idx: 130 |  Loss: (0.9604) |  Loss2: (0.0000) | Acc: (65.00%) (11003/16768)
Epoch: 12 | Batch_idx: 140 |  Loss: (0.9614) |  Loss2: (0.0000) | Acc: (65.00%) (11841/18048)
Epoch: 12 | Batch_idx: 150 |  Loss: (0.9613) |  Loss2: (0.0000) | Acc: (65.00%) (12670/19328)
Epoch: 12 | Batch_idx: 160 |  Loss: (0.9614) |  Loss2: (0.0000) | Acc: (65.00%) (13506/20608)
Epoch: 12 | Batch_idx: 170 |  Loss: (0.9619) |  Loss2: (0.0000) | Acc: (65.00%) (14341/21888)
Epoch: 12 | Batch_idx: 180 |  Loss: (0.9632) |  Loss2: (0.0000) | Acc: (65.00%) (15169/23168)
Epoch: 12 | Batch_idx: 190 |  Loss: (0.9622) |  Loss2: (0.0000) | Acc: (65.00%) (16020/24448)
Epoch: 12 | Batch_idx: 200 |  Loss: (0.9612) |  Loss2: (0.0000) | Acc: (65.00%) (16858/25728)
Epoch: 12 | Batch_idx: 210 |  Loss: (0.9605) |  Loss2: (0.0000) | Acc: (65.00%) (17662/27008)
Epoch: 12 | Batch_idx: 220 |  Loss: (0.9597) |  Loss2: (0.0000) | Acc: (65.00%) (18500/28288)
Epoch: 12 | Batch_idx: 230 |  Loss: (0.9576) |  Loss2: (0.0000) | Acc: (65.00%) (19347/29568)
Epoch: 12 | Batch_idx: 240 |  Loss: (0.9566) |  Loss2: (0.0000) | Acc: (65.00%) (20187/30848)
Epoch: 12 | Batch_idx: 250 |  Loss: (0.9569) |  Loss2: (0.0000) | Acc: (65.00%) (21038/32128)
Epoch: 12 | Batch_idx: 260 |  Loss: (0.9575) |  Loss2: (0.0000) | Acc: (65.00%) (21876/33408)
Epoch: 12 | Batch_idx: 270 |  Loss: (0.9549) |  Loss2: (0.0000) | Acc: (65.00%) (22736/34688)
Epoch: 12 | Batch_idx: 280 |  Loss: (0.9550) |  Loss2: (0.0000) | Acc: (65.00%) (23592/35968)
Epoch: 12 | Batch_idx: 290 |  Loss: (0.9547) |  Loss2: (0.0000) | Acc: (65.00%) (24441/37248)
Epoch: 12 | Batch_idx: 300 |  Loss: (0.9553) |  Loss2: (0.0000) | Acc: (65.00%) (25273/38528)
Epoch: 12 | Batch_idx: 310 |  Loss: (0.9552) |  Loss2: (0.0000) | Acc: (65.00%) (26124/39808)
Epoch: 12 | Batch_idx: 320 |  Loss: (0.9544) |  Loss2: (0.0000) | Acc: (65.00%) (26974/41088)
Epoch: 12 | Batch_idx: 330 |  Loss: (0.9541) |  Loss2: (0.0000) | Acc: (65.00%) (27820/42368)
Epoch: 12 | Batch_idx: 340 |  Loss: (0.9535) |  Loss2: (0.0000) | Acc: (65.00%) (28676/43648)
Epoch: 12 | Batch_idx: 350 |  Loss: (0.9532) |  Loss2: (0.0000) | Acc: (65.00%) (29517/44928)
Epoch: 12 | Batch_idx: 360 |  Loss: (0.9521) |  Loss2: (0.0000) | Acc: (65.00%) (30379/46208)
Epoch: 12 | Batch_idx: 370 |  Loss: (0.9515) |  Loss2: (0.0000) | Acc: (65.00%) (31229/47488)
Epoch: 12 | Batch_idx: 380 |  Loss: (0.9505) |  Loss2: (0.0000) | Acc: (65.00%) (32117/48768)
Epoch: 12 | Batch_idx: 390 |  Loss: (0.9511) |  Loss2: (0.0000) | Acc: (65.00%) (32933/50000)
# TEST : Loss: (0.9483) | Acc: (66.00%) (6633/10000)
percent tensor([0.5181, 0.5261, 0.5184, 0.5126, 0.5215, 0.5186, 0.5265, 0.5168, 0.5221,
        0.5247, 0.5227, 0.5226, 0.5195, 0.5221, 0.5205, 0.5181],
       device='cuda:0') torch.Size([16])
percent tensor([0.4987, 0.4984, 0.4967, 0.4985, 0.4959, 0.5002, 0.4976, 0.4973, 0.4975,
        0.4980, 0.4983, 0.4968, 0.4982, 0.4994, 0.4986, 0.4994],
       device='cuda:0') torch.Size([16])
percent tensor([0.4889, 0.4759, 0.4318, 0.4628, 0.4244, 0.5026, 0.4468, 0.4364, 0.4565,
        0.4575, 0.4768, 0.4348, 0.4848, 0.4868, 0.4887, 0.4917],
       device='cuda:0') torch.Size([16])
percent tensor([0.4771, 0.4549, 0.4625, 0.4709, 0.4575, 0.4979, 0.4521, 0.4582, 0.4665,
        0.4519, 0.4539, 0.4545, 0.4646, 0.4704, 0.4698, 0.4701],
       device='cuda:0') torch.Size([16])
percent tensor([0.5000, 0.4907, 0.4789, 0.4881, 0.4715, 0.5090, 0.4871, 0.4787, 0.4913,
        0.4873, 0.4885, 0.4820, 0.4967, 0.4971, 0.4994, 0.4987],
       device='cuda:0') torch.Size([16])
percent tensor([0.5202, 0.5079, 0.5296, 0.5325, 0.5285, 0.5252, 0.5167, 0.5285, 0.5182,
        0.5106, 0.5089, 0.5217, 0.5123, 0.5133, 0.5196, 0.5213],
       device='cuda:0') torch.Size([16])
percent tensor([0.5321, 0.5355, 0.5465, 0.5432, 0.5491, 0.5292, 0.5395, 0.5513, 0.5294,
        0.5437, 0.5371, 0.5477, 0.5368, 0.5344, 0.5319, 0.5375],
       device='cuda:0') torch.Size([16])
percent tensor([0.9936, 0.9808, 0.9899, 0.9906, 0.9858, 0.9843, 0.9838, 0.9967, 0.9864,
        0.9945, 0.9871, 0.9928, 0.9938, 0.9831, 0.9884, 0.9965],
       device='cuda:0') torch.Size([16])
Epoch: 13 | Batch_idx: 0 |  Loss: (1.0168) |  Loss2: (0.0000) | Acc: (67.00%) (87/128)
Epoch: 13 | Batch_idx: 10 |  Loss: (0.9609) |  Loss2: (0.0000) | Acc: (66.00%) (931/1408)
Epoch: 13 | Batch_idx: 20 |  Loss: (0.9382) |  Loss2: (0.0000) | Acc: (67.00%) (1815/2688)
Epoch: 13 | Batch_idx: 30 |  Loss: (0.9125) |  Loss2: (0.0000) | Acc: (68.00%) (2706/3968)
Epoch: 13 | Batch_idx: 40 |  Loss: (0.9060) |  Loss2: (0.0000) | Acc: (68.00%) (3602/5248)
Epoch: 13 | Batch_idx: 50 |  Loss: (0.9014) |  Loss2: (0.0000) | Acc: (68.00%) (4480/6528)
Epoch: 13 | Batch_idx: 60 |  Loss: (0.9032) |  Loss2: (0.0000) | Acc: (68.00%) (5336/7808)
Epoch: 13 | Batch_idx: 70 |  Loss: (0.9045) |  Loss2: (0.0000) | Acc: (68.00%) (6202/9088)
Epoch: 13 | Batch_idx: 80 |  Loss: (0.9045) |  Loss2: (0.0000) | Acc: (68.00%) (7054/10368)
Epoch: 13 | Batch_idx: 90 |  Loss: (0.9043) |  Loss2: (0.0000) | Acc: (68.00%) (7932/11648)
Epoch: 13 | Batch_idx: 100 |  Loss: (0.9014) |  Loss2: (0.0000) | Acc: (68.00%) (8820/12928)
Epoch: 13 | Batch_idx: 110 |  Loss: (0.9026) |  Loss2: (0.0000) | Acc: (68.00%) (9668/14208)
Epoch: 13 | Batch_idx: 120 |  Loss: (0.9037) |  Loss2: (0.0000) | Acc: (67.00%) (10531/15488)
Epoch: 13 | Batch_idx: 130 |  Loss: (0.9072) |  Loss2: (0.0000) | Acc: (67.00%) (11398/16768)
Epoch: 13 | Batch_idx: 140 |  Loss: (0.9097) |  Loss2: (0.0000) | Acc: (67.00%) (12233/18048)
Epoch: 13 | Batch_idx: 150 |  Loss: (0.9076) |  Loss2: (0.0000) | Acc: (67.00%) (13121/19328)
Epoch: 13 | Batch_idx: 160 |  Loss: (0.9072) |  Loss2: (0.0000) | Acc: (67.00%) (13993/20608)
Epoch: 13 | Batch_idx: 170 |  Loss: (0.9097) |  Loss2: (0.0000) | Acc: (67.00%) (14857/21888)
Epoch: 13 | Batch_idx: 180 |  Loss: (0.9094) |  Loss2: (0.0000) | Acc: (67.00%) (15704/23168)
Epoch: 13 | Batch_idx: 190 |  Loss: (0.9081) |  Loss2: (0.0000) | Acc: (67.00%) (16575/24448)
Epoch: 13 | Batch_idx: 200 |  Loss: (0.9081) |  Loss2: (0.0000) | Acc: (67.00%) (17438/25728)
Epoch: 13 | Batch_idx: 210 |  Loss: (0.9049) |  Loss2: (0.0000) | Acc: (67.00%) (18335/27008)
Epoch: 13 | Batch_idx: 220 |  Loss: (0.9035) |  Loss2: (0.0000) | Acc: (67.00%) (19213/28288)
Epoch: 13 | Batch_idx: 230 |  Loss: (0.9020) |  Loss2: (0.0000) | Acc: (68.00%) (20112/29568)
Epoch: 13 | Batch_idx: 240 |  Loss: (0.8994) |  Loss2: (0.0000) | Acc: (68.00%) (21016/30848)
Epoch: 13 | Batch_idx: 250 |  Loss: (0.8980) |  Loss2: (0.0000) | Acc: (68.00%) (21877/32128)
Epoch: 13 | Batch_idx: 260 |  Loss: (0.8955) |  Loss2: (0.0000) | Acc: (68.00%) (22780/33408)
Epoch: 13 | Batch_idx: 270 |  Loss: (0.8954) |  Loss2: (0.0000) | Acc: (68.00%) (23648/34688)
Epoch: 13 | Batch_idx: 280 |  Loss: (0.8959) |  Loss2: (0.0000) | Acc: (68.00%) (24502/35968)
Epoch: 13 | Batch_idx: 290 |  Loss: (0.8946) |  Loss2: (0.0000) | Acc: (68.00%) (25393/37248)
Epoch: 13 | Batch_idx: 300 |  Loss: (0.8942) |  Loss2: (0.0000) | Acc: (68.00%) (26286/38528)
Epoch: 13 | Batch_idx: 310 |  Loss: (0.8920) |  Loss2: (0.0000) | Acc: (68.00%) (27201/39808)
Epoch: 13 | Batch_idx: 320 |  Loss: (0.8929) |  Loss2: (0.0000) | Acc: (68.00%) (28043/41088)
Epoch: 13 | Batch_idx: 330 |  Loss: (0.8923) |  Loss2: (0.0000) | Acc: (68.00%) (28924/42368)
Epoch: 13 | Batch_idx: 340 |  Loss: (0.8928) |  Loss2: (0.0000) | Acc: (68.00%) (29786/43648)
Epoch: 13 | Batch_idx: 350 |  Loss: (0.8938) |  Loss2: (0.0000) | Acc: (68.00%) (30635/44928)
Epoch: 13 | Batch_idx: 360 |  Loss: (0.8922) |  Loss2: (0.0000) | Acc: (68.00%) (31541/46208)
Epoch: 13 | Batch_idx: 370 |  Loss: (0.8903) |  Loss2: (0.0000) | Acc: (68.00%) (32441/47488)
Epoch: 13 | Batch_idx: 380 |  Loss: (0.8898) |  Loss2: (0.0000) | Acc: (68.00%) (33339/48768)
Epoch: 13 | Batch_idx: 390 |  Loss: (0.8898) |  Loss2: (0.0000) | Acc: (68.00%) (34193/50000)
# TEST : Loss: (0.9123) | Acc: (67.00%) (6742/10000)
percent tensor([0.5176, 0.5264, 0.5189, 0.5126, 0.5214, 0.5177, 0.5265, 0.5173, 0.5216,
        0.5250, 0.5222, 0.5230, 0.5193, 0.5222, 0.5202, 0.5180],
       device='cuda:0') torch.Size([16])
percent tensor([0.4987, 0.4980, 0.4970, 0.4985, 0.4963, 0.5006, 0.4976, 0.4968, 0.4970,
        0.4978, 0.4979, 0.4968, 0.4979, 0.4987, 0.4986, 0.4993],
       device='cuda:0') torch.Size([16])
percent tensor([0.4874, 0.4750, 0.4408, 0.4559, 0.4327, 0.5009, 0.4522, 0.4369, 0.4619,
        0.4568, 0.4761, 0.4438, 0.4819, 0.4826, 0.4843, 0.4869],
       device='cuda:0') torch.Size([16])
percent tensor([0.4730, 0.4534, 0.4648, 0.4706, 0.4600, 0.4942, 0.4514, 0.4573, 0.4640,
        0.4477, 0.4517, 0.4575, 0.4621, 0.4639, 0.4674, 0.4659],
       device='cuda:0') torch.Size([16])
percent tensor([0.4982, 0.4898, 0.4767, 0.4841, 0.4727, 0.5081, 0.4877, 0.4751, 0.4900,
        0.4856, 0.4864, 0.4821, 0.4944, 0.4945, 0.4980, 0.4970],
       device='cuda:0') torch.Size([16])
percent tensor([0.5158, 0.5078, 0.5261, 0.5304, 0.5259, 0.5235, 0.5143, 0.5262, 0.5159,
        0.5069, 0.5072, 0.5186, 0.5098, 0.5134, 0.5174, 0.5189],
       device='cuda:0') torch.Size([16])
percent tensor([0.5375, 0.5362, 0.5456, 0.5469, 0.5482, 0.5317, 0.5423, 0.5537, 0.5315,
        0.5484, 0.5369, 0.5468, 0.5373, 0.5399, 0.5353, 0.5388],
       device='cuda:0') torch.Size([16])
percent tensor([0.9958, 0.9866, 0.9859, 0.9913, 0.9883, 0.9852, 0.9910, 0.9972, 0.9875,
        0.9966, 0.9853, 0.9930, 0.9911, 0.9873, 0.9905, 0.9970],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 14 | Batch_idx: 0 |  Loss: (0.7392) |  Loss2: (0.0000) | Acc: (71.00%) (91/128)
Epoch: 14 | Batch_idx: 10 |  Loss: (0.8777) |  Loss2: (0.0000) | Acc: (67.00%) (953/1408)
Epoch: 14 | Batch_idx: 20 |  Loss: (0.9703) |  Loss2: (0.0000) | Acc: (65.00%) (1767/2688)
Epoch: 14 | Batch_idx: 30 |  Loss: (0.9957) |  Loss2: (0.0000) | Acc: (64.00%) (2551/3968)
Epoch: 14 | Batch_idx: 40 |  Loss: (1.0128) |  Loss2: (0.0000) | Acc: (63.00%) (3323/5248)
Epoch: 14 | Batch_idx: 50 |  Loss: (1.0088) |  Loss2: (0.0000) | Acc: (63.00%) (4152/6528)
Epoch: 14 | Batch_idx: 60 |  Loss: (1.0130) |  Loss2: (0.0000) | Acc: (63.00%) (4975/7808)
Epoch: 14 | Batch_idx: 70 |  Loss: (1.0106) |  Loss2: (0.0000) | Acc: (64.00%) (5817/9088)
Epoch: 14 | Batch_idx: 80 |  Loss: (1.0067) |  Loss2: (0.0000) | Acc: (64.00%) (6661/10368)
Epoch: 14 | Batch_idx: 90 |  Loss: (1.0071) |  Loss2: (0.0000) | Acc: (64.00%) (7486/11648)
Epoch: 14 | Batch_idx: 100 |  Loss: (1.0056) |  Loss2: (0.0000) | Acc: (64.00%) (8307/12928)
Epoch: 14 | Batch_idx: 110 |  Loss: (1.0010) |  Loss2: (0.0000) | Acc: (64.00%) (9142/14208)
Epoch: 14 | Batch_idx: 120 |  Loss: (0.9981) |  Loss2: (0.0000) | Acc: (64.00%) (9982/15488)
Epoch: 14 | Batch_idx: 130 |  Loss: (0.9921) |  Loss2: (0.0000) | Acc: (64.00%) (10841/16768)
Epoch: 14 | Batch_idx: 140 |  Loss: (0.9887) |  Loss2: (0.0000) | Acc: (64.00%) (11699/18048)
Epoch: 14 | Batch_idx: 150 |  Loss: (0.9873) |  Loss2: (0.0000) | Acc: (64.00%) (12512/19328)
Epoch: 14 | Batch_idx: 160 |  Loss: (0.9881) |  Loss2: (0.0000) | Acc: (64.00%) (13326/20608)
Epoch: 14 | Batch_idx: 170 |  Loss: (0.9867) |  Loss2: (0.0000) | Acc: (64.00%) (14173/21888)
Epoch: 14 | Batch_idx: 180 |  Loss: (0.9840) |  Loss2: (0.0000) | Acc: (64.00%) (15026/23168)
Epoch: 14 | Batch_idx: 190 |  Loss: (0.9821) |  Loss2: (0.0000) | Acc: (64.00%) (15864/24448)
Epoch: 14 | Batch_idx: 200 |  Loss: (0.9808) |  Loss2: (0.0000) | Acc: (64.00%) (16693/25728)
Epoch: 14 | Batch_idx: 210 |  Loss: (0.9779) |  Loss2: (0.0000) | Acc: (64.00%) (17547/27008)
Epoch: 14 | Batch_idx: 220 |  Loss: (0.9760) |  Loss2: (0.0000) | Acc: (65.00%) (18415/28288)
Epoch: 14 | Batch_idx: 230 |  Loss: (0.9768) |  Loss2: (0.0000) | Acc: (65.00%) (19229/29568)
Epoch: 14 | Batch_idx: 240 |  Loss: (0.9754) |  Loss2: (0.0000) | Acc: (65.00%) (20059/30848)
Epoch: 14 | Batch_idx: 250 |  Loss: (0.9743) |  Loss2: (0.0000) | Acc: (64.00%) (20874/32128)
Epoch: 14 | Batch_idx: 260 |  Loss: (0.9735) |  Loss2: (0.0000) | Acc: (65.00%) (21717/33408)
Epoch: 14 | Batch_idx: 270 |  Loss: (0.9721) |  Loss2: (0.0000) | Acc: (65.00%) (22590/34688)
Epoch: 14 | Batch_idx: 280 |  Loss: (0.9715) |  Loss2: (0.0000) | Acc: (65.00%) (23435/35968)
Epoch: 14 | Batch_idx: 290 |  Loss: (0.9694) |  Loss2: (0.0000) | Acc: (65.00%) (24305/37248)
Epoch: 14 | Batch_idx: 300 |  Loss: (0.9678) |  Loss2: (0.0000) | Acc: (65.00%) (25176/38528)
Epoch: 14 | Batch_idx: 310 |  Loss: (0.9660) |  Loss2: (0.0000) | Acc: (65.00%) (26038/39808)
Epoch: 14 | Batch_idx: 320 |  Loss: (0.9658) |  Loss2: (0.0000) | Acc: (65.00%) (26871/41088)
Epoch: 14 | Batch_idx: 330 |  Loss: (0.9647) |  Loss2: (0.0000) | Acc: (65.00%) (27735/42368)
Epoch: 14 | Batch_idx: 340 |  Loss: (0.9635) |  Loss2: (0.0000) | Acc: (65.00%) (28590/43648)
Epoch: 14 | Batch_idx: 350 |  Loss: (0.9613) |  Loss2: (0.0000) | Acc: (65.00%) (29454/44928)
Epoch: 14 | Batch_idx: 360 |  Loss: (0.9608) |  Loss2: (0.0000) | Acc: (65.00%) (30309/46208)
Epoch: 14 | Batch_idx: 370 |  Loss: (0.9598) |  Loss2: (0.0000) | Acc: (65.00%) (31188/47488)
Epoch: 14 | Batch_idx: 380 |  Loss: (0.9578) |  Loss2: (0.0000) | Acc: (65.00%) (32077/48768)
Epoch: 14 | Batch_idx: 390 |  Loss: (0.9575) |  Loss2: (0.0000) | Acc: (65.00%) (32882/50000)
# TEST : Loss: (0.9245) | Acc: (67.00%) (6727/10000)
percent tensor([0.5344, 0.5479, 0.5394, 0.5284, 0.5444, 0.5373, 0.5500, 0.5359, 0.5411,
        0.5464, 0.5409, 0.5453, 0.5363, 0.5405, 0.5399, 0.5360],
       device='cuda:0') torch.Size([16])
percent tensor([0.4915, 0.4890, 0.4886, 0.4912, 0.4871, 0.4945, 0.4887, 0.4881, 0.4879,
        0.4898, 0.4892, 0.4884, 0.4899, 0.4900, 0.4907, 0.4925],
       device='cuda:0') torch.Size([16])
percent tensor([0.4846, 0.4680, 0.4404, 0.4588, 0.4400, 0.5005, 0.4505, 0.4430, 0.4632,
        0.4533, 0.4696, 0.4418, 0.4759, 0.4830, 0.4829, 0.4822],
       device='cuda:0') torch.Size([16])
percent tensor([0.4805, 0.4452, 0.4692, 0.4773, 0.4602, 0.5099, 0.4455, 0.4568, 0.4624,
        0.4423, 0.4463, 0.4592, 0.4635, 0.4592, 0.4700, 0.4732],
       device='cuda:0') torch.Size([16])
percent tensor([0.5173, 0.5059, 0.5027, 0.5077, 0.4976, 0.5222, 0.5054, 0.5004, 0.5102,
        0.5040, 0.5053, 0.5066, 0.5160, 0.5113, 0.5168, 0.5155],
       device='cuda:0') torch.Size([16])
percent tensor([0.5269, 0.5123, 0.5350, 0.5423, 0.5314, 0.5363, 0.5201, 0.5317, 0.5260,
        0.5102, 0.5142, 0.5297, 0.5211, 0.5208, 0.5244, 0.5281],
       device='cuda:0') torch.Size([16])
percent tensor([0.5529, 0.5480, 0.5394, 0.5413, 0.5396, 0.5490, 0.5468, 0.5370, 0.5464,
        0.5560, 0.5513, 0.5499, 0.5554, 0.5577, 0.5372, 0.5448],
       device='cuda:0') torch.Size([16])
percent tensor([0.9961, 0.9915, 0.9903, 0.9935, 0.9895, 0.9876, 0.9917, 0.9975, 0.9893,
        0.9968, 0.9886, 0.9922, 0.9915, 0.9911, 0.9868, 0.9954],
       device='cuda:0') torch.Size([16])
Epoch: 15 | Batch_idx: 0 |  Loss: (0.6645) |  Loss2: (0.0000) | Acc: (75.00%) (97/128)
Epoch: 15 | Batch_idx: 10 |  Loss: (0.8686) |  Loss2: (0.0000) | Acc: (69.00%) (973/1408)
Epoch: 15 | Batch_idx: 20 |  Loss: (0.8816) |  Loss2: (0.0000) | Acc: (69.00%) (1856/2688)
Epoch: 15 | Batch_idx: 30 |  Loss: (0.8920) |  Loss2: (0.0000) | Acc: (68.00%) (2735/3968)
Epoch: 15 | Batch_idx: 40 |  Loss: (0.8943) |  Loss2: (0.0000) | Acc: (68.00%) (3607/5248)
Epoch: 15 | Batch_idx: 50 |  Loss: (0.9021) |  Loss2: (0.0000) | Acc: (68.00%) (4448/6528)
Epoch: 15 | Batch_idx: 60 |  Loss: (0.8989) |  Loss2: (0.0000) | Acc: (68.00%) (5345/7808)
Epoch: 15 | Batch_idx: 70 |  Loss: (0.9047) |  Loss2: (0.0000) | Acc: (68.00%) (6212/9088)
Epoch: 15 | Batch_idx: 80 |  Loss: (0.9087) |  Loss2: (0.0000) | Acc: (68.00%) (7082/10368)
Epoch: 15 | Batch_idx: 90 |  Loss: (0.9047) |  Loss2: (0.0000) | Acc: (68.00%) (7965/11648)
Epoch: 15 | Batch_idx: 100 |  Loss: (0.9073) |  Loss2: (0.0000) | Acc: (68.00%) (8828/12928)
Epoch: 15 | Batch_idx: 110 |  Loss: (0.9092) |  Loss2: (0.0000) | Acc: (68.00%) (9695/14208)
Epoch: 15 | Batch_idx: 120 |  Loss: (0.9124) |  Loss2: (0.0000) | Acc: (68.00%) (10547/15488)
Epoch: 15 | Batch_idx: 130 |  Loss: (0.9106) |  Loss2: (0.0000) | Acc: (68.00%) (11415/16768)
Epoch: 15 | Batch_idx: 140 |  Loss: (0.9100) |  Loss2: (0.0000) | Acc: (68.00%) (12279/18048)
Epoch: 15 | Batch_idx: 150 |  Loss: (0.9083) |  Loss2: (0.0000) | Acc: (67.00%) (13134/19328)
Epoch: 15 | Batch_idx: 160 |  Loss: (0.9076) |  Loss2: (0.0000) | Acc: (67.00%) (13990/20608)
Epoch: 15 | Batch_idx: 170 |  Loss: (0.9062) |  Loss2: (0.0000) | Acc: (67.00%) (14876/21888)
Epoch: 15 | Batch_idx: 180 |  Loss: (0.9064) |  Loss2: (0.0000) | Acc: (67.00%) (15732/23168)
Epoch: 15 | Batch_idx: 190 |  Loss: (0.9076) |  Loss2: (0.0000) | Acc: (67.00%) (16598/24448)
Epoch: 15 | Batch_idx: 200 |  Loss: (0.9099) |  Loss2: (0.0000) | Acc: (67.00%) (17458/25728)
Epoch: 15 | Batch_idx: 210 |  Loss: (0.9096) |  Loss2: (0.0000) | Acc: (67.00%) (18338/27008)
Epoch: 15 | Batch_idx: 220 |  Loss: (0.9087) |  Loss2: (0.0000) | Acc: (67.00%) (19207/28288)
Epoch: 15 | Batch_idx: 230 |  Loss: (0.9096) |  Loss2: (0.0000) | Acc: (67.00%) (20070/29568)
Epoch: 15 | Batch_idx: 240 |  Loss: (0.9079) |  Loss2: (0.0000) | Acc: (67.00%) (20968/30848)
Epoch: 15 | Batch_idx: 250 |  Loss: (0.9084) |  Loss2: (0.0000) | Acc: (67.00%) (21819/32128)
Epoch: 15 | Batch_idx: 260 |  Loss: (0.9067) |  Loss2: (0.0000) | Acc: (67.00%) (22712/33408)
Epoch: 15 | Batch_idx: 270 |  Loss: (0.9043) |  Loss2: (0.0000) | Acc: (68.00%) (23614/34688)
Epoch: 15 | Batch_idx: 280 |  Loss: (0.9021) |  Loss2: (0.0000) | Acc: (68.00%) (24516/35968)
Epoch: 15 | Batch_idx: 290 |  Loss: (0.9017) |  Loss2: (0.0000) | Acc: (68.00%) (25392/37248)
Epoch: 15 | Batch_idx: 300 |  Loss: (0.9003) |  Loss2: (0.0000) | Acc: (68.00%) (26277/38528)
Epoch: 15 | Batch_idx: 310 |  Loss: (0.8990) |  Loss2: (0.0000) | Acc: (68.00%) (27162/39808)
Epoch: 15 | Batch_idx: 320 |  Loss: (0.8974) |  Loss2: (0.0000) | Acc: (68.00%) (28052/41088)
Epoch: 15 | Batch_idx: 330 |  Loss: (0.8969) |  Loss2: (0.0000) | Acc: (68.00%) (28914/42368)
Epoch: 15 | Batch_idx: 340 |  Loss: (0.8967) |  Loss2: (0.0000) | Acc: (68.00%) (29794/43648)
Epoch: 15 | Batch_idx: 350 |  Loss: (0.8961) |  Loss2: (0.0000) | Acc: (68.00%) (30676/44928)
Epoch: 15 | Batch_idx: 360 |  Loss: (0.8952) |  Loss2: (0.0000) | Acc: (68.00%) (31560/46208)
Epoch: 15 | Batch_idx: 370 |  Loss: (0.8951) |  Loss2: (0.0000) | Acc: (68.00%) (32410/47488)
Epoch: 15 | Batch_idx: 380 |  Loss: (0.8966) |  Loss2: (0.0000) | Acc: (68.00%) (33254/48768)
Epoch: 15 | Batch_idx: 390 |  Loss: (0.8966) |  Loss2: (0.0000) | Acc: (68.00%) (34086/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_015.pth.tar'
# TEST : Loss: (0.8913) | Acc: (68.00%) (6857/10000)
percent tensor([0.5363, 0.5507, 0.5402, 0.5303, 0.5460, 0.5420, 0.5523, 0.5368, 0.5426,
        0.5482, 0.5430, 0.5463, 0.5377, 0.5420, 0.5434, 0.5385],
       device='cuda:0') torch.Size([16])
percent tensor([0.4866, 0.4820, 0.4834, 0.4866, 0.4808, 0.4902, 0.4821, 0.4822, 0.4815,
        0.4834, 0.4829, 0.4824, 0.4842, 0.4839, 0.4848, 0.4876],
       device='cuda:0') torch.Size([16])
percent tensor([0.4801, 0.4674, 0.4309, 0.4520, 0.4358, 0.4966, 0.4491, 0.4391, 0.4611,
        0.4500, 0.4680, 0.4351, 0.4717, 0.4847, 0.4809, 0.4779],
       device='cuda:0') torch.Size([16])
percent tensor([0.4921, 0.4479, 0.4760, 0.4855, 0.4680, 0.5211, 0.4499, 0.4621, 0.4679,
        0.4440, 0.4506, 0.4647, 0.4688, 0.4652, 0.4796, 0.4852],
       device='cuda:0') torch.Size([16])
percent tensor([0.5347, 0.5213, 0.5208, 0.5275, 0.5139, 0.5355, 0.5220, 0.5207, 0.5277,
        0.5194, 0.5234, 0.5251, 0.5352, 0.5276, 0.5339, 0.5320],
       device='cuda:0') torch.Size([16])
percent tensor([0.5401, 0.5198, 0.5487, 0.5565, 0.5430, 0.5507, 0.5301, 0.5416, 0.5385,
        0.5169, 0.5248, 0.5436, 0.5338, 0.5328, 0.5358, 0.5396],
       device='cuda:0') torch.Size([16])
percent tensor([0.5735, 0.5648, 0.5472, 0.5487, 0.5471, 0.5706, 0.5620, 0.5387, 0.5654,
        0.5743, 0.5717, 0.5644, 0.5796, 0.5799, 0.5502, 0.5618],
       device='cuda:0') torch.Size([16])
percent tensor([0.9975, 0.9931, 0.9934, 0.9956, 0.9924, 0.9909, 0.9940, 0.9984, 0.9924,
        0.9979, 0.9916, 0.9947, 0.9942, 0.9930, 0.9919, 0.9973],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 16 | Batch_idx: 0 |  Loss: (0.9435) |  Loss2: (0.0000) | Acc: (67.00%) (87/128)
Epoch: 16 | Batch_idx: 10 |  Loss: (0.8578) |  Loss2: (0.0000) | Acc: (68.00%) (964/1408)
Epoch: 16 | Batch_idx: 20 |  Loss: (0.8533) |  Loss2: (0.0000) | Acc: (68.00%) (1852/2688)
Epoch: 16 | Batch_idx: 30 |  Loss: (0.8691) |  Loss2: (0.0000) | Acc: (68.00%) (2727/3968)
Epoch: 16 | Batch_idx: 40 |  Loss: (0.8611) |  Loss2: (0.0000) | Acc: (69.00%) (3629/5248)
Epoch: 16 | Batch_idx: 50 |  Loss: (0.8560) |  Loss2: (0.0000) | Acc: (69.00%) (4514/6528)
Epoch: 16 | Batch_idx: 60 |  Loss: (0.8603) |  Loss2: (0.0000) | Acc: (69.00%) (5388/7808)
Epoch: 16 | Batch_idx: 70 |  Loss: (0.8539) |  Loss2: (0.0000) | Acc: (69.00%) (6305/9088)
Epoch: 16 | Batch_idx: 80 |  Loss: (0.8562) |  Loss2: (0.0000) | Acc: (69.00%) (7188/10368)
Epoch: 16 | Batch_idx: 90 |  Loss: (0.8561) |  Loss2: (0.0000) | Acc: (69.00%) (8082/11648)
Epoch: 16 | Batch_idx: 100 |  Loss: (0.8540) |  Loss2: (0.0000) | Acc: (69.00%) (8981/12928)
Epoch: 16 | Batch_idx: 110 |  Loss: (0.8534) |  Loss2: (0.0000) | Acc: (69.00%) (9863/14208)
Epoch: 16 | Batch_idx: 120 |  Loss: (0.8530) |  Loss2: (0.0000) | Acc: (69.00%) (10741/15488)
Epoch: 16 | Batch_idx: 130 |  Loss: (0.8535) |  Loss2: (0.0000) | Acc: (69.00%) (11630/16768)
Epoch: 16 | Batch_idx: 140 |  Loss: (0.8528) |  Loss2: (0.0000) | Acc: (69.00%) (12532/18048)
Epoch: 16 | Batch_idx: 150 |  Loss: (0.8512) |  Loss2: (0.0000) | Acc: (69.00%) (13427/19328)
Epoch: 16 | Batch_idx: 160 |  Loss: (0.8528) |  Loss2: (0.0000) | Acc: (69.00%) (14332/20608)
Epoch: 16 | Batch_idx: 170 |  Loss: (0.8520) |  Loss2: (0.0000) | Acc: (69.00%) (15226/21888)
Epoch: 16 | Batch_idx: 180 |  Loss: (0.8509) |  Loss2: (0.0000) | Acc: (69.00%) (16120/23168)
Epoch: 16 | Batch_idx: 190 |  Loss: (0.8522) |  Loss2: (0.0000) | Acc: (69.00%) (16998/24448)
Epoch: 16 | Batch_idx: 200 |  Loss: (0.8529) |  Loss2: (0.0000) | Acc: (69.00%) (17871/25728)
Epoch: 16 | Batch_idx: 210 |  Loss: (0.8537) |  Loss2: (0.0000) | Acc: (69.00%) (18764/27008)
Epoch: 16 | Batch_idx: 220 |  Loss: (0.8557) |  Loss2: (0.0000) | Acc: (69.00%) (19627/28288)
Epoch: 16 | Batch_idx: 230 |  Loss: (0.8557) |  Loss2: (0.0000) | Acc: (69.00%) (20528/29568)
Epoch: 16 | Batch_idx: 240 |  Loss: (0.8537) |  Loss2: (0.0000) | Acc: (69.00%) (21444/30848)
Epoch: 16 | Batch_idx: 250 |  Loss: (0.8528) |  Loss2: (0.0000) | Acc: (69.00%) (22348/32128)
Epoch: 16 | Batch_idx: 260 |  Loss: (0.8521) |  Loss2: (0.0000) | Acc: (69.00%) (23236/33408)
Epoch: 16 | Batch_idx: 270 |  Loss: (0.8506) |  Loss2: (0.0000) | Acc: (69.00%) (24146/34688)
Epoch: 16 | Batch_idx: 280 |  Loss: (0.8511) |  Loss2: (0.0000) | Acc: (69.00%) (25028/35968)
Epoch: 16 | Batch_idx: 290 |  Loss: (0.8493) |  Loss2: (0.0000) | Acc: (69.00%) (25951/37248)
Epoch: 16 | Batch_idx: 300 |  Loss: (0.8489) |  Loss2: (0.0000) | Acc: (69.00%) (26838/38528)
Epoch: 16 | Batch_idx: 310 |  Loss: (0.8487) |  Loss2: (0.0000) | Acc: (69.00%) (27734/39808)
Epoch: 16 | Batch_idx: 320 |  Loss: (0.8488) |  Loss2: (0.0000) | Acc: (69.00%) (28632/41088)
Epoch: 16 | Batch_idx: 330 |  Loss: (0.8480) |  Loss2: (0.0000) | Acc: (69.00%) (29553/42368)
Epoch: 16 | Batch_idx: 340 |  Loss: (0.8470) |  Loss2: (0.0000) | Acc: (69.00%) (30448/43648)
Epoch: 16 | Batch_idx: 350 |  Loss: (0.8459) |  Loss2: (0.0000) | Acc: (69.00%) (31377/44928)
Epoch: 16 | Batch_idx: 360 |  Loss: (0.8447) |  Loss2: (0.0000) | Acc: (69.00%) (32291/46208)
Epoch: 16 | Batch_idx: 370 |  Loss: (0.8441) |  Loss2: (0.0000) | Acc: (69.00%) (33214/47488)
Epoch: 16 | Batch_idx: 380 |  Loss: (0.8433) |  Loss2: (0.0000) | Acc: (69.00%) (34123/48768)
Epoch: 16 | Batch_idx: 390 |  Loss: (0.8437) |  Loss2: (0.0000) | Acc: (69.00%) (34952/50000)
# TEST : Loss: (1.0799) | Acc: (64.00%) (6402/10000)
percent tensor([0.5370, 0.5506, 0.5375, 0.5274, 0.5422, 0.5401, 0.5509, 0.5353, 0.5410,
        0.5477, 0.5436, 0.5436, 0.5382, 0.5411, 0.5428, 0.5376],
       device='cuda:0') torch.Size([16])
percent tensor([0.4871, 0.4826, 0.4850, 0.4870, 0.4827, 0.4917, 0.4829, 0.4825, 0.4835,
        0.4837, 0.4843, 0.4840, 0.4853, 0.4837, 0.4853, 0.4881],
       device='cuda:0') torch.Size([16])
percent tensor([0.4817, 0.4707, 0.4284, 0.4539, 0.4359, 0.4896, 0.4509, 0.4396, 0.4599,
        0.4505, 0.4731, 0.4365, 0.4765, 0.4850, 0.4798, 0.4812],
       device='cuda:0') torch.Size([16])
percent tensor([0.4834, 0.4451, 0.4661, 0.4799, 0.4600, 0.5169, 0.4462, 0.4573, 0.4712,
        0.4380, 0.4512, 0.4558, 0.4619, 0.4660, 0.4713, 0.4829],
       device='cuda:0') torch.Size([16])
percent tensor([0.5337, 0.5217, 0.5170, 0.5261, 0.5086, 0.5361, 0.5217, 0.5177, 0.5293,
        0.5165, 0.5246, 0.5205, 0.5339, 0.5311, 0.5323, 0.5334],
       device='cuda:0') torch.Size([16])
percent tensor([0.5328, 0.5173, 0.5394, 0.5516, 0.5382, 0.5459, 0.5271, 0.5359, 0.5352,
        0.5142, 0.5204, 0.5325, 0.5249, 0.5319, 0.5272, 0.5379],
       device='cuda:0') torch.Size([16])
percent tensor([0.5759, 0.5675, 0.5518, 0.5515, 0.5544, 0.5727, 0.5677, 0.5409, 0.5619,
        0.5720, 0.5727, 0.5670, 0.5825, 0.5820, 0.5524, 0.5634],
       device='cuda:0') torch.Size([16])
percent tensor([0.9985, 0.9948, 0.9940, 0.9940, 0.9958, 0.9928, 0.9968, 0.9985, 0.9927,
        0.9984, 0.9938, 0.9961, 0.9960, 0.9960, 0.9940, 0.9984],
       device='cuda:0') torch.Size([16])
Epoch: 17 | Batch_idx: 0 |  Loss: (0.9043) |  Loss2: (0.0000) | Acc: (67.00%) (87/128)
Epoch: 17 | Batch_idx: 10 |  Loss: (0.8289) |  Loss2: (0.0000) | Acc: (72.00%) (1015/1408)
Epoch: 17 | Batch_idx: 20 |  Loss: (0.8011) |  Loss2: (0.0000) | Acc: (71.00%) (1934/2688)
Epoch: 17 | Batch_idx: 30 |  Loss: (0.8003) |  Loss2: (0.0000) | Acc: (71.00%) (2850/3968)
Epoch: 17 | Batch_idx: 40 |  Loss: (0.7943) |  Loss2: (0.0000) | Acc: (71.00%) (3771/5248)
Epoch: 17 | Batch_idx: 50 |  Loss: (0.7934) |  Loss2: (0.0000) | Acc: (71.00%) (4683/6528)
Epoch: 17 | Batch_idx: 60 |  Loss: (0.7978) |  Loss2: (0.0000) | Acc: (71.00%) (5598/7808)
Epoch: 17 | Batch_idx: 70 |  Loss: (0.7956) |  Loss2: (0.0000) | Acc: (71.00%) (6514/9088)
Epoch: 17 | Batch_idx: 80 |  Loss: (0.7989) |  Loss2: (0.0000) | Acc: (71.00%) (7429/10368)
Epoch: 17 | Batch_idx: 90 |  Loss: (0.8035) |  Loss2: (0.0000) | Acc: (71.00%) (8321/11648)
Epoch: 17 | Batch_idx: 100 |  Loss: (0.8030) |  Loss2: (0.0000) | Acc: (71.00%) (9243/12928)
Epoch: 17 | Batch_idx: 110 |  Loss: (0.8015) |  Loss2: (0.0000) | Acc: (71.00%) (10148/14208)
Epoch: 17 | Batch_idx: 120 |  Loss: (0.8033) |  Loss2: (0.0000) | Acc: (71.00%) (11058/15488)
Epoch: 17 | Batch_idx: 130 |  Loss: (0.8057) |  Loss2: (0.0000) | Acc: (71.00%) (11961/16768)
Epoch: 17 | Batch_idx: 140 |  Loss: (0.8021) |  Loss2: (0.0000) | Acc: (71.00%) (12899/18048)
Epoch: 17 | Batch_idx: 150 |  Loss: (0.8003) |  Loss2: (0.0000) | Acc: (71.00%) (13809/19328)
Epoch: 17 | Batch_idx: 160 |  Loss: (0.7994) |  Loss2: (0.0000) | Acc: (71.00%) (14728/20608)
Epoch: 17 | Batch_idx: 170 |  Loss: (0.7998) |  Loss2: (0.0000) | Acc: (71.00%) (15643/21888)
Epoch: 17 | Batch_idx: 180 |  Loss: (0.7982) |  Loss2: (0.0000) | Acc: (71.00%) (16586/23168)
Epoch: 17 | Batch_idx: 190 |  Loss: (0.7970) |  Loss2: (0.0000) | Acc: (71.00%) (17503/24448)
Epoch: 17 | Batch_idx: 200 |  Loss: (0.7983) |  Loss2: (0.0000) | Acc: (71.00%) (18398/25728)
Epoch: 17 | Batch_idx: 210 |  Loss: (0.7953) |  Loss2: (0.0000) | Acc: (71.00%) (19337/27008)
Epoch: 17 | Batch_idx: 220 |  Loss: (0.7951) |  Loss2: (0.0000) | Acc: (71.00%) (20258/28288)
Epoch: 17 | Batch_idx: 230 |  Loss: (0.7955) |  Loss2: (0.0000) | Acc: (71.00%) (21168/29568)
Epoch: 17 | Batch_idx: 240 |  Loss: (0.7955) |  Loss2: (0.0000) | Acc: (71.00%) (22077/30848)
Epoch: 17 | Batch_idx: 250 |  Loss: (0.7977) |  Loss2: (0.0000) | Acc: (71.00%) (22973/32128)
Epoch: 17 | Batch_idx: 260 |  Loss: (0.7972) |  Loss2: (0.0000) | Acc: (71.00%) (23885/33408)
Epoch: 17 | Batch_idx: 270 |  Loss: (0.7966) |  Loss2: (0.0000) | Acc: (71.00%) (24814/34688)
Epoch: 17 | Batch_idx: 280 |  Loss: (0.7965) |  Loss2: (0.0000) | Acc: (71.00%) (25731/35968)
Epoch: 17 | Batch_idx: 290 |  Loss: (0.7948) |  Loss2: (0.0000) | Acc: (71.00%) (26671/37248)
Epoch: 17 | Batch_idx: 300 |  Loss: (0.7947) |  Loss2: (0.0000) | Acc: (71.00%) (27605/38528)
Epoch: 17 | Batch_idx: 310 |  Loss: (0.7943) |  Loss2: (0.0000) | Acc: (71.00%) (28532/39808)
Epoch: 17 | Batch_idx: 320 |  Loss: (0.7932) |  Loss2: (0.0000) | Acc: (71.00%) (29475/41088)
Epoch: 17 | Batch_idx: 330 |  Loss: (0.7914) |  Loss2: (0.0000) | Acc: (71.00%) (30415/42368)
Epoch: 17 | Batch_idx: 340 |  Loss: (0.7912) |  Loss2: (0.0000) | Acc: (71.00%) (31336/43648)
Epoch: 17 | Batch_idx: 350 |  Loss: (0.7917) |  Loss2: (0.0000) | Acc: (71.00%) (32249/44928)
Epoch: 17 | Batch_idx: 360 |  Loss: (0.7907) |  Loss2: (0.0000) | Acc: (71.00%) (33187/46208)
Epoch: 17 | Batch_idx: 370 |  Loss: (0.7905) |  Loss2: (0.0000) | Acc: (71.00%) (34118/47488)
Epoch: 17 | Batch_idx: 380 |  Loss: (0.7897) |  Loss2: (0.0000) | Acc: (71.00%) (35048/48768)
Epoch: 17 | Batch_idx: 390 |  Loss: (0.7891) |  Loss2: (0.0000) | Acc: (71.00%) (35947/50000)
# TEST : Loss: (0.8339) | Acc: (70.00%) (7069/10000)
percent tensor([0.5379, 0.5506, 0.5389, 0.5287, 0.5434, 0.5401, 0.5513, 0.5367, 0.5413,
        0.5490, 0.5440, 0.5455, 0.5390, 0.5398, 0.5431, 0.5382],
       device='cuda:0') torch.Size([16])
percent tensor([0.4856, 0.4818, 0.4854, 0.4870, 0.4824, 0.4894, 0.4824, 0.4832, 0.4826,
        0.4834, 0.4831, 0.4840, 0.4841, 0.4831, 0.4838, 0.4871],
       device='cuda:0') torch.Size([16])
percent tensor([0.4782, 0.4675, 0.4329, 0.4498, 0.4354, 0.4888, 0.4514, 0.4416, 0.4611,
        0.4501, 0.4701, 0.4375, 0.4716, 0.4793, 0.4767, 0.4787],
       device='cuda:0') torch.Size([16])
percent tensor([0.4835, 0.4474, 0.4678, 0.4771, 0.4609, 0.5177, 0.4507, 0.4550, 0.4722,
        0.4404, 0.4555, 0.4558, 0.4602, 0.4715, 0.4741, 0.4835],
       device='cuda:0') torch.Size([16])
percent tensor([0.5338, 0.5242, 0.5155, 0.5247, 0.5088, 0.5364, 0.5247, 0.5188, 0.5301,
        0.5203, 0.5273, 0.5219, 0.5349, 0.5306, 0.5349, 0.5334],
       device='cuda:0') torch.Size([16])
percent tensor([0.5383, 0.5250, 0.5415, 0.5514, 0.5392, 0.5483, 0.5314, 0.5370, 0.5352,
        0.5162, 0.5249, 0.5350, 0.5291, 0.5392, 0.5353, 0.5396],
       device='cuda:0') torch.Size([16])
percent tensor([0.5744, 0.5622, 0.5504, 0.5558, 0.5535, 0.5716, 0.5661, 0.5383, 0.5624,
        0.5656, 0.5753, 0.5653, 0.5804, 0.5823, 0.5536, 0.5621],
       device='cuda:0') torch.Size([16])
percent tensor([0.9988, 0.9918, 0.9933, 0.9947, 0.9957, 0.9939, 0.9972, 0.9977, 0.9921,
        0.9979, 0.9932, 0.9962, 0.9958, 0.9948, 0.9957, 0.9981],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 18 | Batch_idx: 0 |  Loss: (0.7597) |  Loss2: (0.0000) | Acc: (70.00%) (90/128)
Epoch: 18 | Batch_idx: 10 |  Loss: (0.7991) |  Loss2: (0.0000) | Acc: (69.00%) (982/1408)
Epoch: 18 | Batch_idx: 20 |  Loss: (0.8273) |  Loss2: (0.0000) | Acc: (69.00%) (1858/2688)
Epoch: 18 | Batch_idx: 30 |  Loss: (0.8471) |  Loss2: (0.0000) | Acc: (68.00%) (2724/3968)
Epoch: 18 | Batch_idx: 40 |  Loss: (0.8612) |  Loss2: (0.0000) | Acc: (68.00%) (3607/5248)
Epoch: 18 | Batch_idx: 50 |  Loss: (0.8656) |  Loss2: (0.0000) | Acc: (68.00%) (4483/6528)
Epoch: 18 | Batch_idx: 60 |  Loss: (0.8638) |  Loss2: (0.0000) | Acc: (68.00%) (5387/7808)
Epoch: 18 | Batch_idx: 70 |  Loss: (0.8689) |  Loss2: (0.0000) | Acc: (68.00%) (6257/9088)
Epoch: 18 | Batch_idx: 80 |  Loss: (0.8749) |  Loss2: (0.0000) | Acc: (68.00%) (7113/10368)
Epoch: 18 | Batch_idx: 90 |  Loss: (0.8724) |  Loss2: (0.0000) | Acc: (68.00%) (7998/11648)
Epoch: 18 | Batch_idx: 100 |  Loss: (0.8771) |  Loss2: (0.0000) | Acc: (68.00%) (8865/12928)
Epoch: 18 | Batch_idx: 110 |  Loss: (0.8776) |  Loss2: (0.0000) | Acc: (68.00%) (9756/14208)
Epoch: 18 | Batch_idx: 120 |  Loss: (0.8772) |  Loss2: (0.0000) | Acc: (68.00%) (10643/15488)
Epoch: 18 | Batch_idx: 130 |  Loss: (0.8778) |  Loss2: (0.0000) | Acc: (68.00%) (11519/16768)
Epoch: 18 | Batch_idx: 140 |  Loss: (0.8771) |  Loss2: (0.0000) | Acc: (68.00%) (12415/18048)
Epoch: 18 | Batch_idx: 150 |  Loss: (0.8753) |  Loss2: (0.0000) | Acc: (68.00%) (13311/19328)
Epoch: 18 | Batch_idx: 160 |  Loss: (0.8757) |  Loss2: (0.0000) | Acc: (68.00%) (14192/20608)
Epoch: 18 | Batch_idx: 170 |  Loss: (0.8784) |  Loss2: (0.0000) | Acc: (68.00%) (15072/21888)
Epoch: 18 | Batch_idx: 180 |  Loss: (0.8780) |  Loss2: (0.0000) | Acc: (68.00%) (15954/23168)
Epoch: 18 | Batch_idx: 190 |  Loss: (0.8769) |  Loss2: (0.0000) | Acc: (68.00%) (16839/24448)
Epoch: 18 | Batch_idx: 200 |  Loss: (0.8752) |  Loss2: (0.0000) | Acc: (68.00%) (17723/25728)
Epoch: 18 | Batch_idx: 210 |  Loss: (0.8772) |  Loss2: (0.0000) | Acc: (68.00%) (18594/27008)
Epoch: 18 | Batch_idx: 220 |  Loss: (0.8727) |  Loss2: (0.0000) | Acc: (69.00%) (19542/28288)
Epoch: 18 | Batch_idx: 230 |  Loss: (0.8727) |  Loss2: (0.0000) | Acc: (69.00%) (20424/29568)
Epoch: 18 | Batch_idx: 240 |  Loss: (0.8698) |  Loss2: (0.0000) | Acc: (69.00%) (21353/30848)
Epoch: 18 | Batch_idx: 250 |  Loss: (0.8676) |  Loss2: (0.0000) | Acc: (69.00%) (22264/32128)
Epoch: 18 | Batch_idx: 260 |  Loss: (0.8660) |  Loss2: (0.0000) | Acc: (69.00%) (23184/33408)
Epoch: 18 | Batch_idx: 270 |  Loss: (0.8632) |  Loss2: (0.0000) | Acc: (69.00%) (24101/34688)
Epoch: 18 | Batch_idx: 280 |  Loss: (0.8611) |  Loss2: (0.0000) | Acc: (69.00%) (25007/35968)
Epoch: 18 | Batch_idx: 290 |  Loss: (0.8598) |  Loss2: (0.0000) | Acc: (69.00%) (25928/37248)
Epoch: 18 | Batch_idx: 300 |  Loss: (0.8594) |  Loss2: (0.0000) | Acc: (69.00%) (26812/38528)
Epoch: 18 | Batch_idx: 310 |  Loss: (0.8582) |  Loss2: (0.0000) | Acc: (69.00%) (27695/39808)
Epoch: 18 | Batch_idx: 320 |  Loss: (0.8576) |  Loss2: (0.0000) | Acc: (69.00%) (28597/41088)
Epoch: 18 | Batch_idx: 330 |  Loss: (0.8569) |  Loss2: (0.0000) | Acc: (69.00%) (29502/42368)
Epoch: 18 | Batch_idx: 340 |  Loss: (0.8558) |  Loss2: (0.0000) | Acc: (69.00%) (30400/43648)
Epoch: 18 | Batch_idx: 350 |  Loss: (0.8554) |  Loss2: (0.0000) | Acc: (69.00%) (31292/44928)
Epoch: 18 | Batch_idx: 360 |  Loss: (0.8527) |  Loss2: (0.0000) | Acc: (69.00%) (32237/46208)
Epoch: 18 | Batch_idx: 370 |  Loss: (0.8515) |  Loss2: (0.0000) | Acc: (69.00%) (33146/47488)
Epoch: 18 | Batch_idx: 380 |  Loss: (0.8505) |  Loss2: (0.0000) | Acc: (69.00%) (34057/48768)
Epoch: 18 | Batch_idx: 390 |  Loss: (0.8490) |  Loss2: (0.0000) | Acc: (69.00%) (34938/50000)
# TEST : Loss: (0.8497) | Acc: (70.00%) (7017/10000)
percent tensor([0.5212, 0.5305, 0.5283, 0.5175, 0.5314, 0.5272, 0.5327, 0.5229, 0.5240,
        0.5311, 0.5254, 0.5323, 0.5215, 0.5204, 0.5271, 0.5213],
       device='cuda:0') torch.Size([16])
percent tensor([0.4915, 0.4875, 0.4929, 0.4950, 0.4906, 0.4962, 0.4887, 0.4909, 0.4890,
        0.4895, 0.4885, 0.4909, 0.4896, 0.4885, 0.4907, 0.4929],
       device='cuda:0') torch.Size([16])
percent tensor([0.4866, 0.4777, 0.4111, 0.4378, 0.4268, 0.5036, 0.4528, 0.4280, 0.4600,
        0.4502, 0.4798, 0.4220, 0.4783, 0.4881, 0.4901, 0.4868],
       device='cuda:0') torch.Size([16])
percent tensor([0.5047, 0.4469, 0.5026, 0.5116, 0.4990, 0.5342, 0.4656, 0.4910, 0.4926,
        0.4433, 0.4627, 0.4824, 0.4719, 0.4792, 0.4957, 0.5022],
       device='cuda:0') torch.Size([16])
percent tensor([0.5566, 0.5482, 0.5299, 0.5391, 0.5254, 0.5551, 0.5480, 0.5408, 0.5472,
        0.5417, 0.5496, 0.5431, 0.5573, 0.5510, 0.5643, 0.5554],
       device='cuda:0') torch.Size([16])
percent tensor([0.5617, 0.5422, 0.5791, 0.5915, 0.5717, 0.5734, 0.5540, 0.5734, 0.5575,
        0.5293, 0.5431, 0.5664, 0.5480, 0.5617, 0.5615, 0.5624],
       device='cuda:0') torch.Size([16])
percent tensor([0.5819, 0.5707, 0.5464, 0.5498, 0.5497, 0.5741, 0.5663, 0.5284, 0.5660,
        0.5741, 0.5845, 0.5615, 0.5863, 0.5919, 0.5454, 0.5630],
       device='cuda:0') torch.Size([16])
percent tensor([0.9979, 0.9903, 0.9940, 0.9956, 0.9948, 0.9938, 0.9960, 0.9984, 0.9899,
        0.9972, 0.9925, 0.9941, 0.9940, 0.9939, 0.9957, 0.9981],
       device='cuda:0') torch.Size([16])
Epoch: 19 | Batch_idx: 0 |  Loss: (0.9728) |  Loss2: (0.0000) | Acc: (67.00%) (87/128)
Epoch: 19 | Batch_idx: 10 |  Loss: (0.8114) |  Loss2: (0.0000) | Acc: (71.00%) (1007/1408)
Epoch: 19 | Batch_idx: 20 |  Loss: (0.8063) |  Loss2: (0.0000) | Acc: (71.00%) (1928/2688)
Epoch: 19 | Batch_idx: 30 |  Loss: (0.8126) |  Loss2: (0.0000) | Acc: (71.00%) (2841/3968)
Epoch: 19 | Batch_idx: 40 |  Loss: (0.8063) |  Loss2: (0.0000) | Acc: (71.00%) (3748/5248)
Epoch: 19 | Batch_idx: 50 |  Loss: (0.8021) |  Loss2: (0.0000) | Acc: (71.00%) (4690/6528)
Epoch: 19 | Batch_idx: 60 |  Loss: (0.8004) |  Loss2: (0.0000) | Acc: (71.00%) (5617/7808)
Epoch: 19 | Batch_idx: 70 |  Loss: (0.7972) |  Loss2: (0.0000) | Acc: (72.00%) (6547/9088)
Epoch: 19 | Batch_idx: 80 |  Loss: (0.7952) |  Loss2: (0.0000) | Acc: (71.00%) (7461/10368)
Epoch: 19 | Batch_idx: 90 |  Loss: (0.7957) |  Loss2: (0.0000) | Acc: (71.00%) (8382/11648)
Epoch: 19 | Batch_idx: 100 |  Loss: (0.8007) |  Loss2: (0.0000) | Acc: (71.00%) (9294/12928)
Epoch: 19 | Batch_idx: 110 |  Loss: (0.7971) |  Loss2: (0.0000) | Acc: (71.00%) (10221/14208)
Epoch: 19 | Batch_idx: 120 |  Loss: (0.7984) |  Loss2: (0.0000) | Acc: (71.00%) (11120/15488)
Epoch: 19 | Batch_idx: 130 |  Loss: (0.7983) |  Loss2: (0.0000) | Acc: (71.00%) (12041/16768)
Epoch: 19 | Batch_idx: 140 |  Loss: (0.7964) |  Loss2: (0.0000) | Acc: (71.00%) (12958/18048)
Epoch: 19 | Batch_idx: 150 |  Loss: (0.7989) |  Loss2: (0.0000) | Acc: (71.00%) (13848/19328)
Epoch: 19 | Batch_idx: 160 |  Loss: (0.8001) |  Loss2: (0.0000) | Acc: (71.00%) (14759/20608)
Epoch: 19 | Batch_idx: 170 |  Loss: (0.8009) |  Loss2: (0.0000) | Acc: (71.00%) (15671/21888)
Epoch: 19 | Batch_idx: 180 |  Loss: (0.7992) |  Loss2: (0.0000) | Acc: (71.00%) (16616/23168)
Epoch: 19 | Batch_idx: 190 |  Loss: (0.7996) |  Loss2: (0.0000) | Acc: (71.00%) (17533/24448)
Epoch: 19 | Batch_idx: 200 |  Loss: (0.7998) |  Loss2: (0.0000) | Acc: (71.00%) (18435/25728)
Epoch: 19 | Batch_idx: 210 |  Loss: (0.8015) |  Loss2: (0.0000) | Acc: (71.00%) (19349/27008)
Epoch: 19 | Batch_idx: 220 |  Loss: (0.8031) |  Loss2: (0.0000) | Acc: (71.00%) (20257/28288)
Epoch: 19 | Batch_idx: 230 |  Loss: (0.8036) |  Loss2: (0.0000) | Acc: (71.00%) (21176/29568)
Epoch: 19 | Batch_idx: 240 |  Loss: (0.8031) |  Loss2: (0.0000) | Acc: (71.00%) (22105/30848)
Epoch: 19 | Batch_idx: 250 |  Loss: (0.8019) |  Loss2: (0.0000) | Acc: (71.00%) (23027/32128)
Epoch: 19 | Batch_idx: 260 |  Loss: (0.8024) |  Loss2: (0.0000) | Acc: (71.00%) (23941/33408)
Epoch: 19 | Batch_idx: 270 |  Loss: (0.8037) |  Loss2: (0.0000) | Acc: (71.00%) (24833/34688)
Epoch: 19 | Batch_idx: 280 |  Loss: (0.8019) |  Loss2: (0.0000) | Acc: (71.00%) (25769/35968)
Epoch: 19 | Batch_idx: 290 |  Loss: (0.8008) |  Loss2: (0.0000) | Acc: (71.00%) (26692/37248)
Epoch: 19 | Batch_idx: 300 |  Loss: (0.7999) |  Loss2: (0.0000) | Acc: (71.00%) (27608/38528)
Epoch: 19 | Batch_idx: 310 |  Loss: (0.8000) |  Loss2: (0.0000) | Acc: (71.00%) (28515/39808)
Epoch: 19 | Batch_idx: 320 |  Loss: (0.7999) |  Loss2: (0.0000) | Acc: (71.00%) (29445/41088)
Epoch: 19 | Batch_idx: 330 |  Loss: (0.7991) |  Loss2: (0.0000) | Acc: (71.00%) (30363/42368)
Epoch: 19 | Batch_idx: 340 |  Loss: (0.7982) |  Loss2: (0.0000) | Acc: (71.00%) (31301/43648)
Epoch: 19 | Batch_idx: 350 |  Loss: (0.7985) |  Loss2: (0.0000) | Acc: (71.00%) (32213/44928)
Epoch: 19 | Batch_idx: 360 |  Loss: (0.7987) |  Loss2: (0.0000) | Acc: (71.00%) (33098/46208)
Epoch: 19 | Batch_idx: 370 |  Loss: (0.7978) |  Loss2: (0.0000) | Acc: (71.00%) (34035/47488)
Epoch: 19 | Batch_idx: 380 |  Loss: (0.7964) |  Loss2: (0.0000) | Acc: (71.00%) (34975/48768)
Epoch: 19 | Batch_idx: 390 |  Loss: (0.7963) |  Loss2: (0.0000) | Acc: (71.00%) (35861/50000)
# TEST : Loss: (0.8099) | Acc: (71.00%) (7153/10000)
percent tensor([0.5210, 0.5313, 0.5318, 0.5189, 0.5349, 0.5294, 0.5343, 0.5250, 0.5242,
        0.5325, 0.5253, 0.5353, 0.5212, 0.5202, 0.5286, 0.5215],
       device='cuda:0') torch.Size([16])
percent tensor([0.4962, 0.4924, 0.4986, 0.5006, 0.4965, 0.5008, 0.4938, 0.4967, 0.4943,
        0.4942, 0.4933, 0.4962, 0.4942, 0.4935, 0.4960, 0.4977],
       device='cuda:0') torch.Size([16])
percent tensor([0.4938, 0.4877, 0.4032, 0.4307, 0.4266, 0.5117, 0.4597, 0.4253, 0.4635,
        0.4553, 0.4897, 0.4195, 0.4845, 0.4934, 0.4999, 0.4924],
       device='cuda:0') torch.Size([16])
percent tensor([0.5126, 0.4474, 0.5116, 0.5208, 0.5091, 0.5414, 0.4714, 0.5019, 0.4996,
        0.4434, 0.4650, 0.4894, 0.4741, 0.4841, 0.5063, 0.5079],
       device='cuda:0') torch.Size([16])
percent tensor([0.5744, 0.5653, 0.5475, 0.5558, 0.5393, 0.5656, 0.5658, 0.5618, 0.5630,
        0.5591, 0.5659, 0.5633, 0.5744, 0.5672, 0.5847, 0.5723],
       device='cuda:0') torch.Size([16])
percent tensor([0.5751, 0.5514, 0.6034, 0.6173, 0.5928, 0.5882, 0.5658, 0.5958, 0.5730,
        0.5365, 0.5526, 0.5856, 0.5578, 0.5763, 0.5755, 0.5745],
       device='cuda:0') torch.Size([16])
percent tensor([0.5930, 0.5848, 0.5441, 0.5472, 0.5527, 0.5845, 0.5751, 0.5210, 0.5788,
        0.5877, 0.6013, 0.5644, 0.6038, 0.6085, 0.5440, 0.5677],
       device='cuda:0') torch.Size([16])
percent tensor([0.9983, 0.9922, 0.9949, 0.9963, 0.9963, 0.9946, 0.9967, 0.9989, 0.9923,
        0.9979, 0.9941, 0.9950, 0.9957, 0.9952, 0.9967, 0.9984],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 20 | Batch_idx: 0 |  Loss: (0.7370) |  Loss2: (0.0000) | Acc: (73.00%) (94/128)
Epoch: 20 | Batch_idx: 10 |  Loss: (0.7570) |  Loss2: (0.0000) | Acc: (73.00%) (1031/1408)
Epoch: 20 | Batch_idx: 20 |  Loss: (0.7504) |  Loss2: (0.0000) | Acc: (73.00%) (1976/2688)
Epoch: 20 | Batch_idx: 30 |  Loss: (0.7523) |  Loss2: (0.0000) | Acc: (73.00%) (2919/3968)
Epoch: 20 | Batch_idx: 40 |  Loss: (0.7531) |  Loss2: (0.0000) | Acc: (73.00%) (3852/5248)
Epoch: 20 | Batch_idx: 50 |  Loss: (0.7520) |  Loss2: (0.0000) | Acc: (73.00%) (4792/6528)
Epoch: 20 | Batch_idx: 60 |  Loss: (0.7484) |  Loss2: (0.0000) | Acc: (73.00%) (5734/7808)
Epoch: 20 | Batch_idx: 70 |  Loss: (0.7494) |  Loss2: (0.0000) | Acc: (73.00%) (6686/9088)
Epoch: 20 | Batch_idx: 80 |  Loss: (0.7479) |  Loss2: (0.0000) | Acc: (73.00%) (7640/10368)
Epoch: 20 | Batch_idx: 90 |  Loss: (0.7466) |  Loss2: (0.0000) | Acc: (73.00%) (8571/11648)
Epoch: 20 | Batch_idx: 100 |  Loss: (0.7487) |  Loss2: (0.0000) | Acc: (73.00%) (9504/12928)
Epoch: 20 | Batch_idx: 110 |  Loss: (0.7486) |  Loss2: (0.0000) | Acc: (73.00%) (10437/14208)
Epoch: 20 | Batch_idx: 120 |  Loss: (0.7488) |  Loss2: (0.0000) | Acc: (73.00%) (11380/15488)
Epoch: 20 | Batch_idx: 130 |  Loss: (0.7455) |  Loss2: (0.0000) | Acc: (73.00%) (12343/16768)
Epoch: 20 | Batch_idx: 140 |  Loss: (0.7469) |  Loss2: (0.0000) | Acc: (73.00%) (13271/18048)
Epoch: 20 | Batch_idx: 150 |  Loss: (0.7457) |  Loss2: (0.0000) | Acc: (73.00%) (14226/19328)
Epoch: 20 | Batch_idx: 160 |  Loss: (0.7456) |  Loss2: (0.0000) | Acc: (73.00%) (15169/20608)
Epoch: 20 | Batch_idx: 170 |  Loss: (0.7473) |  Loss2: (0.0000) | Acc: (73.00%) (16092/21888)
Epoch: 20 | Batch_idx: 180 |  Loss: (0.7456) |  Loss2: (0.0000) | Acc: (73.00%) (17047/23168)
Epoch: 20 | Batch_idx: 190 |  Loss: (0.7469) |  Loss2: (0.0000) | Acc: (73.00%) (17984/24448)
Epoch: 20 | Batch_idx: 200 |  Loss: (0.7451) |  Loss2: (0.0000) | Acc: (73.00%) (18935/25728)
Epoch: 20 | Batch_idx: 210 |  Loss: (0.7457) |  Loss2: (0.0000) | Acc: (73.00%) (19870/27008)
Epoch: 20 | Batch_idx: 220 |  Loss: (0.7461) |  Loss2: (0.0000) | Acc: (73.00%) (20808/28288)
Epoch: 20 | Batch_idx: 230 |  Loss: (0.7425) |  Loss2: (0.0000) | Acc: (73.00%) (21798/29568)
Epoch: 20 | Batch_idx: 240 |  Loss: (0.7422) |  Loss2: (0.0000) | Acc: (73.00%) (22747/30848)
Epoch: 20 | Batch_idx: 250 |  Loss: (0.7418) |  Loss2: (0.0000) | Acc: (73.00%) (23681/32128)
Epoch: 20 | Batch_idx: 260 |  Loss: (0.7426) |  Loss2: (0.0000) | Acc: (73.00%) (24610/33408)
Epoch: 20 | Batch_idx: 270 |  Loss: (0.7420) |  Loss2: (0.0000) | Acc: (73.00%) (25541/34688)
Epoch: 20 | Batch_idx: 280 |  Loss: (0.7418) |  Loss2: (0.0000) | Acc: (73.00%) (26487/35968)
Epoch: 20 | Batch_idx: 290 |  Loss: (0.7417) |  Loss2: (0.0000) | Acc: (73.00%) (27435/37248)
Epoch: 20 | Batch_idx: 300 |  Loss: (0.7427) |  Loss2: (0.0000) | Acc: (73.00%) (28371/38528)
Epoch: 20 | Batch_idx: 310 |  Loss: (0.7433) |  Loss2: (0.0000) | Acc: (73.00%) (29319/39808)
Epoch: 20 | Batch_idx: 320 |  Loss: (0.7437) |  Loss2: (0.0000) | Acc: (73.00%) (30258/41088)
Epoch: 20 | Batch_idx: 330 |  Loss: (0.7441) |  Loss2: (0.0000) | Acc: (73.00%) (31182/42368)
Epoch: 20 | Batch_idx: 340 |  Loss: (0.7434) |  Loss2: (0.0000) | Acc: (73.00%) (32126/43648)
Epoch: 20 | Batch_idx: 350 |  Loss: (0.7443) |  Loss2: (0.0000) | Acc: (73.00%) (33077/44928)
Epoch: 20 | Batch_idx: 360 |  Loss: (0.7440) |  Loss2: (0.0000) | Acc: (73.00%) (34026/46208)
Epoch: 20 | Batch_idx: 370 |  Loss: (0.7430) |  Loss2: (0.0000) | Acc: (73.00%) (34984/47488)
Epoch: 20 | Batch_idx: 380 |  Loss: (0.7418) |  Loss2: (0.0000) | Acc: (73.00%) (35952/48768)
Epoch: 20 | Batch_idx: 390 |  Loss: (0.7406) |  Loss2: (0.0000) | Acc: (73.00%) (36887/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_020.pth.tar'
# TEST : Loss: (0.7673) | Acc: (72.00%) (7293/10000)
percent tensor([0.5216, 0.5341, 0.5289, 0.5191, 0.5321, 0.5306, 0.5349, 0.5238, 0.5240,
        0.5326, 0.5269, 0.5331, 0.5218, 0.5241, 0.5307, 0.5235],
       device='cuda:0') torch.Size([16])
percent tensor([0.4963, 0.4926, 0.4993, 0.5004, 0.4974, 0.5005, 0.4942, 0.4976, 0.4945,
        0.4943, 0.4932, 0.4967, 0.4942, 0.4935, 0.4957, 0.4973],
       device='cuda:0') torch.Size([16])
percent tensor([0.4956, 0.4891, 0.4172, 0.4394, 0.4291, 0.5011, 0.4592, 0.4320, 0.4615,
        0.4567, 0.4858, 0.4300, 0.4858, 0.4919, 0.4962, 0.4929],
       device='cuda:0') torch.Size([16])
percent tensor([0.5065, 0.4482, 0.5074, 0.5222, 0.5067, 0.5378, 0.4691, 0.5037, 0.4897,
        0.4418, 0.4568, 0.4786, 0.4628, 0.4753, 0.5020, 0.5027],
       device='cuda:0') torch.Size([16])
percent tensor([0.5754, 0.5615, 0.5532, 0.5545, 0.5397, 0.5581, 0.5648, 0.5562, 0.5657,
        0.5580, 0.5636, 0.5689, 0.5777, 0.5662, 0.5760, 0.5683],
       device='cuda:0') torch.Size([16])
percent tensor([0.5696, 0.5458, 0.5845, 0.6067, 0.5848, 0.5834, 0.5650, 0.5794, 0.5682,
        0.5390, 0.5493, 0.5842, 0.5567, 0.5708, 0.5668, 0.5642],
       device='cuda:0') torch.Size([16])
percent tensor([0.5901, 0.5851, 0.5451, 0.5401, 0.5559, 0.5863, 0.5788, 0.5272, 0.5838,
        0.5831, 0.6011, 0.5639, 0.6131, 0.6043, 0.5459, 0.5769],
       device='cuda:0') torch.Size([16])
percent tensor([0.9989, 0.9917, 0.9951, 0.9937, 0.9958, 0.9910, 0.9980, 0.9988, 0.9945,
        0.9980, 0.9966, 0.9976, 0.9977, 0.9948, 0.9938, 0.9976],
       device='cuda:0') torch.Size([16])
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(170.1261, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(782.7952, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(777.2084, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1528.3845, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(505.7541, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2175.0605, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4305.0024, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1439.7015, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6104.2983, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12179.6484, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4062.2659, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(17202.0605, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 21 | Batch_idx: 0 |  Loss: (0.7152) |  Loss2: (0.0000) | Acc: (76.00%) (98/128)
Epoch: 21 | Batch_idx: 10 |  Loss: (0.6988) |  Loss2: (0.0000) | Acc: (75.00%) (1069/1408)
Epoch: 21 | Batch_idx: 20 |  Loss: (0.7055) |  Loss2: (0.0000) | Acc: (75.00%) (2023/2688)
Epoch: 21 | Batch_idx: 30 |  Loss: (0.7023) |  Loss2: (0.0000) | Acc: (75.00%) (3000/3968)
Epoch: 21 | Batch_idx: 40 |  Loss: (0.6890) |  Loss2: (0.0000) | Acc: (76.00%) (3994/5248)
Epoch: 21 | Batch_idx: 50 |  Loss: (0.6974) |  Loss2: (0.0000) | Acc: (75.00%) (4928/6528)
Epoch: 21 | Batch_idx: 60 |  Loss: (0.7014) |  Loss2: (0.0000) | Acc: (75.00%) (5888/7808)
Epoch: 21 | Batch_idx: 70 |  Loss: (0.7008) |  Loss2: (0.0000) | Acc: (75.00%) (6861/9088)
Epoch: 21 | Batch_idx: 80 |  Loss: (0.6993) |  Loss2: (0.0000) | Acc: (75.00%) (7845/10368)
Epoch: 21 | Batch_idx: 90 |  Loss: (0.7001) |  Loss2: (0.0000) | Acc: (75.00%) (8798/11648)
Epoch: 21 | Batch_idx: 100 |  Loss: (0.7035) |  Loss2: (0.0000) | Acc: (75.00%) (9759/12928)
Epoch: 21 | Batch_idx: 110 |  Loss: (0.7052) |  Loss2: (0.0000) | Acc: (75.00%) (10699/14208)
Epoch: 21 | Batch_idx: 120 |  Loss: (0.7063) |  Loss2: (0.0000) | Acc: (75.00%) (11666/15488)
Epoch: 21 | Batch_idx: 130 |  Loss: (0.7072) |  Loss2: (0.0000) | Acc: (75.00%) (12620/16768)
Epoch: 21 | Batch_idx: 140 |  Loss: (0.7081) |  Loss2: (0.0000) | Acc: (75.00%) (13578/18048)
Epoch: 21 | Batch_idx: 150 |  Loss: (0.7077) |  Loss2: (0.0000) | Acc: (75.00%) (14546/19328)
Epoch: 21 | Batch_idx: 160 |  Loss: (0.7060) |  Loss2: (0.0000) | Acc: (75.00%) (15511/20608)
Epoch: 21 | Batch_idx: 170 |  Loss: (0.7056) |  Loss2: (0.0000) | Acc: (75.00%) (16475/21888)
Epoch: 21 | Batch_idx: 180 |  Loss: (0.7053) |  Loss2: (0.0000) | Acc: (75.00%) (17442/23168)
Epoch: 21 | Batch_idx: 190 |  Loss: (0.7058) |  Loss2: (0.0000) | Acc: (75.00%) (18408/24448)
Epoch: 21 | Batch_idx: 200 |  Loss: (0.7055) |  Loss2: (0.0000) | Acc: (75.00%) (19366/25728)
Epoch: 21 | Batch_idx: 210 |  Loss: (0.7035) |  Loss2: (0.0000) | Acc: (75.00%) (20359/27008)
Epoch: 21 | Batch_idx: 220 |  Loss: (0.7033) |  Loss2: (0.0000) | Acc: (75.00%) (21334/28288)
Epoch: 21 | Batch_idx: 230 |  Loss: (0.7020) |  Loss2: (0.0000) | Acc: (75.00%) (22312/29568)
Epoch: 21 | Batch_idx: 240 |  Loss: (0.7032) |  Loss2: (0.0000) | Acc: (75.00%) (23244/30848)
Epoch: 21 | Batch_idx: 250 |  Loss: (0.7019) |  Loss2: (0.0000) | Acc: (75.00%) (24216/32128)
Epoch: 21 | Batch_idx: 260 |  Loss: (0.7010) |  Loss2: (0.0000) | Acc: (75.00%) (25190/33408)
Epoch: 21 | Batch_idx: 270 |  Loss: (0.7003) |  Loss2: (0.0000) | Acc: (75.00%) (26159/34688)
Epoch: 21 | Batch_idx: 280 |  Loss: (0.6991) |  Loss2: (0.0000) | Acc: (75.00%) (27135/35968)
Epoch: 21 | Batch_idx: 290 |  Loss: (0.6990) |  Loss2: (0.0000) | Acc: (75.00%) (28101/37248)
Epoch: 21 | Batch_idx: 300 |  Loss: (0.6998) |  Loss2: (0.0000) | Acc: (75.00%) (29055/38528)
Epoch: 21 | Batch_idx: 310 |  Loss: (0.7000) |  Loss2: (0.0000) | Acc: (75.00%) (30020/39808)
Epoch: 21 | Batch_idx: 320 |  Loss: (0.7008) |  Loss2: (0.0000) | Acc: (75.00%) (30958/41088)
Epoch: 21 | Batch_idx: 330 |  Loss: (0.7006) |  Loss2: (0.0000) | Acc: (75.00%) (31924/42368)
Epoch: 21 | Batch_idx: 340 |  Loss: (0.7006) |  Loss2: (0.0000) | Acc: (75.00%) (32896/43648)
Epoch: 21 | Batch_idx: 350 |  Loss: (0.7013) |  Loss2: (0.0000) | Acc: (75.00%) (33843/44928)
Epoch: 21 | Batch_idx: 360 |  Loss: (0.7008) |  Loss2: (0.0000) | Acc: (75.00%) (34828/46208)
Epoch: 21 | Batch_idx: 370 |  Loss: (0.7002) |  Loss2: (0.0000) | Acc: (75.00%) (35814/47488)
Epoch: 21 | Batch_idx: 380 |  Loss: (0.7003) |  Loss2: (0.0000) | Acc: (75.00%) (36768/48768)
Epoch: 21 | Batch_idx: 390 |  Loss: (0.7006) |  Loss2: (0.0000) | Acc: (75.00%) (37695/50000)
# TEST : Loss: (0.8073) | Acc: (71.00%) (7157/10000)
percent tensor([0.5217, 0.5338, 0.5283, 0.5176, 0.5311, 0.5300, 0.5346, 0.5226, 0.5241,
        0.5316, 0.5268, 0.5316, 0.5217, 0.5236, 0.5301, 0.5233],
       device='cuda:0') torch.Size([16])
percent tensor([0.4959, 0.4931, 0.4977, 0.4992, 0.4957, 0.5003, 0.4941, 0.4965, 0.4946,
        0.4941, 0.4935, 0.4954, 0.4943, 0.4944, 0.4958, 0.4979],
       device='cuda:0') torch.Size([16])
percent tensor([0.4948, 0.4930, 0.4138, 0.4303, 0.4222, 0.4958, 0.4629, 0.4354, 0.4651,
        0.4622, 0.4886, 0.4251, 0.4872, 0.4948, 0.4960, 0.4925],
       device='cuda:0') torch.Size([16])
percent tensor([0.5008, 0.4530, 0.4955, 0.5145, 0.4883, 0.5304, 0.4666, 0.5033, 0.4870,
        0.4436, 0.4627, 0.4722, 0.4592, 0.4914, 0.4954, 0.5020],
       device='cuda:0') torch.Size([16])
percent tensor([0.5749, 0.5658, 0.5543, 0.5558, 0.5352, 0.5598, 0.5669, 0.5570, 0.5654,
        0.5627, 0.5655, 0.5679, 0.5782, 0.5675, 0.5805, 0.5706],
       device='cuda:0') torch.Size([16])
percent tensor([0.5671, 0.5571, 0.5829, 0.6049, 0.5814, 0.5750, 0.5680, 0.5822, 0.5747,
        0.5477, 0.5603, 0.5885, 0.5567, 0.5892, 0.5706, 0.5702],
       device='cuda:0') torch.Size([16])
percent tensor([0.5925, 0.5787, 0.5472, 0.5461, 0.5504, 0.5734, 0.5693, 0.5296, 0.5804,
        0.5778, 0.5974, 0.5582, 0.6026, 0.6013, 0.5456, 0.5647],
       device='cuda:0') torch.Size([16])
percent tensor([0.9974, 0.9941, 0.9969, 0.9954, 0.9958, 0.9929, 0.9972, 0.9983, 0.9951,
        0.9978, 0.9968, 0.9987, 0.9978, 0.9956, 0.9949, 0.9952],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 22 | Batch_idx: 0 |  Loss: (0.6323) |  Loss2: (0.0000) | Acc: (76.00%) (98/128)
Epoch: 22 | Batch_idx: 10 |  Loss: (0.6667) |  Loss2: (0.0000) | Acc: (77.00%) (1086/1408)
Epoch: 22 | Batch_idx: 20 |  Loss: (0.7009) |  Loss2: (0.0000) | Acc: (75.00%) (2019/2688)
Epoch: 22 | Batch_idx: 30 |  Loss: (0.7221) |  Loss2: (0.0000) | Acc: (74.00%) (2953/3968)
Epoch: 22 | Batch_idx: 40 |  Loss: (0.7329) |  Loss2: (0.0000) | Acc: (73.00%) (3874/5248)
Epoch: 22 | Batch_idx: 50 |  Loss: (0.7487) |  Loss2: (0.0000) | Acc: (73.00%) (4774/6528)
Epoch: 22 | Batch_idx: 60 |  Loss: (0.7606) |  Loss2: (0.0000) | Acc: (72.00%) (5693/7808)
Epoch: 22 | Batch_idx: 70 |  Loss: (0.7653) |  Loss2: (0.0000) | Acc: (72.00%) (6610/9088)
Epoch: 22 | Batch_idx: 80 |  Loss: (0.7709) |  Loss2: (0.0000) | Acc: (72.00%) (7546/10368)
Epoch: 22 | Batch_idx: 90 |  Loss: (0.7712) |  Loss2: (0.0000) | Acc: (72.00%) (8480/11648)
Epoch: 22 | Batch_idx: 100 |  Loss: (0.7679) |  Loss2: (0.0000) | Acc: (72.00%) (9413/12928)
Epoch: 22 | Batch_idx: 110 |  Loss: (0.7678) |  Loss2: (0.0000) | Acc: (72.00%) (10346/14208)
Epoch: 22 | Batch_idx: 120 |  Loss: (0.7652) |  Loss2: (0.0000) | Acc: (72.00%) (11281/15488)
Epoch: 22 | Batch_idx: 130 |  Loss: (0.7622) |  Loss2: (0.0000) | Acc: (72.00%) (12220/16768)
Epoch: 22 | Batch_idx: 140 |  Loss: (0.7613) |  Loss2: (0.0000) | Acc: (72.00%) (13137/18048)
Epoch: 22 | Batch_idx: 150 |  Loss: (0.7580) |  Loss2: (0.0000) | Acc: (72.00%) (14097/19328)
Epoch: 22 | Batch_idx: 160 |  Loss: (0.7543) |  Loss2: (0.0000) | Acc: (73.00%) (15071/20608)
Epoch: 22 | Batch_idx: 170 |  Loss: (0.7527) |  Loss2: (0.0000) | Acc: (73.00%) (16019/21888)
Epoch: 22 | Batch_idx: 180 |  Loss: (0.7513) |  Loss2: (0.0000) | Acc: (73.00%) (16958/23168)
Epoch: 22 | Batch_idx: 190 |  Loss: (0.7495) |  Loss2: (0.0000) | Acc: (73.00%) (17913/24448)
Epoch: 22 | Batch_idx: 200 |  Loss: (0.7469) |  Loss2: (0.0000) | Acc: (73.00%) (18858/25728)
Epoch: 22 | Batch_idx: 210 |  Loss: (0.7476) |  Loss2: (0.0000) | Acc: (73.00%) (19801/27008)
Epoch: 22 | Batch_idx: 220 |  Loss: (0.7473) |  Loss2: (0.0000) | Acc: (73.00%) (20745/28288)
Epoch: 22 | Batch_idx: 230 |  Loss: (0.7456) |  Loss2: (0.0000) | Acc: (73.00%) (21709/29568)
Epoch: 22 | Batch_idx: 240 |  Loss: (0.7457) |  Loss2: (0.0000) | Acc: (73.00%) (22658/30848)
Epoch: 22 | Batch_idx: 250 |  Loss: (0.7456) |  Loss2: (0.0000) | Acc: (73.00%) (23608/32128)
Epoch: 22 | Batch_idx: 260 |  Loss: (0.7456) |  Loss2: (0.0000) | Acc: (73.00%) (24559/33408)
Epoch: 22 | Batch_idx: 270 |  Loss: (0.7444) |  Loss2: (0.0000) | Acc: (73.00%) (25521/34688)
Epoch: 22 | Batch_idx: 280 |  Loss: (0.7438) |  Loss2: (0.0000) | Acc: (73.00%) (26468/35968)
Epoch: 22 | Batch_idx: 290 |  Loss: (0.7437) |  Loss2: (0.0000) | Acc: (73.00%) (27402/37248)
Epoch: 22 | Batch_idx: 300 |  Loss: (0.7418) |  Loss2: (0.0000) | Acc: (73.00%) (28383/38528)
Epoch: 22 | Batch_idx: 310 |  Loss: (0.7426) |  Loss2: (0.0000) | Acc: (73.00%) (29322/39808)
Epoch: 22 | Batch_idx: 320 |  Loss: (0.7408) |  Loss2: (0.0000) | Acc: (73.00%) (30314/41088)
Epoch: 22 | Batch_idx: 330 |  Loss: (0.7397) |  Loss2: (0.0000) | Acc: (73.00%) (31291/42368)
Epoch: 22 | Batch_idx: 340 |  Loss: (0.7398) |  Loss2: (0.0000) | Acc: (73.00%) (32251/43648)
Epoch: 22 | Batch_idx: 350 |  Loss: (0.7402) |  Loss2: (0.0000) | Acc: (73.00%) (33182/44928)
Epoch: 22 | Batch_idx: 360 |  Loss: (0.7400) |  Loss2: (0.0000) | Acc: (73.00%) (34129/46208)
Epoch: 22 | Batch_idx: 370 |  Loss: (0.7390) |  Loss2: (0.0000) | Acc: (73.00%) (35099/47488)
Epoch: 22 | Batch_idx: 380 |  Loss: (0.7373) |  Loss2: (0.0000) | Acc: (73.00%) (36072/48768)
Epoch: 22 | Batch_idx: 390 |  Loss: (0.7373) |  Loss2: (0.0000) | Acc: (73.00%) (36981/50000)
# TEST : Loss: (0.7517) | Acc: (73.00%) (7392/10000)
percent tensor([0.5109, 0.5200, 0.5159, 0.5078, 0.5183, 0.5192, 0.5205, 0.5118, 0.5118,
        0.5181, 0.5149, 0.5184, 0.5105, 0.5107, 0.5183, 0.5117],
       device='cuda:0') torch.Size([16])
percent tensor([0.4930, 0.4886, 0.4942, 0.4962, 0.4924, 0.4990, 0.4898, 0.4929, 0.4903,
        0.4898, 0.4893, 0.4913, 0.4905, 0.4906, 0.4923, 0.4951],
       device='cuda:0') torch.Size([16])
percent tensor([0.4744, 0.5092, 0.3813, 0.3915, 0.3836, 0.4554, 0.4646, 0.4112, 0.4512,
        0.4708, 0.4888, 0.4178, 0.4858, 0.5067, 0.4850, 0.4754],
       device='cuda:0') torch.Size([16])
percent tensor([0.5230, 0.4819, 0.5025, 0.5198, 0.5041, 0.5396, 0.4952, 0.5144, 0.5049,
        0.4726, 0.4891, 0.4870, 0.4850, 0.5129, 0.5201, 0.5236],
       device='cuda:0') torch.Size([16])
percent tensor([0.5108, 0.5002, 0.5121, 0.5138, 0.4977, 0.5090, 0.5048, 0.5046, 0.5146,
        0.5006, 0.5039, 0.5101, 0.5097, 0.5177, 0.5086, 0.5069],
       device='cuda:0') torch.Size([16])
percent tensor([0.5599, 0.5357, 0.5876, 0.6258, 0.5940, 0.5775, 0.5574, 0.5838, 0.5697,
        0.5317, 0.5452, 0.5790, 0.5364, 0.5813, 0.5532, 0.5599],
       device='cuda:0') torch.Size([16])
percent tensor([0.5883, 0.5762, 0.5362, 0.5329, 0.5389, 0.5602, 0.5713, 0.5173, 0.5786,
        0.5779, 0.5953, 0.5455, 0.6016, 0.5963, 0.5434, 0.5636],
       device='cuda:0') torch.Size([16])
percent tensor([0.9986, 0.9942, 0.9972, 0.9967, 0.9957, 0.9927, 0.9982, 0.9986, 0.9962,
        0.9981, 0.9973, 0.9989, 0.9979, 0.9961, 0.9960, 0.9971],
       device='cuda:0') torch.Size([16])
Epoch: 23 | Batch_idx: 0 |  Loss: (0.7816) |  Loss2: (0.0000) | Acc: (72.00%) (93/128)
Epoch: 23 | Batch_idx: 10 |  Loss: (0.7156) |  Loss2: (0.0000) | Acc: (75.00%) (1060/1408)
Epoch: 23 | Batch_idx: 20 |  Loss: (0.7048) |  Loss2: (0.0000) | Acc: (75.00%) (2028/2688)
Epoch: 23 | Batch_idx: 30 |  Loss: (0.7031) |  Loss2: (0.0000) | Acc: (75.00%) (3004/3968)
Epoch: 23 | Batch_idx: 40 |  Loss: (0.6998) |  Loss2: (0.0000) | Acc: (75.00%) (3981/5248)
Epoch: 23 | Batch_idx: 50 |  Loss: (0.7044) |  Loss2: (0.0000) | Acc: (75.00%) (4929/6528)
Epoch: 23 | Batch_idx: 60 |  Loss: (0.7058) |  Loss2: (0.0000) | Acc: (75.00%) (5891/7808)
Epoch: 23 | Batch_idx: 70 |  Loss: (0.7042) |  Loss2: (0.0000) | Acc: (75.00%) (6860/9088)
Epoch: 23 | Batch_idx: 80 |  Loss: (0.7055) |  Loss2: (0.0000) | Acc: (75.00%) (7817/10368)
Epoch: 23 | Batch_idx: 90 |  Loss: (0.7038) |  Loss2: (0.0000) | Acc: (75.00%) (8789/11648)
Epoch: 23 | Batch_idx: 100 |  Loss: (0.7023) |  Loss2: (0.0000) | Acc: (75.00%) (9749/12928)
Epoch: 23 | Batch_idx: 110 |  Loss: (0.7029) |  Loss2: (0.0000) | Acc: (75.00%) (10714/14208)
Epoch: 23 | Batch_idx: 120 |  Loss: (0.7033) |  Loss2: (0.0000) | Acc: (75.00%) (11669/15488)
Epoch: 23 | Batch_idx: 130 |  Loss: (0.7015) |  Loss2: (0.0000) | Acc: (75.00%) (12639/16768)
Epoch: 23 | Batch_idx: 140 |  Loss: (0.7029) |  Loss2: (0.0000) | Acc: (75.00%) (13572/18048)
Epoch: 23 | Batch_idx: 150 |  Loss: (0.7001) |  Loss2: (0.0000) | Acc: (75.00%) (14572/19328)
Epoch: 23 | Batch_idx: 160 |  Loss: (0.6970) |  Loss2: (0.0000) | Acc: (75.00%) (15565/20608)
Epoch: 23 | Batch_idx: 170 |  Loss: (0.6968) |  Loss2: (0.0000) | Acc: (75.00%) (16527/21888)
Epoch: 23 | Batch_idx: 180 |  Loss: (0.6978) |  Loss2: (0.0000) | Acc: (75.00%) (17492/23168)
Epoch: 23 | Batch_idx: 190 |  Loss: (0.6988) |  Loss2: (0.0000) | Acc: (75.00%) (18443/24448)
Epoch: 23 | Batch_idx: 200 |  Loss: (0.6980) |  Loss2: (0.0000) | Acc: (75.00%) (19429/25728)
Epoch: 23 | Batch_idx: 210 |  Loss: (0.6992) |  Loss2: (0.0000) | Acc: (75.00%) (20382/27008)
Epoch: 23 | Batch_idx: 220 |  Loss: (0.6972) |  Loss2: (0.0000) | Acc: (75.00%) (21365/28288)
Epoch: 23 | Batch_idx: 230 |  Loss: (0.6960) |  Loss2: (0.0000) | Acc: (75.00%) (22341/29568)
Epoch: 23 | Batch_idx: 240 |  Loss: (0.6963) |  Loss2: (0.0000) | Acc: (75.00%) (23313/30848)
Epoch: 23 | Batch_idx: 250 |  Loss: (0.6972) |  Loss2: (0.0000) | Acc: (75.00%) (24272/32128)
Epoch: 23 | Batch_idx: 260 |  Loss: (0.6980) |  Loss2: (0.0000) | Acc: (75.00%) (25234/33408)
Epoch: 23 | Batch_idx: 270 |  Loss: (0.6975) |  Loss2: (0.0000) | Acc: (75.00%) (26203/34688)
Epoch: 23 | Batch_idx: 280 |  Loss: (0.6968) |  Loss2: (0.0000) | Acc: (75.00%) (27177/35968)
Epoch: 23 | Batch_idx: 290 |  Loss: (0.6978) |  Loss2: (0.0000) | Acc: (75.00%) (28142/37248)
Epoch: 23 | Batch_idx: 300 |  Loss: (0.6980) |  Loss2: (0.0000) | Acc: (75.00%) (29107/38528)
Epoch: 23 | Batch_idx: 310 |  Loss: (0.6989) |  Loss2: (0.0000) | Acc: (75.00%) (30052/39808)
Epoch: 23 | Batch_idx: 320 |  Loss: (0.6984) |  Loss2: (0.0000) | Acc: (75.00%) (31029/41088)
Epoch: 23 | Batch_idx: 330 |  Loss: (0.6969) |  Loss2: (0.0000) | Acc: (75.00%) (32034/42368)
Epoch: 23 | Batch_idx: 340 |  Loss: (0.6959) |  Loss2: (0.0000) | Acc: (75.00%) (33018/43648)
Epoch: 23 | Batch_idx: 350 |  Loss: (0.6959) |  Loss2: (0.0000) | Acc: (75.00%) (34001/44928)
Epoch: 23 | Batch_idx: 360 |  Loss: (0.6952) |  Loss2: (0.0000) | Acc: (75.00%) (34984/46208)
Epoch: 23 | Batch_idx: 370 |  Loss: (0.6956) |  Loss2: (0.0000) | Acc: (75.00%) (35935/47488)
Epoch: 23 | Batch_idx: 380 |  Loss: (0.6946) |  Loss2: (0.0000) | Acc: (75.00%) (36922/48768)
Epoch: 23 | Batch_idx: 390 |  Loss: (0.6940) |  Loss2: (0.0000) | Acc: (75.00%) (37882/50000)
# TEST : Loss: (0.7204) | Acc: (74.00%) (7462/10000)
percent tensor([0.5085, 0.5173, 0.5140, 0.5053, 0.5163, 0.5174, 0.5180, 0.5095, 0.5090,
        0.5153, 0.5124, 0.5162, 0.5078, 0.5077, 0.5161, 0.5090],
       device='cuda:0') torch.Size([16])
percent tensor([0.4929, 0.4877, 0.4933, 0.4960, 0.4915, 0.4989, 0.4889, 0.4923, 0.4896,
        0.4888, 0.4886, 0.4902, 0.4898, 0.4903, 0.4919, 0.4949],
       device='cuda:0') torch.Size([16])
percent tensor([0.4735, 0.5187, 0.3789, 0.3875, 0.3796, 0.4489, 0.4728, 0.4099, 0.4552,
        0.4800, 0.4948, 0.4227, 0.4909, 0.5150, 0.4865, 0.4756],
       device='cuda:0') torch.Size([16])
percent tensor([0.5312, 0.4905, 0.5061, 0.5253, 0.5119, 0.5482, 0.5047, 0.5209, 0.5120,
        0.4792, 0.4967, 0.4927, 0.4915, 0.5241, 0.5260, 0.5321],
       device='cuda:0') torch.Size([16])
percent tensor([0.5003, 0.4874, 0.5064, 0.5085, 0.4928, 0.5007, 0.4937, 0.4958, 0.5070,
        0.4895, 0.4935, 0.5015, 0.4986, 0.5098, 0.4959, 0.4953],
       device='cuda:0') torch.Size([16])
percent tensor([0.5671, 0.5392, 0.6037, 0.6479, 0.6151, 0.5880, 0.5654, 0.6015, 0.5798,
        0.5360, 0.5506, 0.5871, 0.5388, 0.5961, 0.5571, 0.5644],
       device='cuda:0') torch.Size([16])
percent tensor([0.6059, 0.5950, 0.5506, 0.5470, 0.5539, 0.5710, 0.5908, 0.5288, 0.5947,
        0.5974, 0.6151, 0.5617, 0.6208, 0.6159, 0.5570, 0.5787],
       device='cuda:0') torch.Size([16])
percent tensor([0.9987, 0.9952, 0.9980, 0.9975, 0.9968, 0.9932, 0.9986, 0.9991, 0.9964,
        0.9985, 0.9978, 0.9989, 0.9981, 0.9967, 0.9966, 0.9973],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 24 | Batch_idx: 0 |  Loss: (0.6605) |  Loss2: (0.0000) | Acc: (78.00%) (101/128)
Epoch: 24 | Batch_idx: 10 |  Loss: (0.6734) |  Loss2: (0.0000) | Acc: (75.00%) (1066/1408)
Epoch: 24 | Batch_idx: 20 |  Loss: (0.6765) |  Loss2: (0.0000) | Acc: (75.00%) (2028/2688)
Epoch: 24 | Batch_idx: 30 |  Loss: (0.6672) |  Loss2: (0.0000) | Acc: (76.00%) (3032/3968)
Epoch: 24 | Batch_idx: 40 |  Loss: (0.6690) |  Loss2: (0.0000) | Acc: (76.00%) (4010/5248)
Epoch: 24 | Batch_idx: 50 |  Loss: (0.6644) |  Loss2: (0.0000) | Acc: (76.00%) (4999/6528)
Epoch: 24 | Batch_idx: 60 |  Loss: (0.6603) |  Loss2: (0.0000) | Acc: (76.00%) (5975/7808)
Epoch: 24 | Batch_idx: 70 |  Loss: (0.6644) |  Loss2: (0.0000) | Acc: (76.00%) (6946/9088)
Epoch: 24 | Batch_idx: 80 |  Loss: (0.6650) |  Loss2: (0.0000) | Acc: (76.00%) (7944/10368)
Epoch: 24 | Batch_idx: 90 |  Loss: (0.6683) |  Loss2: (0.0000) | Acc: (76.00%) (8915/11648)
Epoch: 24 | Batch_idx: 100 |  Loss: (0.6721) |  Loss2: (0.0000) | Acc: (76.00%) (9876/12928)
Epoch: 24 | Batch_idx: 110 |  Loss: (0.6705) |  Loss2: (0.0000) | Acc: (76.00%) (10864/14208)
Epoch: 24 | Batch_idx: 120 |  Loss: (0.6705) |  Loss2: (0.0000) | Acc: (76.00%) (11844/15488)
Epoch: 24 | Batch_idx: 130 |  Loss: (0.6673) |  Loss2: (0.0000) | Acc: (76.00%) (12856/16768)
Epoch: 24 | Batch_idx: 140 |  Loss: (0.6650) |  Loss2: (0.0000) | Acc: (76.00%) (13830/18048)
Epoch: 24 | Batch_idx: 150 |  Loss: (0.6678) |  Loss2: (0.0000) | Acc: (76.00%) (14792/19328)
Epoch: 24 | Batch_idx: 160 |  Loss: (0.6663) |  Loss2: (0.0000) | Acc: (76.00%) (15773/20608)
Epoch: 24 | Batch_idx: 170 |  Loss: (0.6653) |  Loss2: (0.0000) | Acc: (76.00%) (16773/21888)
Epoch: 24 | Batch_idx: 180 |  Loss: (0.6640) |  Loss2: (0.0000) | Acc: (76.00%) (17762/23168)
Epoch: 24 | Batch_idx: 190 |  Loss: (0.6654) |  Loss2: (0.0000) | Acc: (76.00%) (18756/24448)
Epoch: 24 | Batch_idx: 200 |  Loss: (0.6635) |  Loss2: (0.0000) | Acc: (76.00%) (19755/25728)
Epoch: 24 | Batch_idx: 210 |  Loss: (0.6641) |  Loss2: (0.0000) | Acc: (76.00%) (20738/27008)
Epoch: 24 | Batch_idx: 220 |  Loss: (0.6628) |  Loss2: (0.0000) | Acc: (76.00%) (21732/28288)
Epoch: 24 | Batch_idx: 230 |  Loss: (0.6632) |  Loss2: (0.0000) | Acc: (76.00%) (22708/29568)
Epoch: 24 | Batch_idx: 240 |  Loss: (0.6649) |  Loss2: (0.0000) | Acc: (76.00%) (23664/30848)
Epoch: 24 | Batch_idx: 250 |  Loss: (0.6642) |  Loss2: (0.0000) | Acc: (76.00%) (24653/32128)
Epoch: 24 | Batch_idx: 260 |  Loss: (0.6643) |  Loss2: (0.0000) | Acc: (76.00%) (25626/33408)
Epoch: 24 | Batch_idx: 270 |  Loss: (0.6629) |  Loss2: (0.0000) | Acc: (76.00%) (26614/34688)
Epoch: 24 | Batch_idx: 280 |  Loss: (0.6611) |  Loss2: (0.0000) | Acc: (76.00%) (27630/35968)
Epoch: 24 | Batch_idx: 290 |  Loss: (0.6613) |  Loss2: (0.0000) | Acc: (76.00%) (28593/37248)
Epoch: 24 | Batch_idx: 300 |  Loss: (0.6611) |  Loss2: (0.0000) | Acc: (76.00%) (29568/38528)
Epoch: 24 | Batch_idx: 310 |  Loss: (0.6608) |  Loss2: (0.0000) | Acc: (76.00%) (30563/39808)
Epoch: 24 | Batch_idx: 320 |  Loss: (0.6601) |  Loss2: (0.0000) | Acc: (76.00%) (31559/41088)
Epoch: 24 | Batch_idx: 330 |  Loss: (0.6613) |  Loss2: (0.0000) | Acc: (76.00%) (32532/42368)
Epoch: 24 | Batch_idx: 340 |  Loss: (0.6606) |  Loss2: (0.0000) | Acc: (76.00%) (33522/43648)
Epoch: 24 | Batch_idx: 350 |  Loss: (0.6601) |  Loss2: (0.0000) | Acc: (76.00%) (34508/44928)
Epoch: 24 | Batch_idx: 360 |  Loss: (0.6590) |  Loss2: (0.0000) | Acc: (76.00%) (35517/46208)
Epoch: 24 | Batch_idx: 370 |  Loss: (0.6601) |  Loss2: (0.0000) | Acc: (76.00%) (36481/47488)
Epoch: 24 | Batch_idx: 380 |  Loss: (0.6609) |  Loss2: (0.0000) | Acc: (76.00%) (37448/48768)
Epoch: 24 | Batch_idx: 390 |  Loss: (0.6625) |  Loss2: (0.0000) | Acc: (76.00%) (38363/50000)
# TEST : Loss: (0.8447) | Acc: (70.00%) (7061/10000)
percent tensor([0.5084, 0.5184, 0.5141, 0.5056, 0.5164, 0.5170, 0.5192, 0.5103, 0.5093,
        0.5166, 0.5131, 0.5171, 0.5085, 0.5094, 0.5163, 0.5096],
       device='cuda:0') torch.Size([16])
percent tensor([0.4926, 0.4873, 0.4938, 0.4956, 0.4915, 0.4977, 0.4890, 0.4921, 0.4892,
        0.4888, 0.4886, 0.4901, 0.4896, 0.4890, 0.4912, 0.4938],
       device='cuda:0') torch.Size([16])
percent tensor([0.4871, 0.5183, 0.3880, 0.4070, 0.3944, 0.4697, 0.4758, 0.4166, 0.4579,
        0.4823, 0.4990, 0.4264, 0.4981, 0.5140, 0.4942, 0.4872],
       device='cuda:0') torch.Size([16])
percent tensor([0.5320, 0.4793, 0.5111, 0.5316, 0.5186, 0.5515, 0.5026, 0.5162, 0.5109,
        0.4711, 0.4963, 0.4935, 0.4939, 0.5153, 0.5249, 0.5322],
       device='cuda:0') torch.Size([16])
percent tensor([0.5003, 0.4874, 0.5080, 0.5109, 0.4984, 0.5009, 0.4954, 0.4972, 0.5067,
        0.4876, 0.4935, 0.5030, 0.4980, 0.5060, 0.4938, 0.4940],
       device='cuda:0') torch.Size([16])
percent tensor([0.5665, 0.5411, 0.6073, 0.6383, 0.6160, 0.5863, 0.5719, 0.5938, 0.5779,
        0.5351, 0.5534, 0.5987, 0.5465, 0.5934, 0.5575, 0.5640],
       device='cuda:0') torch.Size([16])
percent tensor([0.6073, 0.5914, 0.5601, 0.5534, 0.5646, 0.5903, 0.5886, 0.5310, 0.5897,
        0.5974, 0.6075, 0.5708, 0.6146, 0.6114, 0.5541, 0.5824],
       device='cuda:0') torch.Size([16])
percent tensor([0.9981, 0.9938, 0.9980, 0.9973, 0.9975, 0.9942, 0.9978, 0.9992, 0.9943,
        0.9975, 0.9974, 0.9984, 0.9974, 0.9962, 0.9944, 0.9976],
       device='cuda:0') torch.Size([16])
Epoch: 25 | Batch_idx: 0 |  Loss: (0.5684) |  Loss2: (0.0000) | Acc: (78.00%) (101/128)
Epoch: 25 | Batch_idx: 10 |  Loss: (0.6356) |  Loss2: (0.0000) | Acc: (77.00%) (1091/1408)
Epoch: 25 | Batch_idx: 20 |  Loss: (0.6294) |  Loss2: (0.0000) | Acc: (77.00%) (2093/2688)
Epoch: 25 | Batch_idx: 30 |  Loss: (0.6340) |  Loss2: (0.0000) | Acc: (77.00%) (3088/3968)
Epoch: 25 | Batch_idx: 40 |  Loss: (0.6424) |  Loss2: (0.0000) | Acc: (77.00%) (4057/5248)
Epoch: 25 | Batch_idx: 50 |  Loss: (0.6412) |  Loss2: (0.0000) | Acc: (77.00%) (5068/6528)
Epoch: 25 | Batch_idx: 60 |  Loss: (0.6299) |  Loss2: (0.0000) | Acc: (77.00%) (6083/7808)
Epoch: 25 | Batch_idx: 70 |  Loss: (0.6300) |  Loss2: (0.0000) | Acc: (77.00%) (7076/9088)
Epoch: 25 | Batch_idx: 80 |  Loss: (0.6317) |  Loss2: (0.0000) | Acc: (77.00%) (8064/10368)
Epoch: 25 | Batch_idx: 90 |  Loss: (0.6338) |  Loss2: (0.0000) | Acc: (77.00%) (9059/11648)
Epoch: 25 | Batch_idx: 100 |  Loss: (0.6353) |  Loss2: (0.0000) | Acc: (77.00%) (10037/12928)
Epoch: 25 | Batch_idx: 110 |  Loss: (0.6340) |  Loss2: (0.0000) | Acc: (77.00%) (11041/14208)
Epoch: 25 | Batch_idx: 120 |  Loss: (0.6317) |  Loss2: (0.0000) | Acc: (77.00%) (12053/15488)
Epoch: 25 | Batch_idx: 130 |  Loss: (0.6315) |  Loss2: (0.0000) | Acc: (77.00%) (13063/16768)
Epoch: 25 | Batch_idx: 140 |  Loss: (0.6336) |  Loss2: (0.0000) | Acc: (77.00%) (14065/18048)
Epoch: 25 | Batch_idx: 150 |  Loss: (0.6326) |  Loss2: (0.0000) | Acc: (77.00%) (15051/19328)
Epoch: 25 | Batch_idx: 160 |  Loss: (0.6349) |  Loss2: (0.0000) | Acc: (77.00%) (16024/20608)
Epoch: 25 | Batch_idx: 170 |  Loss: (0.6357) |  Loss2: (0.0000) | Acc: (77.00%) (17010/21888)
Epoch: 25 | Batch_idx: 180 |  Loss: (0.6385) |  Loss2: (0.0000) | Acc: (77.00%) (17968/23168)
Epoch: 25 | Batch_idx: 190 |  Loss: (0.6384) |  Loss2: (0.0000) | Acc: (77.00%) (18956/24448)
Epoch: 25 | Batch_idx: 200 |  Loss: (0.6395) |  Loss2: (0.0000) | Acc: (77.00%) (19961/25728)
Epoch: 25 | Batch_idx: 210 |  Loss: (0.6404) |  Loss2: (0.0000) | Acc: (77.00%) (20956/27008)
Epoch: 25 | Batch_idx: 220 |  Loss: (0.6397) |  Loss2: (0.0000) | Acc: (77.00%) (21964/28288)
Epoch: 25 | Batch_idx: 230 |  Loss: (0.6384) |  Loss2: (0.0000) | Acc: (77.00%) (22971/29568)
Epoch: 25 | Batch_idx: 240 |  Loss: (0.6364) |  Loss2: (0.0000) | Acc: (77.00%) (24003/30848)
Epoch: 25 | Batch_idx: 250 |  Loss: (0.6337) |  Loss2: (0.0000) | Acc: (77.00%) (25043/32128)
Epoch: 25 | Batch_idx: 260 |  Loss: (0.6325) |  Loss2: (0.0000) | Acc: (77.00%) (26037/33408)
Epoch: 25 | Batch_idx: 270 |  Loss: (0.6316) |  Loss2: (0.0000) | Acc: (77.00%) (27033/34688)
Epoch: 25 | Batch_idx: 280 |  Loss: (0.6311) |  Loss2: (0.0000) | Acc: (77.00%) (28035/35968)
Epoch: 25 | Batch_idx: 290 |  Loss: (0.6308) |  Loss2: (0.0000) | Acc: (77.00%) (29032/37248)
Epoch: 25 | Batch_idx: 300 |  Loss: (0.6317) |  Loss2: (0.0000) | Acc: (77.00%) (30020/38528)
Epoch: 25 | Batch_idx: 310 |  Loss: (0.6318) |  Loss2: (0.0000) | Acc: (77.00%) (31015/39808)
Epoch: 25 | Batch_idx: 320 |  Loss: (0.6322) |  Loss2: (0.0000) | Acc: (77.00%) (32019/41088)
Epoch: 25 | Batch_idx: 330 |  Loss: (0.6322) |  Loss2: (0.0000) | Acc: (77.00%) (33022/42368)
Epoch: 25 | Batch_idx: 340 |  Loss: (0.6311) |  Loss2: (0.0000) | Acc: (77.00%) (34036/43648)
Epoch: 25 | Batch_idx: 350 |  Loss: (0.6315) |  Loss2: (0.0000) | Acc: (77.00%) (35030/44928)
Epoch: 25 | Batch_idx: 360 |  Loss: (0.6316) |  Loss2: (0.0000) | Acc: (77.00%) (36025/46208)
Epoch: 25 | Batch_idx: 370 |  Loss: (0.6309) |  Loss2: (0.0000) | Acc: (77.00%) (37022/47488)
Epoch: 25 | Batch_idx: 380 |  Loss: (0.6305) |  Loss2: (0.0000) | Acc: (77.00%) (38025/48768)
Epoch: 25 | Batch_idx: 390 |  Loss: (0.6302) |  Loss2: (0.0000) | Acc: (77.00%) (38988/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_025.pth.tar'
# TEST : Loss: (0.7915) | Acc: (72.00%) (7268/10000)
percent tensor([0.5098, 0.5179, 0.5137, 0.5051, 0.5161, 0.5177, 0.5185, 0.5100, 0.5100,
        0.5160, 0.5138, 0.5164, 0.5091, 0.5081, 0.5165, 0.5101],
       device='cuda:0') torch.Size([16])
percent tensor([0.4927, 0.4882, 0.4946, 0.4963, 0.4921, 0.4986, 0.4898, 0.4918, 0.4906,
        0.4895, 0.4894, 0.4912, 0.4901, 0.4903, 0.4916, 0.4943],
       device='cuda:0') torch.Size([16])
percent tensor([0.4837, 0.5152, 0.3949, 0.4032, 0.3965, 0.4609, 0.4771, 0.4218, 0.4594,
        0.4835, 0.4961, 0.4316, 0.4975, 0.5048, 0.4905, 0.4836],
       device='cuda:0') torch.Size([16])
percent tensor([0.5327, 0.4874, 0.5233, 0.5309, 0.5219, 0.5496, 0.5083, 0.5233, 0.5167,
        0.4773, 0.4959, 0.5019, 0.4949, 0.5212, 0.5237, 0.5345],
       device='cuda:0') torch.Size([16])
percent tensor([0.4996, 0.4915, 0.5080, 0.5104, 0.4973, 0.4983, 0.4974, 0.4990, 0.5089,
        0.4904, 0.4945, 0.5066, 0.5007, 0.5109, 0.4945, 0.4953],
       device='cuda:0') torch.Size([16])
percent tensor([0.5640, 0.5468, 0.6121, 0.6315, 0.6160, 0.5809, 0.5642, 0.5928, 0.5835,
        0.5405, 0.5532, 0.5996, 0.5515, 0.6026, 0.5601, 0.5637],
       device='cuda:0') torch.Size([16])
percent tensor([0.6013, 0.5907, 0.5518, 0.5484, 0.5548, 0.5850, 0.5832, 0.5277, 0.5878,
        0.5970, 0.6048, 0.5606, 0.6139, 0.6244, 0.5426, 0.5813],
       device='cuda:0') torch.Size([16])
percent tensor([0.9984, 0.9958, 0.9977, 0.9961, 0.9976, 0.9903, 0.9984, 0.9986, 0.9970,
        0.9986, 0.9976, 0.9981, 0.9987, 0.9983, 0.9959, 0.9974],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 26 | Batch_idx: 0 |  Loss: (0.5478) |  Loss2: (0.0000) | Acc: (80.00%) (103/128)
Epoch: 26 | Batch_idx: 10 |  Loss: (0.6456) |  Loss2: (0.0000) | Acc: (78.00%) (1109/1408)
Epoch: 26 | Batch_idx: 20 |  Loss: (0.6774) |  Loss2: (0.0000) | Acc: (76.00%) (2059/2688)
Epoch: 26 | Batch_idx: 30 |  Loss: (0.6885) |  Loss2: (0.0000) | Acc: (76.00%) (3031/3968)
Epoch: 26 | Batch_idx: 40 |  Loss: (0.7018) |  Loss2: (0.0000) | Acc: (75.00%) (3973/5248)
Epoch: 26 | Batch_idx: 50 |  Loss: (0.7123) |  Loss2: (0.0000) | Acc: (75.00%) (4913/6528)
Epoch: 26 | Batch_idx: 60 |  Loss: (0.7112) |  Loss2: (0.0000) | Acc: (75.00%) (5866/7808)
Epoch: 26 | Batch_idx: 70 |  Loss: (0.7213) |  Loss2: (0.0000) | Acc: (74.00%) (6785/9088)
Epoch: 26 | Batch_idx: 80 |  Loss: (0.7173) |  Loss2: (0.0000) | Acc: (74.00%) (7742/10368)
Epoch: 26 | Batch_idx: 90 |  Loss: (0.7204) |  Loss2: (0.0000) | Acc: (74.00%) (8688/11648)
Epoch: 26 | Batch_idx: 100 |  Loss: (0.7169) |  Loss2: (0.0000) | Acc: (74.00%) (9659/12928)
Epoch: 26 | Batch_idx: 110 |  Loss: (0.7172) |  Loss2: (0.0000) | Acc: (74.00%) (10623/14208)
Epoch: 26 | Batch_idx: 120 |  Loss: (0.7166) |  Loss2: (0.0000) | Acc: (74.00%) (11582/15488)
Epoch: 26 | Batch_idx: 130 |  Loss: (0.7198) |  Loss2: (0.0000) | Acc: (74.00%) (12522/16768)
Epoch: 26 | Batch_idx: 140 |  Loss: (0.7190) |  Loss2: (0.0000) | Acc: (74.00%) (13484/18048)
Epoch: 26 | Batch_idx: 150 |  Loss: (0.7184) |  Loss2: (0.0000) | Acc: (74.00%) (14440/19328)
Epoch: 26 | Batch_idx: 160 |  Loss: (0.7154) |  Loss2: (0.0000) | Acc: (74.00%) (15432/20608)
Epoch: 26 | Batch_idx: 170 |  Loss: (0.7134) |  Loss2: (0.0000) | Acc: (75.00%) (16417/21888)
Epoch: 26 | Batch_idx: 180 |  Loss: (0.7116) |  Loss2: (0.0000) | Acc: (75.00%) (17393/23168)
Epoch: 26 | Batch_idx: 190 |  Loss: (0.7086) |  Loss2: (0.0000) | Acc: (75.00%) (18386/24448)
Epoch: 26 | Batch_idx: 200 |  Loss: (0.7052) |  Loss2: (0.0000) | Acc: (75.00%) (19376/25728)
Epoch: 26 | Batch_idx: 210 |  Loss: (0.7019) |  Loss2: (0.0000) | Acc: (75.00%) (20359/27008)
Epoch: 26 | Batch_idx: 220 |  Loss: (0.7001) |  Loss2: (0.0000) | Acc: (75.00%) (21342/28288)
Epoch: 26 | Batch_idx: 230 |  Loss: (0.6976) |  Loss2: (0.0000) | Acc: (75.00%) (22330/29568)
Epoch: 26 | Batch_idx: 240 |  Loss: (0.6992) |  Loss2: (0.0000) | Acc: (75.00%) (23290/30848)
Epoch: 26 | Batch_idx: 250 |  Loss: (0.6979) |  Loss2: (0.0000) | Acc: (75.00%) (24269/32128)
Epoch: 26 | Batch_idx: 260 |  Loss: (0.6966) |  Loss2: (0.0000) | Acc: (75.00%) (25241/33408)
Epoch: 26 | Batch_idx: 270 |  Loss: (0.6961) |  Loss2: (0.0000) | Acc: (75.00%) (26220/34688)
Epoch: 26 | Batch_idx: 280 |  Loss: (0.6937) |  Loss2: (0.0000) | Acc: (75.00%) (27226/35968)
Epoch: 26 | Batch_idx: 290 |  Loss: (0.6935) |  Loss2: (0.0000) | Acc: (75.00%) (28196/37248)
Epoch: 26 | Batch_idx: 300 |  Loss: (0.6916) |  Loss2: (0.0000) | Acc: (75.00%) (29196/38528)
Epoch: 26 | Batch_idx: 310 |  Loss: (0.6905) |  Loss2: (0.0000) | Acc: (75.00%) (30189/39808)
Epoch: 26 | Batch_idx: 320 |  Loss: (0.6896) |  Loss2: (0.0000) | Acc: (75.00%) (31165/41088)
Epoch: 26 | Batch_idx: 330 |  Loss: (0.6885) |  Loss2: (0.0000) | Acc: (75.00%) (32147/42368)
Epoch: 26 | Batch_idx: 340 |  Loss: (0.6878) |  Loss2: (0.0000) | Acc: (75.00%) (33113/43648)
Epoch: 26 | Batch_idx: 350 |  Loss: (0.6871) |  Loss2: (0.0000) | Acc: (75.00%) (34095/44928)
Epoch: 26 | Batch_idx: 360 |  Loss: (0.6874) |  Loss2: (0.0000) | Acc: (75.00%) (35055/46208)
Epoch: 26 | Batch_idx: 370 |  Loss: (0.6862) |  Loss2: (0.0000) | Acc: (75.00%) (36043/47488)
Epoch: 26 | Batch_idx: 380 |  Loss: (0.6855) |  Loss2: (0.0000) | Acc: (75.00%) (37012/48768)
Epoch: 26 | Batch_idx: 390 |  Loss: (0.6839) |  Loss2: (0.0000) | Acc: (75.00%) (37981/50000)
# TEST : Loss: (0.6930) | Acc: (75.00%) (7586/10000)
percent tensor([0.5256, 0.5388, 0.5327, 0.5202, 0.5360, 0.5358, 0.5398, 0.5285, 0.5263,
        0.5362, 0.5305, 0.5365, 0.5250, 0.5259, 0.5359, 0.5273],
       device='cuda:0') torch.Size([16])
percent tensor([0.4884, 0.4849, 0.4905, 0.4921, 0.4879, 0.4953, 0.4862, 0.4878, 0.4864,
        0.4863, 0.4855, 0.4880, 0.4863, 0.4869, 0.4879, 0.4907],
       device='cuda:0') torch.Size([16])
percent tensor([0.4938, 0.5252, 0.4115, 0.4098, 0.4104, 0.4619, 0.4915, 0.4335, 0.4693,
        0.4981, 0.5072, 0.4491, 0.5099, 0.5070, 0.4983, 0.4906],
       device='cuda:0') torch.Size([16])
percent tensor([0.5032, 0.4517, 0.5057, 0.5260, 0.5037, 0.5399, 0.4742, 0.5076, 0.4934,
        0.4417, 0.4599, 0.4725, 0.4565, 0.5042, 0.4908, 0.5062],
       device='cuda:0') torch.Size([16])
percent tensor([0.4862, 0.4840, 0.4954, 0.4982, 0.4862, 0.4767, 0.4897, 0.4911, 0.5064,
        0.4856, 0.4909, 0.5007, 0.4912, 0.5123, 0.4824, 0.4781],
       device='cuda:0') torch.Size([16])
percent tensor([0.5799, 0.5654, 0.6183, 0.6556, 0.6278, 0.5906, 0.5876, 0.6227, 0.6011,
        0.5625, 0.5709, 0.6167, 0.5602, 0.6242, 0.5865, 0.5772],
       device='cuda:0') torch.Size([16])
percent tensor([0.5739, 0.5608, 0.5353, 0.5270, 0.5402, 0.5480, 0.5648, 0.5183, 0.5620,
        0.5718, 0.5715, 0.5394, 0.5739, 0.5857, 0.5250, 0.5583],
       device='cuda:0') torch.Size([16])
percent tensor([0.9990, 0.9948, 0.9979, 0.9969, 0.9977, 0.9897, 0.9987, 0.9987, 0.9976,
        0.9986, 0.9977, 0.9981, 0.9988, 0.9980, 0.9956, 0.9971],
       device='cuda:0') torch.Size([16])
Epoch: 27 | Batch_idx: 0 |  Loss: (0.6233) |  Loss2: (0.0000) | Acc: (75.00%) (97/128)
Epoch: 27 | Batch_idx: 10 |  Loss: (0.6532) |  Loss2: (0.0000) | Acc: (76.00%) (1078/1408)
Epoch: 27 | Batch_idx: 20 |  Loss: (0.6458) |  Loss2: (0.0000) | Acc: (77.00%) (2074/2688)
Epoch: 27 | Batch_idx: 30 |  Loss: (0.6346) |  Loss2: (0.0000) | Acc: (77.00%) (3071/3968)
Epoch: 27 | Batch_idx: 40 |  Loss: (0.6366) |  Loss2: (0.0000) | Acc: (77.00%) (4065/5248)
Epoch: 27 | Batch_idx: 50 |  Loss: (0.6437) |  Loss2: (0.0000) | Acc: (77.00%) (5030/6528)
Epoch: 27 | Batch_idx: 60 |  Loss: (0.6443) |  Loss2: (0.0000) | Acc: (77.00%) (6033/7808)
Epoch: 27 | Batch_idx: 70 |  Loss: (0.6503) |  Loss2: (0.0000) | Acc: (77.00%) (6999/9088)
Epoch: 27 | Batch_idx: 80 |  Loss: (0.6518) |  Loss2: (0.0000) | Acc: (76.00%) (7971/10368)
Epoch: 27 | Batch_idx: 90 |  Loss: (0.6494) |  Loss2: (0.0000) | Acc: (76.00%) (8968/11648)
Epoch: 27 | Batch_idx: 100 |  Loss: (0.6470) |  Loss2: (0.0000) | Acc: (77.00%) (9967/12928)
Epoch: 27 | Batch_idx: 110 |  Loss: (0.6428) |  Loss2: (0.0000) | Acc: (77.00%) (10990/14208)
Epoch: 27 | Batch_idx: 120 |  Loss: (0.6446) |  Loss2: (0.0000) | Acc: (77.00%) (11954/15488)
Epoch: 27 | Batch_idx: 130 |  Loss: (0.6445) |  Loss2: (0.0000) | Acc: (77.00%) (12941/16768)
Epoch: 27 | Batch_idx: 140 |  Loss: (0.6433) |  Loss2: (0.0000) | Acc: (77.00%) (13943/18048)
Epoch: 27 | Batch_idx: 150 |  Loss: (0.6452) |  Loss2: (0.0000) | Acc: (77.00%) (14916/19328)
Epoch: 27 | Batch_idx: 160 |  Loss: (0.6438) |  Loss2: (0.0000) | Acc: (77.00%) (15923/20608)
Epoch: 27 | Batch_idx: 170 |  Loss: (0.6407) |  Loss2: (0.0000) | Acc: (77.00%) (16940/21888)
Epoch: 27 | Batch_idx: 180 |  Loss: (0.6425) |  Loss2: (0.0000) | Acc: (77.00%) (17915/23168)
Epoch: 27 | Batch_idx: 190 |  Loss: (0.6409) |  Loss2: (0.0000) | Acc: (77.00%) (18909/24448)
Epoch: 27 | Batch_idx: 200 |  Loss: (0.6401) |  Loss2: (0.0000) | Acc: (77.00%) (19893/25728)
Epoch: 27 | Batch_idx: 210 |  Loss: (0.6401) |  Loss2: (0.0000) | Acc: (77.00%) (20873/27008)
Epoch: 27 | Batch_idx: 220 |  Loss: (0.6402) |  Loss2: (0.0000) | Acc: (77.00%) (21865/28288)
Epoch: 27 | Batch_idx: 230 |  Loss: (0.6385) |  Loss2: (0.0000) | Acc: (77.00%) (22878/29568)
Epoch: 27 | Batch_idx: 240 |  Loss: (0.6392) |  Loss2: (0.0000) | Acc: (77.00%) (23855/30848)
Epoch: 27 | Batch_idx: 250 |  Loss: (0.6399) |  Loss2: (0.0000) | Acc: (77.00%) (24858/32128)
Epoch: 27 | Batch_idx: 260 |  Loss: (0.6389) |  Loss2: (0.0000) | Acc: (77.00%) (25871/33408)
Epoch: 27 | Batch_idx: 270 |  Loss: (0.6385) |  Loss2: (0.0000) | Acc: (77.00%) (26869/34688)
Epoch: 27 | Batch_idx: 280 |  Loss: (0.6370) |  Loss2: (0.0000) | Acc: (77.00%) (27888/35968)
Epoch: 27 | Batch_idx: 290 |  Loss: (0.6368) |  Loss2: (0.0000) | Acc: (77.00%) (28881/37248)
Epoch: 27 | Batch_idx: 300 |  Loss: (0.6373) |  Loss2: (0.0000) | Acc: (77.00%) (29862/38528)
Epoch: 27 | Batch_idx: 310 |  Loss: (0.6371) |  Loss2: (0.0000) | Acc: (77.00%) (30850/39808)
Epoch: 27 | Batch_idx: 320 |  Loss: (0.6359) |  Loss2: (0.0000) | Acc: (77.00%) (31858/41088)
Epoch: 27 | Batch_idx: 330 |  Loss: (0.6360) |  Loss2: (0.0000) | Acc: (77.00%) (32865/42368)
Epoch: 27 | Batch_idx: 340 |  Loss: (0.6366) |  Loss2: (0.0000) | Acc: (77.00%) (33842/43648)
Epoch: 27 | Batch_idx: 350 |  Loss: (0.6361) |  Loss2: (0.0000) | Acc: (77.00%) (34838/44928)
Epoch: 27 | Batch_idx: 360 |  Loss: (0.6364) |  Loss2: (0.0000) | Acc: (77.00%) (35829/46208)
Epoch: 27 | Batch_idx: 370 |  Loss: (0.6349) |  Loss2: (0.0000) | Acc: (77.00%) (36841/47488)
Epoch: 27 | Batch_idx: 380 |  Loss: (0.6345) |  Loss2: (0.0000) | Acc: (77.00%) (37839/48768)
Epoch: 27 | Batch_idx: 390 |  Loss: (0.6338) |  Loss2: (0.0000) | Acc: (77.00%) (38807/50000)
# TEST : Loss: (0.6613) | Acc: (76.00%) (7693/10000)
percent tensor([0.5286, 0.5440, 0.5362, 0.5234, 0.5400, 0.5399, 0.5447, 0.5328, 0.5295,
        0.5408, 0.5342, 0.5407, 0.5283, 0.5305, 0.5405, 0.5312],
       device='cuda:0') torch.Size([16])
percent tensor([0.4868, 0.4837, 0.4893, 0.4906, 0.4865, 0.4940, 0.4851, 0.4864, 0.4852,
        0.4853, 0.4844, 0.4869, 0.4852, 0.4856, 0.4865, 0.4893],
       device='cuda:0') torch.Size([16])
percent tensor([0.4970, 0.5276, 0.4114, 0.4103, 0.4075, 0.4632, 0.4945, 0.4328, 0.4692,
        0.5011, 0.5120, 0.4512, 0.5136, 0.5081, 0.5000, 0.4941],
       device='cuda:0') torch.Size([16])
percent tensor([0.5129, 0.4644, 0.5145, 0.5344, 0.5149, 0.5469, 0.4872, 0.5198, 0.5025,
        0.4557, 0.4700, 0.4841, 0.4661, 0.5162, 0.5015, 0.5184],
       device='cuda:0') torch.Size([16])
percent tensor([0.4831, 0.4840, 0.4934, 0.4967, 0.4852, 0.4676, 0.4901, 0.4924, 0.5091,
        0.4882, 0.4918, 0.5034, 0.4909, 0.5169, 0.4793, 0.4726],
       device='cuda:0') torch.Size([16])
percent tensor([0.5929, 0.5805, 0.6317, 0.6649, 0.6421, 0.5952, 0.6039, 0.6430, 0.6117,
        0.5794, 0.5828, 0.6336, 0.5705, 0.6362, 0.6051, 0.5898],
       device='cuda:0') torch.Size([16])
percent tensor([0.5844, 0.5681, 0.5469, 0.5382, 0.5551, 0.5520, 0.5801, 0.5350, 0.5674,
        0.5859, 0.5777, 0.5497, 0.5759, 0.5968, 0.5368, 0.5684],
       device='cuda:0') torch.Size([16])
percent tensor([0.9990, 0.9953, 0.9981, 0.9971, 0.9981, 0.9911, 0.9990, 0.9989, 0.9978,
        0.9989, 0.9978, 0.9982, 0.9988, 0.9982, 0.9963, 0.9974],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 28 | Batch_idx: 0 |  Loss: (0.6263) |  Loss2: (0.0000) | Acc: (79.00%) (102/128)
Epoch: 28 | Batch_idx: 10 |  Loss: (0.6314) |  Loss2: (0.0000) | Acc: (78.00%) (1112/1408)
Epoch: 28 | Batch_idx: 20 |  Loss: (0.6043) |  Loss2: (0.0000) | Acc: (79.00%) (2137/2688)
Epoch: 28 | Batch_idx: 30 |  Loss: (0.6121) |  Loss2: (0.0000) | Acc: (78.00%) (3132/3968)
Epoch: 28 | Batch_idx: 40 |  Loss: (0.6150) |  Loss2: (0.0000) | Acc: (78.00%) (4132/5248)
Epoch: 28 | Batch_idx: 50 |  Loss: (0.6070) |  Loss2: (0.0000) | Acc: (78.00%) (5147/6528)
Epoch: 28 | Batch_idx: 60 |  Loss: (0.6122) |  Loss2: (0.0000) | Acc: (78.00%) (6154/7808)
Epoch: 28 | Batch_idx: 70 |  Loss: (0.6062) |  Loss2: (0.0000) | Acc: (79.00%) (7187/9088)
Epoch: 28 | Batch_idx: 80 |  Loss: (0.6064) |  Loss2: (0.0000) | Acc: (78.00%) (8186/10368)
Epoch: 28 | Batch_idx: 90 |  Loss: (0.6101) |  Loss2: (0.0000) | Acc: (78.00%) (9163/11648)
Epoch: 28 | Batch_idx: 100 |  Loss: (0.6123) |  Loss2: (0.0000) | Acc: (78.00%) (10155/12928)
Epoch: 28 | Batch_idx: 110 |  Loss: (0.6146) |  Loss2: (0.0000) | Acc: (78.00%) (11154/14208)
Epoch: 28 | Batch_idx: 120 |  Loss: (0.6171) |  Loss2: (0.0000) | Acc: (78.00%) (12138/15488)
Epoch: 28 | Batch_idx: 130 |  Loss: (0.6160) |  Loss2: (0.0000) | Acc: (78.00%) (13160/16768)
Epoch: 28 | Batch_idx: 140 |  Loss: (0.6156) |  Loss2: (0.0000) | Acc: (78.00%) (14173/18048)
Epoch: 28 | Batch_idx: 150 |  Loss: (0.6175) |  Loss2: (0.0000) | Acc: (78.00%) (15168/19328)
Epoch: 28 | Batch_idx: 160 |  Loss: (0.6148) |  Loss2: (0.0000) | Acc: (78.00%) (16196/20608)
Epoch: 28 | Batch_idx: 170 |  Loss: (0.6133) |  Loss2: (0.0000) | Acc: (78.00%) (17215/21888)
Epoch: 28 | Batch_idx: 180 |  Loss: (0.6132) |  Loss2: (0.0000) | Acc: (78.00%) (18235/23168)
Epoch: 28 | Batch_idx: 190 |  Loss: (0.6118) |  Loss2: (0.0000) | Acc: (78.00%) (19263/24448)
Epoch: 28 | Batch_idx: 200 |  Loss: (0.6107) |  Loss2: (0.0000) | Acc: (78.00%) (20290/25728)
Epoch: 28 | Batch_idx: 210 |  Loss: (0.6098) |  Loss2: (0.0000) | Acc: (78.00%) (21303/27008)
Epoch: 28 | Batch_idx: 220 |  Loss: (0.6106) |  Loss2: (0.0000) | Acc: (78.00%) (22318/28288)
Epoch: 28 | Batch_idx: 230 |  Loss: (0.6096) |  Loss2: (0.0000) | Acc: (78.00%) (23344/29568)
Epoch: 28 | Batch_idx: 240 |  Loss: (0.6078) |  Loss2: (0.0000) | Acc: (78.00%) (24366/30848)
Epoch: 28 | Batch_idx: 250 |  Loss: (0.6065) |  Loss2: (0.0000) | Acc: (79.00%) (25387/32128)
Epoch: 28 | Batch_idx: 260 |  Loss: (0.6065) |  Loss2: (0.0000) | Acc: (78.00%) (26392/33408)
Epoch: 28 | Batch_idx: 270 |  Loss: (0.6064) |  Loss2: (0.0000) | Acc: (79.00%) (27404/34688)
Epoch: 28 | Batch_idx: 280 |  Loss: (0.6055) |  Loss2: (0.0000) | Acc: (79.00%) (28430/35968)
Epoch: 28 | Batch_idx: 290 |  Loss: (0.6052) |  Loss2: (0.0000) | Acc: (79.00%) (29442/37248)
Epoch: 28 | Batch_idx: 300 |  Loss: (0.6051) |  Loss2: (0.0000) | Acc: (79.00%) (30452/38528)
Epoch: 28 | Batch_idx: 310 |  Loss: (0.6058) |  Loss2: (0.0000) | Acc: (79.00%) (31458/39808)
Epoch: 28 | Batch_idx: 320 |  Loss: (0.6052) |  Loss2: (0.0000) | Acc: (79.00%) (32479/41088)
Epoch: 28 | Batch_idx: 330 |  Loss: (0.6042) |  Loss2: (0.0000) | Acc: (79.00%) (33518/42368)
Epoch: 28 | Batch_idx: 340 |  Loss: (0.6040) |  Loss2: (0.0000) | Acc: (79.00%) (34536/43648)
Epoch: 28 | Batch_idx: 350 |  Loss: (0.6030) |  Loss2: (0.0000) | Acc: (79.00%) (35565/44928)
Epoch: 28 | Batch_idx: 360 |  Loss: (0.6022) |  Loss2: (0.0000) | Acc: (79.00%) (36576/46208)
Epoch: 28 | Batch_idx: 370 |  Loss: (0.6025) |  Loss2: (0.0000) | Acc: (79.00%) (37580/47488)
Epoch: 28 | Batch_idx: 380 |  Loss: (0.6018) |  Loss2: (0.0000) | Acc: (79.00%) (38594/48768)
Epoch: 28 | Batch_idx: 390 |  Loss: (0.6018) |  Loss2: (0.0000) | Acc: (79.00%) (39565/50000)
# TEST : Loss: (0.7559) | Acc: (74.00%) (7428/10000)
percent tensor([0.5280, 0.5446, 0.5340, 0.5233, 0.5372, 0.5397, 0.5443, 0.5323, 0.5273,
        0.5411, 0.5340, 0.5393, 0.5277, 0.5318, 0.5408, 0.5312],
       device='cuda:0') torch.Size([16])
percent tensor([0.4880, 0.4833, 0.4908, 0.4923, 0.4881, 0.4934, 0.4855, 0.4880, 0.4856,
        0.4854, 0.4841, 0.4878, 0.4858, 0.4849, 0.4864, 0.4895],
       device='cuda:0') torch.Size([16])
percent tensor([0.4965, 0.5258, 0.4076, 0.4110, 0.4061, 0.4718, 0.4904, 0.4315, 0.4602,
        0.4966, 0.5074, 0.4494, 0.5125, 0.5072, 0.5008, 0.4940],
       device='cuda:0') torch.Size([16])
percent tensor([0.5213, 0.4636, 0.5230, 0.5351, 0.5226, 0.5513, 0.4945, 0.5281, 0.5114,
        0.4655, 0.4769, 0.4952, 0.4717, 0.5146, 0.5065, 0.5226],
       device='cuda:0') torch.Size([16])
percent tensor([0.4838, 0.4854, 0.4959, 0.4999, 0.4854, 0.4728, 0.4920, 0.4931, 0.5072,
        0.4873, 0.4953, 0.5044, 0.4900, 0.5181, 0.4836, 0.4756],
       device='cuda:0') torch.Size([16])
percent tensor([0.5919, 0.5795, 0.6434, 0.6646, 0.6456, 0.5943, 0.6125, 0.6503, 0.6211,
        0.5859, 0.5920, 0.6407, 0.5728, 0.6391, 0.6077, 0.5916],
       device='cuda:0') torch.Size([16])
percent tensor([0.5917, 0.5785, 0.5467, 0.5471, 0.5582, 0.5639, 0.5846, 0.5364, 0.5745,
        0.5815, 0.5793, 0.5505, 0.5847, 0.5985, 0.5395, 0.5773],
       device='cuda:0') torch.Size([16])
percent tensor([0.9996, 0.9958, 0.9973, 0.9972, 0.9977, 0.9964, 0.9992, 0.9986, 0.9970,
        0.9982, 0.9977, 0.9985, 0.9986, 0.9976, 0.9968, 0.9988],
       device='cuda:0') torch.Size([16])
Epoch: 29 | Batch_idx: 0 |  Loss: (0.5289) |  Loss2: (0.0000) | Acc: (82.00%) (106/128)
Epoch: 29 | Batch_idx: 10 |  Loss: (0.5875) |  Loss2: (0.0000) | Acc: (78.00%) (1108/1408)
Epoch: 29 | Batch_idx: 20 |  Loss: (0.5664) |  Loss2: (0.0000) | Acc: (79.00%) (2147/2688)
Epoch: 29 | Batch_idx: 30 |  Loss: (0.5655) |  Loss2: (0.0000) | Acc: (80.00%) (3177/3968)
Epoch: 29 | Batch_idx: 40 |  Loss: (0.5539) |  Loss2: (0.0000) | Acc: (80.00%) (4214/5248)
Epoch: 29 | Batch_idx: 50 |  Loss: (0.5556) |  Loss2: (0.0000) | Acc: (80.00%) (5227/6528)
Epoch: 29 | Batch_idx: 60 |  Loss: (0.5594) |  Loss2: (0.0000) | Acc: (79.00%) (6244/7808)
Epoch: 29 | Batch_idx: 70 |  Loss: (0.5586) |  Loss2: (0.0000) | Acc: (80.00%) (7280/9088)
Epoch: 29 | Batch_idx: 80 |  Loss: (0.5644) |  Loss2: (0.0000) | Acc: (80.00%) (8297/10368)
Epoch: 29 | Batch_idx: 90 |  Loss: (0.5670) |  Loss2: (0.0000) | Acc: (79.00%) (9301/11648)
Epoch: 29 | Batch_idx: 100 |  Loss: (0.5655) |  Loss2: (0.0000) | Acc: (79.00%) (10339/12928)
Epoch: 29 | Batch_idx: 110 |  Loss: (0.5630) |  Loss2: (0.0000) | Acc: (80.00%) (11387/14208)
Epoch: 29 | Batch_idx: 120 |  Loss: (0.5640) |  Loss2: (0.0000) | Acc: (80.00%) (12416/15488)
Epoch: 29 | Batch_idx: 130 |  Loss: (0.5644) |  Loss2: (0.0000) | Acc: (80.00%) (13427/16768)
Epoch: 29 | Batch_idx: 140 |  Loss: (0.5664) |  Loss2: (0.0000) | Acc: (80.00%) (14461/18048)
Epoch: 29 | Batch_idx: 150 |  Loss: (0.5679) |  Loss2: (0.0000) | Acc: (80.00%) (15478/19328)
Epoch: 29 | Batch_idx: 160 |  Loss: (0.5702) |  Loss2: (0.0000) | Acc: (80.00%) (16492/20608)
Epoch: 29 | Batch_idx: 170 |  Loss: (0.5707) |  Loss2: (0.0000) | Acc: (80.00%) (17531/21888)
Epoch: 29 | Batch_idx: 180 |  Loss: (0.5702) |  Loss2: (0.0000) | Acc: (80.00%) (18551/23168)
Epoch: 29 | Batch_idx: 190 |  Loss: (0.5715) |  Loss2: (0.0000) | Acc: (79.00%) (19552/24448)
Epoch: 29 | Batch_idx: 200 |  Loss: (0.5719) |  Loss2: (0.0000) | Acc: (79.00%) (20580/25728)
Epoch: 29 | Batch_idx: 210 |  Loss: (0.5714) |  Loss2: (0.0000) | Acc: (80.00%) (21623/27008)
Epoch: 29 | Batch_idx: 220 |  Loss: (0.5729) |  Loss2: (0.0000) | Acc: (79.00%) (22628/28288)
Epoch: 29 | Batch_idx: 230 |  Loss: (0.5742) |  Loss2: (0.0000) | Acc: (79.00%) (23641/29568)
Epoch: 29 | Batch_idx: 240 |  Loss: (0.5745) |  Loss2: (0.0000) | Acc: (79.00%) (24649/30848)
Epoch: 29 | Batch_idx: 250 |  Loss: (0.5765) |  Loss2: (0.0000) | Acc: (79.00%) (25648/32128)
Epoch: 29 | Batch_idx: 260 |  Loss: (0.5756) |  Loss2: (0.0000) | Acc: (79.00%) (26678/33408)
Epoch: 29 | Batch_idx: 270 |  Loss: (0.5758) |  Loss2: (0.0000) | Acc: (79.00%) (27699/34688)
Epoch: 29 | Batch_idx: 280 |  Loss: (0.5745) |  Loss2: (0.0000) | Acc: (79.00%) (28740/35968)
Epoch: 29 | Batch_idx: 290 |  Loss: (0.5752) |  Loss2: (0.0000) | Acc: (79.00%) (29761/37248)
Epoch: 29 | Batch_idx: 300 |  Loss: (0.5754) |  Loss2: (0.0000) | Acc: (79.00%) (30783/38528)
Epoch: 29 | Batch_idx: 310 |  Loss: (0.5755) |  Loss2: (0.0000) | Acc: (79.00%) (31810/39808)
Epoch: 29 | Batch_idx: 320 |  Loss: (0.5756) |  Loss2: (0.0000) | Acc: (79.00%) (32840/41088)
Epoch: 29 | Batch_idx: 330 |  Loss: (0.5747) |  Loss2: (0.0000) | Acc: (79.00%) (33890/42368)
Epoch: 29 | Batch_idx: 340 |  Loss: (0.5739) |  Loss2: (0.0000) | Acc: (80.00%) (34941/43648)
Epoch: 29 | Batch_idx: 350 |  Loss: (0.5743) |  Loss2: (0.0000) | Acc: (80.00%) (35967/44928)
Epoch: 29 | Batch_idx: 360 |  Loss: (0.5752) |  Loss2: (0.0000) | Acc: (80.00%) (36982/46208)
Epoch: 29 | Batch_idx: 370 |  Loss: (0.5762) |  Loss2: (0.0000) | Acc: (79.00%) (37968/47488)
Epoch: 29 | Batch_idx: 380 |  Loss: (0.5760) |  Loss2: (0.0000) | Acc: (79.00%) (38984/48768)
Epoch: 29 | Batch_idx: 390 |  Loss: (0.5761) |  Loss2: (0.0000) | Acc: (79.00%) (39958/50000)
# TEST : Loss: (0.8495) | Acc: (71.00%) (7149/10000)
percent tensor([0.5278, 0.5444, 0.5354, 0.5230, 0.5389, 0.5379, 0.5447, 0.5336, 0.5285,
        0.5416, 0.5335, 0.5402, 0.5281, 0.5313, 0.5402, 0.5308],
       device='cuda:0') torch.Size([16])
percent tensor([0.4877, 0.4840, 0.4907, 0.4913, 0.4877, 0.4936, 0.4861, 0.4876, 0.4856,
        0.4858, 0.4848, 0.4877, 0.4857, 0.4851, 0.4869, 0.4895],
       device='cuda:0') torch.Size([16])
percent tensor([0.5010, 0.5307, 0.4170, 0.4157, 0.4137, 0.4677, 0.4983, 0.4434, 0.4690,
        0.5050, 0.5138, 0.4563, 0.5134, 0.5171, 0.5034, 0.4971],
       device='cuda:0') torch.Size([16])
percent tensor([0.5087, 0.4621, 0.5029, 0.5275, 0.5090, 0.5481, 0.4849, 0.5117, 0.4937,
        0.4575, 0.4669, 0.4778, 0.4592, 0.5127, 0.5021, 0.5118],
       device='cuda:0') torch.Size([16])
percent tensor([0.4833, 0.4803, 0.4922, 0.4999, 0.4861, 0.4753, 0.4876, 0.4849, 0.5019,
        0.4843, 0.4879, 0.5023, 0.4887, 0.5128, 0.4809, 0.4745],
       device='cuda:0') torch.Size([16])
percent tensor([0.5945, 0.5841, 0.6303, 0.6560, 0.6348, 0.5978, 0.6099, 0.6307, 0.6157,
        0.5809, 0.5897, 0.6377, 0.5752, 0.6414, 0.6107, 0.5920],
       device='cuda:0') torch.Size([16])
percent tensor([0.5812, 0.5658, 0.5464, 0.5428, 0.5554, 0.5648, 0.5660, 0.5328, 0.5654,
        0.5746, 0.5717, 0.5391, 0.5713, 0.5881, 0.5255, 0.5634],
       device='cuda:0') torch.Size([16])
percent tensor([0.9986, 0.9953, 0.9985, 0.9972, 0.9984, 0.9931, 0.9977, 0.9993, 0.9970,
        0.9968, 0.9969, 0.9989, 0.9979, 0.9971, 0.9963, 0.9966],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 30 | Batch_idx: 0 |  Loss: (0.5415) |  Loss2: (0.0000) | Acc: (83.00%) (107/128)
Epoch: 30 | Batch_idx: 10 |  Loss: (0.5956) |  Loss2: (0.0000) | Acc: (79.00%) (1125/1408)
Epoch: 30 | Batch_idx: 20 |  Loss: (0.6538) |  Loss2: (0.0000) | Acc: (77.00%) (2075/2688)
Epoch: 30 | Batch_idx: 30 |  Loss: (0.6687) |  Loss2: (0.0000) | Acc: (76.00%) (3048/3968)
Epoch: 30 | Batch_idx: 40 |  Loss: (0.6781) |  Loss2: (0.0000) | Acc: (76.00%) (4012/5248)
Epoch: 30 | Batch_idx: 50 |  Loss: (0.6773) |  Loss2: (0.0000) | Acc: (76.00%) (4987/6528)
Epoch: 30 | Batch_idx: 60 |  Loss: (0.6908) |  Loss2: (0.0000) | Acc: (75.00%) (5920/7808)
Epoch: 30 | Batch_idx: 70 |  Loss: (0.6930) |  Loss2: (0.0000) | Acc: (75.00%) (6887/9088)
Epoch: 30 | Batch_idx: 80 |  Loss: (0.6966) |  Loss2: (0.0000) | Acc: (75.00%) (7838/10368)
Epoch: 30 | Batch_idx: 90 |  Loss: (0.6971) |  Loss2: (0.0000) | Acc: (75.00%) (8795/11648)
Epoch: 30 | Batch_idx: 100 |  Loss: (0.6944) |  Loss2: (0.0000) | Acc: (75.00%) (9775/12928)
Epoch: 30 | Batch_idx: 110 |  Loss: (0.6910) |  Loss2: (0.0000) | Acc: (75.00%) (10742/14208)
Epoch: 30 | Batch_idx: 120 |  Loss: (0.6913) |  Loss2: (0.0000) | Acc: (75.00%) (11703/15488)
Epoch: 30 | Batch_idx: 130 |  Loss: (0.6927) |  Loss2: (0.0000) | Acc: (75.00%) (12651/16768)
Epoch: 30 | Batch_idx: 140 |  Loss: (0.6905) |  Loss2: (0.0000) | Acc: (75.00%) (13628/18048)
Epoch: 30 | Batch_idx: 150 |  Loss: (0.6885) |  Loss2: (0.0000) | Acc: (75.00%) (14615/19328)
Epoch: 30 | Batch_idx: 160 |  Loss: (0.6865) |  Loss2: (0.0000) | Acc: (75.00%) (15597/20608)
Epoch: 30 | Batch_idx: 170 |  Loss: (0.6825) |  Loss2: (0.0000) | Acc: (75.00%) (16602/21888)
Epoch: 30 | Batch_idx: 180 |  Loss: (0.6833) |  Loss2: (0.0000) | Acc: (75.00%) (17561/23168)
Epoch: 30 | Batch_idx: 190 |  Loss: (0.6828) |  Loss2: (0.0000) | Acc: (75.00%) (18539/24448)
Epoch: 30 | Batch_idx: 200 |  Loss: (0.6806) |  Loss2: (0.0000) | Acc: (75.00%) (19534/25728)
Epoch: 30 | Batch_idx: 210 |  Loss: (0.6790) |  Loss2: (0.0000) | Acc: (75.00%) (20521/27008)
Epoch: 30 | Batch_idx: 220 |  Loss: (0.6767) |  Loss2: (0.0000) | Acc: (76.00%) (21526/28288)
Epoch: 30 | Batch_idx: 230 |  Loss: (0.6739) |  Loss2: (0.0000) | Acc: (76.00%) (22530/29568)
Epoch: 30 | Batch_idx: 240 |  Loss: (0.6733) |  Loss2: (0.0000) | Acc: (76.00%) (23522/30848)
Epoch: 30 | Batch_idx: 250 |  Loss: (0.6713) |  Loss2: (0.0000) | Acc: (76.00%) (24530/32128)
Epoch: 30 | Batch_idx: 260 |  Loss: (0.6698) |  Loss2: (0.0000) | Acc: (76.00%) (25545/33408)
Epoch: 30 | Batch_idx: 270 |  Loss: (0.6684) |  Loss2: (0.0000) | Acc: (76.00%) (26550/34688)
Epoch: 30 | Batch_idx: 280 |  Loss: (0.6688) |  Loss2: (0.0000) | Acc: (76.00%) (27535/35968)
Epoch: 30 | Batch_idx: 290 |  Loss: (0.6662) |  Loss2: (0.0000) | Acc: (76.00%) (28556/37248)
Epoch: 30 | Batch_idx: 300 |  Loss: (0.6650) |  Loss2: (0.0000) | Acc: (76.00%) (29556/38528)
Epoch: 30 | Batch_idx: 310 |  Loss: (0.6636) |  Loss2: (0.0000) | Acc: (76.00%) (30576/39808)
Epoch: 30 | Batch_idx: 320 |  Loss: (0.6613) |  Loss2: (0.0000) | Acc: (76.00%) (31587/41088)
Epoch: 30 | Batch_idx: 330 |  Loss: (0.6601) |  Loss2: (0.0000) | Acc: (76.00%) (32563/42368)
Epoch: 30 | Batch_idx: 340 |  Loss: (0.6586) |  Loss2: (0.0000) | Acc: (76.00%) (33565/43648)
Epoch: 30 | Batch_idx: 350 |  Loss: (0.6578) |  Loss2: (0.0000) | Acc: (76.00%) (34582/44928)
Epoch: 30 | Batch_idx: 360 |  Loss: (0.6560) |  Loss2: (0.0000) | Acc: (77.00%) (35600/46208)
Epoch: 30 | Batch_idx: 370 |  Loss: (0.6553) |  Loss2: (0.0000) | Acc: (77.00%) (36590/47488)
Epoch: 30 | Batch_idx: 380 |  Loss: (0.6540) |  Loss2: (0.0000) | Acc: (77.00%) (37593/48768)
Epoch: 30 | Batch_idx: 390 |  Loss: (0.6538) |  Loss2: (0.0000) | Acc: (77.00%) (38547/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_030.pth.tar'
# TEST : Loss: (0.6549) | Acc: (77.00%) (7733/10000)
percent tensor([0.5254, 0.5429, 0.5342, 0.5193, 0.5369, 0.5343, 0.5432, 0.5318, 0.5265,
        0.5399, 0.5316, 0.5387, 0.5264, 0.5293, 0.5378, 0.5276],
       device='cuda:0') torch.Size([16])
percent tensor([0.4863, 0.4824, 0.4872, 0.4892, 0.4852, 0.4928, 0.4838, 0.4856, 0.4833,
        0.4836, 0.4827, 0.4842, 0.4837, 0.4842, 0.4857, 0.4886],
       device='cuda:0') torch.Size([16])
percent tensor([0.4900, 0.5133, 0.4182, 0.4146, 0.4152, 0.4591, 0.4868, 0.4329, 0.4597,
        0.4940, 0.5023, 0.4532, 0.5002, 0.5004, 0.4872, 0.4853],
       device='cuda:0') torch.Size([16])
percent tensor([0.5288, 0.4971, 0.5195, 0.5356, 0.5270, 0.5507, 0.5150, 0.5357, 0.5201,
        0.4877, 0.4985, 0.4973, 0.4888, 0.5352, 0.5257, 0.5338],
       device='cuda:0') torch.Size([16])
percent tensor([0.5122, 0.5019, 0.4976, 0.5012, 0.4919, 0.4988, 0.5082, 0.4927, 0.5165,
        0.5035, 0.5095, 0.5048, 0.5102, 0.5252, 0.5009, 0.5049],
       device='cuda:0') torch.Size([16])
percent tensor([0.5929, 0.5756, 0.6189, 0.6438, 0.6261, 0.5990, 0.5983, 0.6228, 0.6091,
        0.5732, 0.5811, 0.6200, 0.5685, 0.6281, 0.5965, 0.5926],
       device='cuda:0') torch.Size([16])
percent tensor([0.5548, 0.5386, 0.5308, 0.5254, 0.5397, 0.5354, 0.5421, 0.5199, 0.5412,
        0.5475, 0.5411, 0.5124, 0.5374, 0.5592, 0.5074, 0.5402],
       device='cuda:0') torch.Size([16])
percent tensor([0.9995, 0.9960, 0.9986, 0.9977, 0.9983, 0.9962, 0.9983, 0.9992, 0.9979,
        0.9977, 0.9979, 0.9989, 0.9987, 0.9980, 0.9965, 0.9983],
       device='cuda:0') torch.Size([16])
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(171.8753, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(788.7971, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(782.7063, device='cuda:0')
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1527.7256, device='cuda:0')
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(504.0800, device='cuda:0')
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2183.2000, device='cuda:0')
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4294.9673, device='cuda:0')
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1434.3353, device='cuda:0')
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6102.7168, device='cuda:0')
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12135.6367, device='cuda:0')
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4046.5730, device='cuda:0')
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(17125.8926, device='cuda:0')
Epoch: 31 | Batch_idx: 0 |  Loss: (0.7414) |  Loss2: (0.0000) | Acc: (69.00%) (89/128)
Epoch: 31 | Batch_idx: 10 |  Loss: (0.6492) |  Loss2: (0.0000) | Acc: (75.00%) (1070/1408)
Epoch: 31 | Batch_idx: 20 |  Loss: (0.6361) |  Loss2: (0.0000) | Acc: (77.00%) (2080/2688)
Epoch: 31 | Batch_idx: 30 |  Loss: (0.6233) |  Loss2: (0.0000) | Acc: (77.00%) (3095/3968)
Epoch: 31 | Batch_idx: 40 |  Loss: (0.6206) |  Loss2: (0.0000) | Acc: (78.00%) (4101/5248)
Epoch: 31 | Batch_idx: 50 |  Loss: (0.6125) |  Loss2: (0.0000) | Acc: (78.00%) (5137/6528)
Epoch: 31 | Batch_idx: 60 |  Loss: (0.6217) |  Loss2: (0.0000) | Acc: (78.00%) (6107/7808)
Epoch: 31 | Batch_idx: 70 |  Loss: (0.6176) |  Loss2: (0.0000) | Acc: (78.00%) (7124/9088)
Epoch: 31 | Batch_idx: 80 |  Loss: (0.6203) |  Loss2: (0.0000) | Acc: (78.00%) (8119/10368)
Epoch: 31 | Batch_idx: 90 |  Loss: (0.6186) |  Loss2: (0.0000) | Acc: (78.00%) (9125/11648)
Epoch: 31 | Batch_idx: 100 |  Loss: (0.6170) |  Loss2: (0.0000) | Acc: (78.00%) (10139/12928)
Epoch: 31 | Batch_idx: 110 |  Loss: (0.6144) |  Loss2: (0.0000) | Acc: (78.00%) (11158/14208)
Epoch: 31 | Batch_idx: 120 |  Loss: (0.6117) |  Loss2: (0.0000) | Acc: (78.00%) (12177/15488)
Epoch: 31 | Batch_idx: 130 |  Loss: (0.6112) |  Loss2: (0.0000) | Acc: (78.00%) (13191/16768)
Epoch: 31 | Batch_idx: 140 |  Loss: (0.6136) |  Loss2: (0.0000) | Acc: (78.00%) (14165/18048)
Epoch: 31 | Batch_idx: 150 |  Loss: (0.6125) |  Loss2: (0.0000) | Acc: (78.00%) (15193/19328)
Epoch: 31 | Batch_idx: 160 |  Loss: (0.6105) |  Loss2: (0.0000) | Acc: (78.00%) (16222/20608)
Epoch: 31 | Batch_idx: 170 |  Loss: (0.6104) |  Loss2: (0.0000) | Acc: (78.00%) (17227/21888)
Epoch: 31 | Batch_idx: 180 |  Loss: (0.6118) |  Loss2: (0.0000) | Acc: (78.00%) (18210/23168)
Epoch: 31 | Batch_idx: 190 |  Loss: (0.6095) |  Loss2: (0.0000) | Acc: (78.00%) (19239/24448)
Epoch: 31 | Batch_idx: 200 |  Loss: (0.6082) |  Loss2: (0.0000) | Acc: (78.00%) (20255/25728)
Epoch: 31 | Batch_idx: 210 |  Loss: (0.6093) |  Loss2: (0.0000) | Acc: (78.00%) (21252/27008)
Epoch: 31 | Batch_idx: 220 |  Loss: (0.6060) |  Loss2: (0.0000) | Acc: (78.00%) (22298/28288)
Epoch: 31 | Batch_idx: 230 |  Loss: (0.6051) |  Loss2: (0.0000) | Acc: (78.00%) (23315/29568)
Epoch: 31 | Batch_idx: 240 |  Loss: (0.6049) |  Loss2: (0.0000) | Acc: (78.00%) (24343/30848)
Epoch: 31 | Batch_idx: 250 |  Loss: (0.6054) |  Loss2: (0.0000) | Acc: (78.00%) (25342/32128)
Epoch: 31 | Batch_idx: 260 |  Loss: (0.6047) |  Loss2: (0.0000) | Acc: (78.00%) (26346/33408)
Epoch: 31 | Batch_idx: 270 |  Loss: (0.6025) |  Loss2: (0.0000) | Acc: (78.00%) (27377/34688)
Epoch: 31 | Batch_idx: 280 |  Loss: (0.6007) |  Loss2: (0.0000) | Acc: (78.00%) (28409/35968)
Epoch: 31 | Batch_idx: 290 |  Loss: (0.5995) |  Loss2: (0.0000) | Acc: (79.00%) (29429/37248)
Epoch: 31 | Batch_idx: 300 |  Loss: (0.5980) |  Loss2: (0.0000) | Acc: (79.00%) (30464/38528)
Epoch: 31 | Batch_idx: 310 |  Loss: (0.5966) |  Loss2: (0.0000) | Acc: (79.00%) (31501/39808)
Epoch: 31 | Batch_idx: 320 |  Loss: (0.5965) |  Loss2: (0.0000) | Acc: (79.00%) (32525/41088)
Epoch: 31 | Batch_idx: 330 |  Loss: (0.5970) |  Loss2: (0.0000) | Acc: (79.00%) (33529/42368)
Epoch: 31 | Batch_idx: 340 |  Loss: (0.5971) |  Loss2: (0.0000) | Acc: (79.00%) (34552/43648)
Epoch: 31 | Batch_idx: 350 |  Loss: (0.5961) |  Loss2: (0.0000) | Acc: (79.00%) (35595/44928)
Epoch: 31 | Batch_idx: 360 |  Loss: (0.5953) |  Loss2: (0.0000) | Acc: (79.00%) (36614/46208)
Epoch: 31 | Batch_idx: 370 |  Loss: (0.5952) |  Loss2: (0.0000) | Acc: (79.00%) (37625/47488)
Epoch: 31 | Batch_idx: 380 |  Loss: (0.5943) |  Loss2: (0.0000) | Acc: (79.00%) (38669/48768)
Epoch: 31 | Batch_idx: 390 |  Loss: (0.5932) |  Loss2: (0.0000) | Acc: (79.00%) (39662/50000)
# TEST : Loss: (0.6196) | Acc: (78.00%) (7864/10000)
percent tensor([0.5212, 0.5380, 0.5302, 0.5154, 0.5326, 0.5296, 0.5383, 0.5276, 0.5220,
        0.5352, 0.5270, 0.5344, 0.5222, 0.5246, 0.5330, 0.5230],
       device='cuda:0') torch.Size([16])
percent tensor([0.4862, 0.4811, 0.4855, 0.4886, 0.4840, 0.4936, 0.4823, 0.4845, 0.4824,
        0.4818, 0.4813, 0.4822, 0.4826, 0.4836, 0.4853, 0.4884],
       device='cuda:0') torch.Size([16])
percent tensor([0.4969, 0.5140, 0.4271, 0.4231, 0.4215, 0.4649, 0.4905, 0.4396, 0.4672,
        0.4988, 0.5076, 0.4601, 0.5058, 0.5028, 0.4883, 0.4919],
       device='cuda:0') torch.Size([16])
percent tensor([0.5300, 0.4946, 0.5200, 0.5364, 0.5291, 0.5552, 0.5146, 0.5357, 0.5203,
        0.4858, 0.4961, 0.4946, 0.4862, 0.5369, 0.5241, 0.5328],
       device='cuda:0') torch.Size([16])
percent tensor([0.5287, 0.5159, 0.4996, 0.5035, 0.4942, 0.5128, 0.5224, 0.4951, 0.5268,
        0.5156, 0.5234, 0.5071, 0.5234, 0.5395, 0.5107, 0.5235],
       device='cuda:0') torch.Size([16])
percent tensor([0.6017, 0.5833, 0.6239, 0.6503, 0.6379, 0.6122, 0.6069, 0.6270, 0.6144,
        0.5825, 0.5873, 0.6192, 0.5743, 0.6376, 0.6010, 0.6045],
       device='cuda:0') torch.Size([16])
percent tensor([0.5546, 0.5404, 0.5403, 0.5341, 0.5499, 0.5372, 0.5464, 0.5264, 0.5445,
        0.5509, 0.5449, 0.5173, 0.5373, 0.5654, 0.5061, 0.5389],
       device='cuda:0') torch.Size([16])
percent tensor([0.9996, 0.9963, 0.9987, 0.9980, 0.9984, 0.9967, 0.9986, 0.9992, 0.9980,
        0.9979, 0.9983, 0.9987, 0.9989, 0.9983, 0.9969, 0.9985],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 32 | Batch_idx: 0 |  Loss: (0.7515) |  Loss2: (0.0000) | Acc: (75.00%) (97/128)
Epoch: 32 | Batch_idx: 10 |  Loss: (0.5633) |  Loss2: (0.0000) | Acc: (81.00%) (1144/1408)
Epoch: 32 | Batch_idx: 20 |  Loss: (0.5438) |  Loss2: (0.0000) | Acc: (81.00%) (2196/2688)
Epoch: 32 | Batch_idx: 30 |  Loss: (0.5598) |  Loss2: (0.0000) | Acc: (81.00%) (3221/3968)
Epoch: 32 | Batch_idx: 40 |  Loss: (0.5551) |  Loss2: (0.0000) | Acc: (81.00%) (4257/5248)
Epoch: 32 | Batch_idx: 50 |  Loss: (0.5548) |  Loss2: (0.0000) | Acc: (81.00%) (5312/6528)
Epoch: 32 | Batch_idx: 60 |  Loss: (0.5514) |  Loss2: (0.0000) | Acc: (81.00%) (6364/7808)
Epoch: 32 | Batch_idx: 70 |  Loss: (0.5493) |  Loss2: (0.0000) | Acc: (81.00%) (7395/9088)
Epoch: 32 | Batch_idx: 80 |  Loss: (0.5508) |  Loss2: (0.0000) | Acc: (81.00%) (8403/10368)
Epoch: 32 | Batch_idx: 90 |  Loss: (0.5531) |  Loss2: (0.0000) | Acc: (80.00%) (9425/11648)
Epoch: 32 | Batch_idx: 100 |  Loss: (0.5553) |  Loss2: (0.0000) | Acc: (80.00%) (10453/12928)
Epoch: 32 | Batch_idx: 110 |  Loss: (0.5552) |  Loss2: (0.0000) | Acc: (80.00%) (11491/14208)
Epoch: 32 | Batch_idx: 120 |  Loss: (0.5565) |  Loss2: (0.0000) | Acc: (80.00%) (12529/15488)
Epoch: 32 | Batch_idx: 130 |  Loss: (0.5587) |  Loss2: (0.0000) | Acc: (80.00%) (13557/16768)
Epoch: 32 | Batch_idx: 140 |  Loss: (0.5596) |  Loss2: (0.0000) | Acc: (80.00%) (14583/18048)
Epoch: 32 | Batch_idx: 150 |  Loss: (0.5599) |  Loss2: (0.0000) | Acc: (80.00%) (15607/19328)
Epoch: 32 | Batch_idx: 160 |  Loss: (0.5600) |  Loss2: (0.0000) | Acc: (80.00%) (16631/20608)
Epoch: 32 | Batch_idx: 170 |  Loss: (0.5583) |  Loss2: (0.0000) | Acc: (80.00%) (17689/21888)
Epoch: 32 | Batch_idx: 180 |  Loss: (0.5581) |  Loss2: (0.0000) | Acc: (80.00%) (18729/23168)
Epoch: 32 | Batch_idx: 190 |  Loss: (0.5571) |  Loss2: (0.0000) | Acc: (80.00%) (19777/24448)
Epoch: 32 | Batch_idx: 200 |  Loss: (0.5581) |  Loss2: (0.0000) | Acc: (80.00%) (20804/25728)
Epoch: 32 | Batch_idx: 210 |  Loss: (0.5582) |  Loss2: (0.0000) | Acc: (80.00%) (21825/27008)
Epoch: 32 | Batch_idx: 220 |  Loss: (0.5575) |  Loss2: (0.0000) | Acc: (80.00%) (22863/28288)
Epoch: 32 | Batch_idx: 230 |  Loss: (0.5573) |  Loss2: (0.0000) | Acc: (80.00%) (23899/29568)
Epoch: 32 | Batch_idx: 240 |  Loss: (0.5583) |  Loss2: (0.0000) | Acc: (80.00%) (24917/30848)
Epoch: 32 | Batch_idx: 250 |  Loss: (0.5568) |  Loss2: (0.0000) | Acc: (80.00%) (25957/32128)
Epoch: 32 | Batch_idx: 260 |  Loss: (0.5556) |  Loss2: (0.0000) | Acc: (80.00%) (27009/33408)
Epoch: 32 | Batch_idx: 270 |  Loss: (0.5558) |  Loss2: (0.0000) | Acc: (80.00%) (28025/34688)
Epoch: 32 | Batch_idx: 280 |  Loss: (0.5566) |  Loss2: (0.0000) | Acc: (80.00%) (29040/35968)
Epoch: 32 | Batch_idx: 290 |  Loss: (0.5553) |  Loss2: (0.0000) | Acc: (80.00%) (30087/37248)
Epoch: 32 | Batch_idx: 300 |  Loss: (0.5554) |  Loss2: (0.0000) | Acc: (80.00%) (31130/38528)
Epoch: 32 | Batch_idx: 310 |  Loss: (0.5552) |  Loss2: (0.0000) | Acc: (80.00%) (32179/39808)
Epoch: 32 | Batch_idx: 320 |  Loss: (0.5549) |  Loss2: (0.0000) | Acc: (80.00%) (33215/41088)
Epoch: 32 | Batch_idx: 330 |  Loss: (0.5538) |  Loss2: (0.0000) | Acc: (80.00%) (34267/42368)
Epoch: 32 | Batch_idx: 340 |  Loss: (0.5539) |  Loss2: (0.0000) | Acc: (80.00%) (35295/43648)
Epoch: 32 | Batch_idx: 350 |  Loss: (0.5532) |  Loss2: (0.0000) | Acc: (80.00%) (36342/44928)
Epoch: 32 | Batch_idx: 360 |  Loss: (0.5533) |  Loss2: (0.0000) | Acc: (80.00%) (37380/46208)
Epoch: 32 | Batch_idx: 370 |  Loss: (0.5537) |  Loss2: (0.0000) | Acc: (80.00%) (38403/47488)
Epoch: 32 | Batch_idx: 380 |  Loss: (0.5539) |  Loss2: (0.0000) | Acc: (80.00%) (39440/48768)
Epoch: 32 | Batch_idx: 390 |  Loss: (0.5537) |  Loss2: (0.0000) | Acc: (80.00%) (40433/50000)
# TEST : Loss: (0.6407) | Acc: (78.00%) (7833/10000)
percent tensor([0.5228, 0.5370, 0.5329, 0.5164, 0.5356, 0.5340, 0.5392, 0.5274, 0.5231,
        0.5351, 0.5276, 0.5369, 0.5229, 0.5236, 0.5340, 0.5242],
       device='cuda:0') torch.Size([16])
percent tensor([0.4857, 0.4808, 0.4868, 0.4889, 0.4849, 0.4939, 0.4822, 0.4851, 0.4823,
        0.4819, 0.4814, 0.4835, 0.4824, 0.4842, 0.4850, 0.4880],
       device='cuda:0') torch.Size([16])
percent tensor([0.4962, 0.5152, 0.4174, 0.4172, 0.4093, 0.4600, 0.4837, 0.4403, 0.4702,
        0.4968, 0.5079, 0.4543, 0.5087, 0.5066, 0.4850, 0.4906],
       device='cuda:0') torch.Size([16])
percent tensor([0.5253, 0.4823, 0.5266, 0.5360, 0.5337, 0.5538, 0.5129, 0.5382, 0.5170,
        0.4784, 0.4907, 0.5021, 0.4816, 0.5326, 0.5209, 0.5307],
       device='cuda:0') torch.Size([16])
percent tensor([0.5219, 0.5101, 0.5011, 0.5033, 0.4916, 0.5123, 0.5198, 0.4973, 0.5260,
        0.5088, 0.5171, 0.5065, 0.5184, 0.5368, 0.5037, 0.5206],
       device='cuda:0') torch.Size([16])
percent tensor([0.5932, 0.5811, 0.6291, 0.6601, 0.6441, 0.6144, 0.6122, 0.6358, 0.6043,
        0.5750, 0.5805, 0.6190, 0.5671, 0.6419, 0.6055, 0.6037],
       device='cuda:0') torch.Size([16])
percent tensor([0.5465, 0.5362, 0.5371, 0.5335, 0.5477, 0.5340, 0.5459, 0.5238, 0.5435,
        0.5452, 0.5452, 0.5228, 0.5370, 0.5636, 0.5052, 0.5410],
       device='cuda:0') torch.Size([16])
percent tensor([0.9990, 0.9976, 0.9985, 0.9988, 0.9985, 0.9949, 0.9989, 0.9993, 0.9977,
        0.9986, 0.9982, 0.9989, 0.9988, 0.9989, 0.9979, 0.9983],
       device='cuda:0') torch.Size([16])
Epoch: 33 | Batch_idx: 0 |  Loss: (0.6066) |  Loss2: (0.0000) | Acc: (73.00%) (94/128)
Epoch: 33 | Batch_idx: 10 |  Loss: (0.5837) |  Loss2: (0.0000) | Acc: (79.00%) (1119/1408)
Epoch: 33 | Batch_idx: 20 |  Loss: (0.5559) |  Loss2: (0.0000) | Acc: (80.00%) (2161/2688)
Epoch: 33 | Batch_idx: 30 |  Loss: (0.5570) |  Loss2: (0.0000) | Acc: (80.00%) (3182/3968)
Epoch: 33 | Batch_idx: 40 |  Loss: (0.5525) |  Loss2: (0.0000) | Acc: (80.00%) (4218/5248)
Epoch: 33 | Batch_idx: 50 |  Loss: (0.5504) |  Loss2: (0.0000) | Acc: (80.00%) (5248/6528)
Epoch: 33 | Batch_idx: 60 |  Loss: (0.5448) |  Loss2: (0.0000) | Acc: (80.00%) (6287/7808)
Epoch: 33 | Batch_idx: 70 |  Loss: (0.5410) |  Loss2: (0.0000) | Acc: (80.00%) (7353/9088)
Epoch: 33 | Batch_idx: 80 |  Loss: (0.5401) |  Loss2: (0.0000) | Acc: (80.00%) (8388/10368)
Epoch: 33 | Batch_idx: 90 |  Loss: (0.5440) |  Loss2: (0.0000) | Acc: (80.00%) (9420/11648)
Epoch: 33 | Batch_idx: 100 |  Loss: (0.5382) |  Loss2: (0.0000) | Acc: (81.00%) (10480/12928)
Epoch: 33 | Batch_idx: 110 |  Loss: (0.5371) |  Loss2: (0.0000) | Acc: (81.00%) (11532/14208)
Epoch: 33 | Batch_idx: 120 |  Loss: (0.5388) |  Loss2: (0.0000) | Acc: (81.00%) (12561/15488)
Epoch: 33 | Batch_idx: 130 |  Loss: (0.5389) |  Loss2: (0.0000) | Acc: (81.00%) (13605/16768)
Epoch: 33 | Batch_idx: 140 |  Loss: (0.5387) |  Loss2: (0.0000) | Acc: (81.00%) (14649/18048)
Epoch: 33 | Batch_idx: 150 |  Loss: (0.5347) |  Loss2: (0.0000) | Acc: (81.00%) (15710/19328)
Epoch: 33 | Batch_idx: 160 |  Loss: (0.5332) |  Loss2: (0.0000) | Acc: (81.00%) (16763/20608)
Epoch: 33 | Batch_idx: 170 |  Loss: (0.5311) |  Loss2: (0.0000) | Acc: (81.00%) (17824/21888)
Epoch: 33 | Batch_idx: 180 |  Loss: (0.5316) |  Loss2: (0.0000) | Acc: (81.00%) (18843/23168)
Epoch: 33 | Batch_idx: 190 |  Loss: (0.5295) |  Loss2: (0.0000) | Acc: (81.00%) (19903/24448)
Epoch: 33 | Batch_idx: 200 |  Loss: (0.5283) |  Loss2: (0.0000) | Acc: (81.00%) (20961/25728)
Epoch: 33 | Batch_idx: 210 |  Loss: (0.5263) |  Loss2: (0.0000) | Acc: (81.00%) (22034/27008)
Epoch: 33 | Batch_idx: 220 |  Loss: (0.5273) |  Loss2: (0.0000) | Acc: (81.00%) (23076/28288)
Epoch: 33 | Batch_idx: 230 |  Loss: (0.5267) |  Loss2: (0.0000) | Acc: (81.00%) (24126/29568)
Epoch: 33 | Batch_idx: 240 |  Loss: (0.5269) |  Loss2: (0.0000) | Acc: (81.00%) (25153/30848)
Epoch: 33 | Batch_idx: 250 |  Loss: (0.5274) |  Loss2: (0.0000) | Acc: (81.00%) (26203/32128)
Epoch: 33 | Batch_idx: 260 |  Loss: (0.5258) |  Loss2: (0.0000) | Acc: (81.00%) (27264/33408)
Epoch: 33 | Batch_idx: 270 |  Loss: (0.5260) |  Loss2: (0.0000) | Acc: (81.00%) (28310/34688)
Epoch: 33 | Batch_idx: 280 |  Loss: (0.5262) |  Loss2: (0.0000) | Acc: (81.00%) (29343/35968)
Epoch: 33 | Batch_idx: 290 |  Loss: (0.5262) |  Loss2: (0.0000) | Acc: (81.00%) (30409/37248)
Epoch: 33 | Batch_idx: 300 |  Loss: (0.5274) |  Loss2: (0.0000) | Acc: (81.00%) (31443/38528)
Epoch: 33 | Batch_idx: 310 |  Loss: (0.5275) |  Loss2: (0.0000) | Acc: (81.00%) (32495/39808)
Epoch: 33 | Batch_idx: 320 |  Loss: (0.5279) |  Loss2: (0.0000) | Acc: (81.00%) (33539/41088)
Epoch: 33 | Batch_idx: 330 |  Loss: (0.5283) |  Loss2: (0.0000) | Acc: (81.00%) (34574/42368)
Epoch: 33 | Batch_idx: 340 |  Loss: (0.5289) |  Loss2: (0.0000) | Acc: (81.00%) (35602/43648)
Epoch: 33 | Batch_idx: 350 |  Loss: (0.5295) |  Loss2: (0.0000) | Acc: (81.00%) (36634/44928)
Epoch: 33 | Batch_idx: 360 |  Loss: (0.5287) |  Loss2: (0.0000) | Acc: (81.00%) (37704/46208)
Epoch: 33 | Batch_idx: 370 |  Loss: (0.5291) |  Loss2: (0.0000) | Acc: (81.00%) (38725/47488)
Epoch: 33 | Batch_idx: 380 |  Loss: (0.5298) |  Loss2: (0.0000) | Acc: (81.00%) (39766/48768)
Epoch: 33 | Batch_idx: 390 |  Loss: (0.5295) |  Loss2: (0.0000) | Acc: (81.00%) (40782/50000)
# TEST : Loss: (0.6709) | Acc: (77.00%) (7737/10000)
percent tensor([0.5226, 0.5370, 0.5305, 0.5167, 0.5339, 0.5333, 0.5382, 0.5269, 0.5230,
        0.5344, 0.5280, 0.5346, 0.5230, 0.5233, 0.5338, 0.5240],
       device='cuda:0') torch.Size([16])
percent tensor([0.4859, 0.4795, 0.4851, 0.4886, 0.4845, 0.4944, 0.4817, 0.4838, 0.4818,
        0.4807, 0.4805, 0.4814, 0.4820, 0.4819, 0.4850, 0.4875],
       device='cuda:0') torch.Size([16])
percent tensor([0.4984, 0.5158, 0.4039, 0.4155, 0.4006, 0.4609, 0.4820, 0.4328, 0.4689,
        0.4914, 0.5109, 0.4417, 0.5107, 0.5040, 0.4881, 0.4920],
       device='cuda:0') torch.Size([16])
percent tensor([0.5318, 0.4830, 0.5312, 0.5387, 0.5335, 0.5549, 0.5136, 0.5392, 0.5213,
        0.4811, 0.4943, 0.5053, 0.4838, 0.5302, 0.5221, 0.5332],
       device='cuda:0') torch.Size([16])
percent tensor([0.5260, 0.5161, 0.5015, 0.5066, 0.4902, 0.5118, 0.5233, 0.4990, 0.5294,
        0.5108, 0.5236, 0.5098, 0.5219, 0.5394, 0.5076, 0.5223],
       device='cuda:0') torch.Size([16])
percent tensor([0.6017, 0.5822, 0.6439, 0.6598, 0.6534, 0.6158, 0.6190, 0.6372, 0.6126,
        0.5831, 0.5901, 0.6331, 0.5708, 0.6403, 0.6081, 0.6069],
       device='cuda:0') torch.Size([16])
percent tensor([0.5530, 0.5449, 0.5406, 0.5360, 0.5483, 0.5339, 0.5535, 0.5272, 0.5385,
        0.5501, 0.5513, 0.5220, 0.5425, 0.5685, 0.5094, 0.5378],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9985, 0.9981, 0.9984, 0.9981, 0.9946, 0.9996, 0.9989, 0.9977,
        0.9994, 0.9991, 0.9991, 0.9991, 0.9989, 0.9985, 0.9985],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 34 | Batch_idx: 0 |  Loss: (0.5448) |  Loss2: (0.0000) | Acc: (83.00%) (107/128)
Epoch: 34 | Batch_idx: 10 |  Loss: (0.5446) |  Loss2: (0.0000) | Acc: (81.00%) (1141/1408)
Epoch: 34 | Batch_idx: 20 |  Loss: (0.6122) |  Loss2: (0.0000) | Acc: (79.00%) (2128/2688)
Epoch: 34 | Batch_idx: 30 |  Loss: (0.6703) |  Loss2: (0.0000) | Acc: (77.00%) (3072/3968)
Epoch: 34 | Batch_idx: 40 |  Loss: (0.6837) |  Loss2: (0.0000) | Acc: (77.00%) (4047/5248)
Epoch: 34 | Batch_idx: 50 |  Loss: (0.7006) |  Loss2: (0.0000) | Acc: (76.00%) (4989/6528)
Epoch: 34 | Batch_idx: 60 |  Loss: (0.7034) |  Loss2: (0.0000) | Acc: (76.00%) (5984/7808)
Epoch: 34 | Batch_idx: 70 |  Loss: (0.7069) |  Loss2: (0.0000) | Acc: (76.00%) (6954/9088)
Epoch: 34 | Batch_idx: 80 |  Loss: (0.7098) |  Loss2: (0.0000) | Acc: (76.00%) (7891/10368)
Epoch: 34 | Batch_idx: 90 |  Loss: (0.7077) |  Loss2: (0.0000) | Acc: (75.00%) (8840/11648)
Epoch: 34 | Batch_idx: 100 |  Loss: (0.7030) |  Loss2: (0.0000) | Acc: (76.00%) (9831/12928)
Epoch: 34 | Batch_idx: 110 |  Loss: (0.6988) |  Loss2: (0.0000) | Acc: (76.00%) (10823/14208)
Epoch: 34 | Batch_idx: 120 |  Loss: (0.6952) |  Loss2: (0.0000) | Acc: (76.00%) (11813/15488)
Epoch: 34 | Batch_idx: 130 |  Loss: (0.6957) |  Loss2: (0.0000) | Acc: (76.00%) (12780/16768)
Epoch: 34 | Batch_idx: 140 |  Loss: (0.6941) |  Loss2: (0.0000) | Acc: (76.00%) (13779/18048)
Epoch: 34 | Batch_idx: 150 |  Loss: (0.6901) |  Loss2: (0.0000) | Acc: (76.00%) (14783/19328)
Epoch: 34 | Batch_idx: 160 |  Loss: (0.6862) |  Loss2: (0.0000) | Acc: (76.00%) (15776/20608)
Epoch: 34 | Batch_idx: 170 |  Loss: (0.6867) |  Loss2: (0.0000) | Acc: (76.00%) (16744/21888)
Epoch: 34 | Batch_idx: 180 |  Loss: (0.6863) |  Loss2: (0.0000) | Acc: (76.00%) (17718/23168)
Epoch: 34 | Batch_idx: 190 |  Loss: (0.6831) |  Loss2: (0.0000) | Acc: (76.00%) (18723/24448)
Epoch: 34 | Batch_idx: 200 |  Loss: (0.6844) |  Loss2: (0.0000) | Acc: (76.00%) (19697/25728)
Epoch: 34 | Batch_idx: 210 |  Loss: (0.6806) |  Loss2: (0.0000) | Acc: (76.00%) (20716/27008)
Epoch: 34 | Batch_idx: 220 |  Loss: (0.6776) |  Loss2: (0.0000) | Acc: (76.00%) (21705/28288)
Epoch: 34 | Batch_idx: 230 |  Loss: (0.6753) |  Loss2: (0.0000) | Acc: (76.00%) (22705/29568)
Epoch: 34 | Batch_idx: 240 |  Loss: (0.6704) |  Loss2: (0.0000) | Acc: (76.00%) (23729/30848)
Epoch: 34 | Batch_idx: 250 |  Loss: (0.6672) |  Loss2: (0.0000) | Acc: (77.00%) (24751/32128)
Epoch: 34 | Batch_idx: 260 |  Loss: (0.6663) |  Loss2: (0.0000) | Acc: (77.00%) (25757/33408)
Epoch: 34 | Batch_idx: 270 |  Loss: (0.6649) |  Loss2: (0.0000) | Acc: (77.00%) (26755/34688)
Epoch: 34 | Batch_idx: 280 |  Loss: (0.6631) |  Loss2: (0.0000) | Acc: (77.00%) (27757/35968)
Epoch: 34 | Batch_idx: 290 |  Loss: (0.6617) |  Loss2: (0.0000) | Acc: (77.00%) (28742/37248)
Epoch: 34 | Batch_idx: 300 |  Loss: (0.6592) |  Loss2: (0.0000) | Acc: (77.00%) (29747/38528)
Epoch: 34 | Batch_idx: 310 |  Loss: (0.6577) |  Loss2: (0.0000) | Acc: (77.00%) (30737/39808)
Epoch: 34 | Batch_idx: 320 |  Loss: (0.6571) |  Loss2: (0.0000) | Acc: (77.00%) (31744/41088)
Epoch: 34 | Batch_idx: 330 |  Loss: (0.6550) |  Loss2: (0.0000) | Acc: (77.00%) (32766/42368)
Epoch: 34 | Batch_idx: 340 |  Loss: (0.6524) |  Loss2: (0.0000) | Acc: (77.00%) (33782/43648)
Epoch: 34 | Batch_idx: 350 |  Loss: (0.6513) |  Loss2: (0.0000) | Acc: (77.00%) (34790/44928)
Epoch: 34 | Batch_idx: 360 |  Loss: (0.6483) |  Loss2: (0.0000) | Acc: (77.00%) (35804/46208)
Epoch: 34 | Batch_idx: 370 |  Loss: (0.6467) |  Loss2: (0.0000) | Acc: (77.00%) (36820/47488)
Epoch: 34 | Batch_idx: 380 |  Loss: (0.6458) |  Loss2: (0.0000) | Acc: (77.00%) (37824/48768)
Epoch: 34 | Batch_idx: 390 |  Loss: (0.6440) |  Loss2: (0.0000) | Acc: (77.00%) (38814/50000)
# TEST : Loss: (0.6458) | Acc: (77.00%) (7756/10000)
percent tensor([0.5036, 0.5124, 0.5098, 0.5019, 0.5115, 0.5148, 0.5127, 0.5060, 0.5028,
        0.5098, 0.5075, 0.5098, 0.5021, 0.5049, 0.5119, 0.5042],
       device='cuda:0') torch.Size([16])
percent tensor([0.4921, 0.4851, 0.4923, 0.4964, 0.4918, 0.5001, 0.4876, 0.4899, 0.4884,
        0.4858, 0.4862, 0.4875, 0.4876, 0.4889, 0.4906, 0.4936],
       device='cuda:0') torch.Size([16])
percent tensor([0.4969, 0.5176, 0.3942, 0.4166, 0.3928, 0.4651, 0.4756, 0.4284, 0.4689,
        0.4791, 0.5098, 0.4217, 0.4954, 0.5226, 0.4882, 0.4890],
       device='cuda:0') torch.Size([16])
percent tensor([0.5142, 0.4388, 0.5243, 0.5408, 0.5283, 0.5589, 0.4750, 0.5233, 0.4949,
        0.4343, 0.4523, 0.4730, 0.4426, 0.4983, 0.4885, 0.5166],
       device='cuda:0') torch.Size([16])
percent tensor([0.5050, 0.5190, 0.4925, 0.4960, 0.4795, 0.4888, 0.5192, 0.4842, 0.5266,
        0.5088, 0.5236, 0.5106, 0.5179, 0.5507, 0.4930, 0.4998],
       device='cuda:0') torch.Size([16])
percent tensor([0.5545, 0.5485, 0.6151, 0.6305, 0.6230, 0.5729, 0.5835, 0.5853, 0.5804,
        0.5441, 0.5561, 0.6080, 0.5362, 0.5974, 0.5554, 0.5518],
       device='cuda:0') torch.Size([16])
percent tensor([0.5768, 0.5719, 0.5451, 0.5409, 0.5502, 0.5458, 0.5716, 0.5398, 0.5536,
        0.5776, 0.5802, 0.5352, 0.5669, 0.5852, 0.5225, 0.5614],
       device='cuda:0') torch.Size([16])
percent tensor([0.9994, 0.9983, 0.9978, 0.9988, 0.9976, 0.9950, 0.9994, 0.9991, 0.9967,
        0.9991, 0.9987, 0.9989, 0.9986, 0.9985, 0.9974, 0.9974],
       device='cuda:0') torch.Size([16])
Epoch: 35 | Batch_idx: 0 |  Loss: (0.6755) |  Loss2: (0.0000) | Acc: (77.00%) (99/128)
Epoch: 35 | Batch_idx: 10 |  Loss: (0.5797) |  Loss2: (0.0000) | Acc: (78.00%) (1110/1408)
Epoch: 35 | Batch_idx: 20 |  Loss: (0.6019) |  Loss2: (0.0000) | Acc: (78.00%) (2098/2688)
Epoch: 35 | Batch_idx: 30 |  Loss: (0.5957) |  Loss2: (0.0000) | Acc: (78.00%) (3105/3968)
Epoch: 35 | Batch_idx: 40 |  Loss: (0.5926) |  Loss2: (0.0000) | Acc: (78.00%) (4137/5248)
Epoch: 35 | Batch_idx: 50 |  Loss: (0.5842) |  Loss2: (0.0000) | Acc: (79.00%) (5176/6528)
Epoch: 35 | Batch_idx: 60 |  Loss: (0.5778) |  Loss2: (0.0000) | Acc: (79.00%) (6213/7808)
Epoch: 35 | Batch_idx: 70 |  Loss: (0.5786) |  Loss2: (0.0000) | Acc: (79.00%) (7236/9088)
Epoch: 35 | Batch_idx: 80 |  Loss: (0.5773) |  Loss2: (0.0000) | Acc: (79.00%) (8257/10368)
Epoch: 35 | Batch_idx: 90 |  Loss: (0.5795) |  Loss2: (0.0000) | Acc: (79.00%) (9253/11648)
Epoch: 35 | Batch_idx: 100 |  Loss: (0.5814) |  Loss2: (0.0000) | Acc: (79.00%) (10272/12928)
Epoch: 35 | Batch_idx: 110 |  Loss: (0.5754) |  Loss2: (0.0000) | Acc: (79.00%) (11324/14208)
Epoch: 35 | Batch_idx: 120 |  Loss: (0.5750) |  Loss2: (0.0000) | Acc: (79.00%) (12349/15488)
Epoch: 35 | Batch_idx: 130 |  Loss: (0.5782) |  Loss2: (0.0000) | Acc: (79.00%) (13366/16768)
Epoch: 35 | Batch_idx: 140 |  Loss: (0.5786) |  Loss2: (0.0000) | Acc: (79.00%) (14393/18048)
Epoch: 35 | Batch_idx: 150 |  Loss: (0.5768) |  Loss2: (0.0000) | Acc: (79.00%) (15430/19328)
Epoch: 35 | Batch_idx: 160 |  Loss: (0.5780) |  Loss2: (0.0000) | Acc: (79.00%) (16451/20608)
Epoch: 35 | Batch_idx: 170 |  Loss: (0.5766) |  Loss2: (0.0000) | Acc: (79.00%) (17488/21888)
Epoch: 35 | Batch_idx: 180 |  Loss: (0.5780) |  Loss2: (0.0000) | Acc: (79.00%) (18483/23168)
Epoch: 35 | Batch_idx: 190 |  Loss: (0.5768) |  Loss2: (0.0000) | Acc: (79.00%) (19503/24448)
Epoch: 35 | Batch_idx: 200 |  Loss: (0.5750) |  Loss2: (0.0000) | Acc: (79.00%) (20539/25728)
Epoch: 35 | Batch_idx: 210 |  Loss: (0.5727) |  Loss2: (0.0000) | Acc: (79.00%) (21576/27008)
Epoch: 35 | Batch_idx: 220 |  Loss: (0.5711) |  Loss2: (0.0000) | Acc: (79.00%) (22620/28288)
Epoch: 35 | Batch_idx: 230 |  Loss: (0.5709) |  Loss2: (0.0000) | Acc: (79.00%) (23641/29568)
Epoch: 35 | Batch_idx: 240 |  Loss: (0.5709) |  Loss2: (0.0000) | Acc: (79.00%) (24651/30848)
Epoch: 35 | Batch_idx: 250 |  Loss: (0.5725) |  Loss2: (0.0000) | Acc: (79.00%) (25663/32128)
Epoch: 35 | Batch_idx: 260 |  Loss: (0.5723) |  Loss2: (0.0000) | Acc: (79.00%) (26703/33408)
Epoch: 35 | Batch_idx: 270 |  Loss: (0.5717) |  Loss2: (0.0000) | Acc: (79.00%) (27742/34688)
Epoch: 35 | Batch_idx: 280 |  Loss: (0.5714) |  Loss2: (0.0000) | Acc: (79.00%) (28764/35968)
Epoch: 35 | Batch_idx: 290 |  Loss: (0.5709) |  Loss2: (0.0000) | Acc: (79.00%) (29797/37248)
Epoch: 35 | Batch_idx: 300 |  Loss: (0.5695) |  Loss2: (0.0000) | Acc: (80.00%) (30844/38528)
Epoch: 35 | Batch_idx: 310 |  Loss: (0.5699) |  Loss2: (0.0000) | Acc: (80.00%) (31872/39808)
Epoch: 35 | Batch_idx: 320 |  Loss: (0.5681) |  Loss2: (0.0000) | Acc: (80.00%) (32923/41088)
Epoch: 35 | Batch_idx: 330 |  Loss: (0.5678) |  Loss2: (0.0000) | Acc: (80.00%) (33954/42368)
Epoch: 35 | Batch_idx: 340 |  Loss: (0.5676) |  Loss2: (0.0000) | Acc: (80.00%) (34980/43648)
Epoch: 35 | Batch_idx: 350 |  Loss: (0.5675) |  Loss2: (0.0000) | Acc: (80.00%) (36010/44928)
Epoch: 35 | Batch_idx: 360 |  Loss: (0.5678) |  Loss2: (0.0000) | Acc: (80.00%) (37022/46208)
Epoch: 35 | Batch_idx: 370 |  Loss: (0.5673) |  Loss2: (0.0000) | Acc: (80.00%) (38056/47488)
Epoch: 35 | Batch_idx: 380 |  Loss: (0.5671) |  Loss2: (0.0000) | Acc: (80.00%) (39101/48768)
Epoch: 35 | Batch_idx: 390 |  Loss: (0.5670) |  Loss2: (0.0000) | Acc: (80.00%) (40096/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_035.pth.tar'
# TEST : Loss: (0.6015) | Acc: (79.00%) (7920/10000)
percent tensor([0.5080, 0.5160, 0.5130, 0.5042, 0.5151, 0.5192, 0.5168, 0.5094, 0.5070,
        0.5141, 0.5116, 0.5138, 0.5062, 0.5073, 0.5160, 0.5083],
       device='cuda:0') torch.Size([16])
percent tensor([0.4954, 0.4891, 0.4960, 0.5003, 0.4954, 0.5022, 0.4914, 0.4940, 0.4921,
        0.4897, 0.4899, 0.4908, 0.4912, 0.4929, 0.4941, 0.4970],
       device='cuda:0') torch.Size([16])
percent tensor([0.5057, 0.5175, 0.4096, 0.4293, 0.4093, 0.4757, 0.4813, 0.4410, 0.4721,
        0.4856, 0.5125, 0.4333, 0.5002, 0.5191, 0.4945, 0.4956],
       device='cuda:0') torch.Size([16])
percent tensor([0.5232, 0.4367, 0.5289, 0.5539, 0.5352, 0.5712, 0.4758, 0.5334, 0.4934,
        0.4277, 0.4517, 0.4687, 0.4372, 0.5079, 0.4941, 0.5263],
       device='cuda:0') torch.Size([16])
percent tensor([0.5080, 0.5298, 0.4949, 0.4974, 0.4749, 0.4874, 0.5235, 0.4834, 0.5346,
        0.5179, 0.5353, 0.5214, 0.5305, 0.5615, 0.4966, 0.5009],
       device='cuda:0') torch.Size([16])
percent tensor([0.5649, 0.5670, 0.6167, 0.6370, 0.6222, 0.5739, 0.5969, 0.5879, 0.5976,
        0.5542, 0.5769, 0.6228, 0.5515, 0.6343, 0.5689, 0.5556],
       device='cuda:0') torch.Size([16])
percent tensor([0.6084, 0.5994, 0.5616, 0.5573, 0.5654, 0.5705, 0.5990, 0.5522, 0.5787,
        0.6046, 0.6087, 0.5482, 0.5963, 0.6197, 0.5353, 0.5854],
       device='cuda:0') torch.Size([16])
percent tensor([0.9996, 0.9982, 0.9982, 0.9990, 0.9976, 0.9945, 0.9995, 0.9991, 0.9970,
        0.9991, 0.9988, 0.9990, 0.9987, 0.9987, 0.9977, 0.9975],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 36 | Batch_idx: 0 |  Loss: (0.5255) |  Loss2: (0.0000) | Acc: (80.00%) (103/128)
Epoch: 36 | Batch_idx: 10 |  Loss: (0.5116) |  Loss2: (0.0000) | Acc: (82.00%) (1155/1408)
Epoch: 36 | Batch_idx: 20 |  Loss: (0.5011) |  Loss2: (0.0000) | Acc: (82.00%) (2219/2688)
Epoch: 36 | Batch_idx: 30 |  Loss: (0.5045) |  Loss2: (0.0000) | Acc: (82.00%) (3279/3968)
Epoch: 36 | Batch_idx: 40 |  Loss: (0.5108) |  Loss2: (0.0000) | Acc: (82.00%) (4319/5248)
Epoch: 36 | Batch_idx: 50 |  Loss: (0.5083) |  Loss2: (0.0000) | Acc: (82.00%) (5373/6528)
Epoch: 36 | Batch_idx: 60 |  Loss: (0.5193) |  Loss2: (0.0000) | Acc: (81.00%) (6394/7808)
Epoch: 36 | Batch_idx: 70 |  Loss: (0.5199) |  Loss2: (0.0000) | Acc: (81.00%) (7431/9088)
Epoch: 36 | Batch_idx: 80 |  Loss: (0.5150) |  Loss2: (0.0000) | Acc: (81.00%) (8499/10368)
Epoch: 36 | Batch_idx: 90 |  Loss: (0.5161) |  Loss2: (0.0000) | Acc: (82.00%) (9552/11648)
Epoch: 36 | Batch_idx: 100 |  Loss: (0.5133) |  Loss2: (0.0000) | Acc: (82.00%) (10611/12928)
Epoch: 36 | Batch_idx: 110 |  Loss: (0.5173) |  Loss2: (0.0000) | Acc: (81.00%) (11640/14208)
Epoch: 36 | Batch_idx: 120 |  Loss: (0.5153) |  Loss2: (0.0000) | Acc: (82.00%) (12705/15488)
Epoch: 36 | Batch_idx: 130 |  Loss: (0.5159) |  Loss2: (0.0000) | Acc: (81.00%) (13749/16768)
Epoch: 36 | Batch_idx: 140 |  Loss: (0.5144) |  Loss2: (0.0000) | Acc: (82.00%) (14811/18048)
Epoch: 36 | Batch_idx: 150 |  Loss: (0.5121) |  Loss2: (0.0000) | Acc: (82.00%) (15879/19328)
Epoch: 36 | Batch_idx: 160 |  Loss: (0.5172) |  Loss2: (0.0000) | Acc: (82.00%) (16904/20608)
Epoch: 36 | Batch_idx: 170 |  Loss: (0.5171) |  Loss2: (0.0000) | Acc: (82.00%) (17965/21888)
Epoch: 36 | Batch_idx: 180 |  Loss: (0.5169) |  Loss2: (0.0000) | Acc: (82.00%) (19022/23168)
Epoch: 36 | Batch_idx: 190 |  Loss: (0.5174) |  Loss2: (0.0000) | Acc: (82.00%) (20074/24448)
Epoch: 36 | Batch_idx: 200 |  Loss: (0.5155) |  Loss2: (0.0000) | Acc: (82.00%) (21135/25728)
Epoch: 36 | Batch_idx: 210 |  Loss: (0.5159) |  Loss2: (0.0000) | Acc: (82.00%) (22199/27008)
Epoch: 36 | Batch_idx: 220 |  Loss: (0.5163) |  Loss2: (0.0000) | Acc: (82.00%) (23247/28288)
Epoch: 36 | Batch_idx: 230 |  Loss: (0.5154) |  Loss2: (0.0000) | Acc: (82.00%) (24299/29568)
Epoch: 36 | Batch_idx: 240 |  Loss: (0.5152) |  Loss2: (0.0000) | Acc: (82.00%) (25353/30848)
Epoch: 36 | Batch_idx: 250 |  Loss: (0.5135) |  Loss2: (0.0000) | Acc: (82.00%) (26420/32128)
Epoch: 36 | Batch_idx: 260 |  Loss: (0.5132) |  Loss2: (0.0000) | Acc: (82.00%) (27468/33408)
Epoch: 36 | Batch_idx: 270 |  Loss: (0.5129) |  Loss2: (0.0000) | Acc: (82.00%) (28511/34688)
Epoch: 36 | Batch_idx: 280 |  Loss: (0.5122) |  Loss2: (0.0000) | Acc: (82.00%) (29575/35968)
Epoch: 36 | Batch_idx: 290 |  Loss: (0.5130) |  Loss2: (0.0000) | Acc: (82.00%) (30606/37248)
Epoch: 36 | Batch_idx: 300 |  Loss: (0.5142) |  Loss2: (0.0000) | Acc: (82.00%) (31639/38528)
Epoch: 36 | Batch_idx: 310 |  Loss: (0.5139) |  Loss2: (0.0000) | Acc: (82.00%) (32705/39808)
Epoch: 36 | Batch_idx: 320 |  Loss: (0.5129) |  Loss2: (0.0000) | Acc: (82.00%) (33757/41088)
Epoch: 36 | Batch_idx: 330 |  Loss: (0.5126) |  Loss2: (0.0000) | Acc: (82.00%) (34807/42368)
Epoch: 36 | Batch_idx: 340 |  Loss: (0.5123) |  Loss2: (0.0000) | Acc: (82.00%) (35871/43648)
Epoch: 36 | Batch_idx: 350 |  Loss: (0.5139) |  Loss2: (0.0000) | Acc: (82.00%) (36897/44928)
Epoch: 36 | Batch_idx: 360 |  Loss: (0.5137) |  Loss2: (0.0000) | Acc: (82.00%) (37953/46208)
Epoch: 36 | Batch_idx: 370 |  Loss: (0.5129) |  Loss2: (0.0000) | Acc: (82.00%) (39024/47488)
Epoch: 36 | Batch_idx: 380 |  Loss: (0.5130) |  Loss2: (0.0000) | Acc: (82.00%) (40077/48768)
Epoch: 36 | Batch_idx: 390 |  Loss: (0.5122) |  Loss2: (0.0000) | Acc: (82.00%) (41105/50000)
# TEST : Loss: (0.6504) | Acc: (78.00%) (7896/10000)
percent tensor([0.5079, 0.5172, 0.5122, 0.5034, 0.5139, 0.5176, 0.5174, 0.5095, 0.5067,
        0.5151, 0.5118, 0.5145, 0.5071, 0.5063, 0.5166, 0.5079],
       device='cuda:0') torch.Size([16])
percent tensor([0.4959, 0.4906, 0.4977, 0.5005, 0.4957, 0.5021, 0.4927, 0.4959, 0.4929,
        0.4916, 0.4914, 0.4921, 0.4920, 0.4932, 0.4948, 0.4977],
       device='cuda:0') torch.Size([16])
percent tensor([0.5065, 0.5201, 0.4058, 0.4083, 0.4019, 0.4672, 0.4929, 0.4401, 0.4714,
        0.4955, 0.5184, 0.4405, 0.5095, 0.5080, 0.4964, 0.4941],
       device='cuda:0') torch.Size([16])
percent tensor([0.5222, 0.4367, 0.5464, 0.5587, 0.5446, 0.5683, 0.4855, 0.5472, 0.5036,
        0.4432, 0.4608, 0.4843, 0.4379, 0.5020, 0.4964, 0.5304],
       device='cuda:0') torch.Size([16])
percent tensor([0.5029, 0.5125, 0.4852, 0.4994, 0.4749, 0.4962, 0.5139, 0.4725, 0.5298,
        0.5129, 0.5273, 0.5161, 0.5235, 0.5522, 0.4941, 0.4967],
       device='cuda:0') torch.Size([16])
percent tensor([0.5613, 0.5404, 0.6180, 0.6494, 0.6247, 0.5912, 0.5810, 0.5842, 0.5945,
        0.5376, 0.5683, 0.6197, 0.5440, 0.6240, 0.5660, 0.5575],
       device='cuda:0') torch.Size([16])
percent tensor([0.6011, 0.5906, 0.5637, 0.5578, 0.5795, 0.5887, 0.5972, 0.5436, 0.5907,
        0.6032, 0.6007, 0.5556, 0.5933, 0.6215, 0.5391, 0.5871],
       device='cuda:0') torch.Size([16])
percent tensor([0.9986, 0.9959, 0.9978, 0.9989, 0.9994, 0.9948, 0.9991, 0.9997, 0.9983,
        0.9981, 0.9973, 0.9988, 0.9986, 0.9982, 0.9971, 0.9984],
       device='cuda:0') torch.Size([16])
Epoch: 37 | Batch_idx: 0 |  Loss: (0.5672) |  Loss2: (0.0000) | Acc: (78.00%) (100/128)
Epoch: 37 | Batch_idx: 10 |  Loss: (0.5196) |  Loss2: (0.0000) | Acc: (81.00%) (1151/1408)
Epoch: 37 | Batch_idx: 20 |  Loss: (0.4895) |  Loss2: (0.0000) | Acc: (82.00%) (2229/2688)
Epoch: 37 | Batch_idx: 30 |  Loss: (0.4899) |  Loss2: (0.0000) | Acc: (83.00%) (3295/3968)
Epoch: 37 | Batch_idx: 40 |  Loss: (0.4945) |  Loss2: (0.0000) | Acc: (82.00%) (4349/5248)
Epoch: 37 | Batch_idx: 50 |  Loss: (0.5025) |  Loss2: (0.0000) | Acc: (82.00%) (5385/6528)
Epoch: 37 | Batch_idx: 60 |  Loss: (0.5020) |  Loss2: (0.0000) | Acc: (82.00%) (6449/7808)
Epoch: 37 | Batch_idx: 70 |  Loss: (0.5024) |  Loss2: (0.0000) | Acc: (82.00%) (7504/9088)
Epoch: 37 | Batch_idx: 80 |  Loss: (0.4998) |  Loss2: (0.0000) | Acc: (82.00%) (8568/10368)
Epoch: 37 | Batch_idx: 90 |  Loss: (0.4950) |  Loss2: (0.0000) | Acc: (82.00%) (9650/11648)
Epoch: 37 | Batch_idx: 100 |  Loss: (0.4952) |  Loss2: (0.0000) | Acc: (82.00%) (10701/12928)
Epoch: 37 | Batch_idx: 110 |  Loss: (0.4961) |  Loss2: (0.0000) | Acc: (82.00%) (11748/14208)
Epoch: 37 | Batch_idx: 120 |  Loss: (0.4952) |  Loss2: (0.0000) | Acc: (82.00%) (12830/15488)
Epoch: 37 | Batch_idx: 130 |  Loss: (0.4964) |  Loss2: (0.0000) | Acc: (82.00%) (13865/16768)
Epoch: 37 | Batch_idx: 140 |  Loss: (0.4963) |  Loss2: (0.0000) | Acc: (82.00%) (14896/18048)
Epoch: 37 | Batch_idx: 150 |  Loss: (0.4951) |  Loss2: (0.0000) | Acc: (82.00%) (15963/19328)
Epoch: 37 | Batch_idx: 160 |  Loss: (0.4935) |  Loss2: (0.0000) | Acc: (82.00%) (17034/20608)
Epoch: 37 | Batch_idx: 170 |  Loss: (0.4921) |  Loss2: (0.0000) | Acc: (82.00%) (18106/21888)
Epoch: 37 | Batch_idx: 180 |  Loss: (0.4952) |  Loss2: (0.0000) | Acc: (82.00%) (19132/23168)
Epoch: 37 | Batch_idx: 190 |  Loss: (0.4935) |  Loss2: (0.0000) | Acc: (82.00%) (20208/24448)
Epoch: 37 | Batch_idx: 200 |  Loss: (0.4914) |  Loss2: (0.0000) | Acc: (82.00%) (21296/25728)
Epoch: 37 | Batch_idx: 210 |  Loss: (0.4907) |  Loss2: (0.0000) | Acc: (82.00%) (22366/27008)
Epoch: 37 | Batch_idx: 220 |  Loss: (0.4892) |  Loss2: (0.0000) | Acc: (82.00%) (23442/28288)
Epoch: 37 | Batch_idx: 230 |  Loss: (0.4901) |  Loss2: (0.0000) | Acc: (82.00%) (24499/29568)
Epoch: 37 | Batch_idx: 240 |  Loss: (0.4899) |  Loss2: (0.0000) | Acc: (82.00%) (25568/30848)
Epoch: 37 | Batch_idx: 250 |  Loss: (0.4892) |  Loss2: (0.0000) | Acc: (82.00%) (26627/32128)
Epoch: 37 | Batch_idx: 260 |  Loss: (0.4880) |  Loss2: (0.0000) | Acc: (82.00%) (27698/33408)
Epoch: 37 | Batch_idx: 270 |  Loss: (0.4891) |  Loss2: (0.0000) | Acc: (82.00%) (28754/34688)
Epoch: 37 | Batch_idx: 280 |  Loss: (0.4916) |  Loss2: (0.0000) | Acc: (82.00%) (29787/35968)
Epoch: 37 | Batch_idx: 290 |  Loss: (0.4920) |  Loss2: (0.0000) | Acc: (82.00%) (30850/37248)
Epoch: 37 | Batch_idx: 300 |  Loss: (0.4917) |  Loss2: (0.0000) | Acc: (82.00%) (31921/38528)
Epoch: 37 | Batch_idx: 310 |  Loss: (0.4914) |  Loss2: (0.0000) | Acc: (82.00%) (32990/39808)
Epoch: 37 | Batch_idx: 320 |  Loss: (0.4922) |  Loss2: (0.0000) | Acc: (82.00%) (34033/41088)
Epoch: 37 | Batch_idx: 330 |  Loss: (0.4927) |  Loss2: (0.0000) | Acc: (82.00%) (35088/42368)
Epoch: 37 | Batch_idx: 340 |  Loss: (0.4927) |  Loss2: (0.0000) | Acc: (82.00%) (36145/43648)
Epoch: 37 | Batch_idx: 350 |  Loss: (0.4936) |  Loss2: (0.0000) | Acc: (82.00%) (37198/44928)
Epoch: 37 | Batch_idx: 360 |  Loss: (0.4928) |  Loss2: (0.0000) | Acc: (82.00%) (38266/46208)
Epoch: 37 | Batch_idx: 370 |  Loss: (0.4923) |  Loss2: (0.0000) | Acc: (82.00%) (39343/47488)
Epoch: 37 | Batch_idx: 380 |  Loss: (0.4927) |  Loss2: (0.0000) | Acc: (82.00%) (40380/48768)
Epoch: 37 | Batch_idx: 390 |  Loss: (0.4918) |  Loss2: (0.0000) | Acc: (82.00%) (41418/50000)
# TEST : Loss: (0.5620) | Acc: (80.00%) (8095/10000)
percent tensor([0.5088, 0.5178, 0.5126, 0.5036, 0.5147, 0.5179, 0.5180, 0.5096, 0.5074,
        0.5154, 0.5128, 0.5151, 0.5080, 0.5073, 0.5170, 0.5088],
       device='cuda:0') torch.Size([16])
percent tensor([0.4956, 0.4900, 0.4971, 0.5002, 0.4953, 0.5026, 0.4917, 0.4953, 0.4927,
        0.4903, 0.4907, 0.4914, 0.4914, 0.4932, 0.4948, 0.4977],
       device='cuda:0') torch.Size([16])
percent tensor([0.5063, 0.5178, 0.4130, 0.4138, 0.4090, 0.4667, 0.4905, 0.4409, 0.4688,
        0.4922, 0.5159, 0.4455, 0.5091, 0.5034, 0.4957, 0.4947],
       device='cuda:0') torch.Size([16])
percent tensor([0.5199, 0.4322, 0.5305, 0.5487, 0.5333, 0.5711, 0.4728, 0.5358, 0.4847,
        0.4295, 0.4506, 0.4651, 0.4299, 0.5034, 0.4988, 0.5294],
       device='cuda:0') torch.Size([16])
percent tensor([0.5099, 0.5232, 0.4898, 0.4985, 0.4754, 0.4854, 0.5200, 0.4818, 0.5351,
        0.5207, 0.5339, 0.5204, 0.5270, 0.5547, 0.4966, 0.5005],
       device='cuda:0') torch.Size([16])
percent tensor([0.5736, 0.5556, 0.6106, 0.6467, 0.6190, 0.5875, 0.5922, 0.5951, 0.5918,
        0.5482, 0.5765, 0.6180, 0.5507, 0.6385, 0.5816, 0.5644],
       device='cuda:0') torch.Size([16])
percent tensor([0.5989, 0.5874, 0.5641, 0.5499, 0.5701, 0.5834, 0.5923, 0.5358, 0.5898,
        0.6041, 0.5979, 0.5491, 0.5887, 0.6235, 0.5313, 0.5845],
       device='cuda:0') torch.Size([16])
percent tensor([0.9995, 0.9982, 0.9977, 0.9981, 0.9984, 0.9969, 0.9993, 0.9992, 0.9987,
        0.9992, 0.9985, 0.9986, 0.9988, 0.9989, 0.9980, 0.9987],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 38 | Batch_idx: 0 |  Loss: (0.4892) |  Loss2: (0.0000) | Acc: (82.00%) (106/128)
Epoch: 38 | Batch_idx: 10 |  Loss: (0.4587) |  Loss2: (0.0000) | Acc: (84.00%) (1195/1408)
Epoch: 38 | Batch_idx: 20 |  Loss: (0.4819) |  Loss2: (0.0000) | Acc: (83.00%) (2254/2688)
Epoch: 38 | Batch_idx: 30 |  Loss: (0.5007) |  Loss2: (0.0000) | Acc: (83.00%) (3303/3968)
Epoch: 38 | Batch_idx: 40 |  Loss: (0.5099) |  Loss2: (0.0000) | Acc: (82.00%) (4345/5248)
Epoch: 38 | Batch_idx: 50 |  Loss: (0.5141) |  Loss2: (0.0000) | Acc: (82.00%) (5360/6528)
Epoch: 38 | Batch_idx: 60 |  Loss: (0.5291) |  Loss2: (0.0000) | Acc: (81.00%) (6370/7808)
Epoch: 38 | Batch_idx: 70 |  Loss: (0.5290) |  Loss2: (0.0000) | Acc: (81.00%) (7421/9088)
Epoch: 38 | Batch_idx: 80 |  Loss: (0.5302) |  Loss2: (0.0000) | Acc: (81.00%) (8469/10368)
Epoch: 38 | Batch_idx: 90 |  Loss: (0.5271) |  Loss2: (0.0000) | Acc: (81.00%) (9525/11648)
Epoch: 38 | Batch_idx: 100 |  Loss: (0.5267) |  Loss2: (0.0000) | Acc: (81.00%) (10573/12928)
Epoch: 38 | Batch_idx: 110 |  Loss: (0.5324) |  Loss2: (0.0000) | Acc: (81.00%) (11610/14208)
Epoch: 38 | Batch_idx: 120 |  Loss: (0.5341) |  Loss2: (0.0000) | Acc: (81.00%) (12631/15488)
Epoch: 38 | Batch_idx: 130 |  Loss: (0.5378) |  Loss2: (0.0000) | Acc: (81.00%) (13659/16768)
Epoch: 38 | Batch_idx: 140 |  Loss: (0.5386) |  Loss2: (0.0000) | Acc: (81.00%) (14695/18048)
Epoch: 38 | Batch_idx: 150 |  Loss: (0.5388) |  Loss2: (0.0000) | Acc: (81.00%) (15739/19328)
Epoch: 38 | Batch_idx: 160 |  Loss: (0.5395) |  Loss2: (0.0000) | Acc: (81.00%) (16765/20608)
Epoch: 38 | Batch_idx: 170 |  Loss: (0.5377) |  Loss2: (0.0000) | Acc: (81.00%) (17822/21888)
Epoch: 38 | Batch_idx: 180 |  Loss: (0.5372) |  Loss2: (0.0000) | Acc: (81.00%) (18868/23168)
Epoch: 38 | Batch_idx: 190 |  Loss: (0.5380) |  Loss2: (0.0000) | Acc: (81.00%) (19907/24448)
Epoch: 38 | Batch_idx: 200 |  Loss: (0.5393) |  Loss2: (0.0000) | Acc: (81.00%) (20929/25728)
Epoch: 38 | Batch_idx: 210 |  Loss: (0.5370) |  Loss2: (0.0000) | Acc: (81.00%) (21996/27008)
Epoch: 38 | Batch_idx: 220 |  Loss: (0.5376) |  Loss2: (0.0000) | Acc: (81.00%) (23020/28288)
Epoch: 38 | Batch_idx: 230 |  Loss: (0.5373) |  Loss2: (0.0000) | Acc: (81.00%) (24079/29568)
Epoch: 38 | Batch_idx: 240 |  Loss: (0.5369) |  Loss2: (0.0000) | Acc: (81.00%) (25114/30848)
Epoch: 38 | Batch_idx: 250 |  Loss: (0.5344) |  Loss2: (0.0000) | Acc: (81.00%) (26168/32128)
Epoch: 38 | Batch_idx: 260 |  Loss: (0.5346) |  Loss2: (0.0000) | Acc: (81.00%) (27199/33408)
Epoch: 38 | Batch_idx: 270 |  Loss: (0.5363) |  Loss2: (0.0000) | Acc: (81.00%) (28220/34688)
Epoch: 38 | Batch_idx: 280 |  Loss: (0.5350) |  Loss2: (0.0000) | Acc: (81.00%) (29268/35968)
Epoch: 38 | Batch_idx: 290 |  Loss: (0.5346) |  Loss2: (0.0000) | Acc: (81.00%) (30332/37248)
Epoch: 38 | Batch_idx: 300 |  Loss: (0.5336) |  Loss2: (0.0000) | Acc: (81.00%) (31393/38528)
Epoch: 38 | Batch_idx: 310 |  Loss: (0.5320) |  Loss2: (0.0000) | Acc: (81.00%) (32450/39808)
Epoch: 38 | Batch_idx: 320 |  Loss: (0.5314) |  Loss2: (0.0000) | Acc: (81.00%) (33514/41088)
Epoch: 38 | Batch_idx: 330 |  Loss: (0.5309) |  Loss2: (0.0000) | Acc: (81.00%) (34563/42368)
Epoch: 38 | Batch_idx: 340 |  Loss: (0.5310) |  Loss2: (0.0000) | Acc: (81.00%) (35618/43648)
Epoch: 38 | Batch_idx: 350 |  Loss: (0.5299) |  Loss2: (0.0000) | Acc: (81.00%) (36682/44928)
Epoch: 38 | Batch_idx: 360 |  Loss: (0.5297) |  Loss2: (0.0000) | Acc: (81.00%) (37729/46208)
Epoch: 38 | Batch_idx: 370 |  Loss: (0.5300) |  Loss2: (0.0000) | Acc: (81.00%) (38772/47488)
Epoch: 38 | Batch_idx: 380 |  Loss: (0.5292) |  Loss2: (0.0000) | Acc: (81.00%) (39832/48768)
Epoch: 38 | Batch_idx: 390 |  Loss: (0.5294) |  Loss2: (0.0000) | Acc: (81.00%) (40837/50000)
# TEST : Loss: (0.5623) | Acc: (80.00%) (8088/10000)
percent tensor([0.5147, 0.5242, 0.5176, 0.5071, 0.5206, 0.5257, 0.5247, 0.5140, 0.5134,
        0.5209, 0.5195, 0.5199, 0.5133, 0.5115, 0.5239, 0.5150],
       device='cuda:0') torch.Size([16])
percent tensor([0.4958, 0.4902, 0.4974, 0.5014, 0.4959, 0.5039, 0.4916, 0.4958, 0.4927,
        0.4902, 0.4907, 0.4906, 0.4911, 0.4942, 0.4955, 0.4986],
       device='cuda:0') torch.Size([16])
percent tensor([0.5217, 0.5332, 0.4034, 0.4175, 0.4117, 0.4864, 0.5053, 0.4443, 0.4773,
        0.4952, 0.5289, 0.4360, 0.5138, 0.5264, 0.5133, 0.5130],
       device='cuda:0') torch.Size([16])
percent tensor([0.5220, 0.4382, 0.5170, 0.5492, 0.5262, 0.5838, 0.4726, 0.5329, 0.4835,
        0.4315, 0.4527, 0.4461, 0.4241, 0.5180, 0.5052, 0.5454],
       device='cuda:0') torch.Size([16])
percent tensor([0.5091, 0.5223, 0.5067, 0.5061, 0.4922, 0.4845, 0.5204, 0.4996, 0.5364,
        0.5184, 0.5275, 0.5291, 0.5266, 0.5466, 0.5013, 0.4972],
       device='cuda:0') torch.Size([16])
percent tensor([0.6136, 0.5876, 0.6657, 0.7023, 0.6717, 0.6339, 0.6316, 0.6471, 0.6433,
        0.5838, 0.6147, 0.6640, 0.5893, 0.6839, 0.6196, 0.6123],
       device='cuda:0') torch.Size([16])
percent tensor([0.6125, 0.5930, 0.5609, 0.5438, 0.5700, 0.5886, 0.5903, 0.5302, 0.5960,
        0.6118, 0.6018, 0.5456, 0.6057, 0.6187, 0.5300, 0.5931],
       device='cuda:0') torch.Size([16])
percent tensor([0.9995, 0.9982, 0.9981, 0.9986, 0.9985, 0.9973, 0.9992, 0.9992, 0.9987,
        0.9990, 0.9981, 0.9985, 0.9989, 0.9986, 0.9976, 0.9988],
       device='cuda:0') torch.Size([16])
Epoch: 39 | Batch_idx: 0 |  Loss: (0.5189) |  Loss2: (0.0000) | Acc: (85.00%) (109/128)
Epoch: 39 | Batch_idx: 10 |  Loss: (0.5510) |  Loss2: (0.0000) | Acc: (80.00%) (1137/1408)
Epoch: 39 | Batch_idx: 20 |  Loss: (0.5237) |  Loss2: (0.0000) | Acc: (81.00%) (2194/2688)
Epoch: 39 | Batch_idx: 30 |  Loss: (0.5228) |  Loss2: (0.0000) | Acc: (81.00%) (3237/3968)
Epoch: 39 | Batch_idx: 40 |  Loss: (0.5186) |  Loss2: (0.0000) | Acc: (81.00%) (4296/5248)
Epoch: 39 | Batch_idx: 50 |  Loss: (0.5198) |  Loss2: (0.0000) | Acc: (81.00%) (5337/6528)
Epoch: 39 | Batch_idx: 60 |  Loss: (0.5172) |  Loss2: (0.0000) | Acc: (81.00%) (6399/7808)
Epoch: 39 | Batch_idx: 70 |  Loss: (0.5216) |  Loss2: (0.0000) | Acc: (81.00%) (7441/9088)
Epoch: 39 | Batch_idx: 80 |  Loss: (0.5161) |  Loss2: (0.0000) | Acc: (82.00%) (8502/10368)
Epoch: 39 | Batch_idx: 90 |  Loss: (0.5096) |  Loss2: (0.0000) | Acc: (82.00%) (9597/11648)
Epoch: 39 | Batch_idx: 100 |  Loss: (0.5095) |  Loss2: (0.0000) | Acc: (82.00%) (10637/12928)
Epoch: 39 | Batch_idx: 110 |  Loss: (0.5043) |  Loss2: (0.0000) | Acc: (82.00%) (11714/14208)
Epoch: 39 | Batch_idx: 120 |  Loss: (0.5034) |  Loss2: (0.0000) | Acc: (82.00%) (12781/15488)
Epoch: 39 | Batch_idx: 130 |  Loss: (0.5035) |  Loss2: (0.0000) | Acc: (82.00%) (13845/16768)
Epoch: 39 | Batch_idx: 140 |  Loss: (0.5043) |  Loss2: (0.0000) | Acc: (82.00%) (14891/18048)
Epoch: 39 | Batch_idx: 150 |  Loss: (0.5035) |  Loss2: (0.0000) | Acc: (82.00%) (15943/19328)
Epoch: 39 | Batch_idx: 160 |  Loss: (0.5055) |  Loss2: (0.0000) | Acc: (82.00%) (16991/20608)
Epoch: 39 | Batch_idx: 170 |  Loss: (0.5032) |  Loss2: (0.0000) | Acc: (82.00%) (18065/21888)
Epoch: 39 | Batch_idx: 180 |  Loss: (0.5059) |  Loss2: (0.0000) | Acc: (82.00%) (19095/23168)
Epoch: 39 | Batch_idx: 190 |  Loss: (0.5059) |  Loss2: (0.0000) | Acc: (82.00%) (20138/24448)
Epoch: 39 | Batch_idx: 200 |  Loss: (0.5063) |  Loss2: (0.0000) | Acc: (82.00%) (21182/25728)
Epoch: 39 | Batch_idx: 210 |  Loss: (0.5062) |  Loss2: (0.0000) | Acc: (82.00%) (22237/27008)
Epoch: 39 | Batch_idx: 220 |  Loss: (0.5066) |  Loss2: (0.0000) | Acc: (82.00%) (23271/28288)
Epoch: 39 | Batch_idx: 230 |  Loss: (0.5053) |  Loss2: (0.0000) | Acc: (82.00%) (24347/29568)
Epoch: 39 | Batch_idx: 240 |  Loss: (0.5031) |  Loss2: (0.0000) | Acc: (82.00%) (25423/30848)
Epoch: 39 | Batch_idx: 250 |  Loss: (0.5041) |  Loss2: (0.0000) | Acc: (82.00%) (26469/32128)
Epoch: 39 | Batch_idx: 260 |  Loss: (0.5028) |  Loss2: (0.0000) | Acc: (82.00%) (27537/33408)
Epoch: 39 | Batch_idx: 270 |  Loss: (0.5031) |  Loss2: (0.0000) | Acc: (82.00%) (28583/34688)
Epoch: 39 | Batch_idx: 280 |  Loss: (0.5026) |  Loss2: (0.0000) | Acc: (82.00%) (29634/35968)
Epoch: 39 | Batch_idx: 290 |  Loss: (0.5013) |  Loss2: (0.0000) | Acc: (82.00%) (30713/37248)
Epoch: 39 | Batch_idx: 300 |  Loss: (0.5001) |  Loss2: (0.0000) | Acc: (82.00%) (31764/38528)
Epoch: 39 | Batch_idx: 310 |  Loss: (0.4992) |  Loss2: (0.0000) | Acc: (82.00%) (32832/39808)
Epoch: 39 | Batch_idx: 320 |  Loss: (0.4985) |  Loss2: (0.0000) | Acc: (82.00%) (33896/41088)
Epoch: 39 | Batch_idx: 330 |  Loss: (0.4978) |  Loss2: (0.0000) | Acc: (82.00%) (34964/42368)
Epoch: 39 | Batch_idx: 340 |  Loss: (0.4979) |  Loss2: (0.0000) | Acc: (82.00%) (36026/43648)
Epoch: 39 | Batch_idx: 350 |  Loss: (0.4986) |  Loss2: (0.0000) | Acc: (82.00%) (37070/44928)
Epoch: 39 | Batch_idx: 360 |  Loss: (0.4985) |  Loss2: (0.0000) | Acc: (82.00%) (38126/46208)
Epoch: 39 | Batch_idx: 370 |  Loss: (0.4982) |  Loss2: (0.0000) | Acc: (82.00%) (39198/47488)
Epoch: 39 | Batch_idx: 380 |  Loss: (0.4981) |  Loss2: (0.0000) | Acc: (82.00%) (40261/48768)
Epoch: 39 | Batch_idx: 390 |  Loss: (0.4985) |  Loss2: (0.0000) | Acc: (82.00%) (41272/50000)
# TEST : Loss: (0.5416) | Acc: (81.00%) (8136/10000)
percent tensor([0.5192, 0.5287, 0.5218, 0.5095, 0.5256, 0.5317, 0.5299, 0.5171, 0.5177,
        0.5253, 0.5244, 0.5244, 0.5175, 0.5139, 0.5291, 0.5194],
       device='cuda:0') torch.Size([16])
percent tensor([0.4967, 0.4908, 0.4982, 0.5025, 0.4968, 0.5051, 0.4922, 0.4967, 0.4936,
        0.4907, 0.4914, 0.4911, 0.4917, 0.4951, 0.4965, 0.4997],
       device='cuda:0') torch.Size([16])
percent tensor([0.5201, 0.5287, 0.4007, 0.4128, 0.4114, 0.4859, 0.5008, 0.4397, 0.4710,
        0.4858, 0.5223, 0.4283, 0.5041, 0.5235, 0.5112, 0.5074],
       device='cuda:0') torch.Size([16])
percent tensor([0.5342, 0.4470, 0.5223, 0.5526, 0.5335, 0.5945, 0.4832, 0.5384, 0.4911,
        0.4407, 0.4637, 0.4515, 0.4323, 0.5269, 0.5165, 0.5595],
       device='cuda:0') torch.Size([16])
percent tensor([0.5124, 0.5266, 0.5203, 0.5173, 0.5012, 0.4862, 0.5240, 0.5137, 0.5412,
        0.5219, 0.5295, 0.5431, 0.5341, 0.5490, 0.5083, 0.4984],
       device='cuda:0') torch.Size([16])
percent tensor([0.6140, 0.5873, 0.6716, 0.7118, 0.6760, 0.6422, 0.6301, 0.6497, 0.6513,
        0.5808, 0.6184, 0.6755, 0.5946, 0.6940, 0.6198, 0.6141],
       device='cuda:0') torch.Size([16])
percent tensor([0.6305, 0.6065, 0.5759, 0.5569, 0.5881, 0.6062, 0.6051, 0.5372, 0.6089,
        0.6256, 0.6167, 0.5551, 0.6201, 0.6354, 0.5345, 0.6078],
       device='cuda:0') torch.Size([16])
percent tensor([0.9996, 0.9984, 0.9983, 0.9987, 0.9988, 0.9973, 0.9992, 0.9992, 0.9988,
        0.9990, 0.9984, 0.9987, 0.9991, 0.9987, 0.9979, 0.9989],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 40 | Batch_idx: 0 |  Loss: (0.5334) |  Loss2: (0.0000) | Acc: (82.00%) (106/128)
Epoch: 40 | Batch_idx: 10 |  Loss: (0.4838) |  Loss2: (0.0000) | Acc: (83.00%) (1176/1408)
Epoch: 40 | Batch_idx: 20 |  Loss: (0.5065) |  Loss2: (0.0000) | Acc: (82.00%) (2223/2688)
Epoch: 40 | Batch_idx: 30 |  Loss: (0.4892) |  Loss2: (0.0000) | Acc: (82.00%) (3290/3968)
Epoch: 40 | Batch_idx: 40 |  Loss: (0.4839) |  Loss2: (0.0000) | Acc: (83.00%) (4367/5248)
Epoch: 40 | Batch_idx: 50 |  Loss: (0.4922) |  Loss2: (0.0000) | Acc: (83.00%) (5426/6528)
Epoch: 40 | Batch_idx: 60 |  Loss: (0.4958) |  Loss2: (0.0000) | Acc: (82.00%) (6470/7808)
Epoch: 40 | Batch_idx: 70 |  Loss: (0.4923) |  Loss2: (0.0000) | Acc: (83.00%) (7544/9088)
Epoch: 40 | Batch_idx: 80 |  Loss: (0.4920) |  Loss2: (0.0000) | Acc: (83.00%) (8611/10368)
Epoch: 40 | Batch_idx: 90 |  Loss: (0.4899) |  Loss2: (0.0000) | Acc: (83.00%) (9686/11648)
Epoch: 40 | Batch_idx: 100 |  Loss: (0.4903) |  Loss2: (0.0000) | Acc: (83.00%) (10750/12928)
Epoch: 40 | Batch_idx: 110 |  Loss: (0.4945) |  Loss2: (0.0000) | Acc: (83.00%) (11795/14208)
Epoch: 40 | Batch_idx: 120 |  Loss: (0.4959) |  Loss2: (0.0000) | Acc: (82.00%) (12836/15488)
Epoch: 40 | Batch_idx: 130 |  Loss: (0.4928) |  Loss2: (0.0000) | Acc: (83.00%) (13921/16768)
Epoch: 40 | Batch_idx: 140 |  Loss: (0.4924) |  Loss2: (0.0000) | Acc: (82.00%) (14973/18048)
Epoch: 40 | Batch_idx: 150 |  Loss: (0.4901) |  Loss2: (0.0000) | Acc: (83.00%) (16049/19328)
Epoch: 40 | Batch_idx: 160 |  Loss: (0.4901) |  Loss2: (0.0000) | Acc: (82.00%) (17101/20608)
Epoch: 40 | Batch_idx: 170 |  Loss: (0.4879) |  Loss2: (0.0000) | Acc: (83.00%) (18186/21888)
Epoch: 40 | Batch_idx: 180 |  Loss: (0.4883) |  Loss2: (0.0000) | Acc: (83.00%) (19247/23168)
Epoch: 40 | Batch_idx: 190 |  Loss: (0.4893) |  Loss2: (0.0000) | Acc: (83.00%) (20295/24448)
Epoch: 40 | Batch_idx: 200 |  Loss: (0.4890) |  Loss2: (0.0000) | Acc: (83.00%) (21355/25728)
Epoch: 40 | Batch_idx: 210 |  Loss: (0.4888) |  Loss2: (0.0000) | Acc: (83.00%) (22422/27008)
Epoch: 40 | Batch_idx: 220 |  Loss: (0.4894) |  Loss2: (0.0000) | Acc: (83.00%) (23495/28288)
Epoch: 40 | Batch_idx: 230 |  Loss: (0.4888) |  Loss2: (0.0000) | Acc: (83.00%) (24562/29568)
Epoch: 40 | Batch_idx: 240 |  Loss: (0.4881) |  Loss2: (0.0000) | Acc: (83.00%) (25637/30848)
Epoch: 40 | Batch_idx: 250 |  Loss: (0.4853) |  Loss2: (0.0000) | Acc: (83.00%) (26745/32128)
Epoch: 40 | Batch_idx: 260 |  Loss: (0.4837) |  Loss2: (0.0000) | Acc: (83.00%) (27826/33408)
Epoch: 40 | Batch_idx: 270 |  Loss: (0.4811) |  Loss2: (0.0000) | Acc: (83.00%) (28925/34688)
Epoch: 40 | Batch_idx: 280 |  Loss: (0.4810) |  Loss2: (0.0000) | Acc: (83.00%) (30001/35968)
Epoch: 40 | Batch_idx: 290 |  Loss: (0.4811) |  Loss2: (0.0000) | Acc: (83.00%) (31068/37248)
Epoch: 40 | Batch_idx: 300 |  Loss: (0.4806) |  Loss2: (0.0000) | Acc: (83.00%) (32148/38528)
Epoch: 40 | Batch_idx: 310 |  Loss: (0.4800) |  Loss2: (0.0000) | Acc: (83.00%) (33216/39808)
Epoch: 40 | Batch_idx: 320 |  Loss: (0.4786) |  Loss2: (0.0000) | Acc: (83.00%) (34297/41088)
Epoch: 40 | Batch_idx: 330 |  Loss: (0.4805) |  Loss2: (0.0000) | Acc: (83.00%) (35340/42368)
Epoch: 40 | Batch_idx: 340 |  Loss: (0.4793) |  Loss2: (0.0000) | Acc: (83.00%) (36425/43648)
Epoch: 40 | Batch_idx: 350 |  Loss: (0.4794) |  Loss2: (0.0000) | Acc: (83.00%) (37506/44928)
Epoch: 40 | Batch_idx: 360 |  Loss: (0.4786) |  Loss2: (0.0000) | Acc: (83.00%) (38597/46208)
Epoch: 40 | Batch_idx: 370 |  Loss: (0.4790) |  Loss2: (0.0000) | Acc: (83.00%) (39665/47488)
Epoch: 40 | Batch_idx: 380 |  Loss: (0.4778) |  Loss2: (0.0000) | Acc: (83.00%) (40760/48768)
Epoch: 40 | Batch_idx: 390 |  Loss: (0.4771) |  Loss2: (0.0000) | Acc: (83.00%) (41794/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_040.pth.tar'
# TEST : Loss: (0.5665) | Acc: (80.00%) (8079/10000)
percent tensor([0.5185, 0.5277, 0.5215, 0.5091, 0.5256, 0.5315, 0.5293, 0.5173, 0.5159,
        0.5254, 0.5235, 0.5246, 0.5169, 0.5114, 0.5289, 0.5185],
       device='cuda:0') torch.Size([16])
percent tensor([0.4971, 0.4911, 0.4984, 0.5028, 0.4971, 0.5056, 0.4934, 0.4970, 0.4941,
        0.4914, 0.4915, 0.4922, 0.4922, 0.4958, 0.4967, 0.4998],
       device='cuda:0') torch.Size([16])
percent tensor([0.5244, 0.5251, 0.4349, 0.4257, 0.4347, 0.4846, 0.5057, 0.4569, 0.4744,
        0.5044, 0.5236, 0.4620, 0.5106, 0.5123, 0.5104, 0.5062],
       device='cuda:0') torch.Size([16])
percent tensor([0.5494, 0.4523, 0.5410, 0.5595, 0.5471, 0.5912, 0.5006, 0.5507, 0.5088,
        0.4542, 0.4797, 0.4715, 0.4446, 0.5321, 0.5140, 0.5642],
       device='cuda:0') torch.Size([16])
percent tensor([0.5145, 0.5252, 0.5169, 0.5224, 0.4997, 0.4979, 0.5217, 0.5107, 0.5389,
        0.5164, 0.5324, 0.5411, 0.5361, 0.5492, 0.5163, 0.5024],
       device='cuda:0') torch.Size([16])
percent tensor([0.6263, 0.5939, 0.6607, 0.7146, 0.6704, 0.6497, 0.6363, 0.6469, 0.6561,
        0.5809, 0.6355, 0.6786, 0.6071, 0.7088, 0.6284, 0.6252],
       device='cuda:0') torch.Size([16])
percent tensor([0.6302, 0.6235, 0.5800, 0.5651, 0.5898, 0.5964, 0.6161, 0.5415, 0.6163,
        0.6350, 0.6282, 0.5671, 0.6242, 0.6496, 0.5366, 0.6070],
       device='cuda:0') torch.Size([16])
percent tensor([0.9995, 0.9979, 0.9990, 0.9990, 0.9986, 0.9963, 0.9990, 0.9993, 0.9989,
        0.9987, 0.9990, 0.9991, 0.9990, 0.9991, 0.9982, 0.9989],
       device='cuda:0') torch.Size([16])
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(173.3784, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(793.8259, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(787.6420, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1526.3325, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(502.4445, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2191.6284, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4287.6079, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1429.2827, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6106.8276, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12094.3916, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4030.8752, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(17053.0449, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 41 | Batch_idx: 0 |  Loss: (0.4484) |  Loss2: (0.0000) | Acc: (84.00%) (108/128)
Epoch: 41 | Batch_idx: 10 |  Loss: (0.4641) |  Loss2: (0.0000) | Acc: (83.00%) (1182/1408)
Epoch: 41 | Batch_idx: 20 |  Loss: (0.4559) |  Loss2: (0.0000) | Acc: (84.00%) (2264/2688)
Epoch: 41 | Batch_idx: 30 |  Loss: (0.4473) |  Loss2: (0.0000) | Acc: (84.00%) (3344/3968)
Epoch: 41 | Batch_idx: 40 |  Loss: (0.4459) |  Loss2: (0.0000) | Acc: (84.00%) (4444/5248)
Epoch: 41 | Batch_idx: 50 |  Loss: (0.4568) |  Loss2: (0.0000) | Acc: (84.00%) (5505/6528)
Epoch: 41 | Batch_idx: 60 |  Loss: (0.4596) |  Loss2: (0.0000) | Acc: (83.00%) (6558/7808)
Epoch: 41 | Batch_idx: 70 |  Loss: (0.4614) |  Loss2: (0.0000) | Acc: (83.00%) (7622/9088)
Epoch: 41 | Batch_idx: 80 |  Loss: (0.4591) |  Loss2: (0.0000) | Acc: (83.00%) (8705/10368)
Epoch: 41 | Batch_idx: 90 |  Loss: (0.4607) |  Loss2: (0.0000) | Acc: (83.00%) (9776/11648)
Epoch: 41 | Batch_idx: 100 |  Loss: (0.4609) |  Loss2: (0.0000) | Acc: (83.00%) (10850/12928)
Epoch: 41 | Batch_idx: 110 |  Loss: (0.4609) |  Loss2: (0.0000) | Acc: (83.00%) (11923/14208)
Epoch: 41 | Batch_idx: 120 |  Loss: (0.4624) |  Loss2: (0.0000) | Acc: (83.00%) (12984/15488)
Epoch: 41 | Batch_idx: 130 |  Loss: (0.4631) |  Loss2: (0.0000) | Acc: (83.00%) (14050/16768)
Epoch: 41 | Batch_idx: 140 |  Loss: (0.4656) |  Loss2: (0.0000) | Acc: (83.00%) (15100/18048)
Epoch: 41 | Batch_idx: 150 |  Loss: (0.4650) |  Loss2: (0.0000) | Acc: (83.00%) (16170/19328)
Epoch: 41 | Batch_idx: 160 |  Loss: (0.4626) |  Loss2: (0.0000) | Acc: (83.00%) (17268/20608)
Epoch: 41 | Batch_idx: 170 |  Loss: (0.4617) |  Loss2: (0.0000) | Acc: (83.00%) (18357/21888)
Epoch: 41 | Batch_idx: 180 |  Loss: (0.4614) |  Loss2: (0.0000) | Acc: (83.00%) (19426/23168)
Epoch: 41 | Batch_idx: 190 |  Loss: (0.4592) |  Loss2: (0.0000) | Acc: (83.00%) (20522/24448)
Epoch: 41 | Batch_idx: 200 |  Loss: (0.4568) |  Loss2: (0.0000) | Acc: (84.00%) (21620/25728)
Epoch: 41 | Batch_idx: 210 |  Loss: (0.4569) |  Loss2: (0.0000) | Acc: (84.00%) (22703/27008)
Epoch: 41 | Batch_idx: 220 |  Loss: (0.4567) |  Loss2: (0.0000) | Acc: (84.00%) (23788/28288)
Epoch: 41 | Batch_idx: 230 |  Loss: (0.4576) |  Loss2: (0.0000) | Acc: (84.00%) (24848/29568)
Epoch: 41 | Batch_idx: 240 |  Loss: (0.4596) |  Loss2: (0.0000) | Acc: (83.00%) (25907/30848)
Epoch: 41 | Batch_idx: 250 |  Loss: (0.4596) |  Loss2: (0.0000) | Acc: (84.00%) (26990/32128)
Epoch: 41 | Batch_idx: 260 |  Loss: (0.4601) |  Loss2: (0.0000) | Acc: (84.00%) (28069/33408)
Epoch: 41 | Batch_idx: 270 |  Loss: (0.4592) |  Loss2: (0.0000) | Acc: (84.00%) (29143/34688)
Epoch: 41 | Batch_idx: 280 |  Loss: (0.4592) |  Loss2: (0.0000) | Acc: (83.00%) (30212/35968)
Epoch: 41 | Batch_idx: 290 |  Loss: (0.4594) |  Loss2: (0.0000) | Acc: (84.00%) (31300/37248)
Epoch: 41 | Batch_idx: 300 |  Loss: (0.4586) |  Loss2: (0.0000) | Acc: (84.00%) (32396/38528)
Epoch: 41 | Batch_idx: 310 |  Loss: (0.4589) |  Loss2: (0.0000) | Acc: (84.00%) (33465/39808)
Epoch: 41 | Batch_idx: 320 |  Loss: (0.4589) |  Loss2: (0.0000) | Acc: (84.00%) (34537/41088)
Epoch: 41 | Batch_idx: 330 |  Loss: (0.4579) |  Loss2: (0.0000) | Acc: (84.00%) (35637/42368)
Epoch: 41 | Batch_idx: 340 |  Loss: (0.4582) |  Loss2: (0.0000) | Acc: (84.00%) (36702/43648)
Epoch: 41 | Batch_idx: 350 |  Loss: (0.4593) |  Loss2: (0.0000) | Acc: (84.00%) (37755/44928)
Epoch: 41 | Batch_idx: 360 |  Loss: (0.4588) |  Loss2: (0.0000) | Acc: (84.00%) (38845/46208)
Epoch: 41 | Batch_idx: 370 |  Loss: (0.4602) |  Loss2: (0.0000) | Acc: (84.00%) (39892/47488)
Epoch: 41 | Batch_idx: 380 |  Loss: (0.4592) |  Loss2: (0.0000) | Acc: (84.00%) (40973/48768)
Epoch: 41 | Batch_idx: 390 |  Loss: (0.4589) |  Loss2: (0.0000) | Acc: (84.00%) (42021/50000)
# TEST : Loss: (0.6074) | Acc: (79.00%) (7942/10000)
percent tensor([0.5196, 0.5305, 0.5236, 0.5095, 0.5269, 0.5312, 0.5319, 0.5185, 0.5188,
        0.5269, 0.5249, 0.5273, 0.5182, 0.5161, 0.5295, 0.5198],
       device='cuda:0') torch.Size([16])
percent tensor([0.4974, 0.4910, 0.4979, 0.5019, 0.4966, 0.5049, 0.4932, 0.4960, 0.4937,
        0.4909, 0.4917, 0.4918, 0.4923, 0.4950, 0.4961, 0.4996],
       device='cuda:0') torch.Size([16])
percent tensor([0.5247, 0.5280, 0.4212, 0.4297, 0.4218, 0.4918, 0.5022, 0.4527, 0.4707,
        0.4969, 0.5266, 0.4421, 0.5089, 0.5226, 0.5136, 0.5091],
       device='cuda:0') torch.Size([16])
percent tensor([0.5450, 0.4485, 0.5327, 0.5499, 0.5396, 0.5875, 0.4898, 0.5373, 0.4961,
        0.4455, 0.4742, 0.4640, 0.4438, 0.5184, 0.5076, 0.5612],
       device='cuda:0') torch.Size([16])
percent tensor([0.5120, 0.5241, 0.5214, 0.5204, 0.4974, 0.4906, 0.5247, 0.5114, 0.5405,
        0.5184, 0.5291, 0.5427, 0.5361, 0.5500, 0.5120, 0.5002],
       device='cuda:0') torch.Size([16])
percent tensor([0.6220, 0.5965, 0.6701, 0.7116, 0.6781, 0.6362, 0.6381, 0.6370, 0.6539,
        0.5870, 0.6277, 0.6752, 0.6065, 0.6933, 0.6247, 0.6271],
       device='cuda:0') torch.Size([16])
percent tensor([0.6196, 0.6119, 0.5817, 0.5646, 0.5915, 0.5909, 0.6026, 0.5480, 0.6080,
        0.6225, 0.6220, 0.5584, 0.6155, 0.6382, 0.5310, 0.6011],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9974, 0.9991, 0.9996, 0.9994, 0.9973, 0.9993, 0.9993, 0.9992,
        0.9986, 0.9986, 0.9992, 0.9994, 0.9989, 0.9984, 0.9994],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 42 | Batch_idx: 0 |  Loss: (0.4664) |  Loss2: (0.0000) | Acc: (80.00%) (103/128)
Epoch: 42 | Batch_idx: 10 |  Loss: (0.4680) |  Loss2: (0.0000) | Acc: (83.00%) (1174/1408)
Epoch: 42 | Batch_idx: 20 |  Loss: (0.4864) |  Loss2: (0.0000) | Acc: (83.00%) (2239/2688)
Epoch: 42 | Batch_idx: 30 |  Loss: (0.5046) |  Loss2: (0.0000) | Acc: (82.00%) (3275/3968)
Epoch: 42 | Batch_idx: 40 |  Loss: (0.5037) |  Loss2: (0.0000) | Acc: (82.00%) (4315/5248)
Epoch: 42 | Batch_idx: 50 |  Loss: (0.5099) |  Loss2: (0.0000) | Acc: (82.00%) (5353/6528)
Epoch: 42 | Batch_idx: 60 |  Loss: (0.5119) |  Loss2: (0.0000) | Acc: (81.00%) (6386/7808)
Epoch: 42 | Batch_idx: 70 |  Loss: (0.5146) |  Loss2: (0.0000) | Acc: (81.00%) (7445/9088)
Epoch: 42 | Batch_idx: 80 |  Loss: (0.5153) |  Loss2: (0.0000) | Acc: (81.00%) (8486/10368)
Epoch: 42 | Batch_idx: 90 |  Loss: (0.5149) |  Loss2: (0.0000) | Acc: (81.00%) (9535/11648)
Epoch: 42 | Batch_idx: 100 |  Loss: (0.5127) |  Loss2: (0.0000) | Acc: (81.00%) (10590/12928)
Epoch: 42 | Batch_idx: 110 |  Loss: (0.5156) |  Loss2: (0.0000) | Acc: (81.00%) (11624/14208)
Epoch: 42 | Batch_idx: 120 |  Loss: (0.5115) |  Loss2: (0.0000) | Acc: (82.00%) (12702/15488)
Epoch: 42 | Batch_idx: 130 |  Loss: (0.5099) |  Loss2: (0.0000) | Acc: (82.00%) (13766/16768)
Epoch: 42 | Batch_idx: 140 |  Loss: (0.5111) |  Loss2: (0.0000) | Acc: (82.00%) (14809/18048)
Epoch: 42 | Batch_idx: 150 |  Loss: (0.5124) |  Loss2: (0.0000) | Acc: (82.00%) (15851/19328)
Epoch: 42 | Batch_idx: 160 |  Loss: (0.5132) |  Loss2: (0.0000) | Acc: (81.00%) (16898/20608)
Epoch: 42 | Batch_idx: 170 |  Loss: (0.5113) |  Loss2: (0.0000) | Acc: (82.00%) (17984/21888)
Epoch: 42 | Batch_idx: 180 |  Loss: (0.5099) |  Loss2: (0.0000) | Acc: (82.00%) (19045/23168)
Epoch: 42 | Batch_idx: 190 |  Loss: (0.5081) |  Loss2: (0.0000) | Acc: (82.00%) (20112/24448)
Epoch: 42 | Batch_idx: 200 |  Loss: (0.5086) |  Loss2: (0.0000) | Acc: (82.00%) (21168/25728)
Epoch: 42 | Batch_idx: 210 |  Loss: (0.5085) |  Loss2: (0.0000) | Acc: (82.00%) (22207/27008)
Epoch: 42 | Batch_idx: 220 |  Loss: (0.5062) |  Loss2: (0.0000) | Acc: (82.00%) (23284/28288)
Epoch: 42 | Batch_idx: 230 |  Loss: (0.5045) |  Loss2: (0.0000) | Acc: (82.00%) (24353/29568)
Epoch: 42 | Batch_idx: 240 |  Loss: (0.5051) |  Loss2: (0.0000) | Acc: (82.00%) (25394/30848)
Epoch: 42 | Batch_idx: 250 |  Loss: (0.5044) |  Loss2: (0.0000) | Acc: (82.00%) (26456/32128)
Epoch: 42 | Batch_idx: 260 |  Loss: (0.5042) |  Loss2: (0.0000) | Acc: (82.00%) (27519/33408)
Epoch: 42 | Batch_idx: 270 |  Loss: (0.5030) |  Loss2: (0.0000) | Acc: (82.00%) (28576/34688)
Epoch: 42 | Batch_idx: 280 |  Loss: (0.5008) |  Loss2: (0.0000) | Acc: (82.00%) (29653/35968)
Epoch: 42 | Batch_idx: 290 |  Loss: (0.5004) |  Loss2: (0.0000) | Acc: (82.00%) (30728/37248)
Epoch: 42 | Batch_idx: 300 |  Loss: (0.5002) |  Loss2: (0.0000) | Acc: (82.00%) (31787/38528)
Epoch: 42 | Batch_idx: 310 |  Loss: (0.4998) |  Loss2: (0.0000) | Acc: (82.00%) (32859/39808)
Epoch: 42 | Batch_idx: 320 |  Loss: (0.4979) |  Loss2: (0.0000) | Acc: (82.00%) (33950/41088)
Epoch: 42 | Batch_idx: 330 |  Loss: (0.4973) |  Loss2: (0.0000) | Acc: (82.00%) (35014/42368)
Epoch: 42 | Batch_idx: 340 |  Loss: (0.4954) |  Loss2: (0.0000) | Acc: (82.00%) (36104/43648)
Epoch: 42 | Batch_idx: 350 |  Loss: (0.4948) |  Loss2: (0.0000) | Acc: (82.00%) (37168/44928)
Epoch: 42 | Batch_idx: 360 |  Loss: (0.4949) |  Loss2: (0.0000) | Acc: (82.00%) (38220/46208)
Epoch: 42 | Batch_idx: 370 |  Loss: (0.4954) |  Loss2: (0.0000) | Acc: (82.00%) (39272/47488)
Epoch: 42 | Batch_idx: 380 |  Loss: (0.4949) |  Loss2: (0.0000) | Acc: (82.00%) (40348/48768)
Epoch: 42 | Batch_idx: 390 |  Loss: (0.4937) |  Loss2: (0.0000) | Acc: (82.00%) (41395/50000)
# TEST : Loss: (0.5366) | Acc: (81.00%) (8157/10000)
percent tensor([0.5246, 0.5378, 0.5291, 0.5123, 0.5322, 0.5357, 0.5394, 0.5228, 0.5247,
        0.5330, 0.5309, 0.5333, 0.5235, 0.5216, 0.5354, 0.5245],
       device='cuda:0') torch.Size([16])
percent tensor([0.4953, 0.4900, 0.4964, 0.4989, 0.4949, 0.5031, 0.4920, 0.4944, 0.4922,
        0.4899, 0.4900, 0.4907, 0.4907, 0.4933, 0.4944, 0.4976],
       device='cuda:0') torch.Size([16])
percent tensor([0.5414, 0.5422, 0.4277, 0.4376, 0.4282, 0.5112, 0.5142, 0.4589, 0.4758,
        0.5068, 0.5341, 0.4452, 0.5203, 0.5370, 0.5313, 0.5263],
       device='cuda:0') torch.Size([16])
percent tensor([0.5885, 0.4650, 0.5745, 0.6094, 0.5921, 0.6322, 0.5201, 0.5918, 0.5382,
        0.4635, 0.4972, 0.4834, 0.4546, 0.5628, 0.5534, 0.6067],
       device='cuda:0') torch.Size([16])
percent tensor([0.5454, 0.5563, 0.5460, 0.5347, 0.5200, 0.5111, 0.5564, 0.5422, 0.5593,
        0.5530, 0.5561, 0.5719, 0.5669, 0.5634, 0.5495, 0.5364],
       device='cuda:0') torch.Size([16])
percent tensor([0.5920, 0.5689, 0.6195, 0.6769, 0.6286, 0.6287, 0.5960, 0.5918, 0.6233,
        0.5511, 0.5997, 0.6340, 0.5814, 0.6585, 0.5892, 0.5986],
       device='cuda:0') torch.Size([16])
percent tensor([0.6486, 0.6380, 0.5864, 0.5601, 0.5993, 0.6213, 0.6172, 0.5425, 0.6382,
        0.6473, 0.6577, 0.5543, 0.6592, 0.6702, 0.5352, 0.6152],
       device='cuda:0') torch.Size([16])
percent tensor([0.9993, 0.9975, 0.9990, 0.9993, 0.9994, 0.9965, 0.9987, 0.9994, 0.9989,
        0.9984, 0.9985, 0.9991, 0.9994, 0.9985, 0.9984, 0.9988],
       device='cuda:0') torch.Size([16])
Epoch: 43 | Batch_idx: 0 |  Loss: (0.4575) |  Loss2: (0.0000) | Acc: (85.00%) (110/128)
Epoch: 43 | Batch_idx: 10 |  Loss: (0.4952) |  Loss2: (0.0000) | Acc: (82.00%) (1164/1408)
Epoch: 43 | Batch_idx: 20 |  Loss: (0.4634) |  Loss2: (0.0000) | Acc: (84.00%) (2258/2688)
Epoch: 43 | Batch_idx: 30 |  Loss: (0.4692) |  Loss2: (0.0000) | Acc: (83.00%) (3320/3968)
Epoch: 43 | Batch_idx: 40 |  Loss: (0.4821) |  Loss2: (0.0000) | Acc: (83.00%) (4364/5248)
Epoch: 43 | Batch_idx: 50 |  Loss: (0.4779) |  Loss2: (0.0000) | Acc: (83.00%) (5445/6528)
Epoch: 43 | Batch_idx: 60 |  Loss: (0.4758) |  Loss2: (0.0000) | Acc: (83.00%) (6514/7808)
Epoch: 43 | Batch_idx: 70 |  Loss: (0.4766) |  Loss2: (0.0000) | Acc: (83.00%) (7575/9088)
Epoch: 43 | Batch_idx: 80 |  Loss: (0.4724) |  Loss2: (0.0000) | Acc: (83.00%) (8650/10368)
Epoch: 43 | Batch_idx: 90 |  Loss: (0.4710) |  Loss2: (0.0000) | Acc: (83.00%) (9721/11648)
Epoch: 43 | Batch_idx: 100 |  Loss: (0.4722) |  Loss2: (0.0000) | Acc: (83.00%) (10777/12928)
Epoch: 43 | Batch_idx: 110 |  Loss: (0.4702) |  Loss2: (0.0000) | Acc: (83.00%) (11857/14208)
Epoch: 43 | Batch_idx: 120 |  Loss: (0.4678) |  Loss2: (0.0000) | Acc: (83.00%) (12934/15488)
Epoch: 43 | Batch_idx: 130 |  Loss: (0.4673) |  Loss2: (0.0000) | Acc: (83.00%) (14005/16768)
Epoch: 43 | Batch_idx: 140 |  Loss: (0.4654) |  Loss2: (0.0000) | Acc: (83.00%) (15099/18048)
Epoch: 43 | Batch_idx: 150 |  Loss: (0.4657) |  Loss2: (0.0000) | Acc: (83.00%) (16155/19328)
Epoch: 43 | Batch_idx: 160 |  Loss: (0.4652) |  Loss2: (0.0000) | Acc: (83.00%) (17245/20608)
Epoch: 43 | Batch_idx: 170 |  Loss: (0.4672) |  Loss2: (0.0000) | Acc: (83.00%) (18310/21888)
Epoch: 43 | Batch_idx: 180 |  Loss: (0.4682) |  Loss2: (0.0000) | Acc: (83.00%) (19382/23168)
Epoch: 43 | Batch_idx: 190 |  Loss: (0.4674) |  Loss2: (0.0000) | Acc: (83.00%) (20473/24448)
Epoch: 43 | Batch_idx: 200 |  Loss: (0.4668) |  Loss2: (0.0000) | Acc: (83.00%) (21558/25728)
Epoch: 43 | Batch_idx: 210 |  Loss: (0.4659) |  Loss2: (0.0000) | Acc: (83.00%) (22646/27008)
Epoch: 43 | Batch_idx: 220 |  Loss: (0.4661) |  Loss2: (0.0000) | Acc: (83.00%) (23711/28288)
Epoch: 43 | Batch_idx: 230 |  Loss: (0.4650) |  Loss2: (0.0000) | Acc: (83.00%) (24800/29568)
Epoch: 43 | Batch_idx: 240 |  Loss: (0.4634) |  Loss2: (0.0000) | Acc: (83.00%) (25891/30848)
Epoch: 43 | Batch_idx: 250 |  Loss: (0.4628) |  Loss2: (0.0000) | Acc: (83.00%) (26982/32128)
Epoch: 43 | Batch_idx: 260 |  Loss: (0.4631) |  Loss2: (0.0000) | Acc: (83.00%) (28042/33408)
Epoch: 43 | Batch_idx: 270 |  Loss: (0.4619) |  Loss2: (0.0000) | Acc: (83.00%) (29126/34688)
Epoch: 43 | Batch_idx: 280 |  Loss: (0.4608) |  Loss2: (0.0000) | Acc: (84.00%) (30218/35968)
Epoch: 43 | Batch_idx: 290 |  Loss: (0.4607) |  Loss2: (0.0000) | Acc: (84.00%) (31291/37248)
Epoch: 43 | Batch_idx: 300 |  Loss: (0.4610) |  Loss2: (0.0000) | Acc: (84.00%) (32367/38528)
Epoch: 43 | Batch_idx: 310 |  Loss: (0.4603) |  Loss2: (0.0000) | Acc: (84.00%) (33450/39808)
Epoch: 43 | Batch_idx: 320 |  Loss: (0.4614) |  Loss2: (0.0000) | Acc: (83.00%) (34506/41088)
Epoch: 43 | Batch_idx: 330 |  Loss: (0.4619) |  Loss2: (0.0000) | Acc: (83.00%) (35566/42368)
Epoch: 43 | Batch_idx: 340 |  Loss: (0.4609) |  Loss2: (0.0000) | Acc: (84.00%) (36666/43648)
Epoch: 43 | Batch_idx: 350 |  Loss: (0.4604) |  Loss2: (0.0000) | Acc: (84.00%) (37763/44928)
Epoch: 43 | Batch_idx: 360 |  Loss: (0.4610) |  Loss2: (0.0000) | Acc: (84.00%) (38826/46208)
Epoch: 43 | Batch_idx: 370 |  Loss: (0.4609) |  Loss2: (0.0000) | Acc: (84.00%) (39904/47488)
Epoch: 43 | Batch_idx: 380 |  Loss: (0.4612) |  Loss2: (0.0000) | Acc: (84.00%) (40978/48768)
Epoch: 43 | Batch_idx: 390 |  Loss: (0.4621) |  Loss2: (0.0000) | Acc: (84.00%) (42001/50000)
# TEST : Loss: (0.5168) | Acc: (82.00%) (8225/10000)
percent tensor([0.5232, 0.5356, 0.5268, 0.5104, 0.5298, 0.5341, 0.5372, 0.5201, 0.5230,
        0.5307, 0.5295, 0.5309, 0.5220, 0.5189, 0.5334, 0.5225],
       device='cuda:0') torch.Size([16])
percent tensor([0.4953, 0.4907, 0.4966, 0.4986, 0.4952, 0.5028, 0.4927, 0.4946, 0.4923,
        0.4906, 0.4902, 0.4913, 0.4911, 0.4935, 0.4947, 0.4976],
       device='cuda:0') torch.Size([16])
percent tensor([0.5445, 0.5449, 0.4202, 0.4295, 0.4191, 0.5146, 0.5138, 0.4520, 0.4718,
        0.5088, 0.5367, 0.4407, 0.5259, 0.5379, 0.5350, 0.5306],
       device='cuda:0') torch.Size([16])
percent tensor([0.5871, 0.4588, 0.5825, 0.6147, 0.5982, 0.6359, 0.5205, 0.5945, 0.5458,
        0.4613, 0.4973, 0.4874, 0.4501, 0.5669, 0.5490, 0.6035],
       device='cuda:0') torch.Size([16])
percent tensor([0.5604, 0.5689, 0.5557, 0.5447, 0.5301, 0.5250, 0.5704, 0.5572, 0.5683,
        0.5653, 0.5666, 0.5814, 0.5778, 0.5721, 0.5645, 0.5526],
       device='cuda:0') torch.Size([16])
percent tensor([0.5860, 0.5682, 0.6172, 0.6707, 0.6254, 0.6294, 0.5930, 0.5859, 0.6222,
        0.5483, 0.5982, 0.6316, 0.5807, 0.6552, 0.5818, 0.5950],
       device='cuda:0') torch.Size([16])
percent tensor([0.6655, 0.6542, 0.5924, 0.5637, 0.6025, 0.6470, 0.6284, 0.5341, 0.6522,
        0.6597, 0.6775, 0.5623, 0.6821, 0.6927, 0.5371, 0.6275],
       device='cuda:0') torch.Size([16])
percent tensor([0.9994, 0.9975, 0.9990, 0.9994, 0.9995, 0.9971, 0.9989, 0.9994, 0.9989,
        0.9986, 0.9986, 0.9992, 0.9994, 0.9987, 0.9984, 0.9990],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 44 | Batch_idx: 0 |  Loss: (0.3783) |  Loss2: (0.0000) | Acc: (87.00%) (112/128)
Epoch: 44 | Batch_idx: 10 |  Loss: (0.4403) |  Loss2: (0.0000) | Acc: (84.00%) (1190/1408)
Epoch: 44 | Batch_idx: 20 |  Loss: (0.4497) |  Loss2: (0.0000) | Acc: (84.00%) (2280/2688)
Epoch: 44 | Batch_idx: 30 |  Loss: (0.4505) |  Loss2: (0.0000) | Acc: (84.00%) (3366/3968)
Epoch: 44 | Batch_idx: 40 |  Loss: (0.4590) |  Loss2: (0.0000) | Acc: (84.00%) (4424/5248)
Epoch: 44 | Batch_idx: 50 |  Loss: (0.4558) |  Loss2: (0.0000) | Acc: (84.00%) (5496/6528)
Epoch: 44 | Batch_idx: 60 |  Loss: (0.4540) |  Loss2: (0.0000) | Acc: (84.00%) (6577/7808)
Epoch: 44 | Batch_idx: 70 |  Loss: (0.4506) |  Loss2: (0.0000) | Acc: (84.00%) (7671/9088)
Epoch: 44 | Batch_idx: 80 |  Loss: (0.4528) |  Loss2: (0.0000) | Acc: (84.00%) (8754/10368)
Epoch: 44 | Batch_idx: 90 |  Loss: (0.4535) |  Loss2: (0.0000) | Acc: (84.00%) (9824/11648)
Epoch: 44 | Batch_idx: 100 |  Loss: (0.4548) |  Loss2: (0.0000) | Acc: (84.00%) (10906/12928)
Epoch: 44 | Batch_idx: 110 |  Loss: (0.4575) |  Loss2: (0.0000) | Acc: (84.00%) (11958/14208)
Epoch: 44 | Batch_idx: 120 |  Loss: (0.4565) |  Loss2: (0.0000) | Acc: (84.00%) (13028/15488)
Epoch: 44 | Batch_idx: 130 |  Loss: (0.4534) |  Loss2: (0.0000) | Acc: (84.00%) (14101/16768)
Epoch: 44 | Batch_idx: 140 |  Loss: (0.4543) |  Loss2: (0.0000) | Acc: (84.00%) (15183/18048)
Epoch: 44 | Batch_idx: 150 |  Loss: (0.4551) |  Loss2: (0.0000) | Acc: (84.00%) (16261/19328)
Epoch: 44 | Batch_idx: 160 |  Loss: (0.4523) |  Loss2: (0.0000) | Acc: (84.00%) (17347/20608)
Epoch: 44 | Batch_idx: 170 |  Loss: (0.4529) |  Loss2: (0.0000) | Acc: (84.00%) (18425/21888)
Epoch: 44 | Batch_idx: 180 |  Loss: (0.4544) |  Loss2: (0.0000) | Acc: (84.00%) (19499/23168)
Epoch: 44 | Batch_idx: 190 |  Loss: (0.4532) |  Loss2: (0.0000) | Acc: (84.00%) (20581/24448)
Epoch: 44 | Batch_idx: 200 |  Loss: (0.4531) |  Loss2: (0.0000) | Acc: (84.00%) (21653/25728)
Epoch: 44 | Batch_idx: 210 |  Loss: (0.4535) |  Loss2: (0.0000) | Acc: (84.00%) (22718/27008)
Epoch: 44 | Batch_idx: 220 |  Loss: (0.4532) |  Loss2: (0.0000) | Acc: (84.00%) (23796/28288)
Epoch: 44 | Batch_idx: 230 |  Loss: (0.4538) |  Loss2: (0.0000) | Acc: (84.00%) (24863/29568)
Epoch: 44 | Batch_idx: 240 |  Loss: (0.4543) |  Loss2: (0.0000) | Acc: (84.00%) (25939/30848)
Epoch: 44 | Batch_idx: 250 |  Loss: (0.4547) |  Loss2: (0.0000) | Acc: (84.00%) (27012/32128)
Epoch: 44 | Batch_idx: 260 |  Loss: (0.4545) |  Loss2: (0.0000) | Acc: (84.00%) (28097/33408)
Epoch: 44 | Batch_idx: 270 |  Loss: (0.4540) |  Loss2: (0.0000) | Acc: (84.00%) (29182/34688)
Epoch: 44 | Batch_idx: 280 |  Loss: (0.4529) |  Loss2: (0.0000) | Acc: (84.00%) (30273/35968)
Epoch: 44 | Batch_idx: 290 |  Loss: (0.4517) |  Loss2: (0.0000) | Acc: (84.00%) (31362/37248)
Epoch: 44 | Batch_idx: 300 |  Loss: (0.4496) |  Loss2: (0.0000) | Acc: (84.00%) (32475/38528)
Epoch: 44 | Batch_idx: 310 |  Loss: (0.4503) |  Loss2: (0.0000) | Acc: (84.00%) (33539/39808)
Epoch: 44 | Batch_idx: 320 |  Loss: (0.4496) |  Loss2: (0.0000) | Acc: (84.00%) (34617/41088)
Epoch: 44 | Batch_idx: 330 |  Loss: (0.4491) |  Loss2: (0.0000) | Acc: (84.00%) (35703/42368)
Epoch: 44 | Batch_idx: 340 |  Loss: (0.4478) |  Loss2: (0.0000) | Acc: (84.00%) (36799/43648)
Epoch: 44 | Batch_idx: 350 |  Loss: (0.4473) |  Loss2: (0.0000) | Acc: (84.00%) (37894/44928)
Epoch: 44 | Batch_idx: 360 |  Loss: (0.4479) |  Loss2: (0.0000) | Acc: (84.00%) (38958/46208)
Epoch: 44 | Batch_idx: 370 |  Loss: (0.4476) |  Loss2: (0.0000) | Acc: (84.00%) (40052/47488)
Epoch: 44 | Batch_idx: 380 |  Loss: (0.4473) |  Loss2: (0.0000) | Acc: (84.00%) (41137/48768)
Epoch: 44 | Batch_idx: 390 |  Loss: (0.4467) |  Loss2: (0.0000) | Acc: (84.00%) (42200/50000)
# TEST : Loss: (0.5651) | Acc: (80.00%) (8070/10000)
percent tensor([0.5232, 0.5359, 0.5246, 0.5100, 0.5292, 0.5360, 0.5371, 0.5197, 0.5226,
        0.5311, 0.5297, 0.5294, 0.5225, 0.5183, 0.5349, 0.5232],
       device='cuda:0') torch.Size([16])
percent tensor([0.4957, 0.4909, 0.4978, 0.4994, 0.4965, 0.5033, 0.4934, 0.4957, 0.4929,
        0.4913, 0.4904, 0.4926, 0.4916, 0.4937, 0.4953, 0.4979],
       device='cuda:0') torch.Size([16])
percent tensor([0.5387, 0.5420, 0.4175, 0.4129, 0.4128, 0.5043, 0.5101, 0.4495, 0.4716,
        0.5105, 0.5393, 0.4449, 0.5263, 0.5184, 0.5314, 0.5238],
       device='cuda:0') torch.Size([16])
percent tensor([0.5879, 0.4653, 0.5931, 0.6142, 0.6057, 0.6280, 0.5401, 0.6129, 0.5618,
        0.4738, 0.5093, 0.5123, 0.4604, 0.5899, 0.5484, 0.5979],
       device='cuda:0') torch.Size([16])
percent tensor([0.5644, 0.5691, 0.5437, 0.5429, 0.5212, 0.5319, 0.5629, 0.5434, 0.5645,
        0.5620, 0.5676, 0.5699, 0.5821, 0.5721, 0.5624, 0.5565],
       device='cuda:0') torch.Size([16])
percent tensor([0.5877, 0.5585, 0.6324, 0.6731, 0.6390, 0.6159, 0.5966, 0.5903, 0.6196,
        0.5398, 0.6039, 0.6471, 0.5806, 0.6686, 0.5849, 0.5801],
       device='cuda:0') torch.Size([16])
percent tensor([0.6603, 0.6457, 0.5876, 0.5733, 0.6012, 0.6415, 0.6267, 0.5195, 0.6583,
        0.6570, 0.6716, 0.5692, 0.6596, 0.6995, 0.5378, 0.6231],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9987, 0.9990, 0.9993, 0.9992, 0.9973, 0.9992, 0.9996, 0.9992,
        0.9990, 0.9992, 0.9991, 0.9993, 0.9997, 0.9990, 0.9992],
       device='cuda:0') torch.Size([16])
Epoch: 45 | Batch_idx: 0 |  Loss: (0.5118) |  Loss2: (0.0000) | Acc: (80.00%) (103/128)
Epoch: 45 | Batch_idx: 10 |  Loss: (0.4137) |  Loss2: (0.0000) | Acc: (85.00%) (1206/1408)
Epoch: 45 | Batch_idx: 20 |  Loss: (0.4218) |  Loss2: (0.0000) | Acc: (85.00%) (2309/2688)
Epoch: 45 | Batch_idx: 30 |  Loss: (0.4329) |  Loss2: (0.0000) | Acc: (85.00%) (3398/3968)
Epoch: 45 | Batch_idx: 40 |  Loss: (0.4374) |  Loss2: (0.0000) | Acc: (85.00%) (4472/5248)
Epoch: 45 | Batch_idx: 50 |  Loss: (0.4423) |  Loss2: (0.0000) | Acc: (84.00%) (5546/6528)
Epoch: 45 | Batch_idx: 60 |  Loss: (0.4392) |  Loss2: (0.0000) | Acc: (85.00%) (6644/7808)
Epoch: 45 | Batch_idx: 70 |  Loss: (0.4352) |  Loss2: (0.0000) | Acc: (85.00%) (7740/9088)
Epoch: 45 | Batch_idx: 80 |  Loss: (0.4323) |  Loss2: (0.0000) | Acc: (85.00%) (8846/10368)
Epoch: 45 | Batch_idx: 90 |  Loss: (0.4329) |  Loss2: (0.0000) | Acc: (85.00%) (9924/11648)
Epoch: 45 | Batch_idx: 100 |  Loss: (0.4306) |  Loss2: (0.0000) | Acc: (85.00%) (11022/12928)
Epoch: 45 | Batch_idx: 110 |  Loss: (0.4318) |  Loss2: (0.0000) | Acc: (85.00%) (12105/14208)
Epoch: 45 | Batch_idx: 120 |  Loss: (0.4317) |  Loss2: (0.0000) | Acc: (85.00%) (13193/15488)
Epoch: 45 | Batch_idx: 130 |  Loss: (0.4323) |  Loss2: (0.0000) | Acc: (85.00%) (14276/16768)
Epoch: 45 | Batch_idx: 140 |  Loss: (0.4323) |  Loss2: (0.0000) | Acc: (85.00%) (15360/18048)
Epoch: 45 | Batch_idx: 150 |  Loss: (0.4308) |  Loss2: (0.0000) | Acc: (85.00%) (16461/19328)
Epoch: 45 | Batch_idx: 160 |  Loss: (0.4331) |  Loss2: (0.0000) | Acc: (85.00%) (17535/20608)
Epoch: 45 | Batch_idx: 170 |  Loss: (0.4340) |  Loss2: (0.0000) | Acc: (85.00%) (18619/21888)
Epoch: 45 | Batch_idx: 180 |  Loss: (0.4316) |  Loss2: (0.0000) | Acc: (85.00%) (19725/23168)
Epoch: 45 | Batch_idx: 190 |  Loss: (0.4304) |  Loss2: (0.0000) | Acc: (85.00%) (20821/24448)
Epoch: 45 | Batch_idx: 200 |  Loss: (0.4303) |  Loss2: (0.0000) | Acc: (85.00%) (21910/25728)
Epoch: 45 | Batch_idx: 210 |  Loss: (0.4307) |  Loss2: (0.0000) | Acc: (85.00%) (22998/27008)
Epoch: 45 | Batch_idx: 220 |  Loss: (0.4292) |  Loss2: (0.0000) | Acc: (85.00%) (24100/28288)
Epoch: 45 | Batch_idx: 230 |  Loss: (0.4304) |  Loss2: (0.0000) | Acc: (85.00%) (25175/29568)
Epoch: 45 | Batch_idx: 240 |  Loss: (0.4309) |  Loss2: (0.0000) | Acc: (85.00%) (26276/30848)
Epoch: 45 | Batch_idx: 250 |  Loss: (0.4297) |  Loss2: (0.0000) | Acc: (85.00%) (27373/32128)
Epoch: 45 | Batch_idx: 260 |  Loss: (0.4302) |  Loss2: (0.0000) | Acc: (85.00%) (28456/33408)
Epoch: 45 | Batch_idx: 270 |  Loss: (0.4298) |  Loss2: (0.0000) | Acc: (85.00%) (29554/34688)
Epoch: 45 | Batch_idx: 280 |  Loss: (0.4282) |  Loss2: (0.0000) | Acc: (85.00%) (30663/35968)
Epoch: 45 | Batch_idx: 290 |  Loss: (0.4297) |  Loss2: (0.0000) | Acc: (85.00%) (31728/37248)
Epoch: 45 | Batch_idx: 300 |  Loss: (0.4312) |  Loss2: (0.0000) | Acc: (85.00%) (32805/38528)
Epoch: 45 | Batch_idx: 310 |  Loss: (0.4309) |  Loss2: (0.0000) | Acc: (85.00%) (33903/39808)
Epoch: 45 | Batch_idx: 320 |  Loss: (0.4302) |  Loss2: (0.0000) | Acc: (85.00%) (35002/41088)
Epoch: 45 | Batch_idx: 330 |  Loss: (0.4306) |  Loss2: (0.0000) | Acc: (85.00%) (36093/42368)
Epoch: 45 | Batch_idx: 340 |  Loss: (0.4298) |  Loss2: (0.0000) | Acc: (85.00%) (37204/43648)
Epoch: 45 | Batch_idx: 350 |  Loss: (0.4288) |  Loss2: (0.0000) | Acc: (85.00%) (38317/44928)
Epoch: 45 | Batch_idx: 360 |  Loss: (0.4299) |  Loss2: (0.0000) | Acc: (85.00%) (39389/46208)
Epoch: 45 | Batch_idx: 370 |  Loss: (0.4297) |  Loss2: (0.0000) | Acc: (85.00%) (40488/47488)
Epoch: 45 | Batch_idx: 380 |  Loss: (0.4290) |  Loss2: (0.0000) | Acc: (85.00%) (41590/48768)
Epoch: 45 | Batch_idx: 390 |  Loss: (0.4290) |  Loss2: (0.0000) | Acc: (85.00%) (42650/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_045.pth.tar'
# TEST : Loss: (0.5999) | Acc: (80.00%) (8007/10000)
percent tensor([0.5247, 0.5352, 0.5266, 0.5115, 0.5319, 0.5388, 0.5375, 0.5197, 0.5243,
        0.5313, 0.5308, 0.5315, 0.5237, 0.5180, 0.5354, 0.5241],
       device='cuda:0') torch.Size([16])
percent tensor([0.4951, 0.4907, 0.4969, 0.4992, 0.4949, 0.5022, 0.4928, 0.4957, 0.4917,
        0.4911, 0.4901, 0.4920, 0.4913, 0.4928, 0.4949, 0.4972],
       device='cuda:0') torch.Size([16])
percent tensor([0.5442, 0.5374, 0.4261, 0.4183, 0.4251, 0.5115, 0.5123, 0.4522, 0.4694,
        0.5096, 0.5374, 0.4535, 0.5260, 0.5155, 0.5321, 0.5263],
       device='cuda:0') torch.Size([16])
percent tensor([0.5874, 0.4688, 0.5947, 0.6117, 0.6043, 0.6341, 0.5418, 0.6136, 0.5479,
        0.4892, 0.5094, 0.5285, 0.4615, 0.5708, 0.5589, 0.6017],
       device='cuda:0') torch.Size([16])
percent tensor([0.5605, 0.5691, 0.5463, 0.5459, 0.5243, 0.5270, 0.5694, 0.5429, 0.5702,
        0.5598, 0.5668, 0.5721, 0.5779, 0.5739, 0.5626, 0.5515],
       device='cuda:0') torch.Size([16])
percent tensor([0.5918, 0.5849, 0.6414, 0.6718, 0.6481, 0.6329, 0.6256, 0.6007, 0.6279,
        0.5751, 0.6167, 0.6610, 0.5925, 0.6766, 0.5956, 0.5981],
       device='cuda:0') torch.Size([16])
percent tensor([0.6713, 0.6530, 0.5934, 0.5747, 0.6148, 0.6474, 0.6361, 0.5371, 0.6501,
        0.6477, 0.6698, 0.5662, 0.6633, 0.6878, 0.5488, 0.6388],
       device='cuda:0') torch.Size([16])
percent tensor([0.9996, 0.9978, 0.9992, 0.9996, 0.9990, 0.9975, 0.9994, 0.9996, 0.9988,
        0.9987, 0.9988, 0.9996, 0.9993, 0.9989, 0.9987, 0.9992],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 46 | Batch_idx: 0 |  Loss: (0.4540) |  Loss2: (0.0000) | Acc: (85.00%) (110/128)
Epoch: 46 | Batch_idx: 10 |  Loss: (0.4010) |  Loss2: (0.0000) | Acc: (86.00%) (1212/1408)
Epoch: 46 | Batch_idx: 20 |  Loss: (0.4329) |  Loss2: (0.0000) | Acc: (84.00%) (2265/2688)
Epoch: 46 | Batch_idx: 30 |  Loss: (0.4599) |  Loss2: (0.0000) | Acc: (83.00%) (3310/3968)
Epoch: 46 | Batch_idx: 40 |  Loss: (0.4725) |  Loss2: (0.0000) | Acc: (83.00%) (4357/5248)
Epoch: 46 | Batch_idx: 50 |  Loss: (0.4799) |  Loss2: (0.0000) | Acc: (82.00%) (5400/6528)
Epoch: 46 | Batch_idx: 60 |  Loss: (0.4798) |  Loss2: (0.0000) | Acc: (82.00%) (6459/7808)
Epoch: 46 | Batch_idx: 70 |  Loss: (0.4899) |  Loss2: (0.0000) | Acc: (82.00%) (7473/9088)
Epoch: 46 | Batch_idx: 80 |  Loss: (0.4910) |  Loss2: (0.0000) | Acc: (82.00%) (8524/10368)
Epoch: 46 | Batch_idx: 90 |  Loss: (0.4887) |  Loss2: (0.0000) | Acc: (82.00%) (9604/11648)
Epoch: 46 | Batch_idx: 100 |  Loss: (0.4889) |  Loss2: (0.0000) | Acc: (82.00%) (10671/12928)
Epoch: 46 | Batch_idx: 110 |  Loss: (0.4888) |  Loss2: (0.0000) | Acc: (82.00%) (11714/14208)
Epoch: 46 | Batch_idx: 120 |  Loss: (0.4889) |  Loss2: (0.0000) | Acc: (82.00%) (12767/15488)
Epoch: 46 | Batch_idx: 130 |  Loss: (0.4867) |  Loss2: (0.0000) | Acc: (82.00%) (13838/16768)
Epoch: 46 | Batch_idx: 140 |  Loss: (0.4869) |  Loss2: (0.0000) | Acc: (82.00%) (14913/18048)
Epoch: 46 | Batch_idx: 150 |  Loss: (0.4876) |  Loss2: (0.0000) | Acc: (82.00%) (15976/19328)
Epoch: 46 | Batch_idx: 160 |  Loss: (0.4863) |  Loss2: (0.0000) | Acc: (82.00%) (17049/20608)
Epoch: 46 | Batch_idx: 170 |  Loss: (0.4856) |  Loss2: (0.0000) | Acc: (82.00%) (18113/21888)
Epoch: 46 | Batch_idx: 180 |  Loss: (0.4861) |  Loss2: (0.0000) | Acc: (82.00%) (19175/23168)
Epoch: 46 | Batch_idx: 190 |  Loss: (0.4835) |  Loss2: (0.0000) | Acc: (82.00%) (20256/24448)
Epoch: 46 | Batch_idx: 200 |  Loss: (0.4831) |  Loss2: (0.0000) | Acc: (82.00%) (21342/25728)
Epoch: 46 | Batch_idx: 210 |  Loss: (0.4824) |  Loss2: (0.0000) | Acc: (83.00%) (22426/27008)
Epoch: 46 | Batch_idx: 220 |  Loss: (0.4799) |  Loss2: (0.0000) | Acc: (83.00%) (23519/28288)
Epoch: 46 | Batch_idx: 230 |  Loss: (0.4803) |  Loss2: (0.0000) | Acc: (83.00%) (24596/29568)
Epoch: 46 | Batch_idx: 240 |  Loss: (0.4805) |  Loss2: (0.0000) | Acc: (83.00%) (25658/30848)
Epoch: 46 | Batch_idx: 250 |  Loss: (0.4806) |  Loss2: (0.0000) | Acc: (83.00%) (26718/32128)
Epoch: 46 | Batch_idx: 260 |  Loss: (0.4807) |  Loss2: (0.0000) | Acc: (83.00%) (27781/33408)
Epoch: 46 | Batch_idx: 270 |  Loss: (0.4803) |  Loss2: (0.0000) | Acc: (83.00%) (28852/34688)
Epoch: 46 | Batch_idx: 280 |  Loss: (0.4789) |  Loss2: (0.0000) | Acc: (83.00%) (29935/35968)
Epoch: 46 | Batch_idx: 290 |  Loss: (0.4787) |  Loss2: (0.0000) | Acc: (83.00%) (30995/37248)
Epoch: 46 | Batch_idx: 300 |  Loss: (0.4786) |  Loss2: (0.0000) | Acc: (83.00%) (32048/38528)
Epoch: 46 | Batch_idx: 310 |  Loss: (0.4779) |  Loss2: (0.0000) | Acc: (83.00%) (33126/39808)
Epoch: 46 | Batch_idx: 320 |  Loss: (0.4764) |  Loss2: (0.0000) | Acc: (83.00%) (34216/41088)
Epoch: 46 | Batch_idx: 330 |  Loss: (0.4771) |  Loss2: (0.0000) | Acc: (83.00%) (35269/42368)
Epoch: 46 | Batch_idx: 340 |  Loss: (0.4755) |  Loss2: (0.0000) | Acc: (83.00%) (36356/43648)
Epoch: 46 | Batch_idx: 350 |  Loss: (0.4744) |  Loss2: (0.0000) | Acc: (83.00%) (37445/44928)
Epoch: 46 | Batch_idx: 360 |  Loss: (0.4736) |  Loss2: (0.0000) | Acc: (83.00%) (38523/46208)
Epoch: 46 | Batch_idx: 370 |  Loss: (0.4724) |  Loss2: (0.0000) | Acc: (83.00%) (39621/47488)
Epoch: 46 | Batch_idx: 380 |  Loss: (0.4709) |  Loss2: (0.0000) | Acc: (83.00%) (40716/48768)
Epoch: 46 | Batch_idx: 390 |  Loss: (0.4708) |  Loss2: (0.0000) | Acc: (83.00%) (41730/50000)
# TEST : Loss: (0.5367) | Acc: (81.00%) (8120/10000)
percent tensor([0.5137, 0.5216, 0.5154, 0.5034, 0.5203, 0.5268, 0.5242, 0.5086, 0.5123,
        0.5187, 0.5188, 0.5195, 0.5128, 0.5069, 0.5231, 0.5125],
       device='cuda:0') torch.Size([16])
percent tensor([0.4986, 0.4944, 0.5005, 0.5034, 0.4992, 0.5056, 0.4966, 0.4997, 0.4952,
        0.4947, 0.4935, 0.4956, 0.4947, 0.4959, 0.4988, 0.5008],
       device='cuda:0') torch.Size([16])
percent tensor([0.5466, 0.5356, 0.4295, 0.4194, 0.4257, 0.5207, 0.5072, 0.4455, 0.4588,
        0.5077, 0.5335, 0.4550, 0.5319, 0.5044, 0.5356, 0.5312],
       device='cuda:0') torch.Size([16])
percent tensor([0.5853, 0.4895, 0.5843, 0.6021, 0.5929, 0.6226, 0.5491, 0.5979, 0.5572,
        0.5077, 0.5375, 0.5318, 0.4926, 0.5663, 0.5582, 0.5978],
       device='cuda:0') torch.Size([16])
percent tensor([0.5664, 0.5698, 0.5663, 0.5632, 0.5458, 0.5215, 0.5833, 0.5700, 0.5741,
        0.5683, 0.5651, 0.5880, 0.5750, 0.5792, 0.5742, 0.5537],
       device='cuda:0') torch.Size([16])
percent tensor([0.5359, 0.5366, 0.5888, 0.6267, 0.6005, 0.5840, 0.5750, 0.5332, 0.5781,
        0.5264, 0.5774, 0.6197, 0.5412, 0.6284, 0.5418, 0.5392],
       device='cuda:0') torch.Size([16])
percent tensor([0.6056, 0.6010, 0.4980, 0.4730, 0.5285, 0.5942, 0.5695, 0.4589, 0.5947,
        0.5907, 0.6130, 0.4748, 0.6046, 0.6321, 0.4755, 0.5778],
       device='cuda:0') torch.Size([16])
percent tensor([0.9995, 0.9986, 0.9991, 0.9993, 0.9989, 0.9952, 0.9995, 0.9996, 0.9990,
        0.9993, 0.9989, 0.9995, 0.9993, 0.9993, 0.9988, 0.9989],
       device='cuda:0') torch.Size([16])
Epoch: 47 | Batch_idx: 0 |  Loss: (0.6303) |  Loss2: (0.0000) | Acc: (77.00%) (99/128)
Epoch: 47 | Batch_idx: 10 |  Loss: (0.4709) |  Loss2: (0.0000) | Acc: (83.00%) (1173/1408)
Epoch: 47 | Batch_idx: 20 |  Loss: (0.4642) |  Loss2: (0.0000) | Acc: (83.00%) (2254/2688)
Epoch: 47 | Batch_idx: 30 |  Loss: (0.4573) |  Loss2: (0.0000) | Acc: (84.00%) (3340/3968)
Epoch: 47 | Batch_idx: 40 |  Loss: (0.4590) |  Loss2: (0.0000) | Acc: (84.00%) (4416/5248)
Epoch: 47 | Batch_idx: 50 |  Loss: (0.4542) |  Loss2: (0.0000) | Acc: (84.00%) (5513/6528)
Epoch: 47 | Batch_idx: 60 |  Loss: (0.4525) |  Loss2: (0.0000) | Acc: (84.00%) (6593/7808)
Epoch: 47 | Batch_idx: 70 |  Loss: (0.4498) |  Loss2: (0.0000) | Acc: (84.00%) (7695/9088)
Epoch: 47 | Batch_idx: 80 |  Loss: (0.4512) |  Loss2: (0.0000) | Acc: (84.00%) (8772/10368)
Epoch: 47 | Batch_idx: 90 |  Loss: (0.4487) |  Loss2: (0.0000) | Acc: (84.00%) (9862/11648)
Epoch: 47 | Batch_idx: 100 |  Loss: (0.4473) |  Loss2: (0.0000) | Acc: (84.00%) (10956/12928)
Epoch: 47 | Batch_idx: 110 |  Loss: (0.4490) |  Loss2: (0.0000) | Acc: (84.00%) (12014/14208)
Epoch: 47 | Batch_idx: 120 |  Loss: (0.4488) |  Loss2: (0.0000) | Acc: (84.00%) (13091/15488)
Epoch: 47 | Batch_idx: 130 |  Loss: (0.4490) |  Loss2: (0.0000) | Acc: (84.00%) (14169/16768)
Epoch: 47 | Batch_idx: 140 |  Loss: (0.4485) |  Loss2: (0.0000) | Acc: (84.00%) (15253/18048)
Epoch: 47 | Batch_idx: 150 |  Loss: (0.4474) |  Loss2: (0.0000) | Acc: (84.00%) (16343/19328)
Epoch: 47 | Batch_idx: 160 |  Loss: (0.4456) |  Loss2: (0.0000) | Acc: (84.00%) (17434/20608)
Epoch: 47 | Batch_idx: 170 |  Loss: (0.4441) |  Loss2: (0.0000) | Acc: (84.00%) (18528/21888)
Epoch: 47 | Batch_idx: 180 |  Loss: (0.4443) |  Loss2: (0.0000) | Acc: (84.00%) (19611/23168)
Epoch: 47 | Batch_idx: 190 |  Loss: (0.4446) |  Loss2: (0.0000) | Acc: (84.00%) (20678/24448)
Epoch: 47 | Batch_idx: 200 |  Loss: (0.4451) |  Loss2: (0.0000) | Acc: (84.00%) (21759/25728)
Epoch: 47 | Batch_idx: 210 |  Loss: (0.4438) |  Loss2: (0.0000) | Acc: (84.00%) (22858/27008)
Epoch: 47 | Batch_idx: 220 |  Loss: (0.4434) |  Loss2: (0.0000) | Acc: (84.00%) (23940/28288)
Epoch: 47 | Batch_idx: 230 |  Loss: (0.4423) |  Loss2: (0.0000) | Acc: (84.00%) (25038/29568)
Epoch: 47 | Batch_idx: 240 |  Loss: (0.4434) |  Loss2: (0.0000) | Acc: (84.00%) (26106/30848)
Epoch: 47 | Batch_idx: 250 |  Loss: (0.4435) |  Loss2: (0.0000) | Acc: (84.00%) (27190/32128)
Epoch: 47 | Batch_idx: 260 |  Loss: (0.4420) |  Loss2: (0.0000) | Acc: (84.00%) (28295/33408)
Epoch: 47 | Batch_idx: 270 |  Loss: (0.4406) |  Loss2: (0.0000) | Acc: (84.00%) (29403/34688)
Epoch: 47 | Batch_idx: 280 |  Loss: (0.4406) |  Loss2: (0.0000) | Acc: (84.00%) (30493/35968)
Epoch: 47 | Batch_idx: 290 |  Loss: (0.4389) |  Loss2: (0.0000) | Acc: (84.00%) (31593/37248)
Epoch: 47 | Batch_idx: 300 |  Loss: (0.4380) |  Loss2: (0.0000) | Acc: (84.00%) (32706/38528)
Epoch: 47 | Batch_idx: 310 |  Loss: (0.4375) |  Loss2: (0.0000) | Acc: (84.00%) (33801/39808)
Epoch: 47 | Batch_idx: 320 |  Loss: (0.4374) |  Loss2: (0.0000) | Acc: (84.00%) (34886/41088)
Epoch: 47 | Batch_idx: 330 |  Loss: (0.4376) |  Loss2: (0.0000) | Acc: (84.00%) (35971/42368)
Epoch: 47 | Batch_idx: 340 |  Loss: (0.4370) |  Loss2: (0.0000) | Acc: (84.00%) (37060/43648)
Epoch: 47 | Batch_idx: 350 |  Loss: (0.4354) |  Loss2: (0.0000) | Acc: (84.00%) (38168/44928)
Epoch: 47 | Batch_idx: 360 |  Loss: (0.4350) |  Loss2: (0.0000) | Acc: (84.00%) (39262/46208)
Epoch: 47 | Batch_idx: 370 |  Loss: (0.4348) |  Loss2: (0.0000) | Acc: (84.00%) (40364/47488)
Epoch: 47 | Batch_idx: 380 |  Loss: (0.4345) |  Loss2: (0.0000) | Acc: (85.00%) (41457/48768)
Epoch: 47 | Batch_idx: 390 |  Loss: (0.4348) |  Loss2: (0.0000) | Acc: (84.00%) (42499/50000)
# TEST : Loss: (0.5086) | Acc: (82.00%) (8239/10000)
percent tensor([0.5137, 0.5216, 0.5153, 0.5038, 0.5202, 0.5271, 0.5241, 0.5091, 0.5122,
        0.5187, 0.5187, 0.5194, 0.5128, 0.5072, 0.5234, 0.5127],
       device='cuda:0') torch.Size([16])
percent tensor([0.5007, 0.4969, 0.5025, 0.5056, 0.5021, 0.5074, 0.4993, 0.5024, 0.4975,
        0.4973, 0.4956, 0.4983, 0.4968, 0.4984, 0.5012, 0.5030],
       device='cuda:0') torch.Size([16])
percent tensor([0.5530, 0.5433, 0.4368, 0.4293, 0.4284, 0.5292, 0.5133, 0.4516, 0.4613,
        0.5152, 0.5380, 0.4615, 0.5396, 0.5125, 0.5430, 0.5420],
       device='cuda:0') torch.Size([16])
percent tensor([0.5779, 0.4836, 0.5796, 0.5970, 0.5864, 0.6211, 0.5400, 0.5926, 0.5500,
        0.5014, 0.5319, 0.5216, 0.4865, 0.5593, 0.5467, 0.5909],
       device='cuda:0') torch.Size([16])
percent tensor([0.5787, 0.5755, 0.5865, 0.5818, 0.5651, 0.5272, 0.5998, 0.5926, 0.5843,
        0.5789, 0.5719, 0.6040, 0.5786, 0.5874, 0.5904, 0.5631],
       device='cuda:0') torch.Size([16])
percent tensor([0.5367, 0.5411, 0.5878, 0.6269, 0.6020, 0.5864, 0.5758, 0.5296, 0.5800,
        0.5279, 0.5834, 0.6236, 0.5437, 0.6304, 0.5440, 0.5420],
       device='cuda:0') torch.Size([16])
percent tensor([0.6279, 0.6175, 0.4925, 0.4664, 0.5308, 0.6192, 0.5773, 0.4471, 0.6087,
        0.6043, 0.6320, 0.4619, 0.6278, 0.6499, 0.4630, 0.5987],
       device='cuda:0') torch.Size([16])
percent tensor([0.9995, 0.9986, 0.9992, 0.9994, 0.9990, 0.9953, 0.9995, 0.9996, 0.9990,
        0.9993, 0.9990, 0.9996, 0.9993, 0.9993, 0.9989, 0.9989],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 48 | Batch_idx: 0 |  Loss: (0.4651) |  Loss2: (0.0000) | Acc: (85.00%) (109/128)
Epoch: 48 | Batch_idx: 10 |  Loss: (0.4498) |  Loss2: (0.0000) | Acc: (84.00%) (1195/1408)
Epoch: 48 | Batch_idx: 20 |  Loss: (0.4221) |  Loss2: (0.0000) | Acc: (85.00%) (2295/2688)
Epoch: 48 | Batch_idx: 30 |  Loss: (0.4266) |  Loss2: (0.0000) | Acc: (85.00%) (3394/3968)
Epoch: 48 | Batch_idx: 40 |  Loss: (0.4260) |  Loss2: (0.0000) | Acc: (85.00%) (4484/5248)
Epoch: 48 | Batch_idx: 50 |  Loss: (0.4206) |  Loss2: (0.0000) | Acc: (85.00%) (5601/6528)
Epoch: 48 | Batch_idx: 60 |  Loss: (0.4208) |  Loss2: (0.0000) | Acc: (85.00%) (6699/7808)
Epoch: 48 | Batch_idx: 70 |  Loss: (0.4195) |  Loss2: (0.0000) | Acc: (85.00%) (7806/9088)
Epoch: 48 | Batch_idx: 80 |  Loss: (0.4201) |  Loss2: (0.0000) | Acc: (85.00%) (8889/10368)
Epoch: 48 | Batch_idx: 90 |  Loss: (0.4189) |  Loss2: (0.0000) | Acc: (85.00%) (10000/11648)
Epoch: 48 | Batch_idx: 100 |  Loss: (0.4174) |  Loss2: (0.0000) | Acc: (85.00%) (11114/12928)
Epoch: 48 | Batch_idx: 110 |  Loss: (0.4181) |  Loss2: (0.0000) | Acc: (85.00%) (12206/14208)
Epoch: 48 | Batch_idx: 120 |  Loss: (0.4205) |  Loss2: (0.0000) | Acc: (85.00%) (13294/15488)
Epoch: 48 | Batch_idx: 130 |  Loss: (0.4198) |  Loss2: (0.0000) | Acc: (85.00%) (14391/16768)
Epoch: 48 | Batch_idx: 140 |  Loss: (0.4223) |  Loss2: (0.0000) | Acc: (85.00%) (15478/18048)
Epoch: 48 | Batch_idx: 150 |  Loss: (0.4211) |  Loss2: (0.0000) | Acc: (85.00%) (16573/19328)
Epoch: 48 | Batch_idx: 160 |  Loss: (0.4184) |  Loss2: (0.0000) | Acc: (85.00%) (17686/20608)
Epoch: 48 | Batch_idx: 170 |  Loss: (0.4185) |  Loss2: (0.0000) | Acc: (85.00%) (18781/21888)
Epoch: 48 | Batch_idx: 180 |  Loss: (0.4186) |  Loss2: (0.0000) | Acc: (85.00%) (19879/23168)
Epoch: 48 | Batch_idx: 190 |  Loss: (0.4206) |  Loss2: (0.0000) | Acc: (85.00%) (20949/24448)
Epoch: 48 | Batch_idx: 200 |  Loss: (0.4226) |  Loss2: (0.0000) | Acc: (85.00%) (22007/25728)
Epoch: 48 | Batch_idx: 210 |  Loss: (0.4218) |  Loss2: (0.0000) | Acc: (85.00%) (23116/27008)
Epoch: 48 | Batch_idx: 220 |  Loss: (0.4226) |  Loss2: (0.0000) | Acc: (85.00%) (24205/28288)
Epoch: 48 | Batch_idx: 230 |  Loss: (0.4230) |  Loss2: (0.0000) | Acc: (85.00%) (25291/29568)
Epoch: 48 | Batch_idx: 240 |  Loss: (0.4251) |  Loss2: (0.0000) | Acc: (85.00%) (26363/30848)
Epoch: 48 | Batch_idx: 250 |  Loss: (0.4242) |  Loss2: (0.0000) | Acc: (85.00%) (27481/32128)
Epoch: 48 | Batch_idx: 260 |  Loss: (0.4240) |  Loss2: (0.0000) | Acc: (85.00%) (28577/33408)
Epoch: 48 | Batch_idx: 270 |  Loss: (0.4231) |  Loss2: (0.0000) | Acc: (85.00%) (29683/34688)
Epoch: 48 | Batch_idx: 280 |  Loss: (0.4223) |  Loss2: (0.0000) | Acc: (85.00%) (30784/35968)
Epoch: 48 | Batch_idx: 290 |  Loss: (0.4207) |  Loss2: (0.0000) | Acc: (85.00%) (31898/37248)
Epoch: 48 | Batch_idx: 300 |  Loss: (0.4212) |  Loss2: (0.0000) | Acc: (85.00%) (32971/38528)
Epoch: 48 | Batch_idx: 310 |  Loss: (0.4214) |  Loss2: (0.0000) | Acc: (85.00%) (34047/39808)
Epoch: 48 | Batch_idx: 320 |  Loss: (0.4211) |  Loss2: (0.0000) | Acc: (85.00%) (35142/41088)
Epoch: 48 | Batch_idx: 330 |  Loss: (0.4207) |  Loss2: (0.0000) | Acc: (85.00%) (36249/42368)
Epoch: 48 | Batch_idx: 340 |  Loss: (0.4204) |  Loss2: (0.0000) | Acc: (85.00%) (37341/43648)
Epoch: 48 | Batch_idx: 350 |  Loss: (0.4197) |  Loss2: (0.0000) | Acc: (85.00%) (38447/44928)
Epoch: 48 | Batch_idx: 360 |  Loss: (0.4213) |  Loss2: (0.0000) | Acc: (85.00%) (39515/46208)
Epoch: 48 | Batch_idx: 370 |  Loss: (0.4205) |  Loss2: (0.0000) | Acc: (85.00%) (40631/47488)
Epoch: 48 | Batch_idx: 380 |  Loss: (0.4208) |  Loss2: (0.0000) | Acc: (85.00%) (41733/48768)
Epoch: 48 | Batch_idx: 390 |  Loss: (0.4205) |  Loss2: (0.0000) | Acc: (85.00%) (42786/50000)
# TEST : Loss: (0.5547) | Acc: (81.00%) (8146/10000)
percent tensor([0.5150, 0.5230, 0.5151, 0.5045, 0.5176, 0.5264, 0.5232, 0.5110, 0.5127,
        0.5187, 0.5197, 0.5178, 0.5132, 0.5075, 0.5238, 0.5136],
       device='cuda:0') torch.Size([16])
percent tensor([0.5005, 0.4971, 0.5012, 0.5048, 0.5008, 0.5071, 0.4994, 0.5013, 0.4983,
        0.4972, 0.4967, 0.4971, 0.4971, 0.5000, 0.5014, 0.5033],
       device='cuda:0') torch.Size([16])
percent tensor([0.5494, 0.5506, 0.4473, 0.4277, 0.4217, 0.5187, 0.5180, 0.4648, 0.4855,
        0.5186, 0.5491, 0.4671, 0.5413, 0.5184, 0.5382, 0.5377],
       device='cuda:0') torch.Size([16])
percent tensor([0.5744, 0.4892, 0.5733, 0.5945, 0.5860, 0.6215, 0.5425, 0.5863, 0.5473,
        0.4980, 0.5261, 0.5155, 0.4762, 0.5696, 0.5501, 0.5911],
       device='cuda:0') torch.Size([16])
percent tensor([0.5833, 0.5743, 0.5880, 0.5791, 0.5671, 0.5344, 0.5938, 0.5920, 0.5906,
        0.5823, 0.5785, 0.6025, 0.5818, 0.5870, 0.5931, 0.5661],
       device='cuda:0') torch.Size([16])
percent tensor([0.5394, 0.5575, 0.5720, 0.6257, 0.5915, 0.5978, 0.5824, 0.5291, 0.5910,
        0.5278, 0.5872, 0.6082, 0.5322, 0.6481, 0.5573, 0.5492],
       device='cuda:0') torch.Size([16])
percent tensor([0.6062, 0.5860, 0.5047, 0.4789, 0.5262, 0.6155, 0.5646, 0.4442, 0.5663,
        0.5822, 0.5875, 0.4691, 0.6069, 0.6045, 0.4477, 0.5978],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9983, 0.9992, 0.9994, 0.9994, 0.9973, 0.9994, 0.9995, 0.9989,
        0.9990, 0.9984, 0.9995, 0.9989, 0.9992, 0.9991, 0.9993],
       device='cuda:0') torch.Size([16])
Epoch: 49 | Batch_idx: 0 |  Loss: (0.4003) |  Loss2: (0.0000) | Acc: (87.00%) (112/128)
Epoch: 49 | Batch_idx: 10 |  Loss: (0.3841) |  Loss2: (0.0000) | Acc: (86.00%) (1219/1408)
Epoch: 49 | Batch_idx: 20 |  Loss: (0.3780) |  Loss2: (0.0000) | Acc: (87.00%) (2348/2688)
Epoch: 49 | Batch_idx: 30 |  Loss: (0.3781) |  Loss2: (0.0000) | Acc: (87.00%) (3475/3968)
Epoch: 49 | Batch_idx: 40 |  Loss: (0.3815) |  Loss2: (0.0000) | Acc: (87.00%) (4584/5248)
Epoch: 49 | Batch_idx: 50 |  Loss: (0.3878) |  Loss2: (0.0000) | Acc: (86.00%) (5679/6528)
Epoch: 49 | Batch_idx: 60 |  Loss: (0.3918) |  Loss2: (0.0000) | Acc: (86.00%) (6770/7808)
Epoch: 49 | Batch_idx: 70 |  Loss: (0.3925) |  Loss2: (0.0000) | Acc: (86.00%) (7871/9088)
Epoch: 49 | Batch_idx: 80 |  Loss: (0.3927) |  Loss2: (0.0000) | Acc: (86.00%) (8988/10368)
Epoch: 49 | Batch_idx: 90 |  Loss: (0.3936) |  Loss2: (0.0000) | Acc: (86.00%) (10092/11648)
Epoch: 49 | Batch_idx: 100 |  Loss: (0.3970) |  Loss2: (0.0000) | Acc: (86.00%) (11178/12928)
Epoch: 49 | Batch_idx: 110 |  Loss: (0.4014) |  Loss2: (0.0000) | Acc: (86.00%) (12265/14208)
Epoch: 49 | Batch_idx: 120 |  Loss: (0.3991) |  Loss2: (0.0000) | Acc: (86.00%) (13378/15488)
Epoch: 49 | Batch_idx: 130 |  Loss: (0.4024) |  Loss2: (0.0000) | Acc: (86.00%) (14465/16768)
Epoch: 49 | Batch_idx: 140 |  Loss: (0.4074) |  Loss2: (0.0000) | Acc: (86.00%) (15538/18048)
Epoch: 49 | Batch_idx: 150 |  Loss: (0.4076) |  Loss2: (0.0000) | Acc: (86.00%) (16648/19328)
Epoch: 49 | Batch_idx: 160 |  Loss: (0.4092) |  Loss2: (0.0000) | Acc: (86.00%) (17731/20608)
Epoch: 49 | Batch_idx: 170 |  Loss: (0.4087) |  Loss2: (0.0000) | Acc: (86.00%) (18841/21888)
Epoch: 49 | Batch_idx: 180 |  Loss: (0.4071) |  Loss2: (0.0000) | Acc: (86.00%) (19951/23168)
Epoch: 49 | Batch_idx: 190 |  Loss: (0.4073) |  Loss2: (0.0000) | Acc: (86.00%) (21036/24448)
Epoch: 49 | Batch_idx: 200 |  Loss: (0.4054) |  Loss2: (0.0000) | Acc: (86.00%) (22157/25728)
Epoch: 49 | Batch_idx: 210 |  Loss: (0.4050) |  Loss2: (0.0000) | Acc: (86.00%) (23255/27008)
Epoch: 49 | Batch_idx: 220 |  Loss: (0.4066) |  Loss2: (0.0000) | Acc: (86.00%) (24331/28288)
Epoch: 49 | Batch_idx: 230 |  Loss: (0.4059) |  Loss2: (0.0000) | Acc: (86.00%) (25441/29568)
Epoch: 49 | Batch_idx: 240 |  Loss: (0.4053) |  Loss2: (0.0000) | Acc: (86.00%) (26550/30848)
Epoch: 49 | Batch_idx: 250 |  Loss: (0.4045) |  Loss2: (0.0000) | Acc: (86.00%) (27664/32128)
Epoch: 49 | Batch_idx: 260 |  Loss: (0.4029) |  Loss2: (0.0000) | Acc: (86.00%) (28788/33408)
Epoch: 49 | Batch_idx: 270 |  Loss: (0.4031) |  Loss2: (0.0000) | Acc: (86.00%) (29889/34688)
Epoch: 49 | Batch_idx: 280 |  Loss: (0.4029) |  Loss2: (0.0000) | Acc: (86.00%) (30987/35968)
Epoch: 49 | Batch_idx: 290 |  Loss: (0.4031) |  Loss2: (0.0000) | Acc: (86.00%) (32071/37248)
Epoch: 49 | Batch_idx: 300 |  Loss: (0.4037) |  Loss2: (0.0000) | Acc: (86.00%) (33166/38528)
Epoch: 49 | Batch_idx: 310 |  Loss: (0.4034) |  Loss2: (0.0000) | Acc: (86.00%) (34275/39808)
Epoch: 49 | Batch_idx: 320 |  Loss: (0.4031) |  Loss2: (0.0000) | Acc: (86.00%) (35385/41088)
Epoch: 49 | Batch_idx: 330 |  Loss: (0.4041) |  Loss2: (0.0000) | Acc: (86.00%) (36474/42368)
Epoch: 49 | Batch_idx: 340 |  Loss: (0.4043) |  Loss2: (0.0000) | Acc: (86.00%) (37580/43648)
Epoch: 49 | Batch_idx: 350 |  Loss: (0.4033) |  Loss2: (0.0000) | Acc: (86.00%) (38700/44928)
Epoch: 49 | Batch_idx: 360 |  Loss: (0.4027) |  Loss2: (0.0000) | Acc: (86.00%) (39798/46208)
Epoch: 49 | Batch_idx: 370 |  Loss: (0.4052) |  Loss2: (0.0000) | Acc: (86.00%) (40853/47488)
Epoch: 49 | Batch_idx: 380 |  Loss: (0.4059) |  Loss2: (0.0000) | Acc: (85.00%) (41932/48768)
Epoch: 49 | Batch_idx: 390 |  Loss: (0.4061) |  Loss2: (0.0000) | Acc: (85.00%) (42993/50000)
# TEST : Loss: (0.5276) | Acc: (81.00%) (8181/10000)
percent tensor([0.5142, 0.5201, 0.5155, 0.5046, 0.5185, 0.5262, 0.5215, 0.5098, 0.5106,
        0.5174, 0.5176, 0.5174, 0.5121, 0.5049, 0.5226, 0.5130],
       device='cuda:0') torch.Size([16])
percent tensor([0.5000, 0.4968, 0.5020, 0.5049, 0.5014, 0.5062, 0.4991, 0.5011, 0.4981,
        0.4973, 0.4958, 0.4975, 0.4966, 0.4996, 0.5005, 0.5028],
       device='cuda:0') torch.Size([16])
percent tensor([0.5451, 0.5386, 0.4346, 0.4242, 0.4210, 0.5198, 0.5076, 0.4472, 0.4655,
        0.5100, 0.5324, 0.4607, 0.5309, 0.5079, 0.5330, 0.5337],
       device='cuda:0') torch.Size([16])
percent tensor([0.5730, 0.4684, 0.5850, 0.6032, 0.5898, 0.6110, 0.5314, 0.5913, 0.5527,
        0.4953, 0.5191, 0.5204, 0.4784, 0.5543, 0.5413, 0.5867],
       device='cuda:0') torch.Size([16])
percent tensor([0.5793, 0.5765, 0.5727, 0.5720, 0.5604, 0.5372, 0.5928, 0.5876, 0.5819,
        0.5796, 0.5740, 0.5947, 0.5769, 0.5868, 0.5927, 0.5642],
       device='cuda:0') torch.Size([16])
percent tensor([0.5342, 0.5353, 0.5887, 0.6219, 0.6098, 0.5861, 0.5588, 0.5484, 0.5963,
        0.5113, 0.5765, 0.6207, 0.5412, 0.6255, 0.5476, 0.5374],
       device='cuda:0') torch.Size([16])
percent tensor([0.6407, 0.5981, 0.5383, 0.5149, 0.5620, 0.6404, 0.5916, 0.4446, 0.6211,
        0.6104, 0.6177, 0.5041, 0.6316, 0.6404, 0.4822, 0.6076],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9984, 0.9991, 0.9993, 0.9993, 0.9973, 0.9996, 0.9995, 0.9990,
        0.9985, 0.9988, 0.9992, 0.9993, 0.9988, 0.9985, 0.9991],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 50 | Batch_idx: 0 |  Loss: (0.4491) |  Loss2: (0.0000) | Acc: (81.00%) (104/128)
Epoch: 50 | Batch_idx: 10 |  Loss: (0.4792) |  Loss2: (0.0000) | Acc: (82.00%) (1168/1408)
Epoch: 50 | Batch_idx: 20 |  Loss: (0.5235) |  Loss2: (0.0000) | Acc: (82.00%) (2209/2688)
Epoch: 50 | Batch_idx: 30 |  Loss: (0.5467) |  Loss2: (0.0000) | Acc: (81.00%) (3216/3968)
Epoch: 50 | Batch_idx: 40 |  Loss: (0.5693) |  Loss2: (0.0000) | Acc: (80.00%) (4220/5248)
Epoch: 50 | Batch_idx: 50 |  Loss: (0.5759) |  Loss2: (0.0000) | Acc: (80.00%) (5250/6528)
Epoch: 50 | Batch_idx: 60 |  Loss: (0.5814) |  Loss2: (0.0000) | Acc: (80.00%) (6251/7808)
Epoch: 50 | Batch_idx: 70 |  Loss: (0.5845) |  Loss2: (0.0000) | Acc: (79.00%) (7249/9088)
Epoch: 50 | Batch_idx: 80 |  Loss: (0.5873) |  Loss2: (0.0000) | Acc: (79.00%) (8259/10368)
Epoch: 50 | Batch_idx: 90 |  Loss: (0.5821) |  Loss2: (0.0000) | Acc: (79.00%) (9296/11648)
Epoch: 50 | Batch_idx: 100 |  Loss: (0.5797) |  Loss2: (0.0000) | Acc: (79.00%) (10328/12928)
Epoch: 50 | Batch_idx: 110 |  Loss: (0.5752) |  Loss2: (0.0000) | Acc: (80.00%) (11368/14208)
Epoch: 50 | Batch_idx: 120 |  Loss: (0.5707) |  Loss2: (0.0000) | Acc: (80.00%) (12432/15488)
Epoch: 50 | Batch_idx: 130 |  Loss: (0.5699) |  Loss2: (0.0000) | Acc: (80.00%) (13476/16768)
Epoch: 50 | Batch_idx: 140 |  Loss: (0.5677) |  Loss2: (0.0000) | Acc: (80.00%) (14516/18048)
Epoch: 50 | Batch_idx: 150 |  Loss: (0.5633) |  Loss2: (0.0000) | Acc: (80.00%) (15570/19328)
Epoch: 50 | Batch_idx: 160 |  Loss: (0.5603) |  Loss2: (0.0000) | Acc: (80.00%) (16620/20608)
Epoch: 50 | Batch_idx: 170 |  Loss: (0.5591) |  Loss2: (0.0000) | Acc: (80.00%) (17663/21888)
Epoch: 50 | Batch_idx: 180 |  Loss: (0.5561) |  Loss2: (0.0000) | Acc: (80.00%) (18718/23168)
Epoch: 50 | Batch_idx: 190 |  Loss: (0.5536) |  Loss2: (0.0000) | Acc: (80.00%) (19769/24448)
Epoch: 50 | Batch_idx: 200 |  Loss: (0.5495) |  Loss2: (0.0000) | Acc: (80.00%) (20829/25728)
Epoch: 50 | Batch_idx: 210 |  Loss: (0.5491) |  Loss2: (0.0000) | Acc: (80.00%) (21874/27008)
Epoch: 50 | Batch_idx: 220 |  Loss: (0.5451) |  Loss2: (0.0000) | Acc: (81.00%) (22950/28288)
Epoch: 50 | Batch_idx: 230 |  Loss: (0.5418) |  Loss2: (0.0000) | Acc: (81.00%) (24032/29568)
Epoch: 50 | Batch_idx: 240 |  Loss: (0.5396) |  Loss2: (0.0000) | Acc: (81.00%) (25093/30848)
Epoch: 50 | Batch_idx: 250 |  Loss: (0.5368) |  Loss2: (0.0000) | Acc: (81.00%) (26170/32128)
Epoch: 50 | Batch_idx: 260 |  Loss: (0.5367) |  Loss2: (0.0000) | Acc: (81.00%) (27202/33408)
Epoch: 50 | Batch_idx: 270 |  Loss: (0.5356) |  Loss2: (0.0000) | Acc: (81.00%) (28256/34688)
Epoch: 50 | Batch_idx: 280 |  Loss: (0.5335) |  Loss2: (0.0000) | Acc: (81.00%) (29323/35968)
Epoch: 50 | Batch_idx: 290 |  Loss: (0.5303) |  Loss2: (0.0000) | Acc: (81.00%) (30409/37248)
Epoch: 50 | Batch_idx: 300 |  Loss: (0.5279) |  Loss2: (0.0000) | Acc: (81.00%) (31479/38528)
Epoch: 50 | Batch_idx: 310 |  Loss: (0.5253) |  Loss2: (0.0000) | Acc: (81.00%) (32560/39808)
Epoch: 50 | Batch_idx: 320 |  Loss: (0.5259) |  Loss2: (0.0000) | Acc: (81.00%) (33600/41088)
Epoch: 50 | Batch_idx: 330 |  Loss: (0.5236) |  Loss2: (0.0000) | Acc: (81.00%) (34679/42368)
Epoch: 50 | Batch_idx: 340 |  Loss: (0.5220) |  Loss2: (0.0000) | Acc: (81.00%) (35753/43648)
Epoch: 50 | Batch_idx: 350 |  Loss: (0.5205) |  Loss2: (0.0000) | Acc: (81.00%) (36820/44928)
Epoch: 50 | Batch_idx: 360 |  Loss: (0.5184) |  Loss2: (0.0000) | Acc: (82.00%) (37901/46208)
Epoch: 50 | Batch_idx: 370 |  Loss: (0.5176) |  Loss2: (0.0000) | Acc: (82.00%) (38965/47488)
Epoch: 50 | Batch_idx: 380 |  Loss: (0.5166) |  Loss2: (0.0000) | Acc: (82.00%) (40036/48768)
Epoch: 50 | Batch_idx: 390 |  Loss: (0.5143) |  Loss2: (0.0000) | Acc: (82.00%) (41101/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_050.pth.tar'
# TEST : Loss: (0.5527) | Acc: (81.00%) (8106/10000)
percent tensor([0.5211, 0.5280, 0.5256, 0.5092, 0.5297, 0.5330, 0.5314, 0.5191, 0.5189,
        0.5270, 0.5249, 0.5281, 0.5193, 0.5095, 0.5304, 0.5200],
       device='cuda:0') torch.Size([16])
percent tensor([0.5065, 0.5035, 0.5093, 0.5118, 0.5095, 0.5113, 0.5059, 0.5094, 0.5058,
        0.5043, 0.5023, 0.5044, 0.5033, 0.5056, 0.5073, 0.5094],
       device='cuda:0') torch.Size([16])
percent tensor([0.5569, 0.5653, 0.4401, 0.4227, 0.4213, 0.5153, 0.5316, 0.4669, 0.4862,
        0.5413, 0.5560, 0.4760, 0.5502, 0.5477, 0.5444, 0.5509],
       device='cuda:0') torch.Size([16])
percent tensor([0.5957, 0.4776, 0.6136, 0.6353, 0.6103, 0.6291, 0.5436, 0.6263, 0.5600,
        0.4993, 0.5219, 0.5179, 0.4792, 0.5754, 0.5614, 0.6051],
       device='cuda:0') torch.Size([16])
percent tensor([0.5576, 0.5712, 0.5623, 0.5421, 0.5443, 0.4981, 0.5797, 0.5822, 0.5632,
        0.5596, 0.5514, 0.5835, 0.5658, 0.5588, 0.5829, 0.5245],
       device='cuda:0') torch.Size([16])
percent tensor([0.5491, 0.5520, 0.6156, 0.6542, 0.6284, 0.5976, 0.5807, 0.5683, 0.6081,
        0.5120, 0.5842, 0.6501, 0.5469, 0.6548, 0.5597, 0.5466],
       device='cuda:0') torch.Size([16])
percent tensor([0.5666, 0.5572, 0.4601, 0.3990, 0.4797, 0.5696, 0.5204, 0.3695, 0.5767,
        0.5842, 0.5670, 0.4210, 0.5853, 0.5632, 0.4178, 0.5615],
       device='cuda:0') torch.Size([16])
percent tensor([0.9996, 0.9987, 0.9986, 0.9992, 0.9988, 0.9966, 0.9993, 0.9995, 0.9992,
        0.9989, 0.9985, 0.9990, 0.9994, 0.9986, 0.9984, 0.9987],
       device='cuda:0') torch.Size([16])
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(174.7595, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(798.3098, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(792.1032, device='cuda:0')
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1525.2675, device='cuda:0')
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(500.8163, device='cuda:0')
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2200.6497, device='cuda:0')
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4281.9761, device='cuda:0')
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1423.9954, device='cuda:0')
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6112.8950, device='cuda:0')
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12054.5176, device='cuda:0')
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4015.3340, device='cuda:0')
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16982.1426, device='cuda:0')
Epoch: 51 | Batch_idx: 0 |  Loss: (0.4863) |  Loss2: (0.0000) | Acc: (79.00%) (102/128)
Epoch: 51 | Batch_idx: 10 |  Loss: (0.4447) |  Loss2: (0.0000) | Acc: (84.00%) (1192/1408)
Epoch: 51 | Batch_idx: 20 |  Loss: (0.4457) |  Loss2: (0.0000) | Acc: (84.00%) (2278/2688)
Epoch: 51 | Batch_idx: 30 |  Loss: (0.4405) |  Loss2: (0.0000) | Acc: (84.00%) (3370/3968)
Epoch: 51 | Batch_idx: 40 |  Loss: (0.4409) |  Loss2: (0.0000) | Acc: (84.00%) (4452/5248)
Epoch: 51 | Batch_idx: 50 |  Loss: (0.4466) |  Loss2: (0.0000) | Acc: (84.00%) (5498/6528)
Epoch: 51 | Batch_idx: 60 |  Loss: (0.4500) |  Loss2: (0.0000) | Acc: (84.00%) (6576/7808)
Epoch: 51 | Batch_idx: 70 |  Loss: (0.4551) |  Loss2: (0.0000) | Acc: (84.00%) (7638/9088)
Epoch: 51 | Batch_idx: 80 |  Loss: (0.4586) |  Loss2: (0.0000) | Acc: (83.00%) (8701/10368)
Epoch: 51 | Batch_idx: 90 |  Loss: (0.4550) |  Loss2: (0.0000) | Acc: (84.00%) (9787/11648)
Epoch: 51 | Batch_idx: 100 |  Loss: (0.4553) |  Loss2: (0.0000) | Acc: (84.00%) (10873/12928)
Epoch: 51 | Batch_idx: 110 |  Loss: (0.4559) |  Loss2: (0.0000) | Acc: (84.00%) (11939/14208)
Epoch: 51 | Batch_idx: 120 |  Loss: (0.4574) |  Loss2: (0.0000) | Acc: (84.00%) (13010/15488)
Epoch: 51 | Batch_idx: 130 |  Loss: (0.4550) |  Loss2: (0.0000) | Acc: (84.00%) (14095/16768)
Epoch: 51 | Batch_idx: 140 |  Loss: (0.4563) |  Loss2: (0.0000) | Acc: (84.00%) (15162/18048)
Epoch: 51 | Batch_idx: 150 |  Loss: (0.4560) |  Loss2: (0.0000) | Acc: (84.00%) (16247/19328)
Epoch: 51 | Batch_idx: 160 |  Loss: (0.4547) |  Loss2: (0.0000) | Acc: (84.00%) (17331/20608)
Epoch: 51 | Batch_idx: 170 |  Loss: (0.4533) |  Loss2: (0.0000) | Acc: (84.00%) (18417/21888)
Epoch: 51 | Batch_idx: 180 |  Loss: (0.4525) |  Loss2: (0.0000) | Acc: (84.00%) (19507/23168)
Epoch: 51 | Batch_idx: 190 |  Loss: (0.4530) |  Loss2: (0.0000) | Acc: (84.00%) (20572/24448)
Epoch: 51 | Batch_idx: 200 |  Loss: (0.4548) |  Loss2: (0.0000) | Acc: (84.00%) (21635/25728)
Epoch: 51 | Batch_idx: 210 |  Loss: (0.4555) |  Loss2: (0.0000) | Acc: (84.00%) (22717/27008)
Epoch: 51 | Batch_idx: 220 |  Loss: (0.4531) |  Loss2: (0.0000) | Acc: (84.00%) (23818/28288)
Epoch: 51 | Batch_idx: 230 |  Loss: (0.4537) |  Loss2: (0.0000) | Acc: (84.00%) (24892/29568)
Epoch: 51 | Batch_idx: 240 |  Loss: (0.4508) |  Loss2: (0.0000) | Acc: (84.00%) (26005/30848)
Epoch: 51 | Batch_idx: 250 |  Loss: (0.4500) |  Loss2: (0.0000) | Acc: (84.00%) (27103/32128)
Epoch: 51 | Batch_idx: 260 |  Loss: (0.4494) |  Loss2: (0.0000) | Acc: (84.00%) (28184/33408)
Epoch: 51 | Batch_idx: 270 |  Loss: (0.4489) |  Loss2: (0.0000) | Acc: (84.00%) (29276/34688)
Epoch: 51 | Batch_idx: 280 |  Loss: (0.4478) |  Loss2: (0.0000) | Acc: (84.00%) (30366/35968)
Epoch: 51 | Batch_idx: 290 |  Loss: (0.4470) |  Loss2: (0.0000) | Acc: (84.00%) (31452/37248)
Epoch: 51 | Batch_idx: 300 |  Loss: (0.4461) |  Loss2: (0.0000) | Acc: (84.00%) (32548/38528)
Epoch: 51 | Batch_idx: 310 |  Loss: (0.4465) |  Loss2: (0.0000) | Acc: (84.00%) (33630/39808)
Epoch: 51 | Batch_idx: 320 |  Loss: (0.4473) |  Loss2: (0.0000) | Acc: (84.00%) (34703/41088)
Epoch: 51 | Batch_idx: 330 |  Loss: (0.4467) |  Loss2: (0.0000) | Acc: (84.00%) (35794/42368)
Epoch: 51 | Batch_idx: 340 |  Loss: (0.4466) |  Loss2: (0.0000) | Acc: (84.00%) (36875/43648)
Epoch: 51 | Batch_idx: 350 |  Loss: (0.4460) |  Loss2: (0.0000) | Acc: (84.00%) (37970/44928)
Epoch: 51 | Batch_idx: 360 |  Loss: (0.4455) |  Loss2: (0.0000) | Acc: (84.00%) (39050/46208)
Epoch: 51 | Batch_idx: 370 |  Loss: (0.4459) |  Loss2: (0.0000) | Acc: (84.00%) (40127/47488)
Epoch: 51 | Batch_idx: 380 |  Loss: (0.4450) |  Loss2: (0.0000) | Acc: (84.00%) (41223/48768)
Epoch: 51 | Batch_idx: 390 |  Loss: (0.4436) |  Loss2: (0.0000) | Acc: (84.00%) (42296/50000)
# TEST : Loss: (0.5166) | Acc: (82.00%) (8249/10000)
percent tensor([0.5176, 0.5248, 0.5213, 0.5062, 0.5254, 0.5286, 0.5278, 0.5154, 0.5154,
        0.5236, 0.5218, 0.5244, 0.5160, 0.5066, 0.5267, 0.5165],
       device='cuda:0') torch.Size([16])
percent tensor([0.5089, 0.5058, 0.5118, 0.5143, 0.5121, 0.5133, 0.5084, 0.5117, 0.5084,
        0.5069, 0.5048, 0.5071, 0.5056, 0.5079, 0.5097, 0.5114],
       device='cuda:0') torch.Size([16])
percent tensor([0.5520, 0.5638, 0.4293, 0.4075, 0.4144, 0.5046, 0.5278, 0.4627, 0.4784,
        0.5372, 0.5536, 0.4688, 0.5451, 0.5397, 0.5383, 0.5432],
       device='cuda:0') torch.Size([16])
percent tensor([0.6086, 0.4818, 0.6274, 0.6443, 0.6225, 0.6402, 0.5532, 0.6382, 0.5699,
        0.5022, 0.5301, 0.5287, 0.4833, 0.5889, 0.5724, 0.6142],
       device='cuda:0') torch.Size([16])
percent tensor([0.5662, 0.5892, 0.5772, 0.5458, 0.5497, 0.4880, 0.5953, 0.6030, 0.5749,
        0.5697, 0.5618, 0.6037, 0.5824, 0.5652, 0.6036, 0.5206],
       device='cuda:0') torch.Size([16])
percent tensor([0.5436, 0.5452, 0.6230, 0.6619, 0.6363, 0.5989, 0.5770, 0.5633, 0.6103,
        0.5039, 0.5853, 0.6603, 0.5411, 0.6582, 0.5550, 0.5395],
       device='cuda:0') torch.Size([16])
percent tensor([0.5683, 0.5715, 0.4593, 0.4020, 0.4920, 0.5745, 0.5297, 0.3769, 0.5810,
        0.5911, 0.5743, 0.4274, 0.5929, 0.5690, 0.4235, 0.5706],
       device='cuda:0') torch.Size([16])
percent tensor([0.9996, 0.9988, 0.9989, 0.9994, 0.9991, 0.9967, 0.9993, 0.9996, 0.9993,
        0.9988, 0.9986, 0.9992, 0.9995, 0.9986, 0.9986, 0.9988],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 52 | Batch_idx: 0 |  Loss: (0.3720) |  Loss2: (0.0000) | Acc: (85.00%) (109/128)
Epoch: 52 | Batch_idx: 10 |  Loss: (0.3937) |  Loss2: (0.0000) | Acc: (86.00%) (1212/1408)
Epoch: 52 | Batch_idx: 20 |  Loss: (0.3930) |  Loss2: (0.0000) | Acc: (86.00%) (2318/2688)
Epoch: 52 | Batch_idx: 30 |  Loss: (0.3924) |  Loss2: (0.0000) | Acc: (86.00%) (3432/3968)
Epoch: 52 | Batch_idx: 40 |  Loss: (0.4007) |  Loss2: (0.0000) | Acc: (86.00%) (4521/5248)
Epoch: 52 | Batch_idx: 50 |  Loss: (0.3993) |  Loss2: (0.0000) | Acc: (86.00%) (5632/6528)
Epoch: 52 | Batch_idx: 60 |  Loss: (0.3912) |  Loss2: (0.0000) | Acc: (86.00%) (6771/7808)
Epoch: 52 | Batch_idx: 70 |  Loss: (0.3931) |  Loss2: (0.0000) | Acc: (86.00%) (7849/9088)
Epoch: 52 | Batch_idx: 80 |  Loss: (0.3936) |  Loss2: (0.0000) | Acc: (86.00%) (8958/10368)
Epoch: 52 | Batch_idx: 90 |  Loss: (0.3918) |  Loss2: (0.0000) | Acc: (86.00%) (10078/11648)
Epoch: 52 | Batch_idx: 100 |  Loss: (0.3910) |  Loss2: (0.0000) | Acc: (86.00%) (11193/12928)
Epoch: 52 | Batch_idx: 110 |  Loss: (0.3922) |  Loss2: (0.0000) | Acc: (86.00%) (12287/14208)
Epoch: 52 | Batch_idx: 120 |  Loss: (0.3953) |  Loss2: (0.0000) | Acc: (86.00%) (13383/15488)
Epoch: 52 | Batch_idx: 130 |  Loss: (0.3967) |  Loss2: (0.0000) | Acc: (86.00%) (14483/16768)
Epoch: 52 | Batch_idx: 140 |  Loss: (0.3983) |  Loss2: (0.0000) | Acc: (86.00%) (15572/18048)
Epoch: 52 | Batch_idx: 150 |  Loss: (0.4009) |  Loss2: (0.0000) | Acc: (86.00%) (16654/19328)
Epoch: 52 | Batch_idx: 160 |  Loss: (0.4005) |  Loss2: (0.0000) | Acc: (86.00%) (17756/20608)
Epoch: 52 | Batch_idx: 170 |  Loss: (0.4020) |  Loss2: (0.0000) | Acc: (86.00%) (18856/21888)
Epoch: 52 | Batch_idx: 180 |  Loss: (0.4026) |  Loss2: (0.0000) | Acc: (86.00%) (19966/23168)
Epoch: 52 | Batch_idx: 190 |  Loss: (0.4021) |  Loss2: (0.0000) | Acc: (86.00%) (21069/24448)
Epoch: 52 | Batch_idx: 200 |  Loss: (0.4021) |  Loss2: (0.0000) | Acc: (86.00%) (22174/25728)
Epoch: 52 | Batch_idx: 210 |  Loss: (0.4003) |  Loss2: (0.0000) | Acc: (86.00%) (23294/27008)
Epoch: 52 | Batch_idx: 220 |  Loss: (0.4001) |  Loss2: (0.0000) | Acc: (86.00%) (24394/28288)
Epoch: 52 | Batch_idx: 230 |  Loss: (0.4012) |  Loss2: (0.0000) | Acc: (86.00%) (25495/29568)
Epoch: 52 | Batch_idx: 240 |  Loss: (0.4020) |  Loss2: (0.0000) | Acc: (86.00%) (26581/30848)
Epoch: 52 | Batch_idx: 250 |  Loss: (0.4008) |  Loss2: (0.0000) | Acc: (86.00%) (27692/32128)
Epoch: 52 | Batch_idx: 260 |  Loss: (0.4014) |  Loss2: (0.0000) | Acc: (86.00%) (28792/33408)
Epoch: 52 | Batch_idx: 270 |  Loss: (0.4008) |  Loss2: (0.0000) | Acc: (86.00%) (29900/34688)
Epoch: 52 | Batch_idx: 280 |  Loss: (0.4009) |  Loss2: (0.0000) | Acc: (86.00%) (31013/35968)
Epoch: 52 | Batch_idx: 290 |  Loss: (0.4011) |  Loss2: (0.0000) | Acc: (86.00%) (32111/37248)
Epoch: 52 | Batch_idx: 300 |  Loss: (0.4008) |  Loss2: (0.0000) | Acc: (86.00%) (33219/38528)
Epoch: 52 | Batch_idx: 310 |  Loss: (0.4007) |  Loss2: (0.0000) | Acc: (86.00%) (34318/39808)
Epoch: 52 | Batch_idx: 320 |  Loss: (0.4005) |  Loss2: (0.0000) | Acc: (86.00%) (35425/41088)
Epoch: 52 | Batch_idx: 330 |  Loss: (0.4005) |  Loss2: (0.0000) | Acc: (86.00%) (36533/42368)
Epoch: 52 | Batch_idx: 340 |  Loss: (0.4005) |  Loss2: (0.0000) | Acc: (86.00%) (37636/43648)
Epoch: 52 | Batch_idx: 350 |  Loss: (0.4000) |  Loss2: (0.0000) | Acc: (86.00%) (38753/44928)
Epoch: 52 | Batch_idx: 360 |  Loss: (0.3996) |  Loss2: (0.0000) | Acc: (86.00%) (39853/46208)
Epoch: 52 | Batch_idx: 370 |  Loss: (0.4001) |  Loss2: (0.0000) | Acc: (86.00%) (40946/47488)
Epoch: 52 | Batch_idx: 380 |  Loss: (0.4009) |  Loss2: (0.0000) | Acc: (86.00%) (42033/48768)
Epoch: 52 | Batch_idx: 390 |  Loss: (0.4006) |  Loss2: (0.0000) | Acc: (86.00%) (43104/50000)
# TEST : Loss: (0.5170) | Acc: (82.00%) (8227/10000)
percent tensor([0.5179, 0.5262, 0.5211, 0.5072, 0.5242, 0.5300, 0.5278, 0.5157, 0.5143,
        0.5241, 0.5222, 0.5239, 0.5160, 0.5070, 0.5281, 0.5172],
       device='cuda:0') torch.Size([16])
percent tensor([0.5082, 0.5054, 0.5098, 0.5124, 0.5107, 0.5124, 0.5085, 0.5107, 0.5078,
        0.5060, 0.5048, 0.5066, 0.5050, 0.5080, 0.5091, 0.5108],
       device='cuda:0') torch.Size([16])
percent tensor([0.5520, 0.5575, 0.4337, 0.4097, 0.4233, 0.5045, 0.5246, 0.4547, 0.4696,
        0.5326, 0.5473, 0.4677, 0.5493, 0.5311, 0.5335, 0.5357],
       device='cuda:0') torch.Size([16])
percent tensor([0.5968, 0.4909, 0.5919, 0.6152, 0.6096, 0.6357, 0.5527, 0.6141, 0.5735,
        0.5010, 0.5368, 0.5324, 0.4830, 0.5981, 0.5661, 0.6042],
       device='cuda:0') torch.Size([16])
percent tensor([0.5875, 0.5831, 0.5952, 0.5649, 0.5663, 0.4936, 0.6046, 0.6052, 0.5864,
        0.5756, 0.5740, 0.6085, 0.5924, 0.5645, 0.6097, 0.5376],
       device='cuda:0') torch.Size([16])
percent tensor([0.5458, 0.5753, 0.6306, 0.6471, 0.6352, 0.5926, 0.6094, 0.5563, 0.6450,
        0.5514, 0.6152, 0.6822, 0.5843, 0.6668, 0.5783, 0.5466],
       device='cuda:0') torch.Size([16])
percent tensor([0.5637, 0.5400, 0.4388, 0.4353, 0.4845, 0.5595, 0.5017, 0.3978, 0.5366,
        0.5499, 0.5408, 0.4029, 0.5658, 0.5443, 0.4040, 0.5500],
       device='cuda:0') torch.Size([16])
percent tensor([0.9996, 0.9982, 0.9996, 0.9994, 0.9996, 0.9971, 0.9992, 0.9994, 0.9995,
        0.9988, 0.9987, 0.9996, 0.9993, 0.9987, 0.9991, 0.9991],
       device='cuda:0') torch.Size([16])
Epoch: 53 | Batch_idx: 0 |  Loss: (0.5084) |  Loss2: (0.0000) | Acc: (82.00%) (106/128)
Epoch: 53 | Batch_idx: 10 |  Loss: (0.4043) |  Loss2: (0.0000) | Acc: (85.00%) (1206/1408)
Epoch: 53 | Batch_idx: 20 |  Loss: (0.3972) |  Loss2: (0.0000) | Acc: (86.00%) (2319/2688)
Epoch: 53 | Batch_idx: 30 |  Loss: (0.4018) |  Loss2: (0.0000) | Acc: (86.00%) (3430/3968)
Epoch: 53 | Batch_idx: 40 |  Loss: (0.3976) |  Loss2: (0.0000) | Acc: (86.00%) (4548/5248)
Epoch: 53 | Batch_idx: 50 |  Loss: (0.3898) |  Loss2: (0.0000) | Acc: (86.00%) (5675/6528)
Epoch: 53 | Batch_idx: 60 |  Loss: (0.3897) |  Loss2: (0.0000) | Acc: (86.00%) (6781/7808)
Epoch: 53 | Batch_idx: 70 |  Loss: (0.3859) |  Loss2: (0.0000) | Acc: (86.00%) (7902/9088)
Epoch: 53 | Batch_idx: 80 |  Loss: (0.3842) |  Loss2: (0.0000) | Acc: (87.00%) (9032/10368)
Epoch: 53 | Batch_idx: 90 |  Loss: (0.3812) |  Loss2: (0.0000) | Acc: (87.00%) (10159/11648)
Epoch: 53 | Batch_idx: 100 |  Loss: (0.3827) |  Loss2: (0.0000) | Acc: (87.00%) (11274/12928)
Epoch: 53 | Batch_idx: 110 |  Loss: (0.3820) |  Loss2: (0.0000) | Acc: (87.00%) (12389/14208)
Epoch: 53 | Batch_idx: 120 |  Loss: (0.3806) |  Loss2: (0.0000) | Acc: (87.00%) (13490/15488)
Epoch: 53 | Batch_idx: 130 |  Loss: (0.3835) |  Loss2: (0.0000) | Acc: (86.00%) (14576/16768)
Epoch: 53 | Batch_idx: 140 |  Loss: (0.3845) |  Loss2: (0.0000) | Acc: (86.00%) (15676/18048)
Epoch: 53 | Batch_idx: 150 |  Loss: (0.3853) |  Loss2: (0.0000) | Acc: (86.00%) (16773/19328)
Epoch: 53 | Batch_idx: 160 |  Loss: (0.3849) |  Loss2: (0.0000) | Acc: (86.00%) (17888/20608)
Epoch: 53 | Batch_idx: 170 |  Loss: (0.3851) |  Loss2: (0.0000) | Acc: (86.00%) (18994/21888)
Epoch: 53 | Batch_idx: 180 |  Loss: (0.3844) |  Loss2: (0.0000) | Acc: (86.00%) (20109/23168)
Epoch: 53 | Batch_idx: 190 |  Loss: (0.3856) |  Loss2: (0.0000) | Acc: (86.00%) (21206/24448)
Epoch: 53 | Batch_idx: 200 |  Loss: (0.3857) |  Loss2: (0.0000) | Acc: (86.00%) (22311/25728)
Epoch: 53 | Batch_idx: 210 |  Loss: (0.3849) |  Loss2: (0.0000) | Acc: (86.00%) (23423/27008)
Epoch: 53 | Batch_idx: 220 |  Loss: (0.3850) |  Loss2: (0.0000) | Acc: (86.00%) (24540/28288)
Epoch: 53 | Batch_idx: 230 |  Loss: (0.3867) |  Loss2: (0.0000) | Acc: (86.00%) (25635/29568)
Epoch: 53 | Batch_idx: 240 |  Loss: (0.3867) |  Loss2: (0.0000) | Acc: (86.00%) (26742/30848)
Epoch: 53 | Batch_idx: 250 |  Loss: (0.3881) |  Loss2: (0.0000) | Acc: (86.00%) (27833/32128)
Epoch: 53 | Batch_idx: 260 |  Loss: (0.3871) |  Loss2: (0.0000) | Acc: (86.00%) (28954/33408)
Epoch: 53 | Batch_idx: 270 |  Loss: (0.3872) |  Loss2: (0.0000) | Acc: (86.00%) (30066/34688)
Epoch: 53 | Batch_idx: 280 |  Loss: (0.3874) |  Loss2: (0.0000) | Acc: (86.00%) (31171/35968)
Epoch: 53 | Batch_idx: 290 |  Loss: (0.3874) |  Loss2: (0.0000) | Acc: (86.00%) (32264/37248)
Epoch: 53 | Batch_idx: 300 |  Loss: (0.3890) |  Loss2: (0.0000) | Acc: (86.00%) (33345/38528)
Epoch: 53 | Batch_idx: 310 |  Loss: (0.3894) |  Loss2: (0.0000) | Acc: (86.00%) (34448/39808)
Epoch: 53 | Batch_idx: 320 |  Loss: (0.3887) |  Loss2: (0.0000) | Acc: (86.00%) (35564/41088)
Epoch: 53 | Batch_idx: 330 |  Loss: (0.3888) |  Loss2: (0.0000) | Acc: (86.00%) (36666/42368)
Epoch: 53 | Batch_idx: 340 |  Loss: (0.3891) |  Loss2: (0.0000) | Acc: (86.00%) (37784/43648)
Epoch: 53 | Batch_idx: 350 |  Loss: (0.3892) |  Loss2: (0.0000) | Acc: (86.00%) (38883/44928)
Epoch: 53 | Batch_idx: 360 |  Loss: (0.3894) |  Loss2: (0.0000) | Acc: (86.00%) (39990/46208)
Epoch: 53 | Batch_idx: 370 |  Loss: (0.3895) |  Loss2: (0.0000) | Acc: (86.00%) (41098/47488)
Epoch: 53 | Batch_idx: 380 |  Loss: (0.3884) |  Loss2: (0.0000) | Acc: (86.00%) (42218/48768)
Epoch: 53 | Batch_idx: 390 |  Loss: (0.3884) |  Loss2: (0.0000) | Acc: (86.00%) (43300/50000)
# TEST : Loss: (0.5286) | Acc: (82.00%) (8232/10000)
percent tensor([0.5180, 0.5267, 0.5228, 0.5062, 0.5256, 0.5291, 0.5289, 0.5168, 0.5155,
        0.5253, 0.5232, 0.5256, 0.5168, 0.5069, 0.5279, 0.5172],
       device='cuda:0') torch.Size([16])
percent tensor([0.5087, 0.5062, 0.5110, 0.5135, 0.5118, 0.5139, 0.5093, 0.5117, 0.5079,
        0.5067, 0.5050, 0.5072, 0.5053, 0.5083, 0.5099, 0.5114],
       device='cuda:0') torch.Size([16])
percent tensor([0.5485, 0.5643, 0.4189, 0.4161, 0.4165, 0.5098, 0.5272, 0.4550, 0.4708,
        0.5390, 0.5512, 0.4610, 0.5478, 0.5349, 0.5422, 0.5419],
       device='cuda:0') torch.Size([16])
percent tensor([0.6115, 0.4950, 0.6116, 0.6331, 0.6210, 0.6523, 0.5618, 0.6307, 0.5926,
        0.5048, 0.5477, 0.5503, 0.4933, 0.6022, 0.5858, 0.6172],
       device='cuda:0') torch.Size([16])
percent tensor([0.5701, 0.5861, 0.5869, 0.5551, 0.5536, 0.4834, 0.5976, 0.5940, 0.5773,
        0.5790, 0.5721, 0.6063, 0.5783, 0.5695, 0.6028, 0.5218],
       device='cuda:0') torch.Size([16])
percent tensor([0.5340, 0.5507, 0.6171, 0.6522, 0.6265, 0.6074, 0.5737, 0.5498, 0.6203,
        0.5156, 0.5964, 0.6606, 0.5497, 0.6570, 0.5546, 0.5481],
       device='cuda:0') torch.Size([16])
percent tensor([0.5743, 0.5753, 0.4599, 0.4201, 0.4975, 0.5538, 0.5258, 0.4081, 0.5587,
        0.5788, 0.5662, 0.4272, 0.6002, 0.5499, 0.4275, 0.5731],
       device='cuda:0') torch.Size([16])
percent tensor([0.9995, 0.9987, 0.9995, 0.9993, 0.9996, 0.9974, 0.9994, 0.9995, 0.9987,
        0.9990, 0.9987, 0.9997, 0.9990, 0.9990, 0.9990, 0.9991],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 54 | Batch_idx: 0 |  Loss: (0.2803) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 54 | Batch_idx: 10 |  Loss: (0.3772) |  Loss2: (0.0000) | Acc: (87.00%) (1229/1408)
Epoch: 54 | Batch_idx: 20 |  Loss: (0.3758) |  Loss2: (0.0000) | Acc: (87.00%) (2351/2688)
Epoch: 54 | Batch_idx: 30 |  Loss: (0.4074) |  Loss2: (0.0000) | Acc: (86.00%) (3421/3968)
Epoch: 54 | Batch_idx: 40 |  Loss: (0.4123) |  Loss2: (0.0000) | Acc: (85.00%) (4498/5248)
Epoch: 54 | Batch_idx: 50 |  Loss: (0.4146) |  Loss2: (0.0000) | Acc: (85.00%) (5597/6528)
Epoch: 54 | Batch_idx: 60 |  Loss: (0.4191) |  Loss2: (0.0000) | Acc: (85.00%) (6682/7808)
Epoch: 54 | Batch_idx: 70 |  Loss: (0.4196) |  Loss2: (0.0000) | Acc: (85.00%) (7770/9088)
Epoch: 54 | Batch_idx: 80 |  Loss: (0.4197) |  Loss2: (0.0000) | Acc: (85.00%) (8855/10368)
Epoch: 54 | Batch_idx: 90 |  Loss: (0.4216) |  Loss2: (0.0000) | Acc: (85.00%) (9941/11648)
Epoch: 54 | Batch_idx: 100 |  Loss: (0.4220) |  Loss2: (0.0000) | Acc: (85.00%) (11034/12928)
Epoch: 54 | Batch_idx: 110 |  Loss: (0.4200) |  Loss2: (0.0000) | Acc: (85.00%) (12144/14208)
Epoch: 54 | Batch_idx: 120 |  Loss: (0.4186) |  Loss2: (0.0000) | Acc: (85.00%) (13240/15488)
Epoch: 54 | Batch_idx: 130 |  Loss: (0.4181) |  Loss2: (0.0000) | Acc: (85.00%) (14340/16768)
Epoch: 54 | Batch_idx: 140 |  Loss: (0.4191) |  Loss2: (0.0000) | Acc: (85.00%) (15430/18048)
Epoch: 54 | Batch_idx: 150 |  Loss: (0.4188) |  Loss2: (0.0000) | Acc: (85.00%) (16532/19328)
Epoch: 54 | Batch_idx: 160 |  Loss: (0.4164) |  Loss2: (0.0000) | Acc: (85.00%) (17645/20608)
Epoch: 54 | Batch_idx: 170 |  Loss: (0.4159) |  Loss2: (0.0000) | Acc: (85.00%) (18748/21888)
Epoch: 54 | Batch_idx: 180 |  Loss: (0.4177) |  Loss2: (0.0000) | Acc: (85.00%) (19835/23168)
Epoch: 54 | Batch_idx: 190 |  Loss: (0.4179) |  Loss2: (0.0000) | Acc: (85.00%) (20930/24448)
Epoch: 54 | Batch_idx: 200 |  Loss: (0.4174) |  Loss2: (0.0000) | Acc: (85.00%) (22028/25728)
Epoch: 54 | Batch_idx: 210 |  Loss: (0.4164) |  Loss2: (0.0000) | Acc: (85.00%) (23135/27008)
Epoch: 54 | Batch_idx: 220 |  Loss: (0.4163) |  Loss2: (0.0000) | Acc: (85.00%) (24232/28288)
Epoch: 54 | Batch_idx: 230 |  Loss: (0.4154) |  Loss2: (0.0000) | Acc: (85.00%) (25336/29568)
Epoch: 54 | Batch_idx: 240 |  Loss: (0.4144) |  Loss2: (0.0000) | Acc: (85.00%) (26424/30848)
Epoch: 54 | Batch_idx: 250 |  Loss: (0.4128) |  Loss2: (0.0000) | Acc: (85.00%) (27536/32128)
Epoch: 54 | Batch_idx: 260 |  Loss: (0.4119) |  Loss2: (0.0000) | Acc: (85.00%) (28636/33408)
Epoch: 54 | Batch_idx: 270 |  Loss: (0.4110) |  Loss2: (0.0000) | Acc: (85.00%) (29743/34688)
Epoch: 54 | Batch_idx: 280 |  Loss: (0.4107) |  Loss2: (0.0000) | Acc: (85.00%) (30837/35968)
Epoch: 54 | Batch_idx: 290 |  Loss: (0.4111) |  Loss2: (0.0000) | Acc: (85.00%) (31910/37248)
Epoch: 54 | Batch_idx: 300 |  Loss: (0.4114) |  Loss2: (0.0000) | Acc: (85.00%) (33008/38528)
Epoch: 54 | Batch_idx: 310 |  Loss: (0.4110) |  Loss2: (0.0000) | Acc: (85.00%) (34109/39808)
Epoch: 54 | Batch_idx: 320 |  Loss: (0.4118) |  Loss2: (0.0000) | Acc: (85.00%) (35196/41088)
Epoch: 54 | Batch_idx: 330 |  Loss: (0.4118) |  Loss2: (0.0000) | Acc: (85.00%) (36290/42368)
Epoch: 54 | Batch_idx: 340 |  Loss: (0.4120) |  Loss2: (0.0000) | Acc: (85.00%) (37386/43648)
Epoch: 54 | Batch_idx: 350 |  Loss: (0.4120) |  Loss2: (0.0000) | Acc: (85.00%) (38484/44928)
Epoch: 54 | Batch_idx: 360 |  Loss: (0.4120) |  Loss2: (0.0000) | Acc: (85.00%) (39571/46208)
Epoch: 54 | Batch_idx: 370 |  Loss: (0.4115) |  Loss2: (0.0000) | Acc: (85.00%) (40669/47488)
Epoch: 54 | Batch_idx: 380 |  Loss: (0.4113) |  Loss2: (0.0000) | Acc: (85.00%) (41769/48768)
Epoch: 54 | Batch_idx: 390 |  Loss: (0.4107) |  Loss2: (0.0000) | Acc: (85.00%) (42830/50000)
# TEST : Loss: (0.4824) | Acc: (83.00%) (8330/10000)
percent tensor([0.5314, 0.5423, 0.5411, 0.5169, 0.5441, 0.5429, 0.5468, 0.5321, 0.5302,
        0.5425, 0.5378, 0.5454, 0.5309, 0.5170, 0.5432, 0.5301],
       device='cuda:0') torch.Size([16])
percent tensor([0.5109, 0.5094, 0.5115, 0.5143, 0.5125, 0.5148, 0.5114, 0.5130, 0.5104,
        0.5095, 0.5085, 0.5081, 0.5082, 0.5113, 0.5118, 0.5131],
       device='cuda:0') torch.Size([16])
percent tensor([0.5511, 0.5623, 0.4385, 0.4304, 0.4278, 0.5181, 0.5316, 0.4580, 0.4854,
        0.5483, 0.5576, 0.4777, 0.5574, 0.5432, 0.5401, 0.5489],
       device='cuda:0') torch.Size([16])
percent tensor([0.5918, 0.4687, 0.5966, 0.6279, 0.6066, 0.6472, 0.5361, 0.6272, 0.5787,
        0.4806, 0.5240, 0.5298, 0.4669, 0.5844, 0.5716, 0.6022],
       device='cuda:0') torch.Size([16])
percent tensor([0.5571, 0.5664, 0.5687, 0.5490, 0.5415, 0.4831, 0.5762, 0.5697, 0.5648,
        0.5580, 0.5606, 0.5818, 0.5599, 0.5619, 0.5811, 0.5085],
       device='cuda:0') torch.Size([16])
percent tensor([0.5438, 0.5561, 0.6284, 0.6669, 0.6311, 0.6217, 0.5886, 0.5656, 0.6347,
        0.5251, 0.6028, 0.6721, 0.5561, 0.6662, 0.5686, 0.5635],
       device='cuda:0') torch.Size([16])
percent tensor([0.5459, 0.5375, 0.4781, 0.4272, 0.5261, 0.5529, 0.5177, 0.4304, 0.5283,
        0.5573, 0.5345, 0.4289, 0.5553, 0.5289, 0.4306, 0.5624],
       device='cuda:0') torch.Size([16])
percent tensor([0.9994, 0.9989, 0.9995, 0.9994, 0.9995, 0.9969, 0.9993, 0.9994, 0.9989,
        0.9989, 0.9988, 0.9997, 0.9990, 0.9992, 0.9989, 0.9992],
       device='cuda:0') torch.Size([16])
Epoch: 55 | Batch_idx: 0 |  Loss: (0.4134) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 55 | Batch_idx: 10 |  Loss: (0.3672) |  Loss2: (0.0000) | Acc: (88.00%) (1241/1408)
Epoch: 55 | Batch_idx: 20 |  Loss: (0.3701) |  Loss2: (0.0000) | Acc: (87.00%) (2356/2688)
Epoch: 55 | Batch_idx: 30 |  Loss: (0.3800) |  Loss2: (0.0000) | Acc: (87.00%) (3467/3968)
Epoch: 55 | Batch_idx: 40 |  Loss: (0.3856) |  Loss2: (0.0000) | Acc: (86.00%) (4561/5248)
Epoch: 55 | Batch_idx: 50 |  Loss: (0.3890) |  Loss2: (0.0000) | Acc: (86.00%) (5667/6528)
Epoch: 55 | Batch_idx: 60 |  Loss: (0.3939) |  Loss2: (0.0000) | Acc: (86.00%) (6750/7808)
Epoch: 55 | Batch_idx: 70 |  Loss: (0.3947) |  Loss2: (0.0000) | Acc: (86.00%) (7848/9088)
Epoch: 55 | Batch_idx: 80 |  Loss: (0.3973) |  Loss2: (0.0000) | Acc: (86.00%) (8942/10368)
Epoch: 55 | Batch_idx: 90 |  Loss: (0.3966) |  Loss2: (0.0000) | Acc: (86.00%) (10049/11648)
Epoch: 55 | Batch_idx: 100 |  Loss: (0.3965) |  Loss2: (0.0000) | Acc: (86.00%) (11154/12928)
Epoch: 55 | Batch_idx: 110 |  Loss: (0.3964) |  Loss2: (0.0000) | Acc: (86.00%) (12279/14208)
Epoch: 55 | Batch_idx: 120 |  Loss: (0.3979) |  Loss2: (0.0000) | Acc: (86.00%) (13375/15488)
Epoch: 55 | Batch_idx: 130 |  Loss: (0.3970) |  Loss2: (0.0000) | Acc: (86.00%) (14478/16768)
Epoch: 55 | Batch_idx: 140 |  Loss: (0.3937) |  Loss2: (0.0000) | Acc: (86.00%) (15605/18048)
Epoch: 55 | Batch_idx: 150 |  Loss: (0.3932) |  Loss2: (0.0000) | Acc: (86.00%) (16708/19328)
Epoch: 55 | Batch_idx: 160 |  Loss: (0.3918) |  Loss2: (0.0000) | Acc: (86.00%) (17818/20608)
Epoch: 55 | Batch_idx: 170 |  Loss: (0.3893) |  Loss2: (0.0000) | Acc: (86.00%) (18950/21888)
Epoch: 55 | Batch_idx: 180 |  Loss: (0.3897) |  Loss2: (0.0000) | Acc: (86.00%) (20062/23168)
Epoch: 55 | Batch_idx: 190 |  Loss: (0.3880) |  Loss2: (0.0000) | Acc: (86.00%) (21194/24448)
Epoch: 55 | Batch_idx: 200 |  Loss: (0.3876) |  Loss2: (0.0000) | Acc: (86.00%) (22299/25728)
Epoch: 55 | Batch_idx: 210 |  Loss: (0.3865) |  Loss2: (0.0000) | Acc: (86.00%) (23422/27008)
Epoch: 55 | Batch_idx: 220 |  Loss: (0.3868) |  Loss2: (0.0000) | Acc: (86.00%) (24526/28288)
Epoch: 55 | Batch_idx: 230 |  Loss: (0.3870) |  Loss2: (0.0000) | Acc: (86.00%) (25635/29568)
Epoch: 55 | Batch_idx: 240 |  Loss: (0.3850) |  Loss2: (0.0000) | Acc: (86.00%) (26774/30848)
Epoch: 55 | Batch_idx: 250 |  Loss: (0.3833) |  Loss2: (0.0000) | Acc: (86.00%) (27915/32128)
Epoch: 55 | Batch_idx: 260 |  Loss: (0.3832) |  Loss2: (0.0000) | Acc: (86.00%) (29041/33408)
Epoch: 55 | Batch_idx: 270 |  Loss: (0.3836) |  Loss2: (0.0000) | Acc: (86.00%) (30139/34688)
Epoch: 55 | Batch_idx: 280 |  Loss: (0.3832) |  Loss2: (0.0000) | Acc: (86.00%) (31253/35968)
Epoch: 55 | Batch_idx: 290 |  Loss: (0.3836) |  Loss2: (0.0000) | Acc: (86.00%) (32359/37248)
Epoch: 55 | Batch_idx: 300 |  Loss: (0.3826) |  Loss2: (0.0000) | Acc: (86.00%) (33484/38528)
Epoch: 55 | Batch_idx: 310 |  Loss: (0.3830) |  Loss2: (0.0000) | Acc: (86.00%) (34588/39808)
Epoch: 55 | Batch_idx: 320 |  Loss: (0.3836) |  Loss2: (0.0000) | Acc: (86.00%) (35691/41088)
Epoch: 55 | Batch_idx: 330 |  Loss: (0.3844) |  Loss2: (0.0000) | Acc: (86.00%) (36784/42368)
Epoch: 55 | Batch_idx: 340 |  Loss: (0.3836) |  Loss2: (0.0000) | Acc: (86.00%) (37899/43648)
Epoch: 55 | Batch_idx: 350 |  Loss: (0.3833) |  Loss2: (0.0000) | Acc: (86.00%) (39008/44928)
Epoch: 55 | Batch_idx: 360 |  Loss: (0.3828) |  Loss2: (0.0000) | Acc: (86.00%) (40112/46208)
Epoch: 55 | Batch_idx: 370 |  Loss: (0.3832) |  Loss2: (0.0000) | Acc: (86.00%) (41208/47488)
Epoch: 55 | Batch_idx: 380 |  Loss: (0.3836) |  Loss2: (0.0000) | Acc: (86.00%) (42314/48768)
Epoch: 55 | Batch_idx: 390 |  Loss: (0.3840) |  Loss2: (0.0000) | Acc: (86.00%) (43378/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_055.pth.tar'
# TEST : Loss: (0.4654) | Acc: (83.00%) (8393/10000)
percent tensor([0.5335, 0.5452, 0.5450, 0.5178, 0.5476, 0.5441, 0.5503, 0.5348, 0.5328,
        0.5463, 0.5408, 0.5502, 0.5336, 0.5178, 0.5456, 0.5318],
       device='cuda:0') torch.Size([16])
percent tensor([0.5120, 0.5106, 0.5128, 0.5155, 0.5137, 0.5156, 0.5125, 0.5141, 0.5115,
        0.5107, 0.5097, 0.5093, 0.5094, 0.5124, 0.5128, 0.5141],
       device='cuda:0') torch.Size([16])
percent tensor([0.5466, 0.5613, 0.4393, 0.4299, 0.4276, 0.5149, 0.5286, 0.4551, 0.4856,
        0.5484, 0.5561, 0.4804, 0.5570, 0.5408, 0.5349, 0.5453],
       device='cuda:0') torch.Size([16])
percent tensor([0.5924, 0.4645, 0.6078, 0.6379, 0.6165, 0.6523, 0.5350, 0.6351, 0.5821,
        0.4795, 0.5209, 0.5369, 0.4672, 0.5793, 0.5712, 0.6040],
       device='cuda:0') torch.Size([16])
percent tensor([0.5646, 0.5693, 0.5693, 0.5528, 0.5443, 0.4907, 0.5827, 0.5738, 0.5717,
        0.5618, 0.5679, 0.5833, 0.5624, 0.5729, 0.5869, 0.5151],
       device='cuda:0') torch.Size([16])
percent tensor([0.5474, 0.5640, 0.6315, 0.6698, 0.6376, 0.6284, 0.5928, 0.5709, 0.6393,
        0.5326, 0.6092, 0.6760, 0.5631, 0.6668, 0.5731, 0.5710],
       device='cuda:0') torch.Size([16])
percent tensor([0.5623, 0.5478, 0.4984, 0.4498, 0.5458, 0.5743, 0.5308, 0.4484, 0.5432,
        0.5677, 0.5471, 0.4423, 0.5683, 0.5507, 0.4435, 0.5808],
       device='cuda:0') torch.Size([16])
percent tensor([0.9996, 0.9990, 0.9994, 0.9995, 0.9995, 0.9973, 0.9994, 0.9994, 0.9991,
        0.9989, 0.9990, 0.9997, 0.9992, 0.9992, 0.9989, 0.9993],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 56 | Batch_idx: 0 |  Loss: (0.3476) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 56 | Batch_idx: 10 |  Loss: (0.3754) |  Loss2: (0.0000) | Acc: (87.00%) (1225/1408)
Epoch: 56 | Batch_idx: 20 |  Loss: (0.3777) |  Loss2: (0.0000) | Acc: (86.00%) (2318/2688)
Epoch: 56 | Batch_idx: 30 |  Loss: (0.3714) |  Loss2: (0.0000) | Acc: (86.00%) (3447/3968)
Epoch: 56 | Batch_idx: 40 |  Loss: (0.3643) |  Loss2: (0.0000) | Acc: (86.00%) (4555/5248)
Epoch: 56 | Batch_idx: 50 |  Loss: (0.3638) |  Loss2: (0.0000) | Acc: (86.00%) (5663/6528)
Epoch: 56 | Batch_idx: 60 |  Loss: (0.3627) |  Loss2: (0.0000) | Acc: (86.00%) (6784/7808)
Epoch: 56 | Batch_idx: 70 |  Loss: (0.3680) |  Loss2: (0.0000) | Acc: (86.00%) (7892/9088)
Epoch: 56 | Batch_idx: 80 |  Loss: (0.3647) |  Loss2: (0.0000) | Acc: (86.00%) (9014/10368)
Epoch: 56 | Batch_idx: 90 |  Loss: (0.3680) |  Loss2: (0.0000) | Acc: (86.00%) (10110/11648)
Epoch: 56 | Batch_idx: 100 |  Loss: (0.3687) |  Loss2: (0.0000) | Acc: (86.00%) (11227/12928)
Epoch: 56 | Batch_idx: 110 |  Loss: (0.3663) |  Loss2: (0.0000) | Acc: (87.00%) (12362/14208)
Epoch: 56 | Batch_idx: 120 |  Loss: (0.3647) |  Loss2: (0.0000) | Acc: (87.00%) (13489/15488)
Epoch: 56 | Batch_idx: 130 |  Loss: (0.3650) |  Loss2: (0.0000) | Acc: (87.00%) (14609/16768)
Epoch: 56 | Batch_idx: 140 |  Loss: (0.3658) |  Loss2: (0.0000) | Acc: (87.00%) (15734/18048)
Epoch: 56 | Batch_idx: 150 |  Loss: (0.3629) |  Loss2: (0.0000) | Acc: (87.00%) (16881/19328)
Epoch: 56 | Batch_idx: 160 |  Loss: (0.3643) |  Loss2: (0.0000) | Acc: (87.00%) (17997/20608)
Epoch: 56 | Batch_idx: 170 |  Loss: (0.3648) |  Loss2: (0.0000) | Acc: (87.00%) (19109/21888)
Epoch: 56 | Batch_idx: 180 |  Loss: (0.3651) |  Loss2: (0.0000) | Acc: (87.00%) (20230/23168)
Epoch: 56 | Batch_idx: 190 |  Loss: (0.3673) |  Loss2: (0.0000) | Acc: (87.00%) (21328/24448)
Epoch: 56 | Batch_idx: 200 |  Loss: (0.3676) |  Loss2: (0.0000) | Acc: (87.00%) (22448/25728)
Epoch: 56 | Batch_idx: 210 |  Loss: (0.3679) |  Loss2: (0.0000) | Acc: (87.00%) (23562/27008)
Epoch: 56 | Batch_idx: 220 |  Loss: (0.3697) |  Loss2: (0.0000) | Acc: (87.00%) (24670/28288)
Epoch: 56 | Batch_idx: 230 |  Loss: (0.3694) |  Loss2: (0.0000) | Acc: (87.00%) (25773/29568)
Epoch: 56 | Batch_idx: 240 |  Loss: (0.3698) |  Loss2: (0.0000) | Acc: (87.00%) (26887/30848)
Epoch: 56 | Batch_idx: 250 |  Loss: (0.3696) |  Loss2: (0.0000) | Acc: (87.00%) (28006/32128)
Epoch: 56 | Batch_idx: 260 |  Loss: (0.3704) |  Loss2: (0.0000) | Acc: (87.00%) (29113/33408)
Epoch: 56 | Batch_idx: 270 |  Loss: (0.3698) |  Loss2: (0.0000) | Acc: (87.00%) (30238/34688)
Epoch: 56 | Batch_idx: 280 |  Loss: (0.3699) |  Loss2: (0.0000) | Acc: (87.00%) (31344/35968)
Epoch: 56 | Batch_idx: 290 |  Loss: (0.3705) |  Loss2: (0.0000) | Acc: (87.00%) (32460/37248)
Epoch: 56 | Batch_idx: 300 |  Loss: (0.3716) |  Loss2: (0.0000) | Acc: (87.00%) (33559/38528)
Epoch: 56 | Batch_idx: 310 |  Loss: (0.3719) |  Loss2: (0.0000) | Acc: (87.00%) (34665/39808)
Epoch: 56 | Batch_idx: 320 |  Loss: (0.3722) |  Loss2: (0.0000) | Acc: (87.00%) (35753/41088)
Epoch: 56 | Batch_idx: 330 |  Loss: (0.3726) |  Loss2: (0.0000) | Acc: (87.00%) (36873/42368)
Epoch: 56 | Batch_idx: 340 |  Loss: (0.3721) |  Loss2: (0.0000) | Acc: (87.00%) (37998/43648)
Epoch: 56 | Batch_idx: 350 |  Loss: (0.3728) |  Loss2: (0.0000) | Acc: (87.00%) (39113/44928)
Epoch: 56 | Batch_idx: 360 |  Loss: (0.3728) |  Loss2: (0.0000) | Acc: (87.00%) (40230/46208)
Epoch: 56 | Batch_idx: 370 |  Loss: (0.3741) |  Loss2: (0.0000) | Acc: (87.00%) (41327/47488)
Epoch: 56 | Batch_idx: 380 |  Loss: (0.3742) |  Loss2: (0.0000) | Acc: (87.00%) (42454/48768)
Epoch: 56 | Batch_idx: 390 |  Loss: (0.3736) |  Loss2: (0.0000) | Acc: (87.00%) (43538/50000)
# TEST : Loss: (0.4949) | Acc: (83.00%) (8360/10000)
percent tensor([0.5327, 0.5483, 0.5364, 0.5164, 0.5407, 0.5439, 0.5504, 0.5320, 0.5317,
        0.5453, 0.5416, 0.5442, 0.5325, 0.5252, 0.5466, 0.5322],
       device='cuda:0') torch.Size([16])
percent tensor([0.5125, 0.5108, 0.5142, 0.5163, 0.5149, 0.5160, 0.5129, 0.5141, 0.5111,
        0.5109, 0.5094, 0.5108, 0.5093, 0.5126, 0.5129, 0.5144],
       device='cuda:0') torch.Size([16])
percent tensor([0.5575, 0.5655, 0.4504, 0.4372, 0.4350, 0.5251, 0.5377, 0.4665, 0.4937,
        0.5527, 0.5662, 0.4908, 0.5655, 0.5518, 0.5418, 0.5522],
       device='cuda:0') torch.Size([16])
percent tensor([0.5987, 0.4687, 0.6307, 0.6361, 0.6369, 0.6523, 0.5525, 0.6320, 0.5868,
        0.4850, 0.5280, 0.5565, 0.4770, 0.5728, 0.5612, 0.6040],
       device='cuda:0') torch.Size([16])
percent tensor([0.5602, 0.5612, 0.5553, 0.5471, 0.5334, 0.4890, 0.5787, 0.5770, 0.5713,
        0.5520, 0.5578, 0.5714, 0.5596, 0.5685, 0.5843, 0.5118],
       device='cuda:0') torch.Size([16])
percent tensor([0.5582, 0.5796, 0.6463, 0.6716, 0.6616, 0.6323, 0.6119, 0.5744, 0.6318,
        0.5529, 0.6076, 0.6801, 0.5713, 0.6512, 0.5842, 0.5898],
       device='cuda:0') torch.Size([16])
percent tensor([0.5685, 0.5547, 0.4855, 0.4818, 0.5377, 0.5972, 0.5307, 0.4297, 0.5525,
        0.5656, 0.5643, 0.4415, 0.5661, 0.5809, 0.4456, 0.5827],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9987, 0.9995, 0.9994, 0.9992, 0.9978, 0.9991, 0.9995, 0.9992,
        0.9989, 0.9993, 0.9995, 0.9994, 0.9987, 0.9986, 0.9991],
       device='cuda:0') torch.Size([16])
Epoch: 57 | Batch_idx: 0 |  Loss: (0.4146) |  Loss2: (0.0000) | Acc: (85.00%) (110/128)
Epoch: 57 | Batch_idx: 10 |  Loss: (0.3705) |  Loss2: (0.0000) | Acc: (87.00%) (1228/1408)
Epoch: 57 | Batch_idx: 20 |  Loss: (0.3773) |  Loss2: (0.0000) | Acc: (86.00%) (2337/2688)
Epoch: 57 | Batch_idx: 30 |  Loss: (0.3658) |  Loss2: (0.0000) | Acc: (87.00%) (3466/3968)
Epoch: 57 | Batch_idx: 40 |  Loss: (0.3559) |  Loss2: (0.0000) | Acc: (87.00%) (4607/5248)
Epoch: 57 | Batch_idx: 50 |  Loss: (0.3548) |  Loss2: (0.0000) | Acc: (87.00%) (5732/6528)
Epoch: 57 | Batch_idx: 60 |  Loss: (0.3581) |  Loss2: (0.0000) | Acc: (87.00%) (6860/7808)
Epoch: 57 | Batch_idx: 70 |  Loss: (0.3622) |  Loss2: (0.0000) | Acc: (87.00%) (7979/9088)
Epoch: 57 | Batch_idx: 80 |  Loss: (0.3612) |  Loss2: (0.0000) | Acc: (87.00%) (9105/10368)
Epoch: 57 | Batch_idx: 90 |  Loss: (0.3611) |  Loss2: (0.0000) | Acc: (87.00%) (10219/11648)
Epoch: 57 | Batch_idx: 100 |  Loss: (0.3629) |  Loss2: (0.0000) | Acc: (87.00%) (11342/12928)
Epoch: 57 | Batch_idx: 110 |  Loss: (0.3635) |  Loss2: (0.0000) | Acc: (87.00%) (12466/14208)
Epoch: 57 | Batch_idx: 120 |  Loss: (0.3631) |  Loss2: (0.0000) | Acc: (87.00%) (13584/15488)
Epoch: 57 | Batch_idx: 130 |  Loss: (0.3626) |  Loss2: (0.0000) | Acc: (87.00%) (14718/16768)
Epoch: 57 | Batch_idx: 140 |  Loss: (0.3641) |  Loss2: (0.0000) | Acc: (87.00%) (15829/18048)
Epoch: 57 | Batch_idx: 150 |  Loss: (0.3648) |  Loss2: (0.0000) | Acc: (87.00%) (16937/19328)
Epoch: 57 | Batch_idx: 160 |  Loss: (0.3657) |  Loss2: (0.0000) | Acc: (87.00%) (18040/20608)
Epoch: 57 | Batch_idx: 170 |  Loss: (0.3646) |  Loss2: (0.0000) | Acc: (87.00%) (19167/21888)
Epoch: 57 | Batch_idx: 180 |  Loss: (0.3638) |  Loss2: (0.0000) | Acc: (87.00%) (20296/23168)
Epoch: 57 | Batch_idx: 190 |  Loss: (0.3626) |  Loss2: (0.0000) | Acc: (87.00%) (21426/24448)
Epoch: 57 | Batch_idx: 200 |  Loss: (0.3605) |  Loss2: (0.0000) | Acc: (87.00%) (22570/25728)
Epoch: 57 | Batch_idx: 210 |  Loss: (0.3605) |  Loss2: (0.0000) | Acc: (87.00%) (23702/27008)
Epoch: 57 | Batch_idx: 220 |  Loss: (0.3602) |  Loss2: (0.0000) | Acc: (87.00%) (24828/28288)
Epoch: 57 | Batch_idx: 230 |  Loss: (0.3597) |  Loss2: (0.0000) | Acc: (87.00%) (25943/29568)
Epoch: 57 | Batch_idx: 240 |  Loss: (0.3595) |  Loss2: (0.0000) | Acc: (87.00%) (27070/30848)
Epoch: 57 | Batch_idx: 250 |  Loss: (0.3581) |  Loss2: (0.0000) | Acc: (87.00%) (28214/32128)
Epoch: 57 | Batch_idx: 260 |  Loss: (0.3586) |  Loss2: (0.0000) | Acc: (87.00%) (29336/33408)
Epoch: 57 | Batch_idx: 270 |  Loss: (0.3604) |  Loss2: (0.0000) | Acc: (87.00%) (30433/34688)
Epoch: 57 | Batch_idx: 280 |  Loss: (0.3606) |  Loss2: (0.0000) | Acc: (87.00%) (31557/35968)
Epoch: 57 | Batch_idx: 290 |  Loss: (0.3605) |  Loss2: (0.0000) | Acc: (87.00%) (32688/37248)
Epoch: 57 | Batch_idx: 300 |  Loss: (0.3611) |  Loss2: (0.0000) | Acc: (87.00%) (33793/38528)
Epoch: 57 | Batch_idx: 310 |  Loss: (0.3620) |  Loss2: (0.0000) | Acc: (87.00%) (34891/39808)
Epoch: 57 | Batch_idx: 320 |  Loss: (0.3618) |  Loss2: (0.0000) | Acc: (87.00%) (36017/41088)
Epoch: 57 | Batch_idx: 330 |  Loss: (0.3600) |  Loss2: (0.0000) | Acc: (87.00%) (37166/42368)
Epoch: 57 | Batch_idx: 340 |  Loss: (0.3605) |  Loss2: (0.0000) | Acc: (87.00%) (38278/43648)
Epoch: 57 | Batch_idx: 350 |  Loss: (0.3614) |  Loss2: (0.0000) | Acc: (87.00%) (39390/44928)
Epoch: 57 | Batch_idx: 360 |  Loss: (0.3616) |  Loss2: (0.0000) | Acc: (87.00%) (40505/46208)
Epoch: 57 | Batch_idx: 370 |  Loss: (0.3619) |  Loss2: (0.0000) | Acc: (87.00%) (41613/47488)
Epoch: 57 | Batch_idx: 380 |  Loss: (0.3634) |  Loss2: (0.0000) | Acc: (87.00%) (42705/48768)
Epoch: 57 | Batch_idx: 390 |  Loss: (0.3638) |  Loss2: (0.0000) | Acc: (87.00%) (43770/50000)
# TEST : Loss: (0.5005) | Acc: (83.00%) (8323/10000)
percent tensor([0.5328, 0.5490, 0.5376, 0.5174, 0.5405, 0.5441, 0.5506, 0.5326, 0.5318,
        0.5458, 0.5411, 0.5446, 0.5332, 0.5255, 0.5470, 0.5330],
       device='cuda:0') torch.Size([16])
percent tensor([0.5128, 0.5109, 0.5144, 0.5167, 0.5152, 0.5161, 0.5130, 0.5143, 0.5115,
        0.5109, 0.5097, 0.5107, 0.5095, 0.5132, 0.5129, 0.5145],
       device='cuda:0') torch.Size([16])
percent tensor([0.5462, 0.5662, 0.4229, 0.4172, 0.4105, 0.5054, 0.5305, 0.4479, 0.4822,
        0.5461, 0.5597, 0.4616, 0.5571, 0.5483, 0.5337, 0.5414],
       device='cuda:0') torch.Size([16])
percent tensor([0.5973, 0.4624, 0.6265, 0.6468, 0.6345, 0.6585, 0.5444, 0.6341, 0.5817,
        0.4716, 0.5238, 0.5444, 0.4644, 0.5755, 0.5667, 0.6084],
       device='cuda:0') torch.Size([16])
percent tensor([0.5590, 0.5636, 0.5633, 0.5399, 0.5400, 0.4896, 0.5754, 0.5739, 0.5683,
        0.5598, 0.5578, 0.5834, 0.5673, 0.5643, 0.5839, 0.5128],
       device='cuda:0') torch.Size([16])
percent tensor([0.5669, 0.5781, 0.6413, 0.6707, 0.6610, 0.6416, 0.6110, 0.5811, 0.6387,
        0.5379, 0.6084, 0.6855, 0.5779, 0.6566, 0.5825, 0.5904],
       device='cuda:0') torch.Size([16])
percent tensor([0.5716, 0.5513, 0.4898, 0.4860, 0.5315, 0.5931, 0.5315, 0.4281, 0.5596,
        0.5586, 0.5676, 0.4313, 0.5662, 0.5829, 0.4469, 0.5697],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9989, 0.9995, 0.9996, 0.9996, 0.9970, 0.9992, 0.9995, 0.9990,
        0.9990, 0.9993, 0.9997, 0.9995, 0.9990, 0.9987, 0.9993],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 58 | Batch_idx: 0 |  Loss: (0.3436) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 58 | Batch_idx: 10 |  Loss: (0.3959) |  Loss2: (0.0000) | Acc: (86.00%) (1221/1408)
Epoch: 58 | Batch_idx: 20 |  Loss: (0.3972) |  Loss2: (0.0000) | Acc: (85.00%) (2311/2688)
Epoch: 58 | Batch_idx: 30 |  Loss: (0.4142) |  Loss2: (0.0000) | Acc: (85.00%) (3390/3968)
Epoch: 58 | Batch_idx: 40 |  Loss: (0.4122) |  Loss2: (0.0000) | Acc: (85.00%) (4488/5248)
Epoch: 58 | Batch_idx: 50 |  Loss: (0.4200) |  Loss2: (0.0000) | Acc: (85.00%) (5559/6528)
Epoch: 58 | Batch_idx: 60 |  Loss: (0.4155) |  Loss2: (0.0000) | Acc: (85.00%) (6671/7808)
Epoch: 58 | Batch_idx: 70 |  Loss: (0.4143) |  Loss2: (0.0000) | Acc: (85.00%) (7780/9088)
Epoch: 58 | Batch_idx: 80 |  Loss: (0.4153) |  Loss2: (0.0000) | Acc: (85.00%) (8873/10368)
Epoch: 58 | Batch_idx: 90 |  Loss: (0.4130) |  Loss2: (0.0000) | Acc: (85.00%) (9976/11648)
Epoch: 58 | Batch_idx: 100 |  Loss: (0.4170) |  Loss2: (0.0000) | Acc: (85.00%) (11064/12928)
Epoch: 58 | Batch_idx: 110 |  Loss: (0.4197) |  Loss2: (0.0000) | Acc: (85.00%) (12157/14208)
Epoch: 58 | Batch_idx: 120 |  Loss: (0.4179) |  Loss2: (0.0000) | Acc: (85.00%) (13249/15488)
Epoch: 58 | Batch_idx: 130 |  Loss: (0.4167) |  Loss2: (0.0000) | Acc: (85.00%) (14343/16768)
Epoch: 58 | Batch_idx: 140 |  Loss: (0.4187) |  Loss2: (0.0000) | Acc: (85.00%) (15431/18048)
Epoch: 58 | Batch_idx: 150 |  Loss: (0.4199) |  Loss2: (0.0000) | Acc: (85.00%) (16506/19328)
Epoch: 58 | Batch_idx: 160 |  Loss: (0.4216) |  Loss2: (0.0000) | Acc: (85.00%) (17588/20608)
Epoch: 58 | Batch_idx: 170 |  Loss: (0.4219) |  Loss2: (0.0000) | Acc: (85.00%) (18676/21888)
Epoch: 58 | Batch_idx: 180 |  Loss: (0.4208) |  Loss2: (0.0000) | Acc: (85.00%) (19789/23168)
Epoch: 58 | Batch_idx: 190 |  Loss: (0.4211) |  Loss2: (0.0000) | Acc: (85.00%) (20880/24448)
Epoch: 58 | Batch_idx: 200 |  Loss: (0.4195) |  Loss2: (0.0000) | Acc: (85.00%) (21993/25728)
Epoch: 58 | Batch_idx: 210 |  Loss: (0.4208) |  Loss2: (0.0000) | Acc: (85.00%) (23081/27008)
Epoch: 58 | Batch_idx: 220 |  Loss: (0.4189) |  Loss2: (0.0000) | Acc: (85.00%) (24196/28288)
Epoch: 58 | Batch_idx: 230 |  Loss: (0.4201) |  Loss2: (0.0000) | Acc: (85.00%) (25268/29568)
Epoch: 58 | Batch_idx: 240 |  Loss: (0.4178) |  Loss2: (0.0000) | Acc: (85.00%) (26382/30848)
Epoch: 58 | Batch_idx: 250 |  Loss: (0.4165) |  Loss2: (0.0000) | Acc: (85.00%) (27502/32128)
Epoch: 58 | Batch_idx: 260 |  Loss: (0.4165) |  Loss2: (0.0000) | Acc: (85.00%) (28584/33408)
Epoch: 58 | Batch_idx: 270 |  Loss: (0.4178) |  Loss2: (0.0000) | Acc: (85.00%) (29660/34688)
Epoch: 58 | Batch_idx: 280 |  Loss: (0.4174) |  Loss2: (0.0000) | Acc: (85.00%) (30762/35968)
Epoch: 58 | Batch_idx: 290 |  Loss: (0.4183) |  Loss2: (0.0000) | Acc: (85.00%) (31848/37248)
Epoch: 58 | Batch_idx: 300 |  Loss: (0.4174) |  Loss2: (0.0000) | Acc: (85.00%) (32945/38528)
Epoch: 58 | Batch_idx: 310 |  Loss: (0.4168) |  Loss2: (0.0000) | Acc: (85.00%) (34063/39808)
Epoch: 58 | Batch_idx: 320 |  Loss: (0.4165) |  Loss2: (0.0000) | Acc: (85.00%) (35165/41088)
Epoch: 58 | Batch_idx: 330 |  Loss: (0.4152) |  Loss2: (0.0000) | Acc: (85.00%) (36281/42368)
Epoch: 58 | Batch_idx: 340 |  Loss: (0.4158) |  Loss2: (0.0000) | Acc: (85.00%) (37369/43648)
Epoch: 58 | Batch_idx: 350 |  Loss: (0.4155) |  Loss2: (0.0000) | Acc: (85.00%) (38468/44928)
Epoch: 58 | Batch_idx: 360 |  Loss: (0.4160) |  Loss2: (0.0000) | Acc: (85.00%) (39552/46208)
Epoch: 58 | Batch_idx: 370 |  Loss: (0.4138) |  Loss2: (0.0000) | Acc: (85.00%) (40689/47488)
Epoch: 58 | Batch_idx: 380 |  Loss: (0.4141) |  Loss2: (0.0000) | Acc: (85.00%) (41779/48768)
Epoch: 58 | Batch_idx: 390 |  Loss: (0.4125) |  Loss2: (0.0000) | Acc: (85.00%) (42855/50000)
# TEST : Loss: (0.4799) | Acc: (83.00%) (8336/10000)
percent tensor([0.5237, 0.5384, 0.5278, 0.5090, 0.5301, 0.5344, 0.5398, 0.5216, 0.5223,
        0.5351, 0.5320, 0.5334, 0.5237, 0.5162, 0.5366, 0.5244],
       device='cuda:0') torch.Size([16])
percent tensor([0.5129, 0.5110, 0.5147, 0.5172, 0.5152, 0.5160, 0.5131, 0.5149, 0.5118,
        0.5111, 0.5098, 0.5109, 0.5098, 0.5132, 0.5131, 0.5147],
       device='cuda:0') torch.Size([16])
percent tensor([0.5595, 0.5743, 0.4290, 0.4315, 0.4265, 0.5200, 0.5426, 0.4577, 0.4846,
        0.5469, 0.5625, 0.4634, 0.5563, 0.5501, 0.5426, 0.5553],
       device='cuda:0') torch.Size([16])
percent tensor([0.6464, 0.5422, 0.6469, 0.6710, 0.6605, 0.6838, 0.6073, 0.6642, 0.6232,
        0.5482, 0.5849, 0.5934, 0.5454, 0.6294, 0.6171, 0.6628],
       device='cuda:0') torch.Size([16])
percent tensor([0.5866, 0.5961, 0.5744, 0.5572, 0.5549, 0.5180, 0.6048, 0.5940, 0.5966,
        0.5931, 0.5925, 0.6011, 0.5964, 0.6058, 0.6087, 0.5462],
       device='cuda:0') torch.Size([16])
percent tensor([0.4959, 0.5021, 0.5932, 0.6143, 0.6167, 0.5937, 0.5367, 0.5289, 0.5841,
        0.4805, 0.5334, 0.6300, 0.5181, 0.5712, 0.5100, 0.5256],
       device='cuda:0') torch.Size([16])
percent tensor([0.6165, 0.5859, 0.5389, 0.5378, 0.5848, 0.6565, 0.5662, 0.4630, 0.5844,
        0.5978, 0.5957, 0.4409, 0.5963, 0.6164, 0.4844, 0.6196],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9988, 0.9991, 0.9995, 0.9995, 0.9972, 0.9990, 0.9994, 0.9989,
        0.9990, 0.9991, 0.9994, 0.9994, 0.9988, 0.9986, 0.9991],
       device='cuda:0') torch.Size([16])
Epoch: 59 | Batch_idx: 0 |  Loss: (0.3871) |  Loss2: (0.0000) | Acc: (85.00%) (110/128)
Epoch: 59 | Batch_idx: 10 |  Loss: (0.3998) |  Loss2: (0.0000) | Acc: (87.00%) (1227/1408)
Epoch: 59 | Batch_idx: 20 |  Loss: (0.3937) |  Loss2: (0.0000) | Acc: (87.00%) (2341/2688)
Epoch: 59 | Batch_idx: 30 |  Loss: (0.4023) |  Loss2: (0.0000) | Acc: (86.00%) (3428/3968)
Epoch: 59 | Batch_idx: 40 |  Loss: (0.4007) |  Loss2: (0.0000) | Acc: (86.00%) (4533/5248)
Epoch: 59 | Batch_idx: 50 |  Loss: (0.4021) |  Loss2: (0.0000) | Acc: (86.00%) (5641/6528)
Epoch: 59 | Batch_idx: 60 |  Loss: (0.4023) |  Loss2: (0.0000) | Acc: (86.00%) (6743/7808)
Epoch: 59 | Batch_idx: 70 |  Loss: (0.4018) |  Loss2: (0.0000) | Acc: (86.00%) (7863/9088)
Epoch: 59 | Batch_idx: 80 |  Loss: (0.3987) |  Loss2: (0.0000) | Acc: (86.00%) (8979/10368)
Epoch: 59 | Batch_idx: 90 |  Loss: (0.3932) |  Loss2: (0.0000) | Acc: (86.00%) (10108/11648)
Epoch: 59 | Batch_idx: 100 |  Loss: (0.3900) |  Loss2: (0.0000) | Acc: (86.00%) (11232/12928)
Epoch: 59 | Batch_idx: 110 |  Loss: (0.3908) |  Loss2: (0.0000) | Acc: (86.00%) (12334/14208)
Epoch: 59 | Batch_idx: 120 |  Loss: (0.3884) |  Loss2: (0.0000) | Acc: (86.00%) (13449/15488)
Epoch: 59 | Batch_idx: 130 |  Loss: (0.3864) |  Loss2: (0.0000) | Acc: (86.00%) (14569/16768)
Epoch: 59 | Batch_idx: 140 |  Loss: (0.3843) |  Loss2: (0.0000) | Acc: (86.00%) (15683/18048)
Epoch: 59 | Batch_idx: 150 |  Loss: (0.3831) |  Loss2: (0.0000) | Acc: (86.00%) (16793/19328)
Epoch: 59 | Batch_idx: 160 |  Loss: (0.3830) |  Loss2: (0.0000) | Acc: (86.00%) (17902/20608)
Epoch: 59 | Batch_idx: 170 |  Loss: (0.3831) |  Loss2: (0.0000) | Acc: (86.00%) (19015/21888)
Epoch: 59 | Batch_idx: 180 |  Loss: (0.3822) |  Loss2: (0.0000) | Acc: (86.00%) (20134/23168)
Epoch: 59 | Batch_idx: 190 |  Loss: (0.3826) |  Loss2: (0.0000) | Acc: (86.00%) (21235/24448)
Epoch: 59 | Batch_idx: 200 |  Loss: (0.3820) |  Loss2: (0.0000) | Acc: (86.00%) (22347/25728)
Epoch: 59 | Batch_idx: 210 |  Loss: (0.3820) |  Loss2: (0.0000) | Acc: (86.00%) (23453/27008)
Epoch: 59 | Batch_idx: 220 |  Loss: (0.3811) |  Loss2: (0.0000) | Acc: (86.00%) (24566/28288)
Epoch: 59 | Batch_idx: 230 |  Loss: (0.3804) |  Loss2: (0.0000) | Acc: (86.00%) (25686/29568)
Epoch: 59 | Batch_idx: 240 |  Loss: (0.3811) |  Loss2: (0.0000) | Acc: (86.00%) (26797/30848)
Epoch: 59 | Batch_idx: 250 |  Loss: (0.3820) |  Loss2: (0.0000) | Acc: (86.00%) (27886/32128)
Epoch: 59 | Batch_idx: 260 |  Loss: (0.3812) |  Loss2: (0.0000) | Acc: (86.00%) (28994/33408)
Epoch: 59 | Batch_idx: 270 |  Loss: (0.3810) |  Loss2: (0.0000) | Acc: (86.00%) (30113/34688)
Epoch: 59 | Batch_idx: 280 |  Loss: (0.3799) |  Loss2: (0.0000) | Acc: (86.00%) (31226/35968)
Epoch: 59 | Batch_idx: 290 |  Loss: (0.3799) |  Loss2: (0.0000) | Acc: (86.00%) (32338/37248)
Epoch: 59 | Batch_idx: 300 |  Loss: (0.3810) |  Loss2: (0.0000) | Acc: (86.00%) (33433/38528)
Epoch: 59 | Batch_idx: 310 |  Loss: (0.3820) |  Loss2: (0.0000) | Acc: (86.00%) (34533/39808)
Epoch: 59 | Batch_idx: 320 |  Loss: (0.3817) |  Loss2: (0.0000) | Acc: (86.00%) (35663/41088)
Epoch: 59 | Batch_idx: 330 |  Loss: (0.3820) |  Loss2: (0.0000) | Acc: (86.00%) (36771/42368)
Epoch: 59 | Batch_idx: 340 |  Loss: (0.3825) |  Loss2: (0.0000) | Acc: (86.00%) (37880/43648)
Epoch: 59 | Batch_idx: 350 |  Loss: (0.3819) |  Loss2: (0.0000) | Acc: (86.00%) (39003/44928)
Epoch: 59 | Batch_idx: 360 |  Loss: (0.3812) |  Loss2: (0.0000) | Acc: (86.00%) (40129/46208)
Epoch: 59 | Batch_idx: 370 |  Loss: (0.3816) |  Loss2: (0.0000) | Acc: (86.00%) (41232/47488)
Epoch: 59 | Batch_idx: 380 |  Loss: (0.3815) |  Loss2: (0.0000) | Acc: (86.00%) (42339/48768)
Epoch: 59 | Batch_idx: 390 |  Loss: (0.3814) |  Loss2: (0.0000) | Acc: (86.00%) (43403/50000)
# TEST : Loss: (0.4571) | Acc: (84.00%) (8405/10000)
percent tensor([0.5217, 0.5353, 0.5259, 0.5069, 0.5278, 0.5318, 0.5370, 0.5190, 0.5197,
        0.5323, 0.5296, 0.5308, 0.5213, 0.5127, 0.5339, 0.5222],
       device='cuda:0') torch.Size([16])
percent tensor([0.5141, 0.5125, 0.5162, 0.5187, 0.5166, 0.5174, 0.5145, 0.5166, 0.5135,
        0.5125, 0.5113, 0.5123, 0.5113, 0.5149, 0.5147, 0.5162],
       device='cuda:0') torch.Size([16])
percent tensor([0.5676, 0.5755, 0.4347, 0.4336, 0.4331, 0.5230, 0.5469, 0.4629, 0.4823,
        0.5432, 0.5616, 0.4621, 0.5534, 0.5441, 0.5473, 0.5613],
       device='cuda:0') torch.Size([16])
percent tensor([0.6401, 0.5402, 0.6417, 0.6670, 0.6554, 0.6832, 0.5997, 0.6576, 0.6135,
        0.5440, 0.5783, 0.5821, 0.5420, 0.6211, 0.6104, 0.6596],
       device='cuda:0') torch.Size([16])
percent tensor([0.5941, 0.6048, 0.5843, 0.5659, 0.5650, 0.5282, 0.6117, 0.6017, 0.6062,
        0.6060, 0.6038, 0.6113, 0.6068, 0.6145, 0.6139, 0.5557],
       device='cuda:0') torch.Size([16])
percent tensor([0.4932, 0.5020, 0.5916, 0.6139, 0.6175, 0.5899, 0.5360, 0.5342, 0.5831,
        0.4774, 0.5259, 0.6291, 0.5122, 0.5651, 0.5153, 0.5256],
       device='cuda:0') torch.Size([16])
percent tensor([0.6373, 0.5974, 0.5409, 0.5373, 0.5906, 0.6834, 0.5826, 0.4668, 0.5897,
        0.6096, 0.6011, 0.4232, 0.6044, 0.6305, 0.4844, 0.6458],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9988, 0.9991, 0.9995, 0.9994, 0.9977, 0.9992, 0.9994, 0.9989,
        0.9991, 0.9992, 0.9994, 0.9994, 0.9989, 0.9987, 0.9992],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 60 | Batch_idx: 0 |  Loss: (0.3113) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 60 | Batch_idx: 10 |  Loss: (0.3777) |  Loss2: (0.0000) | Acc: (87.00%) (1229/1408)
Epoch: 60 | Batch_idx: 20 |  Loss: (0.3790) |  Loss2: (0.0000) | Acc: (86.00%) (2336/2688)
Epoch: 60 | Batch_idx: 30 |  Loss: (0.3726) |  Loss2: (0.0000) | Acc: (86.00%) (3448/3968)
Epoch: 60 | Batch_idx: 40 |  Loss: (0.3628) |  Loss2: (0.0000) | Acc: (87.00%) (4576/5248)
Epoch: 60 | Batch_idx: 50 |  Loss: (0.3566) |  Loss2: (0.0000) | Acc: (87.00%) (5724/6528)
Epoch: 60 | Batch_idx: 60 |  Loss: (0.3627) |  Loss2: (0.0000) | Acc: (87.00%) (6828/7808)
Epoch: 60 | Batch_idx: 70 |  Loss: (0.3636) |  Loss2: (0.0000) | Acc: (87.00%) (7939/9088)
Epoch: 60 | Batch_idx: 80 |  Loss: (0.3577) |  Loss2: (0.0000) | Acc: (87.00%) (9080/10368)
Epoch: 60 | Batch_idx: 90 |  Loss: (0.3580) |  Loss2: (0.0000) | Acc: (87.00%) (10200/11648)
Epoch: 60 | Batch_idx: 100 |  Loss: (0.3590) |  Loss2: (0.0000) | Acc: (87.00%) (11324/12928)
Epoch: 60 | Batch_idx: 110 |  Loss: (0.3596) |  Loss2: (0.0000) | Acc: (87.00%) (12437/14208)
Epoch: 60 | Batch_idx: 120 |  Loss: (0.3584) |  Loss2: (0.0000) | Acc: (87.00%) (13570/15488)
Epoch: 60 | Batch_idx: 130 |  Loss: (0.3590) |  Loss2: (0.0000) | Acc: (87.00%) (14681/16768)
Epoch: 60 | Batch_idx: 140 |  Loss: (0.3582) |  Loss2: (0.0000) | Acc: (87.00%) (15805/18048)
Epoch: 60 | Batch_idx: 150 |  Loss: (0.3584) |  Loss2: (0.0000) | Acc: (87.00%) (16928/19328)
Epoch: 60 | Batch_idx: 160 |  Loss: (0.3575) |  Loss2: (0.0000) | Acc: (87.00%) (18064/20608)
Epoch: 60 | Batch_idx: 170 |  Loss: (0.3587) |  Loss2: (0.0000) | Acc: (87.00%) (19168/21888)
Epoch: 60 | Batch_idx: 180 |  Loss: (0.3568) |  Loss2: (0.0000) | Acc: (87.00%) (20315/23168)
Epoch: 60 | Batch_idx: 190 |  Loss: (0.3570) |  Loss2: (0.0000) | Acc: (87.00%) (21434/24448)
Epoch: 60 | Batch_idx: 200 |  Loss: (0.3562) |  Loss2: (0.0000) | Acc: (87.00%) (22584/25728)
Epoch: 60 | Batch_idx: 210 |  Loss: (0.3581) |  Loss2: (0.0000) | Acc: (87.00%) (23692/27008)
Epoch: 60 | Batch_idx: 220 |  Loss: (0.3574) |  Loss2: (0.0000) | Acc: (87.00%) (24815/28288)
Epoch: 60 | Batch_idx: 230 |  Loss: (0.3578) |  Loss2: (0.0000) | Acc: (87.00%) (25927/29568)
Epoch: 60 | Batch_idx: 240 |  Loss: (0.3579) |  Loss2: (0.0000) | Acc: (87.00%) (27036/30848)
Epoch: 60 | Batch_idx: 250 |  Loss: (0.3570) |  Loss2: (0.0000) | Acc: (87.00%) (28187/32128)
Epoch: 60 | Batch_idx: 260 |  Loss: (0.3567) |  Loss2: (0.0000) | Acc: (87.00%) (29311/33408)
Epoch: 60 | Batch_idx: 270 |  Loss: (0.3572) |  Loss2: (0.0000) | Acc: (87.00%) (30419/34688)
Epoch: 60 | Batch_idx: 280 |  Loss: (0.3582) |  Loss2: (0.0000) | Acc: (87.00%) (31522/35968)
Epoch: 60 | Batch_idx: 290 |  Loss: (0.3578) |  Loss2: (0.0000) | Acc: (87.00%) (32635/37248)
Epoch: 60 | Batch_idx: 300 |  Loss: (0.3583) |  Loss2: (0.0000) | Acc: (87.00%) (33769/38528)
Epoch: 60 | Batch_idx: 310 |  Loss: (0.3585) |  Loss2: (0.0000) | Acc: (87.00%) (34899/39808)
Epoch: 60 | Batch_idx: 320 |  Loss: (0.3593) |  Loss2: (0.0000) | Acc: (87.00%) (36012/41088)
Epoch: 60 | Batch_idx: 330 |  Loss: (0.3584) |  Loss2: (0.0000) | Acc: (87.00%) (37152/42368)
Epoch: 60 | Batch_idx: 340 |  Loss: (0.3585) |  Loss2: (0.0000) | Acc: (87.00%) (38267/43648)
Epoch: 60 | Batch_idx: 350 |  Loss: (0.3588) |  Loss2: (0.0000) | Acc: (87.00%) (39379/44928)
Epoch: 60 | Batch_idx: 360 |  Loss: (0.3591) |  Loss2: (0.0000) | Acc: (87.00%) (40496/46208)
Epoch: 60 | Batch_idx: 370 |  Loss: (0.3590) |  Loss2: (0.0000) | Acc: (87.00%) (41620/47488)
Epoch: 60 | Batch_idx: 380 |  Loss: (0.3584) |  Loss2: (0.0000) | Acc: (87.00%) (42766/48768)
Epoch: 60 | Batch_idx: 390 |  Loss: (0.3580) |  Loss2: (0.0000) | Acc: (87.00%) (43849/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_060.pth.tar'
# TEST : Loss: (0.5267) | Acc: (83.00%) (8302/10000)
percent tensor([0.5230, 0.5347, 0.5249, 0.5076, 0.5274, 0.5321, 0.5356, 0.5192, 0.5203,
        0.5319, 0.5305, 0.5295, 0.5221, 0.5111, 0.5339, 0.5223],
       device='cuda:0') torch.Size([16])
percent tensor([0.5141, 0.5118, 0.5161, 0.5184, 0.5167, 0.5182, 0.5144, 0.5163, 0.5133,
        0.5129, 0.5109, 0.5123, 0.5110, 0.5129, 0.5149, 0.5160],
       device='cuda:0') torch.Size([16])
percent tensor([0.5644, 0.5693, 0.4442, 0.4229, 0.4296, 0.5205, 0.5425, 0.4612, 0.4887,
        0.5399, 0.5678, 0.4742, 0.5527, 0.5266, 0.5461, 0.5565],
       device='cuda:0') torch.Size([16])
percent tensor([0.6434, 0.5441, 0.6580, 0.6705, 0.6632, 0.6890, 0.6024, 0.6609, 0.6082,
        0.5583, 0.5708, 0.5874, 0.5348, 0.6095, 0.6160, 0.6640],
       device='cuda:0') torch.Size([16])
percent tensor([0.5936, 0.5939, 0.5865, 0.5664, 0.5692, 0.5159, 0.6098, 0.6010, 0.6102,
        0.5971, 0.6076, 0.6162, 0.6041, 0.6014, 0.6098, 0.5501],
       device='cuda:0') torch.Size([16])
percent tensor([0.4812, 0.5210, 0.5734, 0.6023, 0.5973, 0.5750, 0.5338, 0.5162, 0.5671,
        0.4791, 0.5303, 0.5930, 0.4928, 0.5951, 0.5165, 0.5132],
       device='cuda:0') torch.Size([16])
percent tensor([0.6307, 0.6239, 0.5341, 0.5203, 0.5839, 0.6617, 0.6024, 0.4856, 0.6023,
        0.6298, 0.6068, 0.4532, 0.6186, 0.6515, 0.4667, 0.6730],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9986, 0.9989, 0.9993, 0.9994, 0.9980, 0.9997, 0.9993, 0.9994,
        0.9992, 0.9993, 0.9996, 0.9995, 0.9992, 0.9990, 0.9996],
       device='cuda:0') torch.Size([16])
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(176.0820, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(802.2728, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(796.1594, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1524.5459, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(499.3108, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2209.8250, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4276.5005, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1418.8961, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6119.9873, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12015.2061, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3999.8899, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16912.3711, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 61 | Batch_idx: 0 |  Loss: (0.4646) |  Loss2: (0.0000) | Acc: (82.00%) (106/128)
Epoch: 61 | Batch_idx: 10 |  Loss: (0.3614) |  Loss2: (0.0000) | Acc: (87.00%) (1235/1408)
Epoch: 61 | Batch_idx: 20 |  Loss: (0.3474) |  Loss2: (0.0000) | Acc: (87.00%) (2359/2688)
Epoch: 61 | Batch_idx: 30 |  Loss: (0.3396) |  Loss2: (0.0000) | Acc: (87.00%) (3488/3968)
Epoch: 61 | Batch_idx: 40 |  Loss: (0.3493) |  Loss2: (0.0000) | Acc: (87.00%) (4588/5248)
Epoch: 61 | Batch_idx: 50 |  Loss: (0.3508) |  Loss2: (0.0000) | Acc: (87.00%) (5710/6528)
Epoch: 61 | Batch_idx: 60 |  Loss: (0.3526) |  Loss2: (0.0000) | Acc: (87.00%) (6831/7808)
Epoch: 61 | Batch_idx: 70 |  Loss: (0.3490) |  Loss2: (0.0000) | Acc: (87.00%) (7969/9088)
Epoch: 61 | Batch_idx: 80 |  Loss: (0.3508) |  Loss2: (0.0000) | Acc: (87.00%) (9086/10368)
Epoch: 61 | Batch_idx: 90 |  Loss: (0.3513) |  Loss2: (0.0000) | Acc: (87.00%) (10215/11648)
Epoch: 61 | Batch_idx: 100 |  Loss: (0.3493) |  Loss2: (0.0000) | Acc: (87.00%) (11342/12928)
Epoch: 61 | Batch_idx: 110 |  Loss: (0.3478) |  Loss2: (0.0000) | Acc: (87.00%) (12486/14208)
Epoch: 61 | Batch_idx: 120 |  Loss: (0.3492) |  Loss2: (0.0000) | Acc: (87.00%) (13607/15488)
Epoch: 61 | Batch_idx: 130 |  Loss: (0.3491) |  Loss2: (0.0000) | Acc: (87.00%) (14729/16768)
Epoch: 61 | Batch_idx: 140 |  Loss: (0.3490) |  Loss2: (0.0000) | Acc: (87.00%) (15856/18048)
Epoch: 61 | Batch_idx: 150 |  Loss: (0.3486) |  Loss2: (0.0000) | Acc: (87.00%) (16991/19328)
Epoch: 61 | Batch_idx: 160 |  Loss: (0.3480) |  Loss2: (0.0000) | Acc: (87.00%) (18113/20608)
Epoch: 61 | Batch_idx: 170 |  Loss: (0.3490) |  Loss2: (0.0000) | Acc: (87.00%) (19233/21888)
Epoch: 61 | Batch_idx: 180 |  Loss: (0.3478) |  Loss2: (0.0000) | Acc: (87.00%) (20376/23168)
Epoch: 61 | Batch_idx: 190 |  Loss: (0.3475) |  Loss2: (0.0000) | Acc: (87.00%) (21510/24448)
Epoch: 61 | Batch_idx: 200 |  Loss: (0.3473) |  Loss2: (0.0000) | Acc: (87.00%) (22640/25728)
Epoch: 61 | Batch_idx: 210 |  Loss: (0.3493) |  Loss2: (0.0000) | Acc: (87.00%) (23752/27008)
Epoch: 61 | Batch_idx: 220 |  Loss: (0.3486) |  Loss2: (0.0000) | Acc: (87.00%) (24880/28288)
Epoch: 61 | Batch_idx: 230 |  Loss: (0.3480) |  Loss2: (0.0000) | Acc: (88.00%) (26020/29568)
Epoch: 61 | Batch_idx: 240 |  Loss: (0.3472) |  Loss2: (0.0000) | Acc: (88.00%) (27158/30848)
Epoch: 61 | Batch_idx: 250 |  Loss: (0.3470) |  Loss2: (0.0000) | Acc: (88.00%) (28285/32128)
Epoch: 61 | Batch_idx: 260 |  Loss: (0.3469) |  Loss2: (0.0000) | Acc: (88.00%) (29416/33408)
Epoch: 61 | Batch_idx: 270 |  Loss: (0.3466) |  Loss2: (0.0000) | Acc: (88.00%) (30546/34688)
Epoch: 61 | Batch_idx: 280 |  Loss: (0.3473) |  Loss2: (0.0000) | Acc: (88.00%) (31661/35968)
Epoch: 61 | Batch_idx: 290 |  Loss: (0.3475) |  Loss2: (0.0000) | Acc: (88.00%) (32809/37248)
Epoch: 61 | Batch_idx: 300 |  Loss: (0.3484) |  Loss2: (0.0000) | Acc: (88.00%) (33912/38528)
Epoch: 61 | Batch_idx: 310 |  Loss: (0.3471) |  Loss2: (0.0000) | Acc: (88.00%) (35061/39808)
Epoch: 61 | Batch_idx: 320 |  Loss: (0.3466) |  Loss2: (0.0000) | Acc: (88.00%) (36201/41088)
Epoch: 61 | Batch_idx: 330 |  Loss: (0.3464) |  Loss2: (0.0000) | Acc: (88.00%) (37342/42368)
Epoch: 61 | Batch_idx: 340 |  Loss: (0.3459) |  Loss2: (0.0000) | Acc: (88.00%) (38466/43648)
Epoch: 61 | Batch_idx: 350 |  Loss: (0.3464) |  Loss2: (0.0000) | Acc: (88.00%) (39588/44928)
Epoch: 61 | Batch_idx: 360 |  Loss: (0.3469) |  Loss2: (0.0000) | Acc: (88.00%) (40700/46208)
Epoch: 61 | Batch_idx: 370 |  Loss: (0.3471) |  Loss2: (0.0000) | Acc: (88.00%) (41824/47488)
Epoch: 61 | Batch_idx: 380 |  Loss: (0.3463) |  Loss2: (0.0000) | Acc: (88.00%) (42960/48768)
Epoch: 61 | Batch_idx: 390 |  Loss: (0.3463) |  Loss2: (0.0000) | Acc: (88.00%) (44054/50000)
# TEST : Loss: (0.4400) | Acc: (85.00%) (8509/10000)
percent tensor([0.5244, 0.5330, 0.5279, 0.5066, 0.5308, 0.5347, 0.5364, 0.5185, 0.5218,
        0.5318, 0.5317, 0.5325, 0.5233, 0.5084, 0.5340, 0.5226],
       device='cuda:0') torch.Size([16])
percent tensor([0.5141, 0.5123, 0.5159, 0.5186, 0.5162, 0.5170, 0.5144, 0.5167, 0.5135,
        0.5131, 0.5115, 0.5124, 0.5114, 0.5144, 0.5148, 0.5162],
       device='cuda:0') torch.Size([16])
percent tensor([0.5657, 0.5706, 0.4301, 0.4132, 0.4207, 0.5249, 0.5424, 0.4566, 0.4844,
        0.5400, 0.5717, 0.4684, 0.5575, 0.5289, 0.5490, 0.5578],
       device='cuda:0') torch.Size([16])
percent tensor([0.6397, 0.5537, 0.6544, 0.6648, 0.6576, 0.6757, 0.6117, 0.6668, 0.6207,
        0.5644, 0.5863, 0.6012, 0.5377, 0.6332, 0.6139, 0.6615],
       device='cuda:0') torch.Size([16])
percent tensor([0.5898, 0.5934, 0.5800, 0.5708, 0.5637, 0.5286, 0.6033, 0.5938, 0.5965,
        0.5973, 0.5957, 0.6041, 0.5990, 0.6003, 0.6059, 0.5508],
       device='cuda:0') torch.Size([16])
percent tensor([0.4907, 0.5191, 0.5917, 0.6150, 0.6041, 0.5619, 0.5632, 0.5467, 0.5789,
        0.4949, 0.5420, 0.6188, 0.4882, 0.5982, 0.5315, 0.5261],
       device='cuda:0') torch.Size([16])
percent tensor([0.6306, 0.6292, 0.5556, 0.5081, 0.6028, 0.6742, 0.5897, 0.4907, 0.6189,
        0.6230, 0.5990, 0.4671, 0.6217, 0.6287, 0.4685, 0.6612],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9986, 0.9993, 0.9993, 0.9995, 0.9979, 0.9993, 0.9994, 0.9991,
        0.9989, 0.9992, 0.9994, 0.9993, 0.9989, 0.9986, 0.9992],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 62 | Batch_idx: 0 |  Loss: (0.3146) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 62 | Batch_idx: 10 |  Loss: (0.3404) |  Loss2: (0.0000) | Acc: (88.00%) (1248/1408)
Epoch: 62 | Batch_idx: 20 |  Loss: (0.3650) |  Loss2: (0.0000) | Acc: (87.00%) (2358/2688)
Epoch: 62 | Batch_idx: 30 |  Loss: (0.3620) |  Loss2: (0.0000) | Acc: (88.00%) (3495/3968)
Epoch: 62 | Batch_idx: 40 |  Loss: (0.3570) |  Loss2: (0.0000) | Acc: (87.00%) (4617/5248)
Epoch: 62 | Batch_idx: 50 |  Loss: (0.3605) |  Loss2: (0.0000) | Acc: (87.00%) (5732/6528)
Epoch: 62 | Batch_idx: 60 |  Loss: (0.3606) |  Loss2: (0.0000) | Acc: (87.00%) (6846/7808)
Epoch: 62 | Batch_idx: 70 |  Loss: (0.3641) |  Loss2: (0.0000) | Acc: (87.00%) (7947/9088)
Epoch: 62 | Batch_idx: 80 |  Loss: (0.3673) |  Loss2: (0.0000) | Acc: (87.00%) (9047/10368)
Epoch: 62 | Batch_idx: 90 |  Loss: (0.3651) |  Loss2: (0.0000) | Acc: (87.00%) (10169/11648)
Epoch: 62 | Batch_idx: 100 |  Loss: (0.3703) |  Loss2: (0.0000) | Acc: (87.00%) (11264/12928)
Epoch: 62 | Batch_idx: 110 |  Loss: (0.3705) |  Loss2: (0.0000) | Acc: (87.00%) (12376/14208)
Epoch: 62 | Batch_idx: 120 |  Loss: (0.3721) |  Loss2: (0.0000) | Acc: (86.00%) (13474/15488)
Epoch: 62 | Batch_idx: 130 |  Loss: (0.3718) |  Loss2: (0.0000) | Acc: (87.00%) (14598/16768)
Epoch: 62 | Batch_idx: 140 |  Loss: (0.3715) |  Loss2: (0.0000) | Acc: (87.00%) (15718/18048)
Epoch: 62 | Batch_idx: 150 |  Loss: (0.3714) |  Loss2: (0.0000) | Acc: (87.00%) (16842/19328)
Epoch: 62 | Batch_idx: 160 |  Loss: (0.3683) |  Loss2: (0.0000) | Acc: (87.00%) (17975/20608)
Epoch: 62 | Batch_idx: 170 |  Loss: (0.3681) |  Loss2: (0.0000) | Acc: (87.00%) (19089/21888)
Epoch: 62 | Batch_idx: 180 |  Loss: (0.3708) |  Loss2: (0.0000) | Acc: (87.00%) (20199/23168)
Epoch: 62 | Batch_idx: 190 |  Loss: (0.3707) |  Loss2: (0.0000) | Acc: (87.00%) (21317/24448)
Epoch: 62 | Batch_idx: 200 |  Loss: (0.3701) |  Loss2: (0.0000) | Acc: (87.00%) (22431/25728)
Epoch: 62 | Batch_idx: 210 |  Loss: (0.3686) |  Loss2: (0.0000) | Acc: (87.00%) (23564/27008)
Epoch: 62 | Batch_idx: 220 |  Loss: (0.3679) |  Loss2: (0.0000) | Acc: (87.00%) (24682/28288)
Epoch: 62 | Batch_idx: 230 |  Loss: (0.3670) |  Loss2: (0.0000) | Acc: (87.00%) (25814/29568)
Epoch: 62 | Batch_idx: 240 |  Loss: (0.3649) |  Loss2: (0.0000) | Acc: (87.00%) (26945/30848)
Epoch: 62 | Batch_idx: 250 |  Loss: (0.3649) |  Loss2: (0.0000) | Acc: (87.00%) (28073/32128)
Epoch: 62 | Batch_idx: 260 |  Loss: (0.3635) |  Loss2: (0.0000) | Acc: (87.00%) (29208/33408)
Epoch: 62 | Batch_idx: 270 |  Loss: (0.3632) |  Loss2: (0.0000) | Acc: (87.00%) (30331/34688)
Epoch: 62 | Batch_idx: 280 |  Loss: (0.3628) |  Loss2: (0.0000) | Acc: (87.00%) (31461/35968)
Epoch: 62 | Batch_idx: 290 |  Loss: (0.3613) |  Loss2: (0.0000) | Acc: (87.00%) (32611/37248)
Epoch: 62 | Batch_idx: 300 |  Loss: (0.3610) |  Loss2: (0.0000) | Acc: (87.00%) (33740/38528)
Epoch: 62 | Batch_idx: 310 |  Loss: (0.3625) |  Loss2: (0.0000) | Acc: (87.00%) (34843/39808)
Epoch: 62 | Batch_idx: 320 |  Loss: (0.3628) |  Loss2: (0.0000) | Acc: (87.00%) (35957/41088)
Epoch: 62 | Batch_idx: 330 |  Loss: (0.3623) |  Loss2: (0.0000) | Acc: (87.00%) (37099/42368)
Epoch: 62 | Batch_idx: 340 |  Loss: (0.3621) |  Loss2: (0.0000) | Acc: (87.00%) (38236/43648)
Epoch: 62 | Batch_idx: 350 |  Loss: (0.3614) |  Loss2: (0.0000) | Acc: (87.00%) (39368/44928)
Epoch: 62 | Batch_idx: 360 |  Loss: (0.3600) |  Loss2: (0.0000) | Acc: (87.00%) (40504/46208)
Epoch: 62 | Batch_idx: 370 |  Loss: (0.3595) |  Loss2: (0.0000) | Acc: (87.00%) (41634/47488)
Epoch: 62 | Batch_idx: 380 |  Loss: (0.3584) |  Loss2: (0.0000) | Acc: (87.00%) (42779/48768)
Epoch: 62 | Batch_idx: 390 |  Loss: (0.3579) |  Loss2: (0.0000) | Acc: (87.00%) (43872/50000)
# TEST : Loss: (0.4385) | Acc: (85.00%) (8519/10000)
percent tensor([0.5125, 0.5188, 0.5142, 0.4992, 0.5165, 0.5213, 0.5222, 0.5039, 0.5101,
        0.5185, 0.5199, 0.5191, 0.5118, 0.4985, 0.5199, 0.5100],
       device='cuda:0') torch.Size([16])
percent tensor([0.5140, 0.5128, 0.5152, 0.5179, 0.5154, 0.5161, 0.5136, 0.5159, 0.5128,
        0.5128, 0.5119, 0.5119, 0.5121, 0.5148, 0.5140, 0.5154],
       device='cuda:0') torch.Size([16])
percent tensor([0.5491, 0.5673, 0.4006, 0.3925, 0.3797, 0.5029, 0.5222, 0.4300, 0.4737,
        0.5300, 0.5648, 0.4471, 0.5556, 0.5342, 0.5268, 0.5461],
       device='cuda:0') torch.Size([16])
percent tensor([0.6181, 0.5477, 0.6279, 0.6314, 0.6278, 0.6492, 0.5946, 0.6350, 0.6027,
        0.5590, 0.5770, 0.5846, 0.5333, 0.6170, 0.5913, 0.6372],
       device='cuda:0') torch.Size([16])
percent tensor([0.6171, 0.6044, 0.6079, 0.6026, 0.5904, 0.5622, 0.6223, 0.6178, 0.6179,
        0.6052, 0.6143, 0.6193, 0.6131, 0.6137, 0.6276, 0.5768],
       device='cuda:0') torch.Size([16])
percent tensor([0.4773, 0.5042, 0.5940, 0.6168, 0.6089, 0.5509, 0.5537, 0.5349, 0.5671,
        0.4845, 0.5327, 0.6112, 0.4730, 0.5874, 0.5150, 0.5137],
       device='cuda:0') torch.Size([16])
percent tensor([0.5939, 0.5928, 0.5448, 0.4921, 0.5943, 0.6430, 0.5590, 0.4956, 0.5782,
        0.5913, 0.5569, 0.4504, 0.5810, 0.5741, 0.4426, 0.6364],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9989, 0.9994, 0.9994, 0.9994, 0.9980, 0.9995, 0.9995, 0.9994,
        0.9992, 0.9992, 0.9995, 0.9992, 0.9990, 0.9992, 0.9993],
       device='cuda:0') torch.Size([16])
Epoch: 63 | Batch_idx: 0 |  Loss: (0.3117) |  Loss2: (0.0000) | Acc: (86.00%) (111/128)
Epoch: 63 | Batch_idx: 10 |  Loss: (0.3316) |  Loss2: (0.0000) | Acc: (88.00%) (1246/1408)
Epoch: 63 | Batch_idx: 20 |  Loss: (0.3227) |  Loss2: (0.0000) | Acc: (88.00%) (2385/2688)
Epoch: 63 | Batch_idx: 30 |  Loss: (0.3160) |  Loss2: (0.0000) | Acc: (89.00%) (3541/3968)
Epoch: 63 | Batch_idx: 40 |  Loss: (0.3191) |  Loss2: (0.0000) | Acc: (89.00%) (4681/5248)
Epoch: 63 | Batch_idx: 50 |  Loss: (0.3239) |  Loss2: (0.0000) | Acc: (88.00%) (5807/6528)
Epoch: 63 | Batch_idx: 60 |  Loss: (0.3263) |  Loss2: (0.0000) | Acc: (88.00%) (6931/7808)
Epoch: 63 | Batch_idx: 70 |  Loss: (0.3269) |  Loss2: (0.0000) | Acc: (88.00%) (8058/9088)
Epoch: 63 | Batch_idx: 80 |  Loss: (0.3339) |  Loss2: (0.0000) | Acc: (88.00%) (9172/10368)
Epoch: 63 | Batch_idx: 90 |  Loss: (0.3360) |  Loss2: (0.0000) | Acc: (88.00%) (10301/11648)
Epoch: 63 | Batch_idx: 100 |  Loss: (0.3389) |  Loss2: (0.0000) | Acc: (88.00%) (11430/12928)
Epoch: 63 | Batch_idx: 110 |  Loss: (0.3426) |  Loss2: (0.0000) | Acc: (88.00%) (12539/14208)
Epoch: 63 | Batch_idx: 120 |  Loss: (0.3414) |  Loss2: (0.0000) | Acc: (88.00%) (13670/15488)
Epoch: 63 | Batch_idx: 130 |  Loss: (0.3405) |  Loss2: (0.0000) | Acc: (88.00%) (14814/16768)
Epoch: 63 | Batch_idx: 140 |  Loss: (0.3401) |  Loss2: (0.0000) | Acc: (88.00%) (15945/18048)
Epoch: 63 | Batch_idx: 150 |  Loss: (0.3388) |  Loss2: (0.0000) | Acc: (88.00%) (17084/19328)
Epoch: 63 | Batch_idx: 160 |  Loss: (0.3398) |  Loss2: (0.0000) | Acc: (88.00%) (18212/20608)
Epoch: 63 | Batch_idx: 170 |  Loss: (0.3375) |  Loss2: (0.0000) | Acc: (88.00%) (19367/21888)
Epoch: 63 | Batch_idx: 180 |  Loss: (0.3370) |  Loss2: (0.0000) | Acc: (88.00%) (20504/23168)
Epoch: 63 | Batch_idx: 190 |  Loss: (0.3378) |  Loss2: (0.0000) | Acc: (88.00%) (21626/24448)
Epoch: 63 | Batch_idx: 200 |  Loss: (0.3369) |  Loss2: (0.0000) | Acc: (88.00%) (22763/25728)
Epoch: 63 | Batch_idx: 210 |  Loss: (0.3372) |  Loss2: (0.0000) | Acc: (88.00%) (23900/27008)
Epoch: 63 | Batch_idx: 220 |  Loss: (0.3363) |  Loss2: (0.0000) | Acc: (88.00%) (25033/28288)
Epoch: 63 | Batch_idx: 230 |  Loss: (0.3357) |  Loss2: (0.0000) | Acc: (88.00%) (26168/29568)
Epoch: 63 | Batch_idx: 240 |  Loss: (0.3347) |  Loss2: (0.0000) | Acc: (88.00%) (27324/30848)
Epoch: 63 | Batch_idx: 250 |  Loss: (0.3348) |  Loss2: (0.0000) | Acc: (88.00%) (28453/32128)
Epoch: 63 | Batch_idx: 260 |  Loss: (0.3354) |  Loss2: (0.0000) | Acc: (88.00%) (29562/33408)
Epoch: 63 | Batch_idx: 270 |  Loss: (0.3354) |  Loss2: (0.0000) | Acc: (88.00%) (30691/34688)
Epoch: 63 | Batch_idx: 280 |  Loss: (0.3347) |  Loss2: (0.0000) | Acc: (88.00%) (31824/35968)
Epoch: 63 | Batch_idx: 290 |  Loss: (0.3344) |  Loss2: (0.0000) | Acc: (88.00%) (32941/37248)
Epoch: 63 | Batch_idx: 300 |  Loss: (0.3340) |  Loss2: (0.0000) | Acc: (88.00%) (34091/38528)
Epoch: 63 | Batch_idx: 310 |  Loss: (0.3338) |  Loss2: (0.0000) | Acc: (88.00%) (35225/39808)
Epoch: 63 | Batch_idx: 320 |  Loss: (0.3342) |  Loss2: (0.0000) | Acc: (88.00%) (36359/41088)
Epoch: 63 | Batch_idx: 330 |  Loss: (0.3348) |  Loss2: (0.0000) | Acc: (88.00%) (37480/42368)
Epoch: 63 | Batch_idx: 340 |  Loss: (0.3354) |  Loss2: (0.0000) | Acc: (88.00%) (38605/43648)
Epoch: 63 | Batch_idx: 350 |  Loss: (0.3352) |  Loss2: (0.0000) | Acc: (88.00%) (39743/44928)
Epoch: 63 | Batch_idx: 360 |  Loss: (0.3353) |  Loss2: (0.0000) | Acc: (88.00%) (40874/46208)
Epoch: 63 | Batch_idx: 370 |  Loss: (0.3358) |  Loss2: (0.0000) | Acc: (88.00%) (42010/47488)
Epoch: 63 | Batch_idx: 380 |  Loss: (0.3355) |  Loss2: (0.0000) | Acc: (88.00%) (43150/48768)
Epoch: 63 | Batch_idx: 390 |  Loss: (0.3351) |  Loss2: (0.0000) | Acc: (88.00%) (44242/50000)
# TEST : Loss: (0.4270) | Acc: (85.00%) (8569/10000)
percent tensor([0.5140, 0.5200, 0.5149, 0.4994, 0.5176, 0.5230, 0.5235, 0.5049, 0.5115,
        0.5198, 0.5215, 0.5201, 0.5134, 0.4989, 0.5214, 0.5114],
       device='cuda:0') torch.Size([16])
percent tensor([0.5142, 0.5132, 0.5153, 0.5181, 0.5155, 0.5161, 0.5138, 0.5160, 0.5132,
        0.5130, 0.5124, 0.5121, 0.5125, 0.5152, 0.5142, 0.5155],
       device='cuda:0') torch.Size([16])
percent tensor([0.5539, 0.5748, 0.4107, 0.4020, 0.3876, 0.5120, 0.5296, 0.4384, 0.4849,
        0.5411, 0.5719, 0.4585, 0.5655, 0.5472, 0.5318, 0.5540],
       device='cuda:0') torch.Size([16])
percent tensor([0.6192, 0.5543, 0.6262, 0.6299, 0.6254, 0.6506, 0.5968, 0.6310, 0.6046,
        0.5622, 0.5832, 0.5850, 0.5390, 0.6249, 0.5915, 0.6396],
       device='cuda:0') torch.Size([16])
percent tensor([0.6169, 0.5979, 0.6138, 0.6074, 0.5951, 0.5612, 0.6224, 0.6218, 0.6176,
        0.6008, 0.6127, 0.6171, 0.6059, 0.6102, 0.6240, 0.5739],
       device='cuda:0') torch.Size([16])
percent tensor([0.4780, 0.5009, 0.5991, 0.6243, 0.6171, 0.5588, 0.5554, 0.5374, 0.5688,
        0.4823, 0.5336, 0.6154, 0.4700, 0.5875, 0.5176, 0.5181],
       device='cuda:0') torch.Size([16])
percent tensor([0.6169, 0.6109, 0.5605, 0.5107, 0.6088, 0.6674, 0.5728, 0.5026, 0.5960,
        0.6083, 0.5765, 0.4627, 0.6027, 0.5854, 0.4606, 0.6538],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9989, 0.9994, 0.9994, 0.9995, 0.9982, 0.9995, 0.9995, 0.9994,
        0.9992, 0.9993, 0.9996, 0.9993, 0.9990, 0.9992, 0.9993],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 64 | Batch_idx: 0 |  Loss: (0.3749) |  Loss2: (0.0000) | Acc: (86.00%) (111/128)
Epoch: 64 | Batch_idx: 10 |  Loss: (0.3601) |  Loss2: (0.0000) | Acc: (87.00%) (1234/1408)
Epoch: 64 | Batch_idx: 20 |  Loss: (0.3396) |  Loss2: (0.0000) | Acc: (88.00%) (2368/2688)
Epoch: 64 | Batch_idx: 30 |  Loss: (0.3356) |  Loss2: (0.0000) | Acc: (88.00%) (3502/3968)
Epoch: 64 | Batch_idx: 40 |  Loss: (0.3436) |  Loss2: (0.0000) | Acc: (87.00%) (4615/5248)
Epoch: 64 | Batch_idx: 50 |  Loss: (0.3417) |  Loss2: (0.0000) | Acc: (88.00%) (5757/6528)
Epoch: 64 | Batch_idx: 60 |  Loss: (0.3396) |  Loss2: (0.0000) | Acc: (88.00%) (6904/7808)
Epoch: 64 | Batch_idx: 70 |  Loss: (0.3411) |  Loss2: (0.0000) | Acc: (88.00%) (8029/9088)
Epoch: 64 | Batch_idx: 80 |  Loss: (0.3424) |  Loss2: (0.0000) | Acc: (88.00%) (9165/10368)
Epoch: 64 | Batch_idx: 90 |  Loss: (0.3397) |  Loss2: (0.0000) | Acc: (88.00%) (10306/11648)
Epoch: 64 | Batch_idx: 100 |  Loss: (0.3404) |  Loss2: (0.0000) | Acc: (88.00%) (11427/12928)
Epoch: 64 | Batch_idx: 110 |  Loss: (0.3420) |  Loss2: (0.0000) | Acc: (88.00%) (12549/14208)
Epoch: 64 | Batch_idx: 120 |  Loss: (0.3424) |  Loss2: (0.0000) | Acc: (88.00%) (13681/15488)
Epoch: 64 | Batch_idx: 130 |  Loss: (0.3379) |  Loss2: (0.0000) | Acc: (88.00%) (14837/16768)
Epoch: 64 | Batch_idx: 140 |  Loss: (0.3359) |  Loss2: (0.0000) | Acc: (88.00%) (15984/18048)
Epoch: 64 | Batch_idx: 150 |  Loss: (0.3346) |  Loss2: (0.0000) | Acc: (88.00%) (17124/19328)
Epoch: 64 | Batch_idx: 160 |  Loss: (0.3358) |  Loss2: (0.0000) | Acc: (88.00%) (18247/20608)
Epoch: 64 | Batch_idx: 170 |  Loss: (0.3362) |  Loss2: (0.0000) | Acc: (88.00%) (19370/21888)
Epoch: 64 | Batch_idx: 180 |  Loss: (0.3363) |  Loss2: (0.0000) | Acc: (88.00%) (20502/23168)
Epoch: 64 | Batch_idx: 190 |  Loss: (0.3386) |  Loss2: (0.0000) | Acc: (88.00%) (21613/24448)
Epoch: 64 | Batch_idx: 200 |  Loss: (0.3391) |  Loss2: (0.0000) | Acc: (88.00%) (22739/25728)
Epoch: 64 | Batch_idx: 210 |  Loss: (0.3378) |  Loss2: (0.0000) | Acc: (88.00%) (23886/27008)
Epoch: 64 | Batch_idx: 220 |  Loss: (0.3389) |  Loss2: (0.0000) | Acc: (88.00%) (24998/28288)
Epoch: 64 | Batch_idx: 230 |  Loss: (0.3392) |  Loss2: (0.0000) | Acc: (88.00%) (26117/29568)
Epoch: 64 | Batch_idx: 240 |  Loss: (0.3387) |  Loss2: (0.0000) | Acc: (88.00%) (27259/30848)
Epoch: 64 | Batch_idx: 250 |  Loss: (0.3399) |  Loss2: (0.0000) | Acc: (88.00%) (28376/32128)
Epoch: 64 | Batch_idx: 260 |  Loss: (0.3403) |  Loss2: (0.0000) | Acc: (88.00%) (29496/33408)
Epoch: 64 | Batch_idx: 270 |  Loss: (0.3412) |  Loss2: (0.0000) | Acc: (88.00%) (30623/34688)
Epoch: 64 | Batch_idx: 280 |  Loss: (0.3416) |  Loss2: (0.0000) | Acc: (88.00%) (31746/35968)
Epoch: 64 | Batch_idx: 290 |  Loss: (0.3412) |  Loss2: (0.0000) | Acc: (88.00%) (32880/37248)
Epoch: 64 | Batch_idx: 300 |  Loss: (0.3400) |  Loss2: (0.0000) | Acc: (88.00%) (34017/38528)
Epoch: 64 | Batch_idx: 310 |  Loss: (0.3392) |  Loss2: (0.0000) | Acc: (88.00%) (35162/39808)
Epoch: 64 | Batch_idx: 320 |  Loss: (0.3398) |  Loss2: (0.0000) | Acc: (88.00%) (36289/41088)
Epoch: 64 | Batch_idx: 330 |  Loss: (0.3379) |  Loss2: (0.0000) | Acc: (88.00%) (37454/42368)
Epoch: 64 | Batch_idx: 340 |  Loss: (0.3377) |  Loss2: (0.0000) | Acc: (88.00%) (38592/43648)
Epoch: 64 | Batch_idx: 350 |  Loss: (0.3368) |  Loss2: (0.0000) | Acc: (88.00%) (39735/44928)
Epoch: 64 | Batch_idx: 360 |  Loss: (0.3380) |  Loss2: (0.0000) | Acc: (88.00%) (40839/46208)
Epoch: 64 | Batch_idx: 370 |  Loss: (0.3390) |  Loss2: (0.0000) | Acc: (88.00%) (41948/47488)
Epoch: 64 | Batch_idx: 380 |  Loss: (0.3386) |  Loss2: (0.0000) | Acc: (88.00%) (43084/48768)
Epoch: 64 | Batch_idx: 390 |  Loss: (0.3389) |  Loss2: (0.0000) | Acc: (88.00%) (44167/50000)
# TEST : Loss: (0.4821) | Acc: (83.00%) (8382/10000)
percent tensor([0.5109, 0.5208, 0.5100, 0.4984, 0.5118, 0.5197, 0.5212, 0.5041, 0.5077,
        0.5189, 0.5195, 0.5157, 0.5114, 0.5000, 0.5205, 0.5098],
       device='cuda:0') torch.Size([16])
percent tensor([0.5142, 0.5133, 0.5150, 0.5175, 0.5154, 0.5160, 0.5139, 0.5157, 0.5134,
        0.5128, 0.5128, 0.5120, 0.5125, 0.5154, 0.5140, 0.5157],
       device='cuda:0') torch.Size([16])
percent tensor([0.5439, 0.5629, 0.4273, 0.4062, 0.3976, 0.4947, 0.5264, 0.4396, 0.4730,
        0.5422, 0.5589, 0.4709, 0.5547, 0.5433, 0.5200, 0.5389],
       device='cuda:0') torch.Size([16])
percent tensor([0.6223, 0.5536, 0.6309, 0.6359, 0.6274, 0.6591, 0.5938, 0.6297, 0.6067,
        0.5626, 0.5882, 0.5837, 0.5405, 0.6236, 0.5916, 0.6445],
       device='cuda:0') torch.Size([16])
percent tensor([0.6067, 0.5984, 0.6073, 0.5994, 0.5915, 0.5421, 0.6210, 0.6204, 0.6141,
        0.5952, 0.6074, 0.6148, 0.5966, 0.6117, 0.6193, 0.5636],
       device='cuda:0') torch.Size([16])
percent tensor([0.4957, 0.5187, 0.5923, 0.6288, 0.6138, 0.5922, 0.5467, 0.5322, 0.5906,
        0.4999, 0.5476, 0.6247, 0.4959, 0.5929, 0.5131, 0.5319],
       device='cuda:0') torch.Size([16])
percent tensor([0.6228, 0.6133, 0.5430, 0.5012, 0.6026, 0.6643, 0.5827, 0.4787, 0.6079,
        0.6422, 0.6001, 0.4453, 0.6043, 0.6264, 0.4830, 0.6467],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9991, 0.9993, 0.9992, 0.9995, 0.9979, 0.9989, 0.9994, 0.9989,
        0.9990, 0.9995, 0.9995, 0.9996, 0.9992, 0.9992, 0.9992],
       device='cuda:0') torch.Size([16])
Epoch: 65 | Batch_idx: 0 |  Loss: (0.2332) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 65 | Batch_idx: 10 |  Loss: (0.3061) |  Loss2: (0.0000) | Acc: (89.00%) (1259/1408)
Epoch: 65 | Batch_idx: 20 |  Loss: (0.3028) |  Loss2: (0.0000) | Acc: (89.00%) (2404/2688)
Epoch: 65 | Batch_idx: 30 |  Loss: (0.3102) |  Loss2: (0.0000) | Acc: (89.00%) (3553/3968)
Epoch: 65 | Batch_idx: 40 |  Loss: (0.3141) |  Loss2: (0.0000) | Acc: (89.00%) (4677/5248)
Epoch: 65 | Batch_idx: 50 |  Loss: (0.3146) |  Loss2: (0.0000) | Acc: (89.00%) (5826/6528)
Epoch: 65 | Batch_idx: 60 |  Loss: (0.3142) |  Loss2: (0.0000) | Acc: (89.00%) (6976/7808)
Epoch: 65 | Batch_idx: 70 |  Loss: (0.3210) |  Loss2: (0.0000) | Acc: (89.00%) (8091/9088)
Epoch: 65 | Batch_idx: 80 |  Loss: (0.3219) |  Loss2: (0.0000) | Acc: (88.00%) (9212/10368)
Epoch: 65 | Batch_idx: 90 |  Loss: (0.3220) |  Loss2: (0.0000) | Acc: (88.00%) (10350/11648)
Epoch: 65 | Batch_idx: 100 |  Loss: (0.3261) |  Loss2: (0.0000) | Acc: (88.00%) (11471/12928)
Epoch: 65 | Batch_idx: 110 |  Loss: (0.3284) |  Loss2: (0.0000) | Acc: (88.00%) (12592/14208)
Epoch: 65 | Batch_idx: 120 |  Loss: (0.3292) |  Loss2: (0.0000) | Acc: (88.00%) (13730/15488)
Epoch: 65 | Batch_idx: 130 |  Loss: (0.3312) |  Loss2: (0.0000) | Acc: (88.00%) (14848/16768)
Epoch: 65 | Batch_idx: 140 |  Loss: (0.3292) |  Loss2: (0.0000) | Acc: (88.00%) (15996/18048)
Epoch: 65 | Batch_idx: 150 |  Loss: (0.3276) |  Loss2: (0.0000) | Acc: (88.00%) (17146/19328)
Epoch: 65 | Batch_idx: 160 |  Loss: (0.3272) |  Loss2: (0.0000) | Acc: (88.00%) (18277/20608)
Epoch: 65 | Batch_idx: 170 |  Loss: (0.3265) |  Loss2: (0.0000) | Acc: (88.00%) (19409/21888)
Epoch: 65 | Batch_idx: 180 |  Loss: (0.3279) |  Loss2: (0.0000) | Acc: (88.00%) (20536/23168)
Epoch: 65 | Batch_idx: 190 |  Loss: (0.3283) |  Loss2: (0.0000) | Acc: (88.00%) (21665/24448)
Epoch: 65 | Batch_idx: 200 |  Loss: (0.3283) |  Loss2: (0.0000) | Acc: (88.00%) (22811/25728)
Epoch: 65 | Batch_idx: 210 |  Loss: (0.3281) |  Loss2: (0.0000) | Acc: (88.00%) (23951/27008)
Epoch: 65 | Batch_idx: 220 |  Loss: (0.3279) |  Loss2: (0.0000) | Acc: (88.00%) (25087/28288)
Epoch: 65 | Batch_idx: 230 |  Loss: (0.3291) |  Loss2: (0.0000) | Acc: (88.00%) (26192/29568)
Epoch: 65 | Batch_idx: 240 |  Loss: (0.3287) |  Loss2: (0.0000) | Acc: (88.00%) (27330/30848)
Epoch: 65 | Batch_idx: 250 |  Loss: (0.3282) |  Loss2: (0.0000) | Acc: (88.00%) (28459/32128)
Epoch: 65 | Batch_idx: 260 |  Loss: (0.3292) |  Loss2: (0.0000) | Acc: (88.00%) (29578/33408)
Epoch: 65 | Batch_idx: 270 |  Loss: (0.3282) |  Loss2: (0.0000) | Acc: (88.00%) (30719/34688)
Epoch: 65 | Batch_idx: 280 |  Loss: (0.3296) |  Loss2: (0.0000) | Acc: (88.00%) (31846/35968)
Epoch: 65 | Batch_idx: 290 |  Loss: (0.3288) |  Loss2: (0.0000) | Acc: (88.00%) (32997/37248)
Epoch: 65 | Batch_idx: 300 |  Loss: (0.3284) |  Loss2: (0.0000) | Acc: (88.00%) (34147/38528)
Epoch: 65 | Batch_idx: 310 |  Loss: (0.3280) |  Loss2: (0.0000) | Acc: (88.00%) (35280/39808)
Epoch: 65 | Batch_idx: 320 |  Loss: (0.3276) |  Loss2: (0.0000) | Acc: (88.00%) (36418/41088)
Epoch: 65 | Batch_idx: 330 |  Loss: (0.3275) |  Loss2: (0.0000) | Acc: (88.00%) (37550/42368)
Epoch: 65 | Batch_idx: 340 |  Loss: (0.3274) |  Loss2: (0.0000) | Acc: (88.00%) (38679/43648)
Epoch: 65 | Batch_idx: 350 |  Loss: (0.3279) |  Loss2: (0.0000) | Acc: (88.00%) (39804/44928)
Epoch: 65 | Batch_idx: 360 |  Loss: (0.3282) |  Loss2: (0.0000) | Acc: (88.00%) (40948/46208)
Epoch: 65 | Batch_idx: 370 |  Loss: (0.3281) |  Loss2: (0.0000) | Acc: (88.00%) (42068/47488)
Epoch: 65 | Batch_idx: 380 |  Loss: (0.3278) |  Loss2: (0.0000) | Acc: (88.00%) (43213/48768)
Epoch: 65 | Batch_idx: 390 |  Loss: (0.3281) |  Loss2: (0.0000) | Acc: (88.00%) (44305/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_065.pth.tar'
# TEST : Loss: (0.4623) | Acc: (84.00%) (8434/10000)
percent tensor([0.5120, 0.5220, 0.5130, 0.4995, 0.5135, 0.5202, 0.5224, 0.5061, 0.5086,
        0.5202, 0.5201, 0.5170, 0.5121, 0.5000, 0.5218, 0.5114],
       device='cuda:0') torch.Size([16])
percent tensor([0.5140, 0.5132, 0.5146, 0.5179, 0.5152, 0.5160, 0.5137, 0.5158, 0.5134,
        0.5125, 0.5125, 0.5118, 0.5126, 0.5161, 0.5141, 0.5157],
       device='cuda:0') torch.Size([16])
percent tensor([0.5578, 0.5668, 0.4256, 0.4203, 0.4093, 0.5271, 0.5303, 0.4407, 0.4764,
        0.5429, 0.5681, 0.4626, 0.5645, 0.5388, 0.5374, 0.5544],
       device='cuda:0') torch.Size([16])
percent tensor([0.6243, 0.5482, 0.6223, 0.6399, 0.6263, 0.6688, 0.5887, 0.6328, 0.5995,
        0.5501, 0.5825, 0.5730, 0.5389, 0.6225, 0.5997, 0.6467],
       device='cuda:0') torch.Size([16])
percent tensor([0.6127, 0.6173, 0.6228, 0.6063, 0.6038, 0.5456, 0.6369, 0.6259, 0.6241,
        0.6136, 0.6175, 0.6275, 0.6075, 0.6195, 0.6292, 0.5716],
       device='cuda:0') torch.Size([16])
percent tensor([0.4807, 0.5169, 0.5826, 0.6128, 0.6222, 0.5823, 0.5314, 0.5511, 0.5853,
        0.4903, 0.5406, 0.6274, 0.4972, 0.5865, 0.5188, 0.5031],
       device='cuda:0') torch.Size([16])
percent tensor([0.6113, 0.5907, 0.5432, 0.5115, 0.5978, 0.6632, 0.5776, 0.4736, 0.5971,
        0.5968, 0.5979, 0.4172, 0.5880, 0.6076, 0.4616, 0.6352],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9994, 0.9994, 0.9994, 0.9996, 0.9982, 0.9993, 0.9995, 0.9994,
        0.9995, 0.9993, 0.9995, 0.9995, 0.9992, 0.9995, 0.9993],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 66 | Batch_idx: 0 |  Loss: (0.2332) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 66 | Batch_idx: 10 |  Loss: (0.3305) |  Loss2: (0.0000) | Acc: (88.00%) (1248/1408)
Epoch: 66 | Batch_idx: 20 |  Loss: (0.3465) |  Loss2: (0.0000) | Acc: (88.00%) (2367/2688)
Epoch: 66 | Batch_idx: 30 |  Loss: (0.3612) |  Loss2: (0.0000) | Acc: (87.00%) (3473/3968)
Epoch: 66 | Batch_idx: 40 |  Loss: (0.3677) |  Loss2: (0.0000) | Acc: (87.00%) (4582/5248)
Epoch: 66 | Batch_idx: 50 |  Loss: (0.3698) |  Loss2: (0.0000) | Acc: (87.00%) (5693/6528)
Epoch: 66 | Batch_idx: 60 |  Loss: (0.3651) |  Loss2: (0.0000) | Acc: (87.00%) (6802/7808)
Epoch: 66 | Batch_idx: 70 |  Loss: (0.3635) |  Loss2: (0.0000) | Acc: (87.00%) (7925/9088)
Epoch: 66 | Batch_idx: 80 |  Loss: (0.3595) |  Loss2: (0.0000) | Acc: (87.00%) (9061/10368)
Epoch: 66 | Batch_idx: 90 |  Loss: (0.3592) |  Loss2: (0.0000) | Acc: (87.00%) (10187/11648)
Epoch: 66 | Batch_idx: 100 |  Loss: (0.3649) |  Loss2: (0.0000) | Acc: (87.00%) (11287/12928)
Epoch: 66 | Batch_idx: 110 |  Loss: (0.3665) |  Loss2: (0.0000) | Acc: (87.00%) (12401/14208)
Epoch: 66 | Batch_idx: 120 |  Loss: (0.3671) |  Loss2: (0.0000) | Acc: (87.00%) (13504/15488)
Epoch: 66 | Batch_idx: 130 |  Loss: (0.3692) |  Loss2: (0.0000) | Acc: (87.00%) (14616/16768)
Epoch: 66 | Batch_idx: 140 |  Loss: (0.3701) |  Loss2: (0.0000) | Acc: (87.00%) (15726/18048)
Epoch: 66 | Batch_idx: 150 |  Loss: (0.3682) |  Loss2: (0.0000) | Acc: (87.00%) (16859/19328)
Epoch: 66 | Batch_idx: 160 |  Loss: (0.3663) |  Loss2: (0.0000) | Acc: (87.00%) (17997/20608)
Epoch: 66 | Batch_idx: 170 |  Loss: (0.3641) |  Loss2: (0.0000) | Acc: (87.00%) (19130/21888)
Epoch: 66 | Batch_idx: 180 |  Loss: (0.3643) |  Loss2: (0.0000) | Acc: (87.00%) (20247/23168)
Epoch: 66 | Batch_idx: 190 |  Loss: (0.3640) |  Loss2: (0.0000) | Acc: (87.00%) (21371/24448)
Epoch: 66 | Batch_idx: 200 |  Loss: (0.3625) |  Loss2: (0.0000) | Acc: (87.00%) (22501/25728)
Epoch: 66 | Batch_idx: 210 |  Loss: (0.3626) |  Loss2: (0.0000) | Acc: (87.00%) (23608/27008)
Epoch: 66 | Batch_idx: 220 |  Loss: (0.3617) |  Loss2: (0.0000) | Acc: (87.00%) (24721/28288)
Epoch: 66 | Batch_idx: 230 |  Loss: (0.3620) |  Loss2: (0.0000) | Acc: (87.00%) (25839/29568)
Epoch: 66 | Batch_idx: 240 |  Loss: (0.3607) |  Loss2: (0.0000) | Acc: (87.00%) (26965/30848)
Epoch: 66 | Batch_idx: 250 |  Loss: (0.3627) |  Loss2: (0.0000) | Acc: (87.00%) (28060/32128)
Epoch: 66 | Batch_idx: 260 |  Loss: (0.3617) |  Loss2: (0.0000) | Acc: (87.00%) (29194/33408)
Epoch: 66 | Batch_idx: 270 |  Loss: (0.3622) |  Loss2: (0.0000) | Acc: (87.00%) (30314/34688)
Epoch: 66 | Batch_idx: 280 |  Loss: (0.3606) |  Loss2: (0.0000) | Acc: (87.00%) (31457/35968)
Epoch: 66 | Batch_idx: 290 |  Loss: (0.3607) |  Loss2: (0.0000) | Acc: (87.00%) (32579/37248)
Epoch: 66 | Batch_idx: 300 |  Loss: (0.3595) |  Loss2: (0.0000) | Acc: (87.00%) (33717/38528)
Epoch: 66 | Batch_idx: 310 |  Loss: (0.3596) |  Loss2: (0.0000) | Acc: (87.00%) (34836/39808)
Epoch: 66 | Batch_idx: 320 |  Loss: (0.3599) |  Loss2: (0.0000) | Acc: (87.00%) (35954/41088)
Epoch: 66 | Batch_idx: 330 |  Loss: (0.3602) |  Loss2: (0.0000) | Acc: (87.00%) (37076/42368)
Epoch: 66 | Batch_idx: 340 |  Loss: (0.3605) |  Loss2: (0.0000) | Acc: (87.00%) (38186/43648)
Epoch: 66 | Batch_idx: 350 |  Loss: (0.3596) |  Loss2: (0.0000) | Acc: (87.00%) (39321/44928)
Epoch: 66 | Batch_idx: 360 |  Loss: (0.3592) |  Loss2: (0.0000) | Acc: (87.00%) (40459/46208)
Epoch: 66 | Batch_idx: 370 |  Loss: (0.3591) |  Loss2: (0.0000) | Acc: (87.00%) (41579/47488)
Epoch: 66 | Batch_idx: 380 |  Loss: (0.3585) |  Loss2: (0.0000) | Acc: (87.00%) (42705/48768)
Epoch: 66 | Batch_idx: 390 |  Loss: (0.3580) |  Loss2: (0.0000) | Acc: (87.00%) (43775/50000)
# TEST : Loss: (0.4445) | Acc: (84.00%) (8495/10000)
percent tensor([0.5270, 0.5377, 0.5279, 0.5095, 0.5309, 0.5370, 0.5397, 0.5209, 0.5249,
        0.5354, 0.5351, 0.5331, 0.5267, 0.5146, 0.5378, 0.5260],
       device='cuda:0') torch.Size([16])
percent tensor([0.5126, 0.5116, 0.5131, 0.5164, 0.5136, 0.5141, 0.5122, 0.5146, 0.5120,
        0.5113, 0.5112, 0.5107, 0.5114, 0.5143, 0.5123, 0.5142],
       device='cuda:0') torch.Size([16])
percent tensor([0.5875, 0.5857, 0.4624, 0.4384, 0.4492, 0.5693, 0.5584, 0.4600, 0.5078,
        0.5686, 0.5946, 0.4962, 0.5864, 0.5582, 0.5691, 0.5816],
       device='cuda:0') torch.Size([16])
percent tensor([0.6221, 0.5167, 0.6206, 0.6476, 0.6302, 0.6746, 0.5801, 0.6428, 0.5885,
        0.5262, 0.5678, 0.5510, 0.4972, 0.6172, 0.5930, 0.6465],
       device='cuda:0') torch.Size([16])
percent tensor([0.5880, 0.5978, 0.5997, 0.5828, 0.5818, 0.5227, 0.6133, 0.6045, 0.6028,
        0.5865, 0.5925, 0.6011, 0.5780, 0.6081, 0.6043, 0.5445],
       device='cuda:0') torch.Size([16])
percent tensor([0.4899, 0.5364, 0.5889, 0.6261, 0.6232, 0.5927, 0.5517, 0.5629, 0.5920,
        0.5058, 0.5618, 0.6372, 0.5178, 0.5904, 0.5366, 0.5209],
       device='cuda:0') torch.Size([16])
percent tensor([0.5895, 0.5771, 0.5230, 0.4666, 0.5800, 0.6396, 0.5623, 0.4494, 0.5931,
        0.5987, 0.5790, 0.3963, 0.5848, 0.5932, 0.4293, 0.6250],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9994, 0.9996, 0.9994, 0.9996, 0.9972, 0.9989, 0.9996, 0.9995,
        0.9993, 0.9995, 0.9996, 0.9996, 0.9992, 0.9993, 0.9991],
       device='cuda:0') torch.Size([16])
Epoch: 67 | Batch_idx: 0 |  Loss: (0.3775) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 67 | Batch_idx: 10 |  Loss: (0.3483) |  Loss2: (0.0000) | Acc: (87.00%) (1236/1408)
Epoch: 67 | Batch_idx: 20 |  Loss: (0.3368) |  Loss2: (0.0000) | Acc: (88.00%) (2368/2688)
Epoch: 67 | Batch_idx: 30 |  Loss: (0.3360) |  Loss2: (0.0000) | Acc: (88.00%) (3497/3968)
Epoch: 67 | Batch_idx: 40 |  Loss: (0.3389) |  Loss2: (0.0000) | Acc: (87.00%) (4616/5248)
Epoch: 67 | Batch_idx: 50 |  Loss: (0.3404) |  Loss2: (0.0000) | Acc: (87.00%) (5740/6528)
Epoch: 67 | Batch_idx: 60 |  Loss: (0.3365) |  Loss2: (0.0000) | Acc: (88.00%) (6878/7808)
Epoch: 67 | Batch_idx: 70 |  Loss: (0.3372) |  Loss2: (0.0000) | Acc: (88.00%) (8014/9088)
Epoch: 67 | Batch_idx: 80 |  Loss: (0.3383) |  Loss2: (0.0000) | Acc: (88.00%) (9137/10368)
Epoch: 67 | Batch_idx: 90 |  Loss: (0.3367) |  Loss2: (0.0000) | Acc: (88.00%) (10261/11648)
Epoch: 67 | Batch_idx: 100 |  Loss: (0.3434) |  Loss2: (0.0000) | Acc: (87.00%) (11360/12928)
Epoch: 67 | Batch_idx: 110 |  Loss: (0.3419) |  Loss2: (0.0000) | Acc: (87.00%) (12484/14208)
Epoch: 67 | Batch_idx: 120 |  Loss: (0.3413) |  Loss2: (0.0000) | Acc: (87.00%) (13620/15488)
Epoch: 67 | Batch_idx: 130 |  Loss: (0.3402) |  Loss2: (0.0000) | Acc: (88.00%) (14769/16768)
Epoch: 67 | Batch_idx: 140 |  Loss: (0.3398) |  Loss2: (0.0000) | Acc: (88.00%) (15897/18048)
Epoch: 67 | Batch_idx: 150 |  Loss: (0.3394) |  Loss2: (0.0000) | Acc: (88.00%) (17029/19328)
Epoch: 67 | Batch_idx: 160 |  Loss: (0.3387) |  Loss2: (0.0000) | Acc: (88.00%) (18178/20608)
Epoch: 67 | Batch_idx: 170 |  Loss: (0.3385) |  Loss2: (0.0000) | Acc: (88.00%) (19316/21888)
Epoch: 67 | Batch_idx: 180 |  Loss: (0.3393) |  Loss2: (0.0000) | Acc: (88.00%) (20437/23168)
Epoch: 67 | Batch_idx: 190 |  Loss: (0.3401) |  Loss2: (0.0000) | Acc: (88.00%) (21555/24448)
Epoch: 67 | Batch_idx: 200 |  Loss: (0.3413) |  Loss2: (0.0000) | Acc: (88.00%) (22662/25728)
Epoch: 67 | Batch_idx: 210 |  Loss: (0.3398) |  Loss2: (0.0000) | Acc: (88.00%) (23814/27008)
Epoch: 67 | Batch_idx: 220 |  Loss: (0.3373) |  Loss2: (0.0000) | Acc: (88.00%) (24969/28288)
Epoch: 67 | Batch_idx: 230 |  Loss: (0.3383) |  Loss2: (0.0000) | Acc: (88.00%) (26091/29568)
Epoch: 67 | Batch_idx: 240 |  Loss: (0.3381) |  Loss2: (0.0000) | Acc: (88.00%) (27227/30848)
Epoch: 67 | Batch_idx: 250 |  Loss: (0.3376) |  Loss2: (0.0000) | Acc: (88.00%) (28355/32128)
Epoch: 67 | Batch_idx: 260 |  Loss: (0.3374) |  Loss2: (0.0000) | Acc: (88.00%) (29497/33408)
Epoch: 67 | Batch_idx: 270 |  Loss: (0.3363) |  Loss2: (0.0000) | Acc: (88.00%) (30647/34688)
Epoch: 67 | Batch_idx: 280 |  Loss: (0.3358) |  Loss2: (0.0000) | Acc: (88.00%) (31791/35968)
Epoch: 67 | Batch_idx: 290 |  Loss: (0.3359) |  Loss2: (0.0000) | Acc: (88.00%) (32914/37248)
Epoch: 67 | Batch_idx: 300 |  Loss: (0.3353) |  Loss2: (0.0000) | Acc: (88.00%) (34057/38528)
Epoch: 67 | Batch_idx: 310 |  Loss: (0.3371) |  Loss2: (0.0000) | Acc: (88.00%) (35160/39808)
Epoch: 67 | Batch_idx: 320 |  Loss: (0.3365) |  Loss2: (0.0000) | Acc: (88.00%) (36308/41088)
Epoch: 67 | Batch_idx: 330 |  Loss: (0.3371) |  Loss2: (0.0000) | Acc: (88.00%) (37423/42368)
Epoch: 67 | Batch_idx: 340 |  Loss: (0.3371) |  Loss2: (0.0000) | Acc: (88.00%) (38555/43648)
Epoch: 67 | Batch_idx: 350 |  Loss: (0.3360) |  Loss2: (0.0000) | Acc: (88.00%) (39706/44928)
Epoch: 67 | Batch_idx: 360 |  Loss: (0.3353) |  Loss2: (0.0000) | Acc: (88.00%) (40854/46208)
Epoch: 67 | Batch_idx: 370 |  Loss: (0.3353) |  Loss2: (0.0000) | Acc: (88.00%) (41980/47488)
Epoch: 67 | Batch_idx: 380 |  Loss: (0.3351) |  Loss2: (0.0000) | Acc: (88.00%) (43107/48768)
Epoch: 67 | Batch_idx: 390 |  Loss: (0.3346) |  Loss2: (0.0000) | Acc: (88.00%) (44201/50000)
# TEST : Loss: (0.4315) | Acc: (85.00%) (8520/10000)
percent tensor([0.5258, 0.5366, 0.5265, 0.5087, 0.5299, 0.5373, 0.5385, 0.5193, 0.5235,
        0.5338, 0.5340, 0.5314, 0.5253, 0.5132, 0.5372, 0.5251],
       device='cuda:0') torch.Size([16])
percent tensor([0.5128, 0.5120, 0.5132, 0.5165, 0.5138, 0.5141, 0.5126, 0.5148, 0.5123,
        0.5116, 0.5115, 0.5110, 0.5117, 0.5145, 0.5125, 0.5144],
       device='cuda:0') torch.Size([16])
percent tensor([0.5847, 0.5771, 0.4619, 0.4372, 0.4475, 0.5742, 0.5508, 0.4569, 0.5026,
        0.5626, 0.5890, 0.4902, 0.5793, 0.5481, 0.5652, 0.5809],
       device='cuda:0') torch.Size([16])
percent tensor([0.6268, 0.5179, 0.6261, 0.6527, 0.6375, 0.6779, 0.5837, 0.6512, 0.5900,
        0.5312, 0.5711, 0.5531, 0.4945, 0.6220, 0.5961, 0.6502],
       device='cuda:0') torch.Size([16])
percent tensor([0.5966, 0.6049, 0.6058, 0.5903, 0.5886, 0.5307, 0.6210, 0.6134, 0.6091,
        0.5903, 0.5987, 0.6075, 0.5830, 0.6183, 0.6130, 0.5538],
       device='cuda:0') torch.Size([16])
percent tensor([0.4934, 0.5412, 0.5903, 0.6285, 0.6238, 0.5957, 0.5533, 0.5605, 0.5983,
        0.5117, 0.5722, 0.6467, 0.5285, 0.5934, 0.5383, 0.5230],
       device='cuda:0') torch.Size([16])
percent tensor([0.5976, 0.5829, 0.5420, 0.4857, 0.6002, 0.6576, 0.5676, 0.4631, 0.5998,
        0.6023, 0.5835, 0.4001, 0.5885, 0.5963, 0.4325, 0.6308],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9994, 0.9996, 0.9994, 0.9997, 0.9975, 0.9991, 0.9996, 0.9995,
        0.9993, 0.9995, 0.9997, 0.9997, 0.9993, 0.9993, 0.9992],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 68 | Batch_idx: 0 |  Loss: (0.3326) |  Loss2: (0.0000) | Acc: (85.00%) (110/128)
Epoch: 68 | Batch_idx: 10 |  Loss: (0.3171) |  Loss2: (0.0000) | Acc: (88.00%) (1252/1408)
Epoch: 68 | Batch_idx: 20 |  Loss: (0.3090) |  Loss2: (0.0000) | Acc: (89.00%) (2402/2688)
Epoch: 68 | Batch_idx: 30 |  Loss: (0.3179) |  Loss2: (0.0000) | Acc: (89.00%) (3538/3968)
Epoch: 68 | Batch_idx: 40 |  Loss: (0.3162) |  Loss2: (0.0000) | Acc: (89.00%) (4684/5248)
Epoch: 68 | Batch_idx: 50 |  Loss: (0.3096) |  Loss2: (0.0000) | Acc: (89.00%) (5845/6528)
Epoch: 68 | Batch_idx: 60 |  Loss: (0.3133) |  Loss2: (0.0000) | Acc: (89.00%) (6979/7808)
Epoch: 68 | Batch_idx: 70 |  Loss: (0.3141) |  Loss2: (0.0000) | Acc: (89.00%) (8116/9088)
Epoch: 68 | Batch_idx: 80 |  Loss: (0.3154) |  Loss2: (0.0000) | Acc: (89.00%) (9255/10368)
Epoch: 68 | Batch_idx: 90 |  Loss: (0.3119) |  Loss2: (0.0000) | Acc: (89.00%) (10409/11648)
Epoch: 68 | Batch_idx: 100 |  Loss: (0.3116) |  Loss2: (0.0000) | Acc: (89.00%) (11546/12928)
Epoch: 68 | Batch_idx: 110 |  Loss: (0.3123) |  Loss2: (0.0000) | Acc: (89.00%) (12689/14208)
Epoch: 68 | Batch_idx: 120 |  Loss: (0.3119) |  Loss2: (0.0000) | Acc: (89.00%) (13828/15488)
Epoch: 68 | Batch_idx: 130 |  Loss: (0.3099) |  Loss2: (0.0000) | Acc: (89.00%) (14997/16768)
Epoch: 68 | Batch_idx: 140 |  Loss: (0.3105) |  Loss2: (0.0000) | Acc: (89.00%) (16136/18048)
Epoch: 68 | Batch_idx: 150 |  Loss: (0.3116) |  Loss2: (0.0000) | Acc: (89.00%) (17272/19328)
Epoch: 68 | Batch_idx: 160 |  Loss: (0.3139) |  Loss2: (0.0000) | Acc: (89.00%) (18389/20608)
Epoch: 68 | Batch_idx: 170 |  Loss: (0.3149) |  Loss2: (0.0000) | Acc: (89.00%) (19510/21888)
Epoch: 68 | Batch_idx: 180 |  Loss: (0.3147) |  Loss2: (0.0000) | Acc: (89.00%) (20660/23168)
Epoch: 68 | Batch_idx: 190 |  Loss: (0.3159) |  Loss2: (0.0000) | Acc: (89.00%) (21789/24448)
Epoch: 68 | Batch_idx: 200 |  Loss: (0.3157) |  Loss2: (0.0000) | Acc: (89.00%) (22936/25728)
Epoch: 68 | Batch_idx: 210 |  Loss: (0.3155) |  Loss2: (0.0000) | Acc: (89.00%) (24080/27008)
Epoch: 68 | Batch_idx: 220 |  Loss: (0.3151) |  Loss2: (0.0000) | Acc: (89.00%) (25224/28288)
Epoch: 68 | Batch_idx: 230 |  Loss: (0.3138) |  Loss2: (0.0000) | Acc: (89.00%) (26382/29568)
Epoch: 68 | Batch_idx: 240 |  Loss: (0.3159) |  Loss2: (0.0000) | Acc: (89.00%) (27493/30848)
Epoch: 68 | Batch_idx: 250 |  Loss: (0.3158) |  Loss2: (0.0000) | Acc: (89.00%) (28637/32128)
Epoch: 68 | Batch_idx: 260 |  Loss: (0.3169) |  Loss2: (0.0000) | Acc: (89.00%) (29769/33408)
Epoch: 68 | Batch_idx: 270 |  Loss: (0.3179) |  Loss2: (0.0000) | Acc: (89.00%) (30897/34688)
Epoch: 68 | Batch_idx: 280 |  Loss: (0.3184) |  Loss2: (0.0000) | Acc: (89.00%) (32035/35968)
Epoch: 68 | Batch_idx: 290 |  Loss: (0.3193) |  Loss2: (0.0000) | Acc: (89.00%) (33170/37248)
Epoch: 68 | Batch_idx: 300 |  Loss: (0.3196) |  Loss2: (0.0000) | Acc: (89.00%) (34307/38528)
Epoch: 68 | Batch_idx: 310 |  Loss: (0.3201) |  Loss2: (0.0000) | Acc: (89.00%) (35432/39808)
Epoch: 68 | Batch_idx: 320 |  Loss: (0.3192) |  Loss2: (0.0000) | Acc: (89.00%) (36594/41088)
Epoch: 68 | Batch_idx: 330 |  Loss: (0.3190) |  Loss2: (0.0000) | Acc: (89.00%) (37740/42368)
Epoch: 68 | Batch_idx: 340 |  Loss: (0.3188) |  Loss2: (0.0000) | Acc: (89.00%) (38882/43648)
Epoch: 68 | Batch_idx: 350 |  Loss: (0.3188) |  Loss2: (0.0000) | Acc: (89.00%) (40029/44928)
Epoch: 68 | Batch_idx: 360 |  Loss: (0.3190) |  Loss2: (0.0000) | Acc: (89.00%) (41154/46208)
Epoch: 68 | Batch_idx: 370 |  Loss: (0.3194) |  Loss2: (0.0000) | Acc: (89.00%) (42288/47488)
Epoch: 68 | Batch_idx: 380 |  Loss: (0.3201) |  Loss2: (0.0000) | Acc: (89.00%) (43413/48768)
Epoch: 68 | Batch_idx: 390 |  Loss: (0.3203) |  Loss2: (0.0000) | Acc: (89.00%) (44506/50000)
# TEST : Loss: (0.4823) | Acc: (84.00%) (8402/10000)
percent tensor([0.5275, 0.5373, 0.5265, 0.5101, 0.5306, 0.5402, 0.5394, 0.5198, 0.5236,
        0.5346, 0.5353, 0.5324, 0.5265, 0.5112, 0.5391, 0.5263],
       device='cuda:0') torch.Size([16])
percent tensor([0.5131, 0.5115, 0.5141, 0.5165, 0.5144, 0.5148, 0.5127, 0.5146, 0.5123,
        0.5116, 0.5112, 0.5113, 0.5114, 0.5135, 0.5126, 0.5143],
       device='cuda:0') torch.Size([16])
percent tensor([0.5829, 0.5754, 0.4631, 0.4404, 0.4451, 0.5572, 0.5496, 0.4785, 0.5033,
        0.5626, 0.5815, 0.4925, 0.5822, 0.5426, 0.5551, 0.5771],
       device='cuda:0') torch.Size([16])
percent tensor([0.6391, 0.5368, 0.6330, 0.6588, 0.6441, 0.6932, 0.5962, 0.6440, 0.6126,
        0.5511, 0.5893, 0.5755, 0.5258, 0.6332, 0.6077, 0.6561],
       device='cuda:0') torch.Size([16])
percent tensor([0.5942, 0.5894, 0.6003, 0.5774, 0.5862, 0.5221, 0.6077, 0.6084, 0.6086,
        0.5830, 0.5977, 0.5944, 0.5835, 0.6002, 0.6036, 0.5478],
       device='cuda:0') torch.Size([16])
percent tensor([0.5182, 0.5667, 0.5984, 0.6400, 0.6210, 0.6116, 0.5829, 0.5409, 0.6099,
        0.5367, 0.5916, 0.6412, 0.5483, 0.6280, 0.5584, 0.5484],
       device='cuda:0') torch.Size([16])
percent tensor([0.6065, 0.5877, 0.5318, 0.4921, 0.5933, 0.6672, 0.5667, 0.4627, 0.5977,
        0.5992, 0.5836, 0.4265, 0.5921, 0.5725, 0.4352, 0.6243],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9990, 0.9997, 0.9997, 0.9997, 0.9989, 0.9993, 0.9998, 0.9995,
        0.9991, 0.9996, 0.9997, 0.9997, 0.9993, 0.9990, 0.9992],
       device='cuda:0') torch.Size([16])
Epoch: 69 | Batch_idx: 0 |  Loss: (0.3112) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 69 | Batch_idx: 10 |  Loss: (0.3313) |  Loss2: (0.0000) | Acc: (87.00%) (1232/1408)
Epoch: 69 | Batch_idx: 20 |  Loss: (0.3154) |  Loss2: (0.0000) | Acc: (89.00%) (2400/2688)
Epoch: 69 | Batch_idx: 30 |  Loss: (0.3092) |  Loss2: (0.0000) | Acc: (89.00%) (3566/3968)
Epoch: 69 | Batch_idx: 40 |  Loss: (0.3118) |  Loss2: (0.0000) | Acc: (89.00%) (4710/5248)
Epoch: 69 | Batch_idx: 50 |  Loss: (0.3105) |  Loss2: (0.0000) | Acc: (89.00%) (5846/6528)
Epoch: 69 | Batch_idx: 60 |  Loss: (0.3065) |  Loss2: (0.0000) | Acc: (89.00%) (7003/7808)
Epoch: 69 | Batch_idx: 70 |  Loss: (0.3034) |  Loss2: (0.0000) | Acc: (89.00%) (8155/9088)
Epoch: 69 | Batch_idx: 80 |  Loss: (0.3070) |  Loss2: (0.0000) | Acc: (89.00%) (9287/10368)
Epoch: 69 | Batch_idx: 90 |  Loss: (0.3069) |  Loss2: (0.0000) | Acc: (89.00%) (10436/11648)
Epoch: 69 | Batch_idx: 100 |  Loss: (0.3080) |  Loss2: (0.0000) | Acc: (89.00%) (11579/12928)
Epoch: 69 | Batch_idx: 110 |  Loss: (0.3081) |  Loss2: (0.0000) | Acc: (89.00%) (12719/14208)
Epoch: 69 | Batch_idx: 120 |  Loss: (0.3083) |  Loss2: (0.0000) | Acc: (89.00%) (13861/15488)
Epoch: 69 | Batch_idx: 130 |  Loss: (0.3080) |  Loss2: (0.0000) | Acc: (89.00%) (14994/16768)
Epoch: 69 | Batch_idx: 140 |  Loss: (0.3084) |  Loss2: (0.0000) | Acc: (89.00%) (16148/18048)
Epoch: 69 | Batch_idx: 150 |  Loss: (0.3094) |  Loss2: (0.0000) | Acc: (89.00%) (17279/19328)
Epoch: 69 | Batch_idx: 160 |  Loss: (0.3121) |  Loss2: (0.0000) | Acc: (89.00%) (18397/20608)
Epoch: 69 | Batch_idx: 170 |  Loss: (0.3120) |  Loss2: (0.0000) | Acc: (89.00%) (19537/21888)
Epoch: 69 | Batch_idx: 180 |  Loss: (0.3118) |  Loss2: (0.0000) | Acc: (89.00%) (20683/23168)
Epoch: 69 | Batch_idx: 190 |  Loss: (0.3116) |  Loss2: (0.0000) | Acc: (89.00%) (21814/24448)
Epoch: 69 | Batch_idx: 200 |  Loss: (0.3102) |  Loss2: (0.0000) | Acc: (89.00%) (22959/25728)
Epoch: 69 | Batch_idx: 210 |  Loss: (0.3098) |  Loss2: (0.0000) | Acc: (89.00%) (24102/27008)
Epoch: 69 | Batch_idx: 220 |  Loss: (0.3099) |  Loss2: (0.0000) | Acc: (89.00%) (25243/28288)
Epoch: 69 | Batch_idx: 230 |  Loss: (0.3099) |  Loss2: (0.0000) | Acc: (89.00%) (26395/29568)
Epoch: 69 | Batch_idx: 240 |  Loss: (0.3089) |  Loss2: (0.0000) | Acc: (89.00%) (27551/30848)
Epoch: 69 | Batch_idx: 250 |  Loss: (0.3091) |  Loss2: (0.0000) | Acc: (89.00%) (28708/32128)
Epoch: 69 | Batch_idx: 260 |  Loss: (0.3098) |  Loss2: (0.0000) | Acc: (89.00%) (29846/33408)
Epoch: 69 | Batch_idx: 270 |  Loss: (0.3090) |  Loss2: (0.0000) | Acc: (89.00%) (31000/34688)
Epoch: 69 | Batch_idx: 280 |  Loss: (0.3106) |  Loss2: (0.0000) | Acc: (89.00%) (32126/35968)
Epoch: 69 | Batch_idx: 290 |  Loss: (0.3096) |  Loss2: (0.0000) | Acc: (89.00%) (33282/37248)
Epoch: 69 | Batch_idx: 300 |  Loss: (0.3095) |  Loss2: (0.0000) | Acc: (89.00%) (34422/38528)
Epoch: 69 | Batch_idx: 310 |  Loss: (0.3088) |  Loss2: (0.0000) | Acc: (89.00%) (35569/39808)
Epoch: 69 | Batch_idx: 320 |  Loss: (0.3093) |  Loss2: (0.0000) | Acc: (89.00%) (36701/41088)
Epoch: 69 | Batch_idx: 330 |  Loss: (0.3092) |  Loss2: (0.0000) | Acc: (89.00%) (37846/42368)
Epoch: 69 | Batch_idx: 340 |  Loss: (0.3111) |  Loss2: (0.0000) | Acc: (89.00%) (38962/43648)
Epoch: 69 | Batch_idx: 350 |  Loss: (0.3123) |  Loss2: (0.0000) | Acc: (89.00%) (40091/44928)
Epoch: 69 | Batch_idx: 360 |  Loss: (0.3136) |  Loss2: (0.0000) | Acc: (89.00%) (41209/46208)
Epoch: 69 | Batch_idx: 370 |  Loss: (0.3136) |  Loss2: (0.0000) | Acc: (89.00%) (42346/47488)
Epoch: 69 | Batch_idx: 380 |  Loss: (0.3132) |  Loss2: (0.0000) | Acc: (89.00%) (43489/48768)
Epoch: 69 | Batch_idx: 390 |  Loss: (0.3125) |  Loss2: (0.0000) | Acc: (89.00%) (44603/50000)
# TEST : Loss: (0.4766) | Acc: (84.00%) (8432/10000)
percent tensor([0.5280, 0.5363, 0.5281, 0.5090, 0.5324, 0.5400, 0.5394, 0.5205, 0.5238,
        0.5344, 0.5348, 0.5338, 0.5265, 0.5095, 0.5382, 0.5254],
       device='cuda:0') torch.Size([16])
percent tensor([0.5130, 0.5121, 0.5138, 0.5161, 0.5140, 0.5138, 0.5129, 0.5146, 0.5126,
        0.5118, 0.5118, 0.5113, 0.5120, 0.5138, 0.5126, 0.5142],
       device='cuda:0') torch.Size([16])
percent tensor([0.5878, 0.5800, 0.4681, 0.4521, 0.4538, 0.5670, 0.5525, 0.4808, 0.5170,
        0.5664, 0.5830, 0.4993, 0.5890, 0.5403, 0.5624, 0.5844],
       device='cuda:0') torch.Size([16])
percent tensor([0.6404, 0.5316, 0.6381, 0.6594, 0.6534, 0.6787, 0.5962, 0.6487, 0.6164,
        0.5481, 0.5812, 0.5730, 0.5222, 0.6261, 0.5995, 0.6539],
       device='cuda:0') torch.Size([16])
percent tensor([0.5970, 0.5938, 0.6052, 0.5897, 0.5928, 0.5444, 0.6142, 0.6162, 0.6027,
        0.5838, 0.5939, 0.6012, 0.5819, 0.6020, 0.6152, 0.5550],
       device='cuda:0') torch.Size([16])
percent tensor([0.4886, 0.5354, 0.5726, 0.6365, 0.6090, 0.5972, 0.5561, 0.5290, 0.6020,
        0.4878, 0.5685, 0.6126, 0.5128, 0.6091, 0.5332, 0.5225],
       device='cuda:0') torch.Size([16])
percent tensor([0.5938, 0.5945, 0.5442, 0.5000, 0.6075, 0.6721, 0.5498, 0.4652, 0.5842,
        0.6026, 0.5764, 0.4503, 0.5843, 0.5798, 0.4491, 0.6308],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9985, 0.9997, 0.9995, 0.9997, 0.9983, 0.9994, 0.9996, 0.9992,
        0.9991, 0.9993, 0.9998, 0.9992, 0.9991, 0.9988, 0.9994],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 70 | Batch_idx: 0 |  Loss: (0.2745) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 70 | Batch_idx: 10 |  Loss: (0.3388) |  Loss2: (0.0000) | Acc: (87.00%) (1233/1408)
Epoch: 70 | Batch_idx: 20 |  Loss: (0.3448) |  Loss2: (0.0000) | Acc: (87.00%) (2363/2688)
Epoch: 70 | Batch_idx: 30 |  Loss: (0.3563) |  Loss2: (0.0000) | Acc: (87.00%) (3479/3968)
Epoch: 70 | Batch_idx: 40 |  Loss: (0.3498) |  Loss2: (0.0000) | Acc: (87.00%) (4614/5248)
Epoch: 70 | Batch_idx: 50 |  Loss: (0.3538) |  Loss2: (0.0000) | Acc: (87.00%) (5729/6528)
Epoch: 70 | Batch_idx: 60 |  Loss: (0.3500) |  Loss2: (0.0000) | Acc: (87.00%) (6864/7808)
Epoch: 70 | Batch_idx: 70 |  Loss: (0.3507) |  Loss2: (0.0000) | Acc: (87.00%) (7992/9088)
Epoch: 70 | Batch_idx: 80 |  Loss: (0.3477) |  Loss2: (0.0000) | Acc: (87.00%) (9111/10368)
Epoch: 70 | Batch_idx: 90 |  Loss: (0.3529) |  Loss2: (0.0000) | Acc: (87.00%) (10229/11648)
Epoch: 70 | Batch_idx: 100 |  Loss: (0.3523) |  Loss2: (0.0000) | Acc: (87.00%) (11354/12928)
Epoch: 70 | Batch_idx: 110 |  Loss: (0.3548) |  Loss2: (0.0000) | Acc: (87.00%) (12462/14208)
Epoch: 70 | Batch_idx: 120 |  Loss: (0.3567) |  Loss2: (0.0000) | Acc: (87.00%) (13580/15488)
Epoch: 70 | Batch_idx: 130 |  Loss: (0.3578) |  Loss2: (0.0000) | Acc: (87.00%) (14689/16768)
Epoch: 70 | Batch_idx: 140 |  Loss: (0.3572) |  Loss2: (0.0000) | Acc: (87.00%) (15812/18048)
Epoch: 70 | Batch_idx: 150 |  Loss: (0.3555) |  Loss2: (0.0000) | Acc: (87.00%) (16936/19328)
Epoch: 70 | Batch_idx: 160 |  Loss: (0.3535) |  Loss2: (0.0000) | Acc: (87.00%) (18079/20608)
Epoch: 70 | Batch_idx: 170 |  Loss: (0.3541) |  Loss2: (0.0000) | Acc: (87.00%) (19203/21888)
Epoch: 70 | Batch_idx: 180 |  Loss: (0.3546) |  Loss2: (0.0000) | Acc: (87.00%) (20321/23168)
Epoch: 70 | Batch_idx: 190 |  Loss: (0.3546) |  Loss2: (0.0000) | Acc: (87.00%) (21442/24448)
Epoch: 70 | Batch_idx: 200 |  Loss: (0.3541) |  Loss2: (0.0000) | Acc: (87.00%) (22574/25728)
Epoch: 70 | Batch_idx: 210 |  Loss: (0.3545) |  Loss2: (0.0000) | Acc: (87.00%) (23682/27008)
Epoch: 70 | Batch_idx: 220 |  Loss: (0.3524) |  Loss2: (0.0000) | Acc: (87.00%) (24838/28288)
Epoch: 70 | Batch_idx: 230 |  Loss: (0.3519) |  Loss2: (0.0000) | Acc: (87.00%) (25976/29568)
Epoch: 70 | Batch_idx: 240 |  Loss: (0.3495) |  Loss2: (0.0000) | Acc: (87.00%) (27137/30848)
Epoch: 70 | Batch_idx: 250 |  Loss: (0.3491) |  Loss2: (0.0000) | Acc: (87.00%) (28261/32128)
Epoch: 70 | Batch_idx: 260 |  Loss: (0.3481) |  Loss2: (0.0000) | Acc: (87.00%) (29391/33408)
Epoch: 70 | Batch_idx: 270 |  Loss: (0.3477) |  Loss2: (0.0000) | Acc: (88.00%) (30541/34688)
Epoch: 70 | Batch_idx: 280 |  Loss: (0.3471) |  Loss2: (0.0000) | Acc: (88.00%) (31674/35968)
Epoch: 70 | Batch_idx: 290 |  Loss: (0.3457) |  Loss2: (0.0000) | Acc: (88.00%) (32819/37248)
Epoch: 70 | Batch_idx: 300 |  Loss: (0.3441) |  Loss2: (0.0000) | Acc: (88.00%) (33966/38528)
Epoch: 70 | Batch_idx: 310 |  Loss: (0.3440) |  Loss2: (0.0000) | Acc: (88.00%) (35096/39808)
Epoch: 70 | Batch_idx: 320 |  Loss: (0.3431) |  Loss2: (0.0000) | Acc: (88.00%) (36235/41088)
Epoch: 70 | Batch_idx: 330 |  Loss: (0.3418) |  Loss2: (0.0000) | Acc: (88.00%) (37378/42368)
Epoch: 70 | Batch_idx: 340 |  Loss: (0.3405) |  Loss2: (0.0000) | Acc: (88.00%) (38527/43648)
Epoch: 70 | Batch_idx: 350 |  Loss: (0.3393) |  Loss2: (0.0000) | Acc: (88.00%) (39683/44928)
Epoch: 70 | Batch_idx: 360 |  Loss: (0.3397) |  Loss2: (0.0000) | Acc: (88.00%) (40806/46208)
Epoch: 70 | Batch_idx: 370 |  Loss: (0.3398) |  Loss2: (0.0000) | Acc: (88.00%) (41956/47488)
Epoch: 70 | Batch_idx: 380 |  Loss: (0.3389) |  Loss2: (0.0000) | Acc: (88.00%) (43107/48768)
Epoch: 70 | Batch_idx: 390 |  Loss: (0.3378) |  Loss2: (0.0000) | Acc: (88.00%) (44207/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_070.pth.tar'
# TEST : Loss: (0.4491) | Acc: (84.00%) (8494/10000)
percent tensor([0.5470, 0.5568, 0.5509, 0.5302, 0.5557, 0.5623, 0.5613, 0.5414, 0.5423,
        0.5549, 0.5536, 0.5562, 0.5447, 0.5267, 0.5600, 0.5453],
       device='cuda:0') torch.Size([16])
percent tensor([0.5147, 0.5138, 0.5151, 0.5179, 0.5153, 0.5154, 0.5143, 0.5164, 0.5143,
        0.5135, 0.5135, 0.5124, 0.5137, 0.5156, 0.5143, 0.5161],
       device='cuda:0') torch.Size([16])
percent tensor([0.5794, 0.5770, 0.4458, 0.4328, 0.4313, 0.5674, 0.5442, 0.4580, 0.4928,
        0.5546, 0.5733, 0.4784, 0.5740, 0.5330, 0.5610, 0.5821],
       device='cuda:0') torch.Size([16])
percent tensor([0.6398, 0.5440, 0.6302, 0.6455, 0.6414, 0.6899, 0.5998, 0.6464, 0.6144,
        0.5558, 0.5881, 0.5664, 0.5260, 0.6406, 0.6029, 0.6679],
       device='cuda:0') torch.Size([16])
percent tensor([0.5922, 0.5986, 0.6078, 0.5944, 0.5834, 0.5422, 0.6106, 0.6099, 0.6108,
        0.5824, 0.5966, 0.6062, 0.5867, 0.6074, 0.6159, 0.5461],
       device='cuda:0') torch.Size([16])
percent tensor([0.5115, 0.5631, 0.6028, 0.6679, 0.6323, 0.6302, 0.5810, 0.5609, 0.6288,
        0.5083, 0.5917, 0.6384, 0.5411, 0.6354, 0.5606, 0.5550],
       device='cuda:0') torch.Size([16])
percent tensor([0.5697, 0.5714, 0.5326, 0.4677, 0.6077, 0.6399, 0.5477, 0.4476, 0.5694,
        0.5860, 0.5593, 0.4273, 0.5689, 0.5467, 0.4154, 0.6068],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9988, 0.9996, 0.9996, 0.9996, 0.9977, 0.9993, 0.9994, 0.9996,
        0.9993, 0.9996, 0.9998, 0.9994, 0.9994, 0.9989, 0.9992],
       device='cuda:0') torch.Size([16])
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(177.2492, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(805.7850, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(799.8856, device='cuda:0')
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1523.7743, device='cuda:0')
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(497.7179, device='cuda:0')
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2218.5452, device='cuda:0')
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4272.4575, device='cuda:0')
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1413.6947, device='cuda:0')
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6127.9463, device='cuda:0')
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11978.3496, device='cuda:0')
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3984.4360, device='cuda:0')
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16843.8516, device='cuda:0')
Epoch: 71 | Batch_idx: 0 |  Loss: (0.3703) |  Loss2: (0.0000) | Acc: (86.00%) (111/128)
Epoch: 71 | Batch_idx: 10 |  Loss: (0.3349) |  Loss2: (0.0000) | Acc: (88.00%) (1246/1408)
Epoch: 71 | Batch_idx: 20 |  Loss: (0.2976) |  Loss2: (0.0000) | Acc: (89.00%) (2411/2688)
Epoch: 71 | Batch_idx: 30 |  Loss: (0.3114) |  Loss2: (0.0000) | Acc: (89.00%) (3545/3968)
Epoch: 71 | Batch_idx: 40 |  Loss: (0.3145) |  Loss2: (0.0000) | Acc: (89.00%) (4671/5248)
Epoch: 71 | Batch_idx: 50 |  Loss: (0.3136) |  Loss2: (0.0000) | Acc: (89.00%) (5823/6528)
Epoch: 71 | Batch_idx: 60 |  Loss: (0.3162) |  Loss2: (0.0000) | Acc: (88.00%) (6949/7808)
Epoch: 71 | Batch_idx: 70 |  Loss: (0.3149) |  Loss2: (0.0000) | Acc: (88.00%) (8084/9088)
Epoch: 71 | Batch_idx: 80 |  Loss: (0.3120) |  Loss2: (0.0000) | Acc: (89.00%) (9242/10368)
Epoch: 71 | Batch_idx: 90 |  Loss: (0.3146) |  Loss2: (0.0000) | Acc: (89.00%) (10379/11648)
Epoch: 71 | Batch_idx: 100 |  Loss: (0.3127) |  Loss2: (0.0000) | Acc: (89.00%) (11529/12928)
Epoch: 71 | Batch_idx: 110 |  Loss: (0.3126) |  Loss2: (0.0000) | Acc: (89.00%) (12668/14208)
Epoch: 71 | Batch_idx: 120 |  Loss: (0.3080) |  Loss2: (0.0000) | Acc: (89.00%) (13842/15488)
Epoch: 71 | Batch_idx: 130 |  Loss: (0.3094) |  Loss2: (0.0000) | Acc: (89.00%) (14968/16768)
Epoch: 71 | Batch_idx: 140 |  Loss: (0.3099) |  Loss2: (0.0000) | Acc: (89.00%) (16106/18048)
Epoch: 71 | Batch_idx: 150 |  Loss: (0.3110) |  Loss2: (0.0000) | Acc: (89.00%) (17231/19328)
Epoch: 71 | Batch_idx: 160 |  Loss: (0.3117) |  Loss2: (0.0000) | Acc: (89.00%) (18365/20608)
Epoch: 71 | Batch_idx: 170 |  Loss: (0.3096) |  Loss2: (0.0000) | Acc: (89.00%) (19526/21888)
Epoch: 71 | Batch_idx: 180 |  Loss: (0.3097) |  Loss2: (0.0000) | Acc: (89.00%) (20671/23168)
Epoch: 71 | Batch_idx: 190 |  Loss: (0.3097) |  Loss2: (0.0000) | Acc: (89.00%) (21805/24448)
Epoch: 71 | Batch_idx: 200 |  Loss: (0.3115) |  Loss2: (0.0000) | Acc: (89.00%) (22938/25728)
Epoch: 71 | Batch_idx: 210 |  Loss: (0.3103) |  Loss2: (0.0000) | Acc: (89.00%) (24098/27008)
Epoch: 71 | Batch_idx: 220 |  Loss: (0.3107) |  Loss2: (0.0000) | Acc: (89.00%) (25244/28288)
Epoch: 71 | Batch_idx: 230 |  Loss: (0.3104) |  Loss2: (0.0000) | Acc: (89.00%) (26388/29568)
Epoch: 71 | Batch_idx: 240 |  Loss: (0.3096) |  Loss2: (0.0000) | Acc: (89.00%) (27529/30848)
Epoch: 71 | Batch_idx: 250 |  Loss: (0.3096) |  Loss2: (0.0000) | Acc: (89.00%) (28673/32128)
Epoch: 71 | Batch_idx: 260 |  Loss: (0.3091) |  Loss2: (0.0000) | Acc: (89.00%) (29816/33408)
Epoch: 71 | Batch_idx: 270 |  Loss: (0.3089) |  Loss2: (0.0000) | Acc: (89.00%) (30963/34688)
Epoch: 71 | Batch_idx: 280 |  Loss: (0.3086) |  Loss2: (0.0000) | Acc: (89.00%) (32113/35968)
Epoch: 71 | Batch_idx: 290 |  Loss: (0.3088) |  Loss2: (0.0000) | Acc: (89.00%) (33262/37248)
Epoch: 71 | Batch_idx: 300 |  Loss: (0.3087) |  Loss2: (0.0000) | Acc: (89.00%) (34411/38528)
Epoch: 71 | Batch_idx: 310 |  Loss: (0.3091) |  Loss2: (0.0000) | Acc: (89.00%) (35549/39808)
Epoch: 71 | Batch_idx: 320 |  Loss: (0.3091) |  Loss2: (0.0000) | Acc: (89.00%) (36696/41088)
Epoch: 71 | Batch_idx: 330 |  Loss: (0.3101) |  Loss2: (0.0000) | Acc: (89.00%) (37833/42368)
Epoch: 71 | Batch_idx: 340 |  Loss: (0.3100) |  Loss2: (0.0000) | Acc: (89.00%) (38986/43648)
Epoch: 71 | Batch_idx: 350 |  Loss: (0.3105) |  Loss2: (0.0000) | Acc: (89.00%) (40115/44928)
Epoch: 71 | Batch_idx: 360 |  Loss: (0.3108) |  Loss2: (0.0000) | Acc: (89.00%) (41258/46208)
Epoch: 71 | Batch_idx: 370 |  Loss: (0.3106) |  Loss2: (0.0000) | Acc: (89.00%) (42410/47488)
Epoch: 71 | Batch_idx: 380 |  Loss: (0.3111) |  Loss2: (0.0000) | Acc: (89.00%) (43544/48768)
Epoch: 71 | Batch_idx: 390 |  Loss: (0.3111) |  Loss2: (0.0000) | Acc: (89.00%) (44642/50000)
# TEST : Loss: (0.4293) | Acc: (85.00%) (8558/10000)
percent tensor([0.5452, 0.5547, 0.5492, 0.5299, 0.5540, 0.5614, 0.5590, 0.5398, 0.5405,
        0.5527, 0.5517, 0.5542, 0.5428, 0.5255, 0.5583, 0.5441],
       device='cuda:0') torch.Size([16])
percent tensor([0.5150, 0.5142, 0.5155, 0.5182, 0.5156, 0.5158, 0.5146, 0.5167, 0.5146,
        0.5138, 0.5138, 0.5126, 0.5140, 0.5160, 0.5147, 0.5165],
       device='cuda:0') torch.Size([16])
percent tensor([0.5764, 0.5725, 0.4431, 0.4337, 0.4277, 0.5703, 0.5386, 0.4533, 0.4877,
        0.5476, 0.5696, 0.4747, 0.5686, 0.5315, 0.5578, 0.5805],
       device='cuda:0') torch.Size([16])
percent tensor([0.6438, 0.5497, 0.6292, 0.6462, 0.6372, 0.7049, 0.6003, 0.6431, 0.6150,
        0.5590, 0.5962, 0.5630, 0.5314, 0.6474, 0.6062, 0.6782],
       device='cuda:0') torch.Size([16])
percent tensor([0.5877, 0.5989, 0.6083, 0.5981, 0.5812, 0.5396, 0.6080, 0.6073, 0.6128,
        0.5795, 0.5954, 0.6107, 0.5884, 0.6106, 0.6154, 0.5391],
       device='cuda:0') torch.Size([16])
percent tensor([0.5098, 0.5606, 0.6041, 0.6691, 0.6354, 0.6343, 0.5800, 0.5593, 0.6340,
        0.5057, 0.5971, 0.6400, 0.5384, 0.6370, 0.5563, 0.5524],
       device='cuda:0') torch.Size([16])
percent tensor([0.5794, 0.5876, 0.5565, 0.4912, 0.6343, 0.6565, 0.5598, 0.4619, 0.5890,
        0.6011, 0.5766, 0.4464, 0.5796, 0.5651, 0.4234, 0.6095],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9988, 0.9996, 0.9996, 0.9997, 0.9978, 0.9993, 0.9994, 0.9996,
        0.9993, 0.9996, 0.9998, 0.9993, 0.9994, 0.9988, 0.9992],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 72 | Batch_idx: 0 |  Loss: (0.3126) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 72 | Batch_idx: 10 |  Loss: (0.2726) |  Loss2: (0.0000) | Acc: (90.00%) (1279/1408)
Epoch: 72 | Batch_idx: 20 |  Loss: (0.2911) |  Loss2: (0.0000) | Acc: (90.00%) (2424/2688)
Epoch: 72 | Batch_idx: 30 |  Loss: (0.2989) |  Loss2: (0.0000) | Acc: (89.00%) (3559/3968)
Epoch: 72 | Batch_idx: 40 |  Loss: (0.3008) |  Loss2: (0.0000) | Acc: (89.00%) (4703/5248)
Epoch: 72 | Batch_idx: 50 |  Loss: (0.2995) |  Loss2: (0.0000) | Acc: (89.00%) (5842/6528)
Epoch: 72 | Batch_idx: 60 |  Loss: (0.3037) |  Loss2: (0.0000) | Acc: (89.00%) (6982/7808)
Epoch: 72 | Batch_idx: 70 |  Loss: (0.3058) |  Loss2: (0.0000) | Acc: (89.00%) (8123/9088)
Epoch: 72 | Batch_idx: 80 |  Loss: (0.3054) |  Loss2: (0.0000) | Acc: (89.00%) (9270/10368)
Epoch: 72 | Batch_idx: 90 |  Loss: (0.3066) |  Loss2: (0.0000) | Acc: (89.00%) (10410/11648)
Epoch: 72 | Batch_idx: 100 |  Loss: (0.3066) |  Loss2: (0.0000) | Acc: (89.00%) (11553/12928)
Epoch: 72 | Batch_idx: 110 |  Loss: (0.3037) |  Loss2: (0.0000) | Acc: (89.00%) (12719/14208)
Epoch: 72 | Batch_idx: 120 |  Loss: (0.3027) |  Loss2: (0.0000) | Acc: (89.00%) (13864/15488)
Epoch: 72 | Batch_idx: 130 |  Loss: (0.3043) |  Loss2: (0.0000) | Acc: (89.00%) (14999/16768)
Epoch: 72 | Batch_idx: 140 |  Loss: (0.3026) |  Loss2: (0.0000) | Acc: (89.00%) (16150/18048)
Epoch: 72 | Batch_idx: 150 |  Loss: (0.3047) |  Loss2: (0.0000) | Acc: (89.00%) (17290/19328)
Epoch: 72 | Batch_idx: 160 |  Loss: (0.3057) |  Loss2: (0.0000) | Acc: (89.00%) (18424/20608)
Epoch: 72 | Batch_idx: 170 |  Loss: (0.3039) |  Loss2: (0.0000) | Acc: (89.00%) (19587/21888)
Epoch: 72 | Batch_idx: 180 |  Loss: (0.3052) |  Loss2: (0.0000) | Acc: (89.00%) (20723/23168)
Epoch: 72 | Batch_idx: 190 |  Loss: (0.3053) |  Loss2: (0.0000) | Acc: (89.00%) (21867/24448)
Epoch: 72 | Batch_idx: 200 |  Loss: (0.3051) |  Loss2: (0.0000) | Acc: (89.00%) (23013/25728)
Epoch: 72 | Batch_idx: 210 |  Loss: (0.3059) |  Loss2: (0.0000) | Acc: (89.00%) (24147/27008)
Epoch: 72 | Batch_idx: 220 |  Loss: (0.3054) |  Loss2: (0.0000) | Acc: (89.00%) (25293/28288)
Epoch: 72 | Batch_idx: 230 |  Loss: (0.3047) |  Loss2: (0.0000) | Acc: (89.00%) (26441/29568)
Epoch: 72 | Batch_idx: 240 |  Loss: (0.3044) |  Loss2: (0.0000) | Acc: (89.00%) (27584/30848)
Epoch: 72 | Batch_idx: 250 |  Loss: (0.3051) |  Loss2: (0.0000) | Acc: (89.00%) (28727/32128)
Epoch: 72 | Batch_idx: 260 |  Loss: (0.3049) |  Loss2: (0.0000) | Acc: (89.00%) (29871/33408)
Epoch: 72 | Batch_idx: 270 |  Loss: (0.3052) |  Loss2: (0.0000) | Acc: (89.00%) (31016/34688)
Epoch: 72 | Batch_idx: 280 |  Loss: (0.3059) |  Loss2: (0.0000) | Acc: (89.00%) (32155/35968)
Epoch: 72 | Batch_idx: 290 |  Loss: (0.3067) |  Loss2: (0.0000) | Acc: (89.00%) (33290/37248)
Epoch: 72 | Batch_idx: 300 |  Loss: (0.3063) |  Loss2: (0.0000) | Acc: (89.00%) (34444/38528)
Epoch: 72 | Batch_idx: 310 |  Loss: (0.3057) |  Loss2: (0.0000) | Acc: (89.00%) (35595/39808)
Epoch: 72 | Batch_idx: 320 |  Loss: (0.3044) |  Loss2: (0.0000) | Acc: (89.00%) (36762/41088)
Epoch: 72 | Batch_idx: 330 |  Loss: (0.3038) |  Loss2: (0.0000) | Acc: (89.00%) (37909/42368)
Epoch: 72 | Batch_idx: 340 |  Loss: (0.3044) |  Loss2: (0.0000) | Acc: (89.00%) (39053/43648)
Epoch: 72 | Batch_idx: 350 |  Loss: (0.3051) |  Loss2: (0.0000) | Acc: (89.00%) (40201/44928)
Epoch: 72 | Batch_idx: 360 |  Loss: (0.3063) |  Loss2: (0.0000) | Acc: (89.00%) (41333/46208)
Epoch: 72 | Batch_idx: 370 |  Loss: (0.3052) |  Loss2: (0.0000) | Acc: (89.00%) (42498/47488)
Epoch: 72 | Batch_idx: 380 |  Loss: (0.3060) |  Loss2: (0.0000) | Acc: (89.00%) (43610/48768)
Epoch: 72 | Batch_idx: 390 |  Loss: (0.3055) |  Loss2: (0.0000) | Acc: (89.00%) (44718/50000)
# TEST : Loss: (0.4579) | Acc: (84.00%) (8467/10000)
percent tensor([0.5451, 0.5550, 0.5467, 0.5279, 0.5515, 0.5611, 0.5584, 0.5390, 0.5412,
        0.5515, 0.5522, 0.5514, 0.5427, 0.5271, 0.5579, 0.5436],
       device='cuda:0') torch.Size([16])
percent tensor([0.5148, 0.5140, 0.5158, 0.5190, 0.5161, 0.5167, 0.5145, 0.5168, 0.5143,
        0.5134, 0.5134, 0.5126, 0.5135, 0.5164, 0.5147, 0.5167],
       device='cuda:0') torch.Size([16])
percent tensor([0.5829, 0.5706, 0.4503, 0.4310, 0.4247, 0.5666, 0.5393, 0.4523, 0.5022,
        0.5512, 0.5749, 0.4846, 0.5746, 0.5374, 0.5564, 0.5783],
       device='cuda:0') torch.Size([16])
percent tensor([0.6483, 0.5396, 0.6429, 0.6590, 0.6477, 0.7146, 0.5958, 0.6462, 0.6045,
        0.5613, 0.5910, 0.5693, 0.5322, 0.6429, 0.6098, 0.6832],
       device='cuda:0') torch.Size([16])
percent tensor([0.5985, 0.6162, 0.6073, 0.5985, 0.5868, 0.5307, 0.6187, 0.6155, 0.6311,
        0.5974, 0.6238, 0.6195, 0.6090, 0.6216, 0.6207, 0.5499],
       device='cuda:0') torch.Size([16])
percent tensor([0.5037, 0.5698, 0.6174, 0.6589, 0.6538, 0.6186, 0.5920, 0.5752, 0.6203,
        0.5097, 0.5746, 0.6531, 0.5318, 0.6365, 0.5514, 0.5338],
       device='cuda:0') torch.Size([16])
percent tensor([0.5548, 0.5681, 0.5351, 0.4878, 0.5922, 0.6315, 0.5318, 0.4382, 0.5492,
        0.5899, 0.5548, 0.4246, 0.5525, 0.5580, 0.4079, 0.5826],
       device='cuda:0') torch.Size([16])
percent tensor([0.9996, 0.9995, 0.9996, 0.9994, 0.9997, 0.9982, 0.9991, 0.9995, 0.9996,
        0.9994, 0.9996, 0.9997, 0.9997, 0.9996, 0.9993, 0.9993],
       device='cuda:0') torch.Size([16])
Epoch: 73 | Batch_idx: 0 |  Loss: (0.3221) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 73 | Batch_idx: 10 |  Loss: (0.2903) |  Loss2: (0.0000) | Acc: (89.00%) (1260/1408)
Epoch: 73 | Batch_idx: 20 |  Loss: (0.2871) |  Loss2: (0.0000) | Acc: (89.00%) (2415/2688)
Epoch: 73 | Batch_idx: 30 |  Loss: (0.2908) |  Loss2: (0.0000) | Acc: (89.00%) (3554/3968)
Epoch: 73 | Batch_idx: 40 |  Loss: (0.2891) |  Loss2: (0.0000) | Acc: (89.00%) (4705/5248)
Epoch: 73 | Batch_idx: 50 |  Loss: (0.2857) |  Loss2: (0.0000) | Acc: (89.00%) (5862/6528)
Epoch: 73 | Batch_idx: 60 |  Loss: (0.2931) |  Loss2: (0.0000) | Acc: (89.00%) (6990/7808)
Epoch: 73 | Batch_idx: 70 |  Loss: (0.2957) |  Loss2: (0.0000) | Acc: (89.00%) (8127/9088)
Epoch: 73 | Batch_idx: 80 |  Loss: (0.2967) |  Loss2: (0.0000) | Acc: (89.00%) (9273/10368)
Epoch: 73 | Batch_idx: 90 |  Loss: (0.2968) |  Loss2: (0.0000) | Acc: (89.00%) (10417/11648)
Epoch: 73 | Batch_idx: 100 |  Loss: (0.2985) |  Loss2: (0.0000) | Acc: (89.00%) (11561/12928)
Epoch: 73 | Batch_idx: 110 |  Loss: (0.2987) |  Loss2: (0.0000) | Acc: (89.00%) (12699/14208)
Epoch: 73 | Batch_idx: 120 |  Loss: (0.2992) |  Loss2: (0.0000) | Acc: (89.00%) (13840/15488)
Epoch: 73 | Batch_idx: 130 |  Loss: (0.3003) |  Loss2: (0.0000) | Acc: (89.00%) (14982/16768)
Epoch: 73 | Batch_idx: 140 |  Loss: (0.2996) |  Loss2: (0.0000) | Acc: (89.00%) (16140/18048)
Epoch: 73 | Batch_idx: 150 |  Loss: (0.2979) |  Loss2: (0.0000) | Acc: (89.00%) (17301/19328)
Epoch: 73 | Batch_idx: 160 |  Loss: (0.2979) |  Loss2: (0.0000) | Acc: (89.00%) (18452/20608)
Epoch: 73 | Batch_idx: 170 |  Loss: (0.2963) |  Loss2: (0.0000) | Acc: (89.00%) (19620/21888)
Epoch: 73 | Batch_idx: 180 |  Loss: (0.2968) |  Loss2: (0.0000) | Acc: (89.00%) (20764/23168)
Epoch: 73 | Batch_idx: 190 |  Loss: (0.2960) |  Loss2: (0.0000) | Acc: (89.00%) (21916/24448)
Epoch: 73 | Batch_idx: 200 |  Loss: (0.2971) |  Loss2: (0.0000) | Acc: (89.00%) (23062/25728)
Epoch: 73 | Batch_idx: 210 |  Loss: (0.2953) |  Loss2: (0.0000) | Acc: (89.00%) (24224/27008)
Epoch: 73 | Batch_idx: 220 |  Loss: (0.2953) |  Loss2: (0.0000) | Acc: (89.00%) (25373/28288)
Epoch: 73 | Batch_idx: 230 |  Loss: (0.2955) |  Loss2: (0.0000) | Acc: (89.00%) (26526/29568)
Epoch: 73 | Batch_idx: 240 |  Loss: (0.2952) |  Loss2: (0.0000) | Acc: (89.00%) (27674/30848)
Epoch: 73 | Batch_idx: 250 |  Loss: (0.2958) |  Loss2: (0.0000) | Acc: (89.00%) (28814/32128)
Epoch: 73 | Batch_idx: 260 |  Loss: (0.2963) |  Loss2: (0.0000) | Acc: (89.00%) (29948/33408)
Epoch: 73 | Batch_idx: 270 |  Loss: (0.2962) |  Loss2: (0.0000) | Acc: (89.00%) (31097/34688)
Epoch: 73 | Batch_idx: 280 |  Loss: (0.2942) |  Loss2: (0.0000) | Acc: (89.00%) (32272/35968)
Epoch: 73 | Batch_idx: 290 |  Loss: (0.2950) |  Loss2: (0.0000) | Acc: (89.00%) (33413/37248)
Epoch: 73 | Batch_idx: 300 |  Loss: (0.2954) |  Loss2: (0.0000) | Acc: (89.00%) (34554/38528)
Epoch: 73 | Batch_idx: 310 |  Loss: (0.2955) |  Loss2: (0.0000) | Acc: (89.00%) (35695/39808)
Epoch: 73 | Batch_idx: 320 |  Loss: (0.2960) |  Loss2: (0.0000) | Acc: (89.00%) (36836/41088)
Epoch: 73 | Batch_idx: 330 |  Loss: (0.2967) |  Loss2: (0.0000) | Acc: (89.00%) (37977/42368)
Epoch: 73 | Batch_idx: 340 |  Loss: (0.2966) |  Loss2: (0.0000) | Acc: (89.00%) (39130/43648)
Epoch: 73 | Batch_idx: 350 |  Loss: (0.2967) |  Loss2: (0.0000) | Acc: (89.00%) (40283/44928)
Epoch: 73 | Batch_idx: 360 |  Loss: (0.2964) |  Loss2: (0.0000) | Acc: (89.00%) (41432/46208)
Epoch: 73 | Batch_idx: 370 |  Loss: (0.2965) |  Loss2: (0.0000) | Acc: (89.00%) (42580/47488)
Epoch: 73 | Batch_idx: 380 |  Loss: (0.2965) |  Loss2: (0.0000) | Acc: (89.00%) (43729/48768)
Epoch: 73 | Batch_idx: 390 |  Loss: (0.2970) |  Loss2: (0.0000) | Acc: (89.00%) (44821/50000)
# TEST : Loss: (0.4298) | Acc: (85.00%) (8556/10000)
percent tensor([0.5453, 0.5532, 0.5489, 0.5291, 0.5530, 0.5618, 0.5577, 0.5397, 0.5412,
        0.5514, 0.5522, 0.5527, 0.5429, 0.5247, 0.5579, 0.5438],
       device='cuda:0') torch.Size([16])
percent tensor([0.5143, 0.5136, 0.5148, 0.5185, 0.5153, 0.5162, 0.5139, 0.5164, 0.5134,
        0.5132, 0.5128, 0.5123, 0.5131, 0.5153, 0.5143, 0.5161],
       device='cuda:0') torch.Size([16])
percent tensor([0.5748, 0.5724, 0.4521, 0.4256, 0.4267, 0.5541, 0.5452, 0.4496, 0.4844,
        0.5469, 0.5721, 0.4750, 0.5706, 0.5358, 0.5541, 0.5771],
       device='cuda:0') torch.Size([16])
percent tensor([0.6450, 0.5416, 0.6344, 0.6408, 0.6366, 0.7076, 0.5933, 0.6404, 0.6070,
        0.5663, 0.5987, 0.5650, 0.5274, 0.6489, 0.6031, 0.6801],
       device='cuda:0') torch.Size([16])
percent tensor([0.5914, 0.5944, 0.6133, 0.6103, 0.5834, 0.5533, 0.5999, 0.6099, 0.6132,
        0.5753, 0.6000, 0.6075, 0.5889, 0.6135, 0.6159, 0.5432],
       device='cuda:0') torch.Size([16])
percent tensor([0.5102, 0.5722, 0.6151, 0.6426, 0.6486, 0.6334, 0.5842, 0.5657, 0.6299,
        0.5215, 0.5872, 0.6548, 0.5453, 0.6375, 0.5548, 0.5391],
       device='cuda:0') torch.Size([16])
percent tensor([0.5810, 0.6082, 0.5424, 0.4941, 0.6135, 0.6581, 0.5516, 0.4244, 0.5744,
        0.6097, 0.5848, 0.4395, 0.5942, 0.5541, 0.4436, 0.5888],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9988, 0.9996, 0.9995, 0.9998, 0.9984, 0.9994, 0.9996, 0.9994,
        0.9990, 0.9996, 0.9997, 0.9995, 0.9993, 0.9988, 0.9988],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 74 | Batch_idx: 0 |  Loss: (0.3261) |  Loss2: (0.0000) | Acc: (87.00%) (112/128)
Epoch: 74 | Batch_idx: 10 |  Loss: (0.2958) |  Loss2: (0.0000) | Acc: (88.00%) (1251/1408)
Epoch: 74 | Batch_idx: 20 |  Loss: (0.2982) |  Loss2: (0.0000) | Acc: (89.00%) (2397/2688)
Epoch: 74 | Batch_idx: 30 |  Loss: (0.3065) |  Loss2: (0.0000) | Acc: (88.00%) (3522/3968)
Epoch: 74 | Batch_idx: 40 |  Loss: (0.3079) |  Loss2: (0.0000) | Acc: (88.00%) (4667/5248)
Epoch: 74 | Batch_idx: 50 |  Loss: (0.3058) |  Loss2: (0.0000) | Acc: (89.00%) (5811/6528)
Epoch: 74 | Batch_idx: 60 |  Loss: (0.3192) |  Loss2: (0.0000) | Acc: (88.00%) (6912/7808)
Epoch: 74 | Batch_idx: 70 |  Loss: (0.3245) |  Loss2: (0.0000) | Acc: (88.00%) (8035/9088)
Epoch: 74 | Batch_idx: 80 |  Loss: (0.3211) |  Loss2: (0.0000) | Acc: (88.00%) (9185/10368)
Epoch: 74 | Batch_idx: 90 |  Loss: (0.3218) |  Loss2: (0.0000) | Acc: (88.00%) (10318/11648)
Epoch: 74 | Batch_idx: 100 |  Loss: (0.3221) |  Loss2: (0.0000) | Acc: (88.00%) (11447/12928)
Epoch: 74 | Batch_idx: 110 |  Loss: (0.3226) |  Loss2: (0.0000) | Acc: (88.00%) (12580/14208)
Epoch: 74 | Batch_idx: 120 |  Loss: (0.3218) |  Loss2: (0.0000) | Acc: (88.00%) (13718/15488)
Epoch: 74 | Batch_idx: 130 |  Loss: (0.3230) |  Loss2: (0.0000) | Acc: (88.00%) (14845/16768)
Epoch: 74 | Batch_idx: 140 |  Loss: (0.3237) |  Loss2: (0.0000) | Acc: (88.00%) (15977/18048)
Epoch: 74 | Batch_idx: 150 |  Loss: (0.3227) |  Loss2: (0.0000) | Acc: (88.00%) (17115/19328)
Epoch: 74 | Batch_idx: 160 |  Loss: (0.3244) |  Loss2: (0.0000) | Acc: (88.00%) (18240/20608)
Epoch: 74 | Batch_idx: 170 |  Loss: (0.3245) |  Loss2: (0.0000) | Acc: (88.00%) (19378/21888)
Epoch: 74 | Batch_idx: 180 |  Loss: (0.3258) |  Loss2: (0.0000) | Acc: (88.00%) (20503/23168)
Epoch: 74 | Batch_idx: 190 |  Loss: (0.3264) |  Loss2: (0.0000) | Acc: (88.00%) (21635/24448)
Epoch: 74 | Batch_idx: 200 |  Loss: (0.3254) |  Loss2: (0.0000) | Acc: (88.00%) (22788/25728)
Epoch: 74 | Batch_idx: 210 |  Loss: (0.3267) |  Loss2: (0.0000) | Acc: (88.00%) (23919/27008)
Epoch: 74 | Batch_idx: 220 |  Loss: (0.3252) |  Loss2: (0.0000) | Acc: (88.00%) (25057/28288)
Epoch: 74 | Batch_idx: 230 |  Loss: (0.3246) |  Loss2: (0.0000) | Acc: (88.00%) (26203/29568)
Epoch: 74 | Batch_idx: 240 |  Loss: (0.3250) |  Loss2: (0.0000) | Acc: (88.00%) (27341/30848)
Epoch: 74 | Batch_idx: 250 |  Loss: (0.3249) |  Loss2: (0.0000) | Acc: (88.00%) (28476/32128)
Epoch: 74 | Batch_idx: 260 |  Loss: (0.3241) |  Loss2: (0.0000) | Acc: (88.00%) (29624/33408)
Epoch: 74 | Batch_idx: 270 |  Loss: (0.3232) |  Loss2: (0.0000) | Acc: (88.00%) (30765/34688)
Epoch: 74 | Batch_idx: 280 |  Loss: (0.3232) |  Loss2: (0.0000) | Acc: (88.00%) (31897/35968)
Epoch: 74 | Batch_idx: 290 |  Loss: (0.3221) |  Loss2: (0.0000) | Acc: (88.00%) (33044/37248)
Epoch: 74 | Batch_idx: 300 |  Loss: (0.3219) |  Loss2: (0.0000) | Acc: (88.00%) (34190/38528)
Epoch: 74 | Batch_idx: 310 |  Loss: (0.3204) |  Loss2: (0.0000) | Acc: (88.00%) (35346/39808)
Epoch: 74 | Batch_idx: 320 |  Loss: (0.3196) |  Loss2: (0.0000) | Acc: (88.00%) (36490/41088)
Epoch: 74 | Batch_idx: 330 |  Loss: (0.3199) |  Loss2: (0.0000) | Acc: (88.00%) (37628/42368)
Epoch: 74 | Batch_idx: 340 |  Loss: (0.3196) |  Loss2: (0.0000) | Acc: (88.00%) (38759/43648)
Epoch: 74 | Batch_idx: 350 |  Loss: (0.3191) |  Loss2: (0.0000) | Acc: (88.00%) (39900/44928)
Epoch: 74 | Batch_idx: 360 |  Loss: (0.3192) |  Loss2: (0.0000) | Acc: (88.00%) (41038/46208)
Epoch: 74 | Batch_idx: 370 |  Loss: (0.3193) |  Loss2: (0.0000) | Acc: (88.00%) (42184/47488)
Epoch: 74 | Batch_idx: 380 |  Loss: (0.3197) |  Loss2: (0.0000) | Acc: (88.00%) (43314/48768)
Epoch: 74 | Batch_idx: 390 |  Loss: (0.3198) |  Loss2: (0.0000) | Acc: (88.00%) (44405/50000)
# TEST : Loss: (0.4443) | Acc: (85.00%) (8520/10000)
percent tensor([0.5601, 0.5688, 0.5657, 0.5420, 0.5701, 0.5768, 0.5749, 0.5549, 0.5559,
        0.5671, 0.5667, 0.5698, 0.5572, 0.5377, 0.5732, 0.5577],
       device='cuda:0') torch.Size([16])
percent tensor([0.5124, 0.5108, 0.5125, 0.5161, 0.5128, 0.5139, 0.5115, 0.5140, 0.5112,
        0.5111, 0.5106, 0.5101, 0.5109, 0.5123, 0.5120, 0.5139],
       device='cuda:0') torch.Size([16])
percent tensor([0.5809, 0.5670, 0.4689, 0.4347, 0.4424, 0.5659, 0.5459, 0.4580, 0.4949,
        0.5467, 0.5763, 0.4871, 0.5708, 0.5432, 0.5577, 0.5767],
       device='cuda:0') torch.Size([16])
percent tensor([0.6601, 0.5404, 0.6508, 0.6633, 0.6592, 0.7270, 0.5988, 0.6655, 0.6112,
        0.5669, 0.5979, 0.5670, 0.5283, 0.6535, 0.6157, 0.6972],
       device='cuda:0') torch.Size([16])
percent tensor([0.5890, 0.5979, 0.5961, 0.5810, 0.5669, 0.5451, 0.5912, 0.5936, 0.5802,
        0.5689, 0.5779, 0.5790, 0.5837, 0.5872, 0.6013, 0.5511],
       device='cuda:0') torch.Size([16])
percent tensor([0.5074, 0.5736, 0.5882, 0.6339, 0.6348, 0.6298, 0.5748, 0.5622, 0.6201,
        0.5107, 0.5889, 0.6259, 0.5427, 0.6357, 0.5461, 0.5362],
       device='cuda:0') torch.Size([16])
percent tensor([0.5903, 0.6003, 0.5464, 0.4887, 0.5956, 0.6549, 0.5528, 0.4064, 0.5722,
        0.6129, 0.5878, 0.4480, 0.6013, 0.5632, 0.4325, 0.5818],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9990, 0.9997, 0.9995, 0.9998, 0.9986, 0.9995, 0.9997, 0.9992,
        0.9991, 0.9996, 0.9998, 0.9996, 0.9992, 0.9990, 0.9992],
       device='cuda:0') torch.Size([16])
Epoch: 75 | Batch_idx: 0 |  Loss: (0.3885) |  Loss2: (0.0000) | Acc: (85.00%) (109/128)
Epoch: 75 | Batch_idx: 10 |  Loss: (0.3187) |  Loss2: (0.0000) | Acc: (88.00%) (1253/1408)
Epoch: 75 | Batch_idx: 20 |  Loss: (0.3013) |  Loss2: (0.0000) | Acc: (89.00%) (2404/2688)
Epoch: 75 | Batch_idx: 30 |  Loss: (0.3046) |  Loss2: (0.0000) | Acc: (89.00%) (3544/3968)
Epoch: 75 | Batch_idx: 40 |  Loss: (0.3000) |  Loss2: (0.0000) | Acc: (89.00%) (4707/5248)
Epoch: 75 | Batch_idx: 50 |  Loss: (0.3044) |  Loss2: (0.0000) | Acc: (89.00%) (5844/6528)
Epoch: 75 | Batch_idx: 60 |  Loss: (0.3030) |  Loss2: (0.0000) | Acc: (89.00%) (7001/7808)
Epoch: 75 | Batch_idx: 70 |  Loss: (0.3011) |  Loss2: (0.0000) | Acc: (89.00%) (8147/9088)
Epoch: 75 | Batch_idx: 80 |  Loss: (0.2985) |  Loss2: (0.0000) | Acc: (89.00%) (9298/10368)
Epoch: 75 | Batch_idx: 90 |  Loss: (0.2974) |  Loss2: (0.0000) | Acc: (89.00%) (10451/11648)
Epoch: 75 | Batch_idx: 100 |  Loss: (0.3002) |  Loss2: (0.0000) | Acc: (89.00%) (11573/12928)
Epoch: 75 | Batch_idx: 110 |  Loss: (0.3007) |  Loss2: (0.0000) | Acc: (89.00%) (12720/14208)
Epoch: 75 | Batch_idx: 120 |  Loss: (0.3022) |  Loss2: (0.0000) | Acc: (89.00%) (13854/15488)
Epoch: 75 | Batch_idx: 130 |  Loss: (0.3011) |  Loss2: (0.0000) | Acc: (89.00%) (14997/16768)
Epoch: 75 | Batch_idx: 140 |  Loss: (0.3008) |  Loss2: (0.0000) | Acc: (89.00%) (16149/18048)
Epoch: 75 | Batch_idx: 150 |  Loss: (0.3015) |  Loss2: (0.0000) | Acc: (89.00%) (17295/19328)
Epoch: 75 | Batch_idx: 160 |  Loss: (0.3023) |  Loss2: (0.0000) | Acc: (89.00%) (18427/20608)
Epoch: 75 | Batch_idx: 170 |  Loss: (0.3031) |  Loss2: (0.0000) | Acc: (89.00%) (19570/21888)
Epoch: 75 | Batch_idx: 180 |  Loss: (0.3035) |  Loss2: (0.0000) | Acc: (89.00%) (20705/23168)
Epoch: 75 | Batch_idx: 190 |  Loss: (0.3044) |  Loss2: (0.0000) | Acc: (89.00%) (21832/24448)
Epoch: 75 | Batch_idx: 200 |  Loss: (0.3050) |  Loss2: (0.0000) | Acc: (89.00%) (22972/25728)
Epoch: 75 | Batch_idx: 210 |  Loss: (0.3042) |  Loss2: (0.0000) | Acc: (89.00%) (24126/27008)
Epoch: 75 | Batch_idx: 220 |  Loss: (0.3033) |  Loss2: (0.0000) | Acc: (89.00%) (25290/28288)
Epoch: 75 | Batch_idx: 230 |  Loss: (0.3024) |  Loss2: (0.0000) | Acc: (89.00%) (26446/29568)
Epoch: 75 | Batch_idx: 240 |  Loss: (0.3020) |  Loss2: (0.0000) | Acc: (89.00%) (27605/30848)
Epoch: 75 | Batch_idx: 250 |  Loss: (0.3005) |  Loss2: (0.0000) | Acc: (89.00%) (28762/32128)
Epoch: 75 | Batch_idx: 260 |  Loss: (0.3006) |  Loss2: (0.0000) | Acc: (89.00%) (29905/33408)
Epoch: 75 | Batch_idx: 270 |  Loss: (0.3002) |  Loss2: (0.0000) | Acc: (89.00%) (31052/34688)
Epoch: 75 | Batch_idx: 280 |  Loss: (0.2998) |  Loss2: (0.0000) | Acc: (89.00%) (32206/35968)
Epoch: 75 | Batch_idx: 290 |  Loss: (0.2999) |  Loss2: (0.0000) | Acc: (89.00%) (33340/37248)
Epoch: 75 | Batch_idx: 300 |  Loss: (0.2990) |  Loss2: (0.0000) | Acc: (89.00%) (34498/38528)
Epoch: 75 | Batch_idx: 310 |  Loss: (0.2993) |  Loss2: (0.0000) | Acc: (89.00%) (35644/39808)
Epoch: 75 | Batch_idx: 320 |  Loss: (0.3002) |  Loss2: (0.0000) | Acc: (89.00%) (36780/41088)
Epoch: 75 | Batch_idx: 330 |  Loss: (0.3005) |  Loss2: (0.0000) | Acc: (89.00%) (37930/42368)
Epoch: 75 | Batch_idx: 340 |  Loss: (0.3002) |  Loss2: (0.0000) | Acc: (89.00%) (39079/43648)
Epoch: 75 | Batch_idx: 350 |  Loss: (0.2994) |  Loss2: (0.0000) | Acc: (89.00%) (40237/44928)
Epoch: 75 | Batch_idx: 360 |  Loss: (0.3000) |  Loss2: (0.0000) | Acc: (89.00%) (41383/46208)
Epoch: 75 | Batch_idx: 370 |  Loss: (0.3009) |  Loss2: (0.0000) | Acc: (89.00%) (42507/47488)
Epoch: 75 | Batch_idx: 380 |  Loss: (0.3006) |  Loss2: (0.0000) | Acc: (89.00%) (43660/48768)
Epoch: 75 | Batch_idx: 390 |  Loss: (0.3005) |  Loss2: (0.0000) | Acc: (89.00%) (44779/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_075.pth.tar'
# TEST : Loss: (0.4273) | Acc: (85.00%) (8578/10000)
percent tensor([0.5587, 0.5676, 0.5649, 0.5409, 0.5690, 0.5763, 0.5737, 0.5535, 0.5542,
        0.5656, 0.5652, 0.5686, 0.5556, 0.5361, 0.5723, 0.5563],
       device='cuda:0') torch.Size([16])
percent tensor([0.5116, 0.5098, 0.5118, 0.5151, 0.5121, 0.5130, 0.5107, 0.5131, 0.5103,
        0.5102, 0.5097, 0.5095, 0.5100, 0.5113, 0.5110, 0.5130],
       device='cuda:0') torch.Size([16])
percent tensor([0.5844, 0.5688, 0.4726, 0.4424, 0.4461, 0.5730, 0.5480, 0.4644, 0.4985,
        0.5471, 0.5775, 0.4884, 0.5748, 0.5472, 0.5628, 0.5806],
       device='cuda:0') torch.Size([16])
percent tensor([0.6497, 0.5265, 0.6454, 0.6567, 0.6555, 0.7253, 0.5863, 0.6618, 0.5999,
        0.5551, 0.5821, 0.5554, 0.5131, 0.6403, 0.6056, 0.6895],
       device='cuda:0') torch.Size([16])
percent tensor([0.5932, 0.5977, 0.5956, 0.5790, 0.5686, 0.5530, 0.5910, 0.5987, 0.5786,
        0.5678, 0.5736, 0.5707, 0.5831, 0.5877, 0.5987, 0.5613],
       device='cuda:0') torch.Size([16])
percent tensor([0.4932, 0.5611, 0.5793, 0.6244, 0.6261, 0.6218, 0.5628, 0.5494, 0.6058,
        0.5012, 0.5769, 0.6161, 0.5321, 0.6233, 0.5336, 0.5240],
       device='cuda:0') torch.Size([16])
percent tensor([0.6075, 0.6154, 0.5572, 0.5037, 0.6002, 0.6660, 0.5670, 0.4134, 0.5919,
        0.6273, 0.6104, 0.4661, 0.6183, 0.5869, 0.4466, 0.5938],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9990, 0.9997, 0.9996, 0.9998, 0.9986, 0.9994, 0.9997, 0.9992,
        0.9991, 0.9996, 0.9998, 0.9996, 0.9992, 0.9990, 0.9992],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 76 | Batch_idx: 0 |  Loss: (0.2616) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 76 | Batch_idx: 10 |  Loss: (0.2994) |  Loss2: (0.0000) | Acc: (89.00%) (1267/1408)
Epoch: 76 | Batch_idx: 20 |  Loss: (0.3030) |  Loss2: (0.0000) | Acc: (89.00%) (2415/2688)
Epoch: 76 | Batch_idx: 30 |  Loss: (0.2829) |  Loss2: (0.0000) | Acc: (90.00%) (3595/3968)
Epoch: 76 | Batch_idx: 40 |  Loss: (0.2824) |  Loss2: (0.0000) | Acc: (90.00%) (4749/5248)
Epoch: 76 | Batch_idx: 50 |  Loss: (0.2880) |  Loss2: (0.0000) | Acc: (90.00%) (5900/6528)
Epoch: 76 | Batch_idx: 60 |  Loss: (0.2862) |  Loss2: (0.0000) | Acc: (90.00%) (7047/7808)
Epoch: 76 | Batch_idx: 70 |  Loss: (0.2842) |  Loss2: (0.0000) | Acc: (90.00%) (8209/9088)
Epoch: 76 | Batch_idx: 80 |  Loss: (0.2837) |  Loss2: (0.0000) | Acc: (90.00%) (9357/10368)
Epoch: 76 | Batch_idx: 90 |  Loss: (0.2866) |  Loss2: (0.0000) | Acc: (90.00%) (10509/11648)
Epoch: 76 | Batch_idx: 100 |  Loss: (0.2865) |  Loss2: (0.0000) | Acc: (90.00%) (11664/12928)
Epoch: 76 | Batch_idx: 110 |  Loss: (0.2857) |  Loss2: (0.0000) | Acc: (90.00%) (12825/14208)
Epoch: 76 | Batch_idx: 120 |  Loss: (0.2850) |  Loss2: (0.0000) | Acc: (90.00%) (13977/15488)
Epoch: 76 | Batch_idx: 130 |  Loss: (0.2847) |  Loss2: (0.0000) | Acc: (90.00%) (15135/16768)
Epoch: 76 | Batch_idx: 140 |  Loss: (0.2850) |  Loss2: (0.0000) | Acc: (90.00%) (16284/18048)
Epoch: 76 | Batch_idx: 150 |  Loss: (0.2863) |  Loss2: (0.0000) | Acc: (90.00%) (17427/19328)
Epoch: 76 | Batch_idx: 160 |  Loss: (0.2845) |  Loss2: (0.0000) | Acc: (90.00%) (18594/20608)
Epoch: 76 | Batch_idx: 170 |  Loss: (0.2845) |  Loss2: (0.0000) | Acc: (90.00%) (19749/21888)
Epoch: 76 | Batch_idx: 180 |  Loss: (0.2849) |  Loss2: (0.0000) | Acc: (90.00%) (20906/23168)
Epoch: 76 | Batch_idx: 190 |  Loss: (0.2842) |  Loss2: (0.0000) | Acc: (90.00%) (22062/24448)
Epoch: 76 | Batch_idx: 200 |  Loss: (0.2852) |  Loss2: (0.0000) | Acc: (90.00%) (23212/25728)
Epoch: 76 | Batch_idx: 210 |  Loss: (0.2851) |  Loss2: (0.0000) | Acc: (90.00%) (24360/27008)
Epoch: 76 | Batch_idx: 220 |  Loss: (0.2844) |  Loss2: (0.0000) | Acc: (90.00%) (25522/28288)
Epoch: 76 | Batch_idx: 230 |  Loss: (0.2852) |  Loss2: (0.0000) | Acc: (90.00%) (26663/29568)
Epoch: 76 | Batch_idx: 240 |  Loss: (0.2852) |  Loss2: (0.0000) | Acc: (90.00%) (27820/30848)
Epoch: 76 | Batch_idx: 250 |  Loss: (0.2850) |  Loss2: (0.0000) | Acc: (90.00%) (28978/32128)
Epoch: 76 | Batch_idx: 260 |  Loss: (0.2852) |  Loss2: (0.0000) | Acc: (90.00%) (30148/33408)
Epoch: 76 | Batch_idx: 270 |  Loss: (0.2848) |  Loss2: (0.0000) | Acc: (90.00%) (31313/34688)
Epoch: 76 | Batch_idx: 280 |  Loss: (0.2860) |  Loss2: (0.0000) | Acc: (90.00%) (32451/35968)
Epoch: 76 | Batch_idx: 290 |  Loss: (0.2860) |  Loss2: (0.0000) | Acc: (90.00%) (33602/37248)
Epoch: 76 | Batch_idx: 300 |  Loss: (0.2863) |  Loss2: (0.0000) | Acc: (90.00%) (34736/38528)
Epoch: 76 | Batch_idx: 310 |  Loss: (0.2870) |  Loss2: (0.0000) | Acc: (90.00%) (35879/39808)
Epoch: 76 | Batch_idx: 320 |  Loss: (0.2877) |  Loss2: (0.0000) | Acc: (90.00%) (37023/41088)
Epoch: 76 | Batch_idx: 330 |  Loss: (0.2879) |  Loss2: (0.0000) | Acc: (90.00%) (38180/42368)
Epoch: 76 | Batch_idx: 340 |  Loss: (0.2889) |  Loss2: (0.0000) | Acc: (90.00%) (39315/43648)
Epoch: 76 | Batch_idx: 350 |  Loss: (0.2888) |  Loss2: (0.0000) | Acc: (90.00%) (40473/44928)
Epoch: 76 | Batch_idx: 360 |  Loss: (0.2883) |  Loss2: (0.0000) | Acc: (90.00%) (41645/46208)
Epoch: 76 | Batch_idx: 370 |  Loss: (0.2894) |  Loss2: (0.0000) | Acc: (90.00%) (42770/47488)
Epoch: 76 | Batch_idx: 380 |  Loss: (0.2896) |  Loss2: (0.0000) | Acc: (90.00%) (43931/48768)
Epoch: 76 | Batch_idx: 390 |  Loss: (0.2891) |  Loss2: (0.0000) | Acc: (90.00%) (45043/50000)
# TEST : Loss: (0.4310) | Acc: (85.00%) (8554/10000)
percent tensor([0.5581, 0.5698, 0.5640, 0.5426, 0.5688, 0.5775, 0.5748, 0.5539, 0.5547,
        0.5660, 0.5656, 0.5678, 0.5556, 0.5397, 0.5740, 0.5573],
       device='cuda:0') torch.Size([16])
percent tensor([0.5115, 0.5101, 0.5120, 0.5150, 0.5122, 0.5131, 0.5111, 0.5125, 0.5101,
        0.5102, 0.5099, 0.5097, 0.5098, 0.5116, 0.5111, 0.5130],
       device='cuda:0') torch.Size([16])
percent tensor([0.5879, 0.5644, 0.4917, 0.4638, 0.4578, 0.5809, 0.5474, 0.4780, 0.4956,
        0.5567, 0.5730, 0.5045, 0.5732, 0.5394, 0.5638, 0.5843],
       device='cuda:0') torch.Size([16])
percent tensor([0.6448, 0.5308, 0.6547, 0.6752, 0.6616, 0.7271, 0.5948, 0.6537, 0.5991,
        0.5686, 0.5798, 0.5739, 0.5115, 0.6313, 0.6039, 0.6898],
       device='cuda:0') torch.Size([16])
percent tensor([0.5885, 0.5939, 0.5881, 0.5737, 0.5593, 0.5390, 0.5927, 0.5996, 0.5967,
        0.5625, 0.5751, 0.5627, 0.5836, 0.5958, 0.5960, 0.5537],
       device='cuda:0') torch.Size([16])
percent tensor([0.4972, 0.5715, 0.5914, 0.6351, 0.6210, 0.6071, 0.5835, 0.5405, 0.6027,
        0.5423, 0.5992, 0.6481, 0.5359, 0.6384, 0.5481, 0.5447],
       device='cuda:0') torch.Size([16])
percent tensor([0.5867, 0.5761, 0.5399, 0.5155, 0.5918, 0.6680, 0.5805, 0.4180, 0.5830,
        0.5983, 0.5854, 0.4207, 0.5983, 0.6079, 0.4168, 0.5848],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9992, 0.9997, 0.9996, 0.9997, 0.9985, 0.9996, 0.9997, 0.9997,
        0.9994, 0.9997, 0.9997, 0.9997, 0.9990, 0.9991, 0.9993],
       device='cuda:0') torch.Size([16])
Epoch: 77 | Batch_idx: 0 |  Loss: (0.2835) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 77 | Batch_idx: 10 |  Loss: (0.2665) |  Loss2: (0.0000) | Acc: (90.00%) (1280/1408)
Epoch: 77 | Batch_idx: 20 |  Loss: (0.2682) |  Loss2: (0.0000) | Acc: (90.00%) (2433/2688)
Epoch: 77 | Batch_idx: 30 |  Loss: (0.2746) |  Loss2: (0.0000) | Acc: (90.00%) (3581/3968)
Epoch: 77 | Batch_idx: 40 |  Loss: (0.2714) |  Loss2: (0.0000) | Acc: (90.00%) (4743/5248)
Epoch: 77 | Batch_idx: 50 |  Loss: (0.2667) |  Loss2: (0.0000) | Acc: (90.00%) (5910/6528)
Epoch: 77 | Batch_idx: 60 |  Loss: (0.2687) |  Loss2: (0.0000) | Acc: (90.00%) (7068/7808)
Epoch: 77 | Batch_idx: 70 |  Loss: (0.2663) |  Loss2: (0.0000) | Acc: (90.00%) (8239/9088)
Epoch: 77 | Batch_idx: 80 |  Loss: (0.2683) |  Loss2: (0.0000) | Acc: (90.00%) (9401/10368)
Epoch: 77 | Batch_idx: 90 |  Loss: (0.2645) |  Loss2: (0.0000) | Acc: (90.00%) (10585/11648)
Epoch: 77 | Batch_idx: 100 |  Loss: (0.2659) |  Loss2: (0.0000) | Acc: (90.00%) (11742/12928)
Epoch: 77 | Batch_idx: 110 |  Loss: (0.2666) |  Loss2: (0.0000) | Acc: (90.00%) (12902/14208)
Epoch: 77 | Batch_idx: 120 |  Loss: (0.2687) |  Loss2: (0.0000) | Acc: (90.00%) (14057/15488)
Epoch: 77 | Batch_idx: 130 |  Loss: (0.2687) |  Loss2: (0.0000) | Acc: (90.00%) (15217/16768)
Epoch: 77 | Batch_idx: 140 |  Loss: (0.2694) |  Loss2: (0.0000) | Acc: (90.00%) (16369/18048)
Epoch: 77 | Batch_idx: 150 |  Loss: (0.2683) |  Loss2: (0.0000) | Acc: (90.00%) (17541/19328)
Epoch: 77 | Batch_idx: 160 |  Loss: (0.2683) |  Loss2: (0.0000) | Acc: (90.00%) (18695/20608)
Epoch: 77 | Batch_idx: 170 |  Loss: (0.2697) |  Loss2: (0.0000) | Acc: (90.00%) (19849/21888)
Epoch: 77 | Batch_idx: 180 |  Loss: (0.2720) |  Loss2: (0.0000) | Acc: (90.00%) (20997/23168)
Epoch: 77 | Batch_idx: 190 |  Loss: (0.2727) |  Loss2: (0.0000) | Acc: (90.00%) (22146/24448)
Epoch: 77 | Batch_idx: 200 |  Loss: (0.2732) |  Loss2: (0.0000) | Acc: (90.00%) (23308/25728)
Epoch: 77 | Batch_idx: 210 |  Loss: (0.2743) |  Loss2: (0.0000) | Acc: (90.00%) (24456/27008)
Epoch: 77 | Batch_idx: 220 |  Loss: (0.2741) |  Loss2: (0.0000) | Acc: (90.00%) (25618/28288)
Epoch: 77 | Batch_idx: 230 |  Loss: (0.2742) |  Loss2: (0.0000) | Acc: (90.00%) (26774/29568)
Epoch: 77 | Batch_idx: 240 |  Loss: (0.2746) |  Loss2: (0.0000) | Acc: (90.00%) (27917/30848)
Epoch: 77 | Batch_idx: 250 |  Loss: (0.2771) |  Loss2: (0.0000) | Acc: (90.00%) (29038/32128)
Epoch: 77 | Batch_idx: 260 |  Loss: (0.2784) |  Loss2: (0.0000) | Acc: (90.00%) (30174/33408)
Epoch: 77 | Batch_idx: 270 |  Loss: (0.2781) |  Loss2: (0.0000) | Acc: (90.00%) (31323/34688)
Epoch: 77 | Batch_idx: 280 |  Loss: (0.2778) |  Loss2: (0.0000) | Acc: (90.00%) (32479/35968)
Epoch: 77 | Batch_idx: 290 |  Loss: (0.2779) |  Loss2: (0.0000) | Acc: (90.00%) (33636/37248)
Epoch: 77 | Batch_idx: 300 |  Loss: (0.2785) |  Loss2: (0.0000) | Acc: (90.00%) (34787/38528)
Epoch: 77 | Batch_idx: 310 |  Loss: (0.2785) |  Loss2: (0.0000) | Acc: (90.00%) (35944/39808)
Epoch: 77 | Batch_idx: 320 |  Loss: (0.2794) |  Loss2: (0.0000) | Acc: (90.00%) (37085/41088)
Epoch: 77 | Batch_idx: 330 |  Loss: (0.2795) |  Loss2: (0.0000) | Acc: (90.00%) (38244/42368)
Epoch: 77 | Batch_idx: 340 |  Loss: (0.2786) |  Loss2: (0.0000) | Acc: (90.00%) (39426/43648)
Epoch: 77 | Batch_idx: 350 |  Loss: (0.2788) |  Loss2: (0.0000) | Acc: (90.00%) (40582/44928)
Epoch: 77 | Batch_idx: 360 |  Loss: (0.2786) |  Loss2: (0.0000) | Acc: (90.00%) (41747/46208)
Epoch: 77 | Batch_idx: 370 |  Loss: (0.2796) |  Loss2: (0.0000) | Acc: (90.00%) (42880/47488)
Epoch: 77 | Batch_idx: 380 |  Loss: (0.2791) |  Loss2: (0.0000) | Acc: (90.00%) (44042/48768)
Epoch: 77 | Batch_idx: 390 |  Loss: (0.2789) |  Loss2: (0.0000) | Acc: (90.00%) (45159/50000)
# TEST : Loss: (0.4394) | Acc: (85.00%) (8532/10000)
percent tensor([0.5577, 0.5699, 0.5610, 0.5413, 0.5666, 0.5765, 0.5737, 0.5515, 0.5543,
        0.5655, 0.5657, 0.5659, 0.5555, 0.5403, 0.5737, 0.5570],
       device='cuda:0') torch.Size([16])
percent tensor([0.5114, 0.5099, 0.5120, 0.5145, 0.5120, 0.5126, 0.5110, 0.5126, 0.5099,
        0.5100, 0.5097, 0.5097, 0.5096, 0.5113, 0.5109, 0.5127],
       device='cuda:0') torch.Size([16])
percent tensor([0.5900, 0.5795, 0.4769, 0.4595, 0.4513, 0.5837, 0.5554, 0.4714, 0.5017,
        0.5606, 0.5777, 0.4978, 0.5850, 0.5637, 0.5705, 0.5888],
       device='cuda:0') torch.Size([16])
percent tensor([0.6412, 0.5152, 0.6461, 0.6635, 0.6565, 0.7188, 0.5742, 0.6489, 0.5879,
        0.5496, 0.5682, 0.5654, 0.5069, 0.6129, 0.5984, 0.6785],
       device='cuda:0') torch.Size([16])
percent tensor([0.5923, 0.5977, 0.5915, 0.5779, 0.5587, 0.5470, 0.6006, 0.6023, 0.6020,
        0.5709, 0.5793, 0.5727, 0.5840, 0.6061, 0.5945, 0.5545],
       device='cuda:0') torch.Size([16])
percent tensor([0.5044, 0.5525, 0.5967, 0.6343, 0.6216, 0.6212, 0.5651, 0.5376, 0.6047,
        0.5132, 0.5882, 0.6552, 0.5366, 0.6119, 0.5453, 0.5283],
       device='cuda:0') torch.Size([16])
percent tensor([0.6019, 0.5762, 0.5345, 0.5102, 0.6059, 0.6546, 0.5340, 0.4312, 0.5929,
        0.6116, 0.5815, 0.4381, 0.5997, 0.5847, 0.4350, 0.5858],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9992, 0.9995, 0.9997, 0.9996, 0.9985, 0.9994, 0.9996, 0.9998,
        0.9992, 0.9996, 0.9997, 0.9996, 0.9992, 0.9991, 0.9994],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 78 | Batch_idx: 0 |  Loss: (0.2826) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 78 | Batch_idx: 10 |  Loss: (0.3040) |  Loss2: (0.0000) | Acc: (90.00%) (1270/1408)
Epoch: 78 | Batch_idx: 20 |  Loss: (0.2993) |  Loss2: (0.0000) | Acc: (89.00%) (2418/2688)
Epoch: 78 | Batch_idx: 30 |  Loss: (0.3123) |  Loss2: (0.0000) | Acc: (89.00%) (3546/3968)
Epoch: 78 | Batch_idx: 40 |  Loss: (0.3193) |  Loss2: (0.0000) | Acc: (89.00%) (4673/5248)
Epoch: 78 | Batch_idx: 50 |  Loss: (0.3222) |  Loss2: (0.0000) | Acc: (89.00%) (5818/6528)
Epoch: 78 | Batch_idx: 60 |  Loss: (0.3290) |  Loss2: (0.0000) | Acc: (88.00%) (6928/7808)
Epoch: 78 | Batch_idx: 70 |  Loss: (0.3308) |  Loss2: (0.0000) | Acc: (88.00%) (8056/9088)
Epoch: 78 | Batch_idx: 80 |  Loss: (0.3274) |  Loss2: (0.0000) | Acc: (88.00%) (9190/10368)
Epoch: 78 | Batch_idx: 90 |  Loss: (0.3268) |  Loss2: (0.0000) | Acc: (88.00%) (10333/11648)
Epoch: 78 | Batch_idx: 100 |  Loss: (0.3264) |  Loss2: (0.0000) | Acc: (88.00%) (11459/12928)
Epoch: 78 | Batch_idx: 110 |  Loss: (0.3267) |  Loss2: (0.0000) | Acc: (88.00%) (12592/14208)
Epoch: 78 | Batch_idx: 120 |  Loss: (0.3240) |  Loss2: (0.0000) | Acc: (88.00%) (13731/15488)
Epoch: 78 | Batch_idx: 130 |  Loss: (0.3235) |  Loss2: (0.0000) | Acc: (88.00%) (14868/16768)
Epoch: 78 | Batch_idx: 140 |  Loss: (0.3212) |  Loss2: (0.0000) | Acc: (88.00%) (16011/18048)
Epoch: 78 | Batch_idx: 150 |  Loss: (0.3207) |  Loss2: (0.0000) | Acc: (88.00%) (17150/19328)
Epoch: 78 | Batch_idx: 160 |  Loss: (0.3186) |  Loss2: (0.0000) | Acc: (88.00%) (18312/20608)
Epoch: 78 | Batch_idx: 170 |  Loss: (0.3160) |  Loss2: (0.0000) | Acc: (88.00%) (19461/21888)
Epoch: 78 | Batch_idx: 180 |  Loss: (0.3159) |  Loss2: (0.0000) | Acc: (88.00%) (20604/23168)
Epoch: 78 | Batch_idx: 190 |  Loss: (0.3176) |  Loss2: (0.0000) | Acc: (88.00%) (21730/24448)
Epoch: 78 | Batch_idx: 200 |  Loss: (0.3158) |  Loss2: (0.0000) | Acc: (88.00%) (22883/25728)
Epoch: 78 | Batch_idx: 210 |  Loss: (0.3163) |  Loss2: (0.0000) | Acc: (88.00%) (24018/27008)
Epoch: 78 | Batch_idx: 220 |  Loss: (0.3158) |  Loss2: (0.0000) | Acc: (88.00%) (25158/28288)
Epoch: 78 | Batch_idx: 230 |  Loss: (0.3153) |  Loss2: (0.0000) | Acc: (88.00%) (26291/29568)
Epoch: 78 | Batch_idx: 240 |  Loss: (0.3143) |  Loss2: (0.0000) | Acc: (88.00%) (27447/30848)
Epoch: 78 | Batch_idx: 250 |  Loss: (0.3141) |  Loss2: (0.0000) | Acc: (88.00%) (28569/32128)
Epoch: 78 | Batch_idx: 260 |  Loss: (0.3141) |  Loss2: (0.0000) | Acc: (88.00%) (29732/33408)
Epoch: 78 | Batch_idx: 270 |  Loss: (0.3138) |  Loss2: (0.0000) | Acc: (89.00%) (30887/34688)
Epoch: 78 | Batch_idx: 280 |  Loss: (0.3133) |  Loss2: (0.0000) | Acc: (89.00%) (32019/35968)
Epoch: 78 | Batch_idx: 290 |  Loss: (0.3121) |  Loss2: (0.0000) | Acc: (89.00%) (33166/37248)
Epoch: 78 | Batch_idx: 300 |  Loss: (0.3112) |  Loss2: (0.0000) | Acc: (89.00%) (34313/38528)
Epoch: 78 | Batch_idx: 310 |  Loss: (0.3116) |  Loss2: (0.0000) | Acc: (89.00%) (35442/39808)
Epoch: 78 | Batch_idx: 320 |  Loss: (0.3109) |  Loss2: (0.0000) | Acc: (89.00%) (36602/41088)
Epoch: 78 | Batch_idx: 330 |  Loss: (0.3105) |  Loss2: (0.0000) | Acc: (89.00%) (37750/42368)
Epoch: 78 | Batch_idx: 340 |  Loss: (0.3105) |  Loss2: (0.0000) | Acc: (89.00%) (38899/43648)
Epoch: 78 | Batch_idx: 350 |  Loss: (0.3099) |  Loss2: (0.0000) | Acc: (89.00%) (40054/44928)
Epoch: 78 | Batch_idx: 360 |  Loss: (0.3085) |  Loss2: (0.0000) | Acc: (89.00%) (41223/46208)
Epoch: 78 | Batch_idx: 370 |  Loss: (0.3078) |  Loss2: (0.0000) | Acc: (89.00%) (42380/47488)
Epoch: 78 | Batch_idx: 380 |  Loss: (0.3071) |  Loss2: (0.0000) | Acc: (89.00%) (43534/48768)
Epoch: 78 | Batch_idx: 390 |  Loss: (0.3063) |  Loss2: (0.0000) | Acc: (89.00%) (44647/50000)
# TEST : Loss: (0.4325) | Acc: (85.00%) (8566/10000)
percent tensor([0.5398, 0.5507, 0.5413, 0.5232, 0.5455, 0.5579, 0.5533, 0.5314, 0.5354,
        0.5458, 0.5479, 0.5454, 0.5377, 0.5223, 0.5547, 0.5386],
       device='cuda:0') torch.Size([16])
percent tensor([0.5114, 0.5103, 0.5124, 0.5148, 0.5126, 0.5129, 0.5113, 0.5130, 0.5103,
        0.5103, 0.5098, 0.5102, 0.5099, 0.5117, 0.5113, 0.5129],
       device='cuda:0') torch.Size([16])
percent tensor([0.6145, 0.6127, 0.5000, 0.4804, 0.4776, 0.5919, 0.5888, 0.4985, 0.5360,
        0.6032, 0.6159, 0.5359, 0.6196, 0.5939, 0.5952, 0.6128],
       device='cuda:0') torch.Size([16])
percent tensor([0.6820, 0.5580, 0.6696, 0.6999, 0.6834, 0.7491, 0.6106, 0.6790, 0.6259,
        0.5926, 0.6197, 0.5959, 0.5575, 0.6555, 0.6377, 0.7230],
       device='cuda:0') torch.Size([16])
percent tensor([0.5646, 0.5684, 0.5651, 0.5446, 0.5404, 0.5246, 0.5762, 0.5765, 0.5674,
        0.5439, 0.5441, 0.5452, 0.5488, 0.5727, 0.5655, 0.5278],
       device='cuda:0') torch.Size([16])
percent tensor([0.5625, 0.5989, 0.6472, 0.6862, 0.6686, 0.6654, 0.6171, 0.6043, 0.6614,
        0.5673, 0.6433, 0.7059, 0.5887, 0.6668, 0.5889, 0.5753],
       device='cuda:0') torch.Size([16])
percent tensor([0.6063, 0.5764, 0.5008, 0.4947, 0.5887, 0.6608, 0.5443, 0.4049, 0.6083,
        0.6124, 0.5879, 0.4250, 0.6125, 0.5935, 0.4371, 0.5787],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9992, 0.9994, 0.9997, 0.9997, 0.9984, 0.9994, 0.9996, 0.9997,
        0.9993, 0.9996, 0.9997, 0.9996, 0.9992, 0.9991, 0.9994],
       device='cuda:0') torch.Size([16])
Epoch: 79 | Batch_idx: 0 |  Loss: (0.3289) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 79 | Batch_idx: 10 |  Loss: (0.3128) |  Loss2: (0.0000) | Acc: (88.00%) (1253/1408)
Epoch: 79 | Batch_idx: 20 |  Loss: (0.3070) |  Loss2: (0.0000) | Acc: (89.00%) (2401/2688)
Epoch: 79 | Batch_idx: 30 |  Loss: (0.3009) |  Loss2: (0.0000) | Acc: (89.00%) (3548/3968)
Epoch: 79 | Batch_idx: 40 |  Loss: (0.2959) |  Loss2: (0.0000) | Acc: (89.00%) (4702/5248)
Epoch: 79 | Batch_idx: 50 |  Loss: (0.2947) |  Loss2: (0.0000) | Acc: (89.00%) (5867/6528)
Epoch: 79 | Batch_idx: 60 |  Loss: (0.2952) |  Loss2: (0.0000) | Acc: (89.00%) (7005/7808)
Epoch: 79 | Batch_idx: 70 |  Loss: (0.2899) |  Loss2: (0.0000) | Acc: (89.00%) (8163/9088)
Epoch: 79 | Batch_idx: 80 |  Loss: (0.2899) |  Loss2: (0.0000) | Acc: (89.00%) (9307/10368)
Epoch: 79 | Batch_idx: 90 |  Loss: (0.2872) |  Loss2: (0.0000) | Acc: (89.00%) (10467/11648)
Epoch: 79 | Batch_idx: 100 |  Loss: (0.2869) |  Loss2: (0.0000) | Acc: (89.00%) (11613/12928)
Epoch: 79 | Batch_idx: 110 |  Loss: (0.2856) |  Loss2: (0.0000) | Acc: (89.00%) (12774/14208)
Epoch: 79 | Batch_idx: 120 |  Loss: (0.2869) |  Loss2: (0.0000) | Acc: (89.00%) (13933/15488)
Epoch: 79 | Batch_idx: 130 |  Loss: (0.2868) |  Loss2: (0.0000) | Acc: (89.00%) (15081/16768)
Epoch: 79 | Batch_idx: 140 |  Loss: (0.2869) |  Loss2: (0.0000) | Acc: (89.00%) (16230/18048)
Epoch: 79 | Batch_idx: 150 |  Loss: (0.2867) |  Loss2: (0.0000) | Acc: (89.00%) (17388/19328)
Epoch: 79 | Batch_idx: 160 |  Loss: (0.2876) |  Loss2: (0.0000) | Acc: (89.00%) (18533/20608)
Epoch: 79 | Batch_idx: 170 |  Loss: (0.2861) |  Loss2: (0.0000) | Acc: (89.00%) (19692/21888)
Epoch: 79 | Batch_idx: 180 |  Loss: (0.2860) |  Loss2: (0.0000) | Acc: (90.00%) (20853/23168)
Epoch: 79 | Batch_idx: 190 |  Loss: (0.2870) |  Loss2: (0.0000) | Acc: (89.00%) (21997/24448)
Epoch: 79 | Batch_idx: 200 |  Loss: (0.2860) |  Loss2: (0.0000) | Acc: (89.00%) (23143/25728)
Epoch: 79 | Batch_idx: 210 |  Loss: (0.2875) |  Loss2: (0.0000) | Acc: (89.00%) (24279/27008)
Epoch: 79 | Batch_idx: 220 |  Loss: (0.2860) |  Loss2: (0.0000) | Acc: (89.00%) (25438/28288)
Epoch: 79 | Batch_idx: 230 |  Loss: (0.2861) |  Loss2: (0.0000) | Acc: (89.00%) (26579/29568)
Epoch: 79 | Batch_idx: 240 |  Loss: (0.2867) |  Loss2: (0.0000) | Acc: (89.00%) (27722/30848)
Epoch: 79 | Batch_idx: 250 |  Loss: (0.2867) |  Loss2: (0.0000) | Acc: (89.00%) (28888/32128)
Epoch: 79 | Batch_idx: 260 |  Loss: (0.2865) |  Loss2: (0.0000) | Acc: (89.00%) (30043/33408)
Epoch: 79 | Batch_idx: 270 |  Loss: (0.2859) |  Loss2: (0.0000) | Acc: (89.00%) (31197/34688)
Epoch: 79 | Batch_idx: 280 |  Loss: (0.2860) |  Loss2: (0.0000) | Acc: (89.00%) (32356/35968)
Epoch: 79 | Batch_idx: 290 |  Loss: (0.2857) |  Loss2: (0.0000) | Acc: (89.00%) (33513/37248)
Epoch: 79 | Batch_idx: 300 |  Loss: (0.2861) |  Loss2: (0.0000) | Acc: (89.00%) (34660/38528)
Epoch: 79 | Batch_idx: 310 |  Loss: (0.2861) |  Loss2: (0.0000) | Acc: (89.00%) (35818/39808)
Epoch: 79 | Batch_idx: 320 |  Loss: (0.2855) |  Loss2: (0.0000) | Acc: (90.00%) (36982/41088)
Epoch: 79 | Batch_idx: 330 |  Loss: (0.2850) |  Loss2: (0.0000) | Acc: (90.00%) (38138/42368)
Epoch: 79 | Batch_idx: 340 |  Loss: (0.2833) |  Loss2: (0.0000) | Acc: (90.00%) (39321/43648)
Epoch: 79 | Batch_idx: 350 |  Loss: (0.2827) |  Loss2: (0.0000) | Acc: (90.00%) (40485/44928)
Epoch: 79 | Batch_idx: 360 |  Loss: (0.2833) |  Loss2: (0.0000) | Acc: (90.00%) (41627/46208)
Epoch: 79 | Batch_idx: 370 |  Loss: (0.2833) |  Loss2: (0.0000) | Acc: (90.00%) (42788/47488)
Epoch: 79 | Batch_idx: 380 |  Loss: (0.2833) |  Loss2: (0.0000) | Acc: (90.00%) (43945/48768)
Epoch: 79 | Batch_idx: 390 |  Loss: (0.2842) |  Loss2: (0.0000) | Acc: (90.00%) (45045/50000)
# TEST : Loss: (0.4161) | Acc: (86.00%) (8611/10000)
percent tensor([0.5394, 0.5491, 0.5411, 0.5218, 0.5450, 0.5575, 0.5523, 0.5299, 0.5347,
        0.5446, 0.5472, 0.5447, 0.5369, 0.5195, 0.5538, 0.5374],
       device='cuda:0') torch.Size([16])
percent tensor([0.5118, 0.5110, 0.5126, 0.5150, 0.5130, 0.5133, 0.5120, 0.5134, 0.5109,
        0.5109, 0.5104, 0.5107, 0.5104, 0.5124, 0.5119, 0.5134],
       device='cuda:0') torch.Size([16])
percent tensor([0.6052, 0.6087, 0.4906, 0.4692, 0.4659, 0.5780, 0.5808, 0.4866, 0.5305,
        0.6000, 0.6126, 0.5289, 0.6159, 0.5886, 0.5838, 0.6045],
       device='cuda:0') torch.Size([16])
percent tensor([0.6677, 0.5465, 0.6547, 0.6885, 0.6676, 0.7449, 0.5987, 0.6640, 0.6160,
        0.5827, 0.6125, 0.5817, 0.5458, 0.6497, 0.6185, 0.7135],
       device='cuda:0') torch.Size([16])
percent tensor([0.5752, 0.5757, 0.5720, 0.5500, 0.5473, 0.5302, 0.5833, 0.5842, 0.5741,
        0.5533, 0.5520, 0.5525, 0.5570, 0.5767, 0.5735, 0.5414],
       device='cuda:0') torch.Size([16])
percent tensor([0.5543, 0.5957, 0.6443, 0.6828, 0.6652, 0.6635, 0.6139, 0.5996, 0.6594,
        0.5586, 0.6385, 0.7038, 0.5861, 0.6637, 0.5839, 0.5646],
       device='cuda:0') torch.Size([16])
percent tensor([0.5968, 0.5716, 0.5049, 0.4948, 0.5925, 0.6495, 0.5382, 0.4062, 0.6071,
        0.6072, 0.5878, 0.4258, 0.6040, 0.5828, 0.4333, 0.5637],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9992, 0.9994, 0.9997, 0.9997, 0.9983, 0.9994, 0.9996, 0.9997,
        0.9993, 0.9995, 0.9997, 0.9995, 0.9992, 0.9992, 0.9994],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 80 | Batch_idx: 0 |  Loss: (0.2197) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 80 | Batch_idx: 10 |  Loss: (0.2573) |  Loss2: (0.0000) | Acc: (91.00%) (1284/1408)
Epoch: 80 | Batch_idx: 20 |  Loss: (0.2602) |  Loss2: (0.0000) | Acc: (91.00%) (2454/2688)
Epoch: 80 | Batch_idx: 30 |  Loss: (0.2688) |  Loss2: (0.0000) | Acc: (90.00%) (3605/3968)
Epoch: 80 | Batch_idx: 40 |  Loss: (0.2700) |  Loss2: (0.0000) | Acc: (90.00%) (4758/5248)
Epoch: 80 | Batch_idx: 50 |  Loss: (0.2718) |  Loss2: (0.0000) | Acc: (90.00%) (5920/6528)
Epoch: 80 | Batch_idx: 60 |  Loss: (0.2729) |  Loss2: (0.0000) | Acc: (90.00%) (7084/7808)
Epoch: 80 | Batch_idx: 70 |  Loss: (0.2716) |  Loss2: (0.0000) | Acc: (90.00%) (8251/9088)
Epoch: 80 | Batch_idx: 80 |  Loss: (0.2724) |  Loss2: (0.0000) | Acc: (90.00%) (9410/10368)
Epoch: 80 | Batch_idx: 90 |  Loss: (0.2696) |  Loss2: (0.0000) | Acc: (90.00%) (10590/11648)
Epoch: 80 | Batch_idx: 100 |  Loss: (0.2693) |  Loss2: (0.0000) | Acc: (90.00%) (11758/12928)
Epoch: 80 | Batch_idx: 110 |  Loss: (0.2721) |  Loss2: (0.0000) | Acc: (90.00%) (12908/14208)
Epoch: 80 | Batch_idx: 120 |  Loss: (0.2764) |  Loss2: (0.0000) | Acc: (90.00%) (14054/15488)
Epoch: 80 | Batch_idx: 130 |  Loss: (0.2765) |  Loss2: (0.0000) | Acc: (90.00%) (15212/16768)
Epoch: 80 | Batch_idx: 140 |  Loss: (0.2777) |  Loss2: (0.0000) | Acc: (90.00%) (16361/18048)
Epoch: 80 | Batch_idx: 150 |  Loss: (0.2776) |  Loss2: (0.0000) | Acc: (90.00%) (17534/19328)
Epoch: 80 | Batch_idx: 160 |  Loss: (0.2750) |  Loss2: (0.0000) | Acc: (90.00%) (18718/20608)
Epoch: 80 | Batch_idx: 170 |  Loss: (0.2750) |  Loss2: (0.0000) | Acc: (90.00%) (19883/21888)
Epoch: 80 | Batch_idx: 180 |  Loss: (0.2747) |  Loss2: (0.0000) | Acc: (90.00%) (21036/23168)
Epoch: 80 | Batch_idx: 190 |  Loss: (0.2765) |  Loss2: (0.0000) | Acc: (90.00%) (22185/24448)
Epoch: 80 | Batch_idx: 200 |  Loss: (0.2755) |  Loss2: (0.0000) | Acc: (90.00%) (23354/25728)
Epoch: 80 | Batch_idx: 210 |  Loss: (0.2761) |  Loss2: (0.0000) | Acc: (90.00%) (24513/27008)
Epoch: 80 | Batch_idx: 220 |  Loss: (0.2770) |  Loss2: (0.0000) | Acc: (90.00%) (25654/28288)
Epoch: 80 | Batch_idx: 230 |  Loss: (0.2764) |  Loss2: (0.0000) | Acc: (90.00%) (26814/29568)
Epoch: 80 | Batch_idx: 240 |  Loss: (0.2765) |  Loss2: (0.0000) | Acc: (90.00%) (27981/30848)
Epoch: 80 | Batch_idx: 250 |  Loss: (0.2764) |  Loss2: (0.0000) | Acc: (90.00%) (29147/32128)
Epoch: 80 | Batch_idx: 260 |  Loss: (0.2768) |  Loss2: (0.0000) | Acc: (90.00%) (30293/33408)
Epoch: 80 | Batch_idx: 270 |  Loss: (0.2778) |  Loss2: (0.0000) | Acc: (90.00%) (31450/34688)
Epoch: 80 | Batch_idx: 280 |  Loss: (0.2789) |  Loss2: (0.0000) | Acc: (90.00%) (32591/35968)
Epoch: 80 | Batch_idx: 290 |  Loss: (0.2793) |  Loss2: (0.0000) | Acc: (90.00%) (33730/37248)
Epoch: 80 | Batch_idx: 300 |  Loss: (0.2797) |  Loss2: (0.0000) | Acc: (90.00%) (34881/38528)
Epoch: 80 | Batch_idx: 310 |  Loss: (0.2788) |  Loss2: (0.0000) | Acc: (90.00%) (36053/39808)
Epoch: 80 | Batch_idx: 320 |  Loss: (0.2792) |  Loss2: (0.0000) | Acc: (90.00%) (37200/41088)
Epoch: 80 | Batch_idx: 330 |  Loss: (0.2788) |  Loss2: (0.0000) | Acc: (90.00%) (38358/42368)
Epoch: 80 | Batch_idx: 340 |  Loss: (0.2786) |  Loss2: (0.0000) | Acc: (90.00%) (39518/43648)
Epoch: 80 | Batch_idx: 350 |  Loss: (0.2791) |  Loss2: (0.0000) | Acc: (90.00%) (40672/44928)
Epoch: 80 | Batch_idx: 360 |  Loss: (0.2792) |  Loss2: (0.0000) | Acc: (90.00%) (41831/46208)
Epoch: 80 | Batch_idx: 370 |  Loss: (0.2793) |  Loss2: (0.0000) | Acc: (90.00%) (42985/47488)
Epoch: 80 | Batch_idx: 380 |  Loss: (0.2788) |  Loss2: (0.0000) | Acc: (90.00%) (44137/48768)
Epoch: 80 | Batch_idx: 390 |  Loss: (0.2786) |  Loss2: (0.0000) | Acc: (90.00%) (45261/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_080.pth.tar'
# TEST : Loss: (0.4315) | Acc: (85.00%) (8567/10000)
percent tensor([0.5401, 0.5484, 0.5437, 0.5234, 0.5478, 0.5590, 0.5526, 0.5315, 0.5357,
        0.5448, 0.5471, 0.5458, 0.5374, 0.5190, 0.5539, 0.5383],
       device='cuda:0') torch.Size([16])
percent tensor([0.5120, 0.5110, 0.5123, 0.5151, 0.5128, 0.5137, 0.5117, 0.5135, 0.5111,
        0.5109, 0.5106, 0.5102, 0.5105, 0.5122, 0.5119, 0.5133],
       device='cuda:0') torch.Size([16])
percent tensor([0.6105, 0.6128, 0.4871, 0.4780, 0.4677, 0.5868, 0.5815, 0.4959, 0.5453,
        0.5995, 0.6174, 0.5269, 0.6172, 0.5962, 0.5892, 0.6101],
       device='cuda:0') torch.Size([16])
percent tensor([0.6656, 0.5494, 0.6503, 0.6822, 0.6591, 0.7514, 0.5977, 0.6758, 0.6158,
        0.5730, 0.6176, 0.5599, 0.5454, 0.6481, 0.6249, 0.7121],
       device='cuda:0') torch.Size([16])
percent tensor([0.5708, 0.5845, 0.5814, 0.5591, 0.5600, 0.5154, 0.5904, 0.5886, 0.5729,
        0.5604, 0.5451, 0.5662, 0.5598, 0.5840, 0.5765, 0.5386],
       device='cuda:0') torch.Size([16])
percent tensor([0.5605, 0.6006, 0.6487, 0.6794, 0.6797, 0.6719, 0.6258, 0.5963, 0.6661,
        0.5596, 0.6537, 0.6971, 0.5965, 0.6718, 0.5904, 0.5779],
       device='cuda:0') torch.Size([16])
percent tensor([0.5854, 0.5798, 0.5353, 0.4956, 0.5925, 0.6497, 0.5429, 0.4219, 0.5957,
        0.6224, 0.5967, 0.4363, 0.6001, 0.5906, 0.4362, 0.5721],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9994, 0.9996, 0.9998, 0.9998, 0.9977, 0.9994, 0.9998, 0.9993,
        0.9994, 0.9995, 0.9998, 0.9995, 0.9991, 0.9990, 0.9994],
       device='cuda:0') torch.Size([16])
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(178.2723, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(809.5039, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(802.6822, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1522.8319, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(496.0868, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2227.1494, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4268.4644, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1408.5266, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6137.6118, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11941.7158, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3969.1021, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16775.6133, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 81 | Batch_idx: 0 |  Loss: (0.2868) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 81 | Batch_idx: 10 |  Loss: (0.2681) |  Loss2: (0.0000) | Acc: (91.00%) (1289/1408)
Epoch: 81 | Batch_idx: 20 |  Loss: (0.2661) |  Loss2: (0.0000) | Acc: (91.00%) (2454/2688)
Epoch: 81 | Batch_idx: 30 |  Loss: (0.2674) |  Loss2: (0.0000) | Acc: (91.00%) (3621/3968)
Epoch: 81 | Batch_idx: 40 |  Loss: (0.2676) |  Loss2: (0.0000) | Acc: (91.00%) (4779/5248)
Epoch: 81 | Batch_idx: 50 |  Loss: (0.2639) |  Loss2: (0.0000) | Acc: (91.00%) (5961/6528)
Epoch: 81 | Batch_idx: 60 |  Loss: (0.2596) |  Loss2: (0.0000) | Acc: (91.00%) (7135/7808)
Epoch: 81 | Batch_idx: 70 |  Loss: (0.2575) |  Loss2: (0.0000) | Acc: (91.00%) (8308/9088)
Epoch: 81 | Batch_idx: 80 |  Loss: (0.2586) |  Loss2: (0.0000) | Acc: (91.00%) (9463/10368)
Epoch: 81 | Batch_idx: 90 |  Loss: (0.2587) |  Loss2: (0.0000) | Acc: (91.00%) (10638/11648)
Epoch: 81 | Batch_idx: 100 |  Loss: (0.2615) |  Loss2: (0.0000) | Acc: (91.00%) (11789/12928)
Epoch: 81 | Batch_idx: 110 |  Loss: (0.2633) |  Loss2: (0.0000) | Acc: (91.00%) (12931/14208)
Epoch: 81 | Batch_idx: 120 |  Loss: (0.2651) |  Loss2: (0.0000) | Acc: (90.00%) (14080/15488)
Epoch: 81 | Batch_idx: 130 |  Loss: (0.2676) |  Loss2: (0.0000) | Acc: (90.00%) (15228/16768)
Epoch: 81 | Batch_idx: 140 |  Loss: (0.2668) |  Loss2: (0.0000) | Acc: (90.00%) (16395/18048)
Epoch: 81 | Batch_idx: 150 |  Loss: (0.2662) |  Loss2: (0.0000) | Acc: (90.00%) (17558/19328)
Epoch: 81 | Batch_idx: 160 |  Loss: (0.2665) |  Loss2: (0.0000) | Acc: (90.00%) (18716/20608)
Epoch: 81 | Batch_idx: 170 |  Loss: (0.2664) |  Loss2: (0.0000) | Acc: (90.00%) (19880/21888)
Epoch: 81 | Batch_idx: 180 |  Loss: (0.2674) |  Loss2: (0.0000) | Acc: (90.00%) (21028/23168)
Epoch: 81 | Batch_idx: 190 |  Loss: (0.2675) |  Loss2: (0.0000) | Acc: (90.00%) (22196/24448)
Epoch: 81 | Batch_idx: 200 |  Loss: (0.2664) |  Loss2: (0.0000) | Acc: (90.00%) (23367/25728)
Epoch: 81 | Batch_idx: 210 |  Loss: (0.2669) |  Loss2: (0.0000) | Acc: (90.00%) (24519/27008)
Epoch: 81 | Batch_idx: 220 |  Loss: (0.2660) |  Loss2: (0.0000) | Acc: (90.00%) (25691/28288)
Epoch: 81 | Batch_idx: 230 |  Loss: (0.2659) |  Loss2: (0.0000) | Acc: (90.00%) (26855/29568)
Epoch: 81 | Batch_idx: 240 |  Loss: (0.2663) |  Loss2: (0.0000) | Acc: (90.00%) (28015/30848)
Epoch: 81 | Batch_idx: 250 |  Loss: (0.2662) |  Loss2: (0.0000) | Acc: (90.00%) (29169/32128)
Epoch: 81 | Batch_idx: 260 |  Loss: (0.2668) |  Loss2: (0.0000) | Acc: (90.00%) (30326/33408)
Epoch: 81 | Batch_idx: 270 |  Loss: (0.2661) |  Loss2: (0.0000) | Acc: (90.00%) (31492/34688)
Epoch: 81 | Batch_idx: 280 |  Loss: (0.2664) |  Loss2: (0.0000) | Acc: (90.00%) (32647/35968)
Epoch: 81 | Batch_idx: 290 |  Loss: (0.2672) |  Loss2: (0.0000) | Acc: (90.00%) (33801/37248)
Epoch: 81 | Batch_idx: 300 |  Loss: (0.2676) |  Loss2: (0.0000) | Acc: (90.00%) (34948/38528)
Epoch: 81 | Batch_idx: 310 |  Loss: (0.2680) |  Loss2: (0.0000) | Acc: (90.00%) (36094/39808)
Epoch: 81 | Batch_idx: 320 |  Loss: (0.2672) |  Loss2: (0.0000) | Acc: (90.00%) (37266/41088)
Epoch: 81 | Batch_idx: 330 |  Loss: (0.2681) |  Loss2: (0.0000) | Acc: (90.00%) (38415/42368)
Epoch: 81 | Batch_idx: 340 |  Loss: (0.2675) |  Loss2: (0.0000) | Acc: (90.00%) (39592/43648)
Epoch: 81 | Batch_idx: 350 |  Loss: (0.2678) |  Loss2: (0.0000) | Acc: (90.00%) (40747/44928)
Epoch: 81 | Batch_idx: 360 |  Loss: (0.2678) |  Loss2: (0.0000) | Acc: (90.00%) (41913/46208)
Epoch: 81 | Batch_idx: 370 |  Loss: (0.2672) |  Loss2: (0.0000) | Acc: (90.00%) (43084/47488)
Epoch: 81 | Batch_idx: 380 |  Loss: (0.2683) |  Loss2: (0.0000) | Acc: (90.00%) (44231/48768)
Epoch: 81 | Batch_idx: 390 |  Loss: (0.2685) |  Loss2: (0.0000) | Acc: (90.00%) (45340/50000)
# TEST : Loss: (0.4245) | Acc: (85.00%) (8579/10000)
percent tensor([0.5416, 0.5473, 0.5435, 0.5248, 0.5476, 0.5610, 0.5514, 0.5317, 0.5361,
        0.5446, 0.5484, 0.5456, 0.5386, 0.5146, 0.5552, 0.5393],
       device='cuda:0') torch.Size([16])
percent tensor([0.5119, 0.5110, 0.5120, 0.5141, 0.5125, 0.5130, 0.5121, 0.5132, 0.5111,
        0.5110, 0.5108, 0.5103, 0.5105, 0.5128, 0.5117, 0.5131],
       device='cuda:0') torch.Size([16])
percent tensor([0.6056, 0.6079, 0.4948, 0.4905, 0.4680, 0.5840, 0.5788, 0.4905, 0.5319,
        0.5930, 0.6105, 0.5293, 0.6105, 0.5872, 0.5860, 0.6100],
       device='cuda:0') torch.Size([16])
percent tensor([0.6677, 0.5586, 0.6681, 0.6771, 0.6673, 0.7367, 0.6157, 0.6735, 0.6239,
        0.5949, 0.6179, 0.5879, 0.5534, 0.6592, 0.6210, 0.7139],
       device='cuda:0') torch.Size([16])
percent tensor([0.5737, 0.5765, 0.5722, 0.5491, 0.5432, 0.5398, 0.5768, 0.5762, 0.5746,
        0.5557, 0.5608, 0.5527, 0.5618, 0.5680, 0.5733, 0.5353],
       device='cuda:0') torch.Size([16])
percent tensor([0.5637, 0.6229, 0.6518, 0.6926, 0.6981, 0.6763, 0.6492, 0.5931, 0.6716,
        0.5777, 0.6510, 0.7150, 0.6001, 0.6996, 0.5994, 0.5974],
       device='cuda:0') torch.Size([16])
percent tensor([0.5911, 0.6003, 0.5363, 0.4783, 0.6021, 0.6508, 0.5625, 0.4430, 0.5950,
        0.6180, 0.6059, 0.4331, 0.6008, 0.5898, 0.4300, 0.6064],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9993, 0.9996, 0.9996, 0.9997, 0.9979, 0.9991, 0.9995, 0.9995,
        0.9992, 0.9996, 0.9998, 0.9997, 0.9993, 0.9993, 0.9993],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 82 | Batch_idx: 0 |  Loss: (0.2623) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 82 | Batch_idx: 10 |  Loss: (0.3087) |  Loss2: (0.0000) | Acc: (88.00%) (1251/1408)
Epoch: 82 | Batch_idx: 20 |  Loss: (0.3058) |  Loss2: (0.0000) | Acc: (89.00%) (2397/2688)
Epoch: 82 | Batch_idx: 30 |  Loss: (0.3285) |  Loss2: (0.0000) | Acc: (88.00%) (3509/3968)
Epoch: 82 | Batch_idx: 40 |  Loss: (0.3252) |  Loss2: (0.0000) | Acc: (88.00%) (4640/5248)
Epoch: 82 | Batch_idx: 50 |  Loss: (0.3324) |  Loss2: (0.0000) | Acc: (88.00%) (5764/6528)
Epoch: 82 | Batch_idx: 60 |  Loss: (0.3344) |  Loss2: (0.0000) | Acc: (88.00%) (6886/7808)
Epoch: 82 | Batch_idx: 70 |  Loss: (0.3385) |  Loss2: (0.0000) | Acc: (88.00%) (7998/9088)
Epoch: 82 | Batch_idx: 80 |  Loss: (0.3382) |  Loss2: (0.0000) | Acc: (88.00%) (9144/10368)
Epoch: 82 | Batch_idx: 90 |  Loss: (0.3344) |  Loss2: (0.0000) | Acc: (88.00%) (10283/11648)
Epoch: 82 | Batch_idx: 100 |  Loss: (0.3347) |  Loss2: (0.0000) | Acc: (88.00%) (11397/12928)
Epoch: 82 | Batch_idx: 110 |  Loss: (0.3327) |  Loss2: (0.0000) | Acc: (88.00%) (12535/14208)
Epoch: 82 | Batch_idx: 120 |  Loss: (0.3335) |  Loss2: (0.0000) | Acc: (88.00%) (13663/15488)
Epoch: 82 | Batch_idx: 130 |  Loss: (0.3338) |  Loss2: (0.0000) | Acc: (88.00%) (14796/16768)
Epoch: 82 | Batch_idx: 140 |  Loss: (0.3377) |  Loss2: (0.0000) | Acc: (88.00%) (15898/18048)
Epoch: 82 | Batch_idx: 150 |  Loss: (0.3385) |  Loss2: (0.0000) | Acc: (88.00%) (17029/19328)
Epoch: 82 | Batch_idx: 160 |  Loss: (0.3360) |  Loss2: (0.0000) | Acc: (88.00%) (18170/20608)
Epoch: 82 | Batch_idx: 170 |  Loss: (0.3373) |  Loss2: (0.0000) | Acc: (88.00%) (19289/21888)
Epoch: 82 | Batch_idx: 180 |  Loss: (0.3345) |  Loss2: (0.0000) | Acc: (88.00%) (20445/23168)
Epoch: 82 | Batch_idx: 190 |  Loss: (0.3325) |  Loss2: (0.0000) | Acc: (88.00%) (21599/24448)
Epoch: 82 | Batch_idx: 200 |  Loss: (0.3310) |  Loss2: (0.0000) | Acc: (88.00%) (22741/25728)
Epoch: 82 | Batch_idx: 210 |  Loss: (0.3309) |  Loss2: (0.0000) | Acc: (88.00%) (23879/27008)
Epoch: 82 | Batch_idx: 220 |  Loss: (0.3306) |  Loss2: (0.0000) | Acc: (88.00%) (25016/28288)
Epoch: 82 | Batch_idx: 230 |  Loss: (0.3297) |  Loss2: (0.0000) | Acc: (88.00%) (26160/29568)
Epoch: 82 | Batch_idx: 240 |  Loss: (0.3273) |  Loss2: (0.0000) | Acc: (88.00%) (27320/30848)
Epoch: 82 | Batch_idx: 250 |  Loss: (0.3262) |  Loss2: (0.0000) | Acc: (88.00%) (28471/32128)
Epoch: 82 | Batch_idx: 260 |  Loss: (0.3265) |  Loss2: (0.0000) | Acc: (88.00%) (29601/33408)
Epoch: 82 | Batch_idx: 270 |  Loss: (0.3257) |  Loss2: (0.0000) | Acc: (88.00%) (30740/34688)
Epoch: 82 | Batch_idx: 280 |  Loss: (0.3248) |  Loss2: (0.0000) | Acc: (88.00%) (31885/35968)
Epoch: 82 | Batch_idx: 290 |  Loss: (0.3245) |  Loss2: (0.0000) | Acc: (88.00%) (33033/37248)
Epoch: 82 | Batch_idx: 300 |  Loss: (0.3233) |  Loss2: (0.0000) | Acc: (88.00%) (34187/38528)
Epoch: 82 | Batch_idx: 310 |  Loss: (0.3229) |  Loss2: (0.0000) | Acc: (88.00%) (35342/39808)
Epoch: 82 | Batch_idx: 320 |  Loss: (0.3223) |  Loss2: (0.0000) | Acc: (88.00%) (36496/41088)
Epoch: 82 | Batch_idx: 330 |  Loss: (0.3216) |  Loss2: (0.0000) | Acc: (88.00%) (37644/42368)
Epoch: 82 | Batch_idx: 340 |  Loss: (0.3199) |  Loss2: (0.0000) | Acc: (88.00%) (38815/43648)
Epoch: 82 | Batch_idx: 350 |  Loss: (0.3194) |  Loss2: (0.0000) | Acc: (88.00%) (39960/44928)
Epoch: 82 | Batch_idx: 360 |  Loss: (0.3190) |  Loss2: (0.0000) | Acc: (88.00%) (41100/46208)
Epoch: 82 | Batch_idx: 370 |  Loss: (0.3187) |  Loss2: (0.0000) | Acc: (88.00%) (42237/47488)
Epoch: 82 | Batch_idx: 380 |  Loss: (0.3171) |  Loss2: (0.0000) | Acc: (88.00%) (43402/48768)
Epoch: 82 | Batch_idx: 390 |  Loss: (0.3159) |  Loss2: (0.0000) | Acc: (89.00%) (44525/50000)
# TEST : Loss: (0.4309) | Acc: (85.00%) (8579/10000)
percent tensor([0.5482, 0.5523, 0.5545, 0.5305, 0.5576, 0.5680, 0.5593, 0.5380, 0.5430,
        0.5510, 0.5539, 0.5563, 0.5447, 0.5169, 0.5612, 0.5442],
       device='cuda:0') torch.Size([16])
percent tensor([0.5171, 0.5162, 0.5175, 0.5204, 0.5186, 0.5183, 0.5173, 0.5193, 0.5165,
        0.5162, 0.5160, 0.5152, 0.5156, 0.5184, 0.5169, 0.5187],
       device='cuda:0') torch.Size([16])
percent tensor([0.5743, 0.5761, 0.4613, 0.4508, 0.4316, 0.5510, 0.5415, 0.4398, 0.4902,
        0.5569, 0.5767, 0.4948, 0.5765, 0.5471, 0.5491, 0.5714],
       device='cuda:0') torch.Size([16])
percent tensor([0.6417, 0.5379, 0.6487, 0.6686, 0.6489, 0.7212, 0.5906, 0.6570, 0.6062,
        0.5706, 0.5930, 0.5692, 0.5310, 0.6363, 0.5951, 0.6849],
       device='cuda:0') torch.Size([16])
percent tensor([0.6182, 0.6231, 0.5972, 0.5591, 0.5551, 0.5630, 0.6149, 0.5859, 0.6138,
        0.6033, 0.6131, 0.5894, 0.6171, 0.6127, 0.6079, 0.5796],
       device='cuda:0') torch.Size([16])
percent tensor([0.5462, 0.5972, 0.6393, 0.6712, 0.6836, 0.6543, 0.6236, 0.5939, 0.6444,
        0.5656, 0.6132, 0.6883, 0.5762, 0.6631, 0.5807, 0.5834],
       device='cuda:0') torch.Size([16])
percent tensor([0.5949, 0.6115, 0.5406, 0.4836, 0.6248, 0.6547, 0.5906, 0.4700, 0.5948,
        0.6183, 0.6070, 0.4211, 0.6021, 0.6047, 0.4618, 0.6266],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9991, 0.9996, 0.9995, 0.9996, 0.9985, 0.9993, 0.9995, 0.9994,
        0.9992, 0.9995, 0.9998, 0.9996, 0.9993, 0.9992, 0.9995],
       device='cuda:0') torch.Size([16])
Epoch: 83 | Batch_idx: 0 |  Loss: (0.2440) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 83 | Batch_idx: 10 |  Loss: (0.2676) |  Loss2: (0.0000) | Acc: (90.00%) (1277/1408)
Epoch: 83 | Batch_idx: 20 |  Loss: (0.2761) |  Loss2: (0.0000) | Acc: (90.00%) (2434/2688)
Epoch: 83 | Batch_idx: 30 |  Loss: (0.2843) |  Loss2: (0.0000) | Acc: (90.00%) (3585/3968)
Epoch: 83 | Batch_idx: 40 |  Loss: (0.2884) |  Loss2: (0.0000) | Acc: (90.00%) (4728/5248)
Epoch: 83 | Batch_idx: 50 |  Loss: (0.2904) |  Loss2: (0.0000) | Acc: (90.00%) (5878/6528)
Epoch: 83 | Batch_idx: 60 |  Loss: (0.2857) |  Loss2: (0.0000) | Acc: (90.00%) (7036/7808)
Epoch: 83 | Batch_idx: 70 |  Loss: (0.2858) |  Loss2: (0.0000) | Acc: (89.00%) (8178/9088)
Epoch: 83 | Batch_idx: 80 |  Loss: (0.2851) |  Loss2: (0.0000) | Acc: (90.00%) (9342/10368)
Epoch: 83 | Batch_idx: 90 |  Loss: (0.2837) |  Loss2: (0.0000) | Acc: (90.00%) (10498/11648)
Epoch: 83 | Batch_idx: 100 |  Loss: (0.2852) |  Loss2: (0.0000) | Acc: (90.00%) (11644/12928)
Epoch: 83 | Batch_idx: 110 |  Loss: (0.2802) |  Loss2: (0.0000) | Acc: (90.00%) (12819/14208)
Epoch: 83 | Batch_idx: 120 |  Loss: (0.2826) |  Loss2: (0.0000) | Acc: (90.00%) (13970/15488)
Epoch: 83 | Batch_idx: 130 |  Loss: (0.2855) |  Loss2: (0.0000) | Acc: (90.00%) (15108/16768)
Epoch: 83 | Batch_idx: 140 |  Loss: (0.2865) |  Loss2: (0.0000) | Acc: (90.00%) (16260/18048)
Epoch: 83 | Batch_idx: 150 |  Loss: (0.2865) |  Loss2: (0.0000) | Acc: (90.00%) (17409/19328)
Epoch: 83 | Batch_idx: 160 |  Loss: (0.2876) |  Loss2: (0.0000) | Acc: (90.00%) (18551/20608)
Epoch: 83 | Batch_idx: 170 |  Loss: (0.2871) |  Loss2: (0.0000) | Acc: (90.00%) (19711/21888)
Epoch: 83 | Batch_idx: 180 |  Loss: (0.2880) |  Loss2: (0.0000) | Acc: (89.00%) (20846/23168)
Epoch: 83 | Batch_idx: 190 |  Loss: (0.2882) |  Loss2: (0.0000) | Acc: (89.00%) (21997/24448)
Epoch: 83 | Batch_idx: 200 |  Loss: (0.2886) |  Loss2: (0.0000) | Acc: (89.00%) (23134/25728)
Epoch: 83 | Batch_idx: 210 |  Loss: (0.2903) |  Loss2: (0.0000) | Acc: (89.00%) (24274/27008)
Epoch: 83 | Batch_idx: 220 |  Loss: (0.2897) |  Loss2: (0.0000) | Acc: (89.00%) (25432/28288)
Epoch: 83 | Batch_idx: 230 |  Loss: (0.2908) |  Loss2: (0.0000) | Acc: (89.00%) (26571/29568)
Epoch: 83 | Batch_idx: 240 |  Loss: (0.2909) |  Loss2: (0.0000) | Acc: (89.00%) (27718/30848)
Epoch: 83 | Batch_idx: 250 |  Loss: (0.2903) |  Loss2: (0.0000) | Acc: (89.00%) (28874/32128)
Epoch: 83 | Batch_idx: 260 |  Loss: (0.2905) |  Loss2: (0.0000) | Acc: (89.00%) (30034/33408)
Epoch: 83 | Batch_idx: 270 |  Loss: (0.2889) |  Loss2: (0.0000) | Acc: (89.00%) (31200/34688)
Epoch: 83 | Batch_idx: 280 |  Loss: (0.2886) |  Loss2: (0.0000) | Acc: (89.00%) (32358/35968)
Epoch: 83 | Batch_idx: 290 |  Loss: (0.2884) |  Loss2: (0.0000) | Acc: (89.00%) (33511/37248)
Epoch: 83 | Batch_idx: 300 |  Loss: (0.2879) |  Loss2: (0.0000) | Acc: (89.00%) (34656/38528)
Epoch: 83 | Batch_idx: 310 |  Loss: (0.2884) |  Loss2: (0.0000) | Acc: (89.00%) (35801/39808)
Epoch: 83 | Batch_idx: 320 |  Loss: (0.2881) |  Loss2: (0.0000) | Acc: (89.00%) (36967/41088)
Epoch: 83 | Batch_idx: 330 |  Loss: (0.2872) |  Loss2: (0.0000) | Acc: (89.00%) (38129/42368)
Epoch: 83 | Batch_idx: 340 |  Loss: (0.2864) |  Loss2: (0.0000) | Acc: (90.00%) (39296/43648)
Epoch: 83 | Batch_idx: 350 |  Loss: (0.2860) |  Loss2: (0.0000) | Acc: (90.00%) (40457/44928)
Epoch: 83 | Batch_idx: 360 |  Loss: (0.2864) |  Loss2: (0.0000) | Acc: (90.00%) (41610/46208)
Epoch: 83 | Batch_idx: 370 |  Loss: (0.2855) |  Loss2: (0.0000) | Acc: (90.00%) (42776/47488)
Epoch: 83 | Batch_idx: 380 |  Loss: (0.2845) |  Loss2: (0.0000) | Acc: (90.00%) (43940/48768)
Epoch: 83 | Batch_idx: 390 |  Loss: (0.2846) |  Loss2: (0.0000) | Acc: (90.00%) (45059/50000)
# TEST : Loss: (0.4095) | Acc: (86.00%) (8644/10000)
percent tensor([0.5491, 0.5517, 0.5576, 0.5310, 0.5600, 0.5690, 0.5601, 0.5387, 0.5433,
        0.5517, 0.5540, 0.5589, 0.5452, 0.5138, 0.5617, 0.5441],
       device='cuda:0') torch.Size([16])
percent tensor([0.5202, 0.5191, 0.5209, 0.5240, 0.5222, 0.5215, 0.5204, 0.5228, 0.5196,
        0.5191, 0.5188, 0.5181, 0.5183, 0.5215, 0.5199, 0.5218],
       device='cuda:0') torch.Size([16])
percent tensor([0.5771, 0.5769, 0.4630, 0.4513, 0.4307, 0.5524, 0.5435, 0.4414, 0.4892,
        0.5592, 0.5797, 0.4968, 0.5778, 0.5452, 0.5516, 0.5743],
       device='cuda:0') torch.Size([16])
percent tensor([0.6487, 0.5446, 0.6583, 0.6710, 0.6536, 0.7205, 0.5969, 0.6624, 0.6149,
        0.5783, 0.6007, 0.5817, 0.5422, 0.6399, 0.5989, 0.6849],
       device='cuda:0') torch.Size([16])
percent tensor([0.6155, 0.6254, 0.5878, 0.5533, 0.5472, 0.5651, 0.6121, 0.5736, 0.6132,
        0.6033, 0.6151, 0.5854, 0.6194, 0.6156, 0.6062, 0.5792],
       device='cuda:0') torch.Size([16])
percent tensor([0.5456, 0.5928, 0.6350, 0.6646, 0.6786, 0.6482, 0.6215, 0.6021, 0.6393,
        0.5666, 0.6066, 0.6797, 0.5693, 0.6551, 0.5810, 0.5855],
       device='cuda:0') torch.Size([16])
percent tensor([0.6173, 0.6371, 0.5642, 0.5046, 0.6419, 0.6751, 0.6090, 0.4859, 0.6167,
        0.6395, 0.6322, 0.4518, 0.6291, 0.6265, 0.4924, 0.6467],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9991, 0.9996, 0.9996, 0.9996, 0.9985, 0.9994, 0.9996, 0.9994,
        0.9993, 0.9995, 0.9998, 0.9996, 0.9994, 0.9993, 0.9995],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 84 | Batch_idx: 0 |  Loss: (0.1896) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 84 | Batch_idx: 10 |  Loss: (0.2605) |  Loss2: (0.0000) | Acc: (90.00%) (1280/1408)
Epoch: 84 | Batch_idx: 20 |  Loss: (0.2677) |  Loss2: (0.0000) | Acc: (90.00%) (2438/2688)
Epoch: 84 | Batch_idx: 30 |  Loss: (0.2686) |  Loss2: (0.0000) | Acc: (90.00%) (3601/3968)
Epoch: 84 | Batch_idx: 40 |  Loss: (0.2626) |  Loss2: (0.0000) | Acc: (91.00%) (4779/5248)
Epoch: 84 | Batch_idx: 50 |  Loss: (0.2626) |  Loss2: (0.0000) | Acc: (91.00%) (5941/6528)
Epoch: 84 | Batch_idx: 60 |  Loss: (0.2671) |  Loss2: (0.0000) | Acc: (90.00%) (7094/7808)
Epoch: 84 | Batch_idx: 70 |  Loss: (0.2669) |  Loss2: (0.0000) | Acc: (90.00%) (8250/9088)
Epoch: 84 | Batch_idx: 80 |  Loss: (0.2661) |  Loss2: (0.0000) | Acc: (90.00%) (9425/10368)
Epoch: 84 | Batch_idx: 90 |  Loss: (0.2660) |  Loss2: (0.0000) | Acc: (90.00%) (10596/11648)
Epoch: 84 | Batch_idx: 100 |  Loss: (0.2656) |  Loss2: (0.0000) | Acc: (90.00%) (11762/12928)
Epoch: 84 | Batch_idx: 110 |  Loss: (0.2642) |  Loss2: (0.0000) | Acc: (91.00%) (12931/14208)
Epoch: 84 | Batch_idx: 120 |  Loss: (0.2640) |  Loss2: (0.0000) | Acc: (91.00%) (14095/15488)
Epoch: 84 | Batch_idx: 130 |  Loss: (0.2631) |  Loss2: (0.0000) | Acc: (90.00%) (15253/16768)
Epoch: 84 | Batch_idx: 140 |  Loss: (0.2646) |  Loss2: (0.0000) | Acc: (90.00%) (16407/18048)
Epoch: 84 | Batch_idx: 150 |  Loss: (0.2655) |  Loss2: (0.0000) | Acc: (90.00%) (17568/19328)
Epoch: 84 | Batch_idx: 160 |  Loss: (0.2640) |  Loss2: (0.0000) | Acc: (90.00%) (18745/20608)
Epoch: 84 | Batch_idx: 170 |  Loss: (0.2635) |  Loss2: (0.0000) | Acc: (90.00%) (19917/21888)
Epoch: 84 | Batch_idx: 180 |  Loss: (0.2643) |  Loss2: (0.0000) | Acc: (90.00%) (21080/23168)
Epoch: 84 | Batch_idx: 190 |  Loss: (0.2651) |  Loss2: (0.0000) | Acc: (90.00%) (22234/24448)
Epoch: 84 | Batch_idx: 200 |  Loss: (0.2647) |  Loss2: (0.0000) | Acc: (90.00%) (23408/25728)
Epoch: 84 | Batch_idx: 210 |  Loss: (0.2663) |  Loss2: (0.0000) | Acc: (90.00%) (24560/27008)
Epoch: 84 | Batch_idx: 220 |  Loss: (0.2658) |  Loss2: (0.0000) | Acc: (90.00%) (25721/28288)
Epoch: 84 | Batch_idx: 230 |  Loss: (0.2650) |  Loss2: (0.0000) | Acc: (90.00%) (26894/29568)
Epoch: 84 | Batch_idx: 240 |  Loss: (0.2660) |  Loss2: (0.0000) | Acc: (90.00%) (28058/30848)
Epoch: 84 | Batch_idx: 250 |  Loss: (0.2658) |  Loss2: (0.0000) | Acc: (90.00%) (29218/32128)
Epoch: 84 | Batch_idx: 260 |  Loss: (0.2653) |  Loss2: (0.0000) | Acc: (90.00%) (30387/33408)
Epoch: 84 | Batch_idx: 270 |  Loss: (0.2660) |  Loss2: (0.0000) | Acc: (90.00%) (31543/34688)
Epoch: 84 | Batch_idx: 280 |  Loss: (0.2656) |  Loss2: (0.0000) | Acc: (90.00%) (32721/35968)
Epoch: 84 | Batch_idx: 290 |  Loss: (0.2657) |  Loss2: (0.0000) | Acc: (90.00%) (33891/37248)
Epoch: 84 | Batch_idx: 300 |  Loss: (0.2656) |  Loss2: (0.0000) | Acc: (90.00%) (35059/38528)
Epoch: 84 | Batch_idx: 310 |  Loss: (0.2647) |  Loss2: (0.0000) | Acc: (90.00%) (36223/39808)
Epoch: 84 | Batch_idx: 320 |  Loss: (0.2654) |  Loss2: (0.0000) | Acc: (90.00%) (37383/41088)
Epoch: 84 | Batch_idx: 330 |  Loss: (0.2656) |  Loss2: (0.0000) | Acc: (90.00%) (38547/42368)
Epoch: 84 | Batch_idx: 340 |  Loss: (0.2656) |  Loss2: (0.0000) | Acc: (90.00%) (39713/43648)
Epoch: 84 | Batch_idx: 350 |  Loss: (0.2652) |  Loss2: (0.0000) | Acc: (90.00%) (40872/44928)
Epoch: 84 | Batch_idx: 360 |  Loss: (0.2647) |  Loss2: (0.0000) | Acc: (90.00%) (42043/46208)
Epoch: 84 | Batch_idx: 370 |  Loss: (0.2642) |  Loss2: (0.0000) | Acc: (90.00%) (43212/47488)
Epoch: 84 | Batch_idx: 380 |  Loss: (0.2641) |  Loss2: (0.0000) | Acc: (90.00%) (44371/48768)
Epoch: 84 | Batch_idx: 390 |  Loss: (0.2641) |  Loss2: (0.0000) | Acc: (90.00%) (45489/50000)
# TEST : Loss: (0.4299) | Acc: (86.00%) (8606/10000)
percent tensor([0.5472, 0.5527, 0.5536, 0.5303, 0.5578, 0.5682, 0.5599, 0.5382, 0.5411,
        0.5514, 0.5529, 0.5561, 0.5434, 0.5165, 0.5619, 0.5446],
       device='cuda:0') torch.Size([16])
percent tensor([0.5193, 0.5191, 0.5203, 0.5229, 0.5216, 0.5215, 0.5203, 0.5220, 0.5195,
        0.5184, 0.5185, 0.5178, 0.5177, 0.5219, 0.5197, 0.5212],
       device='cuda:0') torch.Size([16])
percent tensor([0.5753, 0.5657, 0.4561, 0.4304, 0.4229, 0.5468, 0.5366, 0.4592, 0.4960,
        0.5524, 0.5778, 0.4834, 0.5721, 0.5364, 0.5458, 0.5706],
       device='cuda:0') torch.Size([16])
percent tensor([0.6383, 0.5194, 0.6534, 0.6601, 0.6515, 0.7271, 0.5887, 0.6518, 0.6142,
        0.5552, 0.5890, 0.5835, 0.5202, 0.6381, 0.5929, 0.6779],
       device='cuda:0') torch.Size([16])
percent tensor([0.5973, 0.6124, 0.5824, 0.5544, 0.5510, 0.5509, 0.6117, 0.5824, 0.6082,
        0.5909, 0.5925, 0.5731, 0.6055, 0.5984, 0.5964, 0.5718],
       device='cuda:0') torch.Size([16])
percent tensor([0.5327, 0.5846, 0.6075, 0.6514, 0.6587, 0.6418, 0.6037, 0.5951, 0.6352,
        0.5383, 0.5980, 0.6641, 0.5481, 0.6472, 0.5755, 0.5636],
       device='cuda:0') torch.Size([16])
percent tensor([0.6045, 0.6487, 0.5832, 0.5413, 0.6424, 0.6748, 0.5760, 0.4844, 0.6280,
        0.6642, 0.6355, 0.5066, 0.6417, 0.6059, 0.4941, 0.6165],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9991, 0.9995, 0.9996, 0.9998, 0.9978, 0.9993, 0.9996, 0.9995,
        0.9992, 0.9994, 0.9997, 0.9996, 0.9994, 0.9993, 0.9996],
       device='cuda:0') torch.Size([16])
Epoch: 85 | Batch_idx: 0 |  Loss: (0.2669) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 85 | Batch_idx: 10 |  Loss: (0.2484) |  Loss2: (0.0000) | Acc: (91.00%) (1289/1408)
Epoch: 85 | Batch_idx: 20 |  Loss: (0.2529) |  Loss2: (0.0000) | Acc: (91.00%) (2455/2688)
Epoch: 85 | Batch_idx: 30 |  Loss: (0.2530) |  Loss2: (0.0000) | Acc: (91.00%) (3621/3968)
Epoch: 85 | Batch_idx: 40 |  Loss: (0.2550) |  Loss2: (0.0000) | Acc: (91.00%) (4796/5248)
Epoch: 85 | Batch_idx: 50 |  Loss: (0.2533) |  Loss2: (0.0000) | Acc: (91.00%) (5973/6528)
Epoch: 85 | Batch_idx: 60 |  Loss: (0.2497) |  Loss2: (0.0000) | Acc: (91.00%) (7151/7808)
Epoch: 85 | Batch_idx: 70 |  Loss: (0.2512) |  Loss2: (0.0000) | Acc: (91.00%) (8316/9088)
Epoch: 85 | Batch_idx: 80 |  Loss: (0.2519) |  Loss2: (0.0000) | Acc: (91.00%) (9473/10368)
Epoch: 85 | Batch_idx: 90 |  Loss: (0.2505) |  Loss2: (0.0000) | Acc: (91.00%) (10650/11648)
Epoch: 85 | Batch_idx: 100 |  Loss: (0.2505) |  Loss2: (0.0000) | Acc: (91.00%) (11818/12928)
Epoch: 85 | Batch_idx: 110 |  Loss: (0.2495) |  Loss2: (0.0000) | Acc: (91.00%) (12989/14208)
Epoch: 85 | Batch_idx: 120 |  Loss: (0.2486) |  Loss2: (0.0000) | Acc: (91.00%) (14156/15488)
Epoch: 85 | Batch_idx: 130 |  Loss: (0.2499) |  Loss2: (0.0000) | Acc: (91.00%) (15313/16768)
Epoch: 85 | Batch_idx: 140 |  Loss: (0.2510) |  Loss2: (0.0000) | Acc: (91.00%) (16474/18048)
Epoch: 85 | Batch_idx: 150 |  Loss: (0.2516) |  Loss2: (0.0000) | Acc: (91.00%) (17641/19328)
Epoch: 85 | Batch_idx: 160 |  Loss: (0.2519) |  Loss2: (0.0000) | Acc: (91.00%) (18811/20608)
Epoch: 85 | Batch_idx: 170 |  Loss: (0.2533) |  Loss2: (0.0000) | Acc: (91.00%) (19972/21888)
Epoch: 85 | Batch_idx: 180 |  Loss: (0.2543) |  Loss2: (0.0000) | Acc: (91.00%) (21122/23168)
Epoch: 85 | Batch_idx: 190 |  Loss: (0.2544) |  Loss2: (0.0000) | Acc: (91.00%) (22288/24448)
Epoch: 85 | Batch_idx: 200 |  Loss: (0.2539) |  Loss2: (0.0000) | Acc: (91.00%) (23448/25728)
Epoch: 85 | Batch_idx: 210 |  Loss: (0.2528) |  Loss2: (0.0000) | Acc: (91.00%) (24623/27008)
Epoch: 85 | Batch_idx: 220 |  Loss: (0.2530) |  Loss2: (0.0000) | Acc: (91.00%) (25802/28288)
Epoch: 85 | Batch_idx: 230 |  Loss: (0.2522) |  Loss2: (0.0000) | Acc: (91.00%) (26979/29568)
Epoch: 85 | Batch_idx: 240 |  Loss: (0.2527) |  Loss2: (0.0000) | Acc: (91.00%) (28141/30848)
Epoch: 85 | Batch_idx: 250 |  Loss: (0.2539) |  Loss2: (0.0000) | Acc: (91.00%) (29294/32128)
Epoch: 85 | Batch_idx: 260 |  Loss: (0.2544) |  Loss2: (0.0000) | Acc: (91.00%) (30449/33408)
Epoch: 85 | Batch_idx: 270 |  Loss: (0.2552) |  Loss2: (0.0000) | Acc: (91.00%) (31611/34688)
Epoch: 85 | Batch_idx: 280 |  Loss: (0.2545) |  Loss2: (0.0000) | Acc: (91.00%) (32786/35968)
Epoch: 85 | Batch_idx: 290 |  Loss: (0.2544) |  Loss2: (0.0000) | Acc: (91.00%) (33950/37248)
Epoch: 85 | Batch_idx: 300 |  Loss: (0.2548) |  Loss2: (0.0000) | Acc: (91.00%) (35116/38528)
Epoch: 85 | Batch_idx: 310 |  Loss: (0.2557) |  Loss2: (0.0000) | Acc: (91.00%) (36273/39808)
Epoch: 85 | Batch_idx: 320 |  Loss: (0.2560) |  Loss2: (0.0000) | Acc: (91.00%) (37435/41088)
Epoch: 85 | Batch_idx: 330 |  Loss: (0.2562) |  Loss2: (0.0000) | Acc: (91.00%) (38604/42368)
Epoch: 85 | Batch_idx: 340 |  Loss: (0.2562) |  Loss2: (0.0000) | Acc: (91.00%) (39769/43648)
Epoch: 85 | Batch_idx: 350 |  Loss: (0.2565) |  Loss2: (0.0000) | Acc: (91.00%) (40928/44928)
Epoch: 85 | Batch_idx: 360 |  Loss: (0.2568) |  Loss2: (0.0000) | Acc: (91.00%) (42097/46208)
Epoch: 85 | Batch_idx: 370 |  Loss: (0.2568) |  Loss2: (0.0000) | Acc: (91.00%) (43270/47488)
Epoch: 85 | Batch_idx: 380 |  Loss: (0.2565) |  Loss2: (0.0000) | Acc: (91.00%) (44444/48768)
Epoch: 85 | Batch_idx: 390 |  Loss: (0.2566) |  Loss2: (0.0000) | Acc: (91.00%) (45559/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_085.pth.tar'
# TEST : Loss: (0.4021) | Acc: (86.00%) (8681/10000)
percent tensor([0.5478, 0.5530, 0.5520, 0.5291, 0.5556, 0.5683, 0.5594, 0.5372, 0.5409,
        0.5507, 0.5537, 0.5542, 0.5445, 0.5181, 0.5617, 0.5441],
       device='cuda:0') torch.Size([16])
percent tensor([0.5199, 0.5192, 0.5214, 0.5246, 0.5220, 0.5213, 0.5200, 0.5226, 0.5192,
        0.5188, 0.5184, 0.5181, 0.5182, 0.5218, 0.5200, 0.5216],
       device='cuda:0') torch.Size([16])
percent tensor([0.5798, 0.5633, 0.4481, 0.4399, 0.4269, 0.5592, 0.5277, 0.4434, 0.4977,
        0.5500, 0.5747, 0.4759, 0.5764, 0.5303, 0.5514, 0.5732],
       device='cuda:0') torch.Size([16])
percent tensor([0.6423, 0.5237, 0.6439, 0.6616, 0.6468, 0.7271, 0.5820, 0.6441, 0.5991,
        0.5553, 0.5925, 0.5647, 0.5213, 0.6280, 0.5982, 0.6808],
       device='cuda:0') torch.Size([16])
percent tensor([0.6061, 0.6207, 0.5974, 0.5664, 0.5631, 0.5562, 0.6199, 0.5894, 0.6211,
        0.5997, 0.6041, 0.5832, 0.6116, 0.6138, 0.6050, 0.5784],
       device='cuda:0') torch.Size([16])
percent tensor([0.5463, 0.6047, 0.6097, 0.6561, 0.6614, 0.6517, 0.6175, 0.5988, 0.6341,
        0.5497, 0.6215, 0.6637, 0.5676, 0.6565, 0.5800, 0.5770],
       device='cuda:0') torch.Size([16])
percent tensor([0.5884, 0.5865, 0.5869, 0.5430, 0.6439, 0.6810, 0.5709, 0.4754, 0.6053,
        0.6340, 0.5914, 0.4643, 0.6036, 0.5770, 0.4694, 0.5970],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9996, 0.9997, 0.9995, 0.9998, 0.9991, 0.9996, 0.9996, 0.9997,
        0.9995, 0.9997, 0.9998, 0.9997, 0.9995, 0.9993, 0.9994],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 86 | Batch_idx: 0 |  Loss: (0.1385) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 86 | Batch_idx: 10 |  Loss: (0.2769) |  Loss2: (0.0000) | Acc: (90.00%) (1278/1408)
Epoch: 86 | Batch_idx: 20 |  Loss: (0.2875) |  Loss2: (0.0000) | Acc: (89.00%) (2414/2688)
Epoch: 86 | Batch_idx: 30 |  Loss: (0.2860) |  Loss2: (0.0000) | Acc: (89.00%) (3565/3968)
Epoch: 86 | Batch_idx: 40 |  Loss: (0.2913) |  Loss2: (0.0000) | Acc: (89.00%) (4720/5248)
Epoch: 86 | Batch_idx: 50 |  Loss: (0.2905) |  Loss2: (0.0000) | Acc: (90.00%) (5880/6528)
Epoch: 86 | Batch_idx: 60 |  Loss: (0.2942) |  Loss2: (0.0000) | Acc: (89.00%) (7017/7808)
Epoch: 86 | Batch_idx: 70 |  Loss: (0.2951) |  Loss2: (0.0000) | Acc: (89.00%) (8167/9088)
Epoch: 86 | Batch_idx: 80 |  Loss: (0.2940) |  Loss2: (0.0000) | Acc: (89.00%) (9310/10368)
Epoch: 86 | Batch_idx: 90 |  Loss: (0.2943) |  Loss2: (0.0000) | Acc: (89.00%) (10460/11648)
Epoch: 86 | Batch_idx: 100 |  Loss: (0.2905) |  Loss2: (0.0000) | Acc: (89.00%) (11627/12928)
Epoch: 86 | Batch_idx: 110 |  Loss: (0.2875) |  Loss2: (0.0000) | Acc: (90.00%) (12799/14208)
Epoch: 86 | Batch_idx: 120 |  Loss: (0.2876) |  Loss2: (0.0000) | Acc: (90.00%) (13959/15488)
Epoch: 86 | Batch_idx: 130 |  Loss: (0.2861) |  Loss2: (0.0000) | Acc: (90.00%) (15120/16768)
Epoch: 86 | Batch_idx: 140 |  Loss: (0.2860) |  Loss2: (0.0000) | Acc: (90.00%) (16276/18048)
Epoch: 86 | Batch_idx: 150 |  Loss: (0.2849) |  Loss2: (0.0000) | Acc: (90.00%) (17431/19328)
Epoch: 86 | Batch_idx: 160 |  Loss: (0.2870) |  Loss2: (0.0000) | Acc: (90.00%) (18562/20608)
Epoch: 86 | Batch_idx: 170 |  Loss: (0.2868) |  Loss2: (0.0000) | Acc: (90.00%) (19707/21888)
Epoch: 86 | Batch_idx: 180 |  Loss: (0.2877) |  Loss2: (0.0000) | Acc: (89.00%) (20844/23168)
Epoch: 86 | Batch_idx: 190 |  Loss: (0.2875) |  Loss2: (0.0000) | Acc: (89.00%) (21997/24448)
Epoch: 86 | Batch_idx: 200 |  Loss: (0.2875) |  Loss2: (0.0000) | Acc: (89.00%) (23147/25728)
Epoch: 86 | Batch_idx: 210 |  Loss: (0.2867) |  Loss2: (0.0000) | Acc: (90.00%) (24308/27008)
Epoch: 86 | Batch_idx: 220 |  Loss: (0.2858) |  Loss2: (0.0000) | Acc: (89.00%) (25458/28288)
Epoch: 86 | Batch_idx: 230 |  Loss: (0.2845) |  Loss2: (0.0000) | Acc: (90.00%) (26613/29568)
Epoch: 86 | Batch_idx: 240 |  Loss: (0.2834) |  Loss2: (0.0000) | Acc: (90.00%) (27767/30848)
Epoch: 86 | Batch_idx: 250 |  Loss: (0.2823) |  Loss2: (0.0000) | Acc: (90.00%) (28938/32128)
Epoch: 86 | Batch_idx: 260 |  Loss: (0.2819) |  Loss2: (0.0000) | Acc: (90.00%) (30089/33408)
Epoch: 86 | Batch_idx: 270 |  Loss: (0.2831) |  Loss2: (0.0000) | Acc: (90.00%) (31229/34688)
Epoch: 86 | Batch_idx: 280 |  Loss: (0.2828) |  Loss2: (0.0000) | Acc: (90.00%) (32380/35968)
Epoch: 86 | Batch_idx: 290 |  Loss: (0.2826) |  Loss2: (0.0000) | Acc: (90.00%) (33537/37248)
Epoch: 86 | Batch_idx: 300 |  Loss: (0.2821) |  Loss2: (0.0000) | Acc: (90.00%) (34693/38528)
Epoch: 86 | Batch_idx: 310 |  Loss: (0.2808) |  Loss2: (0.0000) | Acc: (90.00%) (35866/39808)
Epoch: 86 | Batch_idx: 320 |  Loss: (0.2798) |  Loss2: (0.0000) | Acc: (90.00%) (37035/41088)
Epoch: 86 | Batch_idx: 330 |  Loss: (0.2788) |  Loss2: (0.0000) | Acc: (90.00%) (38209/42368)
Epoch: 86 | Batch_idx: 340 |  Loss: (0.2787) |  Loss2: (0.0000) | Acc: (90.00%) (39363/43648)
Epoch: 86 | Batch_idx: 350 |  Loss: (0.2782) |  Loss2: (0.0000) | Acc: (90.00%) (40532/44928)
Epoch: 86 | Batch_idx: 360 |  Loss: (0.2777) |  Loss2: (0.0000) | Acc: (90.00%) (41690/46208)
Epoch: 86 | Batch_idx: 370 |  Loss: (0.2773) |  Loss2: (0.0000) | Acc: (90.00%) (42857/47488)
Epoch: 86 | Batch_idx: 380 |  Loss: (0.2768) |  Loss2: (0.0000) | Acc: (90.00%) (44024/48768)
Epoch: 86 | Batch_idx: 390 |  Loss: (0.2762) |  Loss2: (0.0000) | Acc: (90.00%) (45141/50000)
# TEST : Loss: (0.4075) | Acc: (86.00%) (8660/10000)
percent tensor([0.5444, 0.5470, 0.5543, 0.5286, 0.5559, 0.5638, 0.5549, 0.5375, 0.5379,
        0.5479, 0.5488, 0.5549, 0.5404, 0.5117, 0.5568, 0.5400],
       device='cuda:0') torch.Size([16])
percent tensor([0.5237, 0.5236, 0.5254, 0.5292, 0.5262, 0.5254, 0.5244, 0.5268, 0.5234,
        0.5228, 0.5225, 0.5220, 0.5221, 0.5267, 0.5240, 0.5257],
       device='cuda:0') torch.Size([16])
percent tensor([0.5675, 0.5547, 0.4492, 0.4404, 0.4248, 0.5379, 0.5268, 0.4553, 0.5020,
        0.5414, 0.5639, 0.4756, 0.5675, 0.5378, 0.5348, 0.5593],
       device='cuda:0') torch.Size([16])
percent tensor([0.6670, 0.5341, 0.6900, 0.7011, 0.6805, 0.7445, 0.6100, 0.6840, 0.6259,
        0.5786, 0.6064, 0.6144, 0.5367, 0.6595, 0.6173, 0.6977],
       device='cuda:0') torch.Size([16])
percent tensor([0.6327, 0.6396, 0.6066, 0.5689, 0.5763, 0.5899, 0.6379, 0.6067, 0.6382,
        0.6234, 0.6215, 0.5847, 0.6325, 0.6178, 0.6293, 0.6141],
       device='cuda:0') torch.Size([16])
percent tensor([0.5649, 0.6196, 0.6477, 0.6813, 0.6855, 0.6605, 0.6449, 0.6348, 0.6483,
        0.5742, 0.6238, 0.6895, 0.5731, 0.6640, 0.5995, 0.5965],
       device='cuda:0') torch.Size([16])
percent tensor([0.6103, 0.6240, 0.5952, 0.5355, 0.6423, 0.6942, 0.5829, 0.4812, 0.6251,
        0.6662, 0.6135, 0.4910, 0.6455, 0.5755, 0.4921, 0.6124],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9996, 0.9996, 0.9994, 0.9998, 0.9990, 0.9994, 0.9996, 0.9996,
        0.9995, 0.9997, 0.9997, 0.9997, 0.9994, 0.9993, 0.9995],
       device='cuda:0') torch.Size([16])
Epoch: 87 | Batch_idx: 0 |  Loss: (0.2427) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 87 | Batch_idx: 10 |  Loss: (0.2659) |  Loss2: (0.0000) | Acc: (90.00%) (1275/1408)
Epoch: 87 | Batch_idx: 20 |  Loss: (0.2641) |  Loss2: (0.0000) | Acc: (90.00%) (2440/2688)
Epoch: 87 | Batch_idx: 30 |  Loss: (0.2737) |  Loss2: (0.0000) | Acc: (90.00%) (3582/3968)
Epoch: 87 | Batch_idx: 40 |  Loss: (0.2610) |  Loss2: (0.0000) | Acc: (90.00%) (4762/5248)
Epoch: 87 | Batch_idx: 50 |  Loss: (0.2586) |  Loss2: (0.0000) | Acc: (90.00%) (5935/6528)
Epoch: 87 | Batch_idx: 60 |  Loss: (0.2535) |  Loss2: (0.0000) | Acc: (91.00%) (7122/7808)
Epoch: 87 | Batch_idx: 70 |  Loss: (0.2543) |  Loss2: (0.0000) | Acc: (91.00%) (8282/9088)
Epoch: 87 | Batch_idx: 80 |  Loss: (0.2539) |  Loss2: (0.0000) | Acc: (91.00%) (9453/10368)
Epoch: 87 | Batch_idx: 90 |  Loss: (0.2546) |  Loss2: (0.0000) | Acc: (91.00%) (10611/11648)
Epoch: 87 | Batch_idx: 100 |  Loss: (0.2543) |  Loss2: (0.0000) | Acc: (91.00%) (11773/12928)
Epoch: 87 | Batch_idx: 110 |  Loss: (0.2558) |  Loss2: (0.0000) | Acc: (91.00%) (12934/14208)
Epoch: 87 | Batch_idx: 120 |  Loss: (0.2554) |  Loss2: (0.0000) | Acc: (91.00%) (14105/15488)
Epoch: 87 | Batch_idx: 130 |  Loss: (0.2530) |  Loss2: (0.0000) | Acc: (91.00%) (15289/16768)
Epoch: 87 | Batch_idx: 140 |  Loss: (0.2524) |  Loss2: (0.0000) | Acc: (91.00%) (16463/18048)
Epoch: 87 | Batch_idx: 150 |  Loss: (0.2522) |  Loss2: (0.0000) | Acc: (91.00%) (17631/19328)
Epoch: 87 | Batch_idx: 160 |  Loss: (0.2528) |  Loss2: (0.0000) | Acc: (91.00%) (18789/20608)
Epoch: 87 | Batch_idx: 170 |  Loss: (0.2510) |  Loss2: (0.0000) | Acc: (91.00%) (19972/21888)
Epoch: 87 | Batch_idx: 180 |  Loss: (0.2509) |  Loss2: (0.0000) | Acc: (91.00%) (21149/23168)
Epoch: 87 | Batch_idx: 190 |  Loss: (0.2505) |  Loss2: (0.0000) | Acc: (91.00%) (22324/24448)
Epoch: 87 | Batch_idx: 200 |  Loss: (0.2506) |  Loss2: (0.0000) | Acc: (91.00%) (23491/25728)
Epoch: 87 | Batch_idx: 210 |  Loss: (0.2507) |  Loss2: (0.0000) | Acc: (91.00%) (24665/27008)
Epoch: 87 | Batch_idx: 220 |  Loss: (0.2502) |  Loss2: (0.0000) | Acc: (91.00%) (25841/28288)
Epoch: 87 | Batch_idx: 230 |  Loss: (0.2500) |  Loss2: (0.0000) | Acc: (91.00%) (27012/29568)
Epoch: 87 | Batch_idx: 240 |  Loss: (0.2506) |  Loss2: (0.0000) | Acc: (91.00%) (28173/30848)
Epoch: 87 | Batch_idx: 250 |  Loss: (0.2509) |  Loss2: (0.0000) | Acc: (91.00%) (29338/32128)
Epoch: 87 | Batch_idx: 260 |  Loss: (0.2515) |  Loss2: (0.0000) | Acc: (91.00%) (30501/33408)
Epoch: 87 | Batch_idx: 270 |  Loss: (0.2519) |  Loss2: (0.0000) | Acc: (91.00%) (31665/34688)
Epoch: 87 | Batch_idx: 280 |  Loss: (0.2511) |  Loss2: (0.0000) | Acc: (91.00%) (32846/35968)
Epoch: 87 | Batch_idx: 290 |  Loss: (0.2509) |  Loss2: (0.0000) | Acc: (91.00%) (34016/37248)
Epoch: 87 | Batch_idx: 300 |  Loss: (0.2513) |  Loss2: (0.0000) | Acc: (91.00%) (35184/38528)
Epoch: 87 | Batch_idx: 310 |  Loss: (0.2507) |  Loss2: (0.0000) | Acc: (91.00%) (36368/39808)
Epoch: 87 | Batch_idx: 320 |  Loss: (0.2506) |  Loss2: (0.0000) | Acc: (91.00%) (37540/41088)
Epoch: 87 | Batch_idx: 330 |  Loss: (0.2503) |  Loss2: (0.0000) | Acc: (91.00%) (38713/42368)
Epoch: 87 | Batch_idx: 340 |  Loss: (0.2501) |  Loss2: (0.0000) | Acc: (91.00%) (39879/43648)
Epoch: 87 | Batch_idx: 350 |  Loss: (0.2496) |  Loss2: (0.0000) | Acc: (91.00%) (41055/44928)
Epoch: 87 | Batch_idx: 360 |  Loss: (0.2503) |  Loss2: (0.0000) | Acc: (91.00%) (42216/46208)
Epoch: 87 | Batch_idx: 370 |  Loss: (0.2503) |  Loss2: (0.0000) | Acc: (91.00%) (43379/47488)
Epoch: 87 | Batch_idx: 380 |  Loss: (0.2500) |  Loss2: (0.0000) | Acc: (91.00%) (44553/48768)
Epoch: 87 | Batch_idx: 390 |  Loss: (0.2495) |  Loss2: (0.0000) | Acc: (91.00%) (45696/50000)
# TEST : Loss: (0.3944) | Acc: (87.00%) (8703/10000)
percent tensor([0.5459, 0.5476, 0.5571, 0.5310, 0.5585, 0.5644, 0.5561, 0.5405, 0.5399,
        0.5496, 0.5496, 0.5573, 0.5418, 0.5133, 0.5577, 0.5415],
       device='cuda:0') torch.Size([16])
percent tensor([0.5249, 0.5247, 0.5270, 0.5309, 0.5278, 0.5267, 0.5256, 0.5282, 0.5247,
        0.5240, 0.5237, 0.5233, 0.5232, 0.5280, 0.5251, 0.5270],
       device='cuda:0') torch.Size([16])
percent tensor([0.5738, 0.5608, 0.4583, 0.4442, 0.4348, 0.5368, 0.5379, 0.4678, 0.5087,
        0.5478, 0.5683, 0.4845, 0.5701, 0.5441, 0.5404, 0.5624],
       device='cuda:0') torch.Size([16])
percent tensor([0.6676, 0.5255, 0.6948, 0.7042, 0.6836, 0.7429, 0.6102, 0.6877, 0.6292,
        0.5712, 0.6017, 0.6164, 0.5288, 0.6622, 0.6130, 0.6924],
       device='cuda:0') torch.Size([16])
percent tensor([0.6346, 0.6362, 0.6081, 0.5653, 0.5776, 0.5988, 0.6367, 0.6096, 0.6324,
        0.6218, 0.6128, 0.5770, 0.6272, 0.6090, 0.6274, 0.6213],
       device='cuda:0') torch.Size([16])
percent tensor([0.5498, 0.6165, 0.6420, 0.6736, 0.6814, 0.6503, 0.6376, 0.6262, 0.6458,
        0.5665, 0.6159, 0.6851, 0.5674, 0.6550, 0.5880, 0.5797],
       device='cuda:0') torch.Size([16])
percent tensor([0.6151, 0.6373, 0.5956, 0.5362, 0.6465, 0.7004, 0.5917, 0.4806, 0.6273,
        0.6744, 0.6214, 0.4968, 0.6538, 0.5811, 0.4999, 0.6174],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9996, 0.9996, 0.9994, 0.9998, 0.9991, 0.9995, 0.9996, 0.9997,
        0.9995, 0.9997, 0.9998, 0.9997, 0.9995, 0.9993, 0.9995],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 88 | Batch_idx: 0 |  Loss: (0.2102) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 88 | Batch_idx: 10 |  Loss: (0.2455) |  Loss2: (0.0000) | Acc: (91.00%) (1291/1408)
Epoch: 88 | Batch_idx: 20 |  Loss: (0.2436) |  Loss2: (0.0000) | Acc: (92.00%) (2477/2688)
Epoch: 88 | Batch_idx: 30 |  Loss: (0.2432) |  Loss2: (0.0000) | Acc: (92.00%) (3655/3968)
Epoch: 88 | Batch_idx: 40 |  Loss: (0.2472) |  Loss2: (0.0000) | Acc: (92.00%) (4837/5248)
Epoch: 88 | Batch_idx: 50 |  Loss: (0.2428) |  Loss2: (0.0000) | Acc: (92.00%) (6030/6528)
Epoch: 88 | Batch_idx: 60 |  Loss: (0.2392) |  Loss2: (0.0000) | Acc: (92.00%) (7211/7808)
Epoch: 88 | Batch_idx: 70 |  Loss: (0.2415) |  Loss2: (0.0000) | Acc: (92.00%) (8378/9088)
Epoch: 88 | Batch_idx: 80 |  Loss: (0.2417) |  Loss2: (0.0000) | Acc: (92.00%) (9547/10368)
Epoch: 88 | Batch_idx: 90 |  Loss: (0.2398) |  Loss2: (0.0000) | Acc: (92.00%) (10725/11648)
Epoch: 88 | Batch_idx: 100 |  Loss: (0.2379) |  Loss2: (0.0000) | Acc: (92.00%) (11901/12928)
Epoch: 88 | Batch_idx: 110 |  Loss: (0.2409) |  Loss2: (0.0000) | Acc: (91.00%) (13058/14208)
Epoch: 88 | Batch_idx: 120 |  Loss: (0.2407) |  Loss2: (0.0000) | Acc: (91.00%) (14230/15488)
Epoch: 88 | Batch_idx: 130 |  Loss: (0.2419) |  Loss2: (0.0000) | Acc: (91.00%) (15393/16768)
Epoch: 88 | Batch_idx: 140 |  Loss: (0.2416) |  Loss2: (0.0000) | Acc: (91.00%) (16569/18048)
Epoch: 88 | Batch_idx: 150 |  Loss: (0.2421) |  Loss2: (0.0000) | Acc: (91.00%) (17737/19328)
Epoch: 88 | Batch_idx: 160 |  Loss: (0.2443) |  Loss2: (0.0000) | Acc: (91.00%) (18889/20608)
Epoch: 88 | Batch_idx: 170 |  Loss: (0.2434) |  Loss2: (0.0000) | Acc: (91.00%) (20068/21888)
Epoch: 88 | Batch_idx: 180 |  Loss: (0.2455) |  Loss2: (0.0000) | Acc: (91.00%) (21214/23168)
Epoch: 88 | Batch_idx: 190 |  Loss: (0.2461) |  Loss2: (0.0000) | Acc: (91.00%) (22385/24448)
Epoch: 88 | Batch_idx: 200 |  Loss: (0.2457) |  Loss2: (0.0000) | Acc: (91.00%) (23565/25728)
Epoch: 88 | Batch_idx: 210 |  Loss: (0.2468) |  Loss2: (0.0000) | Acc: (91.00%) (24723/27008)
Epoch: 88 | Batch_idx: 220 |  Loss: (0.2459) |  Loss2: (0.0000) | Acc: (91.00%) (25906/28288)
Epoch: 88 | Batch_idx: 230 |  Loss: (0.2458) |  Loss2: (0.0000) | Acc: (91.00%) (27081/29568)
Epoch: 88 | Batch_idx: 240 |  Loss: (0.2458) |  Loss2: (0.0000) | Acc: (91.00%) (28252/30848)
Epoch: 88 | Batch_idx: 250 |  Loss: (0.2466) |  Loss2: (0.0000) | Acc: (91.00%) (29419/32128)
Epoch: 88 | Batch_idx: 260 |  Loss: (0.2459) |  Loss2: (0.0000) | Acc: (91.00%) (30601/33408)
Epoch: 88 | Batch_idx: 270 |  Loss: (0.2464) |  Loss2: (0.0000) | Acc: (91.00%) (31775/34688)
Epoch: 88 | Batch_idx: 280 |  Loss: (0.2468) |  Loss2: (0.0000) | Acc: (91.00%) (32932/35968)
Epoch: 88 | Batch_idx: 290 |  Loss: (0.2475) |  Loss2: (0.0000) | Acc: (91.00%) (34091/37248)
Epoch: 88 | Batch_idx: 300 |  Loss: (0.2485) |  Loss2: (0.0000) | Acc: (91.00%) (35247/38528)
Epoch: 88 | Batch_idx: 310 |  Loss: (0.2486) |  Loss2: (0.0000) | Acc: (91.00%) (36406/39808)
Epoch: 88 | Batch_idx: 320 |  Loss: (0.2484) |  Loss2: (0.0000) | Acc: (91.00%) (37575/41088)
Epoch: 88 | Batch_idx: 330 |  Loss: (0.2483) |  Loss2: (0.0000) | Acc: (91.00%) (38752/42368)
Epoch: 88 | Batch_idx: 340 |  Loss: (0.2494) |  Loss2: (0.0000) | Acc: (91.00%) (39907/43648)
Epoch: 88 | Batch_idx: 350 |  Loss: (0.2489) |  Loss2: (0.0000) | Acc: (91.00%) (41082/44928)
Epoch: 88 | Batch_idx: 360 |  Loss: (0.2492) |  Loss2: (0.0000) | Acc: (91.00%) (42247/46208)
Epoch: 88 | Batch_idx: 370 |  Loss: (0.2490) |  Loss2: (0.0000) | Acc: (91.00%) (43406/47488)
Epoch: 88 | Batch_idx: 380 |  Loss: (0.2489) |  Loss2: (0.0000) | Acc: (91.00%) (44569/48768)
Epoch: 88 | Batch_idx: 390 |  Loss: (0.2488) |  Loss2: (0.0000) | Acc: (91.00%) (45696/50000)
# TEST : Loss: (0.4648) | Acc: (84.00%) (8499/10000)
percent tensor([0.5446, 0.5481, 0.5588, 0.5316, 0.5595, 0.5629, 0.5574, 0.5407, 0.5396,
        0.5498, 0.5480, 0.5587, 0.5405, 0.5147, 0.5569, 0.5404],
       device='cuda:0') torch.Size([16])
percent tensor([0.5262, 0.5244, 0.5282, 0.5314, 0.5292, 0.5274, 0.5261, 0.5289, 0.5255,
        0.5245, 0.5245, 0.5245, 0.5241, 0.5266, 0.5253, 0.5276],
       device='cuda:0') torch.Size([16])
percent tensor([0.5669, 0.5603, 0.4686, 0.4399, 0.4309, 0.5270, 0.5385, 0.4763, 0.4974,
        0.5485, 0.5612, 0.4907, 0.5622, 0.5400, 0.5366, 0.5586],
       device='cuda:0') torch.Size([16])
percent tensor([0.6663, 0.5408, 0.6849, 0.6929, 0.6787, 0.7481, 0.6129, 0.6809, 0.6384,
        0.5692, 0.6082, 0.6059, 0.5235, 0.6669, 0.6155, 0.6917],
       device='cuda:0') torch.Size([16])
percent tensor([0.6326, 0.6297, 0.6180, 0.5730, 0.5771, 0.5840, 0.6336, 0.6149, 0.6214,
        0.6176, 0.6067, 0.5838, 0.6225, 0.6042, 0.6164, 0.6229],
       device='cuda:0') torch.Size([16])
percent tensor([0.5568, 0.6321, 0.6341, 0.6781, 0.6879, 0.6575, 0.6491, 0.6331, 0.6641,
        0.5654, 0.6231, 0.6864, 0.5886, 0.6762, 0.6034, 0.5860],
       device='cuda:0') torch.Size([16])
percent tensor([0.6046, 0.6436, 0.5885, 0.5558, 0.6495, 0.6909, 0.5749, 0.4618, 0.6098,
        0.6415, 0.6198, 0.4943, 0.6365, 0.5891, 0.4750, 0.6207],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9996, 0.9996, 0.9995, 0.9997, 0.9987, 0.9997, 0.9996, 0.9996,
        0.9995, 0.9995, 0.9998, 0.9997, 0.9991, 0.9993, 0.9997],
       device='cuda:0') torch.Size([16])
Epoch: 89 | Batch_idx: 0 |  Loss: (0.2207) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 89 | Batch_idx: 10 |  Loss: (0.2087) |  Loss2: (0.0000) | Acc: (92.00%) (1301/1408)
Epoch: 89 | Batch_idx: 20 |  Loss: (0.2253) |  Loss2: (0.0000) | Acc: (91.00%) (2470/2688)
Epoch: 89 | Batch_idx: 30 |  Loss: (0.2297) |  Loss2: (0.0000) | Acc: (91.00%) (3643/3968)
Epoch: 89 | Batch_idx: 40 |  Loss: (0.2335) |  Loss2: (0.0000) | Acc: (91.00%) (4821/5248)
Epoch: 89 | Batch_idx: 50 |  Loss: (0.2293) |  Loss2: (0.0000) | Acc: (91.00%) (6001/6528)
Epoch: 89 | Batch_idx: 60 |  Loss: (0.2297) |  Loss2: (0.0000) | Acc: (91.00%) (7180/7808)
Epoch: 89 | Batch_idx: 70 |  Loss: (0.2325) |  Loss2: (0.0000) | Acc: (91.00%) (8349/9088)
Epoch: 89 | Batch_idx: 80 |  Loss: (0.2316) |  Loss2: (0.0000) | Acc: (91.00%) (9538/10368)
Epoch: 89 | Batch_idx: 90 |  Loss: (0.2308) |  Loss2: (0.0000) | Acc: (92.00%) (10721/11648)
Epoch: 89 | Batch_idx: 100 |  Loss: (0.2313) |  Loss2: (0.0000) | Acc: (92.00%) (11897/12928)
Epoch: 89 | Batch_idx: 110 |  Loss: (0.2308) |  Loss2: (0.0000) | Acc: (92.00%) (13077/14208)
Epoch: 89 | Batch_idx: 120 |  Loss: (0.2320) |  Loss2: (0.0000) | Acc: (91.00%) (14238/15488)
Epoch: 89 | Batch_idx: 130 |  Loss: (0.2320) |  Loss2: (0.0000) | Acc: (91.00%) (15418/16768)
Epoch: 89 | Batch_idx: 140 |  Loss: (0.2340) |  Loss2: (0.0000) | Acc: (91.00%) (16579/18048)
Epoch: 89 | Batch_idx: 150 |  Loss: (0.2349) |  Loss2: (0.0000) | Acc: (91.00%) (17752/19328)
Epoch: 89 | Batch_idx: 160 |  Loss: (0.2344) |  Loss2: (0.0000) | Acc: (91.00%) (18938/20608)
Epoch: 89 | Batch_idx: 170 |  Loss: (0.2354) |  Loss2: (0.0000) | Acc: (91.00%) (20115/21888)
Epoch: 89 | Batch_idx: 180 |  Loss: (0.2372) |  Loss2: (0.0000) | Acc: (91.00%) (21270/23168)
Epoch: 89 | Batch_idx: 190 |  Loss: (0.2381) |  Loss2: (0.0000) | Acc: (91.00%) (22430/24448)
Epoch: 89 | Batch_idx: 200 |  Loss: (0.2380) |  Loss2: (0.0000) | Acc: (91.00%) (23601/25728)
Epoch: 89 | Batch_idx: 210 |  Loss: (0.2394) |  Loss2: (0.0000) | Acc: (91.00%) (24774/27008)
Epoch: 89 | Batch_idx: 220 |  Loss: (0.2382) |  Loss2: (0.0000) | Acc: (91.00%) (25968/28288)
Epoch: 89 | Batch_idx: 230 |  Loss: (0.2374) |  Loss2: (0.0000) | Acc: (91.00%) (27152/29568)
Epoch: 89 | Batch_idx: 240 |  Loss: (0.2380) |  Loss2: (0.0000) | Acc: (91.00%) (28325/30848)
Epoch: 89 | Batch_idx: 250 |  Loss: (0.2387) |  Loss2: (0.0000) | Acc: (91.00%) (29491/32128)
Epoch: 89 | Batch_idx: 260 |  Loss: (0.2391) |  Loss2: (0.0000) | Acc: (91.00%) (30654/33408)
Epoch: 89 | Batch_idx: 270 |  Loss: (0.2384) |  Loss2: (0.0000) | Acc: (91.00%) (31841/34688)
Epoch: 89 | Batch_idx: 280 |  Loss: (0.2388) |  Loss2: (0.0000) | Acc: (91.00%) (33012/35968)
Epoch: 89 | Batch_idx: 290 |  Loss: (0.2401) |  Loss2: (0.0000) | Acc: (91.00%) (34164/37248)
Epoch: 89 | Batch_idx: 300 |  Loss: (0.2401) |  Loss2: (0.0000) | Acc: (91.00%) (35342/38528)
Epoch: 89 | Batch_idx: 310 |  Loss: (0.2403) |  Loss2: (0.0000) | Acc: (91.00%) (36516/39808)
Epoch: 89 | Batch_idx: 320 |  Loss: (0.2407) |  Loss2: (0.0000) | Acc: (91.00%) (37685/41088)
Epoch: 89 | Batch_idx: 330 |  Loss: (0.2411) |  Loss2: (0.0000) | Acc: (91.00%) (38859/42368)
Epoch: 89 | Batch_idx: 340 |  Loss: (0.2414) |  Loss2: (0.0000) | Acc: (91.00%) (40029/43648)
Epoch: 89 | Batch_idx: 350 |  Loss: (0.2417) |  Loss2: (0.0000) | Acc: (91.00%) (41192/44928)
Epoch: 89 | Batch_idx: 360 |  Loss: (0.2415) |  Loss2: (0.0000) | Acc: (91.00%) (42376/46208)
Epoch: 89 | Batch_idx: 370 |  Loss: (0.2419) |  Loss2: (0.0000) | Acc: (91.00%) (43544/47488)
Epoch: 89 | Batch_idx: 380 |  Loss: (0.2423) |  Loss2: (0.0000) | Acc: (91.00%) (44702/48768)
Epoch: 89 | Batch_idx: 390 |  Loss: (0.2433) |  Loss2: (0.0000) | Acc: (91.00%) (45811/50000)
# TEST : Loss: (0.4721) | Acc: (84.00%) (8472/10000)
percent tensor([0.5439, 0.5505, 0.5516, 0.5301, 0.5542, 0.5634, 0.5568, 0.5392, 0.5390,
        0.5491, 0.5496, 0.5537, 0.5410, 0.5187, 0.5590, 0.5414],
       device='cuda:0') torch.Size([16])
percent tensor([0.5251, 0.5246, 0.5272, 0.5307, 0.5282, 0.5265, 0.5256, 0.5296, 0.5255,
        0.5240, 0.5239, 0.5237, 0.5235, 0.5273, 0.5250, 0.5275],
       device='cuda:0') torch.Size([16])
percent tensor([0.5754, 0.5731, 0.4620, 0.4354, 0.4427, 0.5403, 0.5522, 0.4761, 0.5117,
        0.5608, 0.5809, 0.5009, 0.5807, 0.5441, 0.5506, 0.5691],
       device='cuda:0') torch.Size([16])
percent tensor([0.6639, 0.5307, 0.6928, 0.6937, 0.6909, 0.7488, 0.6059, 0.6930, 0.6299,
        0.5584, 0.5981, 0.6100, 0.5134, 0.6552, 0.6161, 0.6941],
       device='cuda:0') torch.Size([16])
percent tensor([0.6417, 0.6538, 0.6168, 0.5772, 0.5857, 0.6038, 0.6519, 0.6251, 0.6355,
        0.6341, 0.6222, 0.5961, 0.6378, 0.6317, 0.6366, 0.6320],
       device='cuda:0') torch.Size([16])
percent tensor([0.5552, 0.6151, 0.6514, 0.6686, 0.6932, 0.6672, 0.6264, 0.6365, 0.6483,
        0.5632, 0.6106, 0.6829, 0.5788, 0.6661, 0.5903, 0.5836],
       device='cuda:0') torch.Size([16])
percent tensor([0.5993, 0.6436, 0.5617, 0.5272, 0.6206, 0.6950, 0.5673, 0.4369, 0.6154,
        0.6467, 0.6268, 0.4899, 0.6486, 0.5921, 0.4798, 0.5957],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9995, 0.9997, 0.9996, 0.9999, 0.9990, 0.9994, 0.9997, 0.9998,
        0.9996, 0.9997, 0.9998, 0.9996, 0.9995, 0.9994, 0.9994],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 90 | Batch_idx: 0 |  Loss: (0.1518) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 90 | Batch_idx: 10 |  Loss: (0.2255) |  Loss2: (0.0000) | Acc: (92.00%) (1300/1408)
Epoch: 90 | Batch_idx: 20 |  Loss: (0.2447) |  Loss2: (0.0000) | Acc: (91.00%) (2463/2688)
Epoch: 90 | Batch_idx: 30 |  Loss: (0.2609) |  Loss2: (0.0000) | Acc: (91.00%) (3619/3968)
Epoch: 90 | Batch_idx: 40 |  Loss: (0.2665) |  Loss2: (0.0000) | Acc: (90.00%) (4765/5248)
Epoch: 90 | Batch_idx: 50 |  Loss: (0.2707) |  Loss2: (0.0000) | Acc: (90.00%) (5904/6528)
Epoch: 90 | Batch_idx: 60 |  Loss: (0.2723) |  Loss2: (0.0000) | Acc: (90.00%) (7056/7808)
Epoch: 90 | Batch_idx: 70 |  Loss: (0.2717) |  Loss2: (0.0000) | Acc: (90.00%) (8202/9088)
Epoch: 90 | Batch_idx: 80 |  Loss: (0.2715) |  Loss2: (0.0000) | Acc: (90.00%) (9359/10368)
Epoch: 90 | Batch_idx: 90 |  Loss: (0.2729) |  Loss2: (0.0000) | Acc: (90.00%) (10502/11648)
Epoch: 90 | Batch_idx: 100 |  Loss: (0.2748) |  Loss2: (0.0000) | Acc: (90.00%) (11645/12928)
Epoch: 90 | Batch_idx: 110 |  Loss: (0.2733) |  Loss2: (0.0000) | Acc: (90.00%) (12803/14208)
Epoch: 90 | Batch_idx: 120 |  Loss: (0.2753) |  Loss2: (0.0000) | Acc: (90.00%) (13943/15488)
Epoch: 90 | Batch_idx: 130 |  Loss: (0.2744) |  Loss2: (0.0000) | Acc: (90.00%) (15096/16768)
Epoch: 90 | Batch_idx: 140 |  Loss: (0.2740) |  Loss2: (0.0000) | Acc: (90.00%) (16267/18048)
Epoch: 90 | Batch_idx: 150 |  Loss: (0.2737) |  Loss2: (0.0000) | Acc: (90.00%) (17433/19328)
Epoch: 90 | Batch_idx: 160 |  Loss: (0.2735) |  Loss2: (0.0000) | Acc: (90.00%) (18590/20608)
Epoch: 90 | Batch_idx: 170 |  Loss: (0.2728) |  Loss2: (0.0000) | Acc: (90.00%) (19748/21888)
Epoch: 90 | Batch_idx: 180 |  Loss: (0.2718) |  Loss2: (0.0000) | Acc: (90.00%) (20917/23168)
Epoch: 90 | Batch_idx: 190 |  Loss: (0.2743) |  Loss2: (0.0000) | Acc: (90.00%) (22059/24448)
Epoch: 90 | Batch_idx: 200 |  Loss: (0.2734) |  Loss2: (0.0000) | Acc: (90.00%) (23232/25728)
Epoch: 90 | Batch_idx: 210 |  Loss: (0.2746) |  Loss2: (0.0000) | Acc: (90.00%) (24388/27008)
Epoch: 90 | Batch_idx: 220 |  Loss: (0.2758) |  Loss2: (0.0000) | Acc: (90.00%) (25546/28288)
Epoch: 90 | Batch_idx: 230 |  Loss: (0.2765) |  Loss2: (0.0000) | Acc: (90.00%) (26694/29568)
Epoch: 90 | Batch_idx: 240 |  Loss: (0.2757) |  Loss2: (0.0000) | Acc: (90.00%) (27865/30848)
Epoch: 90 | Batch_idx: 250 |  Loss: (0.2759) |  Loss2: (0.0000) | Acc: (90.00%) (29015/32128)
Epoch: 90 | Batch_idx: 260 |  Loss: (0.2769) |  Loss2: (0.0000) | Acc: (90.00%) (30153/33408)
Epoch: 90 | Batch_idx: 270 |  Loss: (0.2768) |  Loss2: (0.0000) | Acc: (90.00%) (31302/34688)
Epoch: 90 | Batch_idx: 280 |  Loss: (0.2758) |  Loss2: (0.0000) | Acc: (90.00%) (32462/35968)
Epoch: 90 | Batch_idx: 290 |  Loss: (0.2744) |  Loss2: (0.0000) | Acc: (90.00%) (33635/37248)
Epoch: 90 | Batch_idx: 300 |  Loss: (0.2747) |  Loss2: (0.0000) | Acc: (90.00%) (34788/38528)
Epoch: 90 | Batch_idx: 310 |  Loss: (0.2748) |  Loss2: (0.0000) | Acc: (90.00%) (35930/39808)
Epoch: 90 | Batch_idx: 320 |  Loss: (0.2742) |  Loss2: (0.0000) | Acc: (90.00%) (37110/41088)
Epoch: 90 | Batch_idx: 330 |  Loss: (0.2750) |  Loss2: (0.0000) | Acc: (90.00%) (38249/42368)
Epoch: 90 | Batch_idx: 340 |  Loss: (0.2745) |  Loss2: (0.0000) | Acc: (90.00%) (39410/43648)
Epoch: 90 | Batch_idx: 350 |  Loss: (0.2734) |  Loss2: (0.0000) | Acc: (90.00%) (40580/44928)
Epoch: 90 | Batch_idx: 360 |  Loss: (0.2727) |  Loss2: (0.0000) | Acc: (90.00%) (41752/46208)
Epoch: 90 | Batch_idx: 370 |  Loss: (0.2722) |  Loss2: (0.0000) | Acc: (90.00%) (42922/47488)
Epoch: 90 | Batch_idx: 380 |  Loss: (0.2721) |  Loss2: (0.0000) | Acc: (90.00%) (44073/48768)
Epoch: 90 | Batch_idx: 390 |  Loss: (0.2718) |  Loss2: (0.0000) | Acc: (90.00%) (45198/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_090.pth.tar'
# TEST : Loss: (0.4144) | Acc: (86.00%) (8629/10000)
percent tensor([0.5460, 0.5523, 0.5583, 0.5326, 0.5600, 0.5666, 0.5601, 0.5431, 0.5417,
        0.5521, 0.5515, 0.5596, 0.5422, 0.5172, 0.5619, 0.5426],
       device='cuda:0') torch.Size([16])
percent tensor([0.5314, 0.5308, 0.5351, 0.5386, 0.5359, 0.5326, 0.5326, 0.5373, 0.5321,
        0.5306, 0.5300, 0.5309, 0.5299, 0.5338, 0.5311, 0.5340],
       device='cuda:0') torch.Size([16])
percent tensor([0.5771, 0.5849, 0.4391, 0.4189, 0.4280, 0.5404, 0.5556, 0.4693, 0.5167,
        0.5608, 0.5881, 0.4841, 0.5801, 0.5560, 0.5603, 0.5717],
       device='cuda:0') torch.Size([16])
percent tensor([0.7076, 0.5784, 0.7348, 0.7357, 0.7396, 0.7624, 0.6607, 0.7373, 0.6676,
        0.6169, 0.6429, 0.6775, 0.5741, 0.6911, 0.6662, 0.7320],
       device='cuda:0') torch.Size([16])
percent tensor([0.5945, 0.6108, 0.5819, 0.5429, 0.5555, 0.5583, 0.6074, 0.5913, 0.5994,
        0.5853, 0.5735, 0.5566, 0.5917, 0.5928, 0.5950, 0.5745],
       device='cuda:0') torch.Size([16])
percent tensor([0.6027, 0.6433, 0.7011, 0.7145, 0.7307, 0.7058, 0.6635, 0.6815, 0.6789,
        0.5991, 0.6488, 0.7250, 0.6187, 0.7012, 0.6241, 0.6243],
       device='cuda:0') torch.Size([16])
percent tensor([0.6087, 0.6562, 0.5849, 0.5238, 0.6207, 0.6810, 0.5742, 0.4478, 0.6282,
        0.6630, 0.6405, 0.5225, 0.6622, 0.5952, 0.4849, 0.5912],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9995, 0.9995, 0.9995, 0.9998, 0.9989, 0.9994, 0.9996, 0.9998,
        0.9996, 0.9997, 0.9998, 0.9997, 0.9994, 0.9993, 0.9996],
       device='cuda:0') torch.Size([16])
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(179.1518, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(812.9104, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(805.9177, device='cuda:0')
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1521.3094, device='cuda:0')
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(494.3682, device='cuda:0')
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2236.0276, device='cuda:0')
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4265.1685, device='cuda:0')
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1403.4409, device='cuda:0')
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6149.2593, device='cuda:0')
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11906.5439, device='cuda:0')
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3953.9055, device='cuda:0')
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16707.9023, device='cuda:0')
Epoch: 91 | Batch_idx: 0 |  Loss: (0.2306) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 91 | Batch_idx: 10 |  Loss: (0.3002) |  Loss2: (0.0000) | Acc: (88.00%) (1244/1408)
Epoch: 91 | Batch_idx: 20 |  Loss: (0.2956) |  Loss2: (0.0000) | Acc: (89.00%) (2403/2688)
Epoch: 91 | Batch_idx: 30 |  Loss: (0.2738) |  Loss2: (0.0000) | Acc: (90.00%) (3578/3968)
Epoch: 91 | Batch_idx: 40 |  Loss: (0.2711) |  Loss2: (0.0000) | Acc: (90.00%) (4747/5248)
Epoch: 91 | Batch_idx: 50 |  Loss: (0.2624) |  Loss2: (0.0000) | Acc: (90.00%) (5919/6528)
Epoch: 91 | Batch_idx: 60 |  Loss: (0.2580) |  Loss2: (0.0000) | Acc: (90.00%) (7090/7808)
Epoch: 91 | Batch_idx: 70 |  Loss: (0.2560) |  Loss2: (0.0000) | Acc: (90.00%) (8259/9088)
Epoch: 91 | Batch_idx: 80 |  Loss: (0.2574) |  Loss2: (0.0000) | Acc: (90.00%) (9420/10368)
Epoch: 91 | Batch_idx: 90 |  Loss: (0.2538) |  Loss2: (0.0000) | Acc: (91.00%) (10606/11648)
Epoch: 91 | Batch_idx: 100 |  Loss: (0.2558) |  Loss2: (0.0000) | Acc: (91.00%) (11766/12928)
Epoch: 91 | Batch_idx: 110 |  Loss: (0.2544) |  Loss2: (0.0000) | Acc: (91.00%) (12944/14208)
Epoch: 91 | Batch_idx: 120 |  Loss: (0.2558) |  Loss2: (0.0000) | Acc: (91.00%) (14111/15488)
Epoch: 91 | Batch_idx: 130 |  Loss: (0.2547) |  Loss2: (0.0000) | Acc: (91.00%) (15288/16768)
Epoch: 91 | Batch_idx: 140 |  Loss: (0.2549) |  Loss2: (0.0000) | Acc: (91.00%) (16460/18048)
Epoch: 91 | Batch_idx: 150 |  Loss: (0.2548) |  Loss2: (0.0000) | Acc: (91.00%) (17634/19328)
Epoch: 91 | Batch_idx: 160 |  Loss: (0.2560) |  Loss2: (0.0000) | Acc: (91.00%) (18791/20608)
Epoch: 91 | Batch_idx: 170 |  Loss: (0.2549) |  Loss2: (0.0000) | Acc: (91.00%) (19968/21888)
Epoch: 91 | Batch_idx: 180 |  Loss: (0.2540) |  Loss2: (0.0000) | Acc: (91.00%) (21147/23168)
Epoch: 91 | Batch_idx: 190 |  Loss: (0.2539) |  Loss2: (0.0000) | Acc: (91.00%) (22300/24448)
Epoch: 91 | Batch_idx: 200 |  Loss: (0.2542) |  Loss2: (0.0000) | Acc: (91.00%) (23448/25728)
Epoch: 91 | Batch_idx: 210 |  Loss: (0.2548) |  Loss2: (0.0000) | Acc: (91.00%) (24603/27008)
Epoch: 91 | Batch_idx: 220 |  Loss: (0.2554) |  Loss2: (0.0000) | Acc: (91.00%) (25766/28288)
Epoch: 91 | Batch_idx: 230 |  Loss: (0.2545) |  Loss2: (0.0000) | Acc: (91.00%) (26933/29568)
Epoch: 91 | Batch_idx: 240 |  Loss: (0.2538) |  Loss2: (0.0000) | Acc: (91.00%) (28100/30848)
Epoch: 91 | Batch_idx: 250 |  Loss: (0.2538) |  Loss2: (0.0000) | Acc: (91.00%) (29278/32128)
Epoch: 91 | Batch_idx: 260 |  Loss: (0.2539) |  Loss2: (0.0000) | Acc: (91.00%) (30440/33408)
Epoch: 91 | Batch_idx: 270 |  Loss: (0.2538) |  Loss2: (0.0000) | Acc: (91.00%) (31615/34688)
Epoch: 91 | Batch_idx: 280 |  Loss: (0.2538) |  Loss2: (0.0000) | Acc: (91.00%) (32786/35968)
Epoch: 91 | Batch_idx: 290 |  Loss: (0.2530) |  Loss2: (0.0000) | Acc: (91.00%) (33967/37248)
Epoch: 91 | Batch_idx: 300 |  Loss: (0.2520) |  Loss2: (0.0000) | Acc: (91.00%) (35152/38528)
Epoch: 91 | Batch_idx: 310 |  Loss: (0.2526) |  Loss2: (0.0000) | Acc: (91.00%) (36316/39808)
Epoch: 91 | Batch_idx: 320 |  Loss: (0.2514) |  Loss2: (0.0000) | Acc: (91.00%) (37501/41088)
Epoch: 91 | Batch_idx: 330 |  Loss: (0.2513) |  Loss2: (0.0000) | Acc: (91.00%) (38682/42368)
Epoch: 91 | Batch_idx: 340 |  Loss: (0.2506) |  Loss2: (0.0000) | Acc: (91.00%) (39861/43648)
Epoch: 91 | Batch_idx: 350 |  Loss: (0.2502) |  Loss2: (0.0000) | Acc: (91.00%) (41034/44928)
Epoch: 91 | Batch_idx: 360 |  Loss: (0.2498) |  Loss2: (0.0000) | Acc: (91.00%) (42221/46208)
Epoch: 91 | Batch_idx: 370 |  Loss: (0.2503) |  Loss2: (0.0000) | Acc: (91.00%) (43379/47488)
Epoch: 91 | Batch_idx: 380 |  Loss: (0.2497) |  Loss2: (0.0000) | Acc: (91.00%) (44558/48768)
Epoch: 91 | Batch_idx: 390 |  Loss: (0.2505) |  Loss2: (0.0000) | Acc: (91.00%) (45670/50000)
# TEST : Loss: (0.4024) | Acc: (86.00%) (8683/10000)
percent tensor([0.5466, 0.5545, 0.5604, 0.5343, 0.5621, 0.5678, 0.5622, 0.5453, 0.5423,
        0.5540, 0.5527, 0.5620, 0.5430, 0.5180, 0.5640, 0.5436],
       device='cuda:0') torch.Size([16])
percent tensor([0.5325, 0.5320, 0.5368, 0.5399, 0.5377, 0.5336, 0.5340, 0.5393, 0.5336,
        0.5319, 0.5311, 0.5326, 0.5310, 0.5351, 0.5323, 0.5350],
       device='cuda:0') torch.Size([16])
percent tensor([0.5790, 0.5846, 0.4404, 0.4218, 0.4291, 0.5500, 0.5560, 0.4700, 0.5141,
        0.5603, 0.5878, 0.4831, 0.5776, 0.5567, 0.5659, 0.5761],
       device='cuda:0') torch.Size([16])
percent tensor([0.6952, 0.5689, 0.7273, 0.7266, 0.7304, 0.7551, 0.6529, 0.7301, 0.6569,
        0.6037, 0.6295, 0.6691, 0.5575, 0.6855, 0.6594, 0.7194],
       device='cuda:0') torch.Size([16])
percent tensor([0.5890, 0.5992, 0.5856, 0.5534, 0.5574, 0.5586, 0.5992, 0.5913, 0.5967,
        0.5728, 0.5657, 0.5584, 0.5832, 0.5860, 0.5894, 0.5671],
       device='cuda:0') torch.Size([16])
percent tensor([0.5919, 0.6368, 0.6869, 0.7000, 0.7210, 0.7050, 0.6530, 0.6644, 0.6711,
        0.5900, 0.6420, 0.7126, 0.6124, 0.6954, 0.6108, 0.6140],
       device='cuda:0') torch.Size([16])
percent tensor([0.6081, 0.6633, 0.5920, 0.5345, 0.6296, 0.6842, 0.5777, 0.4504, 0.6396,
        0.6634, 0.6493, 0.5279, 0.6663, 0.6083, 0.4843, 0.5858],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9995, 0.9996, 0.9996, 0.9998, 0.9991, 0.9995, 0.9997, 0.9998,
        0.9996, 0.9998, 0.9998, 0.9997, 0.9995, 0.9993, 0.9996],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 92 | Batch_idx: 0 |  Loss: (0.2074) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 92 | Batch_idx: 10 |  Loss: (0.2276) |  Loss2: (0.0000) | Acc: (92.00%) (1300/1408)
Epoch: 92 | Batch_idx: 20 |  Loss: (0.2366) |  Loss2: (0.0000) | Acc: (91.00%) (2467/2688)
Epoch: 92 | Batch_idx: 30 |  Loss: (0.2314) |  Loss2: (0.0000) | Acc: (92.00%) (3654/3968)
Epoch: 92 | Batch_idx: 40 |  Loss: (0.2258) |  Loss2: (0.0000) | Acc: (92.00%) (4840/5248)
Epoch: 92 | Batch_idx: 50 |  Loss: (0.2332) |  Loss2: (0.0000) | Acc: (92.00%) (6016/6528)
Epoch: 92 | Batch_idx: 60 |  Loss: (0.2320) |  Loss2: (0.0000) | Acc: (92.00%) (7202/7808)
Epoch: 92 | Batch_idx: 70 |  Loss: (0.2280) |  Loss2: (0.0000) | Acc: (92.00%) (8398/9088)
Epoch: 92 | Batch_idx: 80 |  Loss: (0.2264) |  Loss2: (0.0000) | Acc: (92.00%) (9580/10368)
Epoch: 92 | Batch_idx: 90 |  Loss: (0.2295) |  Loss2: (0.0000) | Acc: (92.00%) (10739/11648)
Epoch: 92 | Batch_idx: 100 |  Loss: (0.2284) |  Loss2: (0.0000) | Acc: (92.00%) (11923/12928)
Epoch: 92 | Batch_idx: 110 |  Loss: (0.2310) |  Loss2: (0.0000) | Acc: (92.00%) (13098/14208)
Epoch: 92 | Batch_idx: 120 |  Loss: (0.2302) |  Loss2: (0.0000) | Acc: (92.00%) (14284/15488)
Epoch: 92 | Batch_idx: 130 |  Loss: (0.2316) |  Loss2: (0.0000) | Acc: (92.00%) (15446/16768)
Epoch: 92 | Batch_idx: 140 |  Loss: (0.2334) |  Loss2: (0.0000) | Acc: (92.00%) (16616/18048)
Epoch: 92 | Batch_idx: 150 |  Loss: (0.2337) |  Loss2: (0.0000) | Acc: (92.00%) (17794/19328)
Epoch: 92 | Batch_idx: 160 |  Loss: (0.2348) |  Loss2: (0.0000) | Acc: (92.00%) (18960/20608)
Epoch: 92 | Batch_idx: 170 |  Loss: (0.2350) |  Loss2: (0.0000) | Acc: (91.00%) (20131/21888)
Epoch: 92 | Batch_idx: 180 |  Loss: (0.2334) |  Loss2: (0.0000) | Acc: (92.00%) (21323/23168)
Epoch: 92 | Batch_idx: 190 |  Loss: (0.2341) |  Loss2: (0.0000) | Acc: (92.00%) (22494/24448)
Epoch: 92 | Batch_idx: 200 |  Loss: (0.2358) |  Loss2: (0.0000) | Acc: (91.00%) (23654/25728)
Epoch: 92 | Batch_idx: 210 |  Loss: (0.2353) |  Loss2: (0.0000) | Acc: (91.00%) (24837/27008)
Epoch: 92 | Batch_idx: 220 |  Loss: (0.2350) |  Loss2: (0.0000) | Acc: (91.00%) (26010/28288)
Epoch: 92 | Batch_idx: 230 |  Loss: (0.2334) |  Loss2: (0.0000) | Acc: (91.00%) (27199/29568)
Epoch: 92 | Batch_idx: 240 |  Loss: (0.2339) |  Loss2: (0.0000) | Acc: (91.00%) (28372/30848)
Epoch: 92 | Batch_idx: 250 |  Loss: (0.2351) |  Loss2: (0.0000) | Acc: (91.00%) (29536/32128)
Epoch: 92 | Batch_idx: 260 |  Loss: (0.2348) |  Loss2: (0.0000) | Acc: (91.00%) (30713/33408)
Epoch: 92 | Batch_idx: 270 |  Loss: (0.2355) |  Loss2: (0.0000) | Acc: (91.00%) (31873/34688)
Epoch: 92 | Batch_idx: 280 |  Loss: (0.2363) |  Loss2: (0.0000) | Acc: (91.00%) (33040/35968)
Epoch: 92 | Batch_idx: 290 |  Loss: (0.2359) |  Loss2: (0.0000) | Acc: (91.00%) (34218/37248)
Epoch: 92 | Batch_idx: 300 |  Loss: (0.2363) |  Loss2: (0.0000) | Acc: (91.00%) (35385/38528)
Epoch: 92 | Batch_idx: 310 |  Loss: (0.2366) |  Loss2: (0.0000) | Acc: (91.00%) (36554/39808)
Epoch: 92 | Batch_idx: 320 |  Loss: (0.2369) |  Loss2: (0.0000) | Acc: (91.00%) (37724/41088)
Epoch: 92 | Batch_idx: 330 |  Loss: (0.2378) |  Loss2: (0.0000) | Acc: (91.00%) (38893/42368)
Epoch: 92 | Batch_idx: 340 |  Loss: (0.2376) |  Loss2: (0.0000) | Acc: (91.00%) (40080/43648)
Epoch: 92 | Batch_idx: 350 |  Loss: (0.2382) |  Loss2: (0.0000) | Acc: (91.00%) (41246/44928)
Epoch: 92 | Batch_idx: 360 |  Loss: (0.2385) |  Loss2: (0.0000) | Acc: (91.00%) (42412/46208)
Epoch: 92 | Batch_idx: 370 |  Loss: (0.2386) |  Loss2: (0.0000) | Acc: (91.00%) (43585/47488)
Epoch: 92 | Batch_idx: 380 |  Loss: (0.2390) |  Loss2: (0.0000) | Acc: (91.00%) (44749/48768)
Epoch: 92 | Batch_idx: 390 |  Loss: (0.2387) |  Loss2: (0.0000) | Acc: (91.00%) (45880/50000)
# TEST : Loss: (0.4535) | Acc: (85.00%) (8562/10000)
percent tensor([0.5472, 0.5562, 0.5613, 0.5344, 0.5629, 0.5689, 0.5639, 0.5456, 0.5437,
        0.5555, 0.5547, 0.5634, 0.5446, 0.5203, 0.5643, 0.5445],
       device='cuda:0') torch.Size([16])
percent tensor([0.5334, 0.5318, 0.5364, 0.5396, 0.5384, 0.5349, 0.5346, 0.5385, 0.5342,
        0.5326, 0.5319, 0.5329, 0.5310, 0.5346, 0.5330, 0.5350],
       device='cuda:0') torch.Size([16])
percent tensor([0.5915, 0.5916, 0.4644, 0.4336, 0.4408, 0.5483, 0.5726, 0.4734, 0.5226,
        0.5772, 0.5896, 0.5034, 0.5899, 0.5741, 0.5696, 0.5837],
       device='cuda:0') torch.Size([16])
percent tensor([0.6945, 0.5789, 0.7101, 0.7162, 0.7162, 0.7550, 0.6558, 0.7254, 0.6681,
        0.6142, 0.6451, 0.6494, 0.5571, 0.6905, 0.6643, 0.7196],
       device='cuda:0') torch.Size([16])
percent tensor([0.5896, 0.5919, 0.5853, 0.5572, 0.5498, 0.5608, 0.5917, 0.5831, 0.5913,
        0.5662, 0.5650, 0.5677, 0.5830, 0.5836, 0.5868, 0.5655],
       device='cuda:0') torch.Size([16])
percent tensor([0.5948, 0.6505, 0.6804, 0.7061, 0.7236, 0.6910, 0.6759, 0.6574, 0.6841,
        0.6139, 0.6623, 0.7151, 0.6220, 0.7078, 0.6296, 0.6274],
       device='cuda:0') torch.Size([16])
percent tensor([0.5972, 0.6746, 0.6105, 0.5486, 0.6411, 0.6731, 0.5823, 0.4709, 0.6431,
        0.6496, 0.6350, 0.4972, 0.6464, 0.6137, 0.4781, 0.6000],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9994, 0.9997, 0.9997, 0.9998, 0.9991, 0.9996, 0.9998, 0.9996,
        0.9993, 0.9997, 0.9997, 0.9996, 0.9995, 0.9994, 0.9995],
       device='cuda:0') torch.Size([16])
Epoch: 93 | Batch_idx: 0 |  Loss: (0.1866) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 93 | Batch_idx: 10 |  Loss: (0.2306) |  Loss2: (0.0000) | Acc: (91.00%) (1283/1408)
Epoch: 93 | Batch_idx: 20 |  Loss: (0.2358) |  Loss2: (0.0000) | Acc: (91.00%) (2450/2688)
Epoch: 93 | Batch_idx: 30 |  Loss: (0.2398) |  Loss2: (0.0000) | Acc: (91.00%) (3623/3968)
Epoch: 93 | Batch_idx: 40 |  Loss: (0.2399) |  Loss2: (0.0000) | Acc: (91.00%) (4800/5248)
Epoch: 93 | Batch_idx: 50 |  Loss: (0.2400) |  Loss2: (0.0000) | Acc: (91.00%) (5983/6528)
Epoch: 93 | Batch_idx: 60 |  Loss: (0.2371) |  Loss2: (0.0000) | Acc: (91.00%) (7164/7808)
Epoch: 93 | Batch_idx: 70 |  Loss: (0.2347) |  Loss2: (0.0000) | Acc: (91.00%) (8347/9088)
Epoch: 93 | Batch_idx: 80 |  Loss: (0.2365) |  Loss2: (0.0000) | Acc: (91.00%) (9526/10368)
Epoch: 93 | Batch_idx: 90 |  Loss: (0.2379) |  Loss2: (0.0000) | Acc: (91.00%) (10699/11648)
Epoch: 93 | Batch_idx: 100 |  Loss: (0.2386) |  Loss2: (0.0000) | Acc: (91.00%) (11875/12928)
Epoch: 93 | Batch_idx: 110 |  Loss: (0.2387) |  Loss2: (0.0000) | Acc: (91.00%) (13043/14208)
Epoch: 93 | Batch_idx: 120 |  Loss: (0.2369) |  Loss2: (0.0000) | Acc: (91.00%) (14227/15488)
Epoch: 93 | Batch_idx: 130 |  Loss: (0.2346) |  Loss2: (0.0000) | Acc: (91.00%) (15416/16768)
Epoch: 93 | Batch_idx: 140 |  Loss: (0.2318) |  Loss2: (0.0000) | Acc: (92.00%) (16608/18048)
Epoch: 93 | Batch_idx: 150 |  Loss: (0.2298) |  Loss2: (0.0000) | Acc: (92.00%) (17799/19328)
Epoch: 93 | Batch_idx: 160 |  Loss: (0.2285) |  Loss2: (0.0000) | Acc: (92.00%) (18991/20608)
Epoch: 93 | Batch_idx: 170 |  Loss: (0.2291) |  Loss2: (0.0000) | Acc: (92.00%) (20165/21888)
Epoch: 93 | Batch_idx: 180 |  Loss: (0.2303) |  Loss2: (0.0000) | Acc: (92.00%) (21325/23168)
Epoch: 93 | Batch_idx: 190 |  Loss: (0.2299) |  Loss2: (0.0000) | Acc: (92.00%) (22506/24448)
Epoch: 93 | Batch_idx: 200 |  Loss: (0.2323) |  Loss2: (0.0000) | Acc: (91.00%) (23665/25728)
Epoch: 93 | Batch_idx: 210 |  Loss: (0.2330) |  Loss2: (0.0000) | Acc: (91.00%) (24823/27008)
Epoch: 93 | Batch_idx: 220 |  Loss: (0.2344) |  Loss2: (0.0000) | Acc: (91.00%) (25988/28288)
Epoch: 93 | Batch_idx: 230 |  Loss: (0.2341) |  Loss2: (0.0000) | Acc: (91.00%) (27180/29568)
Epoch: 93 | Batch_idx: 240 |  Loss: (0.2336) |  Loss2: (0.0000) | Acc: (91.00%) (28364/30848)
Epoch: 93 | Batch_idx: 250 |  Loss: (0.2341) |  Loss2: (0.0000) | Acc: (91.00%) (29540/32128)
Epoch: 93 | Batch_idx: 260 |  Loss: (0.2345) |  Loss2: (0.0000) | Acc: (91.00%) (30715/33408)
Epoch: 93 | Batch_idx: 270 |  Loss: (0.2340) |  Loss2: (0.0000) | Acc: (91.00%) (31899/34688)
Epoch: 93 | Batch_idx: 280 |  Loss: (0.2338) |  Loss2: (0.0000) | Acc: (91.00%) (33078/35968)
Epoch: 93 | Batch_idx: 290 |  Loss: (0.2335) |  Loss2: (0.0000) | Acc: (91.00%) (34259/37248)
Epoch: 93 | Batch_idx: 300 |  Loss: (0.2331) |  Loss2: (0.0000) | Acc: (91.00%) (35435/38528)
Epoch: 93 | Batch_idx: 310 |  Loss: (0.2328) |  Loss2: (0.0000) | Acc: (91.00%) (36612/39808)
Epoch: 93 | Batch_idx: 320 |  Loss: (0.2320) |  Loss2: (0.0000) | Acc: (92.00%) (37809/41088)
Epoch: 93 | Batch_idx: 330 |  Loss: (0.2316) |  Loss2: (0.0000) | Acc: (92.00%) (39006/42368)
Epoch: 93 | Batch_idx: 340 |  Loss: (0.2315) |  Loss2: (0.0000) | Acc: (92.00%) (40178/43648)
Epoch: 93 | Batch_idx: 350 |  Loss: (0.2322) |  Loss2: (0.0000) | Acc: (92.00%) (41346/44928)
Epoch: 93 | Batch_idx: 360 |  Loss: (0.2319) |  Loss2: (0.0000) | Acc: (92.00%) (42531/46208)
Epoch: 93 | Batch_idx: 370 |  Loss: (0.2322) |  Loss2: (0.0000) | Acc: (92.00%) (43704/47488)
Epoch: 93 | Batch_idx: 380 |  Loss: (0.2325) |  Loss2: (0.0000) | Acc: (92.00%) (44876/48768)
Epoch: 93 | Batch_idx: 390 |  Loss: (0.2326) |  Loss2: (0.0000) | Acc: (91.00%) (45998/50000)
# TEST : Loss: (0.4121) | Acc: (86.00%) (8678/10000)
percent tensor([0.5492, 0.5570, 0.5632, 0.5354, 0.5656, 0.5703, 0.5655, 0.5461, 0.5447,
        0.5569, 0.5555, 0.5657, 0.5460, 0.5198, 0.5656, 0.5457],
       device='cuda:0') torch.Size([16])
percent tensor([0.5335, 0.5322, 0.5354, 0.5396, 0.5377, 0.5350, 0.5345, 0.5388, 0.5339,
        0.5323, 0.5316, 0.5323, 0.5311, 0.5356, 0.5334, 0.5353],
       device='cuda:0') torch.Size([16])
percent tensor([0.5945, 0.5851, 0.4875, 0.4521, 0.4557, 0.5581, 0.5660, 0.4810, 0.5158,
        0.5710, 0.5831, 0.5156, 0.5870, 0.5550, 0.5706, 0.5847],
       device='cuda:0') torch.Size([16])
percent tensor([0.6906, 0.5706, 0.7130, 0.7181, 0.7123, 0.7502, 0.6548, 0.7155, 0.6607,
        0.5990, 0.6401, 0.6395, 0.5480, 0.6777, 0.6559, 0.7115],
       device='cuda:0') torch.Size([16])
percent tensor([0.5969, 0.5962, 0.5964, 0.5704, 0.5703, 0.5756, 0.5981, 0.5932, 0.5954,
        0.5692, 0.5679, 0.5755, 0.5892, 0.5903, 0.5962, 0.5816],
       device='cuda:0') torch.Size([16])
percent tensor([0.5779, 0.6395, 0.6572, 0.7041, 0.7061, 0.6904, 0.6526, 0.6440, 0.6810,
        0.5913, 0.6400, 0.6896, 0.5989, 0.6925, 0.6239, 0.6162],
       device='cuda:0') torch.Size([16])
percent tensor([0.5934, 0.6468, 0.5922, 0.5386, 0.6514, 0.6930, 0.5712, 0.4576, 0.6258,
        0.6326, 0.6318, 0.4937, 0.6237, 0.6074, 0.4681, 0.6073],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9994, 0.9995, 0.9997, 0.9998, 0.9995, 0.9994, 0.9997, 0.9997,
        0.9993, 0.9997, 0.9997, 0.9997, 0.9992, 0.9992, 0.9996],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 94 | Batch_idx: 0 |  Loss: (0.1594) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 94 | Batch_idx: 10 |  Loss: (0.2043) |  Loss2: (0.0000) | Acc: (92.00%) (1304/1408)
Epoch: 94 | Batch_idx: 20 |  Loss: (0.2184) |  Loss2: (0.0000) | Acc: (92.00%) (2479/2688)
Epoch: 94 | Batch_idx: 30 |  Loss: (0.2421) |  Loss2: (0.0000) | Acc: (91.00%) (3627/3968)
Epoch: 94 | Batch_idx: 40 |  Loss: (0.2432) |  Loss2: (0.0000) | Acc: (91.00%) (4798/5248)
Epoch: 94 | Batch_idx: 50 |  Loss: (0.2477) |  Loss2: (0.0000) | Acc: (91.00%) (5960/6528)
Epoch: 94 | Batch_idx: 60 |  Loss: (0.2499) |  Loss2: (0.0000) | Acc: (91.00%) (7126/7808)
Epoch: 94 | Batch_idx: 70 |  Loss: (0.2480) |  Loss2: (0.0000) | Acc: (91.00%) (8294/9088)
Epoch: 94 | Batch_idx: 80 |  Loss: (0.2475) |  Loss2: (0.0000) | Acc: (91.00%) (9469/10368)
Epoch: 94 | Batch_idx: 90 |  Loss: (0.2492) |  Loss2: (0.0000) | Acc: (91.00%) (10638/11648)
Epoch: 94 | Batch_idx: 100 |  Loss: (0.2486) |  Loss2: (0.0000) | Acc: (91.00%) (11809/12928)
Epoch: 94 | Batch_idx: 110 |  Loss: (0.2497) |  Loss2: (0.0000) | Acc: (91.00%) (12963/14208)
Epoch: 94 | Batch_idx: 120 |  Loss: (0.2487) |  Loss2: (0.0000) | Acc: (91.00%) (14129/15488)
Epoch: 94 | Batch_idx: 130 |  Loss: (0.2459) |  Loss2: (0.0000) | Acc: (91.00%) (15323/16768)
Epoch: 94 | Batch_idx: 140 |  Loss: (0.2460) |  Loss2: (0.0000) | Acc: (91.00%) (16496/18048)
Epoch: 94 | Batch_idx: 150 |  Loss: (0.2468) |  Loss2: (0.0000) | Acc: (91.00%) (17653/19328)
Epoch: 94 | Batch_idx: 160 |  Loss: (0.2471) |  Loss2: (0.0000) | Acc: (91.00%) (18816/20608)
Epoch: 94 | Batch_idx: 170 |  Loss: (0.2464) |  Loss2: (0.0000) | Acc: (91.00%) (19995/21888)
Epoch: 94 | Batch_idx: 180 |  Loss: (0.2472) |  Loss2: (0.0000) | Acc: (91.00%) (21145/23168)
Epoch: 94 | Batch_idx: 190 |  Loss: (0.2460) |  Loss2: (0.0000) | Acc: (91.00%) (22333/24448)
Epoch: 94 | Batch_idx: 200 |  Loss: (0.2452) |  Loss2: (0.0000) | Acc: (91.00%) (23511/25728)
Epoch: 94 | Batch_idx: 210 |  Loss: (0.2450) |  Loss2: (0.0000) | Acc: (91.00%) (24680/27008)
Epoch: 94 | Batch_idx: 220 |  Loss: (0.2447) |  Loss2: (0.0000) | Acc: (91.00%) (25852/28288)
Epoch: 94 | Batch_idx: 230 |  Loss: (0.2441) |  Loss2: (0.0000) | Acc: (91.00%) (27033/29568)
Epoch: 94 | Batch_idx: 240 |  Loss: (0.2442) |  Loss2: (0.0000) | Acc: (91.00%) (28194/30848)
Epoch: 94 | Batch_idx: 250 |  Loss: (0.2433) |  Loss2: (0.0000) | Acc: (91.00%) (29374/32128)
Epoch: 94 | Batch_idx: 260 |  Loss: (0.2424) |  Loss2: (0.0000) | Acc: (91.00%) (30560/33408)
Epoch: 94 | Batch_idx: 270 |  Loss: (0.2424) |  Loss2: (0.0000) | Acc: (91.00%) (31729/34688)
Epoch: 94 | Batch_idx: 280 |  Loss: (0.2424) |  Loss2: (0.0000) | Acc: (91.00%) (32905/35968)
Epoch: 94 | Batch_idx: 290 |  Loss: (0.2423) |  Loss2: (0.0000) | Acc: (91.00%) (34080/37248)
Epoch: 94 | Batch_idx: 300 |  Loss: (0.2424) |  Loss2: (0.0000) | Acc: (91.00%) (35242/38528)
Epoch: 94 | Batch_idx: 310 |  Loss: (0.2421) |  Loss2: (0.0000) | Acc: (91.00%) (36419/39808)
Epoch: 94 | Batch_idx: 320 |  Loss: (0.2417) |  Loss2: (0.0000) | Acc: (91.00%) (37592/41088)
Epoch: 94 | Batch_idx: 330 |  Loss: (0.2418) |  Loss2: (0.0000) | Acc: (91.00%) (38768/42368)
Epoch: 94 | Batch_idx: 340 |  Loss: (0.2417) |  Loss2: (0.0000) | Acc: (91.00%) (39944/43648)
Epoch: 94 | Batch_idx: 350 |  Loss: (0.2408) |  Loss2: (0.0000) | Acc: (91.00%) (41131/44928)
Epoch: 94 | Batch_idx: 360 |  Loss: (0.2416) |  Loss2: (0.0000) | Acc: (91.00%) (42284/46208)
Epoch: 94 | Batch_idx: 370 |  Loss: (0.2403) |  Loss2: (0.0000) | Acc: (91.00%) (43470/47488)
Epoch: 94 | Batch_idx: 380 |  Loss: (0.2400) |  Loss2: (0.0000) | Acc: (91.00%) (44656/48768)
Epoch: 94 | Batch_idx: 390 |  Loss: (0.2396) |  Loss2: (0.0000) | Acc: (91.00%) (45790/50000)
# TEST : Loss: (0.3922) | Acc: (86.00%) (8696/10000)
percent tensor([0.5454, 0.5543, 0.5576, 0.5305, 0.5607, 0.5664, 0.5620, 0.5411, 0.5417,
        0.5531, 0.5530, 0.5607, 0.5427, 0.5180, 0.5617, 0.5420],
       device='cuda:0') torch.Size([16])
percent tensor([0.5396, 0.5387, 0.5424, 0.5474, 0.5450, 0.5416, 0.5409, 0.5461, 0.5402,
        0.5384, 0.5374, 0.5384, 0.5372, 0.5420, 0.5400, 0.5418],
       device='cuda:0') torch.Size([16])
percent tensor([0.5788, 0.5975, 0.4306, 0.3981, 0.4026, 0.5321, 0.5503, 0.4304, 0.4998,
        0.5650, 0.5852, 0.4801, 0.5879, 0.5523, 0.5554, 0.5700],
       device='cuda:0') torch.Size([16])
percent tensor([0.6590, 0.5463, 0.6665, 0.6895, 0.6716, 0.7271, 0.6116, 0.6762, 0.6262,
        0.5697, 0.6092, 0.5970, 0.5264, 0.6475, 0.6200, 0.6812],
       device='cuda:0') torch.Size([16])
percent tensor([0.6291, 0.6259, 0.6262, 0.5835, 0.5902, 0.6023, 0.6288, 0.6150, 0.6309,
        0.5961, 0.6008, 0.6022, 0.6240, 0.6285, 0.6262, 0.6069],
       device='cuda:0') torch.Size([16])
percent tensor([0.5942, 0.6556, 0.6759, 0.7234, 0.7215, 0.7095, 0.6655, 0.6430, 0.7004,
        0.6024, 0.6542, 0.7108, 0.6232, 0.7075, 0.6291, 0.6204],
       device='cuda:0') torch.Size([16])
percent tensor([0.6006, 0.6573, 0.6035, 0.5413, 0.6627, 0.7072, 0.5736, 0.4361, 0.6458,
        0.6437, 0.6427, 0.4882, 0.6522, 0.6185, 0.4651, 0.6037],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9995, 0.9996, 0.9995, 0.9998, 0.9990, 0.9993, 0.9997, 0.9998,
        0.9994, 0.9997, 0.9998, 0.9997, 0.9992, 0.9991, 0.9995],
       device='cuda:0') torch.Size([16])
Epoch: 95 | Batch_idx: 0 |  Loss: (0.2438) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 95 | Batch_idx: 10 |  Loss: (0.2069) |  Loss2: (0.0000) | Acc: (92.00%) (1298/1408)
Epoch: 95 | Batch_idx: 20 |  Loss: (0.2226) |  Loss2: (0.0000) | Acc: (91.00%) (2472/2688)
Epoch: 95 | Batch_idx: 30 |  Loss: (0.2219) |  Loss2: (0.0000) | Acc: (92.00%) (3652/3968)
Epoch: 95 | Batch_idx: 40 |  Loss: (0.2208) |  Loss2: (0.0000) | Acc: (92.00%) (4838/5248)
Epoch: 95 | Batch_idx: 50 |  Loss: (0.2203) |  Loss2: (0.0000) | Acc: (92.00%) (6028/6528)
Epoch: 95 | Batch_idx: 60 |  Loss: (0.2212) |  Loss2: (0.0000) | Acc: (92.00%) (7203/7808)
Epoch: 95 | Batch_idx: 70 |  Loss: (0.2163) |  Loss2: (0.0000) | Acc: (92.00%) (8407/9088)
Epoch: 95 | Batch_idx: 80 |  Loss: (0.2167) |  Loss2: (0.0000) | Acc: (92.00%) (9595/10368)
Epoch: 95 | Batch_idx: 90 |  Loss: (0.2180) |  Loss2: (0.0000) | Acc: (92.00%) (10769/11648)
Epoch: 95 | Batch_idx: 100 |  Loss: (0.2192) |  Loss2: (0.0000) | Acc: (92.00%) (11940/12928)
Epoch: 95 | Batch_idx: 110 |  Loss: (0.2195) |  Loss2: (0.0000) | Acc: (92.00%) (13121/14208)
Epoch: 95 | Batch_idx: 120 |  Loss: (0.2217) |  Loss2: (0.0000) | Acc: (92.00%) (14298/15488)
Epoch: 95 | Batch_idx: 130 |  Loss: (0.2200) |  Loss2: (0.0000) | Acc: (92.00%) (15492/16768)
Epoch: 95 | Batch_idx: 140 |  Loss: (0.2209) |  Loss2: (0.0000) | Acc: (92.00%) (16665/18048)
Epoch: 95 | Batch_idx: 150 |  Loss: (0.2212) |  Loss2: (0.0000) | Acc: (92.00%) (17830/19328)
Epoch: 95 | Batch_idx: 160 |  Loss: (0.2223) |  Loss2: (0.0000) | Acc: (92.00%) (19002/20608)
Epoch: 95 | Batch_idx: 170 |  Loss: (0.2236) |  Loss2: (0.0000) | Acc: (92.00%) (20172/21888)
Epoch: 95 | Batch_idx: 180 |  Loss: (0.2248) |  Loss2: (0.0000) | Acc: (92.00%) (21343/23168)
Epoch: 95 | Batch_idx: 190 |  Loss: (0.2235) |  Loss2: (0.0000) | Acc: (92.00%) (22533/24448)
Epoch: 95 | Batch_idx: 200 |  Loss: (0.2236) |  Loss2: (0.0000) | Acc: (92.00%) (23724/25728)
Epoch: 95 | Batch_idx: 210 |  Loss: (0.2235) |  Loss2: (0.0000) | Acc: (92.00%) (24901/27008)
Epoch: 95 | Batch_idx: 220 |  Loss: (0.2235) |  Loss2: (0.0000) | Acc: (92.00%) (26073/28288)
Epoch: 95 | Batch_idx: 230 |  Loss: (0.2246) |  Loss2: (0.0000) | Acc: (92.00%) (27240/29568)
Epoch: 95 | Batch_idx: 240 |  Loss: (0.2251) |  Loss2: (0.0000) | Acc: (92.00%) (28416/30848)
Epoch: 95 | Batch_idx: 250 |  Loss: (0.2250) |  Loss2: (0.0000) | Acc: (92.00%) (29598/32128)
Epoch: 95 | Batch_idx: 260 |  Loss: (0.2257) |  Loss2: (0.0000) | Acc: (92.00%) (30763/33408)
Epoch: 95 | Batch_idx: 270 |  Loss: (0.2251) |  Loss2: (0.0000) | Acc: (92.00%) (31943/34688)
Epoch: 95 | Batch_idx: 280 |  Loss: (0.2256) |  Loss2: (0.0000) | Acc: (92.00%) (33125/35968)
Epoch: 95 | Batch_idx: 290 |  Loss: (0.2253) |  Loss2: (0.0000) | Acc: (92.00%) (34302/37248)
Epoch: 95 | Batch_idx: 300 |  Loss: (0.2260) |  Loss2: (0.0000) | Acc: (92.00%) (35469/38528)
Epoch: 95 | Batch_idx: 310 |  Loss: (0.2262) |  Loss2: (0.0000) | Acc: (92.00%) (36649/39808)
Epoch: 95 | Batch_idx: 320 |  Loss: (0.2258) |  Loss2: (0.0000) | Acc: (92.00%) (37837/41088)
Epoch: 95 | Batch_idx: 330 |  Loss: (0.2256) |  Loss2: (0.0000) | Acc: (92.00%) (39010/42368)
Epoch: 95 | Batch_idx: 340 |  Loss: (0.2250) |  Loss2: (0.0000) | Acc: (92.00%) (40198/43648)
Epoch: 95 | Batch_idx: 350 |  Loss: (0.2255) |  Loss2: (0.0000) | Acc: (92.00%) (41377/44928)
Epoch: 95 | Batch_idx: 360 |  Loss: (0.2242) |  Loss2: (0.0000) | Acc: (92.00%) (42582/46208)
Epoch: 95 | Batch_idx: 370 |  Loss: (0.2235) |  Loss2: (0.0000) | Acc: (92.00%) (43781/47488)
Epoch: 95 | Batch_idx: 380 |  Loss: (0.2236) |  Loss2: (0.0000) | Acc: (92.00%) (44960/48768)
Epoch: 95 | Batch_idx: 390 |  Loss: (0.2230) |  Loss2: (0.0000) | Acc: (92.00%) (46110/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_095.pth.tar'
# TEST : Loss: (0.3781) | Acc: (87.00%) (8741/10000)
percent tensor([0.5444, 0.5531, 0.5561, 0.5294, 0.5591, 0.5651, 0.5605, 0.5395, 0.5405,
        0.5518, 0.5520, 0.5590, 0.5417, 0.5171, 0.5604, 0.5409],
       device='cuda:0') torch.Size([16])
percent tensor([0.5392, 0.5382, 0.5417, 0.5470, 0.5445, 0.5412, 0.5404, 0.5457, 0.5397,
        0.5379, 0.5368, 0.5379, 0.5367, 0.5415, 0.5395, 0.5414],
       device='cuda:0') torch.Size([16])
percent tensor([0.5818, 0.6025, 0.4383, 0.4052, 0.4063, 0.5378, 0.5525, 0.4347, 0.5080,
        0.5719, 0.5904, 0.4847, 0.5971, 0.5571, 0.5552, 0.5769],
       device='cuda:0') torch.Size([16])
percent tensor([0.6562, 0.5479, 0.6612, 0.6863, 0.6686, 0.7282, 0.6093, 0.6725, 0.6238,
        0.5695, 0.6072, 0.5922, 0.5269, 0.6495, 0.6157, 0.6832],
       device='cuda:0') torch.Size([16])
percent tensor([0.6336, 0.6321, 0.6282, 0.5842, 0.5896, 0.6079, 0.6319, 0.6166, 0.6362,
        0.5986, 0.6040, 0.6044, 0.6306, 0.6339, 0.6324, 0.6087],
       device='cuda:0') torch.Size([16])
percent tensor([0.5999, 0.6653, 0.6834, 0.7290, 0.7265, 0.7173, 0.6726, 0.6459, 0.7082,
        0.6099, 0.6629, 0.7220, 0.6353, 0.7158, 0.6356, 0.6252],
       device='cuda:0') torch.Size([16])
percent tensor([0.5948, 0.6548, 0.6011, 0.5302, 0.6619, 0.7090, 0.5673, 0.4284, 0.6427,
        0.6416, 0.6429, 0.4772, 0.6542, 0.6107, 0.4555, 0.5944],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9995, 0.9996, 0.9996, 0.9998, 0.9990, 0.9993, 0.9997, 0.9998,
        0.9994, 0.9997, 0.9998, 0.9997, 0.9993, 0.9991, 0.9994],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 96 | Batch_idx: 0 |  Loss: (0.1708) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 96 | Batch_idx: 10 |  Loss: (0.2340) |  Loss2: (0.0000) | Acc: (92.00%) (1304/1408)
Epoch: 96 | Batch_idx: 20 |  Loss: (0.2185) |  Loss2: (0.0000) | Acc: (92.00%) (2494/2688)
Epoch: 96 | Batch_idx: 30 |  Loss: (0.2085) |  Loss2: (0.0000) | Acc: (93.00%) (3692/3968)
Epoch: 96 | Batch_idx: 40 |  Loss: (0.2078) |  Loss2: (0.0000) | Acc: (93.00%) (4881/5248)
Epoch: 96 | Batch_idx: 50 |  Loss: (0.2124) |  Loss2: (0.0000) | Acc: (92.00%) (6056/6528)
Epoch: 96 | Batch_idx: 60 |  Loss: (0.2097) |  Loss2: (0.0000) | Acc: (92.00%) (7256/7808)
Epoch: 96 | Batch_idx: 70 |  Loss: (0.2109) |  Loss2: (0.0000) | Acc: (92.00%) (8428/9088)
Epoch: 96 | Batch_idx: 80 |  Loss: (0.2099) |  Loss2: (0.0000) | Acc: (92.00%) (9625/10368)
Epoch: 96 | Batch_idx: 90 |  Loss: (0.2108) |  Loss2: (0.0000) | Acc: (92.00%) (10810/11648)
Epoch: 96 | Batch_idx: 100 |  Loss: (0.2097) |  Loss2: (0.0000) | Acc: (92.00%) (12006/12928)
Epoch: 96 | Batch_idx: 110 |  Loss: (0.2133) |  Loss2: (0.0000) | Acc: (92.00%) (13184/14208)
Epoch: 96 | Batch_idx: 120 |  Loss: (0.2145) |  Loss2: (0.0000) | Acc: (92.00%) (14371/15488)
Epoch: 96 | Batch_idx: 130 |  Loss: (0.2149) |  Loss2: (0.0000) | Acc: (92.00%) (15553/16768)
Epoch: 96 | Batch_idx: 140 |  Loss: (0.2161) |  Loss2: (0.0000) | Acc: (92.00%) (16725/18048)
Epoch: 96 | Batch_idx: 150 |  Loss: (0.2193) |  Loss2: (0.0000) | Acc: (92.00%) (17897/19328)
Epoch: 96 | Batch_idx: 160 |  Loss: (0.2181) |  Loss2: (0.0000) | Acc: (92.00%) (19086/20608)
Epoch: 96 | Batch_idx: 170 |  Loss: (0.2182) |  Loss2: (0.0000) | Acc: (92.00%) (20257/21888)
Epoch: 96 | Batch_idx: 180 |  Loss: (0.2163) |  Loss2: (0.0000) | Acc: (92.00%) (21453/23168)
Epoch: 96 | Batch_idx: 190 |  Loss: (0.2177) |  Loss2: (0.0000) | Acc: (92.00%) (22629/24448)
Epoch: 96 | Batch_idx: 200 |  Loss: (0.2173) |  Loss2: (0.0000) | Acc: (92.00%) (23818/25728)
Epoch: 96 | Batch_idx: 210 |  Loss: (0.2192) |  Loss2: (0.0000) | Acc: (92.00%) (24993/27008)
Epoch: 96 | Batch_idx: 220 |  Loss: (0.2194) |  Loss2: (0.0000) | Acc: (92.00%) (26171/28288)
Epoch: 96 | Batch_idx: 230 |  Loss: (0.2199) |  Loss2: (0.0000) | Acc: (92.00%) (27346/29568)
Epoch: 96 | Batch_idx: 240 |  Loss: (0.2196) |  Loss2: (0.0000) | Acc: (92.00%) (28523/30848)
Epoch: 96 | Batch_idx: 250 |  Loss: (0.2207) |  Loss2: (0.0000) | Acc: (92.00%) (29684/32128)
Epoch: 96 | Batch_idx: 260 |  Loss: (0.2212) |  Loss2: (0.0000) | Acc: (92.00%) (30857/33408)
Epoch: 96 | Batch_idx: 270 |  Loss: (0.2216) |  Loss2: (0.0000) | Acc: (92.00%) (32033/34688)
Epoch: 96 | Batch_idx: 280 |  Loss: (0.2218) |  Loss2: (0.0000) | Acc: (92.00%) (33206/35968)
Epoch: 96 | Batch_idx: 290 |  Loss: (0.2223) |  Loss2: (0.0000) | Acc: (92.00%) (34382/37248)
Epoch: 96 | Batch_idx: 300 |  Loss: (0.2238) |  Loss2: (0.0000) | Acc: (92.00%) (35532/38528)
Epoch: 96 | Batch_idx: 310 |  Loss: (0.2248) |  Loss2: (0.0000) | Acc: (92.00%) (36705/39808)
Epoch: 96 | Batch_idx: 320 |  Loss: (0.2253) |  Loss2: (0.0000) | Acc: (92.00%) (37879/41088)
Epoch: 96 | Batch_idx: 330 |  Loss: (0.2250) |  Loss2: (0.0000) | Acc: (92.00%) (39063/42368)
Epoch: 96 | Batch_idx: 340 |  Loss: (0.2250) |  Loss2: (0.0000) | Acc: (92.00%) (40250/43648)
Epoch: 96 | Batch_idx: 350 |  Loss: (0.2254) |  Loss2: (0.0000) | Acc: (92.00%) (41418/44928)
Epoch: 96 | Batch_idx: 360 |  Loss: (0.2258) |  Loss2: (0.0000) | Acc: (92.00%) (42603/46208)
Epoch: 96 | Batch_idx: 370 |  Loss: (0.2260) |  Loss2: (0.0000) | Acc: (92.00%) (43787/47488)
Epoch: 96 | Batch_idx: 380 |  Loss: (0.2262) |  Loss2: (0.0000) | Acc: (92.00%) (44966/48768)
Epoch: 96 | Batch_idx: 390 |  Loss: (0.2271) |  Loss2: (0.0000) | Acc: (92.00%) (46089/50000)
# TEST : Loss: (0.4425) | Acc: (85.00%) (8579/10000)
percent tensor([0.5439, 0.5533, 0.5566, 0.5307, 0.5583, 0.5652, 0.5604, 0.5394, 0.5386,
        0.5518, 0.5511, 0.5592, 0.5415, 0.5168, 0.5608, 0.5407],
       device='cuda:0') torch.Size([16])
percent tensor([0.5384, 0.5375, 0.5408, 0.5456, 0.5440, 0.5400, 0.5401, 0.5452, 0.5401,
        0.5377, 0.5374, 0.5375, 0.5364, 0.5411, 0.5388, 0.5410],
       device='cuda:0') torch.Size([16])
percent tensor([0.5845, 0.5984, 0.4389, 0.4058, 0.4040, 0.5277, 0.5550, 0.4416, 0.4938,
        0.5747, 0.5809, 0.4866, 0.5968, 0.5593, 0.5525, 0.5767],
       device='cuda:0') torch.Size([16])
percent tensor([0.6530, 0.5522, 0.6611, 0.6690, 0.6582, 0.7186, 0.6101, 0.6716, 0.6246,
        0.5760, 0.6107, 0.6011, 0.5340, 0.6546, 0.6085, 0.6863],
       device='cuda:0') torch.Size([16])
percent tensor([0.6295, 0.6296, 0.6158, 0.5783, 0.5790, 0.6129, 0.6338, 0.6124, 0.6230,
        0.6063, 0.5906, 0.5943, 0.6196, 0.6232, 0.6291, 0.6058],
       device='cuda:0') torch.Size([16])
percent tensor([0.5890, 0.6685, 0.6689, 0.7161, 0.7109, 0.6987, 0.6700, 0.6615, 0.6939,
        0.6200, 0.6782, 0.7305, 0.6380, 0.7148, 0.6434, 0.6212],
       device='cuda:0') torch.Size([16])
percent tensor([0.5892, 0.6601, 0.6020, 0.4948, 0.6490, 0.6954, 0.5638, 0.4451, 0.6142,
        0.6586, 0.6191, 0.4644, 0.6398, 0.5968, 0.4543, 0.5814],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9993, 0.9997, 0.9998, 0.9996, 0.9991, 0.9995, 0.9997, 0.9995,
        0.9994, 0.9997, 0.9998, 0.9996, 0.9992, 0.9994, 0.9996],
       device='cuda:0') torch.Size([16])
Epoch: 97 | Batch_idx: 0 |  Loss: (0.1481) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 97 | Batch_idx: 10 |  Loss: (0.1956) |  Loss2: (0.0000) | Acc: (93.00%) (1316/1408)
Epoch: 97 | Batch_idx: 20 |  Loss: (0.2012) |  Loss2: (0.0000) | Acc: (93.00%) (2503/2688)
Epoch: 97 | Batch_idx: 30 |  Loss: (0.2090) |  Loss2: (0.0000) | Acc: (92.00%) (3686/3968)
Epoch: 97 | Batch_idx: 40 |  Loss: (0.2094) |  Loss2: (0.0000) | Acc: (92.00%) (4872/5248)
Epoch: 97 | Batch_idx: 50 |  Loss: (0.2108) |  Loss2: (0.0000) | Acc: (92.00%) (6057/6528)
Epoch: 97 | Batch_idx: 60 |  Loss: (0.2115) |  Loss2: (0.0000) | Acc: (92.00%) (7247/7808)
Epoch: 97 | Batch_idx: 70 |  Loss: (0.2113) |  Loss2: (0.0000) | Acc: (92.00%) (8432/9088)
Epoch: 97 | Batch_idx: 80 |  Loss: (0.2122) |  Loss2: (0.0000) | Acc: (92.00%) (9607/10368)
Epoch: 97 | Batch_idx: 90 |  Loss: (0.2122) |  Loss2: (0.0000) | Acc: (92.00%) (10796/11648)
Epoch: 97 | Batch_idx: 100 |  Loss: (0.2125) |  Loss2: (0.0000) | Acc: (92.00%) (11989/12928)
Epoch: 97 | Batch_idx: 110 |  Loss: (0.2154) |  Loss2: (0.0000) | Acc: (92.00%) (13161/14208)
Epoch: 97 | Batch_idx: 120 |  Loss: (0.2152) |  Loss2: (0.0000) | Acc: (92.00%) (14345/15488)
Epoch: 97 | Batch_idx: 130 |  Loss: (0.2159) |  Loss2: (0.0000) | Acc: (92.00%) (15518/16768)
Epoch: 97 | Batch_idx: 140 |  Loss: (0.2166) |  Loss2: (0.0000) | Acc: (92.00%) (16689/18048)
Epoch: 97 | Batch_idx: 150 |  Loss: (0.2163) |  Loss2: (0.0000) | Acc: (92.00%) (17879/19328)
Epoch: 97 | Batch_idx: 160 |  Loss: (0.2165) |  Loss2: (0.0000) | Acc: (92.00%) (19065/20608)
Epoch: 97 | Batch_idx: 170 |  Loss: (0.2169) |  Loss2: (0.0000) | Acc: (92.00%) (20250/21888)
Epoch: 97 | Batch_idx: 180 |  Loss: (0.2174) |  Loss2: (0.0000) | Acc: (92.00%) (21424/23168)
Epoch: 97 | Batch_idx: 190 |  Loss: (0.2190) |  Loss2: (0.0000) | Acc: (92.00%) (22595/24448)
Epoch: 97 | Batch_idx: 200 |  Loss: (0.2188) |  Loss2: (0.0000) | Acc: (92.00%) (23783/25728)
Epoch: 97 | Batch_idx: 210 |  Loss: (0.2176) |  Loss2: (0.0000) | Acc: (92.00%) (24971/27008)
Epoch: 97 | Batch_idx: 220 |  Loss: (0.2198) |  Loss2: (0.0000) | Acc: (92.00%) (26139/28288)
Epoch: 97 | Batch_idx: 230 |  Loss: (0.2213) |  Loss2: (0.0000) | Acc: (92.00%) (27303/29568)
Epoch: 97 | Batch_idx: 240 |  Loss: (0.2233) |  Loss2: (0.0000) | Acc: (92.00%) (28470/30848)
Epoch: 97 | Batch_idx: 250 |  Loss: (0.2239) |  Loss2: (0.0000) | Acc: (92.00%) (29651/32128)
Epoch: 97 | Batch_idx: 260 |  Loss: (0.2231) |  Loss2: (0.0000) | Acc: (92.00%) (30842/33408)
Epoch: 97 | Batch_idx: 270 |  Loss: (0.2229) |  Loss2: (0.0000) | Acc: (92.00%) (32028/34688)
Epoch: 97 | Batch_idx: 280 |  Loss: (0.2243) |  Loss2: (0.0000) | Acc: (92.00%) (33182/35968)
Epoch: 97 | Batch_idx: 290 |  Loss: (0.2232) |  Loss2: (0.0000) | Acc: (92.00%) (34378/37248)
Epoch: 97 | Batch_idx: 300 |  Loss: (0.2226) |  Loss2: (0.0000) | Acc: (92.00%) (35568/38528)
Epoch: 97 | Batch_idx: 310 |  Loss: (0.2226) |  Loss2: (0.0000) | Acc: (92.00%) (36753/39808)
Epoch: 97 | Batch_idx: 320 |  Loss: (0.2224) |  Loss2: (0.0000) | Acc: (92.00%) (37950/41088)
Epoch: 97 | Batch_idx: 330 |  Loss: (0.2226) |  Loss2: (0.0000) | Acc: (92.00%) (39130/42368)
Epoch: 97 | Batch_idx: 340 |  Loss: (0.2229) |  Loss2: (0.0000) | Acc: (92.00%) (40312/43648)
Epoch: 97 | Batch_idx: 350 |  Loss: (0.2231) |  Loss2: (0.0000) | Acc: (92.00%) (41495/44928)
Epoch: 97 | Batch_idx: 360 |  Loss: (0.2231) |  Loss2: (0.0000) | Acc: (92.00%) (42682/46208)
Epoch: 97 | Batch_idx: 370 |  Loss: (0.2227) |  Loss2: (0.0000) | Acc: (92.00%) (43879/47488)
Epoch: 97 | Batch_idx: 380 |  Loss: (0.2223) |  Loss2: (0.0000) | Acc: (92.00%) (45068/48768)
Epoch: 97 | Batch_idx: 390 |  Loss: (0.2227) |  Loss2: (0.0000) | Acc: (92.00%) (46200/50000)
# TEST : Loss: (0.4145) | Acc: (87.00%) (8709/10000)
percent tensor([0.5425, 0.5526, 0.5542, 0.5309, 0.5559, 0.5634, 0.5585, 0.5396, 0.5380,
        0.5505, 0.5509, 0.5565, 0.5409, 0.5169, 0.5602, 0.5408],
       device='cuda:0') torch.Size([16])
percent tensor([0.5387, 0.5386, 0.5408, 0.5457, 0.5437, 0.5400, 0.5405, 0.5445, 0.5402,
        0.5379, 0.5374, 0.5379, 0.5364, 0.5427, 0.5389, 0.5412],
       device='cuda:0') torch.Size([16])
percent tensor([0.5926, 0.6054, 0.4508, 0.4238, 0.4208, 0.5554, 0.5638, 0.4612, 0.5120,
        0.5812, 0.5946, 0.4935, 0.6044, 0.5620, 0.5696, 0.5906],
       device='cuda:0') torch.Size([16])
percent tensor([0.6564, 0.5555, 0.6690, 0.6804, 0.6641, 0.7271, 0.6091, 0.6747, 0.6258,
        0.5761, 0.6152, 0.5922, 0.5250, 0.6693, 0.6176, 0.6898],
       device='cuda:0') torch.Size([16])
percent tensor([0.6353, 0.6326, 0.6218, 0.5777, 0.5817, 0.6022, 0.6316, 0.6207, 0.6331,
        0.6104, 0.6041, 0.6099, 0.6360, 0.6177, 0.6290, 0.6033],
       device='cuda:0') torch.Size([16])
percent tensor([0.5855, 0.6621, 0.6712, 0.7097, 0.7171, 0.7081, 0.6601, 0.6383, 0.6992,
        0.6135, 0.6703, 0.7285, 0.6252, 0.7309, 0.6278, 0.6220],
       device='cuda:0') torch.Size([16])
percent tensor([0.5911, 0.6574, 0.6061, 0.5386, 0.6841, 0.7228, 0.5931, 0.4599, 0.6224,
        0.6591, 0.6387, 0.4830, 0.6402, 0.6089, 0.4899, 0.6058],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9994, 0.9996, 0.9997, 0.9998, 0.9991, 0.9996, 0.9996, 0.9997,
        0.9995, 0.9997, 0.9997, 0.9997, 0.9993, 0.9992, 0.9996],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 98 | Batch_idx: 0 |  Loss: (0.2583) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 98 | Batch_idx: 10 |  Loss: (0.2438) |  Loss2: (0.0000) | Acc: (92.00%) (1297/1408)
Epoch: 98 | Batch_idx: 20 |  Loss: (0.2447) |  Loss2: (0.0000) | Acc: (91.00%) (2464/2688)
Epoch: 98 | Batch_idx: 30 |  Loss: (0.2564) |  Loss2: (0.0000) | Acc: (91.00%) (3616/3968)
Epoch: 98 | Batch_idx: 40 |  Loss: (0.2710) |  Loss2: (0.0000) | Acc: (90.00%) (4749/5248)
Epoch: 98 | Batch_idx: 50 |  Loss: (0.2739) |  Loss2: (0.0000) | Acc: (90.00%) (5892/6528)
Epoch: 98 | Batch_idx: 60 |  Loss: (0.2788) |  Loss2: (0.0000) | Acc: (90.00%) (7028/7808)
Epoch: 98 | Batch_idx: 70 |  Loss: (0.2832) |  Loss2: (0.0000) | Acc: (89.00%) (8168/9088)
Epoch: 98 | Batch_idx: 80 |  Loss: (0.2832) |  Loss2: (0.0000) | Acc: (89.00%) (9313/10368)
Epoch: 98 | Batch_idx: 90 |  Loss: (0.2860) |  Loss2: (0.0000) | Acc: (89.00%) (10452/11648)
Epoch: 98 | Batch_idx: 100 |  Loss: (0.2832) |  Loss2: (0.0000) | Acc: (89.00%) (11614/12928)
Epoch: 98 | Batch_idx: 110 |  Loss: (0.2849) |  Loss2: (0.0000) | Acc: (89.00%) (12760/14208)
Epoch: 98 | Batch_idx: 120 |  Loss: (0.2838) |  Loss2: (0.0000) | Acc: (89.00%) (13922/15488)
Epoch: 98 | Batch_idx: 130 |  Loss: (0.2811) |  Loss2: (0.0000) | Acc: (89.00%) (15075/16768)
Epoch: 98 | Batch_idx: 140 |  Loss: (0.2810) |  Loss2: (0.0000) | Acc: (89.00%) (16217/18048)
Epoch: 98 | Batch_idx: 150 |  Loss: (0.2815) |  Loss2: (0.0000) | Acc: (89.00%) (17361/19328)
Epoch: 98 | Batch_idx: 160 |  Loss: (0.2794) |  Loss2: (0.0000) | Acc: (89.00%) (18537/20608)
Epoch: 98 | Batch_idx: 170 |  Loss: (0.2776) |  Loss2: (0.0000) | Acc: (90.00%) (19708/21888)
Epoch: 98 | Batch_idx: 180 |  Loss: (0.2767) |  Loss2: (0.0000) | Acc: (90.00%) (20869/23168)
Epoch: 98 | Batch_idx: 190 |  Loss: (0.2750) |  Loss2: (0.0000) | Acc: (90.00%) (22042/24448)
Epoch: 98 | Batch_idx: 200 |  Loss: (0.2742) |  Loss2: (0.0000) | Acc: (90.00%) (23194/25728)
Epoch: 98 | Batch_idx: 210 |  Loss: (0.2727) |  Loss2: (0.0000) | Acc: (90.00%) (24363/27008)
Epoch: 98 | Batch_idx: 220 |  Loss: (0.2719) |  Loss2: (0.0000) | Acc: (90.00%) (25522/28288)
Epoch: 98 | Batch_idx: 230 |  Loss: (0.2708) |  Loss2: (0.0000) | Acc: (90.00%) (26681/29568)
Epoch: 98 | Batch_idx: 240 |  Loss: (0.2689) |  Loss2: (0.0000) | Acc: (90.00%) (27855/30848)
Epoch: 98 | Batch_idx: 250 |  Loss: (0.2701) |  Loss2: (0.0000) | Acc: (90.00%) (29000/32128)
Epoch: 98 | Batch_idx: 260 |  Loss: (0.2691) |  Loss2: (0.0000) | Acc: (90.00%) (30168/33408)
Epoch: 98 | Batch_idx: 270 |  Loss: (0.2683) |  Loss2: (0.0000) | Acc: (90.00%) (31345/34688)
Epoch: 98 | Batch_idx: 280 |  Loss: (0.2671) |  Loss2: (0.0000) | Acc: (90.00%) (32527/35968)
Epoch: 98 | Batch_idx: 290 |  Loss: (0.2665) |  Loss2: (0.0000) | Acc: (90.00%) (33697/37248)
Epoch: 98 | Batch_idx: 300 |  Loss: (0.2649) |  Loss2: (0.0000) | Acc: (90.00%) (34880/38528)
Epoch: 98 | Batch_idx: 310 |  Loss: (0.2638) |  Loss2: (0.0000) | Acc: (90.00%) (36063/39808)
Epoch: 98 | Batch_idx: 320 |  Loss: (0.2633) |  Loss2: (0.0000) | Acc: (90.00%) (37225/41088)
Epoch: 98 | Batch_idx: 330 |  Loss: (0.2632) |  Loss2: (0.0000) | Acc: (90.00%) (38387/42368)
Epoch: 98 | Batch_idx: 340 |  Loss: (0.2627) |  Loss2: (0.0000) | Acc: (90.00%) (39557/43648)
Epoch: 98 | Batch_idx: 350 |  Loss: (0.2630) |  Loss2: (0.0000) | Acc: (90.00%) (40698/44928)
Epoch: 98 | Batch_idx: 360 |  Loss: (0.2628) |  Loss2: (0.0000) | Acc: (90.00%) (41862/46208)
Epoch: 98 | Batch_idx: 370 |  Loss: (0.2620) |  Loss2: (0.0000) | Acc: (90.00%) (43039/47488)
Epoch: 98 | Batch_idx: 380 |  Loss: (0.2616) |  Loss2: (0.0000) | Acc: (90.00%) (44220/48768)
Epoch: 98 | Batch_idx: 390 |  Loss: (0.2615) |  Loss2: (0.0000) | Acc: (90.00%) (45350/50000)
# TEST : Loss: (0.4164) | Acc: (86.00%) (8662/10000)
percent tensor([0.5366, 0.5508, 0.5599, 0.5309, 0.5595, 0.5567, 0.5579, 0.5434, 0.5358,
        0.5509, 0.5461, 0.5614, 0.5367, 0.5142, 0.5567, 0.5363],
       device='cuda:0') torch.Size([16])
percent tensor([0.5292, 0.5298, 0.5315, 0.5358, 0.5339, 0.5312, 0.5314, 0.5348, 0.5315,
        0.5292, 0.5285, 0.5290, 0.5279, 0.5337, 0.5302, 0.5316],
       device='cuda:0') torch.Size([16])
percent tensor([0.5719, 0.5982, 0.4291, 0.4088, 0.4057, 0.5199, 0.5525, 0.4507, 0.4933,
        0.5665, 0.5781, 0.4804, 0.5827, 0.5521, 0.5540, 0.5725],
       device='cuda:0') torch.Size([16])
percent tensor([0.6800, 0.5926, 0.6756, 0.7029, 0.6780, 0.7370, 0.6384, 0.6966, 0.6518,
        0.6057, 0.6497, 0.6150, 0.5580, 0.6926, 0.6535, 0.7092],
       device='cuda:0') torch.Size([16])
percent tensor([0.6503, 0.6554, 0.6350, 0.5777, 0.5821, 0.6232, 0.6514, 0.6139, 0.6601,
        0.6289, 0.6379, 0.6294, 0.6668, 0.6509, 0.6377, 0.6218],
       device='cuda:0') torch.Size([16])
percent tensor([0.6328, 0.6996, 0.7013, 0.7534, 0.7545, 0.7561, 0.6958, 0.6781, 0.7403,
        0.6437, 0.7074, 0.7500, 0.6650, 0.7740, 0.6700, 0.6685],
       device='cuda:0') torch.Size([16])
percent tensor([0.5691, 0.6506, 0.5758, 0.4766, 0.6594, 0.7098, 0.5628, 0.4502, 0.5978,
        0.6441, 0.6194, 0.4283, 0.6420, 0.5701, 0.4556, 0.5750],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9995, 0.9995, 0.9997, 0.9998, 0.9984, 0.9997, 0.9996, 0.9998,
        0.9996, 0.9998, 0.9998, 0.9997, 0.9994, 0.9991, 0.9996],
       device='cuda:0') torch.Size([16])
Epoch: 99 | Batch_idx: 0 |  Loss: (0.1499) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 99 | Batch_idx: 10 |  Loss: (0.2135) |  Loss2: (0.0000) | Acc: (92.00%) (1303/1408)
Epoch: 99 | Batch_idx: 20 |  Loss: (0.2202) |  Loss2: (0.0000) | Acc: (92.00%) (2488/2688)
Epoch: 99 | Batch_idx: 30 |  Loss: (0.2340) |  Loss2: (0.0000) | Acc: (91.00%) (3643/3968)
Epoch: 99 | Batch_idx: 40 |  Loss: (0.2365) |  Loss2: (0.0000) | Acc: (91.00%) (4815/5248)
Epoch: 99 | Batch_idx: 50 |  Loss: (0.2352) |  Loss2: (0.0000) | Acc: (91.00%) (5988/6528)
Epoch: 99 | Batch_idx: 60 |  Loss: (0.2436) |  Loss2: (0.0000) | Acc: (91.00%) (7141/7808)
Epoch: 99 | Batch_idx: 70 |  Loss: (0.2443) |  Loss2: (0.0000) | Acc: (91.00%) (8305/9088)
Epoch: 99 | Batch_idx: 80 |  Loss: (0.2478) |  Loss2: (0.0000) | Acc: (91.00%) (9456/10368)
Epoch: 99 | Batch_idx: 90 |  Loss: (0.2455) |  Loss2: (0.0000) | Acc: (91.00%) (10630/11648)
Epoch: 99 | Batch_idx: 100 |  Loss: (0.2465) |  Loss2: (0.0000) | Acc: (91.00%) (11793/12928)
Epoch: 99 | Batch_idx: 110 |  Loss: (0.2437) |  Loss2: (0.0000) | Acc: (91.00%) (12979/14208)
Epoch: 99 | Batch_idx: 120 |  Loss: (0.2433) |  Loss2: (0.0000) | Acc: (91.00%) (14151/15488)
Epoch: 99 | Batch_idx: 130 |  Loss: (0.2453) |  Loss2: (0.0000) | Acc: (91.00%) (15311/16768)
Epoch: 99 | Batch_idx: 140 |  Loss: (0.2446) |  Loss2: (0.0000) | Acc: (91.00%) (16488/18048)
Epoch: 99 | Batch_idx: 150 |  Loss: (0.2448) |  Loss2: (0.0000) | Acc: (91.00%) (17669/19328)
Epoch: 99 | Batch_idx: 160 |  Loss: (0.2456) |  Loss2: (0.0000) | Acc: (91.00%) (18831/20608)
Epoch: 99 | Batch_idx: 170 |  Loss: (0.2445) |  Loss2: (0.0000) | Acc: (91.00%) (20012/21888)
Epoch: 99 | Batch_idx: 180 |  Loss: (0.2441) |  Loss2: (0.0000) | Acc: (91.00%) (21191/23168)
Epoch: 99 | Batch_idx: 190 |  Loss: (0.2429) |  Loss2: (0.0000) | Acc: (91.00%) (22365/24448)
Epoch: 99 | Batch_idx: 200 |  Loss: (0.2419) |  Loss2: (0.0000) | Acc: (91.00%) (23546/25728)
Epoch: 99 | Batch_idx: 210 |  Loss: (0.2407) |  Loss2: (0.0000) | Acc: (91.00%) (24736/27008)
Epoch: 99 | Batch_idx: 220 |  Loss: (0.2393) |  Loss2: (0.0000) | Acc: (91.00%) (25918/28288)
Epoch: 99 | Batch_idx: 230 |  Loss: (0.2388) |  Loss2: (0.0000) | Acc: (91.00%) (27096/29568)
Epoch: 99 | Batch_idx: 240 |  Loss: (0.2377) |  Loss2: (0.0000) | Acc: (91.00%) (28294/30848)
Epoch: 99 | Batch_idx: 250 |  Loss: (0.2378) |  Loss2: (0.0000) | Acc: (91.00%) (29467/32128)
Epoch: 99 | Batch_idx: 260 |  Loss: (0.2379) |  Loss2: (0.0000) | Acc: (91.00%) (30637/33408)
Epoch: 99 | Batch_idx: 270 |  Loss: (0.2379) |  Loss2: (0.0000) | Acc: (91.00%) (31815/34688)
Epoch: 99 | Batch_idx: 280 |  Loss: (0.2383) |  Loss2: (0.0000) | Acc: (91.00%) (32992/35968)
Epoch: 99 | Batch_idx: 290 |  Loss: (0.2378) |  Loss2: (0.0000) | Acc: (91.00%) (34170/37248)
Epoch: 99 | Batch_idx: 300 |  Loss: (0.2379) |  Loss2: (0.0000) | Acc: (91.00%) (35350/38528)
Epoch: 99 | Batch_idx: 310 |  Loss: (0.2379) |  Loss2: (0.0000) | Acc: (91.00%) (36527/39808)
Epoch: 99 | Batch_idx: 320 |  Loss: (0.2374) |  Loss2: (0.0000) | Acc: (91.00%) (37712/41088)
Epoch: 99 | Batch_idx: 330 |  Loss: (0.2373) |  Loss2: (0.0000) | Acc: (91.00%) (38900/42368)
Epoch: 99 | Batch_idx: 340 |  Loss: (0.2368) |  Loss2: (0.0000) | Acc: (91.00%) (40082/43648)
Epoch: 99 | Batch_idx: 350 |  Loss: (0.2360) |  Loss2: (0.0000) | Acc: (91.00%) (41275/44928)
Epoch: 99 | Batch_idx: 360 |  Loss: (0.2361) |  Loss2: (0.0000) | Acc: (91.00%) (42445/46208)
Epoch: 99 | Batch_idx: 370 |  Loss: (0.2361) |  Loss2: (0.0000) | Acc: (91.00%) (43623/47488)
Epoch: 99 | Batch_idx: 380 |  Loss: (0.2353) |  Loss2: (0.0000) | Acc: (91.00%) (44811/48768)
Epoch: 99 | Batch_idx: 390 |  Loss: (0.2357) |  Loss2: (0.0000) | Acc: (91.00%) (45936/50000)
# TEST : Loss: (0.3932) | Acc: (87.00%) (8738/10000)
percent tensor([0.5321, 0.5470, 0.5600, 0.5287, 0.5581, 0.5506, 0.5547, 0.5427, 0.5326,
        0.5490, 0.5419, 0.5615, 0.5330, 0.5101, 0.5522, 0.5319],
       device='cuda:0') torch.Size([16])
percent tensor([0.5252, 0.5258, 0.5279, 0.5318, 0.5299, 0.5270, 0.5272, 0.5307, 0.5276,
        0.5252, 0.5244, 0.5255, 0.5241, 0.5294, 0.5260, 0.5273],
       device='cuda:0') torch.Size([16])
percent tensor([0.5782, 0.5973, 0.4384, 0.4177, 0.4155, 0.5294, 0.5580, 0.4577, 0.4990,
        0.5701, 0.5838, 0.4873, 0.5872, 0.5551, 0.5581, 0.5781],
       device='cuda:0') torch.Size([16])
percent tensor([0.6750, 0.5915, 0.6771, 0.7013, 0.6764, 0.7292, 0.6368, 0.6930, 0.6500,
        0.6039, 0.6471, 0.6231, 0.5591, 0.6884, 0.6509, 0.7002],
       device='cuda:0') torch.Size([16])
percent tensor([0.6438, 0.6543, 0.6262, 0.5725, 0.5734, 0.6252, 0.6501, 0.5996, 0.6644,
        0.6241, 0.6433, 0.6234, 0.6657, 0.6594, 0.6327, 0.6176],
       device='cuda:0') torch.Size([16])
percent tensor([0.6249, 0.6894, 0.6941, 0.7508, 0.7506, 0.7518, 0.6894, 0.6694, 0.7330,
        0.6375, 0.7043, 0.7440, 0.6554, 0.7659, 0.6620, 0.6631],
       device='cuda:0') torch.Size([16])
percent tensor([0.5642, 0.6439, 0.5620, 0.4618, 0.6489, 0.7174, 0.5556, 0.4427, 0.5950,
        0.6358, 0.6176, 0.4030, 0.6338, 0.5737, 0.4460, 0.5747],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9996, 0.9995, 0.9997, 0.9998, 0.9985, 0.9996, 0.9996, 0.9998,
        0.9996, 0.9998, 0.9998, 0.9997, 0.9994, 0.9991, 0.9996],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 100 | Batch_idx: 0 |  Loss: (0.2103) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 100 | Batch_idx: 10 |  Loss: (0.2190) |  Loss2: (0.0000) | Acc: (91.00%) (1294/1408)
Epoch: 100 | Batch_idx: 20 |  Loss: (0.2090) |  Loss2: (0.0000) | Acc: (92.00%) (2490/2688)
Epoch: 100 | Batch_idx: 30 |  Loss: (0.2144) |  Loss2: (0.0000) | Acc: (92.00%) (3662/3968)
Epoch: 100 | Batch_idx: 40 |  Loss: (0.2172) |  Loss2: (0.0000) | Acc: (92.00%) (4845/5248)
Epoch: 100 | Batch_idx: 50 |  Loss: (0.2154) |  Loss2: (0.0000) | Acc: (92.00%) (6037/6528)
Epoch: 100 | Batch_idx: 60 |  Loss: (0.2140) |  Loss2: (0.0000) | Acc: (92.00%) (7223/7808)
Epoch: 100 | Batch_idx: 70 |  Loss: (0.2142) |  Loss2: (0.0000) | Acc: (92.00%) (8414/9088)
Epoch: 100 | Batch_idx: 80 |  Loss: (0.2140) |  Loss2: (0.0000) | Acc: (92.00%) (9595/10368)
Epoch: 100 | Batch_idx: 90 |  Loss: (0.2143) |  Loss2: (0.0000) | Acc: (92.00%) (10789/11648)
Epoch: 100 | Batch_idx: 100 |  Loss: (0.2126) |  Loss2: (0.0000) | Acc: (92.00%) (11990/12928)
Epoch: 100 | Batch_idx: 110 |  Loss: (0.2126) |  Loss2: (0.0000) | Acc: (92.00%) (13180/14208)
Epoch: 100 | Batch_idx: 120 |  Loss: (0.2158) |  Loss2: (0.0000) | Acc: (92.00%) (14335/15488)
Epoch: 100 | Batch_idx: 130 |  Loss: (0.2181) |  Loss2: (0.0000) | Acc: (92.00%) (15514/16768)
Epoch: 100 | Batch_idx: 140 |  Loss: (0.2192) |  Loss2: (0.0000) | Acc: (92.00%) (16687/18048)
Epoch: 100 | Batch_idx: 150 |  Loss: (0.2211) |  Loss2: (0.0000) | Acc: (92.00%) (17862/19328)
Epoch: 100 | Batch_idx: 160 |  Loss: (0.2227) |  Loss2: (0.0000) | Acc: (92.00%) (19033/20608)
Epoch: 100 | Batch_idx: 170 |  Loss: (0.2219) |  Loss2: (0.0000) | Acc: (92.00%) (20218/21888)
Epoch: 100 | Batch_idx: 180 |  Loss: (0.2225) |  Loss2: (0.0000) | Acc: (92.00%) (21403/23168)
Epoch: 100 | Batch_idx: 190 |  Loss: (0.2213) |  Loss2: (0.0000) | Acc: (92.00%) (22596/24448)
Epoch: 100 | Batch_idx: 200 |  Loss: (0.2204) |  Loss2: (0.0000) | Acc: (92.00%) (23783/25728)
Epoch: 100 | Batch_idx: 210 |  Loss: (0.2204) |  Loss2: (0.0000) | Acc: (92.00%) (24969/27008)
Epoch: 100 | Batch_idx: 220 |  Loss: (0.2211) |  Loss2: (0.0000) | Acc: (92.00%) (26142/28288)
Epoch: 100 | Batch_idx: 230 |  Loss: (0.2210) |  Loss2: (0.0000) | Acc: (92.00%) (27315/29568)
Epoch: 100 | Batch_idx: 240 |  Loss: (0.2216) |  Loss2: (0.0000) | Acc: (92.00%) (28495/30848)
Epoch: 100 | Batch_idx: 250 |  Loss: (0.2212) |  Loss2: (0.0000) | Acc: (92.00%) (29679/32128)
Epoch: 100 | Batch_idx: 260 |  Loss: (0.2221) |  Loss2: (0.0000) | Acc: (92.00%) (30846/33408)
Epoch: 100 | Batch_idx: 270 |  Loss: (0.2221) |  Loss2: (0.0000) | Acc: (92.00%) (32022/34688)
Epoch: 100 | Batch_idx: 280 |  Loss: (0.2218) |  Loss2: (0.0000) | Acc: (92.00%) (33203/35968)
Epoch: 100 | Batch_idx: 290 |  Loss: (0.2212) |  Loss2: (0.0000) | Acc: (92.00%) (34393/37248)
Epoch: 100 | Batch_idx: 300 |  Loss: (0.2211) |  Loss2: (0.0000) | Acc: (92.00%) (35584/38528)
Epoch: 100 | Batch_idx: 310 |  Loss: (0.2214) |  Loss2: (0.0000) | Acc: (92.00%) (36755/39808)
Epoch: 100 | Batch_idx: 320 |  Loss: (0.2220) |  Loss2: (0.0000) | Acc: (92.00%) (37924/41088)
Epoch: 100 | Batch_idx: 330 |  Loss: (0.2218) |  Loss2: (0.0000) | Acc: (92.00%) (39106/42368)
Epoch: 100 | Batch_idx: 340 |  Loss: (0.2227) |  Loss2: (0.0000) | Acc: (92.00%) (40280/43648)
Epoch: 100 | Batch_idx: 350 |  Loss: (0.2232) |  Loss2: (0.0000) | Acc: (92.00%) (41458/44928)
Epoch: 100 | Batch_idx: 360 |  Loss: (0.2233) |  Loss2: (0.0000) | Acc: (92.00%) (42633/46208)
Epoch: 100 | Batch_idx: 370 |  Loss: (0.2228) |  Loss2: (0.0000) | Acc: (92.00%) (43819/47488)
Epoch: 100 | Batch_idx: 380 |  Loss: (0.2232) |  Loss2: (0.0000) | Acc: (92.00%) (44997/48768)
Epoch: 100 | Batch_idx: 390 |  Loss: (0.2226) |  Loss2: (0.0000) | Acc: (92.00%) (46146/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_100.pth.tar'
# TEST : Loss: (0.3907) | Acc: (87.00%) (8735/10000)
percent tensor([0.5347, 0.5440, 0.5634, 0.5270, 0.5595, 0.5517, 0.5546, 0.5424, 0.5325,
        0.5488, 0.5411, 0.5631, 0.5336, 0.5066, 0.5508, 0.5317],
       device='cuda:0') torch.Size([16])
percent tensor([0.5254, 0.5254, 0.5279, 0.5326, 0.5302, 0.5275, 0.5271, 0.5306, 0.5269,
        0.5252, 0.5240, 0.5255, 0.5239, 0.5280, 0.5261, 0.5273],
       device='cuda:0') torch.Size([16])
percent tensor([0.5836, 0.5921, 0.4574, 0.4051, 0.4211, 0.5378, 0.5627, 0.4610, 0.5076,
        0.5772, 0.5927, 0.5086, 0.5914, 0.5629, 0.5572, 0.5741],
       device='cuda:0') torch.Size([16])
percent tensor([0.6745, 0.5744, 0.6867, 0.6969, 0.6833, 0.7385, 0.6399, 0.6877, 0.6474,
        0.6019, 0.6355, 0.6332, 0.5612, 0.6683, 0.6463, 0.6988],
       device='cuda:0') torch.Size([16])
percent tensor([0.6206, 0.6323, 0.6199, 0.5843, 0.5721, 0.6170, 0.6323, 0.5991, 0.6534,
        0.6111, 0.6293, 0.6069, 0.6422, 0.6503, 0.6164, 0.6034],
       device='cuda:0') torch.Size([16])
percent tensor([0.6176, 0.6716, 0.6762, 0.7456, 0.7377, 0.7381, 0.6890, 0.6508, 0.7117,
        0.6373, 0.6895, 0.7304, 0.6411, 0.7503, 0.6523, 0.6439],
       device='cuda:0') torch.Size([16])
percent tensor([0.5791, 0.6628, 0.5921, 0.4842, 0.6650, 0.6949, 0.5752, 0.4515, 0.6261,
        0.6509, 0.6340, 0.4640, 0.6354, 0.5998, 0.4577, 0.5702],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9995, 0.9995, 0.9996, 0.9998, 0.9991, 0.9995, 0.9996, 0.9998,
        0.9995, 0.9997, 0.9997, 0.9998, 0.9994, 0.9992, 0.9994],
       device='cuda:0') torch.Size([16])
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(179.9164, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(815.9327, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(808.7521, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1519.4613, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(492.6266, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2243.6353, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4262.1519, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1398.3960, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6161.8589, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11871.5713, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3938.7996, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16640.7227, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 101 | Batch_idx: 0 |  Loss: (0.1134) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 101 | Batch_idx: 10 |  Loss: (0.1948) |  Loss2: (0.0000) | Acc: (93.00%) (1313/1408)
Epoch: 101 | Batch_idx: 20 |  Loss: (0.2079) |  Loss2: (0.0000) | Acc: (92.00%) (2486/2688)
Epoch: 101 | Batch_idx: 30 |  Loss: (0.2069) |  Loss2: (0.0000) | Acc: (92.00%) (3675/3968)
Epoch: 101 | Batch_idx: 40 |  Loss: (0.2020) |  Loss2: (0.0000) | Acc: (92.00%) (4868/5248)
Epoch: 101 | Batch_idx: 50 |  Loss: (0.2078) |  Loss2: (0.0000) | Acc: (92.00%) (6050/6528)
Epoch: 101 | Batch_idx: 60 |  Loss: (0.2089) |  Loss2: (0.0000) | Acc: (92.00%) (7238/7808)
Epoch: 101 | Batch_idx: 70 |  Loss: (0.2103) |  Loss2: (0.0000) | Acc: (92.00%) (8421/9088)
Epoch: 101 | Batch_idx: 80 |  Loss: (0.2122) |  Loss2: (0.0000) | Acc: (92.00%) (9598/10368)
Epoch: 101 | Batch_idx: 90 |  Loss: (0.2099) |  Loss2: (0.0000) | Acc: (92.00%) (10799/11648)
Epoch: 101 | Batch_idx: 100 |  Loss: (0.2116) |  Loss2: (0.0000) | Acc: (92.00%) (11977/12928)
Epoch: 101 | Batch_idx: 110 |  Loss: (0.2129) |  Loss2: (0.0000) | Acc: (92.00%) (13155/14208)
Epoch: 101 | Batch_idx: 120 |  Loss: (0.2130) |  Loss2: (0.0000) | Acc: (92.00%) (14338/15488)
Epoch: 101 | Batch_idx: 130 |  Loss: (0.2132) |  Loss2: (0.0000) | Acc: (92.00%) (15525/16768)
Epoch: 101 | Batch_idx: 140 |  Loss: (0.2110) |  Loss2: (0.0000) | Acc: (92.00%) (16732/18048)
Epoch: 101 | Batch_idx: 150 |  Loss: (0.2100) |  Loss2: (0.0000) | Acc: (92.00%) (17934/19328)
Epoch: 101 | Batch_idx: 160 |  Loss: (0.2088) |  Loss2: (0.0000) | Acc: (92.00%) (19134/20608)
Epoch: 101 | Batch_idx: 170 |  Loss: (0.2092) |  Loss2: (0.0000) | Acc: (92.00%) (20321/21888)
Epoch: 101 | Batch_idx: 180 |  Loss: (0.2078) |  Loss2: (0.0000) | Acc: (92.00%) (21525/23168)
Epoch: 101 | Batch_idx: 190 |  Loss: (0.2092) |  Loss2: (0.0000) | Acc: (92.00%) (22709/24448)
Epoch: 101 | Batch_idx: 200 |  Loss: (0.2077) |  Loss2: (0.0000) | Acc: (92.00%) (23905/25728)
Epoch: 101 | Batch_idx: 210 |  Loss: (0.2081) |  Loss2: (0.0000) | Acc: (92.00%) (25090/27008)
Epoch: 101 | Batch_idx: 220 |  Loss: (0.2093) |  Loss2: (0.0000) | Acc: (92.00%) (26264/28288)
Epoch: 101 | Batch_idx: 230 |  Loss: (0.2078) |  Loss2: (0.0000) | Acc: (92.00%) (27467/29568)
Epoch: 101 | Batch_idx: 240 |  Loss: (0.2069) |  Loss2: (0.0000) | Acc: (92.00%) (28665/30848)
Epoch: 101 | Batch_idx: 250 |  Loss: (0.2065) |  Loss2: (0.0000) | Acc: (92.00%) (29856/32128)
Epoch: 101 | Batch_idx: 260 |  Loss: (0.2077) |  Loss2: (0.0000) | Acc: (92.00%) (31040/33408)
Epoch: 101 | Batch_idx: 270 |  Loss: (0.2078) |  Loss2: (0.0000) | Acc: (92.00%) (32234/34688)
Epoch: 101 | Batch_idx: 280 |  Loss: (0.2081) |  Loss2: (0.0000) | Acc: (92.00%) (33427/35968)
Epoch: 101 | Batch_idx: 290 |  Loss: (0.2093) |  Loss2: (0.0000) | Acc: (92.00%) (34605/37248)
Epoch: 101 | Batch_idx: 300 |  Loss: (0.2094) |  Loss2: (0.0000) | Acc: (92.00%) (35795/38528)
Epoch: 101 | Batch_idx: 310 |  Loss: (0.2094) |  Loss2: (0.0000) | Acc: (92.00%) (36984/39808)
Epoch: 101 | Batch_idx: 320 |  Loss: (0.2094) |  Loss2: (0.0000) | Acc: (92.00%) (38167/41088)
Epoch: 101 | Batch_idx: 330 |  Loss: (0.2096) |  Loss2: (0.0000) | Acc: (92.00%) (39344/42368)
Epoch: 101 | Batch_idx: 340 |  Loss: (0.2102) |  Loss2: (0.0000) | Acc: (92.00%) (40531/43648)
Epoch: 101 | Batch_idx: 350 |  Loss: (0.2107) |  Loss2: (0.0000) | Acc: (92.00%) (41718/44928)
Epoch: 101 | Batch_idx: 360 |  Loss: (0.2113) |  Loss2: (0.0000) | Acc: (92.00%) (42904/46208)
Epoch: 101 | Batch_idx: 370 |  Loss: (0.2116) |  Loss2: (0.0000) | Acc: (92.00%) (44083/47488)
Epoch: 101 | Batch_idx: 380 |  Loss: (0.2127) |  Loss2: (0.0000) | Acc: (92.00%) (45265/48768)
Epoch: 101 | Batch_idx: 390 |  Loss: (0.2130) |  Loss2: (0.0000) | Acc: (92.00%) (46410/50000)
# TEST : Loss: (0.4019) | Acc: (87.00%) (8710/10000)
percent tensor([0.5318, 0.5465, 0.5594, 0.5296, 0.5566, 0.5521, 0.5538, 0.5431, 0.5308,
        0.5496, 0.5402, 0.5617, 0.5322, 0.5108, 0.5527, 0.5328],
       device='cuda:0') torch.Size([16])
percent tensor([0.5261, 0.5256, 0.5299, 0.5325, 0.5312, 0.5272, 0.5273, 0.5313, 0.5278,
        0.5258, 0.5250, 0.5264, 0.5248, 0.5282, 0.5260, 0.5278],
       device='cuda:0') torch.Size([16])
percent tensor([0.5812, 0.5972, 0.4485, 0.4240, 0.4169, 0.5422, 0.5597, 0.4621, 0.4888,
        0.5759, 0.5850, 0.4994, 0.5877, 0.5628, 0.5578, 0.5826],
       device='cuda:0') torch.Size([16])
percent tensor([0.6770, 0.5805, 0.7046, 0.7033, 0.6973, 0.7374, 0.6438, 0.6929, 0.6580,
        0.6044, 0.6387, 0.6528, 0.5677, 0.6815, 0.6473, 0.7039],
       device='cuda:0') torch.Size([16])
percent tensor([0.6185, 0.6408, 0.5932, 0.5773, 0.5500, 0.5910, 0.6332, 0.5931, 0.6399,
        0.6138, 0.6184, 0.5932, 0.6401, 0.6451, 0.6122, 0.6024],
       device='cuda:0') torch.Size([16])
percent tensor([0.6370, 0.6983, 0.7078, 0.7555, 0.7594, 0.7563, 0.7171, 0.6677, 0.7374,
        0.6530, 0.7214, 0.7534, 0.6780, 0.7586, 0.6756, 0.6678],
       device='cuda:0') torch.Size([16])
percent tensor([0.5698, 0.6617, 0.5563, 0.4747, 0.6396, 0.6741, 0.5485, 0.4349, 0.6224,
        0.6420, 0.6197, 0.4582, 0.6382, 0.5688, 0.4435, 0.5560],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9992, 0.9997, 0.9997, 0.9999, 0.9987, 0.9995, 0.9997, 0.9997,
        0.9993, 0.9996, 0.9998, 0.9996, 0.9990, 0.9993, 0.9996],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 102 | Batch_idx: 0 |  Loss: (0.1985) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 102 | Batch_idx: 10 |  Loss: (0.2046) |  Loss2: (0.0000) | Acc: (92.00%) (1307/1408)
Epoch: 102 | Batch_idx: 20 |  Loss: (0.2177) |  Loss2: (0.0000) | Acc: (92.00%) (2489/2688)
Epoch: 102 | Batch_idx: 30 |  Loss: (0.2239) |  Loss2: (0.0000) | Acc: (92.00%) (3660/3968)
Epoch: 102 | Batch_idx: 40 |  Loss: (0.2369) |  Loss2: (0.0000) | Acc: (91.00%) (4822/5248)
Epoch: 102 | Batch_idx: 50 |  Loss: (0.2485) |  Loss2: (0.0000) | Acc: (91.00%) (5966/6528)
Epoch: 102 | Batch_idx: 60 |  Loss: (0.2485) |  Loss2: (0.0000) | Acc: (91.00%) (7132/7808)
Epoch: 102 | Batch_idx: 70 |  Loss: (0.2530) |  Loss2: (0.0000) | Acc: (91.00%) (8285/9088)
Epoch: 102 | Batch_idx: 80 |  Loss: (0.2543) |  Loss2: (0.0000) | Acc: (91.00%) (9450/10368)
Epoch: 102 | Batch_idx: 90 |  Loss: (0.2532) |  Loss2: (0.0000) | Acc: (91.00%) (10623/11648)
Epoch: 102 | Batch_idx: 100 |  Loss: (0.2529) |  Loss2: (0.0000) | Acc: (91.00%) (11796/12928)
Epoch: 102 | Batch_idx: 110 |  Loss: (0.2521) |  Loss2: (0.0000) | Acc: (91.00%) (12970/14208)
Epoch: 102 | Batch_idx: 120 |  Loss: (0.2545) |  Loss2: (0.0000) | Acc: (91.00%) (14121/15488)
Epoch: 102 | Batch_idx: 130 |  Loss: (0.2559) |  Loss2: (0.0000) | Acc: (91.00%) (15283/16768)
Epoch: 102 | Batch_idx: 140 |  Loss: (0.2562) |  Loss2: (0.0000) | Acc: (91.00%) (16450/18048)
Epoch: 102 | Batch_idx: 150 |  Loss: (0.2588) |  Loss2: (0.0000) | Acc: (91.00%) (17597/19328)
Epoch: 102 | Batch_idx: 160 |  Loss: (0.2596) |  Loss2: (0.0000) | Acc: (90.00%) (18750/20608)
Epoch: 102 | Batch_idx: 170 |  Loss: (0.2593) |  Loss2: (0.0000) | Acc: (90.00%) (19913/21888)
Epoch: 102 | Batch_idx: 180 |  Loss: (0.2571) |  Loss2: (0.0000) | Acc: (91.00%) (21096/23168)
Epoch: 102 | Batch_idx: 190 |  Loss: (0.2582) |  Loss2: (0.0000) | Acc: (91.00%) (22259/24448)
Epoch: 102 | Batch_idx: 200 |  Loss: (0.2575) |  Loss2: (0.0000) | Acc: (91.00%) (23420/25728)
Epoch: 102 | Batch_idx: 210 |  Loss: (0.2586) |  Loss2: (0.0000) | Acc: (91.00%) (24582/27008)
Epoch: 102 | Batch_idx: 220 |  Loss: (0.2579) |  Loss2: (0.0000) | Acc: (91.00%) (25761/28288)
Epoch: 102 | Batch_idx: 230 |  Loss: (0.2569) |  Loss2: (0.0000) | Acc: (91.00%) (26944/29568)
Epoch: 102 | Batch_idx: 240 |  Loss: (0.2545) |  Loss2: (0.0000) | Acc: (91.00%) (28126/30848)
Epoch: 102 | Batch_idx: 250 |  Loss: (0.2544) |  Loss2: (0.0000) | Acc: (91.00%) (29299/32128)
Epoch: 102 | Batch_idx: 260 |  Loss: (0.2538) |  Loss2: (0.0000) | Acc: (91.00%) (30477/33408)
Epoch: 102 | Batch_idx: 270 |  Loss: (0.2536) |  Loss2: (0.0000) | Acc: (91.00%) (31652/34688)
Epoch: 102 | Batch_idx: 280 |  Loss: (0.2537) |  Loss2: (0.0000) | Acc: (91.00%) (32826/35968)
Epoch: 102 | Batch_idx: 290 |  Loss: (0.2539) |  Loss2: (0.0000) | Acc: (91.00%) (33992/37248)
Epoch: 102 | Batch_idx: 300 |  Loss: (0.2535) |  Loss2: (0.0000) | Acc: (91.00%) (35168/38528)
Epoch: 102 | Batch_idx: 310 |  Loss: (0.2534) |  Loss2: (0.0000) | Acc: (91.00%) (36334/39808)
Epoch: 102 | Batch_idx: 320 |  Loss: (0.2537) |  Loss2: (0.0000) | Acc: (91.00%) (37501/41088)
Epoch: 102 | Batch_idx: 330 |  Loss: (0.2538) |  Loss2: (0.0000) | Acc: (91.00%) (38665/42368)
Epoch: 102 | Batch_idx: 340 |  Loss: (0.2534) |  Loss2: (0.0000) | Acc: (91.00%) (39839/43648)
Epoch: 102 | Batch_idx: 350 |  Loss: (0.2529) |  Loss2: (0.0000) | Acc: (91.00%) (41022/44928)
Epoch: 102 | Batch_idx: 360 |  Loss: (0.2529) |  Loss2: (0.0000) | Acc: (91.00%) (42187/46208)
Epoch: 102 | Batch_idx: 370 |  Loss: (0.2522) |  Loss2: (0.0000) | Acc: (91.00%) (43371/47488)
Epoch: 102 | Batch_idx: 380 |  Loss: (0.2515) |  Loss2: (0.0000) | Acc: (91.00%) (44554/48768)
Epoch: 102 | Batch_idx: 390 |  Loss: (0.2512) |  Loss2: (0.0000) | Acc: (91.00%) (45695/50000)
# TEST : Loss: (0.4128) | Acc: (86.00%) (8653/10000)
percent tensor([0.5376, 0.5566, 0.5608, 0.5355, 0.5616, 0.5630, 0.5623, 0.5469, 0.5375,
        0.5560, 0.5479, 0.5653, 0.5376, 0.5228, 0.5626, 0.5401],
       device='cuda:0') torch.Size([16])
percent tensor([0.5252, 0.5252, 0.5291, 0.5308, 0.5299, 0.5257, 0.5265, 0.5306, 0.5269,
        0.5253, 0.5240, 0.5257, 0.5247, 0.5272, 0.5249, 0.5266],
       device='cuda:0') torch.Size([16])
percent tensor([0.5711, 0.6005, 0.4336, 0.4107, 0.3957, 0.5520, 0.5527, 0.4417, 0.4842,
        0.5739, 0.5848, 0.4901, 0.5798, 0.5793, 0.5573, 0.5789],
       device='cuda:0') torch.Size([16])
percent tensor([0.7028, 0.6022, 0.7224, 0.7280, 0.7207, 0.7694, 0.6677, 0.7159, 0.6790,
        0.6300, 0.6573, 0.6803, 0.5940, 0.7017, 0.6776, 0.7368],
       device='cuda:0') torch.Size([16])
percent tensor([0.6208, 0.6367, 0.6051, 0.5781, 0.5571, 0.5869, 0.6348, 0.6035, 0.6384,
        0.6139, 0.6209, 0.5900, 0.6361, 0.6486, 0.6157, 0.5970],
       device='cuda:0') torch.Size([16])
percent tensor([0.5922, 0.6635, 0.6741, 0.7121, 0.7162, 0.7217, 0.6713, 0.6082, 0.6898,
        0.5960, 0.6723, 0.7228, 0.6364, 0.7088, 0.6184, 0.5998],
       device='cuda:0') torch.Size([16])
percent tensor([0.5636, 0.6626, 0.5551, 0.4544, 0.6476, 0.6687, 0.5287, 0.4301, 0.6319,
        0.6288, 0.6133, 0.4486, 0.6422, 0.5704, 0.4313, 0.5279],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9995, 0.9998, 0.9996, 0.9999, 0.9989, 0.9996, 0.9997, 0.9998,
        0.9994, 0.9997, 0.9998, 0.9998, 0.9991, 0.9993, 0.9994],
       device='cuda:0') torch.Size([16])
Epoch: 103 | Batch_idx: 0 |  Loss: (0.1805) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 103 | Batch_idx: 10 |  Loss: (0.2475) |  Loss2: (0.0000) | Acc: (92.00%) (1300/1408)
Epoch: 103 | Batch_idx: 20 |  Loss: (0.2513) |  Loss2: (0.0000) | Acc: (91.00%) (2462/2688)
Epoch: 103 | Batch_idx: 30 |  Loss: (0.2411) |  Loss2: (0.0000) | Acc: (91.00%) (3642/3968)
Epoch: 103 | Batch_idx: 40 |  Loss: (0.2341) |  Loss2: (0.0000) | Acc: (92.00%) (4833/5248)
Epoch: 103 | Batch_idx: 50 |  Loss: (0.2311) |  Loss2: (0.0000) | Acc: (92.00%) (6020/6528)
Epoch: 103 | Batch_idx: 60 |  Loss: (0.2308) |  Loss2: (0.0000) | Acc: (92.00%) (7195/7808)
Epoch: 103 | Batch_idx: 70 |  Loss: (0.2309) |  Loss2: (0.0000) | Acc: (92.00%) (8368/9088)
Epoch: 103 | Batch_idx: 80 |  Loss: (0.2294) |  Loss2: (0.0000) | Acc: (92.00%) (9546/10368)
Epoch: 103 | Batch_idx: 90 |  Loss: (0.2317) |  Loss2: (0.0000) | Acc: (91.00%) (10714/11648)
Epoch: 103 | Batch_idx: 100 |  Loss: (0.2302) |  Loss2: (0.0000) | Acc: (91.00%) (11890/12928)
Epoch: 103 | Batch_idx: 110 |  Loss: (0.2311) |  Loss2: (0.0000) | Acc: (91.00%) (13060/14208)
Epoch: 103 | Batch_idx: 120 |  Loss: (0.2305) |  Loss2: (0.0000) | Acc: (91.00%) (14243/15488)
Epoch: 103 | Batch_idx: 130 |  Loss: (0.2307) |  Loss2: (0.0000) | Acc: (91.00%) (15421/16768)
Epoch: 103 | Batch_idx: 140 |  Loss: (0.2308) |  Loss2: (0.0000) | Acc: (91.00%) (16600/18048)
Epoch: 103 | Batch_idx: 150 |  Loss: (0.2304) |  Loss2: (0.0000) | Acc: (91.00%) (17780/19328)
Epoch: 103 | Batch_idx: 160 |  Loss: (0.2300) |  Loss2: (0.0000) | Acc: (92.00%) (18968/20608)
Epoch: 103 | Batch_idx: 170 |  Loss: (0.2308) |  Loss2: (0.0000) | Acc: (92.00%) (20141/21888)
Epoch: 103 | Batch_idx: 180 |  Loss: (0.2292) |  Loss2: (0.0000) | Acc: (92.00%) (21326/23168)
Epoch: 103 | Batch_idx: 190 |  Loss: (0.2276) |  Loss2: (0.0000) | Acc: (92.00%) (22516/24448)
Epoch: 103 | Batch_idx: 200 |  Loss: (0.2265) |  Loss2: (0.0000) | Acc: (92.00%) (23709/25728)
Epoch: 103 | Batch_idx: 210 |  Loss: (0.2277) |  Loss2: (0.0000) | Acc: (92.00%) (24864/27008)
Epoch: 103 | Batch_idx: 220 |  Loss: (0.2274) |  Loss2: (0.0000) | Acc: (92.00%) (26059/28288)
Epoch: 103 | Batch_idx: 230 |  Loss: (0.2281) |  Loss2: (0.0000) | Acc: (92.00%) (27231/29568)
Epoch: 103 | Batch_idx: 240 |  Loss: (0.2280) |  Loss2: (0.0000) | Acc: (92.00%) (28412/30848)
Epoch: 103 | Batch_idx: 250 |  Loss: (0.2277) |  Loss2: (0.0000) | Acc: (92.00%) (29595/32128)
Epoch: 103 | Batch_idx: 260 |  Loss: (0.2278) |  Loss2: (0.0000) | Acc: (92.00%) (30783/33408)
Epoch: 103 | Batch_idx: 270 |  Loss: (0.2275) |  Loss2: (0.0000) | Acc: (92.00%) (31968/34688)
Epoch: 103 | Batch_idx: 280 |  Loss: (0.2265) |  Loss2: (0.0000) | Acc: (92.00%) (33177/35968)
Epoch: 103 | Batch_idx: 290 |  Loss: (0.2260) |  Loss2: (0.0000) | Acc: (92.00%) (34363/37248)
Epoch: 103 | Batch_idx: 300 |  Loss: (0.2253) |  Loss2: (0.0000) | Acc: (92.00%) (35555/38528)
Epoch: 103 | Batch_idx: 310 |  Loss: (0.2246) |  Loss2: (0.0000) | Acc: (92.00%) (36745/39808)
Epoch: 103 | Batch_idx: 320 |  Loss: (0.2239) |  Loss2: (0.0000) | Acc: (92.00%) (37934/41088)
Epoch: 103 | Batch_idx: 330 |  Loss: (0.2242) |  Loss2: (0.0000) | Acc: (92.00%) (39119/42368)
Epoch: 103 | Batch_idx: 340 |  Loss: (0.2242) |  Loss2: (0.0000) | Acc: (92.00%) (40299/43648)
Epoch: 103 | Batch_idx: 350 |  Loss: (0.2246) |  Loss2: (0.0000) | Acc: (92.00%) (41475/44928)
Epoch: 103 | Batch_idx: 360 |  Loss: (0.2242) |  Loss2: (0.0000) | Acc: (92.00%) (42670/46208)
Epoch: 103 | Batch_idx: 370 |  Loss: (0.2236) |  Loss2: (0.0000) | Acc: (92.00%) (43864/47488)
Epoch: 103 | Batch_idx: 380 |  Loss: (0.2238) |  Loss2: (0.0000) | Acc: (92.00%) (45037/48768)
Epoch: 103 | Batch_idx: 390 |  Loss: (0.2246) |  Loss2: (0.0000) | Acc: (92.00%) (46165/50000)
# TEST : Loss: (0.3953) | Acc: (87.00%) (8719/10000)
percent tensor([0.5340, 0.5535, 0.5558, 0.5317, 0.5571, 0.5603, 0.5588, 0.5423, 0.5341,
        0.5524, 0.5447, 0.5611, 0.5341, 0.5205, 0.5593, 0.5369],
       device='cuda:0') torch.Size([16])
percent tensor([0.5264, 0.5270, 0.5302, 0.5318, 0.5313, 0.5262, 0.5282, 0.5322, 0.5283,
        0.5269, 0.5255, 0.5271, 0.5261, 0.5290, 0.5261, 0.5279],
       device='cuda:0') torch.Size([16])
percent tensor([0.5759, 0.6052, 0.4349, 0.4117, 0.3951, 0.5626, 0.5549, 0.4397, 0.4901,
        0.5819, 0.5927, 0.4971, 0.5895, 0.5832, 0.5641, 0.5846],
       device='cuda:0') torch.Size([16])
percent tensor([0.6983, 0.5971, 0.7154, 0.7242, 0.7174, 0.7761, 0.6628, 0.7114, 0.6751,
        0.6258, 0.6503, 0.6736, 0.5840, 0.7037, 0.6771, 0.7392],
       device='cuda:0') torch.Size([16])
percent tensor([0.6315, 0.6385, 0.6230, 0.5922, 0.5723, 0.5889, 0.6433, 0.6204, 0.6444,
        0.6174, 0.6249, 0.5988, 0.6370, 0.6439, 0.6260, 0.5987],
       device='cuda:0') torch.Size([16])
percent tensor([0.5796, 0.6629, 0.6626, 0.7039, 0.7064, 0.7148, 0.6635, 0.5939, 0.6873,
        0.5859, 0.6684, 0.7160, 0.6344, 0.7072, 0.6066, 0.5830],
       device='cuda:0') torch.Size([16])
percent tensor([0.5737, 0.6736, 0.5642, 0.4601, 0.6577, 0.6755, 0.5404, 0.4392, 0.6414,
        0.6360, 0.6242, 0.4571, 0.6528, 0.5737, 0.4380, 0.5302],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9995, 0.9998, 0.9997, 0.9999, 0.9990, 0.9996, 0.9997, 0.9998,
        0.9994, 0.9997, 0.9998, 0.9998, 0.9991, 0.9994, 0.9994],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 104 | Batch_idx: 0 |  Loss: (0.1649) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 104 | Batch_idx: 10 |  Loss: (0.1915) |  Loss2: (0.0000) | Acc: (93.00%) (1315/1408)
Epoch: 104 | Batch_idx: 20 |  Loss: (0.2031) |  Loss2: (0.0000) | Acc: (93.00%) (2500/2688)
Epoch: 104 | Batch_idx: 30 |  Loss: (0.1977) |  Loss2: (0.0000) | Acc: (93.00%) (3696/3968)
Epoch: 104 | Batch_idx: 40 |  Loss: (0.1949) |  Loss2: (0.0000) | Acc: (93.00%) (4908/5248)
Epoch: 104 | Batch_idx: 50 |  Loss: (0.1968) |  Loss2: (0.0000) | Acc: (93.00%) (6098/6528)
Epoch: 104 | Batch_idx: 60 |  Loss: (0.1942) |  Loss2: (0.0000) | Acc: (93.00%) (7295/7808)
Epoch: 104 | Batch_idx: 70 |  Loss: (0.1926) |  Loss2: (0.0000) | Acc: (93.00%) (8494/9088)
Epoch: 104 | Batch_idx: 80 |  Loss: (0.1942) |  Loss2: (0.0000) | Acc: (93.00%) (9688/10368)
Epoch: 104 | Batch_idx: 90 |  Loss: (0.1957) |  Loss2: (0.0000) | Acc: (93.00%) (10877/11648)
Epoch: 104 | Batch_idx: 100 |  Loss: (0.1956) |  Loss2: (0.0000) | Acc: (93.00%) (12084/12928)
Epoch: 104 | Batch_idx: 110 |  Loss: (0.1996) |  Loss2: (0.0000) | Acc: (93.00%) (13261/14208)
Epoch: 104 | Batch_idx: 120 |  Loss: (0.2022) |  Loss2: (0.0000) | Acc: (93.00%) (14438/15488)
Epoch: 104 | Batch_idx: 130 |  Loss: (0.2029) |  Loss2: (0.0000) | Acc: (93.00%) (15612/16768)
Epoch: 104 | Batch_idx: 140 |  Loss: (0.2020) |  Loss2: (0.0000) | Acc: (93.00%) (16805/18048)
Epoch: 104 | Batch_idx: 150 |  Loss: (0.2013) |  Loss2: (0.0000) | Acc: (93.00%) (17990/19328)
Epoch: 104 | Batch_idx: 160 |  Loss: (0.2018) |  Loss2: (0.0000) | Acc: (93.00%) (19176/20608)
Epoch: 104 | Batch_idx: 170 |  Loss: (0.2027) |  Loss2: (0.0000) | Acc: (93.00%) (20364/21888)
Epoch: 104 | Batch_idx: 180 |  Loss: (0.2026) |  Loss2: (0.0000) | Acc: (93.00%) (21552/23168)
Epoch: 104 | Batch_idx: 190 |  Loss: (0.2034) |  Loss2: (0.0000) | Acc: (93.00%) (22751/24448)
Epoch: 104 | Batch_idx: 200 |  Loss: (0.2036) |  Loss2: (0.0000) | Acc: (93.00%) (23936/25728)
Epoch: 104 | Batch_idx: 210 |  Loss: (0.2041) |  Loss2: (0.0000) | Acc: (93.00%) (25118/27008)
Epoch: 104 | Batch_idx: 220 |  Loss: (0.2041) |  Loss2: (0.0000) | Acc: (92.00%) (26302/28288)
Epoch: 104 | Batch_idx: 230 |  Loss: (0.2035) |  Loss2: (0.0000) | Acc: (93.00%) (27500/29568)
Epoch: 104 | Batch_idx: 240 |  Loss: (0.2047) |  Loss2: (0.0000) | Acc: (92.00%) (28676/30848)
Epoch: 104 | Batch_idx: 250 |  Loss: (0.2049) |  Loss2: (0.0000) | Acc: (92.00%) (29853/32128)
Epoch: 104 | Batch_idx: 260 |  Loss: (0.2052) |  Loss2: (0.0000) | Acc: (92.00%) (31039/33408)
Epoch: 104 | Batch_idx: 270 |  Loss: (0.2055) |  Loss2: (0.0000) | Acc: (92.00%) (32230/34688)
Epoch: 104 | Batch_idx: 280 |  Loss: (0.2052) |  Loss2: (0.0000) | Acc: (92.00%) (33429/35968)
Epoch: 104 | Batch_idx: 290 |  Loss: (0.2058) |  Loss2: (0.0000) | Acc: (92.00%) (34608/37248)
Epoch: 104 | Batch_idx: 300 |  Loss: (0.2071) |  Loss2: (0.0000) | Acc: (92.00%) (35779/38528)
Epoch: 104 | Batch_idx: 310 |  Loss: (0.2074) |  Loss2: (0.0000) | Acc: (92.00%) (36957/39808)
Epoch: 104 | Batch_idx: 320 |  Loss: (0.2073) |  Loss2: (0.0000) | Acc: (92.00%) (38142/41088)
Epoch: 104 | Batch_idx: 330 |  Loss: (0.2065) |  Loss2: (0.0000) | Acc: (92.00%) (39343/42368)
Epoch: 104 | Batch_idx: 340 |  Loss: (0.2071) |  Loss2: (0.0000) | Acc: (92.00%) (40521/43648)
Epoch: 104 | Batch_idx: 350 |  Loss: (0.2076) |  Loss2: (0.0000) | Acc: (92.00%) (41705/44928)
Epoch: 104 | Batch_idx: 360 |  Loss: (0.2075) |  Loss2: (0.0000) | Acc: (92.00%) (42898/46208)
Epoch: 104 | Batch_idx: 370 |  Loss: (0.2074) |  Loss2: (0.0000) | Acc: (92.00%) (44091/47488)
Epoch: 104 | Batch_idx: 380 |  Loss: (0.2072) |  Loss2: (0.0000) | Acc: (92.00%) (45273/48768)
Epoch: 104 | Batch_idx: 390 |  Loss: (0.2073) |  Loss2: (0.0000) | Acc: (92.00%) (46415/50000)
# TEST : Loss: (0.4199) | Acc: (86.00%) (8658/10000)
percent tensor([0.5397, 0.5523, 0.5637, 0.5309, 0.5638, 0.5632, 0.5621, 0.5432, 0.5381,
        0.5546, 0.5480, 0.5685, 0.5382, 0.5177, 0.5594, 0.5379],
       device='cuda:0') torch.Size([16])
percent tensor([0.5260, 0.5272, 0.5284, 0.5319, 0.5310, 0.5260, 0.5291, 0.5319, 0.5279,
        0.5266, 0.5255, 0.5269, 0.5252, 0.5300, 0.5264, 0.5280],
       device='cuda:0') torch.Size([16])
percent tensor([0.5984, 0.6044, 0.4714, 0.4188, 0.4227, 0.5739, 0.5671, 0.4411, 0.5135,
        0.5949, 0.6075, 0.5212, 0.6154, 0.5782, 0.5755, 0.5931],
       device='cuda:0') torch.Size([16])
percent tensor([0.7098, 0.6058, 0.7130, 0.7300, 0.7128, 0.7716, 0.6691, 0.7217, 0.6783,
        0.6369, 0.6627, 0.6646, 0.5878, 0.7168, 0.6860, 0.7436],
       device='cuda:0') torch.Size([16])
percent tensor([0.6521, 0.6464, 0.6328, 0.5962, 0.5935, 0.6165, 0.6502, 0.6141, 0.6589,
        0.6234, 0.6307, 0.6153, 0.6501, 0.6391, 0.6366, 0.6106],
       device='cuda:0') torch.Size([16])
percent tensor([0.5761, 0.6412, 0.6479, 0.6983, 0.7032, 0.7030, 0.6696, 0.6031, 0.6872,
        0.5956, 0.6645, 0.7189, 0.6348, 0.7232, 0.6084, 0.5952],
       device='cuda:0') torch.Size([16])
percent tensor([0.5675, 0.6344, 0.5643, 0.4740, 0.6381, 0.7076, 0.5485, 0.4147, 0.6453,
        0.6403, 0.6315, 0.4394, 0.6410, 0.5948, 0.4368, 0.5456],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9993, 0.9997, 0.9996, 0.9999, 0.9991, 0.9996, 0.9997, 0.9997,
        0.9989, 0.9997, 0.9997, 0.9997, 0.9994, 0.9994, 0.9997],
       device='cuda:0') torch.Size([16])
Epoch: 105 | Batch_idx: 0 |  Loss: (0.1709) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 105 | Batch_idx: 10 |  Loss: (0.1991) |  Loss2: (0.0000) | Acc: (93.00%) (1310/1408)
Epoch: 105 | Batch_idx: 20 |  Loss: (0.1969) |  Loss2: (0.0000) | Acc: (93.00%) (2506/2688)
Epoch: 105 | Batch_idx: 30 |  Loss: (0.1986) |  Loss2: (0.0000) | Acc: (93.00%) (3702/3968)
Epoch: 105 | Batch_idx: 40 |  Loss: (0.1945) |  Loss2: (0.0000) | Acc: (93.00%) (4898/5248)
Epoch: 105 | Batch_idx: 50 |  Loss: (0.1956) |  Loss2: (0.0000) | Acc: (93.00%) (6092/6528)
Epoch: 105 | Batch_idx: 60 |  Loss: (0.1982) |  Loss2: (0.0000) | Acc: (93.00%) (7279/7808)
Epoch: 105 | Batch_idx: 70 |  Loss: (0.1995) |  Loss2: (0.0000) | Acc: (93.00%) (8470/9088)
Epoch: 105 | Batch_idx: 80 |  Loss: (0.1994) |  Loss2: (0.0000) | Acc: (93.00%) (9666/10368)
Epoch: 105 | Batch_idx: 90 |  Loss: (0.1985) |  Loss2: (0.0000) | Acc: (93.00%) (10856/11648)
Epoch: 105 | Batch_idx: 100 |  Loss: (0.1987) |  Loss2: (0.0000) | Acc: (93.00%) (12042/12928)
Epoch: 105 | Batch_idx: 110 |  Loss: (0.1967) |  Loss2: (0.0000) | Acc: (93.00%) (13238/14208)
Epoch: 105 | Batch_idx: 120 |  Loss: (0.1998) |  Loss2: (0.0000) | Acc: (93.00%) (14416/15488)
Epoch: 105 | Batch_idx: 130 |  Loss: (0.2005) |  Loss2: (0.0000) | Acc: (93.00%) (15606/16768)
Epoch: 105 | Batch_idx: 140 |  Loss: (0.2027) |  Loss2: (0.0000) | Acc: (93.00%) (16798/18048)
Epoch: 105 | Batch_idx: 150 |  Loss: (0.2030) |  Loss2: (0.0000) | Acc: (93.00%) (17983/19328)
Epoch: 105 | Batch_idx: 160 |  Loss: (0.2065) |  Loss2: (0.0000) | Acc: (92.00%) (19144/20608)
Epoch: 105 | Batch_idx: 170 |  Loss: (0.2055) |  Loss2: (0.0000) | Acc: (92.00%) (20341/21888)
Epoch: 105 | Batch_idx: 180 |  Loss: (0.2062) |  Loss2: (0.0000) | Acc: (92.00%) (21526/23168)
Epoch: 105 | Batch_idx: 190 |  Loss: (0.2056) |  Loss2: (0.0000) | Acc: (92.00%) (22716/24448)
Epoch: 105 | Batch_idx: 200 |  Loss: (0.2050) |  Loss2: (0.0000) | Acc: (92.00%) (23904/25728)
Epoch: 105 | Batch_idx: 210 |  Loss: (0.2064) |  Loss2: (0.0000) | Acc: (92.00%) (25074/27008)
Epoch: 105 | Batch_idx: 220 |  Loss: (0.2063) |  Loss2: (0.0000) | Acc: (92.00%) (26263/28288)
Epoch: 105 | Batch_idx: 230 |  Loss: (0.2059) |  Loss2: (0.0000) | Acc: (92.00%) (27456/29568)
Epoch: 105 | Batch_idx: 240 |  Loss: (0.2051) |  Loss2: (0.0000) | Acc: (92.00%) (28652/30848)
Epoch: 105 | Batch_idx: 250 |  Loss: (0.2053) |  Loss2: (0.0000) | Acc: (92.00%) (29848/32128)
Epoch: 105 | Batch_idx: 260 |  Loss: (0.2047) |  Loss2: (0.0000) | Acc: (92.00%) (31045/33408)
Epoch: 105 | Batch_idx: 270 |  Loss: (0.2053) |  Loss2: (0.0000) | Acc: (92.00%) (32232/34688)
Epoch: 105 | Batch_idx: 280 |  Loss: (0.2051) |  Loss2: (0.0000) | Acc: (92.00%) (33423/35968)
Epoch: 105 | Batch_idx: 290 |  Loss: (0.2052) |  Loss2: (0.0000) | Acc: (92.00%) (34616/37248)
Epoch: 105 | Batch_idx: 300 |  Loss: (0.2056) |  Loss2: (0.0000) | Acc: (92.00%) (35802/38528)
Epoch: 105 | Batch_idx: 310 |  Loss: (0.2054) |  Loss2: (0.0000) | Acc: (92.00%) (36990/39808)
Epoch: 105 | Batch_idx: 320 |  Loss: (0.2055) |  Loss2: (0.0000) | Acc: (92.00%) (38188/41088)
Epoch: 105 | Batch_idx: 330 |  Loss: (0.2053) |  Loss2: (0.0000) | Acc: (92.00%) (39378/42368)
Epoch: 105 | Batch_idx: 340 |  Loss: (0.2056) |  Loss2: (0.0000) | Acc: (92.00%) (40558/43648)
Epoch: 105 | Batch_idx: 350 |  Loss: (0.2054) |  Loss2: (0.0000) | Acc: (92.00%) (41759/44928)
Epoch: 105 | Batch_idx: 360 |  Loss: (0.2055) |  Loss2: (0.0000) | Acc: (92.00%) (42945/46208)
Epoch: 105 | Batch_idx: 370 |  Loss: (0.2058) |  Loss2: (0.0000) | Acc: (92.00%) (44135/47488)
Epoch: 105 | Batch_idx: 380 |  Loss: (0.2057) |  Loss2: (0.0000) | Acc: (92.00%) (45319/48768)
Epoch: 105 | Batch_idx: 390 |  Loss: (0.2064) |  Loss2: (0.0000) | Acc: (92.00%) (46463/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_105.pth.tar'
# TEST : Loss: (0.3980) | Acc: (87.00%) (8747/10000)
percent tensor([0.5399, 0.5509, 0.5694, 0.5331, 0.5664, 0.5595, 0.5619, 0.5477, 0.5379,
        0.5560, 0.5462, 0.5717, 0.5384, 0.5127, 0.5583, 0.5369],
       device='cuda:0') torch.Size([16])
percent tensor([0.5260, 0.5263, 0.5280, 0.5318, 0.5308, 0.5258, 0.5282, 0.5319, 0.5278,
        0.5259, 0.5255, 0.5258, 0.5254, 0.5288, 0.5260, 0.5279],
       device='cuda:0') torch.Size([16])
percent tensor([0.5836, 0.5959, 0.4492, 0.4102, 0.4025, 0.5615, 0.5464, 0.4408, 0.5051,
        0.5749, 0.5882, 0.4935, 0.6032, 0.5687, 0.5652, 0.5782],
       device='cuda:0') torch.Size([16])
percent tensor([0.7018, 0.5889, 0.7150, 0.7315, 0.7133, 0.7723, 0.6632, 0.7282, 0.6775,
        0.6317, 0.6512, 0.6608, 0.5780, 0.7086, 0.6761, 0.7341],
       device='cuda:0') torch.Size([16])
percent tensor([0.6373, 0.6374, 0.6312, 0.5753, 0.5793, 0.6104, 0.6385, 0.6088, 0.6470,
        0.6129, 0.6249, 0.5975, 0.6361, 0.6250, 0.6305, 0.6003],
       device='cuda:0') torch.Size([16])
percent tensor([0.5632, 0.6393, 0.6321, 0.6921, 0.6901, 0.7076, 0.6597, 0.5891, 0.6924,
        0.5760, 0.6451, 0.7099, 0.6381, 0.7129, 0.6010, 0.5968],
       device='cuda:0') torch.Size([16])
percent tensor([0.5552, 0.6384, 0.5531, 0.4545, 0.6291, 0.6924, 0.5586, 0.4415, 0.6283,
        0.6232, 0.6111, 0.4459, 0.6465, 0.5303, 0.4354, 0.5378],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9996, 0.9996, 0.9996, 0.9999, 0.9992, 0.9996, 0.9997, 0.9998,
        0.9994, 0.9997, 0.9998, 0.9998, 0.9997, 0.9995, 0.9995],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 106 | Batch_idx: 0 |  Loss: (0.2405) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 106 | Batch_idx: 10 |  Loss: (0.1986) |  Loss2: (0.0000) | Acc: (93.00%) (1311/1408)
Epoch: 106 | Batch_idx: 20 |  Loss: (0.1980) |  Loss2: (0.0000) | Acc: (93.00%) (2505/2688)
Epoch: 106 | Batch_idx: 30 |  Loss: (0.2109) |  Loss2: (0.0000) | Acc: (92.00%) (3675/3968)
Epoch: 106 | Batch_idx: 40 |  Loss: (0.2131) |  Loss2: (0.0000) | Acc: (92.00%) (4849/5248)
Epoch: 106 | Batch_idx: 50 |  Loss: (0.2199) |  Loss2: (0.0000) | Acc: (92.00%) (6022/6528)
Epoch: 106 | Batch_idx: 60 |  Loss: (0.2236) |  Loss2: (0.0000) | Acc: (91.00%) (7183/7808)
Epoch: 106 | Batch_idx: 70 |  Loss: (0.2269) |  Loss2: (0.0000) | Acc: (91.00%) (8345/9088)
Epoch: 106 | Batch_idx: 80 |  Loss: (0.2273) |  Loss2: (0.0000) | Acc: (91.00%) (9525/10368)
Epoch: 106 | Batch_idx: 90 |  Loss: (0.2291) |  Loss2: (0.0000) | Acc: (91.00%) (10684/11648)
Epoch: 106 | Batch_idx: 100 |  Loss: (0.2313) |  Loss2: (0.0000) | Acc: (91.00%) (11847/12928)
Epoch: 106 | Batch_idx: 110 |  Loss: (0.2316) |  Loss2: (0.0000) | Acc: (91.00%) (13018/14208)
Epoch: 106 | Batch_idx: 120 |  Loss: (0.2309) |  Loss2: (0.0000) | Acc: (91.00%) (14193/15488)
Epoch: 106 | Batch_idx: 130 |  Loss: (0.2324) |  Loss2: (0.0000) | Acc: (91.00%) (15356/16768)
Epoch: 106 | Batch_idx: 140 |  Loss: (0.2325) |  Loss2: (0.0000) | Acc: (91.00%) (16526/18048)
Epoch: 106 | Batch_idx: 150 |  Loss: (0.2325) |  Loss2: (0.0000) | Acc: (91.00%) (17714/19328)
Epoch: 106 | Batch_idx: 160 |  Loss: (0.2339) |  Loss2: (0.0000) | Acc: (91.00%) (18885/20608)
Epoch: 106 | Batch_idx: 170 |  Loss: (0.2339) |  Loss2: (0.0000) | Acc: (91.00%) (20057/21888)
Epoch: 106 | Batch_idx: 180 |  Loss: (0.2352) |  Loss2: (0.0000) | Acc: (91.00%) (21209/23168)
Epoch: 106 | Batch_idx: 190 |  Loss: (0.2342) |  Loss2: (0.0000) | Acc: (91.00%) (22399/24448)
Epoch: 106 | Batch_idx: 200 |  Loss: (0.2334) |  Loss2: (0.0000) | Acc: (91.00%) (23579/25728)
Epoch: 106 | Batch_idx: 210 |  Loss: (0.2323) |  Loss2: (0.0000) | Acc: (91.00%) (24771/27008)
Epoch: 106 | Batch_idx: 220 |  Loss: (0.2315) |  Loss2: (0.0000) | Acc: (91.00%) (25956/28288)
Epoch: 106 | Batch_idx: 230 |  Loss: (0.2307) |  Loss2: (0.0000) | Acc: (91.00%) (27146/29568)
Epoch: 106 | Batch_idx: 240 |  Loss: (0.2299) |  Loss2: (0.0000) | Acc: (91.00%) (28329/30848)
Epoch: 106 | Batch_idx: 250 |  Loss: (0.2287) |  Loss2: (0.0000) | Acc: (91.00%) (29517/32128)
Epoch: 106 | Batch_idx: 260 |  Loss: (0.2284) |  Loss2: (0.0000) | Acc: (91.00%) (30704/33408)
Epoch: 106 | Batch_idx: 270 |  Loss: (0.2280) |  Loss2: (0.0000) | Acc: (91.00%) (31893/34688)
Epoch: 106 | Batch_idx: 280 |  Loss: (0.2281) |  Loss2: (0.0000) | Acc: (91.00%) (33073/35968)
Epoch: 106 | Batch_idx: 290 |  Loss: (0.2287) |  Loss2: (0.0000) | Acc: (91.00%) (34242/37248)
Epoch: 106 | Batch_idx: 300 |  Loss: (0.2290) |  Loss2: (0.0000) | Acc: (91.00%) (35411/38528)
Epoch: 106 | Batch_idx: 310 |  Loss: (0.2298) |  Loss2: (0.0000) | Acc: (91.00%) (36581/39808)
Epoch: 106 | Batch_idx: 320 |  Loss: (0.2288) |  Loss2: (0.0000) | Acc: (91.00%) (37770/41088)
Epoch: 106 | Batch_idx: 330 |  Loss: (0.2292) |  Loss2: (0.0000) | Acc: (91.00%) (38932/42368)
Epoch: 106 | Batch_idx: 340 |  Loss: (0.2287) |  Loss2: (0.0000) | Acc: (91.00%) (40116/43648)
Epoch: 106 | Batch_idx: 350 |  Loss: (0.2278) |  Loss2: (0.0000) | Acc: (91.00%) (41316/44928)
Epoch: 106 | Batch_idx: 360 |  Loss: (0.2273) |  Loss2: (0.0000) | Acc: (91.00%) (42505/46208)
Epoch: 106 | Batch_idx: 370 |  Loss: (0.2270) |  Loss2: (0.0000) | Acc: (91.00%) (43679/47488)
Epoch: 106 | Batch_idx: 380 |  Loss: (0.2262) |  Loss2: (0.0000) | Acc: (92.00%) (44882/48768)
Epoch: 106 | Batch_idx: 390 |  Loss: (0.2258) |  Loss2: (0.0000) | Acc: (92.00%) (46013/50000)
# TEST : Loss: (0.3878) | Acc: (87.00%) (8749/10000)
percent tensor([0.5474, 0.5588, 0.5747, 0.5382, 0.5729, 0.5646, 0.5698, 0.5549, 0.5461,
        0.5638, 0.5541, 0.5785, 0.5466, 0.5214, 0.5647, 0.5433],
       device='cuda:0') torch.Size([16])
percent tensor([0.5299, 0.5296, 0.5320, 0.5361, 0.5346, 0.5287, 0.5314, 0.5361, 0.5317,
        0.5296, 0.5290, 0.5301, 0.5291, 0.5317, 0.5291, 0.5315],
       device='cuda:0') torch.Size([16])
percent tensor([0.5879, 0.6067, 0.4487, 0.4045, 0.4015, 0.5599, 0.5570, 0.4321, 0.5057,
        0.5876, 0.6008, 0.5064, 0.6154, 0.5696, 0.5721, 0.5821],
       device='cuda:0') torch.Size([16])
percent tensor([0.6970, 0.5883, 0.7127, 0.7280, 0.7115, 0.7633, 0.6598, 0.7251, 0.6724,
        0.6316, 0.6468, 0.6627, 0.5737, 0.7000, 0.6738, 0.7250],
       device='cuda:0') torch.Size([16])
percent tensor([0.6480, 0.6532, 0.6450, 0.5750, 0.6001, 0.6169, 0.6589, 0.6345, 0.6619,
        0.6254, 0.6318, 0.5978, 0.6422, 0.6385, 0.6436, 0.6111],
       device='cuda:0') torch.Size([16])
percent tensor([0.5279, 0.6093, 0.6005, 0.6639, 0.6570, 0.6797, 0.6261, 0.5583, 0.6605,
        0.5430, 0.6082, 0.6846, 0.6012, 0.6863, 0.5725, 0.5564],
       device='cuda:0') torch.Size([16])
percent tensor([0.5586, 0.6222, 0.5215, 0.4355, 0.6037, 0.6945, 0.5528, 0.4234, 0.6096,
        0.6051, 0.5844, 0.4062, 0.6271, 0.5274, 0.4317, 0.5499],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9996, 0.9996, 0.9997, 0.9999, 0.9992, 0.9996, 0.9997, 0.9998,
        0.9994, 0.9996, 0.9998, 0.9997, 0.9996, 0.9994, 0.9994],
       device='cuda:0') torch.Size([16])
Epoch: 107 | Batch_idx: 0 |  Loss: (0.1650) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 107 | Batch_idx: 10 |  Loss: (0.2249) |  Loss2: (0.0000) | Acc: (91.00%) (1288/1408)
Epoch: 107 | Batch_idx: 20 |  Loss: (0.2090) |  Loss2: (0.0000) | Acc: (92.00%) (2486/2688)
Epoch: 107 | Batch_idx: 30 |  Loss: (0.2097) |  Loss2: (0.0000) | Acc: (92.00%) (3681/3968)
Epoch: 107 | Batch_idx: 40 |  Loss: (0.2092) |  Loss2: (0.0000) | Acc: (92.00%) (4866/5248)
Epoch: 107 | Batch_idx: 50 |  Loss: (0.2060) |  Loss2: (0.0000) | Acc: (92.00%) (6062/6528)
Epoch: 107 | Batch_idx: 60 |  Loss: (0.2056) |  Loss2: (0.0000) | Acc: (92.00%) (7259/7808)
Epoch: 107 | Batch_idx: 70 |  Loss: (0.2073) |  Loss2: (0.0000) | Acc: (92.00%) (8435/9088)
Epoch: 107 | Batch_idx: 80 |  Loss: (0.2070) |  Loss2: (0.0000) | Acc: (92.00%) (9621/10368)
Epoch: 107 | Batch_idx: 90 |  Loss: (0.2063) |  Loss2: (0.0000) | Acc: (92.00%) (10808/11648)
Epoch: 107 | Batch_idx: 100 |  Loss: (0.2075) |  Loss2: (0.0000) | Acc: (92.00%) (11998/12928)
Epoch: 107 | Batch_idx: 110 |  Loss: (0.2066) |  Loss2: (0.0000) | Acc: (92.00%) (13190/14208)
Epoch: 107 | Batch_idx: 120 |  Loss: (0.2070) |  Loss2: (0.0000) | Acc: (92.00%) (14368/15488)
Epoch: 107 | Batch_idx: 130 |  Loss: (0.2057) |  Loss2: (0.0000) | Acc: (92.00%) (15571/16768)
Epoch: 107 | Batch_idx: 140 |  Loss: (0.2039) |  Loss2: (0.0000) | Acc: (92.00%) (16776/18048)
Epoch: 107 | Batch_idx: 150 |  Loss: (0.2030) |  Loss2: (0.0000) | Acc: (92.00%) (17970/19328)
Epoch: 107 | Batch_idx: 160 |  Loss: (0.2031) |  Loss2: (0.0000) | Acc: (92.00%) (19164/20608)
Epoch: 107 | Batch_idx: 170 |  Loss: (0.2016) |  Loss2: (0.0000) | Acc: (93.00%) (20369/21888)
Epoch: 107 | Batch_idx: 180 |  Loss: (0.2028) |  Loss2: (0.0000) | Acc: (93.00%) (21558/23168)
Epoch: 107 | Batch_idx: 190 |  Loss: (0.2032) |  Loss2: (0.0000) | Acc: (93.00%) (22746/24448)
Epoch: 107 | Batch_idx: 200 |  Loss: (0.2013) |  Loss2: (0.0000) | Acc: (93.00%) (23958/25728)
Epoch: 107 | Batch_idx: 210 |  Loss: (0.2024) |  Loss2: (0.0000) | Acc: (93.00%) (25133/27008)
Epoch: 107 | Batch_idx: 220 |  Loss: (0.2018) |  Loss2: (0.0000) | Acc: (93.00%) (26331/28288)
Epoch: 107 | Batch_idx: 230 |  Loss: (0.2013) |  Loss2: (0.0000) | Acc: (93.00%) (27527/29568)
Epoch: 107 | Batch_idx: 240 |  Loss: (0.2024) |  Loss2: (0.0000) | Acc: (93.00%) (28694/30848)
Epoch: 107 | Batch_idx: 250 |  Loss: (0.2023) |  Loss2: (0.0000) | Acc: (92.00%) (29878/32128)
Epoch: 107 | Batch_idx: 260 |  Loss: (0.2016) |  Loss2: (0.0000) | Acc: (92.00%) (31068/33408)
Epoch: 107 | Batch_idx: 270 |  Loss: (0.2014) |  Loss2: (0.0000) | Acc: (93.00%) (32263/34688)
Epoch: 107 | Batch_idx: 280 |  Loss: (0.2014) |  Loss2: (0.0000) | Acc: (93.00%) (33455/35968)
Epoch: 107 | Batch_idx: 290 |  Loss: (0.2011) |  Loss2: (0.0000) | Acc: (93.00%) (34650/37248)
Epoch: 107 | Batch_idx: 300 |  Loss: (0.2011) |  Loss2: (0.0000) | Acc: (93.00%) (35846/38528)
Epoch: 107 | Batch_idx: 310 |  Loss: (0.2013) |  Loss2: (0.0000) | Acc: (93.00%) (37033/39808)
Epoch: 107 | Batch_idx: 320 |  Loss: (0.2012) |  Loss2: (0.0000) | Acc: (93.00%) (38224/41088)
Epoch: 107 | Batch_idx: 330 |  Loss: (0.2000) |  Loss2: (0.0000) | Acc: (93.00%) (39433/42368)
Epoch: 107 | Batch_idx: 340 |  Loss: (0.1999) |  Loss2: (0.0000) | Acc: (93.00%) (40634/43648)
Epoch: 107 | Batch_idx: 350 |  Loss: (0.1997) |  Loss2: (0.0000) | Acc: (93.00%) (41823/44928)
Epoch: 107 | Batch_idx: 360 |  Loss: (0.1995) |  Loss2: (0.0000) | Acc: (93.00%) (43026/46208)
Epoch: 107 | Batch_idx: 370 |  Loss: (0.2001) |  Loss2: (0.0000) | Acc: (93.00%) (44207/47488)
Epoch: 107 | Batch_idx: 380 |  Loss: (0.1996) |  Loss2: (0.0000) | Acc: (93.00%) (45398/48768)
Epoch: 107 | Batch_idx: 390 |  Loss: (0.1992) |  Loss2: (0.0000) | Acc: (93.00%) (46549/50000)
# TEST : Loss: (0.3764) | Acc: (87.00%) (8775/10000)
percent tensor([0.5443, 0.5558, 0.5708, 0.5344, 0.5690, 0.5608, 0.5664, 0.5518, 0.5436,
        0.5607, 0.5513, 0.5749, 0.5439, 0.5193, 0.5613, 0.5400],
       device='cuda:0') torch.Size([16])
percent tensor([0.5306, 0.5303, 0.5332, 0.5372, 0.5356, 0.5293, 0.5322, 0.5371, 0.5327,
        0.5304, 0.5298, 0.5310, 0.5298, 0.5326, 0.5298, 0.5324],
       device='cuda:0') torch.Size([16])
percent tensor([0.5941, 0.6140, 0.4533, 0.4084, 0.4069, 0.5608, 0.5668, 0.4393, 0.5126,
        0.5977, 0.6098, 0.5163, 0.6234, 0.5771, 0.5789, 0.5879],
       device='cuda:0') torch.Size([16])
percent tensor([0.6990, 0.5918, 0.7114, 0.7252, 0.7130, 0.7633, 0.6614, 0.7252, 0.6741,
        0.6324, 0.6492, 0.6634, 0.5779, 0.7010, 0.6776, 0.7245],
       device='cuda:0') torch.Size([16])
percent tensor([0.6563, 0.6618, 0.6525, 0.5791, 0.6104, 0.6200, 0.6690, 0.6507, 0.6661,
        0.6336, 0.6328, 0.5982, 0.6446, 0.6435, 0.6502, 0.6174],
       device='cuda:0') torch.Size([16])
percent tensor([0.5390, 0.6168, 0.6085, 0.6698, 0.6604, 0.6913, 0.6318, 0.5613, 0.6671,
        0.5524, 0.6212, 0.6924, 0.6112, 0.6943, 0.5792, 0.5697],
       device='cuda:0') torch.Size([16])
percent tensor([0.5773, 0.6354, 0.5442, 0.4596, 0.6239, 0.7125, 0.5670, 0.4400, 0.6226,
        0.6210, 0.5964, 0.4257, 0.6436, 0.5417, 0.4411, 0.5766],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9996, 0.9995, 0.9997, 0.9998, 0.9991, 0.9996, 0.9997, 0.9998,
        0.9994, 0.9996, 0.9998, 0.9997, 0.9996, 0.9995, 0.9994],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 108 | Batch_idx: 0 |  Loss: (0.1849) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 108 | Batch_idx: 10 |  Loss: (0.1813) |  Loss2: (0.0000) | Acc: (94.00%) (1325/1408)
Epoch: 108 | Batch_idx: 20 |  Loss: (0.1850) |  Loss2: (0.0000) | Acc: (94.00%) (2528/2688)
Epoch: 108 | Batch_idx: 30 |  Loss: (0.1957) |  Loss2: (0.0000) | Acc: (93.00%) (3710/3968)
Epoch: 108 | Batch_idx: 40 |  Loss: (0.1933) |  Loss2: (0.0000) | Acc: (93.00%) (4914/5248)
Epoch: 108 | Batch_idx: 50 |  Loss: (0.1940) |  Loss2: (0.0000) | Acc: (93.00%) (6114/6528)
Epoch: 108 | Batch_idx: 60 |  Loss: (0.1951) |  Loss2: (0.0000) | Acc: (93.00%) (7297/7808)
Epoch: 108 | Batch_idx: 70 |  Loss: (0.1984) |  Loss2: (0.0000) | Acc: (93.00%) (8484/9088)
Epoch: 108 | Batch_idx: 80 |  Loss: (0.1995) |  Loss2: (0.0000) | Acc: (93.00%) (9670/10368)
Epoch: 108 | Batch_idx: 90 |  Loss: (0.1965) |  Loss2: (0.0000) | Acc: (93.00%) (10871/11648)
Epoch: 108 | Batch_idx: 100 |  Loss: (0.1981) |  Loss2: (0.0000) | Acc: (93.00%) (12059/12928)
Epoch: 108 | Batch_idx: 110 |  Loss: (0.1968) |  Loss2: (0.0000) | Acc: (93.00%) (13266/14208)
Epoch: 108 | Batch_idx: 120 |  Loss: (0.1975) |  Loss2: (0.0000) | Acc: (93.00%) (14462/15488)
Epoch: 108 | Batch_idx: 130 |  Loss: (0.1986) |  Loss2: (0.0000) | Acc: (93.00%) (15640/16768)
Epoch: 108 | Batch_idx: 140 |  Loss: (0.1977) |  Loss2: (0.0000) | Acc: (93.00%) (16830/18048)
Epoch: 108 | Batch_idx: 150 |  Loss: (0.1981) |  Loss2: (0.0000) | Acc: (93.00%) (18021/19328)
Epoch: 108 | Batch_idx: 160 |  Loss: (0.1969) |  Loss2: (0.0000) | Acc: (93.00%) (19220/20608)
Epoch: 108 | Batch_idx: 170 |  Loss: (0.1976) |  Loss2: (0.0000) | Acc: (93.00%) (20403/21888)
Epoch: 108 | Batch_idx: 180 |  Loss: (0.1970) |  Loss2: (0.0000) | Acc: (93.00%) (21601/23168)
Epoch: 108 | Batch_idx: 190 |  Loss: (0.1970) |  Loss2: (0.0000) | Acc: (93.00%) (22788/24448)
Epoch: 108 | Batch_idx: 200 |  Loss: (0.1990) |  Loss2: (0.0000) | Acc: (93.00%) (23963/25728)
Epoch: 108 | Batch_idx: 210 |  Loss: (0.1985) |  Loss2: (0.0000) | Acc: (93.00%) (25163/27008)
Epoch: 108 | Batch_idx: 220 |  Loss: (0.1987) |  Loss2: (0.0000) | Acc: (93.00%) (26360/28288)
Epoch: 108 | Batch_idx: 230 |  Loss: (0.1986) |  Loss2: (0.0000) | Acc: (93.00%) (27548/29568)
Epoch: 108 | Batch_idx: 240 |  Loss: (0.1977) |  Loss2: (0.0000) | Acc: (93.00%) (28751/30848)
Epoch: 108 | Batch_idx: 250 |  Loss: (0.1968) |  Loss2: (0.0000) | Acc: (93.00%) (29948/32128)
Epoch: 108 | Batch_idx: 260 |  Loss: (0.1964) |  Loss2: (0.0000) | Acc: (93.00%) (31149/33408)
Epoch: 108 | Batch_idx: 270 |  Loss: (0.1962) |  Loss2: (0.0000) | Acc: (93.00%) (32343/34688)
Epoch: 108 | Batch_idx: 280 |  Loss: (0.1949) |  Loss2: (0.0000) | Acc: (93.00%) (33554/35968)
Epoch: 108 | Batch_idx: 290 |  Loss: (0.1952) |  Loss2: (0.0000) | Acc: (93.00%) (34748/37248)
Epoch: 108 | Batch_idx: 300 |  Loss: (0.1950) |  Loss2: (0.0000) | Acc: (93.00%) (35936/38528)
Epoch: 108 | Batch_idx: 310 |  Loss: (0.1947) |  Loss2: (0.0000) | Acc: (93.00%) (37137/39808)
Epoch: 108 | Batch_idx: 320 |  Loss: (0.1952) |  Loss2: (0.0000) | Acc: (93.00%) (38320/41088)
Epoch: 108 | Batch_idx: 330 |  Loss: (0.1953) |  Loss2: (0.0000) | Acc: (93.00%) (39500/42368)
Epoch: 108 | Batch_idx: 340 |  Loss: (0.1954) |  Loss2: (0.0000) | Acc: (93.00%) (40681/43648)
Epoch: 108 | Batch_idx: 350 |  Loss: (0.1960) |  Loss2: (0.0000) | Acc: (93.00%) (41866/44928)
Epoch: 108 | Batch_idx: 360 |  Loss: (0.1966) |  Loss2: (0.0000) | Acc: (93.00%) (43049/46208)
Epoch: 108 | Batch_idx: 370 |  Loss: (0.1973) |  Loss2: (0.0000) | Acc: (93.00%) (44228/47488)
Epoch: 108 | Batch_idx: 380 |  Loss: (0.1980) |  Loss2: (0.0000) | Acc: (93.00%) (45402/48768)
Epoch: 108 | Batch_idx: 390 |  Loss: (0.1985) |  Loss2: (0.0000) | Acc: (93.00%) (46548/50000)
# TEST : Loss: (0.4472) | Acc: (86.00%) (8649/10000)
percent tensor([0.5429, 0.5595, 0.5617, 0.5346, 0.5636, 0.5615, 0.5663, 0.5480, 0.5437,
        0.5593, 0.5526, 0.5676, 0.5426, 0.5268, 0.5631, 0.5417],
       device='cuda:0') torch.Size([16])
percent tensor([0.5302, 0.5324, 0.5353, 0.5380, 0.5367, 0.5304, 0.5336, 0.5387, 0.5321,
        0.5317, 0.5294, 0.5326, 0.5296, 0.5343, 0.5312, 0.5327],
       device='cuda:0') torch.Size([16])
percent tensor([0.6031, 0.6238, 0.4439, 0.4131, 0.4089, 0.5652, 0.5771, 0.4341, 0.5242,
        0.6045, 0.6238, 0.5108, 0.6300, 0.5886, 0.5863, 0.5999],
       device='cuda:0') torch.Size([16])
percent tensor([0.7034, 0.6179, 0.7029, 0.7290, 0.7132, 0.7721, 0.6714, 0.7227, 0.6663,
        0.6379, 0.6641, 0.6549, 0.5891, 0.7111, 0.6935, 0.7385],
       device='cuda:0') torch.Size([16])
percent tensor([0.6497, 0.6582, 0.6564, 0.5962, 0.6165, 0.6047, 0.6609, 0.6544, 0.6681,
        0.6237, 0.6215, 0.6051, 0.6405, 0.6400, 0.6462, 0.5985],
       device='cuda:0') torch.Size([16])
percent tensor([0.5554, 0.6223, 0.6294, 0.6740, 0.6695, 0.6678, 0.6457, 0.5744, 0.6510,
        0.5677, 0.6210, 0.6852, 0.6229, 0.7009, 0.5634, 0.5607],
       device='cuda:0') torch.Size([16])
percent tensor([0.6067, 0.6648, 0.5793, 0.5061, 0.6277, 0.7039, 0.5847, 0.4558, 0.6479,
        0.6462, 0.6451, 0.4428, 0.6833, 0.5746, 0.4620, 0.5829],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9994, 0.9998, 0.9996, 0.9998, 0.9993, 0.9997, 0.9996, 0.9996,
        0.9995, 0.9998, 0.9997, 0.9998, 0.9993, 0.9992, 0.9993],
       device='cuda:0') torch.Size([16])
Epoch: 109 | Batch_idx: 0 |  Loss: (0.1649) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 109 | Batch_idx: 10 |  Loss: (0.1939) |  Loss2: (0.0000) | Acc: (93.00%) (1321/1408)
Epoch: 109 | Batch_idx: 20 |  Loss: (0.1963) |  Loss2: (0.0000) | Acc: (93.00%) (2510/2688)
Epoch: 109 | Batch_idx: 30 |  Loss: (0.1926) |  Loss2: (0.0000) | Acc: (93.00%) (3725/3968)
Epoch: 109 | Batch_idx: 40 |  Loss: (0.1933) |  Loss2: (0.0000) | Acc: (93.00%) (4902/5248)
Epoch: 109 | Batch_idx: 50 |  Loss: (0.1895) |  Loss2: (0.0000) | Acc: (93.00%) (6110/6528)
Epoch: 109 | Batch_idx: 60 |  Loss: (0.1905) |  Loss2: (0.0000) | Acc: (93.00%) (7311/7808)
Epoch: 109 | Batch_idx: 70 |  Loss: (0.1878) |  Loss2: (0.0000) | Acc: (93.00%) (8513/9088)
Epoch: 109 | Batch_idx: 80 |  Loss: (0.1862) |  Loss2: (0.0000) | Acc: (93.00%) (9721/10368)
Epoch: 109 | Batch_idx: 90 |  Loss: (0.1872) |  Loss2: (0.0000) | Acc: (93.00%) (10907/11648)
Epoch: 109 | Batch_idx: 100 |  Loss: (0.1861) |  Loss2: (0.0000) | Acc: (93.00%) (12114/12928)
Epoch: 109 | Batch_idx: 110 |  Loss: (0.1879) |  Loss2: (0.0000) | Acc: (93.00%) (13304/14208)
Epoch: 109 | Batch_idx: 120 |  Loss: (0.1867) |  Loss2: (0.0000) | Acc: (93.00%) (14505/15488)
Epoch: 109 | Batch_idx: 130 |  Loss: (0.1861) |  Loss2: (0.0000) | Acc: (93.00%) (15700/16768)
Epoch: 109 | Batch_idx: 140 |  Loss: (0.1852) |  Loss2: (0.0000) | Acc: (93.00%) (16900/18048)
Epoch: 109 | Batch_idx: 150 |  Loss: (0.1880) |  Loss2: (0.0000) | Acc: (93.00%) (18079/19328)
Epoch: 109 | Batch_idx: 160 |  Loss: (0.1890) |  Loss2: (0.0000) | Acc: (93.00%) (19268/20608)
Epoch: 109 | Batch_idx: 170 |  Loss: (0.1893) |  Loss2: (0.0000) | Acc: (93.00%) (20467/21888)
Epoch: 109 | Batch_idx: 180 |  Loss: (0.1904) |  Loss2: (0.0000) | Acc: (93.00%) (21651/23168)
Epoch: 109 | Batch_idx: 190 |  Loss: (0.1897) |  Loss2: (0.0000) | Acc: (93.00%) (22852/24448)
Epoch: 109 | Batch_idx: 200 |  Loss: (0.1891) |  Loss2: (0.0000) | Acc: (93.00%) (24041/25728)
Epoch: 109 | Batch_idx: 210 |  Loss: (0.1898) |  Loss2: (0.0000) | Acc: (93.00%) (25230/27008)
Epoch: 109 | Batch_idx: 220 |  Loss: (0.1905) |  Loss2: (0.0000) | Acc: (93.00%) (26420/28288)
Epoch: 109 | Batch_idx: 230 |  Loss: (0.1911) |  Loss2: (0.0000) | Acc: (93.00%) (27613/29568)
Epoch: 109 | Batch_idx: 240 |  Loss: (0.1919) |  Loss2: (0.0000) | Acc: (93.00%) (28794/30848)
Epoch: 109 | Batch_idx: 250 |  Loss: (0.1923) |  Loss2: (0.0000) | Acc: (93.00%) (29985/32128)
Epoch: 109 | Batch_idx: 260 |  Loss: (0.1932) |  Loss2: (0.0000) | Acc: (93.00%) (31168/33408)
Epoch: 109 | Batch_idx: 270 |  Loss: (0.1925) |  Loss2: (0.0000) | Acc: (93.00%) (32373/34688)
Epoch: 109 | Batch_idx: 280 |  Loss: (0.1925) |  Loss2: (0.0000) | Acc: (93.00%) (33574/35968)
Epoch: 109 | Batch_idx: 290 |  Loss: (0.1919) |  Loss2: (0.0000) | Acc: (93.00%) (34775/37248)
Epoch: 109 | Batch_idx: 300 |  Loss: (0.1920) |  Loss2: (0.0000) | Acc: (93.00%) (35974/38528)
Epoch: 109 | Batch_idx: 310 |  Loss: (0.1924) |  Loss2: (0.0000) | Acc: (93.00%) (37156/39808)
Epoch: 109 | Batch_idx: 320 |  Loss: (0.1920) |  Loss2: (0.0000) | Acc: (93.00%) (38363/41088)
Epoch: 109 | Batch_idx: 330 |  Loss: (0.1927) |  Loss2: (0.0000) | Acc: (93.00%) (39541/42368)
Epoch: 109 | Batch_idx: 340 |  Loss: (0.1928) |  Loss2: (0.0000) | Acc: (93.00%) (40733/43648)
Epoch: 109 | Batch_idx: 350 |  Loss: (0.1928) |  Loss2: (0.0000) | Acc: (93.00%) (41928/44928)
Epoch: 109 | Batch_idx: 360 |  Loss: (0.1933) |  Loss2: (0.0000) | Acc: (93.00%) (43116/46208)
Epoch: 109 | Batch_idx: 370 |  Loss: (0.1935) |  Loss2: (0.0000) | Acc: (93.00%) (44313/47488)
Epoch: 109 | Batch_idx: 380 |  Loss: (0.1945) |  Loss2: (0.0000) | Acc: (93.00%) (45499/48768)
Epoch: 109 | Batch_idx: 390 |  Loss: (0.1943) |  Loss2: (0.0000) | Acc: (93.00%) (46656/50000)
# TEST : Loss: (0.4112) | Acc: (86.00%) (8685/10000)
percent tensor([0.5411, 0.5577, 0.5614, 0.5307, 0.5633, 0.5606, 0.5650, 0.5474, 0.5432,
        0.5573, 0.5517, 0.5665, 0.5413, 0.5232, 0.5617, 0.5392],
       device='cuda:0') torch.Size([16])
percent tensor([0.5315, 0.5323, 0.5354, 0.5382, 0.5376, 0.5304, 0.5342, 0.5383, 0.5338,
        0.5324, 0.5310, 0.5333, 0.5312, 0.5343, 0.5310, 0.5333],
       device='cuda:0') torch.Size([16])
percent tensor([0.5968, 0.6230, 0.4421, 0.4041, 0.4028, 0.5465, 0.5791, 0.4552, 0.5194,
        0.6022, 0.6139, 0.5119, 0.6250, 0.5913, 0.5881, 0.5923],
       device='cuda:0') torch.Size([16])
percent tensor([0.7052, 0.6098, 0.7050, 0.7196, 0.7182, 0.7699, 0.6752, 0.7200, 0.6770,
        0.6303, 0.6702, 0.6645, 0.5890, 0.7139, 0.6917, 0.7328],
       device='cuda:0') torch.Size([16])
percent tensor([0.6533, 0.6597, 0.6681, 0.5999, 0.6214, 0.6202, 0.6585, 0.6628, 0.6528,
        0.6228, 0.6123, 0.6076, 0.6331, 0.6362, 0.6492, 0.6104],
       device='cuda:0') torch.Size([16])
percent tensor([0.5572, 0.6291, 0.6302, 0.6824, 0.6780, 0.6926, 0.6435, 0.5710, 0.6848,
        0.5659, 0.6598, 0.7026, 0.6176, 0.7134, 0.5909, 0.5677],
       device='cuda:0') torch.Size([16])
percent tensor([0.5606, 0.6274, 0.5508, 0.4649, 0.6271, 0.6845, 0.5322, 0.4141, 0.6081,
        0.6220, 0.5982, 0.4227, 0.6284, 0.5435, 0.4174, 0.5455],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9995, 0.9997, 0.9997, 0.9998, 0.9992, 0.9996, 0.9998, 0.9995,
        0.9991, 0.9997, 0.9998, 0.9997, 0.9990, 0.9994, 0.9992],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 110 | Batch_idx: 0 |  Loss: (0.2010) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 110 | Batch_idx: 10 |  Loss: (0.2162) |  Loss2: (0.0000) | Acc: (92.00%) (1299/1408)
Epoch: 110 | Batch_idx: 20 |  Loss: (0.2177) |  Loss2: (0.0000) | Acc: (92.00%) (2482/2688)
Epoch: 110 | Batch_idx: 30 |  Loss: (0.2091) |  Loss2: (0.0000) | Acc: (92.00%) (3671/3968)
Epoch: 110 | Batch_idx: 40 |  Loss: (0.2136) |  Loss2: (0.0000) | Acc: (92.00%) (4857/5248)
Epoch: 110 | Batch_idx: 50 |  Loss: (0.2113) |  Loss2: (0.0000) | Acc: (92.00%) (6046/6528)
Epoch: 110 | Batch_idx: 60 |  Loss: (0.2082) |  Loss2: (0.0000) | Acc: (92.00%) (7233/7808)
Epoch: 110 | Batch_idx: 70 |  Loss: (0.2082) |  Loss2: (0.0000) | Acc: (92.00%) (8422/9088)
Epoch: 110 | Batch_idx: 80 |  Loss: (0.2096) |  Loss2: (0.0000) | Acc: (92.00%) (9606/10368)
Epoch: 110 | Batch_idx: 90 |  Loss: (0.2072) |  Loss2: (0.0000) | Acc: (92.00%) (10797/11648)
Epoch: 110 | Batch_idx: 100 |  Loss: (0.2050) |  Loss2: (0.0000) | Acc: (92.00%) (11993/12928)
Epoch: 110 | Batch_idx: 110 |  Loss: (0.2082) |  Loss2: (0.0000) | Acc: (92.00%) (13164/14208)
Epoch: 110 | Batch_idx: 120 |  Loss: (0.2076) |  Loss2: (0.0000) | Acc: (92.00%) (14349/15488)
Epoch: 110 | Batch_idx: 130 |  Loss: (0.2096) |  Loss2: (0.0000) | Acc: (92.00%) (15519/16768)
Epoch: 110 | Batch_idx: 140 |  Loss: (0.2084) |  Loss2: (0.0000) | Acc: (92.00%) (16716/18048)
Epoch: 110 | Batch_idx: 150 |  Loss: (0.2072) |  Loss2: (0.0000) | Acc: (92.00%) (17903/19328)
Epoch: 110 | Batch_idx: 160 |  Loss: (0.2051) |  Loss2: (0.0000) | Acc: (92.00%) (19112/20608)
Epoch: 110 | Batch_idx: 170 |  Loss: (0.2050) |  Loss2: (0.0000) | Acc: (92.00%) (20302/21888)
Epoch: 110 | Batch_idx: 180 |  Loss: (0.2030) |  Loss2: (0.0000) | Acc: (92.00%) (21500/23168)
Epoch: 110 | Batch_idx: 190 |  Loss: (0.2030) |  Loss2: (0.0000) | Acc: (92.00%) (22682/24448)
Epoch: 110 | Batch_idx: 200 |  Loss: (0.2035) |  Loss2: (0.0000) | Acc: (92.00%) (23873/25728)
Epoch: 110 | Batch_idx: 210 |  Loss: (0.2038) |  Loss2: (0.0000) | Acc: (92.00%) (25050/27008)
Epoch: 110 | Batch_idx: 220 |  Loss: (0.2041) |  Loss2: (0.0000) | Acc: (92.00%) (26237/28288)
Epoch: 110 | Batch_idx: 230 |  Loss: (0.2034) |  Loss2: (0.0000) | Acc: (92.00%) (27434/29568)
Epoch: 110 | Batch_idx: 240 |  Loss: (0.2034) |  Loss2: (0.0000) | Acc: (92.00%) (28623/30848)
Epoch: 110 | Batch_idx: 250 |  Loss: (0.2021) |  Loss2: (0.0000) | Acc: (92.00%) (29825/32128)
Epoch: 110 | Batch_idx: 260 |  Loss: (0.2019) |  Loss2: (0.0000) | Acc: (92.00%) (31011/33408)
Epoch: 110 | Batch_idx: 270 |  Loss: (0.2025) |  Loss2: (0.0000) | Acc: (92.00%) (32185/34688)
Epoch: 110 | Batch_idx: 280 |  Loss: (0.2017) |  Loss2: (0.0000) | Acc: (92.00%) (33382/35968)
Epoch: 110 | Batch_idx: 290 |  Loss: (0.2009) |  Loss2: (0.0000) | Acc: (92.00%) (34583/37248)
Epoch: 110 | Batch_idx: 300 |  Loss: (0.2012) |  Loss2: (0.0000) | Acc: (92.00%) (35771/38528)
Epoch: 110 | Batch_idx: 310 |  Loss: (0.2008) |  Loss2: (0.0000) | Acc: (92.00%) (36956/39808)
Epoch: 110 | Batch_idx: 320 |  Loss: (0.1998) |  Loss2: (0.0000) | Acc: (92.00%) (38164/41088)
Epoch: 110 | Batch_idx: 330 |  Loss: (0.1996) |  Loss2: (0.0000) | Acc: (92.00%) (39359/42368)
Epoch: 110 | Batch_idx: 340 |  Loss: (0.2000) |  Loss2: (0.0000) | Acc: (92.00%) (40537/43648)
Epoch: 110 | Batch_idx: 350 |  Loss: (0.2000) |  Loss2: (0.0000) | Acc: (92.00%) (41731/44928)
Epoch: 110 | Batch_idx: 360 |  Loss: (0.2000) |  Loss2: (0.0000) | Acc: (92.00%) (42924/46208)
Epoch: 110 | Batch_idx: 370 |  Loss: (0.2006) |  Loss2: (0.0000) | Acc: (92.00%) (44108/47488)
Epoch: 110 | Batch_idx: 380 |  Loss: (0.2002) |  Loss2: (0.0000) | Acc: (92.00%) (45303/48768)
Epoch: 110 | Batch_idx: 390 |  Loss: (0.2000) |  Loss2: (0.0000) | Acc: (92.00%) (46453/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_110.pth.tar'
# TEST : Loss: (0.3993) | Acc: (87.00%) (8716/10000)
percent tensor([0.5439, 0.5598, 0.5622, 0.5321, 0.5647, 0.5614, 0.5668, 0.5478, 0.5468,
        0.5589, 0.5550, 0.5674, 0.5439, 0.5267, 0.5627, 0.5421],
       device='cuda:0') torch.Size([16])
percent tensor([0.5301, 0.5302, 0.5346, 0.5368, 0.5363, 0.5295, 0.5324, 0.5369, 0.5325,
        0.5311, 0.5293, 0.5324, 0.5296, 0.5319, 0.5293, 0.5315],
       device='cuda:0') torch.Size([16])
percent tensor([0.5881, 0.6153, 0.4339, 0.4059, 0.4043, 0.5337, 0.5657, 0.4400, 0.5101,
        0.5837, 0.6000, 0.4964, 0.6090, 0.5813, 0.5768, 0.5788],
       device='cuda:0') torch.Size([16])
percent tensor([0.7366, 0.6392, 0.7236, 0.7510, 0.7406, 0.7973, 0.6997, 0.7370, 0.7044,
        0.6626, 0.6992, 0.6894, 0.6175, 0.7369, 0.7204, 0.7688],
       device='cuda:0') torch.Size([16])
percent tensor([0.6466, 0.6553, 0.6541, 0.5761, 0.6119, 0.6014, 0.6594, 0.6504, 0.6430,
        0.6213, 0.6141, 0.6034, 0.6367, 0.6298, 0.6484, 0.5988],
       device='cuda:0') torch.Size([16])
percent tensor([0.5563, 0.6398, 0.6503, 0.6965, 0.6992, 0.7045, 0.6450, 0.5869, 0.6907,
        0.5636, 0.6550, 0.7161, 0.6216, 0.7184, 0.5979, 0.5724],
       device='cuda:0') torch.Size([16])
percent tensor([0.6205, 0.6745, 0.6169, 0.5475, 0.6795, 0.7216, 0.5913, 0.4768, 0.6511,
        0.6546, 0.6457, 0.4813, 0.6724, 0.5845, 0.4795, 0.6157],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9996, 0.9998, 0.9996, 0.9998, 0.9993, 0.9996, 0.9998, 0.9995,
        0.9992, 0.9997, 0.9998, 0.9997, 0.9992, 0.9993, 0.9993],
       device='cuda:0') torch.Size([16])
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(180.6819, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(818.5056, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(811.9815, device='cuda:0')
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1518.6918, device='cuda:0')
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(491.0200, device='cuda:0')
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2251.7849, device='cuda:0')
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4259.3560, device='cuda:0')
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1393.1987, device='cuda:0')
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6174.5962, device='cuda:0')
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11836.6670, device='cuda:0')
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3923.7073, device='cuda:0')
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16573.8203, device='cuda:0')
Epoch: 111 | Batch_idx: 0 |  Loss: (0.3228) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 111 | Batch_idx: 10 |  Loss: (0.1938) |  Loss2: (0.0000) | Acc: (93.00%) (1313/1408)
Epoch: 111 | Batch_idx: 20 |  Loss: (0.2021) |  Loss2: (0.0000) | Acc: (92.00%) (2499/2688)
Epoch: 111 | Batch_idx: 30 |  Loss: (0.1904) |  Loss2: (0.0000) | Acc: (93.00%) (3706/3968)
Epoch: 111 | Batch_idx: 40 |  Loss: (0.1936) |  Loss2: (0.0000) | Acc: (93.00%) (4894/5248)
Epoch: 111 | Batch_idx: 50 |  Loss: (0.1936) |  Loss2: (0.0000) | Acc: (93.00%) (6092/6528)
Epoch: 111 | Batch_idx: 60 |  Loss: (0.1890) |  Loss2: (0.0000) | Acc: (93.00%) (7295/7808)
Epoch: 111 | Batch_idx: 70 |  Loss: (0.1884) |  Loss2: (0.0000) | Acc: (93.00%) (8490/9088)
Epoch: 111 | Batch_idx: 80 |  Loss: (0.1875) |  Loss2: (0.0000) | Acc: (93.00%) (9683/10368)
Epoch: 111 | Batch_idx: 90 |  Loss: (0.1852) |  Loss2: (0.0000) | Acc: (93.00%) (10889/11648)
Epoch: 111 | Batch_idx: 100 |  Loss: (0.1846) |  Loss2: (0.0000) | Acc: (93.00%) (12086/12928)
Epoch: 111 | Batch_idx: 110 |  Loss: (0.1872) |  Loss2: (0.0000) | Acc: (93.00%) (13267/14208)
Epoch: 111 | Batch_idx: 120 |  Loss: (0.1862) |  Loss2: (0.0000) | Acc: (93.00%) (14483/15488)
Epoch: 111 | Batch_idx: 130 |  Loss: (0.1869) |  Loss2: (0.0000) | Acc: (93.00%) (15679/16768)
Epoch: 111 | Batch_idx: 140 |  Loss: (0.1866) |  Loss2: (0.0000) | Acc: (93.00%) (16878/18048)
Epoch: 111 | Batch_idx: 150 |  Loss: (0.1861) |  Loss2: (0.0000) | Acc: (93.00%) (18079/19328)
Epoch: 111 | Batch_idx: 160 |  Loss: (0.1865) |  Loss2: (0.0000) | Acc: (93.00%) (19278/20608)
Epoch: 111 | Batch_idx: 170 |  Loss: (0.1873) |  Loss2: (0.0000) | Acc: (93.00%) (20476/21888)
Epoch: 111 | Batch_idx: 180 |  Loss: (0.1868) |  Loss2: (0.0000) | Acc: (93.00%) (21681/23168)
Epoch: 111 | Batch_idx: 190 |  Loss: (0.1875) |  Loss2: (0.0000) | Acc: (93.00%) (22864/24448)
Epoch: 111 | Batch_idx: 200 |  Loss: (0.1873) |  Loss2: (0.0000) | Acc: (93.00%) (24063/25728)
Epoch: 111 | Batch_idx: 210 |  Loss: (0.1879) |  Loss2: (0.0000) | Acc: (93.00%) (25252/27008)
Epoch: 111 | Batch_idx: 220 |  Loss: (0.1886) |  Loss2: (0.0000) | Acc: (93.00%) (26435/28288)
Epoch: 111 | Batch_idx: 230 |  Loss: (0.1878) |  Loss2: (0.0000) | Acc: (93.00%) (27650/29568)
Epoch: 111 | Batch_idx: 240 |  Loss: (0.1875) |  Loss2: (0.0000) | Acc: (93.00%) (28851/30848)
Epoch: 111 | Batch_idx: 250 |  Loss: (0.1880) |  Loss2: (0.0000) | Acc: (93.00%) (30038/32128)
Epoch: 111 | Batch_idx: 260 |  Loss: (0.1882) |  Loss2: (0.0000) | Acc: (93.00%) (31235/33408)
Epoch: 111 | Batch_idx: 270 |  Loss: (0.1872) |  Loss2: (0.0000) | Acc: (93.00%) (32465/34688)
Epoch: 111 | Batch_idx: 280 |  Loss: (0.1872) |  Loss2: (0.0000) | Acc: (93.00%) (33662/35968)
Epoch: 111 | Batch_idx: 290 |  Loss: (0.1876) |  Loss2: (0.0000) | Acc: (93.00%) (34851/37248)
Epoch: 111 | Batch_idx: 300 |  Loss: (0.1874) |  Loss2: (0.0000) | Acc: (93.00%) (36049/38528)
Epoch: 111 | Batch_idx: 310 |  Loss: (0.1869) |  Loss2: (0.0000) | Acc: (93.00%) (37263/39808)
Epoch: 111 | Batch_idx: 320 |  Loss: (0.1864) |  Loss2: (0.0000) | Acc: (93.00%) (38473/41088)
Epoch: 111 | Batch_idx: 330 |  Loss: (0.1858) |  Loss2: (0.0000) | Acc: (93.00%) (39681/42368)
Epoch: 111 | Batch_idx: 340 |  Loss: (0.1864) |  Loss2: (0.0000) | Acc: (93.00%) (40876/43648)
Epoch: 111 | Batch_idx: 350 |  Loss: (0.1868) |  Loss2: (0.0000) | Acc: (93.00%) (42065/44928)
Epoch: 111 | Batch_idx: 360 |  Loss: (0.1868) |  Loss2: (0.0000) | Acc: (93.00%) (43270/46208)
Epoch: 111 | Batch_idx: 370 |  Loss: (0.1864) |  Loss2: (0.0000) | Acc: (93.00%) (44478/47488)
Epoch: 111 | Batch_idx: 380 |  Loss: (0.1875) |  Loss2: (0.0000) | Acc: (93.00%) (45654/48768)
Epoch: 111 | Batch_idx: 390 |  Loss: (0.1874) |  Loss2: (0.0000) | Acc: (93.00%) (46814/50000)
# TEST : Loss: (0.3871) | Acc: (87.00%) (8772/10000)
percent tensor([0.5488, 0.5649, 0.5648, 0.5356, 0.5683, 0.5656, 0.5717, 0.5513, 0.5520,
        0.5633, 0.5604, 0.5707, 0.5488, 0.5323, 0.5671, 0.5469],
       device='cuda:0') torch.Size([16])
percent tensor([0.5282, 0.5285, 0.5325, 0.5350, 0.5344, 0.5278, 0.5305, 0.5350, 0.5307,
        0.5294, 0.5275, 0.5306, 0.5279, 0.5304, 0.5276, 0.5297],
       device='cuda:0') torch.Size([16])
percent tensor([0.5943, 0.6205, 0.4345, 0.4092, 0.4072, 0.5379, 0.5689, 0.4409, 0.5127,
        0.5866, 0.6055, 0.4970, 0.6137, 0.5870, 0.5810, 0.5827],
       device='cuda:0') torch.Size([16])
percent tensor([0.7269, 0.6256, 0.7109, 0.7464, 0.7310, 0.7997, 0.6860, 0.7233, 0.6910,
        0.6512, 0.6867, 0.6743, 0.6025, 0.7280, 0.7119, 0.7649],
       device='cuda:0') torch.Size([16])
percent tensor([0.6408, 0.6522, 0.6491, 0.5659, 0.6050, 0.5925, 0.6540, 0.6469, 0.6396,
        0.6203, 0.6087, 0.6011, 0.6344, 0.6246, 0.6441, 0.5925],
       device='cuda:0') torch.Size([16])
percent tensor([0.5496, 0.6366, 0.6481, 0.6963, 0.6993, 0.6990, 0.6440, 0.5842, 0.6867,
        0.5544, 0.6504, 0.7165, 0.6177, 0.7161, 0.5956, 0.5605],
       device='cuda:0') torch.Size([16])
percent tensor([0.6242, 0.6716, 0.6073, 0.5360, 0.6689, 0.7211, 0.5939, 0.4795, 0.6429,
        0.6497, 0.6395, 0.4702, 0.6672, 0.5826, 0.4823, 0.6186],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9996, 0.9998, 0.9997, 0.9998, 0.9994, 0.9996, 0.9998, 0.9995,
        0.9992, 0.9997, 0.9999, 0.9997, 0.9992, 0.9993, 0.9993],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 112 | Batch_idx: 0 |  Loss: (0.1550) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 112 | Batch_idx: 10 |  Loss: (0.1640) |  Loss2: (0.0000) | Acc: (94.00%) (1324/1408)
Epoch: 112 | Batch_idx: 20 |  Loss: (0.1639) |  Loss2: (0.0000) | Acc: (94.00%) (2538/2688)
Epoch: 112 | Batch_idx: 30 |  Loss: (0.1718) |  Loss2: (0.0000) | Acc: (94.00%) (3735/3968)
Epoch: 112 | Batch_idx: 40 |  Loss: (0.1803) |  Loss2: (0.0000) | Acc: (93.00%) (4932/5248)
Epoch: 112 | Batch_idx: 50 |  Loss: (0.1835) |  Loss2: (0.0000) | Acc: (93.00%) (6135/6528)
Epoch: 112 | Batch_idx: 60 |  Loss: (0.1805) |  Loss2: (0.0000) | Acc: (94.00%) (7344/7808)
Epoch: 112 | Batch_idx: 70 |  Loss: (0.1817) |  Loss2: (0.0000) | Acc: (94.00%) (8548/9088)
Epoch: 112 | Batch_idx: 80 |  Loss: (0.1833) |  Loss2: (0.0000) | Acc: (93.00%) (9745/10368)
Epoch: 112 | Batch_idx: 90 |  Loss: (0.1835) |  Loss2: (0.0000) | Acc: (93.00%) (10946/11648)
Epoch: 112 | Batch_idx: 100 |  Loss: (0.1850) |  Loss2: (0.0000) | Acc: (93.00%) (12137/12928)
Epoch: 112 | Batch_idx: 110 |  Loss: (0.1852) |  Loss2: (0.0000) | Acc: (93.00%) (13326/14208)
Epoch: 112 | Batch_idx: 120 |  Loss: (0.1872) |  Loss2: (0.0000) | Acc: (93.00%) (14507/15488)
Epoch: 112 | Batch_idx: 130 |  Loss: (0.1878) |  Loss2: (0.0000) | Acc: (93.00%) (15698/16768)
Epoch: 112 | Batch_idx: 140 |  Loss: (0.1885) |  Loss2: (0.0000) | Acc: (93.00%) (16878/18048)
Epoch: 112 | Batch_idx: 150 |  Loss: (0.1889) |  Loss2: (0.0000) | Acc: (93.00%) (18075/19328)
Epoch: 112 | Batch_idx: 160 |  Loss: (0.1907) |  Loss2: (0.0000) | Acc: (93.00%) (19263/20608)
Epoch: 112 | Batch_idx: 170 |  Loss: (0.1911) |  Loss2: (0.0000) | Acc: (93.00%) (20457/21888)
Epoch: 112 | Batch_idx: 180 |  Loss: (0.1901) |  Loss2: (0.0000) | Acc: (93.00%) (21663/23168)
Epoch: 112 | Batch_idx: 190 |  Loss: (0.1899) |  Loss2: (0.0000) | Acc: (93.00%) (22865/24448)
Epoch: 112 | Batch_idx: 200 |  Loss: (0.1907) |  Loss2: (0.0000) | Acc: (93.00%) (24054/25728)
Epoch: 112 | Batch_idx: 210 |  Loss: (0.1918) |  Loss2: (0.0000) | Acc: (93.00%) (25231/27008)
Epoch: 112 | Batch_idx: 220 |  Loss: (0.1931) |  Loss2: (0.0000) | Acc: (93.00%) (26410/28288)
Epoch: 112 | Batch_idx: 230 |  Loss: (0.1927) |  Loss2: (0.0000) | Acc: (93.00%) (27613/29568)
Epoch: 112 | Batch_idx: 240 |  Loss: (0.1923) |  Loss2: (0.0000) | Acc: (93.00%) (28815/30848)
Epoch: 112 | Batch_idx: 250 |  Loss: (0.1922) |  Loss2: (0.0000) | Acc: (93.00%) (30019/32128)
Epoch: 112 | Batch_idx: 260 |  Loss: (0.1928) |  Loss2: (0.0000) | Acc: (93.00%) (31201/33408)
Epoch: 112 | Batch_idx: 270 |  Loss: (0.1922) |  Loss2: (0.0000) | Acc: (93.00%) (32403/34688)
Epoch: 112 | Batch_idx: 280 |  Loss: (0.1921) |  Loss2: (0.0000) | Acc: (93.00%) (33598/35968)
Epoch: 112 | Batch_idx: 290 |  Loss: (0.1917) |  Loss2: (0.0000) | Acc: (93.00%) (34793/37248)
Epoch: 112 | Batch_idx: 300 |  Loss: (0.1923) |  Loss2: (0.0000) | Acc: (93.00%) (35982/38528)
Epoch: 112 | Batch_idx: 310 |  Loss: (0.1917) |  Loss2: (0.0000) | Acc: (93.00%) (37176/39808)
Epoch: 112 | Batch_idx: 320 |  Loss: (0.1915) |  Loss2: (0.0000) | Acc: (93.00%) (38359/41088)
Epoch: 112 | Batch_idx: 330 |  Loss: (0.1921) |  Loss2: (0.0000) | Acc: (93.00%) (39554/42368)
Epoch: 112 | Batch_idx: 340 |  Loss: (0.1921) |  Loss2: (0.0000) | Acc: (93.00%) (40748/43648)
Epoch: 112 | Batch_idx: 350 |  Loss: (0.1929) |  Loss2: (0.0000) | Acc: (93.00%) (41933/44928)
Epoch: 112 | Batch_idx: 360 |  Loss: (0.1930) |  Loss2: (0.0000) | Acc: (93.00%) (43124/46208)
Epoch: 112 | Batch_idx: 370 |  Loss: (0.1932) |  Loss2: (0.0000) | Acc: (93.00%) (44320/47488)
Epoch: 112 | Batch_idx: 380 |  Loss: (0.1933) |  Loss2: (0.0000) | Acc: (93.00%) (45508/48768)
Epoch: 112 | Batch_idx: 390 |  Loss: (0.1934) |  Loss2: (0.0000) | Acc: (93.00%) (46656/50000)
# TEST : Loss: (0.4060) | Acc: (87.00%) (8758/10000)
percent tensor([0.5499, 0.5625, 0.5679, 0.5372, 0.5701, 0.5673, 0.5710, 0.5528, 0.5515,
        0.5639, 0.5602, 0.5731, 0.5501, 0.5286, 0.5671, 0.5474],
       device='cuda:0') torch.Size([16])
percent tensor([0.5277, 0.5279, 0.5308, 0.5343, 0.5328, 0.5278, 0.5296, 0.5335, 0.5294,
        0.5280, 0.5269, 0.5288, 0.5269, 0.5313, 0.5270, 0.5293],
       device='cuda:0') torch.Size([16])
percent tensor([0.6052, 0.6196, 0.4483, 0.4131, 0.4190, 0.5590, 0.5709, 0.4614, 0.5234,
        0.5974, 0.6141, 0.5154, 0.6279, 0.5801, 0.5876, 0.5920],
       device='cuda:0') torch.Size([16])
percent tensor([0.7174, 0.6092, 0.7144, 0.7423, 0.7200, 0.7910, 0.6699, 0.7281, 0.6762,
        0.6388, 0.6746, 0.6658, 0.5961, 0.7078, 0.7022, 0.7585],
       device='cuda:0') torch.Size([16])
percent tensor([0.6489, 0.6555, 0.6443, 0.5738, 0.6079, 0.5955, 0.6623, 0.6422, 0.6553,
        0.6358, 0.6195, 0.6072, 0.6372, 0.6456, 0.6541, 0.6000],
       device='cuda:0') torch.Size([16])
percent tensor([0.5526, 0.6542, 0.6226, 0.6952, 0.6677, 0.7042, 0.6464, 0.5722, 0.6919,
        0.5910, 0.6738, 0.7057, 0.6311, 0.7325, 0.5999, 0.5775],
       device='cuda:0') torch.Size([16])
percent tensor([0.6092, 0.6768, 0.6068, 0.5131, 0.6424, 0.7154, 0.6204, 0.4889, 0.6560,
        0.6709, 0.6444, 0.5052, 0.6739, 0.5985, 0.4834, 0.6097],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9997, 0.9998, 0.9997, 0.9999, 0.9996, 0.9996, 0.9998, 0.9997,
        0.9994, 0.9998, 0.9998, 0.9997, 0.9996, 0.9995, 0.9993],
       device='cuda:0') torch.Size([16])
Epoch: 113 | Batch_idx: 0 |  Loss: (0.1834) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 113 | Batch_idx: 10 |  Loss: (0.1769) |  Loss2: (0.0000) | Acc: (92.00%) (1309/1408)
Epoch: 113 | Batch_idx: 20 |  Loss: (0.1762) |  Loss2: (0.0000) | Acc: (93.00%) (2513/2688)
Epoch: 113 | Batch_idx: 30 |  Loss: (0.1778) |  Loss2: (0.0000) | Acc: (93.00%) (3713/3968)
Epoch: 113 | Batch_idx: 40 |  Loss: (0.1773) |  Loss2: (0.0000) | Acc: (93.00%) (4922/5248)
Epoch: 113 | Batch_idx: 50 |  Loss: (0.1791) |  Loss2: (0.0000) | Acc: (93.00%) (6112/6528)
Epoch: 113 | Batch_idx: 60 |  Loss: (0.1824) |  Loss2: (0.0000) | Acc: (93.00%) (7305/7808)
Epoch: 113 | Batch_idx: 70 |  Loss: (0.1825) |  Loss2: (0.0000) | Acc: (93.00%) (8505/9088)
Epoch: 113 | Batch_idx: 80 |  Loss: (0.1816) |  Loss2: (0.0000) | Acc: (93.00%) (9714/10368)
Epoch: 113 | Batch_idx: 90 |  Loss: (0.1816) |  Loss2: (0.0000) | Acc: (93.00%) (10910/11648)
Epoch: 113 | Batch_idx: 100 |  Loss: (0.1812) |  Loss2: (0.0000) | Acc: (93.00%) (12110/12928)
Epoch: 113 | Batch_idx: 110 |  Loss: (0.1827) |  Loss2: (0.0000) | Acc: (93.00%) (13312/14208)
Epoch: 113 | Batch_idx: 120 |  Loss: (0.1802) |  Loss2: (0.0000) | Acc: (93.00%) (14524/15488)
Epoch: 113 | Batch_idx: 130 |  Loss: (0.1779) |  Loss2: (0.0000) | Acc: (93.00%) (15741/16768)
Epoch: 113 | Batch_idx: 140 |  Loss: (0.1773) |  Loss2: (0.0000) | Acc: (93.00%) (16945/18048)
Epoch: 113 | Batch_idx: 150 |  Loss: (0.1787) |  Loss2: (0.0000) | Acc: (93.00%) (18137/19328)
Epoch: 113 | Batch_idx: 160 |  Loss: (0.1799) |  Loss2: (0.0000) | Acc: (93.00%) (19324/20608)
Epoch: 113 | Batch_idx: 170 |  Loss: (0.1792) |  Loss2: (0.0000) | Acc: (93.00%) (20533/21888)
Epoch: 113 | Batch_idx: 180 |  Loss: (0.1795) |  Loss2: (0.0000) | Acc: (93.00%) (21729/23168)
Epoch: 113 | Batch_idx: 190 |  Loss: (0.1801) |  Loss2: (0.0000) | Acc: (93.00%) (22917/24448)
Epoch: 113 | Batch_idx: 200 |  Loss: (0.1807) |  Loss2: (0.0000) | Acc: (93.00%) (24108/25728)
Epoch: 113 | Batch_idx: 210 |  Loss: (0.1814) |  Loss2: (0.0000) | Acc: (93.00%) (25289/27008)
Epoch: 113 | Batch_idx: 220 |  Loss: (0.1817) |  Loss2: (0.0000) | Acc: (93.00%) (26488/28288)
Epoch: 113 | Batch_idx: 230 |  Loss: (0.1819) |  Loss2: (0.0000) | Acc: (93.00%) (27682/29568)
Epoch: 113 | Batch_idx: 240 |  Loss: (0.1813) |  Loss2: (0.0000) | Acc: (93.00%) (28890/30848)
Epoch: 113 | Batch_idx: 250 |  Loss: (0.1815) |  Loss2: (0.0000) | Acc: (93.00%) (30090/32128)
Epoch: 113 | Batch_idx: 260 |  Loss: (0.1830) |  Loss2: (0.0000) | Acc: (93.00%) (31271/33408)
Epoch: 113 | Batch_idx: 270 |  Loss: (0.1837) |  Loss2: (0.0000) | Acc: (93.00%) (32461/34688)
Epoch: 113 | Batch_idx: 280 |  Loss: (0.1837) |  Loss2: (0.0000) | Acc: (93.00%) (33658/35968)
Epoch: 113 | Batch_idx: 290 |  Loss: (0.1837) |  Loss2: (0.0000) | Acc: (93.00%) (34859/37248)
Epoch: 113 | Batch_idx: 300 |  Loss: (0.1834) |  Loss2: (0.0000) | Acc: (93.00%) (36062/38528)
Epoch: 113 | Batch_idx: 310 |  Loss: (0.1835) |  Loss2: (0.0000) | Acc: (93.00%) (37263/39808)
Epoch: 113 | Batch_idx: 320 |  Loss: (0.1834) |  Loss2: (0.0000) | Acc: (93.00%) (38465/41088)
Epoch: 113 | Batch_idx: 330 |  Loss: (0.1849) |  Loss2: (0.0000) | Acc: (93.00%) (39642/42368)
Epoch: 113 | Batch_idx: 340 |  Loss: (0.1859) |  Loss2: (0.0000) | Acc: (93.00%) (40830/43648)
Epoch: 113 | Batch_idx: 350 |  Loss: (0.1863) |  Loss2: (0.0000) | Acc: (93.00%) (42014/44928)
Epoch: 113 | Batch_idx: 360 |  Loss: (0.1859) |  Loss2: (0.0000) | Acc: (93.00%) (43221/46208)
Epoch: 113 | Batch_idx: 370 |  Loss: (0.1859) |  Loss2: (0.0000) | Acc: (93.00%) (44417/47488)
Epoch: 113 | Batch_idx: 380 |  Loss: (0.1863) |  Loss2: (0.0000) | Acc: (93.00%) (45609/48768)
Epoch: 113 | Batch_idx: 390 |  Loss: (0.1866) |  Loss2: (0.0000) | Acc: (93.00%) (46751/50000)
# TEST : Loss: (0.3859) | Acc: (87.00%) (8750/10000)
percent tensor([0.5514, 0.5654, 0.5623, 0.5357, 0.5666, 0.5697, 0.5712, 0.5512, 0.5522,
        0.5636, 0.5631, 0.5696, 0.5511, 0.5324, 0.5693, 0.5491],
       device='cuda:0') torch.Size([16])
percent tensor([0.5281, 0.5286, 0.5323, 0.5350, 0.5339, 0.5279, 0.5306, 0.5350, 0.5306,
        0.5291, 0.5273, 0.5300, 0.5276, 0.5319, 0.5277, 0.5295],
       device='cuda:0') torch.Size([16])
percent tensor([0.6044, 0.6189, 0.4384, 0.4098, 0.4095, 0.5510, 0.5648, 0.4646, 0.5178,
        0.5912, 0.6077, 0.5035, 0.6257, 0.5852, 0.5812, 0.5941],
       device='cuda:0') torch.Size([16])
percent tensor([0.7290, 0.6073, 0.7317, 0.7388, 0.7387, 0.8035, 0.6811, 0.7360, 0.6903,
        0.6436, 0.6767, 0.6811, 0.5932, 0.7185, 0.7082, 0.7646],
       device='cuda:0') torch.Size([16])
percent tensor([0.6357, 0.6529, 0.6325, 0.5766, 0.5945, 0.5760, 0.6515, 0.6390, 0.6538,
        0.6275, 0.6212, 0.6053, 0.6409, 0.6416, 0.6519, 0.5961],
       device='cuda:0') torch.Size([16])
percent tensor([0.5575, 0.6483, 0.6615, 0.7121, 0.7107, 0.7084, 0.6552, 0.5930, 0.6941,
        0.5610, 0.6441, 0.7116, 0.6224, 0.6995, 0.5894, 0.5682],
       device='cuda:0') torch.Size([16])
percent tensor([0.6248, 0.6570, 0.6050, 0.5249, 0.6565, 0.7313, 0.5973, 0.4921, 0.6475,
        0.6452, 0.6282, 0.4895, 0.6628, 0.5885, 0.4784, 0.6072],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9996, 0.9998, 0.9996, 0.9998, 0.9993, 0.9996, 0.9997, 0.9998,
        0.9993, 0.9997, 0.9998, 0.9996, 0.9993, 0.9996, 0.9993],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 114 | Batch_idx: 0 |  Loss: (0.2408) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 114 | Batch_idx: 10 |  Loss: (0.1851) |  Loss2: (0.0000) | Acc: (93.00%) (1322/1408)
Epoch: 114 | Batch_idx: 20 |  Loss: (0.1874) |  Loss2: (0.0000) | Acc: (93.00%) (2517/2688)
Epoch: 114 | Batch_idx: 30 |  Loss: (0.1944) |  Loss2: (0.0000) | Acc: (93.00%) (3704/3968)
Epoch: 114 | Batch_idx: 40 |  Loss: (0.2043) |  Loss2: (0.0000) | Acc: (92.00%) (4875/5248)
Epoch: 114 | Batch_idx: 50 |  Loss: (0.2084) |  Loss2: (0.0000) | Acc: (92.00%) (6055/6528)
Epoch: 114 | Batch_idx: 60 |  Loss: (0.2109) |  Loss2: (0.0000) | Acc: (92.00%) (7232/7808)
Epoch: 114 | Batch_idx: 70 |  Loss: (0.2116) |  Loss2: (0.0000) | Acc: (92.00%) (8414/9088)
Epoch: 114 | Batch_idx: 80 |  Loss: (0.2136) |  Loss2: (0.0000) | Acc: (92.00%) (9586/10368)
Epoch: 114 | Batch_idx: 90 |  Loss: (0.2126) |  Loss2: (0.0000) | Acc: (92.00%) (10771/11648)
Epoch: 114 | Batch_idx: 100 |  Loss: (0.2117) |  Loss2: (0.0000) | Acc: (92.00%) (11950/12928)
Epoch: 114 | Batch_idx: 110 |  Loss: (0.2115) |  Loss2: (0.0000) | Acc: (92.00%) (13131/14208)
Epoch: 114 | Batch_idx: 120 |  Loss: (0.2113) |  Loss2: (0.0000) | Acc: (92.00%) (14307/15488)
Epoch: 114 | Batch_idx: 130 |  Loss: (0.2116) |  Loss2: (0.0000) | Acc: (92.00%) (15493/16768)
Epoch: 114 | Batch_idx: 140 |  Loss: (0.2094) |  Loss2: (0.0000) | Acc: (92.00%) (16690/18048)
Epoch: 114 | Batch_idx: 150 |  Loss: (0.2085) |  Loss2: (0.0000) | Acc: (92.00%) (17886/19328)
Epoch: 114 | Batch_idx: 160 |  Loss: (0.2077) |  Loss2: (0.0000) | Acc: (92.00%) (19080/20608)
Epoch: 114 | Batch_idx: 170 |  Loss: (0.2072) |  Loss2: (0.0000) | Acc: (92.00%) (20275/21888)
Epoch: 114 | Batch_idx: 180 |  Loss: (0.2056) |  Loss2: (0.0000) | Acc: (92.00%) (21480/23168)
Epoch: 114 | Batch_idx: 190 |  Loss: (0.2064) |  Loss2: (0.0000) | Acc: (92.00%) (22656/24448)
Epoch: 114 | Batch_idx: 200 |  Loss: (0.2049) |  Loss2: (0.0000) | Acc: (92.00%) (23865/25728)
Epoch: 114 | Batch_idx: 210 |  Loss: (0.2054) |  Loss2: (0.0000) | Acc: (92.00%) (25047/27008)
Epoch: 114 | Batch_idx: 220 |  Loss: (0.2052) |  Loss2: (0.0000) | Acc: (92.00%) (26243/28288)
Epoch: 114 | Batch_idx: 230 |  Loss: (0.2053) |  Loss2: (0.0000) | Acc: (92.00%) (27428/29568)
Epoch: 114 | Batch_idx: 240 |  Loss: (0.2059) |  Loss2: (0.0000) | Acc: (92.00%) (28612/30848)
Epoch: 114 | Batch_idx: 250 |  Loss: (0.2049) |  Loss2: (0.0000) | Acc: (92.00%) (29810/32128)
Epoch: 114 | Batch_idx: 260 |  Loss: (0.2058) |  Loss2: (0.0000) | Acc: (92.00%) (30979/33408)
Epoch: 114 | Batch_idx: 270 |  Loss: (0.2053) |  Loss2: (0.0000) | Acc: (92.00%) (32184/34688)
Epoch: 114 | Batch_idx: 280 |  Loss: (0.2062) |  Loss2: (0.0000) | Acc: (92.00%) (33354/35968)
Epoch: 114 | Batch_idx: 290 |  Loss: (0.2059) |  Loss2: (0.0000) | Acc: (92.00%) (34551/37248)
Epoch: 114 | Batch_idx: 300 |  Loss: (0.2055) |  Loss2: (0.0000) | Acc: (92.00%) (35745/38528)
Epoch: 114 | Batch_idx: 310 |  Loss: (0.2051) |  Loss2: (0.0000) | Acc: (92.00%) (36943/39808)
Epoch: 114 | Batch_idx: 320 |  Loss: (0.2044) |  Loss2: (0.0000) | Acc: (92.00%) (38146/41088)
Epoch: 114 | Batch_idx: 330 |  Loss: (0.2050) |  Loss2: (0.0000) | Acc: (92.00%) (39319/42368)
Epoch: 114 | Batch_idx: 340 |  Loss: (0.2040) |  Loss2: (0.0000) | Acc: (92.00%) (40524/43648)
Epoch: 114 | Batch_idx: 350 |  Loss: (0.2037) |  Loss2: (0.0000) | Acc: (92.00%) (41711/44928)
Epoch: 114 | Batch_idx: 360 |  Loss: (0.2028) |  Loss2: (0.0000) | Acc: (92.00%) (42919/46208)
Epoch: 114 | Batch_idx: 370 |  Loss: (0.2026) |  Loss2: (0.0000) | Acc: (92.00%) (44111/47488)
Epoch: 114 | Batch_idx: 380 |  Loss: (0.2015) |  Loss2: (0.0000) | Acc: (92.00%) (45320/48768)
Epoch: 114 | Batch_idx: 390 |  Loss: (0.2002) |  Loss2: (0.0000) | Acc: (92.00%) (46493/50000)
# TEST : Loss: (0.3916) | Acc: (87.00%) (8783/10000)
percent tensor([0.5540, 0.5686, 0.5675, 0.5385, 0.5718, 0.5710, 0.5757, 0.5544, 0.5550,
        0.5678, 0.5653, 0.5754, 0.5539, 0.5354, 0.5717, 0.5512],
       device='cuda:0') torch.Size([16])
percent tensor([0.5395, 0.5414, 0.5448, 0.5491, 0.5467, 0.5394, 0.5433, 0.5493, 0.5429,
        0.5412, 0.5391, 0.5413, 0.5391, 0.5450, 0.5402, 0.5419],
       device='cuda:0') torch.Size([16])
percent tensor([0.6011, 0.6192, 0.4444, 0.4129, 0.4112, 0.5466, 0.5681, 0.4571, 0.5225,
        0.5949, 0.6079, 0.5109, 0.6257, 0.5923, 0.5781, 0.5923],
       device='cuda:0') torch.Size([16])
percent tensor([0.6743, 0.5611, 0.6786, 0.6956, 0.6851, 0.7696, 0.6259, 0.6914, 0.6393,
        0.5933, 0.6187, 0.6298, 0.5403, 0.6796, 0.6522, 0.7170],
       device='cuda:0') torch.Size([16])
percent tensor([0.6451, 0.6533, 0.6458, 0.5850, 0.6079, 0.5905, 0.6590, 0.6413, 0.6519,
        0.6252, 0.6178, 0.6055, 0.6439, 0.6370, 0.6574, 0.6058],
       device='cuda:0') torch.Size([16])
percent tensor([0.5462, 0.6400, 0.6643, 0.7141, 0.7051, 0.7023, 0.6515, 0.6019, 0.6864,
        0.5584, 0.6450, 0.7142, 0.6177, 0.6959, 0.5855, 0.5418],
       device='cuda:0') torch.Size([16])
percent tensor([0.5901, 0.6284, 0.6181, 0.5133, 0.6595, 0.7118, 0.5738, 0.4622, 0.6234,
        0.6271, 0.6111, 0.4731, 0.6365, 0.5702, 0.4672, 0.5773],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9996, 0.9998, 0.9997, 0.9999, 0.9994, 0.9996, 0.9996, 0.9997,
        0.9994, 0.9998, 0.9998, 0.9997, 0.9993, 0.9995, 0.9995],
       device='cuda:0') torch.Size([16])
Epoch: 115 | Batch_idx: 0 |  Loss: (0.2328) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 115 | Batch_idx: 10 |  Loss: (0.1994) |  Loss2: (0.0000) | Acc: (92.00%) (1308/1408)
Epoch: 115 | Batch_idx: 20 |  Loss: (0.2070) |  Loss2: (0.0000) | Acc: (92.00%) (2488/2688)
Epoch: 115 | Batch_idx: 30 |  Loss: (0.2042) |  Loss2: (0.0000) | Acc: (92.00%) (3685/3968)
Epoch: 115 | Batch_idx: 40 |  Loss: (0.1952) |  Loss2: (0.0000) | Acc: (93.00%) (4890/5248)
Epoch: 115 | Batch_idx: 50 |  Loss: (0.1948) |  Loss2: (0.0000) | Acc: (93.00%) (6086/6528)
Epoch: 115 | Batch_idx: 60 |  Loss: (0.1940) |  Loss2: (0.0000) | Acc: (93.00%) (7277/7808)
Epoch: 115 | Batch_idx: 70 |  Loss: (0.1921) |  Loss2: (0.0000) | Acc: (93.00%) (8470/9088)
Epoch: 115 | Batch_idx: 80 |  Loss: (0.1925) |  Loss2: (0.0000) | Acc: (93.00%) (9662/10368)
Epoch: 115 | Batch_idx: 90 |  Loss: (0.1890) |  Loss2: (0.0000) | Acc: (93.00%) (10870/11648)
Epoch: 115 | Batch_idx: 100 |  Loss: (0.1891) |  Loss2: (0.0000) | Acc: (93.00%) (12067/12928)
Epoch: 115 | Batch_idx: 110 |  Loss: (0.1888) |  Loss2: (0.0000) | Acc: (93.00%) (13271/14208)
Epoch: 115 | Batch_idx: 120 |  Loss: (0.1883) |  Loss2: (0.0000) | Acc: (93.00%) (14467/15488)
Epoch: 115 | Batch_idx: 130 |  Loss: (0.1868) |  Loss2: (0.0000) | Acc: (93.00%) (15678/16768)
Epoch: 115 | Batch_idx: 140 |  Loss: (0.1854) |  Loss2: (0.0000) | Acc: (93.00%) (16882/18048)
Epoch: 115 | Batch_idx: 150 |  Loss: (0.1839) |  Loss2: (0.0000) | Acc: (93.00%) (18095/19328)
Epoch: 115 | Batch_idx: 160 |  Loss: (0.1833) |  Loss2: (0.0000) | Acc: (93.00%) (19306/20608)
Epoch: 115 | Batch_idx: 170 |  Loss: (0.1844) |  Loss2: (0.0000) | Acc: (93.00%) (20486/21888)
Epoch: 115 | Batch_idx: 180 |  Loss: (0.1844) |  Loss2: (0.0000) | Acc: (93.00%) (21683/23168)
Epoch: 115 | Batch_idx: 190 |  Loss: (0.1851) |  Loss2: (0.0000) | Acc: (93.00%) (22873/24448)
Epoch: 115 | Batch_idx: 200 |  Loss: (0.1840) |  Loss2: (0.0000) | Acc: (93.00%) (24075/25728)
Epoch: 115 | Batch_idx: 210 |  Loss: (0.1844) |  Loss2: (0.0000) | Acc: (93.00%) (25271/27008)
Epoch: 115 | Batch_idx: 220 |  Loss: (0.1845) |  Loss2: (0.0000) | Acc: (93.00%) (26466/28288)
Epoch: 115 | Batch_idx: 230 |  Loss: (0.1844) |  Loss2: (0.0000) | Acc: (93.00%) (27662/29568)
Epoch: 115 | Batch_idx: 240 |  Loss: (0.1843) |  Loss2: (0.0000) | Acc: (93.00%) (28867/30848)
Epoch: 115 | Batch_idx: 250 |  Loss: (0.1835) |  Loss2: (0.0000) | Acc: (93.00%) (30075/32128)
Epoch: 115 | Batch_idx: 260 |  Loss: (0.1840) |  Loss2: (0.0000) | Acc: (93.00%) (31268/33408)
Epoch: 115 | Batch_idx: 270 |  Loss: (0.1841) |  Loss2: (0.0000) | Acc: (93.00%) (32466/34688)
Epoch: 115 | Batch_idx: 280 |  Loss: (0.1837) |  Loss2: (0.0000) | Acc: (93.00%) (33659/35968)
Epoch: 115 | Batch_idx: 290 |  Loss: (0.1839) |  Loss2: (0.0000) | Acc: (93.00%) (34853/37248)
Epoch: 115 | Batch_idx: 300 |  Loss: (0.1853) |  Loss2: (0.0000) | Acc: (93.00%) (36031/38528)
Epoch: 115 | Batch_idx: 310 |  Loss: (0.1848) |  Loss2: (0.0000) | Acc: (93.00%) (37238/39808)
Epoch: 115 | Batch_idx: 320 |  Loss: (0.1847) |  Loss2: (0.0000) | Acc: (93.00%) (38440/41088)
Epoch: 115 | Batch_idx: 330 |  Loss: (0.1840) |  Loss2: (0.0000) | Acc: (93.00%) (39638/42368)
Epoch: 115 | Batch_idx: 340 |  Loss: (0.1838) |  Loss2: (0.0000) | Acc: (93.00%) (40839/43648)
Epoch: 115 | Batch_idx: 350 |  Loss: (0.1828) |  Loss2: (0.0000) | Acc: (93.00%) (42049/44928)
Epoch: 115 | Batch_idx: 360 |  Loss: (0.1827) |  Loss2: (0.0000) | Acc: (93.00%) (43253/46208)
Epoch: 115 | Batch_idx: 370 |  Loss: (0.1824) |  Loss2: (0.0000) | Acc: (93.00%) (44454/47488)
Epoch: 115 | Batch_idx: 380 |  Loss: (0.1822) |  Loss2: (0.0000) | Acc: (93.00%) (45660/48768)
Epoch: 115 | Batch_idx: 390 |  Loss: (0.1819) |  Loss2: (0.0000) | Acc: (93.00%) (46814/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_115.pth.tar'
# TEST : Loss: (0.3781) | Acc: (88.00%) (8828/10000)
percent tensor([0.5486, 0.5631, 0.5636, 0.5338, 0.5666, 0.5638, 0.5704, 0.5498, 0.5497,
        0.5631, 0.5597, 0.5714, 0.5487, 0.5305, 0.5655, 0.5455],
       device='cuda:0') torch.Size([16])
percent tensor([0.5418, 0.5434, 0.5477, 0.5520, 0.5498, 0.5419, 0.5458, 0.5523, 0.5453,
        0.5434, 0.5411, 0.5438, 0.5413, 0.5470, 0.5425, 0.5444],
       device='cuda:0') torch.Size([16])
percent tensor([0.6054, 0.6247, 0.4454, 0.4165, 0.4106, 0.5472, 0.5718, 0.4587, 0.5264,
        0.5986, 0.6127, 0.5127, 0.6311, 0.5990, 0.5803, 0.5958],
       device='cuda:0') torch.Size([16])
percent tensor([0.6563, 0.5396, 0.6644, 0.6800, 0.6681, 0.7568, 0.6048, 0.6747, 0.6219,
        0.5715, 0.5947, 0.6096, 0.5228, 0.6586, 0.6276, 0.7010],
       device='cuda:0') torch.Size([16])
percent tensor([0.6448, 0.6519, 0.6498, 0.5873, 0.6129, 0.5910, 0.6590, 0.6450, 0.6516,
        0.6190, 0.6118, 0.6064, 0.6435, 0.6334, 0.6588, 0.6048],
       device='cuda:0') torch.Size([16])
percent tensor([0.5443, 0.6508, 0.6704, 0.7205, 0.7143, 0.7048, 0.6588, 0.6066, 0.6939,
        0.5693, 0.6535, 0.7202, 0.6251, 0.7029, 0.5927, 0.5470],
       device='cuda:0') torch.Size([16])
percent tensor([0.5892, 0.6368, 0.6129, 0.5091, 0.6623, 0.7097, 0.5784, 0.4552, 0.6334,
        0.6322, 0.6282, 0.4792, 0.6448, 0.5846, 0.4691, 0.5735],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9996, 0.9998, 0.9997, 0.9999, 0.9994, 0.9997, 0.9996, 0.9997,
        0.9994, 0.9998, 0.9998, 0.9997, 0.9994, 0.9995, 0.9995],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 116 | Batch_idx: 0 |  Loss: (0.1340) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 116 | Batch_idx: 10 |  Loss: (0.1541) |  Loss2: (0.0000) | Acc: (94.00%) (1336/1408)
Epoch: 116 | Batch_idx: 20 |  Loss: (0.1567) |  Loss2: (0.0000) | Acc: (95.00%) (2557/2688)
Epoch: 116 | Batch_idx: 30 |  Loss: (0.1681) |  Loss2: (0.0000) | Acc: (94.00%) (3748/3968)
Epoch: 116 | Batch_idx: 40 |  Loss: (0.1672) |  Loss2: (0.0000) | Acc: (94.00%) (4955/5248)
Epoch: 116 | Batch_idx: 50 |  Loss: (0.1664) |  Loss2: (0.0000) | Acc: (94.00%) (6164/6528)
Epoch: 116 | Batch_idx: 60 |  Loss: (0.1663) |  Loss2: (0.0000) | Acc: (94.00%) (7364/7808)
Epoch: 116 | Batch_idx: 70 |  Loss: (0.1678) |  Loss2: (0.0000) | Acc: (94.00%) (8568/9088)
Epoch: 116 | Batch_idx: 80 |  Loss: (0.1682) |  Loss2: (0.0000) | Acc: (94.00%) (9768/10368)
Epoch: 116 | Batch_idx: 90 |  Loss: (0.1708) |  Loss2: (0.0000) | Acc: (94.00%) (10958/11648)
Epoch: 116 | Batch_idx: 100 |  Loss: (0.1724) |  Loss2: (0.0000) | Acc: (93.00%) (12152/12928)
Epoch: 116 | Batch_idx: 110 |  Loss: (0.1723) |  Loss2: (0.0000) | Acc: (94.00%) (13356/14208)
Epoch: 116 | Batch_idx: 120 |  Loss: (0.1724) |  Loss2: (0.0000) | Acc: (93.00%) (14555/15488)
Epoch: 116 | Batch_idx: 130 |  Loss: (0.1736) |  Loss2: (0.0000) | Acc: (93.00%) (15752/16768)
Epoch: 116 | Batch_idx: 140 |  Loss: (0.1722) |  Loss2: (0.0000) | Acc: (93.00%) (16959/18048)
Epoch: 116 | Batch_idx: 150 |  Loss: (0.1743) |  Loss2: (0.0000) | Acc: (93.00%) (18156/19328)
Epoch: 116 | Batch_idx: 160 |  Loss: (0.1746) |  Loss2: (0.0000) | Acc: (93.00%) (19355/20608)
Epoch: 116 | Batch_idx: 170 |  Loss: (0.1752) |  Loss2: (0.0000) | Acc: (93.00%) (20562/21888)
Epoch: 116 | Batch_idx: 180 |  Loss: (0.1762) |  Loss2: (0.0000) | Acc: (93.00%) (21754/23168)
Epoch: 116 | Batch_idx: 190 |  Loss: (0.1759) |  Loss2: (0.0000) | Acc: (93.00%) (22957/24448)
Epoch: 116 | Batch_idx: 200 |  Loss: (0.1756) |  Loss2: (0.0000) | Acc: (93.00%) (24162/25728)
Epoch: 116 | Batch_idx: 210 |  Loss: (0.1747) |  Loss2: (0.0000) | Acc: (93.00%) (25377/27008)
Epoch: 116 | Batch_idx: 220 |  Loss: (0.1756) |  Loss2: (0.0000) | Acc: (93.00%) (26577/28288)
Epoch: 116 | Batch_idx: 230 |  Loss: (0.1761) |  Loss2: (0.0000) | Acc: (93.00%) (27775/29568)
Epoch: 116 | Batch_idx: 240 |  Loss: (0.1769) |  Loss2: (0.0000) | Acc: (93.00%) (28974/30848)
Epoch: 116 | Batch_idx: 250 |  Loss: (0.1770) |  Loss2: (0.0000) | Acc: (93.00%) (30169/32128)
Epoch: 116 | Batch_idx: 260 |  Loss: (0.1770) |  Loss2: (0.0000) | Acc: (93.00%) (31366/33408)
Epoch: 116 | Batch_idx: 270 |  Loss: (0.1771) |  Loss2: (0.0000) | Acc: (93.00%) (32566/34688)
Epoch: 116 | Batch_idx: 280 |  Loss: (0.1774) |  Loss2: (0.0000) | Acc: (93.00%) (33764/35968)
Epoch: 116 | Batch_idx: 290 |  Loss: (0.1772) |  Loss2: (0.0000) | Acc: (93.00%) (34971/37248)
Epoch: 116 | Batch_idx: 300 |  Loss: (0.1774) |  Loss2: (0.0000) | Acc: (93.00%) (36166/38528)
Epoch: 116 | Batch_idx: 310 |  Loss: (0.1775) |  Loss2: (0.0000) | Acc: (93.00%) (37370/39808)
Epoch: 116 | Batch_idx: 320 |  Loss: (0.1775) |  Loss2: (0.0000) | Acc: (93.00%) (38563/41088)
Epoch: 116 | Batch_idx: 330 |  Loss: (0.1777) |  Loss2: (0.0000) | Acc: (93.00%) (39752/42368)
Epoch: 116 | Batch_idx: 340 |  Loss: (0.1779) |  Loss2: (0.0000) | Acc: (93.00%) (40948/43648)
Epoch: 116 | Batch_idx: 350 |  Loss: (0.1790) |  Loss2: (0.0000) | Acc: (93.00%) (42131/44928)
Epoch: 116 | Batch_idx: 360 |  Loss: (0.1800) |  Loss2: (0.0000) | Acc: (93.00%) (43312/46208)
Epoch: 116 | Batch_idx: 370 |  Loss: (0.1798) |  Loss2: (0.0000) | Acc: (93.00%) (44519/47488)
Epoch: 116 | Batch_idx: 380 |  Loss: (0.1805) |  Loss2: (0.0000) | Acc: (93.00%) (45702/48768)
Epoch: 116 | Batch_idx: 390 |  Loss: (0.1807) |  Loss2: (0.0000) | Acc: (93.00%) (46847/50000)
# TEST : Loss: (0.3807) | Acc: (87.00%) (8745/10000)
percent tensor([0.5472, 0.5629, 0.5627, 0.5331, 0.5648, 0.5623, 0.5697, 0.5508, 0.5487,
        0.5630, 0.5593, 0.5688, 0.5485, 0.5294, 0.5649, 0.5456],
       device='cuda:0') torch.Size([16])
percent tensor([0.5421, 0.5427, 0.5485, 0.5514, 0.5504, 0.5419, 0.5453, 0.5514, 0.5454,
        0.5434, 0.5419, 0.5449, 0.5414, 0.5457, 0.5423, 0.5448],
       device='cuda:0') torch.Size([16])
percent tensor([0.5991, 0.6251, 0.4383, 0.4187, 0.4098, 0.5600, 0.5673, 0.4544, 0.5056,
        0.5958, 0.6017, 0.5007, 0.6267, 0.5887, 0.5844, 0.5892],
       device='cuda:0') torch.Size([16])
percent tensor([0.6543, 0.5506, 0.6597, 0.6812, 0.6608, 0.7488, 0.6103, 0.6733, 0.6212,
        0.5763, 0.6077, 0.6064, 0.5399, 0.6620, 0.6287, 0.7057],
       device='cuda:0') torch.Size([16])
percent tensor([0.6501, 0.6505, 0.6433, 0.5849, 0.6033, 0.5989, 0.6482, 0.6500, 0.6494,
        0.6199, 0.6025, 0.5975, 0.6381, 0.6243, 0.6554, 0.6064],
       device='cuda:0') torch.Size([16])
percent tensor([0.5264, 0.6417, 0.6357, 0.6989, 0.6990, 0.6992, 0.6448, 0.5958, 0.6823,
        0.5485, 0.6587, 0.7273, 0.6237, 0.6935, 0.5902, 0.5406],
       device='cuda:0') torch.Size([16])
percent tensor([0.5985, 0.6483, 0.5679, 0.4950, 0.6383, 0.7146, 0.5938, 0.4677, 0.6370,
        0.6328, 0.6434, 0.4597, 0.6454, 0.5838, 0.4599, 0.5949],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9997, 0.9997, 0.9998, 0.9998, 0.9993, 0.9997, 0.9997, 0.9998,
        0.9996, 0.9999, 0.9998, 0.9997, 0.9994, 0.9995, 0.9994],
       device='cuda:0') torch.Size([16])
Epoch: 117 | Batch_idx: 0 |  Loss: (0.0755) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 117 | Batch_idx: 10 |  Loss: (0.1466) |  Loss2: (0.0000) | Acc: (95.00%) (1347/1408)
Epoch: 117 | Batch_idx: 20 |  Loss: (0.1623) |  Loss2: (0.0000) | Acc: (94.00%) (2547/2688)
Epoch: 117 | Batch_idx: 30 |  Loss: (0.1702) |  Loss2: (0.0000) | Acc: (94.00%) (3749/3968)
Epoch: 117 | Batch_idx: 40 |  Loss: (0.1681) |  Loss2: (0.0000) | Acc: (94.00%) (4957/5248)
Epoch: 117 | Batch_idx: 50 |  Loss: (0.1732) |  Loss2: (0.0000) | Acc: (94.00%) (6155/6528)
Epoch: 117 | Batch_idx: 60 |  Loss: (0.1752) |  Loss2: (0.0000) | Acc: (94.00%) (7354/7808)
Epoch: 117 | Batch_idx: 70 |  Loss: (0.1787) |  Loss2: (0.0000) | Acc: (93.00%) (8539/9088)
Epoch: 117 | Batch_idx: 80 |  Loss: (0.1773) |  Loss2: (0.0000) | Acc: (94.00%) (9753/10368)
Epoch: 117 | Batch_idx: 90 |  Loss: (0.1771) |  Loss2: (0.0000) | Acc: (94.00%) (10954/11648)
Epoch: 117 | Batch_idx: 100 |  Loss: (0.1753) |  Loss2: (0.0000) | Acc: (94.00%) (12162/12928)
Epoch: 117 | Batch_idx: 110 |  Loss: (0.1725) |  Loss2: (0.0000) | Acc: (94.00%) (13380/14208)
Epoch: 117 | Batch_idx: 120 |  Loss: (0.1691) |  Loss2: (0.0000) | Acc: (94.00%) (14603/15488)
Epoch: 117 | Batch_idx: 130 |  Loss: (0.1688) |  Loss2: (0.0000) | Acc: (94.00%) (15815/16768)
Epoch: 117 | Batch_idx: 140 |  Loss: (0.1710) |  Loss2: (0.0000) | Acc: (94.00%) (17001/18048)
Epoch: 117 | Batch_idx: 150 |  Loss: (0.1712) |  Loss2: (0.0000) | Acc: (94.00%) (18201/19328)
Epoch: 117 | Batch_idx: 160 |  Loss: (0.1735) |  Loss2: (0.0000) | Acc: (94.00%) (19384/20608)
Epoch: 117 | Batch_idx: 170 |  Loss: (0.1738) |  Loss2: (0.0000) | Acc: (94.00%) (20585/21888)
Epoch: 117 | Batch_idx: 180 |  Loss: (0.1740) |  Loss2: (0.0000) | Acc: (94.00%) (21787/23168)
Epoch: 117 | Batch_idx: 190 |  Loss: (0.1758) |  Loss2: (0.0000) | Acc: (93.00%) (22978/24448)
Epoch: 117 | Batch_idx: 200 |  Loss: (0.1752) |  Loss2: (0.0000) | Acc: (93.00%) (24182/25728)
Epoch: 117 | Batch_idx: 210 |  Loss: (0.1740) |  Loss2: (0.0000) | Acc: (94.00%) (25398/27008)
Epoch: 117 | Batch_idx: 220 |  Loss: (0.1751) |  Loss2: (0.0000) | Acc: (94.00%) (26599/28288)
Epoch: 117 | Batch_idx: 230 |  Loss: (0.1748) |  Loss2: (0.0000) | Acc: (94.00%) (27803/29568)
Epoch: 117 | Batch_idx: 240 |  Loss: (0.1742) |  Loss2: (0.0000) | Acc: (94.00%) (29024/30848)
Epoch: 117 | Batch_idx: 250 |  Loss: (0.1741) |  Loss2: (0.0000) | Acc: (94.00%) (30232/32128)
Epoch: 117 | Batch_idx: 260 |  Loss: (0.1746) |  Loss2: (0.0000) | Acc: (94.00%) (31435/33408)
Epoch: 117 | Batch_idx: 270 |  Loss: (0.1750) |  Loss2: (0.0000) | Acc: (94.00%) (32631/34688)
Epoch: 117 | Batch_idx: 280 |  Loss: (0.1756) |  Loss2: (0.0000) | Acc: (94.00%) (33826/35968)
Epoch: 117 | Batch_idx: 290 |  Loss: (0.1752) |  Loss2: (0.0000) | Acc: (94.00%) (35040/37248)
Epoch: 117 | Batch_idx: 300 |  Loss: (0.1758) |  Loss2: (0.0000) | Acc: (94.00%) (36232/38528)
Epoch: 117 | Batch_idx: 310 |  Loss: (0.1755) |  Loss2: (0.0000) | Acc: (94.00%) (37439/39808)
Epoch: 117 | Batch_idx: 320 |  Loss: (0.1763) |  Loss2: (0.0000) | Acc: (94.00%) (38633/41088)
Epoch: 117 | Batch_idx: 330 |  Loss: (0.1772) |  Loss2: (0.0000) | Acc: (93.00%) (39818/42368)
Epoch: 117 | Batch_idx: 340 |  Loss: (0.1776) |  Loss2: (0.0000) | Acc: (93.00%) (41009/43648)
Epoch: 117 | Batch_idx: 350 |  Loss: (0.1775) |  Loss2: (0.0000) | Acc: (93.00%) (42210/44928)
Epoch: 117 | Batch_idx: 360 |  Loss: (0.1781) |  Loss2: (0.0000) | Acc: (93.00%) (43395/46208)
Epoch: 117 | Batch_idx: 370 |  Loss: (0.1787) |  Loss2: (0.0000) | Acc: (93.00%) (44588/47488)
Epoch: 117 | Batch_idx: 380 |  Loss: (0.1791) |  Loss2: (0.0000) | Acc: (93.00%) (45786/48768)
Epoch: 117 | Batch_idx: 390 |  Loss: (0.1797) |  Loss2: (0.0000) | Acc: (93.00%) (46933/50000)
# TEST : Loss: (0.3868) | Acc: (87.00%) (8791/10000)
percent tensor([0.5466, 0.5610, 0.5650, 0.5356, 0.5671, 0.5605, 0.5689, 0.5530, 0.5490,
        0.5629, 0.5571, 0.5709, 0.5477, 0.5274, 0.5638, 0.5445],
       device='cuda:0') torch.Size([16])
percent tensor([0.5425, 0.5433, 0.5479, 0.5509, 0.5503, 0.5428, 0.5458, 0.5503, 0.5453,
        0.5437, 0.5421, 0.5448, 0.5417, 0.5465, 0.5426, 0.5450],
       device='cuda:0') torch.Size([16])
percent tensor([0.6013, 0.6270, 0.4430, 0.4172, 0.4104, 0.5538, 0.5769, 0.4534, 0.5214,
        0.5972, 0.6141, 0.5043, 0.6270, 0.6015, 0.5812, 0.5937],
       device='cuda:0') torch.Size([16])
percent tensor([0.6651, 0.5416, 0.6789, 0.6896, 0.6822, 0.7533, 0.6156, 0.6809, 0.6290,
        0.5847, 0.6019, 0.6055, 0.5472, 0.6612, 0.6280, 0.7072],
       device='cuda:0') torch.Size([16])
percent tensor([0.6375, 0.6475, 0.6473, 0.5820, 0.5948, 0.5955, 0.6473, 0.6453, 0.6541,
        0.6269, 0.6104, 0.6192, 0.6454, 0.6215, 0.6534, 0.5965],
       device='cuda:0') torch.Size([16])
percent tensor([0.5610, 0.6532, 0.6476, 0.7093, 0.7094, 0.6980, 0.6722, 0.5914, 0.6902,
        0.5763, 0.6678, 0.7200, 0.6376, 0.7142, 0.6137, 0.5657],
       device='cuda:0') torch.Size([16])
percent tensor([0.6028, 0.6421, 0.5657, 0.4815, 0.6215, 0.7020, 0.5909, 0.4420, 0.6472,
        0.6300, 0.6486, 0.4637, 0.6755, 0.5952, 0.4605, 0.5763],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9997, 0.9998, 0.9996, 0.9999, 0.9993, 0.9998, 0.9997, 0.9998,
        0.9995, 0.9998, 0.9999, 0.9998, 0.9995, 0.9997, 0.9994],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 118 | Batch_idx: 0 |  Loss: (0.2015) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 118 | Batch_idx: 10 |  Loss: (0.1876) |  Loss2: (0.0000) | Acc: (92.00%) (1308/1408)
Epoch: 118 | Batch_idx: 20 |  Loss: (0.1940) |  Loss2: (0.0000) | Acc: (92.00%) (2495/2688)
Epoch: 118 | Batch_idx: 30 |  Loss: (0.1973) |  Loss2: (0.0000) | Acc: (92.00%) (3685/3968)
Epoch: 118 | Batch_idx: 40 |  Loss: (0.2068) |  Loss2: (0.0000) | Acc: (92.00%) (4855/5248)
Epoch: 118 | Batch_idx: 50 |  Loss: (0.2066) |  Loss2: (0.0000) | Acc: (92.00%) (6044/6528)
Epoch: 118 | Batch_idx: 60 |  Loss: (0.2070) |  Loss2: (0.0000) | Acc: (92.00%) (7236/7808)
Epoch: 118 | Batch_idx: 70 |  Loss: (0.2094) |  Loss2: (0.0000) | Acc: (92.00%) (8414/9088)
Epoch: 118 | Batch_idx: 80 |  Loss: (0.2124) |  Loss2: (0.0000) | Acc: (92.00%) (9591/10368)
Epoch: 118 | Batch_idx: 90 |  Loss: (0.2111) |  Loss2: (0.0000) | Acc: (92.00%) (10774/11648)
Epoch: 118 | Batch_idx: 100 |  Loss: (0.2110) |  Loss2: (0.0000) | Acc: (92.00%) (11966/12928)
Epoch: 118 | Batch_idx: 110 |  Loss: (0.2110) |  Loss2: (0.0000) | Acc: (92.00%) (13150/14208)
Epoch: 118 | Batch_idx: 120 |  Loss: (0.2121) |  Loss2: (0.0000) | Acc: (92.00%) (14334/15488)
Epoch: 118 | Batch_idx: 130 |  Loss: (0.2100) |  Loss2: (0.0000) | Acc: (92.00%) (15525/16768)
Epoch: 118 | Batch_idx: 140 |  Loss: (0.2106) |  Loss2: (0.0000) | Acc: (92.00%) (16693/18048)
Epoch: 118 | Batch_idx: 150 |  Loss: (0.2101) |  Loss2: (0.0000) | Acc: (92.00%) (17879/19328)
Epoch: 118 | Batch_idx: 160 |  Loss: (0.2106) |  Loss2: (0.0000) | Acc: (92.00%) (19056/20608)
Epoch: 118 | Batch_idx: 170 |  Loss: (0.2094) |  Loss2: (0.0000) | Acc: (92.00%) (20261/21888)
Epoch: 118 | Batch_idx: 180 |  Loss: (0.2089) |  Loss2: (0.0000) | Acc: (92.00%) (21457/23168)
Epoch: 118 | Batch_idx: 190 |  Loss: (0.2076) |  Loss2: (0.0000) | Acc: (92.00%) (22651/24448)
Epoch: 118 | Batch_idx: 200 |  Loss: (0.2060) |  Loss2: (0.0000) | Acc: (92.00%) (23854/25728)
Epoch: 118 | Batch_idx: 210 |  Loss: (0.2050) |  Loss2: (0.0000) | Acc: (92.00%) (25048/27008)
Epoch: 118 | Batch_idx: 220 |  Loss: (0.2039) |  Loss2: (0.0000) | Acc: (92.00%) (26248/28288)
Epoch: 118 | Batch_idx: 230 |  Loss: (0.2051) |  Loss2: (0.0000) | Acc: (92.00%) (27427/29568)
Epoch: 118 | Batch_idx: 240 |  Loss: (0.2044) |  Loss2: (0.0000) | Acc: (92.00%) (28619/30848)
Epoch: 118 | Batch_idx: 250 |  Loss: (0.2037) |  Loss2: (0.0000) | Acc: (92.00%) (29822/32128)
Epoch: 118 | Batch_idx: 260 |  Loss: (0.2034) |  Loss2: (0.0000) | Acc: (92.00%) (31019/33408)
Epoch: 118 | Batch_idx: 270 |  Loss: (0.2037) |  Loss2: (0.0000) | Acc: (92.00%) (32206/34688)
Epoch: 118 | Batch_idx: 280 |  Loss: (0.2026) |  Loss2: (0.0000) | Acc: (92.00%) (33409/35968)
Epoch: 118 | Batch_idx: 290 |  Loss: (0.2020) |  Loss2: (0.0000) | Acc: (92.00%) (34609/37248)
Epoch: 118 | Batch_idx: 300 |  Loss: (0.2023) |  Loss2: (0.0000) | Acc: (92.00%) (35792/38528)
Epoch: 118 | Batch_idx: 310 |  Loss: (0.2020) |  Loss2: (0.0000) | Acc: (92.00%) (36991/39808)
Epoch: 118 | Batch_idx: 320 |  Loss: (0.2016) |  Loss2: (0.0000) | Acc: (92.00%) (38182/41088)
Epoch: 118 | Batch_idx: 330 |  Loss: (0.2003) |  Loss2: (0.0000) | Acc: (92.00%) (39389/42368)
Epoch: 118 | Batch_idx: 340 |  Loss: (0.1997) |  Loss2: (0.0000) | Acc: (92.00%) (40590/43648)
Epoch: 118 | Batch_idx: 350 |  Loss: (0.1988) |  Loss2: (0.0000) | Acc: (93.00%) (41800/44928)
Epoch: 118 | Batch_idx: 360 |  Loss: (0.1992) |  Loss2: (0.0000) | Acc: (93.00%) (42981/46208)
Epoch: 118 | Batch_idx: 370 |  Loss: (0.1984) |  Loss2: (0.0000) | Acc: (93.00%) (44193/47488)
Epoch: 118 | Batch_idx: 380 |  Loss: (0.1988) |  Loss2: (0.0000) | Acc: (93.00%) (45376/48768)
Epoch: 118 | Batch_idx: 390 |  Loss: (0.1988) |  Loss2: (0.0000) | Acc: (93.00%) (46519/50000)
# TEST : Loss: (0.3945) | Acc: (87.00%) (8746/10000)
percent tensor([0.5393, 0.5508, 0.5606, 0.5287, 0.5618, 0.5547, 0.5600, 0.5445, 0.5403,
        0.5539, 0.5481, 0.5651, 0.5395, 0.5163, 0.5559, 0.5363],
       device='cuda:0') torch.Size([16])
percent tensor([0.5390, 0.5401, 0.5440, 0.5475, 0.5462, 0.5395, 0.5422, 0.5466, 0.5421,
        0.5404, 0.5390, 0.5411, 0.5385, 0.5431, 0.5394, 0.5416],
       device='cuda:0') torch.Size([16])
percent tensor([0.6072, 0.6157, 0.4663, 0.4297, 0.4267, 0.5688, 0.5720, 0.4596, 0.5238,
        0.5938, 0.6088, 0.5180, 0.6285, 0.5980, 0.5848, 0.5973],
       device='cuda:0') torch.Size([16])
percent tensor([0.6802, 0.5610, 0.6948, 0.7022, 0.6962, 0.7619, 0.6359, 0.7005, 0.6474,
        0.5999, 0.6202, 0.6197, 0.5643, 0.6690, 0.6552, 0.7198],
       device='cuda:0') torch.Size([16])
percent tensor([0.6589, 0.6507, 0.6723, 0.5927, 0.6166, 0.6051, 0.6613, 0.6612, 0.6562,
        0.6468, 0.6213, 0.6330, 0.6496, 0.6265, 0.6617, 0.6209],
       device='cuda:0') torch.Size([16])
percent tensor([0.5490, 0.6476, 0.6193, 0.6978, 0.6952, 0.6996, 0.6648, 0.5659, 0.6807,
        0.5653, 0.6609, 0.6880, 0.6204, 0.7194, 0.5926, 0.5748],
       device='cuda:0') torch.Size([16])
percent tensor([0.5190, 0.5537, 0.4774, 0.4023, 0.5710, 0.6283, 0.5182, 0.3887, 0.5771,
        0.5528, 0.5607, 0.3712, 0.5769, 0.5324, 0.3759, 0.5099],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9997, 0.9998, 0.9996, 0.9999, 0.9995, 0.9998, 0.9997, 0.9998,
        0.9996, 0.9997, 0.9998, 0.9998, 0.9995, 0.9995, 0.9997],
       device='cuda:0') torch.Size([16])
Epoch: 119 | Batch_idx: 0 |  Loss: (0.1768) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 119 | Batch_idx: 10 |  Loss: (0.1994) |  Loss2: (0.0000) | Acc: (92.00%) (1307/1408)
Epoch: 119 | Batch_idx: 20 |  Loss: (0.1990) |  Loss2: (0.0000) | Acc: (93.00%) (2504/2688)
Epoch: 119 | Batch_idx: 30 |  Loss: (0.1945) |  Loss2: (0.0000) | Acc: (92.00%) (3690/3968)
Epoch: 119 | Batch_idx: 40 |  Loss: (0.1971) |  Loss2: (0.0000) | Acc: (92.00%) (4876/5248)
Epoch: 119 | Batch_idx: 50 |  Loss: (0.1939) |  Loss2: (0.0000) | Acc: (93.00%) (6074/6528)
Epoch: 119 | Batch_idx: 60 |  Loss: (0.1923) |  Loss2: (0.0000) | Acc: (93.00%) (7272/7808)
Epoch: 119 | Batch_idx: 70 |  Loss: (0.1927) |  Loss2: (0.0000) | Acc: (93.00%) (8468/9088)
Epoch: 119 | Batch_idx: 80 |  Loss: (0.1947) |  Loss2: (0.0000) | Acc: (93.00%) (9662/10368)
Epoch: 119 | Batch_idx: 90 |  Loss: (0.1922) |  Loss2: (0.0000) | Acc: (93.00%) (10873/11648)
Epoch: 119 | Batch_idx: 100 |  Loss: (0.1921) |  Loss2: (0.0000) | Acc: (93.00%) (12067/12928)
Epoch: 119 | Batch_idx: 110 |  Loss: (0.1919) |  Loss2: (0.0000) | Acc: (93.00%) (13266/14208)
Epoch: 119 | Batch_idx: 120 |  Loss: (0.1890) |  Loss2: (0.0000) | Acc: (93.00%) (14473/15488)
Epoch: 119 | Batch_idx: 130 |  Loss: (0.1888) |  Loss2: (0.0000) | Acc: (93.00%) (15670/16768)
Epoch: 119 | Batch_idx: 140 |  Loss: (0.1910) |  Loss2: (0.0000) | Acc: (93.00%) (16846/18048)
Epoch: 119 | Batch_idx: 150 |  Loss: (0.1926) |  Loss2: (0.0000) | Acc: (93.00%) (18035/19328)
Epoch: 119 | Batch_idx: 160 |  Loss: (0.1917) |  Loss2: (0.0000) | Acc: (93.00%) (19235/20608)
Epoch: 119 | Batch_idx: 170 |  Loss: (0.1916) |  Loss2: (0.0000) | Acc: (93.00%) (20428/21888)
Epoch: 119 | Batch_idx: 180 |  Loss: (0.1905) |  Loss2: (0.0000) | Acc: (93.00%) (21640/23168)
Epoch: 119 | Batch_idx: 190 |  Loss: (0.1888) |  Loss2: (0.0000) | Acc: (93.00%) (22851/24448)
Epoch: 119 | Batch_idx: 200 |  Loss: (0.1883) |  Loss2: (0.0000) | Acc: (93.00%) (24056/25728)
Epoch: 119 | Batch_idx: 210 |  Loss: (0.1880) |  Loss2: (0.0000) | Acc: (93.00%) (25251/27008)
Epoch: 119 | Batch_idx: 220 |  Loss: (0.1874) |  Loss2: (0.0000) | Acc: (93.00%) (26461/28288)
Epoch: 119 | Batch_idx: 230 |  Loss: (0.1872) |  Loss2: (0.0000) | Acc: (93.00%) (27657/29568)
Epoch: 119 | Batch_idx: 240 |  Loss: (0.1874) |  Loss2: (0.0000) | Acc: (93.00%) (28853/30848)
Epoch: 119 | Batch_idx: 250 |  Loss: (0.1866) |  Loss2: (0.0000) | Acc: (93.00%) (30051/32128)
Epoch: 119 | Batch_idx: 260 |  Loss: (0.1864) |  Loss2: (0.0000) | Acc: (93.00%) (31257/33408)
Epoch: 119 | Batch_idx: 270 |  Loss: (0.1858) |  Loss2: (0.0000) | Acc: (93.00%) (32470/34688)
Epoch: 119 | Batch_idx: 280 |  Loss: (0.1861) |  Loss2: (0.0000) | Acc: (93.00%) (33672/35968)
Epoch: 119 | Batch_idx: 290 |  Loss: (0.1861) |  Loss2: (0.0000) | Acc: (93.00%) (34873/37248)
Epoch: 119 | Batch_idx: 300 |  Loss: (0.1853) |  Loss2: (0.0000) | Acc: (93.00%) (36084/38528)
Epoch: 119 | Batch_idx: 310 |  Loss: (0.1845) |  Loss2: (0.0000) | Acc: (93.00%) (37296/39808)
Epoch: 119 | Batch_idx: 320 |  Loss: (0.1846) |  Loss2: (0.0000) | Acc: (93.00%) (38493/41088)
Epoch: 119 | Batch_idx: 330 |  Loss: (0.1837) |  Loss2: (0.0000) | Acc: (93.00%) (39710/42368)
Epoch: 119 | Batch_idx: 340 |  Loss: (0.1835) |  Loss2: (0.0000) | Acc: (93.00%) (40913/43648)
Epoch: 119 | Batch_idx: 350 |  Loss: (0.1838) |  Loss2: (0.0000) | Acc: (93.00%) (42107/44928)
Epoch: 119 | Batch_idx: 360 |  Loss: (0.1830) |  Loss2: (0.0000) | Acc: (93.00%) (43324/46208)
Epoch: 119 | Batch_idx: 370 |  Loss: (0.1832) |  Loss2: (0.0000) | Acc: (93.00%) (44532/47488)
Epoch: 119 | Batch_idx: 380 |  Loss: (0.1832) |  Loss2: (0.0000) | Acc: (93.00%) (45732/48768)
Epoch: 119 | Batch_idx: 390 |  Loss: (0.1827) |  Loss2: (0.0000) | Acc: (93.00%) (46893/50000)
# TEST : Loss: (0.3833) | Acc: (87.00%) (8781/10000)
percent tensor([0.5372, 0.5481, 0.5578, 0.5262, 0.5591, 0.5537, 0.5573, 0.5417, 0.5380,
        0.5508, 0.5459, 0.5621, 0.5372, 0.5141, 0.5540, 0.5342],
       device='cuda:0') torch.Size([16])
percent tensor([0.5379, 0.5390, 0.5430, 0.5461, 0.5450, 0.5384, 0.5410, 0.5455, 0.5411,
        0.5393, 0.5380, 0.5401, 0.5375, 0.5418, 0.5383, 0.5404],
       device='cuda:0') torch.Size([16])
percent tensor([0.6093, 0.6159, 0.4627, 0.4267, 0.4232, 0.5710, 0.5719, 0.4574, 0.5252,
        0.5944, 0.6100, 0.5183, 0.6292, 0.5991, 0.5885, 0.5984],
       device='cuda:0') torch.Size([16])
percent tensor([0.6770, 0.5544, 0.6897, 0.6958, 0.6905, 0.7577, 0.6303, 0.6986, 0.6416,
        0.5872, 0.6118, 0.6147, 0.5561, 0.6611, 0.6566, 0.7118],
       device='cuda:0') torch.Size([16])
percent tensor([0.6661, 0.6500, 0.6771, 0.5987, 0.6249, 0.6149, 0.6681, 0.6686, 0.6568,
        0.6518, 0.6233, 0.6307, 0.6457, 0.6294, 0.6658, 0.6330],
       device='cuda:0') torch.Size([16])
percent tensor([0.5510, 0.6515, 0.6107, 0.6883, 0.6924, 0.7043, 0.6656, 0.5567, 0.6804,
        0.5733, 0.6621, 0.6795, 0.6257, 0.7172, 0.5861, 0.5833],
       device='cuda:0') torch.Size([16])
percent tensor([0.5389, 0.5764, 0.4941, 0.4206, 0.5897, 0.6412, 0.5380, 0.3971, 0.5969,
        0.5809, 0.5854, 0.3820, 0.5936, 0.5569, 0.3847, 0.5278],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9997, 0.9998, 0.9996, 0.9999, 0.9996, 0.9998, 0.9997, 0.9998,
        0.9997, 0.9998, 0.9998, 0.9998, 0.9996, 0.9995, 0.9997],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 120 | Batch_idx: 0 |  Loss: (0.1180) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 120 | Batch_idx: 10 |  Loss: (0.1788) |  Loss2: (0.0000) | Acc: (93.00%) (1323/1408)
Epoch: 120 | Batch_idx: 20 |  Loss: (0.1679) |  Loss2: (0.0000) | Acc: (94.00%) (2536/2688)
Epoch: 120 | Batch_idx: 30 |  Loss: (0.1646) |  Loss2: (0.0000) | Acc: (94.00%) (3739/3968)
Epoch: 120 | Batch_idx: 40 |  Loss: (0.1666) |  Loss2: (0.0000) | Acc: (94.00%) (4939/5248)
Epoch: 120 | Batch_idx: 50 |  Loss: (0.1651) |  Loss2: (0.0000) | Acc: (94.00%) (6155/6528)
Epoch: 120 | Batch_idx: 60 |  Loss: (0.1634) |  Loss2: (0.0000) | Acc: (94.00%) (7368/7808)
Epoch: 120 | Batch_idx: 70 |  Loss: (0.1620) |  Loss2: (0.0000) | Acc: (94.00%) (8581/9088)
Epoch: 120 | Batch_idx: 80 |  Loss: (0.1634) |  Loss2: (0.0000) | Acc: (94.00%) (9788/10368)
Epoch: 120 | Batch_idx: 90 |  Loss: (0.1623) |  Loss2: (0.0000) | Acc: (94.00%) (11001/11648)
Epoch: 120 | Batch_idx: 100 |  Loss: (0.1635) |  Loss2: (0.0000) | Acc: (94.00%) (12202/12928)
Epoch: 120 | Batch_idx: 110 |  Loss: (0.1640) |  Loss2: (0.0000) | Acc: (94.00%) (13411/14208)
Epoch: 120 | Batch_idx: 120 |  Loss: (0.1640) |  Loss2: (0.0000) | Acc: (94.00%) (14623/15488)
Epoch: 120 | Batch_idx: 130 |  Loss: (0.1650) |  Loss2: (0.0000) | Acc: (94.00%) (15825/16768)
Epoch: 120 | Batch_idx: 140 |  Loss: (0.1653) |  Loss2: (0.0000) | Acc: (94.00%) (17021/18048)
Epoch: 120 | Batch_idx: 150 |  Loss: (0.1662) |  Loss2: (0.0000) | Acc: (94.00%) (18229/19328)
Epoch: 120 | Batch_idx: 160 |  Loss: (0.1669) |  Loss2: (0.0000) | Acc: (94.00%) (19434/20608)
Epoch: 120 | Batch_idx: 170 |  Loss: (0.1676) |  Loss2: (0.0000) | Acc: (94.00%) (20630/21888)
Epoch: 120 | Batch_idx: 180 |  Loss: (0.1695) |  Loss2: (0.0000) | Acc: (94.00%) (21820/23168)
Epoch: 120 | Batch_idx: 190 |  Loss: (0.1708) |  Loss2: (0.0000) | Acc: (94.00%) (23021/24448)
Epoch: 120 | Batch_idx: 200 |  Loss: (0.1717) |  Loss2: (0.0000) | Acc: (94.00%) (24218/25728)
Epoch: 120 | Batch_idx: 210 |  Loss: (0.1722) |  Loss2: (0.0000) | Acc: (94.00%) (25425/27008)
Epoch: 120 | Batch_idx: 220 |  Loss: (0.1719) |  Loss2: (0.0000) | Acc: (94.00%) (26638/28288)
Epoch: 120 | Batch_idx: 230 |  Loss: (0.1728) |  Loss2: (0.0000) | Acc: (94.00%) (27840/29568)
Epoch: 120 | Batch_idx: 240 |  Loss: (0.1740) |  Loss2: (0.0000) | Acc: (94.00%) (29030/30848)
Epoch: 120 | Batch_idx: 250 |  Loss: (0.1737) |  Loss2: (0.0000) | Acc: (94.00%) (30234/32128)
Epoch: 120 | Batch_idx: 260 |  Loss: (0.1734) |  Loss2: (0.0000) | Acc: (94.00%) (31445/33408)
Epoch: 120 | Batch_idx: 270 |  Loss: (0.1738) |  Loss2: (0.0000) | Acc: (94.00%) (32646/34688)
Epoch: 120 | Batch_idx: 280 |  Loss: (0.1739) |  Loss2: (0.0000) | Acc: (94.00%) (33854/35968)
Epoch: 120 | Batch_idx: 290 |  Loss: (0.1734) |  Loss2: (0.0000) | Acc: (94.00%) (35063/37248)
Epoch: 120 | Batch_idx: 300 |  Loss: (0.1742) |  Loss2: (0.0000) | Acc: (94.00%) (36243/38528)
Epoch: 120 | Batch_idx: 310 |  Loss: (0.1749) |  Loss2: (0.0000) | Acc: (94.00%) (37437/39808)
Epoch: 120 | Batch_idx: 320 |  Loss: (0.1755) |  Loss2: (0.0000) | Acc: (94.00%) (38633/41088)
Epoch: 120 | Batch_idx: 330 |  Loss: (0.1754) |  Loss2: (0.0000) | Acc: (94.00%) (39835/42368)
Epoch: 120 | Batch_idx: 340 |  Loss: (0.1750) |  Loss2: (0.0000) | Acc: (94.00%) (41030/43648)
Epoch: 120 | Batch_idx: 350 |  Loss: (0.1748) |  Loss2: (0.0000) | Acc: (94.00%) (42233/44928)
Epoch: 120 | Batch_idx: 360 |  Loss: (0.1756) |  Loss2: (0.0000) | Acc: (93.00%) (43422/46208)
Epoch: 120 | Batch_idx: 370 |  Loss: (0.1755) |  Loss2: (0.0000) | Acc: (93.00%) (44633/47488)
Epoch: 120 | Batch_idx: 380 |  Loss: (0.1758) |  Loss2: (0.0000) | Acc: (93.00%) (45837/48768)
Epoch: 120 | Batch_idx: 390 |  Loss: (0.1759) |  Loss2: (0.0000) | Acc: (93.00%) (46987/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_120.pth.tar'
# TEST : Loss: (0.4303) | Acc: (86.00%) (8691/10000)
percent tensor([0.5362, 0.5491, 0.5581, 0.5241, 0.5590, 0.5503, 0.5585, 0.5409, 0.5396,
        0.5512, 0.5460, 0.5632, 0.5366, 0.5174, 0.5518, 0.5330],
       device='cuda:0') torch.Size([16])
percent tensor([0.5386, 0.5393, 0.5427, 0.5470, 0.5450, 0.5391, 0.5415, 0.5454, 0.5408,
        0.5393, 0.5389, 0.5398, 0.5379, 0.5424, 0.5388, 0.5409],
       device='cuda:0') torch.Size([16])
percent tensor([0.6125, 0.6150, 0.4613, 0.4236, 0.4237, 0.5702, 0.5760, 0.4658, 0.5291,
        0.5964, 0.6045, 0.5192, 0.6264, 0.6005, 0.5910, 0.5955],
       device='cuda:0') torch.Size([16])
percent tensor([0.6781, 0.5748, 0.6849, 0.6975, 0.6849, 0.7570, 0.6385, 0.6938, 0.6377,
        0.5954, 0.6076, 0.6254, 0.5576, 0.6711, 0.6581, 0.7125],
       device='cuda:0') torch.Size([16])
percent tensor([0.6719, 0.6497, 0.6533, 0.5902, 0.6131, 0.6151, 0.6605, 0.6562, 0.6599,
        0.6396, 0.6221, 0.6027, 0.6460, 0.6217, 0.6635, 0.6282],
       device='cuda:0') torch.Size([16])
percent tensor([0.5501, 0.6618, 0.6278, 0.6894, 0.6968, 0.6934, 0.6756, 0.5882, 0.6814,
        0.5968, 0.6663, 0.7007, 0.6509, 0.7106, 0.5934, 0.5810],
       device='cuda:0') torch.Size([16])
percent tensor([0.5321, 0.5895, 0.5403, 0.4507, 0.6069, 0.6383, 0.5274, 0.4151, 0.5876,
        0.6127, 0.5856, 0.3863, 0.5961, 0.5330, 0.3845, 0.5271],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9996, 0.9998, 0.9998, 0.9999, 0.9994, 0.9997, 0.9996, 0.9997,
        0.9994, 0.9998, 0.9999, 0.9998, 0.9994, 0.9995, 0.9994],
       device='cuda:0') torch.Size([16])
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(181.3757, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(820.6768, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(814.6559, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1517.5006, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(489.3974, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2258.7869, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4256.7334, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1388.2632, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6187.0928, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11802.8105, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3908.6516, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16507.1875, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 121 | Batch_idx: 0 |  Loss: (0.1980) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 121 | Batch_idx: 10 |  Loss: (0.1616) |  Loss2: (0.0000) | Acc: (94.00%) (1324/1408)
Epoch: 121 | Batch_idx: 20 |  Loss: (0.1788) |  Loss2: (0.0000) | Acc: (93.00%) (2512/2688)
Epoch: 121 | Batch_idx: 30 |  Loss: (0.1644) |  Loss2: (0.0000) | Acc: (94.00%) (3735/3968)
Epoch: 121 | Batch_idx: 40 |  Loss: (0.1643) |  Loss2: (0.0000) | Acc: (94.00%) (4943/5248)
Epoch: 121 | Batch_idx: 50 |  Loss: (0.1637) |  Loss2: (0.0000) | Acc: (94.00%) (6153/6528)
Epoch: 121 | Batch_idx: 60 |  Loss: (0.1662) |  Loss2: (0.0000) | Acc: (94.00%) (7355/7808)
Epoch: 121 | Batch_idx: 70 |  Loss: (0.1650) |  Loss2: (0.0000) | Acc: (94.00%) (8561/9088)
Epoch: 121 | Batch_idx: 80 |  Loss: (0.1665) |  Loss2: (0.0000) | Acc: (94.00%) (9767/10368)
Epoch: 121 | Batch_idx: 90 |  Loss: (0.1643) |  Loss2: (0.0000) | Acc: (94.00%) (10992/11648)
Epoch: 121 | Batch_idx: 100 |  Loss: (0.1630) |  Loss2: (0.0000) | Acc: (94.00%) (12198/12928)
Epoch: 121 | Batch_idx: 110 |  Loss: (0.1659) |  Loss2: (0.0000) | Acc: (94.00%) (13392/14208)
Epoch: 121 | Batch_idx: 120 |  Loss: (0.1654) |  Loss2: (0.0000) | Acc: (94.00%) (14601/15488)
Epoch: 121 | Batch_idx: 130 |  Loss: (0.1665) |  Loss2: (0.0000) | Acc: (94.00%) (15805/16768)
Epoch: 121 | Batch_idx: 140 |  Loss: (0.1671) |  Loss2: (0.0000) | Acc: (94.00%) (17006/18048)
Epoch: 121 | Batch_idx: 150 |  Loss: (0.1667) |  Loss2: (0.0000) | Acc: (94.00%) (18223/19328)
Epoch: 121 | Batch_idx: 160 |  Loss: (0.1657) |  Loss2: (0.0000) | Acc: (94.00%) (19442/20608)
Epoch: 121 | Batch_idx: 170 |  Loss: (0.1670) |  Loss2: (0.0000) | Acc: (94.00%) (20636/21888)
Epoch: 121 | Batch_idx: 180 |  Loss: (0.1674) |  Loss2: (0.0000) | Acc: (94.00%) (21839/23168)
Epoch: 121 | Batch_idx: 190 |  Loss: (0.1675) |  Loss2: (0.0000) | Acc: (94.00%) (23052/24448)
Epoch: 121 | Batch_idx: 200 |  Loss: (0.1676) |  Loss2: (0.0000) | Acc: (94.00%) (24256/25728)
Epoch: 121 | Batch_idx: 210 |  Loss: (0.1676) |  Loss2: (0.0000) | Acc: (94.00%) (25468/27008)
Epoch: 121 | Batch_idx: 220 |  Loss: (0.1675) |  Loss2: (0.0000) | Acc: (94.00%) (26675/28288)
Epoch: 121 | Batch_idx: 230 |  Loss: (0.1679) |  Loss2: (0.0000) | Acc: (94.00%) (27879/29568)
Epoch: 121 | Batch_idx: 240 |  Loss: (0.1689) |  Loss2: (0.0000) | Acc: (94.00%) (29068/30848)
Epoch: 121 | Batch_idx: 250 |  Loss: (0.1687) |  Loss2: (0.0000) | Acc: (94.00%) (30267/32128)
Epoch: 121 | Batch_idx: 260 |  Loss: (0.1688) |  Loss2: (0.0000) | Acc: (94.00%) (31469/33408)
Epoch: 121 | Batch_idx: 270 |  Loss: (0.1699) |  Loss2: (0.0000) | Acc: (94.00%) (32662/34688)
Epoch: 121 | Batch_idx: 280 |  Loss: (0.1700) |  Loss2: (0.0000) | Acc: (94.00%) (33865/35968)
Epoch: 121 | Batch_idx: 290 |  Loss: (0.1696) |  Loss2: (0.0000) | Acc: (94.00%) (35074/37248)
Epoch: 121 | Batch_idx: 300 |  Loss: (0.1693) |  Loss2: (0.0000) | Acc: (94.00%) (36277/38528)
Epoch: 121 | Batch_idx: 310 |  Loss: (0.1701) |  Loss2: (0.0000) | Acc: (94.00%) (37477/39808)
Epoch: 121 | Batch_idx: 320 |  Loss: (0.1706) |  Loss2: (0.0000) | Acc: (94.00%) (38672/41088)
Epoch: 121 | Batch_idx: 330 |  Loss: (0.1705) |  Loss2: (0.0000) | Acc: (94.00%) (39882/42368)
Epoch: 121 | Batch_idx: 340 |  Loss: (0.1707) |  Loss2: (0.0000) | Acc: (94.00%) (41075/43648)
Epoch: 121 | Batch_idx: 350 |  Loss: (0.1709) |  Loss2: (0.0000) | Acc: (94.00%) (42270/44928)
Epoch: 121 | Batch_idx: 360 |  Loss: (0.1709) |  Loss2: (0.0000) | Acc: (94.00%) (43480/46208)
Epoch: 121 | Batch_idx: 370 |  Loss: (0.1713) |  Loss2: (0.0000) | Acc: (94.00%) (44680/47488)
Epoch: 121 | Batch_idx: 380 |  Loss: (0.1712) |  Loss2: (0.0000) | Acc: (94.00%) (45886/48768)
Epoch: 121 | Batch_idx: 390 |  Loss: (0.1719) |  Loss2: (0.0000) | Acc: (94.00%) (47035/50000)
# TEST : Loss: (0.4276) | Acc: (86.00%) (8690/10000)
percent tensor([0.5369, 0.5492, 0.5559, 0.5241, 0.5567, 0.5515, 0.5571, 0.5406, 0.5374,
        0.5507, 0.5455, 0.5603, 0.5366, 0.5148, 0.5528, 0.5336],
       device='cuda:0') torch.Size([16])
percent tensor([0.5379, 0.5393, 0.5419, 0.5460, 0.5447, 0.5369, 0.5417, 0.5472, 0.5408,
        0.5395, 0.5378, 0.5406, 0.5373, 0.5433, 0.5381, 0.5400],
       device='cuda:0') torch.Size([16])
percent tensor([0.6123, 0.6126, 0.4799, 0.4378, 0.4286, 0.5772, 0.5751, 0.4709, 0.5309,
        0.5987, 0.6081, 0.5199, 0.6285, 0.5850, 0.5950, 0.6008],
       device='cuda:0') torch.Size([16])
percent tensor([0.6784, 0.5567, 0.6893, 0.6975, 0.6914, 0.7535, 0.6315, 0.7055, 0.6391,
        0.5877, 0.6073, 0.6245, 0.5528, 0.6570, 0.6537, 0.7067],
       device='cuda:0') torch.Size([16])
percent tensor([0.6630, 0.6512, 0.6503, 0.5872, 0.6157, 0.6155, 0.6604, 0.6529, 0.6604,
        0.6401, 0.6169, 0.6039, 0.6521, 0.6332, 0.6612, 0.6261],
       device='cuda:0') torch.Size([16])
percent tensor([0.5218, 0.6172, 0.5901, 0.6706, 0.6945, 0.6905, 0.6351, 0.5376, 0.6718,
        0.5609, 0.6323, 0.6772, 0.6238, 0.6894, 0.5686, 0.5551],
       device='cuda:0') torch.Size([16])
percent tensor([0.5328, 0.5845, 0.5129, 0.4428, 0.5884, 0.6489, 0.5184, 0.3807, 0.5759,
        0.6127, 0.5916, 0.4075, 0.5952, 0.5299, 0.4044, 0.5145],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9997, 0.9998, 0.9998, 0.9999, 0.9994, 0.9998, 0.9998, 0.9998,
        0.9997, 0.9997, 0.9999, 0.9998, 0.9996, 0.9996, 0.9996],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 122 | Batch_idx: 0 |  Loss: (0.1286) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 122 | Batch_idx: 10 |  Loss: (0.1719) |  Loss2: (0.0000) | Acc: (95.00%) (1338/1408)
Epoch: 122 | Batch_idx: 20 |  Loss: (0.1898) |  Loss2: (0.0000) | Acc: (93.00%) (2525/2688)
Epoch: 122 | Batch_idx: 30 |  Loss: (0.1942) |  Loss2: (0.0000) | Acc: (93.00%) (3711/3968)
Epoch: 122 | Batch_idx: 40 |  Loss: (0.2008) |  Loss2: (0.0000) | Acc: (93.00%) (4886/5248)
Epoch: 122 | Batch_idx: 50 |  Loss: (0.2025) |  Loss2: (0.0000) | Acc: (92.00%) (6070/6528)
Epoch: 122 | Batch_idx: 60 |  Loss: (0.2049) |  Loss2: (0.0000) | Acc: (92.00%) (7259/7808)
Epoch: 122 | Batch_idx: 70 |  Loss: (0.2069) |  Loss2: (0.0000) | Acc: (92.00%) (8434/9088)
Epoch: 122 | Batch_idx: 80 |  Loss: (0.2038) |  Loss2: (0.0000) | Acc: (92.00%) (9640/10368)
Epoch: 122 | Batch_idx: 90 |  Loss: (0.2069) |  Loss2: (0.0000) | Acc: (92.00%) (10818/11648)
Epoch: 122 | Batch_idx: 100 |  Loss: (0.2091) |  Loss2: (0.0000) | Acc: (92.00%) (11993/12928)
Epoch: 122 | Batch_idx: 110 |  Loss: (0.2093) |  Loss2: (0.0000) | Acc: (92.00%) (13178/14208)
Epoch: 122 | Batch_idx: 120 |  Loss: (0.2074) |  Loss2: (0.0000) | Acc: (92.00%) (14364/15488)
Epoch: 122 | Batch_idx: 130 |  Loss: (0.2064) |  Loss2: (0.0000) | Acc: (92.00%) (15555/16768)
Epoch: 122 | Batch_idx: 140 |  Loss: (0.2068) |  Loss2: (0.0000) | Acc: (92.00%) (16741/18048)
Epoch: 122 | Batch_idx: 150 |  Loss: (0.2054) |  Loss2: (0.0000) | Acc: (92.00%) (17939/19328)
Epoch: 122 | Batch_idx: 160 |  Loss: (0.2044) |  Loss2: (0.0000) | Acc: (92.00%) (19136/20608)
Epoch: 122 | Batch_idx: 170 |  Loss: (0.2046) |  Loss2: (0.0000) | Acc: (92.00%) (20315/21888)
Epoch: 122 | Batch_idx: 180 |  Loss: (0.2061) |  Loss2: (0.0000) | Acc: (92.00%) (21490/23168)
Epoch: 122 | Batch_idx: 190 |  Loss: (0.2057) |  Loss2: (0.0000) | Acc: (92.00%) (22682/24448)
Epoch: 122 | Batch_idx: 200 |  Loss: (0.2059) |  Loss2: (0.0000) | Acc: (92.00%) (23873/25728)
Epoch: 122 | Batch_idx: 210 |  Loss: (0.2061) |  Loss2: (0.0000) | Acc: (92.00%) (25056/27008)
Epoch: 122 | Batch_idx: 220 |  Loss: (0.2054) |  Loss2: (0.0000) | Acc: (92.00%) (26254/28288)
Epoch: 122 | Batch_idx: 230 |  Loss: (0.2056) |  Loss2: (0.0000) | Acc: (92.00%) (27437/29568)
Epoch: 122 | Batch_idx: 240 |  Loss: (0.2041) |  Loss2: (0.0000) | Acc: (92.00%) (28643/30848)
Epoch: 122 | Batch_idx: 250 |  Loss: (0.2030) |  Loss2: (0.0000) | Acc: (92.00%) (29845/32128)
Epoch: 122 | Batch_idx: 260 |  Loss: (0.2029) |  Loss2: (0.0000) | Acc: (92.00%) (31023/33408)
Epoch: 122 | Batch_idx: 270 |  Loss: (0.2027) |  Loss2: (0.0000) | Acc: (92.00%) (32217/34688)
Epoch: 122 | Batch_idx: 280 |  Loss: (0.2024) |  Loss2: (0.0000) | Acc: (92.00%) (33412/35968)
Epoch: 122 | Batch_idx: 290 |  Loss: (0.2009) |  Loss2: (0.0000) | Acc: (92.00%) (34629/37248)
Epoch: 122 | Batch_idx: 300 |  Loss: (0.2003) |  Loss2: (0.0000) | Acc: (92.00%) (35826/38528)
Epoch: 122 | Batch_idx: 310 |  Loss: (0.1998) |  Loss2: (0.0000) | Acc: (93.00%) (37022/39808)
Epoch: 122 | Batch_idx: 320 |  Loss: (0.1992) |  Loss2: (0.0000) | Acc: (93.00%) (38224/41088)
Epoch: 122 | Batch_idx: 330 |  Loss: (0.1995) |  Loss2: (0.0000) | Acc: (93.00%) (39412/42368)
Epoch: 122 | Batch_idx: 340 |  Loss: (0.1986) |  Loss2: (0.0000) | Acc: (93.00%) (40614/43648)
Epoch: 122 | Batch_idx: 350 |  Loss: (0.1996) |  Loss2: (0.0000) | Acc: (93.00%) (41789/44928)
Epoch: 122 | Batch_idx: 360 |  Loss: (0.1996) |  Loss2: (0.0000) | Acc: (92.00%) (42972/46208)
Epoch: 122 | Batch_idx: 370 |  Loss: (0.2003) |  Loss2: (0.0000) | Acc: (92.00%) (44156/47488)
Epoch: 122 | Batch_idx: 380 |  Loss: (0.1999) |  Loss2: (0.0000) | Acc: (93.00%) (45356/48768)
Epoch: 122 | Batch_idx: 390 |  Loss: (0.1990) |  Loss2: (0.0000) | Acc: (93.00%) (46516/50000)
# TEST : Loss: (0.4137) | Acc: (87.00%) (8734/10000)
percent tensor([0.5385, 0.5527, 0.5517, 0.5217, 0.5539, 0.5532, 0.5589, 0.5387, 0.5385,
        0.5521, 0.5489, 0.5580, 0.5388, 0.5176, 0.5551, 0.5352],
       device='cuda:0') torch.Size([16])
percent tensor([0.5338, 0.5349, 0.5384, 0.5428, 0.5406, 0.5334, 0.5370, 0.5436, 0.5370,
        0.5355, 0.5339, 0.5361, 0.5331, 0.5398, 0.5342, 0.5362],
       device='cuda:0') torch.Size([16])
percent tensor([0.6097, 0.6246, 0.4425, 0.4053, 0.3988, 0.5695, 0.5755, 0.4418, 0.5296,
        0.6000, 0.6172, 0.5087, 0.6379, 0.5968, 0.5917, 0.5978],
       device='cuda:0') torch.Size([16])
percent tensor([0.6746, 0.5744, 0.6758, 0.6823, 0.6748, 0.7443, 0.6319, 0.6976, 0.6357,
        0.6016, 0.6166, 0.6218, 0.5666, 0.6628, 0.6555, 0.7077],
       device='cuda:0') torch.Size([16])
percent tensor([0.6589, 0.6388, 0.6537, 0.5940, 0.6236, 0.6086, 0.6609, 0.6585, 0.6603,
        0.6341, 0.6141, 0.5987, 0.6339, 0.6301, 0.6561, 0.6168],
       device='cuda:0') torch.Size([16])
percent tensor([0.5402, 0.6353, 0.6078, 0.6623, 0.6893, 0.7017, 0.6503, 0.5316, 0.6853,
        0.5805, 0.6636, 0.6925, 0.6477, 0.7118, 0.5773, 0.5880],
       device='cuda:0') torch.Size([16])
percent tensor([0.5839, 0.6243, 0.5437, 0.4675, 0.5973, 0.6628, 0.5614, 0.4131, 0.5970,
        0.6577, 0.6090, 0.4326, 0.6244, 0.5636, 0.4472, 0.5795],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9997, 0.9998, 0.9998, 0.9998, 0.9996, 0.9998, 0.9997, 0.9997,
        0.9996, 0.9997, 0.9998, 0.9998, 0.9996, 0.9995, 0.9996],
       device='cuda:0') torch.Size([16])
Epoch: 123 | Batch_idx: 0 |  Loss: (0.1701) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 123 | Batch_idx: 10 |  Loss: (0.1977) |  Loss2: (0.0000) | Acc: (92.00%) (1309/1408)
Epoch: 123 | Batch_idx: 20 |  Loss: (0.1790) |  Loss2: (0.0000) | Acc: (93.00%) (2517/2688)
Epoch: 123 | Batch_idx: 30 |  Loss: (0.1799) |  Loss2: (0.0000) | Acc: (93.00%) (3713/3968)
Epoch: 123 | Batch_idx: 40 |  Loss: (0.1820) |  Loss2: (0.0000) | Acc: (93.00%) (4918/5248)
Epoch: 123 | Batch_idx: 50 |  Loss: (0.1822) |  Loss2: (0.0000) | Acc: (93.00%) (6114/6528)
Epoch: 123 | Batch_idx: 60 |  Loss: (0.1778) |  Loss2: (0.0000) | Acc: (93.00%) (7321/7808)
Epoch: 123 | Batch_idx: 70 |  Loss: (0.1797) |  Loss2: (0.0000) | Acc: (93.00%) (8516/9088)
Epoch: 123 | Batch_idx: 80 |  Loss: (0.1790) |  Loss2: (0.0000) | Acc: (93.00%) (9720/10368)
Epoch: 123 | Batch_idx: 90 |  Loss: (0.1812) |  Loss2: (0.0000) | Acc: (93.00%) (10916/11648)
Epoch: 123 | Batch_idx: 100 |  Loss: (0.1824) |  Loss2: (0.0000) | Acc: (93.00%) (12110/12928)
Epoch: 123 | Batch_idx: 110 |  Loss: (0.1825) |  Loss2: (0.0000) | Acc: (93.00%) (13311/14208)
Epoch: 123 | Batch_idx: 120 |  Loss: (0.1806) |  Loss2: (0.0000) | Acc: (93.00%) (14523/15488)
Epoch: 123 | Batch_idx: 130 |  Loss: (0.1816) |  Loss2: (0.0000) | Acc: (93.00%) (15721/16768)
Epoch: 123 | Batch_idx: 140 |  Loss: (0.1811) |  Loss2: (0.0000) | Acc: (93.00%) (16925/18048)
Epoch: 123 | Batch_idx: 150 |  Loss: (0.1814) |  Loss2: (0.0000) | Acc: (93.00%) (18127/19328)
Epoch: 123 | Batch_idx: 160 |  Loss: (0.1815) |  Loss2: (0.0000) | Acc: (93.00%) (19317/20608)
Epoch: 123 | Batch_idx: 170 |  Loss: (0.1827) |  Loss2: (0.0000) | Acc: (93.00%) (20502/21888)
Epoch: 123 | Batch_idx: 180 |  Loss: (0.1826) |  Loss2: (0.0000) | Acc: (93.00%) (21698/23168)
Epoch: 123 | Batch_idx: 190 |  Loss: (0.1826) |  Loss2: (0.0000) | Acc: (93.00%) (22889/24448)
Epoch: 123 | Batch_idx: 200 |  Loss: (0.1827) |  Loss2: (0.0000) | Acc: (93.00%) (24089/25728)
Epoch: 123 | Batch_idx: 210 |  Loss: (0.1830) |  Loss2: (0.0000) | Acc: (93.00%) (25278/27008)
Epoch: 123 | Batch_idx: 220 |  Loss: (0.1829) |  Loss2: (0.0000) | Acc: (93.00%) (26472/28288)
Epoch: 123 | Batch_idx: 230 |  Loss: (0.1817) |  Loss2: (0.0000) | Acc: (93.00%) (27682/29568)
Epoch: 123 | Batch_idx: 240 |  Loss: (0.1813) |  Loss2: (0.0000) | Acc: (93.00%) (28889/30848)
Epoch: 123 | Batch_idx: 250 |  Loss: (0.1817) |  Loss2: (0.0000) | Acc: (93.00%) (30087/32128)
Epoch: 123 | Batch_idx: 260 |  Loss: (0.1809) |  Loss2: (0.0000) | Acc: (93.00%) (31293/33408)
Epoch: 123 | Batch_idx: 270 |  Loss: (0.1812) |  Loss2: (0.0000) | Acc: (93.00%) (32488/34688)
Epoch: 123 | Batch_idx: 280 |  Loss: (0.1806) |  Loss2: (0.0000) | Acc: (93.00%) (33701/35968)
Epoch: 123 | Batch_idx: 290 |  Loss: (0.1802) |  Loss2: (0.0000) | Acc: (93.00%) (34903/37248)
Epoch: 123 | Batch_idx: 300 |  Loss: (0.1789) |  Loss2: (0.0000) | Acc: (93.00%) (36121/38528)
Epoch: 123 | Batch_idx: 310 |  Loss: (0.1793) |  Loss2: (0.0000) | Acc: (93.00%) (37320/39808)
Epoch: 123 | Batch_idx: 320 |  Loss: (0.1783) |  Loss2: (0.0000) | Acc: (93.00%) (38536/41088)
Epoch: 123 | Batch_idx: 330 |  Loss: (0.1781) |  Loss2: (0.0000) | Acc: (93.00%) (39741/42368)
Epoch: 123 | Batch_idx: 340 |  Loss: (0.1776) |  Loss2: (0.0000) | Acc: (93.00%) (40952/43648)
Epoch: 123 | Batch_idx: 350 |  Loss: (0.1767) |  Loss2: (0.0000) | Acc: (93.00%) (42177/44928)
Epoch: 123 | Batch_idx: 360 |  Loss: (0.1767) |  Loss2: (0.0000) | Acc: (93.00%) (43380/46208)
Epoch: 123 | Batch_idx: 370 |  Loss: (0.1768) |  Loss2: (0.0000) | Acc: (93.00%) (44576/47488)
Epoch: 123 | Batch_idx: 380 |  Loss: (0.1768) |  Loss2: (0.0000) | Acc: (93.00%) (45766/48768)
Epoch: 123 | Batch_idx: 390 |  Loss: (0.1766) |  Loss2: (0.0000) | Acc: (93.00%) (46928/50000)
# TEST : Loss: (0.3969) | Acc: (87.00%) (8790/10000)
percent tensor([0.5428, 0.5574, 0.5545, 0.5240, 0.5577, 0.5572, 0.5637, 0.5417, 0.5431,
        0.5566, 0.5538, 0.5620, 0.5433, 0.5211, 0.5594, 0.5393],
       device='cuda:0') torch.Size([16])
percent tensor([0.5340, 0.5344, 0.5389, 0.5436, 0.5408, 0.5339, 0.5366, 0.5436, 0.5369,
        0.5354, 0.5337, 0.5361, 0.5328, 0.5398, 0.5339, 0.5365],
       device='cuda:0') torch.Size([16])
percent tensor([0.6121, 0.6306, 0.4396, 0.4023, 0.3971, 0.5696, 0.5770, 0.4413, 0.5336,
        0.6032, 0.6224, 0.5143, 0.6430, 0.5980, 0.5933, 0.5983],
       device='cuda:0') torch.Size([16])
percent tensor([0.6814, 0.5887, 0.6803, 0.6819, 0.6774, 0.7451, 0.6421, 0.6990, 0.6444,
        0.6154, 0.6290, 0.6326, 0.5815, 0.6703, 0.6624, 0.7147],
       device='cuda:0') torch.Size([16])
percent tensor([0.6638, 0.6412, 0.6571, 0.5988, 0.6279, 0.6122, 0.6695, 0.6658, 0.6658,
        0.6393, 0.6224, 0.6021, 0.6338, 0.6412, 0.6606, 0.6243],
       device='cuda:0') torch.Size([16])
percent tensor([0.5375, 0.6381, 0.6070, 0.6641, 0.6851, 0.7023, 0.6498, 0.5219, 0.6892,
        0.5782, 0.6714, 0.6972, 0.6521, 0.7214, 0.5786, 0.5808],
       device='cuda:0') torch.Size([16])
percent tensor([0.5980, 0.6354, 0.5505, 0.4775, 0.6077, 0.6729, 0.5749, 0.4265, 0.6087,
        0.6660, 0.6166, 0.4422, 0.6329, 0.5845, 0.4589, 0.5892],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9997, 0.9998, 0.9998, 0.9998, 0.9996, 0.9998, 0.9997, 0.9997,
        0.9996, 0.9997, 0.9998, 0.9999, 0.9996, 0.9995, 0.9996],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 124 | Batch_idx: 0 |  Loss: (0.1239) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 124 | Batch_idx: 10 |  Loss: (0.1802) |  Loss2: (0.0000) | Acc: (93.00%) (1323/1408)
Epoch: 124 | Batch_idx: 20 |  Loss: (0.1688) |  Loss2: (0.0000) | Acc: (94.00%) (2545/2688)
Epoch: 124 | Batch_idx: 30 |  Loss: (0.1637) |  Loss2: (0.0000) | Acc: (94.00%) (3752/3968)
Epoch: 124 | Batch_idx: 40 |  Loss: (0.1659) |  Loss2: (0.0000) | Acc: (94.00%) (4964/5248)
Epoch: 124 | Batch_idx: 50 |  Loss: (0.1670) |  Loss2: (0.0000) | Acc: (94.00%) (6168/6528)
Epoch: 124 | Batch_idx: 60 |  Loss: (0.1673) |  Loss2: (0.0000) | Acc: (94.00%) (7385/7808)
Epoch: 124 | Batch_idx: 70 |  Loss: (0.1677) |  Loss2: (0.0000) | Acc: (94.00%) (8583/9088)
Epoch: 124 | Batch_idx: 80 |  Loss: (0.1659) |  Loss2: (0.0000) | Acc: (94.00%) (9799/10368)
Epoch: 124 | Batch_idx: 90 |  Loss: (0.1654) |  Loss2: (0.0000) | Acc: (94.00%) (11003/11648)
Epoch: 124 | Batch_idx: 100 |  Loss: (0.1661) |  Loss2: (0.0000) | Acc: (94.00%) (12208/12928)
Epoch: 124 | Batch_idx: 110 |  Loss: (0.1655) |  Loss2: (0.0000) | Acc: (94.00%) (13409/14208)
Epoch: 124 | Batch_idx: 120 |  Loss: (0.1663) |  Loss2: (0.0000) | Acc: (94.00%) (14618/15488)
Epoch: 124 | Batch_idx: 130 |  Loss: (0.1648) |  Loss2: (0.0000) | Acc: (94.00%) (15833/16768)
Epoch: 124 | Batch_idx: 140 |  Loss: (0.1642) |  Loss2: (0.0000) | Acc: (94.00%) (17049/18048)
Epoch: 124 | Batch_idx: 150 |  Loss: (0.1638) |  Loss2: (0.0000) | Acc: (94.00%) (18259/19328)
Epoch: 124 | Batch_idx: 160 |  Loss: (0.1636) |  Loss2: (0.0000) | Acc: (94.00%) (19464/20608)
Epoch: 124 | Batch_idx: 170 |  Loss: (0.1631) |  Loss2: (0.0000) | Acc: (94.00%) (20673/21888)
Epoch: 124 | Batch_idx: 180 |  Loss: (0.1620) |  Loss2: (0.0000) | Acc: (94.00%) (21883/23168)
Epoch: 124 | Batch_idx: 190 |  Loss: (0.1636) |  Loss2: (0.0000) | Acc: (94.00%) (23076/24448)
Epoch: 124 | Batch_idx: 200 |  Loss: (0.1639) |  Loss2: (0.0000) | Acc: (94.00%) (24274/25728)
Epoch: 124 | Batch_idx: 210 |  Loss: (0.1650) |  Loss2: (0.0000) | Acc: (94.00%) (25466/27008)
Epoch: 124 | Batch_idx: 220 |  Loss: (0.1652) |  Loss2: (0.0000) | Acc: (94.00%) (26672/28288)
Epoch: 124 | Batch_idx: 230 |  Loss: (0.1642) |  Loss2: (0.0000) | Acc: (94.00%) (27889/29568)
Epoch: 124 | Batch_idx: 240 |  Loss: (0.1651) |  Loss2: (0.0000) | Acc: (94.00%) (29083/30848)
Epoch: 124 | Batch_idx: 250 |  Loss: (0.1664) |  Loss2: (0.0000) | Acc: (94.00%) (30275/32128)
Epoch: 124 | Batch_idx: 260 |  Loss: (0.1663) |  Loss2: (0.0000) | Acc: (94.00%) (31475/33408)
Epoch: 124 | Batch_idx: 270 |  Loss: (0.1663) |  Loss2: (0.0000) | Acc: (94.00%) (32682/34688)
Epoch: 124 | Batch_idx: 280 |  Loss: (0.1664) |  Loss2: (0.0000) | Acc: (94.00%) (33880/35968)
Epoch: 124 | Batch_idx: 290 |  Loss: (0.1667) |  Loss2: (0.0000) | Acc: (94.00%) (35086/37248)
Epoch: 124 | Batch_idx: 300 |  Loss: (0.1665) |  Loss2: (0.0000) | Acc: (94.00%) (36297/38528)
Epoch: 124 | Batch_idx: 310 |  Loss: (0.1667) |  Loss2: (0.0000) | Acc: (94.00%) (37495/39808)
Epoch: 124 | Batch_idx: 320 |  Loss: (0.1670) |  Loss2: (0.0000) | Acc: (94.00%) (38696/41088)
Epoch: 124 | Batch_idx: 330 |  Loss: (0.1667) |  Loss2: (0.0000) | Acc: (94.00%) (39906/42368)
Epoch: 124 | Batch_idx: 340 |  Loss: (0.1672) |  Loss2: (0.0000) | Acc: (94.00%) (41104/43648)
Epoch: 124 | Batch_idx: 350 |  Loss: (0.1675) |  Loss2: (0.0000) | Acc: (94.00%) (42305/44928)
Epoch: 124 | Batch_idx: 360 |  Loss: (0.1678) |  Loss2: (0.0000) | Acc: (94.00%) (43509/46208)
Epoch: 124 | Batch_idx: 370 |  Loss: (0.1689) |  Loss2: (0.0000) | Acc: (94.00%) (44692/47488)
Epoch: 124 | Batch_idx: 380 |  Loss: (0.1691) |  Loss2: (0.0000) | Acc: (94.00%) (45891/48768)
Epoch: 124 | Batch_idx: 390 |  Loss: (0.1695) |  Loss2: (0.0000) | Acc: (94.00%) (47045/50000)
# TEST : Loss: (0.4134) | Acc: (87.00%) (8700/10000)
percent tensor([0.5457, 0.5587, 0.5594, 0.5249, 0.5617, 0.5591, 0.5677, 0.5413, 0.5471,
        0.5584, 0.5579, 0.5671, 0.5460, 0.5235, 0.5605, 0.5407],
       device='cuda:0') torch.Size([16])
percent tensor([0.5349, 0.5351, 0.5405, 0.5444, 0.5420, 0.5358, 0.5372, 0.5443, 0.5374,
        0.5357, 0.5337, 0.5368, 0.5330, 0.5390, 0.5347, 0.5375],
       device='cuda:0') torch.Size([16])
percent tensor([0.6059, 0.6246, 0.4321, 0.4066, 0.3949, 0.5498, 0.5658, 0.4269, 0.5244,
        0.5983, 0.6238, 0.5101, 0.6415, 0.5953, 0.5799, 0.5892],
       device='cuda:0') torch.Size([16])
percent tensor([0.6860, 0.6007, 0.6828, 0.6914, 0.6775, 0.7512, 0.6495, 0.6897, 0.6451,
        0.6206, 0.6412, 0.6421, 0.5856, 0.6766, 0.6671, 0.7203],
       device='cuda:0') torch.Size([16])
percent tensor([0.6677, 0.6519, 0.6466, 0.5822, 0.6205, 0.6089, 0.6762, 0.6532, 0.6670,
        0.6505, 0.6263, 0.6038, 0.6401, 0.6434, 0.6670, 0.6271],
       device='cuda:0') torch.Size([16])
percent tensor([0.5565, 0.6354, 0.6541, 0.6998, 0.6992, 0.7072, 0.6644, 0.5692, 0.6852,
        0.5734, 0.6687, 0.7251, 0.6416, 0.7285, 0.5965, 0.5975],
       device='cuda:0') torch.Size([16])
percent tensor([0.5916, 0.6188, 0.5382, 0.4474, 0.6043, 0.6552, 0.5692, 0.4122, 0.5963,
        0.6379, 0.6286, 0.4213, 0.6250, 0.5934, 0.4522, 0.5775],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9996, 0.9998, 0.9997, 0.9998, 0.9994, 0.9998, 0.9997, 0.9998,
        0.9996, 0.9999, 0.9999, 0.9998, 0.9995, 0.9996, 0.9992],
       device='cuda:0') torch.Size([16])
Epoch: 125 | Batch_idx: 0 |  Loss: (0.1376) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 125 | Batch_idx: 10 |  Loss: (0.1519) |  Loss2: (0.0000) | Acc: (94.00%) (1332/1408)
Epoch: 125 | Batch_idx: 20 |  Loss: (0.1545) |  Loss2: (0.0000) | Acc: (94.00%) (2542/2688)
Epoch: 125 | Batch_idx: 30 |  Loss: (0.1473) |  Loss2: (0.0000) | Acc: (94.00%) (3757/3968)
Epoch: 125 | Batch_idx: 40 |  Loss: (0.1452) |  Loss2: (0.0000) | Acc: (94.00%) (4975/5248)
Epoch: 125 | Batch_idx: 50 |  Loss: (0.1477) |  Loss2: (0.0000) | Acc: (94.00%) (6185/6528)
Epoch: 125 | Batch_idx: 60 |  Loss: (0.1479) |  Loss2: (0.0000) | Acc: (94.00%) (7400/7808)
Epoch: 125 | Batch_idx: 70 |  Loss: (0.1508) |  Loss2: (0.0000) | Acc: (94.00%) (8611/9088)
Epoch: 125 | Batch_idx: 80 |  Loss: (0.1539) |  Loss2: (0.0000) | Acc: (94.00%) (9815/10368)
Epoch: 125 | Batch_idx: 90 |  Loss: (0.1544) |  Loss2: (0.0000) | Acc: (94.00%) (11023/11648)
Epoch: 125 | Batch_idx: 100 |  Loss: (0.1537) |  Loss2: (0.0000) | Acc: (94.00%) (12245/12928)
Epoch: 125 | Batch_idx: 110 |  Loss: (0.1544) |  Loss2: (0.0000) | Acc: (94.00%) (13451/14208)
Epoch: 125 | Batch_idx: 120 |  Loss: (0.1551) |  Loss2: (0.0000) | Acc: (94.00%) (14664/15488)
Epoch: 125 | Batch_idx: 130 |  Loss: (0.1555) |  Loss2: (0.0000) | Acc: (94.00%) (15867/16768)
Epoch: 125 | Batch_idx: 140 |  Loss: (0.1547) |  Loss2: (0.0000) | Acc: (94.00%) (17087/18048)
Epoch: 125 | Batch_idx: 150 |  Loss: (0.1551) |  Loss2: (0.0000) | Acc: (94.00%) (18300/19328)
Epoch: 125 | Batch_idx: 160 |  Loss: (0.1546) |  Loss2: (0.0000) | Acc: (94.00%) (19516/20608)
Epoch: 125 | Batch_idx: 170 |  Loss: (0.1551) |  Loss2: (0.0000) | Acc: (94.00%) (20714/21888)
Epoch: 125 | Batch_idx: 180 |  Loss: (0.1544) |  Loss2: (0.0000) | Acc: (94.00%) (21939/23168)
Epoch: 125 | Batch_idx: 190 |  Loss: (0.1550) |  Loss2: (0.0000) | Acc: (94.00%) (23147/24448)
Epoch: 125 | Batch_idx: 200 |  Loss: (0.1545) |  Loss2: (0.0000) | Acc: (94.00%) (24355/25728)
Epoch: 125 | Batch_idx: 210 |  Loss: (0.1539) |  Loss2: (0.0000) | Acc: (94.00%) (25573/27008)
Epoch: 125 | Batch_idx: 220 |  Loss: (0.1552) |  Loss2: (0.0000) | Acc: (94.00%) (26772/28288)
Epoch: 125 | Batch_idx: 230 |  Loss: (0.1558) |  Loss2: (0.0000) | Acc: (94.00%) (27976/29568)
Epoch: 125 | Batch_idx: 240 |  Loss: (0.1548) |  Loss2: (0.0000) | Acc: (94.00%) (29204/30848)
Epoch: 125 | Batch_idx: 250 |  Loss: (0.1539) |  Loss2: (0.0000) | Acc: (94.00%) (30424/32128)
Epoch: 125 | Batch_idx: 260 |  Loss: (0.1544) |  Loss2: (0.0000) | Acc: (94.00%) (31630/33408)
Epoch: 125 | Batch_idx: 270 |  Loss: (0.1540) |  Loss2: (0.0000) | Acc: (94.00%) (32848/34688)
Epoch: 125 | Batch_idx: 280 |  Loss: (0.1543) |  Loss2: (0.0000) | Acc: (94.00%) (34048/35968)
Epoch: 125 | Batch_idx: 290 |  Loss: (0.1537) |  Loss2: (0.0000) | Acc: (94.00%) (35274/37248)
Epoch: 125 | Batch_idx: 300 |  Loss: (0.1539) |  Loss2: (0.0000) | Acc: (94.00%) (36479/38528)
Epoch: 125 | Batch_idx: 310 |  Loss: (0.1541) |  Loss2: (0.0000) | Acc: (94.00%) (37683/39808)
Epoch: 125 | Batch_idx: 320 |  Loss: (0.1556) |  Loss2: (0.0000) | Acc: (94.00%) (38879/41088)
Epoch: 125 | Batch_idx: 330 |  Loss: (0.1569) |  Loss2: (0.0000) | Acc: (94.00%) (40072/42368)
Epoch: 125 | Batch_idx: 340 |  Loss: (0.1569) |  Loss2: (0.0000) | Acc: (94.00%) (41283/43648)
Epoch: 125 | Batch_idx: 350 |  Loss: (0.1579) |  Loss2: (0.0000) | Acc: (94.00%) (42472/44928)
Epoch: 125 | Batch_idx: 360 |  Loss: (0.1589) |  Loss2: (0.0000) | Acc: (94.00%) (43669/46208)
Epoch: 125 | Batch_idx: 370 |  Loss: (0.1589) |  Loss2: (0.0000) | Acc: (94.00%) (44877/47488)
Epoch: 125 | Batch_idx: 380 |  Loss: (0.1597) |  Loss2: (0.0000) | Acc: (94.00%) (46075/48768)
Epoch: 125 | Batch_idx: 390 |  Loss: (0.1604) |  Loss2: (0.0000) | Acc: (94.00%) (47230/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_125.pth.tar'
# TEST : Loss: (0.4424) | Acc: (86.00%) (8644/10000)
percent tensor([0.5452, 0.5551, 0.5606, 0.5222, 0.5652, 0.5598, 0.5666, 0.5402, 0.5465,
        0.5578, 0.5561, 0.5692, 0.5458, 0.5181, 0.5594, 0.5390],
       device='cuda:0') torch.Size([16])
percent tensor([0.5359, 0.5352, 0.5409, 0.5452, 0.5420, 0.5361, 0.5372, 0.5440, 0.5386,
        0.5360, 0.5345, 0.5371, 0.5339, 0.5408, 0.5349, 0.5384],
       device='cuda:0') torch.Size([16])
percent tensor([0.5967, 0.6307, 0.4314, 0.4038, 0.4021, 0.5462, 0.5682, 0.4313, 0.5110,
        0.5951, 0.6112, 0.5115, 0.6367, 0.5864, 0.5824, 0.5841],
       device='cuda:0') torch.Size([16])
percent tensor([0.6932, 0.5986, 0.6897, 0.6885, 0.6901, 0.7510, 0.6539, 0.6952, 0.6596,
        0.6200, 0.6438, 0.6488, 0.5964, 0.6838, 0.6717, 0.7207],
       device='cuda:0') torch.Size([16])
percent tensor([0.6679, 0.6399, 0.6540, 0.6001, 0.6222, 0.6201, 0.6654, 0.6562, 0.6490,
        0.6376, 0.6208, 0.6013, 0.6187, 0.6278, 0.6629, 0.6337],
       device='cuda:0') torch.Size([16])
percent tensor([0.5302, 0.6710, 0.6376, 0.6984, 0.7065, 0.6901, 0.6741, 0.5812, 0.6984,
        0.5933, 0.6770, 0.7235, 0.6585, 0.7499, 0.5860, 0.5614],
       device='cuda:0') torch.Size([16])
percent tensor([0.6052, 0.6415, 0.5276, 0.4558, 0.6161, 0.6660, 0.5974, 0.4235, 0.6293,
        0.6561, 0.6411, 0.4177, 0.6514, 0.5897, 0.4585, 0.5752],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9998, 0.9998, 0.9997, 0.9997, 0.9994, 0.9998, 0.9997, 0.9998,
        0.9996, 0.9999, 0.9998, 0.9998, 0.9998, 0.9994, 0.9991],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 126 | Batch_idx: 0 |  Loss: (0.1890) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 126 | Batch_idx: 10 |  Loss: (0.1659) |  Loss2: (0.0000) | Acc: (93.00%) (1320/1408)
Epoch: 126 | Batch_idx: 20 |  Loss: (0.1959) |  Loss2: (0.0000) | Acc: (92.00%) (2486/2688)
Epoch: 126 | Batch_idx: 30 |  Loss: (0.2054) |  Loss2: (0.0000) | Acc: (91.00%) (3650/3968)
Epoch: 126 | Batch_idx: 40 |  Loss: (0.2115) |  Loss2: (0.0000) | Acc: (92.00%) (4829/5248)
Epoch: 126 | Batch_idx: 50 |  Loss: (0.2028) |  Loss2: (0.0000) | Acc: (92.00%) (6027/6528)
Epoch: 126 | Batch_idx: 60 |  Loss: (0.2034) |  Loss2: (0.0000) | Acc: (92.00%) (7216/7808)
Epoch: 126 | Batch_idx: 70 |  Loss: (0.2071) |  Loss2: (0.0000) | Acc: (92.00%) (8392/9088)
Epoch: 126 | Batch_idx: 80 |  Loss: (0.2067) |  Loss2: (0.0000) | Acc: (92.00%) (9586/10368)
Epoch: 126 | Batch_idx: 90 |  Loss: (0.2090) |  Loss2: (0.0000) | Acc: (92.00%) (10758/11648)
Epoch: 126 | Batch_idx: 100 |  Loss: (0.2084) |  Loss2: (0.0000) | Acc: (92.00%) (11951/12928)
Epoch: 126 | Batch_idx: 110 |  Loss: (0.2067) |  Loss2: (0.0000) | Acc: (92.00%) (13138/14208)
Epoch: 126 | Batch_idx: 120 |  Loss: (0.2092) |  Loss2: (0.0000) | Acc: (92.00%) (14308/15488)
Epoch: 126 | Batch_idx: 130 |  Loss: (0.2092) |  Loss2: (0.0000) | Acc: (92.00%) (15498/16768)
Epoch: 126 | Batch_idx: 140 |  Loss: (0.2086) |  Loss2: (0.0000) | Acc: (92.00%) (16693/18048)
Epoch: 126 | Batch_idx: 150 |  Loss: (0.2091) |  Loss2: (0.0000) | Acc: (92.00%) (17873/19328)
Epoch: 126 | Batch_idx: 160 |  Loss: (0.2082) |  Loss2: (0.0000) | Acc: (92.00%) (19068/20608)
Epoch: 126 | Batch_idx: 170 |  Loss: (0.2063) |  Loss2: (0.0000) | Acc: (92.00%) (20270/21888)
Epoch: 126 | Batch_idx: 180 |  Loss: (0.2047) |  Loss2: (0.0000) | Acc: (92.00%) (21460/23168)
Epoch: 126 | Batch_idx: 190 |  Loss: (0.2048) |  Loss2: (0.0000) | Acc: (92.00%) (22643/24448)
Epoch: 126 | Batch_idx: 200 |  Loss: (0.2048) |  Loss2: (0.0000) | Acc: (92.00%) (23837/25728)
Epoch: 126 | Batch_idx: 210 |  Loss: (0.2036) |  Loss2: (0.0000) | Acc: (92.00%) (25039/27008)
Epoch: 126 | Batch_idx: 220 |  Loss: (0.2038) |  Loss2: (0.0000) | Acc: (92.00%) (26227/28288)
Epoch: 126 | Batch_idx: 230 |  Loss: (0.2042) |  Loss2: (0.0000) | Acc: (92.00%) (27404/29568)
Epoch: 126 | Batch_idx: 240 |  Loss: (0.2029) |  Loss2: (0.0000) | Acc: (92.00%) (28604/30848)
Epoch: 126 | Batch_idx: 250 |  Loss: (0.2037) |  Loss2: (0.0000) | Acc: (92.00%) (29784/32128)
Epoch: 126 | Batch_idx: 260 |  Loss: (0.2031) |  Loss2: (0.0000) | Acc: (92.00%) (30985/33408)
Epoch: 126 | Batch_idx: 270 |  Loss: (0.2033) |  Loss2: (0.0000) | Acc: (92.00%) (32167/34688)
Epoch: 126 | Batch_idx: 280 |  Loss: (0.2033) |  Loss2: (0.0000) | Acc: (92.00%) (33353/35968)
Epoch: 126 | Batch_idx: 290 |  Loss: (0.2024) |  Loss2: (0.0000) | Acc: (92.00%) (34552/37248)
Epoch: 126 | Batch_idx: 300 |  Loss: (0.2015) |  Loss2: (0.0000) | Acc: (92.00%) (35753/38528)
Epoch: 126 | Batch_idx: 310 |  Loss: (0.2022) |  Loss2: (0.0000) | Acc: (92.00%) (36930/39808)
Epoch: 126 | Batch_idx: 320 |  Loss: (0.2021) |  Loss2: (0.0000) | Acc: (92.00%) (38121/41088)
Epoch: 126 | Batch_idx: 330 |  Loss: (0.2019) |  Loss2: (0.0000) | Acc: (92.00%) (39308/42368)
Epoch: 126 | Batch_idx: 340 |  Loss: (0.2014) |  Loss2: (0.0000) | Acc: (92.00%) (40500/43648)
Epoch: 126 | Batch_idx: 350 |  Loss: (0.2019) |  Loss2: (0.0000) | Acc: (92.00%) (41680/44928)
Epoch: 126 | Batch_idx: 360 |  Loss: (0.2022) |  Loss2: (0.0000) | Acc: (92.00%) (42856/46208)
Epoch: 126 | Batch_idx: 370 |  Loss: (0.2023) |  Loss2: (0.0000) | Acc: (92.00%) (44045/47488)
Epoch: 126 | Batch_idx: 380 |  Loss: (0.2014) |  Loss2: (0.0000) | Acc: (92.00%) (45251/48768)
Epoch: 126 | Batch_idx: 390 |  Loss: (0.2003) |  Loss2: (0.0000) | Acc: (92.00%) (46413/50000)
# TEST : Loss: (0.4165) | Acc: (87.00%) (8719/10000)
percent tensor([0.5333, 0.5432, 0.5434, 0.5096, 0.5480, 0.5495, 0.5525, 0.5249, 0.5338,
        0.5431, 0.5447, 0.5513, 0.5335, 0.5095, 0.5472, 0.5278],
       device='cuda:0') torch.Size([16])
percent tensor([0.5319, 0.5313, 0.5379, 0.5412, 0.5388, 0.5320, 0.5332, 0.5400, 0.5354,
        0.5325, 0.5311, 0.5339, 0.5306, 0.5361, 0.5307, 0.5342],
       device='cuda:0') torch.Size([16])
percent tensor([0.6226, 0.6438, 0.4344, 0.4187, 0.4149, 0.5737, 0.5840, 0.4548, 0.5295,
        0.6046, 0.6240, 0.5130, 0.6512, 0.6096, 0.6101, 0.6095],
       device='cuda:0') torch.Size([16])
percent tensor([0.7027, 0.6068, 0.6878, 0.6868, 0.6937, 0.7637, 0.6608, 0.6962, 0.6650,
        0.6327, 0.6530, 0.6464, 0.6028, 0.6841, 0.6830, 0.7311],
       device='cuda:0') torch.Size([16])
percent tensor([0.6649, 0.6607, 0.6649, 0.6104, 0.6294, 0.6089, 0.6821, 0.6568, 0.6694,
        0.6527, 0.6407, 0.6284, 0.6403, 0.6595, 0.6663, 0.6338],
       device='cuda:0') torch.Size([16])
percent tensor([0.5109, 0.6458, 0.6119, 0.6688, 0.6769, 0.6818, 0.6349, 0.5374, 0.6623,
        0.5447, 0.6471, 0.6864, 0.6324, 0.7339, 0.5626, 0.5440],
       device='cuda:0') torch.Size([16])
percent tensor([0.6746, 0.6851, 0.5717, 0.4931, 0.6583, 0.7090, 0.6371, 0.4572, 0.6693,
        0.6963, 0.6872, 0.4659, 0.7059, 0.6704, 0.4842, 0.6307],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9996, 0.9998, 0.9998, 0.9998, 0.9995, 0.9997, 0.9997, 0.9995,
        0.9994, 0.9998, 0.9998, 0.9997, 0.9997, 0.9994, 0.9990],
       device='cuda:0') torch.Size([16])
Epoch: 127 | Batch_idx: 0 |  Loss: (0.2022) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 127 | Batch_idx: 10 |  Loss: (0.1962) |  Loss2: (0.0000) | Acc: (93.00%) (1310/1408)
Epoch: 127 | Batch_idx: 20 |  Loss: (0.1958) |  Loss2: (0.0000) | Acc: (92.00%) (2494/2688)
Epoch: 127 | Batch_idx: 30 |  Loss: (0.1944) |  Loss2: (0.0000) | Acc: (92.00%) (3677/3968)
Epoch: 127 | Batch_idx: 40 |  Loss: (0.1903) |  Loss2: (0.0000) | Acc: (92.00%) (4875/5248)
Epoch: 127 | Batch_idx: 50 |  Loss: (0.1864) |  Loss2: (0.0000) | Acc: (93.00%) (6073/6528)
Epoch: 127 | Batch_idx: 60 |  Loss: (0.1862) |  Loss2: (0.0000) | Acc: (93.00%) (7274/7808)
Epoch: 127 | Batch_idx: 70 |  Loss: (0.1844) |  Loss2: (0.0000) | Acc: (93.00%) (8465/9088)
Epoch: 127 | Batch_idx: 80 |  Loss: (0.1826) |  Loss2: (0.0000) | Acc: (93.00%) (9656/10368)
Epoch: 127 | Batch_idx: 90 |  Loss: (0.1843) |  Loss2: (0.0000) | Acc: (93.00%) (10842/11648)
Epoch: 127 | Batch_idx: 100 |  Loss: (0.1830) |  Loss2: (0.0000) | Acc: (93.00%) (12052/12928)
Epoch: 127 | Batch_idx: 110 |  Loss: (0.1830) |  Loss2: (0.0000) | Acc: (93.00%) (13249/14208)
Epoch: 127 | Batch_idx: 120 |  Loss: (0.1832) |  Loss2: (0.0000) | Acc: (93.00%) (14445/15488)
Epoch: 127 | Batch_idx: 130 |  Loss: (0.1838) |  Loss2: (0.0000) | Acc: (93.00%) (15643/16768)
Epoch: 127 | Batch_idx: 140 |  Loss: (0.1823) |  Loss2: (0.0000) | Acc: (93.00%) (16844/18048)
Epoch: 127 | Batch_idx: 150 |  Loss: (0.1817) |  Loss2: (0.0000) | Acc: (93.00%) (18045/19328)
Epoch: 127 | Batch_idx: 160 |  Loss: (0.1819) |  Loss2: (0.0000) | Acc: (93.00%) (19242/20608)
Epoch: 127 | Batch_idx: 170 |  Loss: (0.1817) |  Loss2: (0.0000) | Acc: (93.00%) (20444/21888)
Epoch: 127 | Batch_idx: 180 |  Loss: (0.1809) |  Loss2: (0.0000) | Acc: (93.00%) (21645/23168)
Epoch: 127 | Batch_idx: 190 |  Loss: (0.1805) |  Loss2: (0.0000) | Acc: (93.00%) (22838/24448)
Epoch: 127 | Batch_idx: 200 |  Loss: (0.1804) |  Loss2: (0.0000) | Acc: (93.00%) (24042/25728)
Epoch: 127 | Batch_idx: 210 |  Loss: (0.1798) |  Loss2: (0.0000) | Acc: (93.00%) (25242/27008)
Epoch: 127 | Batch_idx: 220 |  Loss: (0.1798) |  Loss2: (0.0000) | Acc: (93.00%) (26447/28288)
Epoch: 127 | Batch_idx: 230 |  Loss: (0.1796) |  Loss2: (0.0000) | Acc: (93.00%) (27652/29568)
Epoch: 127 | Batch_idx: 240 |  Loss: (0.1798) |  Loss2: (0.0000) | Acc: (93.00%) (28848/30848)
Epoch: 127 | Batch_idx: 250 |  Loss: (0.1792) |  Loss2: (0.0000) | Acc: (93.00%) (30060/32128)
Epoch: 127 | Batch_idx: 260 |  Loss: (0.1785) |  Loss2: (0.0000) | Acc: (93.00%) (31278/33408)
Epoch: 127 | Batch_idx: 270 |  Loss: (0.1781) |  Loss2: (0.0000) | Acc: (93.00%) (32496/34688)
Epoch: 127 | Batch_idx: 280 |  Loss: (0.1769) |  Loss2: (0.0000) | Acc: (93.00%) (33710/35968)
Epoch: 127 | Batch_idx: 290 |  Loss: (0.1771) |  Loss2: (0.0000) | Acc: (93.00%) (34924/37248)
Epoch: 127 | Batch_idx: 300 |  Loss: (0.1774) |  Loss2: (0.0000) | Acc: (93.00%) (36117/38528)
Epoch: 127 | Batch_idx: 310 |  Loss: (0.1778) |  Loss2: (0.0000) | Acc: (93.00%) (37312/39808)
Epoch: 127 | Batch_idx: 320 |  Loss: (0.1777) |  Loss2: (0.0000) | Acc: (93.00%) (38513/41088)
Epoch: 127 | Batch_idx: 330 |  Loss: (0.1776) |  Loss2: (0.0000) | Acc: (93.00%) (39709/42368)
Epoch: 127 | Batch_idx: 340 |  Loss: (0.1770) |  Loss2: (0.0000) | Acc: (93.00%) (40922/43648)
Epoch: 127 | Batch_idx: 350 |  Loss: (0.1765) |  Loss2: (0.0000) | Acc: (93.00%) (42129/44928)
Epoch: 127 | Batch_idx: 360 |  Loss: (0.1764) |  Loss2: (0.0000) | Acc: (93.00%) (43338/46208)
Epoch: 127 | Batch_idx: 370 |  Loss: (0.1752) |  Loss2: (0.0000) | Acc: (93.00%) (44564/47488)
Epoch: 127 | Batch_idx: 380 |  Loss: (0.1748) |  Loss2: (0.0000) | Acc: (93.00%) (45779/48768)
Epoch: 127 | Batch_idx: 390 |  Loss: (0.1742) |  Loss2: (0.0000) | Acc: (93.00%) (46946/50000)
# TEST : Loss: (0.3998) | Acc: (87.00%) (8759/10000)
percent tensor([0.5392, 0.5503, 0.5476, 0.5146, 0.5537, 0.5566, 0.5596, 0.5303, 0.5403,
        0.5491, 0.5512, 0.5565, 0.5393, 0.5171, 0.5541, 0.5341],
       device='cuda:0') torch.Size([16])
percent tensor([0.5298, 0.5290, 0.5358, 0.5391, 0.5365, 0.5300, 0.5309, 0.5377, 0.5331,
        0.5302, 0.5290, 0.5317, 0.5284, 0.5337, 0.5284, 0.5320],
       device='cuda:0') torch.Size([16])
percent tensor([0.6104, 0.6315, 0.4219, 0.4038, 0.4074, 0.5647, 0.5723, 0.4452, 0.5149,
        0.5885, 0.6077, 0.5003, 0.6337, 0.5976, 0.6025, 0.5941],
       device='cuda:0') torch.Size([16])
percent tensor([0.6965, 0.5990, 0.6807, 0.6800, 0.6884, 0.7637, 0.6538, 0.6907, 0.6571,
        0.6290, 0.6444, 0.6376, 0.5944, 0.6776, 0.6781, 0.7262],
       device='cuda:0') torch.Size([16])
percent tensor([0.6743, 0.6802, 0.6765, 0.6227, 0.6343, 0.6097, 0.6970, 0.6631, 0.6835,
        0.6679, 0.6584, 0.6469, 0.6601, 0.6779, 0.6792, 0.6451],
       device='cuda:0') torch.Size([16])
percent tensor([0.5212, 0.6516, 0.6205, 0.6779, 0.6835, 0.6909, 0.6416, 0.5345, 0.6711,
        0.5505, 0.6581, 0.7001, 0.6425, 0.7422, 0.5716, 0.5497],
       device='cuda:0') torch.Size([16])
percent tensor([0.6758, 0.6761, 0.5687, 0.4974, 0.6609, 0.7153, 0.6257, 0.4397, 0.6641,
        0.6909, 0.6808, 0.4528, 0.7026, 0.6708, 0.4667, 0.6254],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9997, 0.9998, 0.9998, 0.9998, 0.9995, 0.9997, 0.9997, 0.9996,
        0.9994, 0.9998, 0.9998, 0.9998, 0.9997, 0.9994, 0.9991],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 128 | Batch_idx: 0 |  Loss: (0.0823) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 128 | Batch_idx: 10 |  Loss: (0.1711) |  Loss2: (0.0000) | Acc: (93.00%) (1313/1408)
Epoch: 128 | Batch_idx: 20 |  Loss: (0.1704) |  Loss2: (0.0000) | Acc: (93.00%) (2520/2688)
Epoch: 128 | Batch_idx: 30 |  Loss: (0.1583) |  Loss2: (0.0000) | Acc: (94.00%) (3743/3968)
Epoch: 128 | Batch_idx: 40 |  Loss: (0.1574) |  Loss2: (0.0000) | Acc: (94.00%) (4961/5248)
Epoch: 128 | Batch_idx: 50 |  Loss: (0.1579) |  Loss2: (0.0000) | Acc: (94.00%) (6168/6528)
Epoch: 128 | Batch_idx: 60 |  Loss: (0.1583) |  Loss2: (0.0000) | Acc: (94.00%) (7377/7808)
Epoch: 128 | Batch_idx: 70 |  Loss: (0.1563) |  Loss2: (0.0000) | Acc: (94.00%) (8595/9088)
Epoch: 128 | Batch_idx: 80 |  Loss: (0.1609) |  Loss2: (0.0000) | Acc: (94.00%) (9790/10368)
Epoch: 128 | Batch_idx: 90 |  Loss: (0.1614) |  Loss2: (0.0000) | Acc: (94.00%) (10997/11648)
Epoch: 128 | Batch_idx: 100 |  Loss: (0.1613) |  Loss2: (0.0000) | Acc: (94.00%) (12207/12928)
Epoch: 128 | Batch_idx: 110 |  Loss: (0.1609) |  Loss2: (0.0000) | Acc: (94.00%) (13422/14208)
Epoch: 128 | Batch_idx: 120 |  Loss: (0.1597) |  Loss2: (0.0000) | Acc: (94.00%) (14651/15488)
Epoch: 128 | Batch_idx: 130 |  Loss: (0.1598) |  Loss2: (0.0000) | Acc: (94.00%) (15856/16768)
Epoch: 128 | Batch_idx: 140 |  Loss: (0.1611) |  Loss2: (0.0000) | Acc: (94.00%) (17047/18048)
Epoch: 128 | Batch_idx: 150 |  Loss: (0.1616) |  Loss2: (0.0000) | Acc: (94.00%) (18254/19328)
Epoch: 128 | Batch_idx: 160 |  Loss: (0.1614) |  Loss2: (0.0000) | Acc: (94.00%) (19463/20608)
Epoch: 128 | Batch_idx: 170 |  Loss: (0.1607) |  Loss2: (0.0000) | Acc: (94.00%) (20684/21888)
Epoch: 128 | Batch_idx: 180 |  Loss: (0.1621) |  Loss2: (0.0000) | Acc: (94.00%) (21887/23168)
Epoch: 128 | Batch_idx: 190 |  Loss: (0.1627) |  Loss2: (0.0000) | Acc: (94.00%) (23091/24448)
Epoch: 128 | Batch_idx: 200 |  Loss: (0.1649) |  Loss2: (0.0000) | Acc: (94.00%) (24277/25728)
Epoch: 128 | Batch_idx: 210 |  Loss: (0.1635) |  Loss2: (0.0000) | Acc: (94.00%) (25496/27008)
Epoch: 128 | Batch_idx: 220 |  Loss: (0.1637) |  Loss2: (0.0000) | Acc: (94.00%) (26701/28288)
Epoch: 128 | Batch_idx: 230 |  Loss: (0.1632) |  Loss2: (0.0000) | Acc: (94.00%) (27911/29568)
Epoch: 128 | Batch_idx: 240 |  Loss: (0.1630) |  Loss2: (0.0000) | Acc: (94.00%) (29119/30848)
Epoch: 128 | Batch_idx: 250 |  Loss: (0.1626) |  Loss2: (0.0000) | Acc: (94.00%) (30335/32128)
Epoch: 128 | Batch_idx: 260 |  Loss: (0.1616) |  Loss2: (0.0000) | Acc: (94.00%) (31563/33408)
Epoch: 128 | Batch_idx: 270 |  Loss: (0.1624) |  Loss2: (0.0000) | Acc: (94.00%) (32752/34688)
Epoch: 128 | Batch_idx: 280 |  Loss: (0.1622) |  Loss2: (0.0000) | Acc: (94.00%) (33970/35968)
Epoch: 128 | Batch_idx: 290 |  Loss: (0.1626) |  Loss2: (0.0000) | Acc: (94.00%) (35171/37248)
Epoch: 128 | Batch_idx: 300 |  Loss: (0.1618) |  Loss2: (0.0000) | Acc: (94.00%) (36386/38528)
Epoch: 128 | Batch_idx: 310 |  Loss: (0.1619) |  Loss2: (0.0000) | Acc: (94.00%) (37593/39808)
Epoch: 128 | Batch_idx: 320 |  Loss: (0.1621) |  Loss2: (0.0000) | Acc: (94.00%) (38802/41088)
Epoch: 128 | Batch_idx: 330 |  Loss: (0.1627) |  Loss2: (0.0000) | Acc: (94.00%) (40008/42368)
Epoch: 128 | Batch_idx: 340 |  Loss: (0.1620) |  Loss2: (0.0000) | Acc: (94.00%) (41230/43648)
Epoch: 128 | Batch_idx: 350 |  Loss: (0.1618) |  Loss2: (0.0000) | Acc: (94.00%) (42439/44928)
Epoch: 128 | Batch_idx: 360 |  Loss: (0.1616) |  Loss2: (0.0000) | Acc: (94.00%) (43647/46208)
Epoch: 128 | Batch_idx: 370 |  Loss: (0.1619) |  Loss2: (0.0000) | Acc: (94.00%) (44855/47488)
Epoch: 128 | Batch_idx: 380 |  Loss: (0.1626) |  Loss2: (0.0000) | Acc: (94.00%) (46048/48768)
Epoch: 128 | Batch_idx: 390 |  Loss: (0.1630) |  Loss2: (0.0000) | Acc: (94.00%) (47205/50000)
# TEST : Loss: (0.4725) | Acc: (85.00%) (8549/10000)
percent tensor([0.5384, 0.5525, 0.5486, 0.5188, 0.5540, 0.5554, 0.5601, 0.5343, 0.5403,
        0.5504, 0.5515, 0.5564, 0.5390, 0.5199, 0.5554, 0.5355],
       device='cuda:0') torch.Size([16])
percent tensor([0.5303, 0.5289, 0.5360, 0.5398, 0.5366, 0.5309, 0.5314, 0.5383, 0.5325,
        0.5303, 0.5287, 0.5324, 0.5283, 0.5337, 0.5287, 0.5324],
       device='cuda:0') torch.Size([16])
percent tensor([0.6144, 0.6380, 0.4369, 0.4137, 0.4098, 0.5625, 0.5879, 0.4629, 0.5232,
        0.5952, 0.6146, 0.5054, 0.6371, 0.6142, 0.6105, 0.5992],
       device='cuda:0') torch.Size([16])
percent tensor([0.6918, 0.6049, 0.6906, 0.6941, 0.6922, 0.7583, 0.6532, 0.6970, 0.6508,
        0.6297, 0.6426, 0.6429, 0.5962, 0.6789, 0.6811, 0.7303],
       device='cuda:0') torch.Size([16])
percent tensor([0.6661, 0.6772, 0.6738, 0.6125, 0.6306, 0.6008, 0.6973, 0.6701, 0.6805,
        0.6618, 0.6482, 0.6417, 0.6547, 0.6655, 0.6790, 0.6309],
       device='cuda:0') torch.Size([16])
percent tensor([0.5587, 0.6814, 0.6342, 0.6882, 0.6874, 0.6984, 0.6623, 0.5521, 0.7036,
        0.5963, 0.7040, 0.7337, 0.6779, 0.7439, 0.5908, 0.5784],
       device='cuda:0') torch.Size([16])
percent tensor([0.6527, 0.6333, 0.5565, 0.4898, 0.6283, 0.7122, 0.5712, 0.4056, 0.6566,
        0.6716, 0.6608, 0.4228, 0.6610, 0.6418, 0.4333, 0.5996],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9998, 0.9998, 0.9997, 0.9998, 0.9994, 0.9998, 0.9998, 0.9998,
        0.9997, 0.9998, 0.9999, 0.9998, 0.9996, 0.9996, 0.9995],
       device='cuda:0') torch.Size([16])
Epoch: 129 | Batch_idx: 0 |  Loss: (0.1560) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 129 | Batch_idx: 10 |  Loss: (0.1658) |  Loss2: (0.0000) | Acc: (94.00%) (1329/1408)
Epoch: 129 | Batch_idx: 20 |  Loss: (0.1557) |  Loss2: (0.0000) | Acc: (94.00%) (2545/2688)
Epoch: 129 | Batch_idx: 30 |  Loss: (0.1510) |  Loss2: (0.0000) | Acc: (94.00%) (3765/3968)
Epoch: 129 | Batch_idx: 40 |  Loss: (0.1527) |  Loss2: (0.0000) | Acc: (94.00%) (4983/5248)
Epoch: 129 | Batch_idx: 50 |  Loss: (0.1538) |  Loss2: (0.0000) | Acc: (94.00%) (6191/6528)
Epoch: 129 | Batch_idx: 60 |  Loss: (0.1498) |  Loss2: (0.0000) | Acc: (95.00%) (7420/7808)
Epoch: 129 | Batch_idx: 70 |  Loss: (0.1532) |  Loss2: (0.0000) | Acc: (95.00%) (8634/9088)
Epoch: 129 | Batch_idx: 80 |  Loss: (0.1518) |  Loss2: (0.0000) | Acc: (95.00%) (9852/10368)
Epoch: 129 | Batch_idx: 90 |  Loss: (0.1515) |  Loss2: (0.0000) | Acc: (95.00%) (11068/11648)
Epoch: 129 | Batch_idx: 100 |  Loss: (0.1498) |  Loss2: (0.0000) | Acc: (95.00%) (12292/12928)
Epoch: 129 | Batch_idx: 110 |  Loss: (0.1498) |  Loss2: (0.0000) | Acc: (94.00%) (13496/14208)
Epoch: 129 | Batch_idx: 120 |  Loss: (0.1500) |  Loss2: (0.0000) | Acc: (94.00%) (14713/15488)
Epoch: 129 | Batch_idx: 130 |  Loss: (0.1504) |  Loss2: (0.0000) | Acc: (94.00%) (15922/16768)
Epoch: 129 | Batch_idx: 140 |  Loss: (0.1491) |  Loss2: (0.0000) | Acc: (95.00%) (17147/18048)
Epoch: 129 | Batch_idx: 150 |  Loss: (0.1516) |  Loss2: (0.0000) | Acc: (94.00%) (18342/19328)
Epoch: 129 | Batch_idx: 160 |  Loss: (0.1527) |  Loss2: (0.0000) | Acc: (94.00%) (19545/20608)
Epoch: 129 | Batch_idx: 170 |  Loss: (0.1530) |  Loss2: (0.0000) | Acc: (94.00%) (20752/21888)
Epoch: 129 | Batch_idx: 180 |  Loss: (0.1526) |  Loss2: (0.0000) | Acc: (94.00%) (21977/23168)
Epoch: 129 | Batch_idx: 190 |  Loss: (0.1535) |  Loss2: (0.0000) | Acc: (94.00%) (23183/24448)
Epoch: 129 | Batch_idx: 200 |  Loss: (0.1540) |  Loss2: (0.0000) | Acc: (94.00%) (24394/25728)
Epoch: 129 | Batch_idx: 210 |  Loss: (0.1534) |  Loss2: (0.0000) | Acc: (94.00%) (25612/27008)
Epoch: 129 | Batch_idx: 220 |  Loss: (0.1535) |  Loss2: (0.0000) | Acc: (94.00%) (26822/28288)
Epoch: 129 | Batch_idx: 230 |  Loss: (0.1534) |  Loss2: (0.0000) | Acc: (94.00%) (28038/29568)
Epoch: 129 | Batch_idx: 240 |  Loss: (0.1532) |  Loss2: (0.0000) | Acc: (94.00%) (29256/30848)
Epoch: 129 | Batch_idx: 250 |  Loss: (0.1535) |  Loss2: (0.0000) | Acc: (94.00%) (30465/32128)
Epoch: 129 | Batch_idx: 260 |  Loss: (0.1544) |  Loss2: (0.0000) | Acc: (94.00%) (31664/33408)
Epoch: 129 | Batch_idx: 270 |  Loss: (0.1538) |  Loss2: (0.0000) | Acc: (94.00%) (32890/34688)
Epoch: 129 | Batch_idx: 280 |  Loss: (0.1538) |  Loss2: (0.0000) | Acc: (94.00%) (34107/35968)
Epoch: 129 | Batch_idx: 290 |  Loss: (0.1537) |  Loss2: (0.0000) | Acc: (94.00%) (35325/37248)
Epoch: 129 | Batch_idx: 300 |  Loss: (0.1547) |  Loss2: (0.0000) | Acc: (94.00%) (36523/38528)
Epoch: 129 | Batch_idx: 310 |  Loss: (0.1550) |  Loss2: (0.0000) | Acc: (94.00%) (37733/39808)
Epoch: 129 | Batch_idx: 320 |  Loss: (0.1556) |  Loss2: (0.0000) | Acc: (94.00%) (38932/41088)
Epoch: 129 | Batch_idx: 330 |  Loss: (0.1557) |  Loss2: (0.0000) | Acc: (94.00%) (40140/42368)
Epoch: 129 | Batch_idx: 340 |  Loss: (0.1555) |  Loss2: (0.0000) | Acc: (94.00%) (41357/43648)
Epoch: 129 | Batch_idx: 350 |  Loss: (0.1554) |  Loss2: (0.0000) | Acc: (94.00%) (42561/44928)
Epoch: 129 | Batch_idx: 360 |  Loss: (0.1562) |  Loss2: (0.0000) | Acc: (94.00%) (43756/46208)
Epoch: 129 | Batch_idx: 370 |  Loss: (0.1563) |  Loss2: (0.0000) | Acc: (94.00%) (44969/47488)
Epoch: 129 | Batch_idx: 380 |  Loss: (0.1567) |  Loss2: (0.0000) | Acc: (94.00%) (46169/48768)
Epoch: 129 | Batch_idx: 390 |  Loss: (0.1567) |  Loss2: (0.0000) | Acc: (94.00%) (47330/50000)
# TEST : Loss: (0.4033) | Acc: (88.00%) (8805/10000)
percent tensor([0.5397, 0.5540, 0.5445, 0.5202, 0.5509, 0.5599, 0.5594, 0.5335, 0.5385,
        0.5501, 0.5520, 0.5527, 0.5394, 0.5196, 0.5579, 0.5373],
       device='cuda:0') torch.Size([16])
percent tensor([0.5298, 0.5291, 0.5351, 0.5384, 0.5360, 0.5295, 0.5314, 0.5379, 0.5327,
        0.5304, 0.5287, 0.5321, 0.5284, 0.5344, 0.5289, 0.5317],
       device='cuda:0') torch.Size([16])
percent tensor([0.6181, 0.6298, 0.4602, 0.4281, 0.4217, 0.5775, 0.5832, 0.4701, 0.5256,
        0.5984, 0.6172, 0.5211, 0.6399, 0.5862, 0.6048, 0.6016],
       device='cuda:0') torch.Size([16])
percent tensor([0.7020, 0.6102, 0.7000, 0.6964, 0.6946, 0.7552, 0.6615, 0.7084, 0.6574,
        0.6380, 0.6505, 0.6485, 0.6032, 0.6865, 0.6850, 0.7328],
       device='cuda:0') torch.Size([16])
percent tensor([0.6601, 0.6748, 0.6561, 0.6039, 0.6239, 0.6027, 0.6868, 0.6580, 0.6696,
        0.6573, 0.6409, 0.6300, 0.6575, 0.6522, 0.6720, 0.6285],
       device='cuda:0') torch.Size([16])
percent tensor([0.5775, 0.6499, 0.6331, 0.6920, 0.6888, 0.7048, 0.6660, 0.5725, 0.7024,
        0.5951, 0.6905, 0.7140, 0.6517, 0.7345, 0.5999, 0.5655],
       device='cuda:0') torch.Size([16])
percent tensor([0.6573, 0.6613, 0.5819, 0.5115, 0.6640, 0.7164, 0.6089, 0.4499, 0.6686,
        0.6911, 0.6612, 0.4530, 0.6773, 0.6275, 0.4637, 0.6062],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9997, 0.9998, 0.9997, 0.9998, 0.9991, 0.9997, 0.9997, 0.9997,
        0.9996, 0.9998, 0.9998, 0.9998, 0.9995, 0.9995, 0.9995],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 130 | Batch_idx: 0 |  Loss: (0.1098) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 130 | Batch_idx: 10 |  Loss: (0.1600) |  Loss2: (0.0000) | Acc: (94.00%) (1333/1408)
Epoch: 130 | Batch_idx: 20 |  Loss: (0.1653) |  Loss2: (0.0000) | Acc: (94.00%) (2533/2688)
Epoch: 130 | Batch_idx: 30 |  Loss: (0.1615) |  Loss2: (0.0000) | Acc: (94.00%) (3746/3968)
Epoch: 130 | Batch_idx: 40 |  Loss: (0.1668) |  Loss2: (0.0000) | Acc: (94.00%) (4943/5248)
Epoch: 130 | Batch_idx: 50 |  Loss: (0.1709) |  Loss2: (0.0000) | Acc: (94.00%) (6140/6528)
Epoch: 130 | Batch_idx: 60 |  Loss: (0.1694) |  Loss2: (0.0000) | Acc: (94.00%) (7351/7808)
Epoch: 130 | Batch_idx: 70 |  Loss: (0.1715) |  Loss2: (0.0000) | Acc: (94.00%) (8549/9088)
Epoch: 130 | Batch_idx: 80 |  Loss: (0.1704) |  Loss2: (0.0000) | Acc: (94.00%) (9756/10368)
Epoch: 130 | Batch_idx: 90 |  Loss: (0.1687) |  Loss2: (0.0000) | Acc: (94.00%) (10972/11648)
Epoch: 130 | Batch_idx: 100 |  Loss: (0.1694) |  Loss2: (0.0000) | Acc: (94.00%) (12175/12928)
Epoch: 130 | Batch_idx: 110 |  Loss: (0.1715) |  Loss2: (0.0000) | Acc: (94.00%) (13356/14208)
Epoch: 130 | Batch_idx: 120 |  Loss: (0.1729) |  Loss2: (0.0000) | Acc: (93.00%) (14545/15488)
Epoch: 130 | Batch_idx: 130 |  Loss: (0.1727) |  Loss2: (0.0000) | Acc: (93.00%) (15739/16768)
Epoch: 130 | Batch_idx: 140 |  Loss: (0.1720) |  Loss2: (0.0000) | Acc: (93.00%) (16943/18048)
Epoch: 130 | Batch_idx: 150 |  Loss: (0.1717) |  Loss2: (0.0000) | Acc: (93.00%) (18148/19328)
Epoch: 130 | Batch_idx: 160 |  Loss: (0.1711) |  Loss2: (0.0000) | Acc: (93.00%) (19355/20608)
Epoch: 130 | Batch_idx: 170 |  Loss: (0.1714) |  Loss2: (0.0000) | Acc: (93.00%) (20549/21888)
Epoch: 130 | Batch_idx: 180 |  Loss: (0.1705) |  Loss2: (0.0000) | Acc: (93.00%) (21765/23168)
Epoch: 130 | Batch_idx: 190 |  Loss: (0.1710) |  Loss2: (0.0000) | Acc: (93.00%) (22957/24448)
Epoch: 130 | Batch_idx: 200 |  Loss: (0.1704) |  Loss2: (0.0000) | Acc: (93.00%) (24157/25728)
Epoch: 130 | Batch_idx: 210 |  Loss: (0.1707) |  Loss2: (0.0000) | Acc: (93.00%) (25360/27008)
Epoch: 130 | Batch_idx: 220 |  Loss: (0.1705) |  Loss2: (0.0000) | Acc: (93.00%) (26569/28288)
Epoch: 130 | Batch_idx: 230 |  Loss: (0.1698) |  Loss2: (0.0000) | Acc: (93.00%) (27783/29568)
Epoch: 130 | Batch_idx: 240 |  Loss: (0.1698) |  Loss2: (0.0000) | Acc: (93.00%) (28973/30848)
Epoch: 130 | Batch_idx: 250 |  Loss: (0.1699) |  Loss2: (0.0000) | Acc: (93.00%) (30182/32128)
Epoch: 130 | Batch_idx: 260 |  Loss: (0.1700) |  Loss2: (0.0000) | Acc: (93.00%) (31375/33408)
Epoch: 130 | Batch_idx: 270 |  Loss: (0.1696) |  Loss2: (0.0000) | Acc: (93.00%) (32584/34688)
Epoch: 130 | Batch_idx: 280 |  Loss: (0.1694) |  Loss2: (0.0000) | Acc: (93.00%) (33790/35968)
Epoch: 130 | Batch_idx: 290 |  Loss: (0.1686) |  Loss2: (0.0000) | Acc: (93.00%) (35001/37248)
Epoch: 130 | Batch_idx: 300 |  Loss: (0.1686) |  Loss2: (0.0000) | Acc: (93.00%) (36207/38528)
Epoch: 130 | Batch_idx: 310 |  Loss: (0.1684) |  Loss2: (0.0000) | Acc: (93.00%) (37412/39808)
Epoch: 130 | Batch_idx: 320 |  Loss: (0.1676) |  Loss2: (0.0000) | Acc: (93.00%) (38619/41088)
Epoch: 130 | Batch_idx: 330 |  Loss: (0.1670) |  Loss2: (0.0000) | Acc: (94.00%) (39834/42368)
Epoch: 130 | Batch_idx: 340 |  Loss: (0.1656) |  Loss2: (0.0000) | Acc: (94.00%) (41064/43648)
Epoch: 130 | Batch_idx: 350 |  Loss: (0.1645) |  Loss2: (0.0000) | Acc: (94.00%) (42292/44928)
Epoch: 130 | Batch_idx: 360 |  Loss: (0.1649) |  Loss2: (0.0000) | Acc: (94.00%) (43495/46208)
Epoch: 130 | Batch_idx: 370 |  Loss: (0.1648) |  Loss2: (0.0000) | Acc: (94.00%) (44701/47488)
Epoch: 130 | Batch_idx: 380 |  Loss: (0.1638) |  Loss2: (0.0000) | Acc: (94.00%) (45931/48768)
Epoch: 130 | Batch_idx: 390 |  Loss: (0.1634) |  Loss2: (0.0000) | Acc: (94.00%) (47103/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_130.pth.tar'
# TEST : Loss: (0.3873) | Acc: (88.00%) (8807/10000)
percent tensor([0.5376, 0.5512, 0.5404, 0.5165, 0.5469, 0.5580, 0.5561, 0.5290, 0.5368,
        0.5462, 0.5501, 0.5477, 0.5364, 0.5186, 0.5546, 0.5352],
       device='cuda:0') torch.Size([16])
percent tensor([0.5296, 0.5293, 0.5360, 0.5389, 0.5369, 0.5296, 0.5315, 0.5381, 0.5326,
        0.5307, 0.5285, 0.5327, 0.5283, 0.5339, 0.5290, 0.5314],
       device='cuda:0') torch.Size([16])
percent tensor([0.6393, 0.6500, 0.4680, 0.4403, 0.4292, 0.5942, 0.6029, 0.4817, 0.5557,
        0.6247, 0.6462, 0.5347, 0.6579, 0.6188, 0.6243, 0.6245],
       device='cuda:0') torch.Size([16])
percent tensor([0.6956, 0.6072, 0.6920, 0.6848, 0.6863, 0.7486, 0.6557, 0.7031, 0.6510,
        0.6306, 0.6449, 0.6378, 0.5918, 0.6800, 0.6795, 0.7234],
       device='cuda:0') torch.Size([16])
percent tensor([0.6654, 0.6669, 0.6671, 0.6069, 0.6345, 0.6030, 0.6859, 0.6635, 0.6773,
        0.6411, 0.6399, 0.6360, 0.6587, 0.6463, 0.6660, 0.6164],
       device='cuda:0') torch.Size([16])
percent tensor([0.5771, 0.6601, 0.6348, 0.7075, 0.6970, 0.7168, 0.6666, 0.5588, 0.7091,
        0.5898, 0.6923, 0.7164, 0.6644, 0.7522, 0.6022, 0.5716],
       device='cuda:0') torch.Size([16])
percent tensor([0.6569, 0.6642, 0.5609, 0.4781, 0.6268, 0.6979, 0.5992, 0.4410, 0.6534,
        0.6849, 0.6594, 0.4273, 0.6714, 0.6183, 0.4613, 0.6290],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9997, 0.9998, 0.9997, 0.9998, 0.9989, 0.9997, 0.9997, 0.9997,
        0.9995, 0.9998, 0.9999, 0.9998, 0.9996, 0.9995, 0.9994],
       device='cuda:0') torch.Size([16])
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(182.0991, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(822.9017, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(817.2150, device='cuda:0')
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1516.0300, device='cuda:0')
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(487.8087, device='cuda:0')
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2265.9138, device='cuda:0')
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4254.4292, device='cuda:0')
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1383.3668, device='cuda:0')
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6200.6807, device='cuda:0')
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11769.4893, device='cuda:0')
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3893.6975, device='cuda:0')
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16441.0801, device='cuda:0')
Epoch: 131 | Batch_idx: 0 |  Loss: (0.1056) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 131 | Batch_idx: 10 |  Loss: (0.1639) |  Loss2: (0.0000) | Acc: (94.00%) (1325/1408)
Epoch: 131 | Batch_idx: 20 |  Loss: (0.1498) |  Loss2: (0.0000) | Acc: (94.00%) (2543/2688)
Epoch: 131 | Batch_idx: 30 |  Loss: (0.1450) |  Loss2: (0.0000) | Acc: (94.00%) (3767/3968)
Epoch: 131 | Batch_idx: 40 |  Loss: (0.1462) |  Loss2: (0.0000) | Acc: (95.00%) (4986/5248)
Epoch: 131 | Batch_idx: 50 |  Loss: (0.1482) |  Loss2: (0.0000) | Acc: (94.00%) (6199/6528)
Epoch: 131 | Batch_idx: 60 |  Loss: (0.1480) |  Loss2: (0.0000) | Acc: (94.00%) (7411/7808)
Epoch: 131 | Batch_idx: 70 |  Loss: (0.1505) |  Loss2: (0.0000) | Acc: (94.00%) (8615/9088)
Epoch: 131 | Batch_idx: 80 |  Loss: (0.1496) |  Loss2: (0.0000) | Acc: (94.00%) (9835/10368)
Epoch: 131 | Batch_idx: 90 |  Loss: (0.1490) |  Loss2: (0.0000) | Acc: (94.00%) (11057/11648)
Epoch: 131 | Batch_idx: 100 |  Loss: (0.1509) |  Loss2: (0.0000) | Acc: (94.00%) (12273/12928)
Epoch: 131 | Batch_idx: 110 |  Loss: (0.1503) |  Loss2: (0.0000) | Acc: (94.00%) (13486/14208)
Epoch: 131 | Batch_idx: 120 |  Loss: (0.1514) |  Loss2: (0.0000) | Acc: (94.00%) (14691/15488)
Epoch: 131 | Batch_idx: 130 |  Loss: (0.1509) |  Loss2: (0.0000) | Acc: (94.00%) (15913/16768)
Epoch: 131 | Batch_idx: 140 |  Loss: (0.1504) |  Loss2: (0.0000) | Acc: (94.00%) (17137/18048)
Epoch: 131 | Batch_idx: 150 |  Loss: (0.1500) |  Loss2: (0.0000) | Acc: (94.00%) (18356/19328)
Epoch: 131 | Batch_idx: 160 |  Loss: (0.1492) |  Loss2: (0.0000) | Acc: (94.00%) (19575/20608)
Epoch: 131 | Batch_idx: 170 |  Loss: (0.1492) |  Loss2: (0.0000) | Acc: (95.00%) (20795/21888)
Epoch: 131 | Batch_idx: 180 |  Loss: (0.1493) |  Loss2: (0.0000) | Acc: (94.00%) (22004/23168)
Epoch: 131 | Batch_idx: 190 |  Loss: (0.1500) |  Loss2: (0.0000) | Acc: (94.00%) (23205/24448)
Epoch: 131 | Batch_idx: 200 |  Loss: (0.1505) |  Loss2: (0.0000) | Acc: (94.00%) (24420/25728)
Epoch: 131 | Batch_idx: 210 |  Loss: (0.1512) |  Loss2: (0.0000) | Acc: (94.00%) (25629/27008)
Epoch: 131 | Batch_idx: 220 |  Loss: (0.1500) |  Loss2: (0.0000) | Acc: (94.00%) (26852/28288)
Epoch: 131 | Batch_idx: 230 |  Loss: (0.1504) |  Loss2: (0.0000) | Acc: (94.00%) (28062/29568)
Epoch: 131 | Batch_idx: 240 |  Loss: (0.1499) |  Loss2: (0.0000) | Acc: (94.00%) (29281/30848)
Epoch: 131 | Batch_idx: 250 |  Loss: (0.1506) |  Loss2: (0.0000) | Acc: (94.00%) (30484/32128)
Epoch: 131 | Batch_idx: 260 |  Loss: (0.1500) |  Loss2: (0.0000) | Acc: (94.00%) (31704/33408)
Epoch: 131 | Batch_idx: 270 |  Loss: (0.1499) |  Loss2: (0.0000) | Acc: (94.00%) (32926/34688)
Epoch: 131 | Batch_idx: 280 |  Loss: (0.1496) |  Loss2: (0.0000) | Acc: (94.00%) (34144/35968)
Epoch: 131 | Batch_idx: 290 |  Loss: (0.1503) |  Loss2: (0.0000) | Acc: (94.00%) (35354/37248)
Epoch: 131 | Batch_idx: 300 |  Loss: (0.1496) |  Loss2: (0.0000) | Acc: (94.00%) (36579/38528)
Epoch: 131 | Batch_idx: 310 |  Loss: (0.1499) |  Loss2: (0.0000) | Acc: (94.00%) (37789/39808)
Epoch: 131 | Batch_idx: 320 |  Loss: (0.1500) |  Loss2: (0.0000) | Acc: (94.00%) (38999/41088)
Epoch: 131 | Batch_idx: 330 |  Loss: (0.1503) |  Loss2: (0.0000) | Acc: (94.00%) (40206/42368)
Epoch: 131 | Batch_idx: 340 |  Loss: (0.1512) |  Loss2: (0.0000) | Acc: (94.00%) (41402/43648)
Epoch: 131 | Batch_idx: 350 |  Loss: (0.1514) |  Loss2: (0.0000) | Acc: (94.00%) (42613/44928)
Epoch: 131 | Batch_idx: 360 |  Loss: (0.1512) |  Loss2: (0.0000) | Acc: (94.00%) (43832/46208)
Epoch: 131 | Batch_idx: 370 |  Loss: (0.1508) |  Loss2: (0.0000) | Acc: (94.00%) (45050/47488)
Epoch: 131 | Batch_idx: 380 |  Loss: (0.1502) |  Loss2: (0.0000) | Acc: (94.00%) (46282/48768)
Epoch: 131 | Batch_idx: 390 |  Loss: (0.1499) |  Loss2: (0.0000) | Acc: (94.00%) (47453/50000)
# TEST : Loss: (0.3799) | Acc: (88.00%) (8844/10000)
percent tensor([0.5384, 0.5517, 0.5392, 0.5163, 0.5465, 0.5598, 0.5563, 0.5278, 0.5373,
        0.5460, 0.5513, 0.5468, 0.5367, 0.5192, 0.5554, 0.5363],
       device='cuda:0') torch.Size([16])
percent tensor([0.5283, 0.5280, 0.5348, 0.5375, 0.5355, 0.5286, 0.5301, 0.5366, 0.5312,
        0.5293, 0.5272, 0.5314, 0.5271, 0.5324, 0.5279, 0.5300],
       device='cuda:0') torch.Size([16])
percent tensor([0.6390, 0.6508, 0.4652, 0.4383, 0.4272, 0.5939, 0.6029, 0.4778, 0.5535,
        0.6256, 0.6487, 0.5326, 0.6578, 0.6210, 0.6215, 0.6249],
       device='cuda:0') torch.Size([16])
percent tensor([0.6940, 0.6064, 0.6883, 0.6829, 0.6830, 0.7477, 0.6537, 0.6981, 0.6492,
        0.6285, 0.6441, 0.6358, 0.5901, 0.6797, 0.6777, 0.7226],
       device='cuda:0') torch.Size([16])
percent tensor([0.6618, 0.6641, 0.6725, 0.6113, 0.6397, 0.5971, 0.6861, 0.6689, 0.6796,
        0.6322, 0.6337, 0.6395, 0.6551, 0.6459, 0.6644, 0.6062],
       device='cuda:0') torch.Size([16])
percent tensor([0.5784, 0.6693, 0.6329, 0.7101, 0.6944, 0.7214, 0.6731, 0.5479, 0.7165,
        0.5956, 0.7046, 0.7218, 0.6732, 0.7640, 0.6037, 0.5726],
       device='cuda:0') torch.Size([16])
percent tensor([0.6719, 0.6751, 0.5577, 0.4746, 0.6207, 0.6977, 0.6122, 0.4485, 0.6632,
        0.7003, 0.6686, 0.4212, 0.6786, 0.6236, 0.4680, 0.6439],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9997, 0.9998, 0.9997, 0.9998, 0.9990, 0.9998, 0.9997, 0.9998,
        0.9995, 0.9998, 0.9999, 0.9998, 0.9997, 0.9995, 0.9995],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 132 | Batch_idx: 0 |  Loss: (0.1166) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 132 | Batch_idx: 10 |  Loss: (0.1462) |  Loss2: (0.0000) | Acc: (94.00%) (1336/1408)
Epoch: 132 | Batch_idx: 20 |  Loss: (0.1468) |  Loss2: (0.0000) | Acc: (95.00%) (2555/2688)
Epoch: 132 | Batch_idx: 30 |  Loss: (0.1417) |  Loss2: (0.0000) | Acc: (95.00%) (3774/3968)
Epoch: 132 | Batch_idx: 40 |  Loss: (0.1416) |  Loss2: (0.0000) | Acc: (95.00%) (4993/5248)
Epoch: 132 | Batch_idx: 50 |  Loss: (0.1412) |  Loss2: (0.0000) | Acc: (95.00%) (6214/6528)
Epoch: 132 | Batch_idx: 60 |  Loss: (0.1426) |  Loss2: (0.0000) | Acc: (95.00%) (7428/7808)
Epoch: 132 | Batch_idx: 70 |  Loss: (0.1453) |  Loss2: (0.0000) | Acc: (95.00%) (8638/9088)
Epoch: 132 | Batch_idx: 80 |  Loss: (0.1469) |  Loss2: (0.0000) | Acc: (94.00%) (9849/10368)
Epoch: 132 | Batch_idx: 90 |  Loss: (0.1499) |  Loss2: (0.0000) | Acc: (94.00%) (11049/11648)
Epoch: 132 | Batch_idx: 100 |  Loss: (0.1497) |  Loss2: (0.0000) | Acc: (94.00%) (12267/12928)
Epoch: 132 | Batch_idx: 110 |  Loss: (0.1506) |  Loss2: (0.0000) | Acc: (94.00%) (13467/14208)
Epoch: 132 | Batch_idx: 120 |  Loss: (0.1503) |  Loss2: (0.0000) | Acc: (94.00%) (14683/15488)
Epoch: 132 | Batch_idx: 130 |  Loss: (0.1496) |  Loss2: (0.0000) | Acc: (94.00%) (15899/16768)
Epoch: 132 | Batch_idx: 140 |  Loss: (0.1490) |  Loss2: (0.0000) | Acc: (94.00%) (17115/18048)
Epoch: 132 | Batch_idx: 150 |  Loss: (0.1481) |  Loss2: (0.0000) | Acc: (94.00%) (18339/19328)
Epoch: 132 | Batch_idx: 160 |  Loss: (0.1485) |  Loss2: (0.0000) | Acc: (94.00%) (19548/20608)
Epoch: 132 | Batch_idx: 170 |  Loss: (0.1492) |  Loss2: (0.0000) | Acc: (94.00%) (20754/21888)
Epoch: 132 | Batch_idx: 180 |  Loss: (0.1503) |  Loss2: (0.0000) | Acc: (94.00%) (21959/23168)
Epoch: 132 | Batch_idx: 190 |  Loss: (0.1516) |  Loss2: (0.0000) | Acc: (94.00%) (23157/24448)
Epoch: 132 | Batch_idx: 200 |  Loss: (0.1518) |  Loss2: (0.0000) | Acc: (94.00%) (24369/25728)
Epoch: 132 | Batch_idx: 210 |  Loss: (0.1526) |  Loss2: (0.0000) | Acc: (94.00%) (25576/27008)
Epoch: 132 | Batch_idx: 220 |  Loss: (0.1528) |  Loss2: (0.0000) | Acc: (94.00%) (26793/28288)
Epoch: 132 | Batch_idx: 230 |  Loss: (0.1522) |  Loss2: (0.0000) | Acc: (94.00%) (28023/29568)
Epoch: 132 | Batch_idx: 240 |  Loss: (0.1539) |  Loss2: (0.0000) | Acc: (94.00%) (29219/30848)
Epoch: 132 | Batch_idx: 250 |  Loss: (0.1538) |  Loss2: (0.0000) | Acc: (94.00%) (30436/32128)
Epoch: 132 | Batch_idx: 260 |  Loss: (0.1543) |  Loss2: (0.0000) | Acc: (94.00%) (31643/33408)
Epoch: 132 | Batch_idx: 270 |  Loss: (0.1540) |  Loss2: (0.0000) | Acc: (94.00%) (32860/34688)
Epoch: 132 | Batch_idx: 280 |  Loss: (0.1547) |  Loss2: (0.0000) | Acc: (94.00%) (34059/35968)
Epoch: 132 | Batch_idx: 290 |  Loss: (0.1550) |  Loss2: (0.0000) | Acc: (94.00%) (35272/37248)
Epoch: 132 | Batch_idx: 300 |  Loss: (0.1544) |  Loss2: (0.0000) | Acc: (94.00%) (36489/38528)
Epoch: 132 | Batch_idx: 310 |  Loss: (0.1538) |  Loss2: (0.0000) | Acc: (94.00%) (37701/39808)
Epoch: 132 | Batch_idx: 320 |  Loss: (0.1548) |  Loss2: (0.0000) | Acc: (94.00%) (38900/41088)
Epoch: 132 | Batch_idx: 330 |  Loss: (0.1551) |  Loss2: (0.0000) | Acc: (94.00%) (40113/42368)
Epoch: 132 | Batch_idx: 340 |  Loss: (0.1552) |  Loss2: (0.0000) | Acc: (94.00%) (41327/43648)
Epoch: 132 | Batch_idx: 350 |  Loss: (0.1549) |  Loss2: (0.0000) | Acc: (94.00%) (42543/44928)
Epoch: 132 | Batch_idx: 360 |  Loss: (0.1557) |  Loss2: (0.0000) | Acc: (94.00%) (43741/46208)
Epoch: 132 | Batch_idx: 370 |  Loss: (0.1558) |  Loss2: (0.0000) | Acc: (94.00%) (44954/47488)
Epoch: 132 | Batch_idx: 380 |  Loss: (0.1558) |  Loss2: (0.0000) | Acc: (94.00%) (46159/48768)
Epoch: 132 | Batch_idx: 390 |  Loss: (0.1558) |  Loss2: (0.0000) | Acc: (94.00%) (47324/50000)
# TEST : Loss: (0.3995) | Acc: (87.00%) (8783/10000)
percent tensor([0.5411, 0.5526, 0.5464, 0.5187, 0.5532, 0.5609, 0.5598, 0.5308, 0.5406,
        0.5494, 0.5533, 0.5549, 0.5398, 0.5197, 0.5567, 0.5375],
       device='cuda:0') torch.Size([16])
percent tensor([0.5284, 0.5276, 0.5346, 0.5389, 0.5350, 0.5295, 0.5295, 0.5373, 0.5307,
        0.5288, 0.5265, 0.5312, 0.5268, 0.5321, 0.5277, 0.5306],
       device='cuda:0') torch.Size([16])
percent tensor([0.6403, 0.6465, 0.4699, 0.4271, 0.4413, 0.5901, 0.6034, 0.4645, 0.5556,
        0.6226, 0.6467, 0.5390, 0.6585, 0.6186, 0.6209, 0.6210],
       device='cuda:0') torch.Size([16])
percent tensor([0.6911, 0.5981, 0.6826, 0.6818, 0.6887, 0.7540, 0.6463, 0.6910, 0.6530,
        0.6219, 0.6411, 0.6469, 0.5907, 0.6846, 0.6759, 0.7243],
       device='cuda:0') torch.Size([16])
percent tensor([0.6600, 0.6656, 0.6685, 0.6185, 0.6296, 0.5950, 0.6862, 0.6726, 0.6695,
        0.6305, 0.6307, 0.6084, 0.6401, 0.6473, 0.6683, 0.6064],
       device='cuda:0') torch.Size([16])
percent tensor([0.5598, 0.6746, 0.6460, 0.7055, 0.6971, 0.7351, 0.6928, 0.5510, 0.7221,
        0.6100, 0.7084, 0.7713, 0.6888, 0.7591, 0.5940, 0.5946],
       device='cuda:0') torch.Size([16])
percent tensor([0.6754, 0.6738, 0.5749, 0.5143, 0.6242, 0.7146, 0.6252, 0.4424, 0.6883,
        0.7062, 0.6705, 0.4301, 0.6790, 0.6359, 0.4925, 0.6363],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9997, 0.9999, 0.9998, 0.9999, 0.9990, 0.9998, 0.9997, 0.9998,
        0.9995, 0.9998, 0.9999, 0.9998, 0.9993, 0.9995, 0.9990],
       device='cuda:0') torch.Size([16])
Epoch: 133 | Batch_idx: 0 |  Loss: (0.1231) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 133 | Batch_idx: 10 |  Loss: (0.1389) |  Loss2: (0.0000) | Acc: (95.00%) (1339/1408)
Epoch: 133 | Batch_idx: 20 |  Loss: (0.1392) |  Loss2: (0.0000) | Acc: (95.00%) (2563/2688)
Epoch: 133 | Batch_idx: 30 |  Loss: (0.1353) |  Loss2: (0.0000) | Acc: (95.00%) (3791/3968)
Epoch: 133 | Batch_idx: 40 |  Loss: (0.1381) |  Loss2: (0.0000) | Acc: (95.00%) (5007/5248)
Epoch: 133 | Batch_idx: 50 |  Loss: (0.1335) |  Loss2: (0.0000) | Acc: (95.00%) (6238/6528)
Epoch: 133 | Batch_idx: 60 |  Loss: (0.1339) |  Loss2: (0.0000) | Acc: (95.00%) (7459/7808)
Epoch: 133 | Batch_idx: 70 |  Loss: (0.1364) |  Loss2: (0.0000) | Acc: (95.00%) (8671/9088)
Epoch: 133 | Batch_idx: 80 |  Loss: (0.1371) |  Loss2: (0.0000) | Acc: (95.00%) (9881/10368)
Epoch: 133 | Batch_idx: 90 |  Loss: (0.1387) |  Loss2: (0.0000) | Acc: (95.00%) (11095/11648)
Epoch: 133 | Batch_idx: 100 |  Loss: (0.1411) |  Loss2: (0.0000) | Acc: (95.00%) (12304/12928)
Epoch: 133 | Batch_idx: 110 |  Loss: (0.1402) |  Loss2: (0.0000) | Acc: (95.00%) (13529/14208)
Epoch: 133 | Batch_idx: 120 |  Loss: (0.1396) |  Loss2: (0.0000) | Acc: (95.00%) (14748/15488)
Epoch: 133 | Batch_idx: 130 |  Loss: (0.1424) |  Loss2: (0.0000) | Acc: (95.00%) (15939/16768)
Epoch: 133 | Batch_idx: 140 |  Loss: (0.1415) |  Loss2: (0.0000) | Acc: (95.00%) (17171/18048)
Epoch: 133 | Batch_idx: 150 |  Loss: (0.1417) |  Loss2: (0.0000) | Acc: (95.00%) (18388/19328)
Epoch: 133 | Batch_idx: 160 |  Loss: (0.1428) |  Loss2: (0.0000) | Acc: (95.00%) (19602/20608)
Epoch: 133 | Batch_idx: 170 |  Loss: (0.1423) |  Loss2: (0.0000) | Acc: (95.00%) (20829/21888)
Epoch: 133 | Batch_idx: 180 |  Loss: (0.1421) |  Loss2: (0.0000) | Acc: (95.00%) (22046/23168)
Epoch: 133 | Batch_idx: 190 |  Loss: (0.1422) |  Loss2: (0.0000) | Acc: (95.00%) (23262/24448)
Epoch: 133 | Batch_idx: 200 |  Loss: (0.1431) |  Loss2: (0.0000) | Acc: (95.00%) (24476/25728)
Epoch: 133 | Batch_idx: 210 |  Loss: (0.1420) |  Loss2: (0.0000) | Acc: (95.00%) (25692/27008)
Epoch: 133 | Batch_idx: 220 |  Loss: (0.1425) |  Loss2: (0.0000) | Acc: (95.00%) (26899/28288)
Epoch: 133 | Batch_idx: 230 |  Loss: (0.1425) |  Loss2: (0.0000) | Acc: (95.00%) (28118/29568)
Epoch: 133 | Batch_idx: 240 |  Loss: (0.1425) |  Loss2: (0.0000) | Acc: (95.00%) (29332/30848)
Epoch: 133 | Batch_idx: 250 |  Loss: (0.1432) |  Loss2: (0.0000) | Acc: (95.00%) (30545/32128)
Epoch: 133 | Batch_idx: 260 |  Loss: (0.1434) |  Loss2: (0.0000) | Acc: (95.00%) (31754/33408)
Epoch: 133 | Batch_idx: 270 |  Loss: (0.1436) |  Loss2: (0.0000) | Acc: (95.00%) (32959/34688)
Epoch: 133 | Batch_idx: 280 |  Loss: (0.1439) |  Loss2: (0.0000) | Acc: (94.00%) (34169/35968)
Epoch: 133 | Batch_idx: 290 |  Loss: (0.1448) |  Loss2: (0.0000) | Acc: (94.00%) (35382/37248)
Epoch: 133 | Batch_idx: 300 |  Loss: (0.1447) |  Loss2: (0.0000) | Acc: (95.00%) (36603/38528)
Epoch: 133 | Batch_idx: 310 |  Loss: (0.1452) |  Loss2: (0.0000) | Acc: (94.00%) (37810/39808)
Epoch: 133 | Batch_idx: 320 |  Loss: (0.1453) |  Loss2: (0.0000) | Acc: (94.00%) (39014/41088)
Epoch: 133 | Batch_idx: 330 |  Loss: (0.1457) |  Loss2: (0.0000) | Acc: (94.00%) (40226/42368)
Epoch: 133 | Batch_idx: 340 |  Loss: (0.1462) |  Loss2: (0.0000) | Acc: (94.00%) (41436/43648)
Epoch: 133 | Batch_idx: 350 |  Loss: (0.1463) |  Loss2: (0.0000) | Acc: (94.00%) (42643/44928)
Epoch: 133 | Batch_idx: 360 |  Loss: (0.1466) |  Loss2: (0.0000) | Acc: (94.00%) (43852/46208)
Epoch: 133 | Batch_idx: 370 |  Loss: (0.1472) |  Loss2: (0.0000) | Acc: (94.00%) (45057/47488)
Epoch: 133 | Batch_idx: 380 |  Loss: (0.1474) |  Loss2: (0.0000) | Acc: (94.00%) (46268/48768)
Epoch: 133 | Batch_idx: 390 |  Loss: (0.1477) |  Loss2: (0.0000) | Acc: (94.00%) (47433/50000)
# TEST : Loss: (0.4145) | Acc: (87.00%) (8746/10000)
percent tensor([0.5377, 0.5534, 0.5409, 0.5195, 0.5476, 0.5584, 0.5574, 0.5309, 0.5373,
        0.5480, 0.5514, 0.5488, 0.5371, 0.5217, 0.5561, 0.5372],
       device='cuda:0') torch.Size([16])
percent tensor([0.5287, 0.5278, 0.5346, 0.5376, 0.5349, 0.5291, 0.5298, 0.5361, 0.5312,
        0.5293, 0.5274, 0.5311, 0.5275, 0.5320, 0.5278, 0.5306],
       device='cuda:0') torch.Size([16])
percent tensor([0.6312, 0.6497, 0.4812, 0.4443, 0.4386, 0.5823, 0.6026, 0.4872, 0.5434,
        0.6287, 0.6432, 0.5504, 0.6532, 0.6273, 0.6168, 0.6228],
       device='cuda:0') torch.Size([16])
percent tensor([0.6946, 0.5969, 0.6902, 0.6940, 0.6926, 0.7568, 0.6481, 0.6893, 0.6573,
        0.6212, 0.6481, 0.6362, 0.5957, 0.6824, 0.6804, 0.7306],
       device='cuda:0') torch.Size([16])
percent tensor([0.6586, 0.6652, 0.6879, 0.6231, 0.6439, 0.5992, 0.6860, 0.6811, 0.6714,
        0.6382, 0.6293, 0.6409, 0.6380, 0.6507, 0.6667, 0.6041],
       device='cuda:0') torch.Size([16])
percent tensor([0.5465, 0.6640, 0.6288, 0.6947, 0.6961, 0.7293, 0.6804, 0.5505, 0.7090,
        0.5680, 0.6981, 0.7228, 0.6807, 0.7509, 0.5829, 0.5542],
       device='cuda:0') torch.Size([16])
percent tensor([0.6738, 0.6789, 0.5576, 0.5308, 0.6172, 0.7183, 0.6280, 0.4492, 0.6767,
        0.6990, 0.6844, 0.4848, 0.6929, 0.6555, 0.4827, 0.6260],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9997, 0.9998, 0.9997, 0.9998, 0.9991, 0.9998, 0.9996, 0.9998,
        0.9995, 0.9998, 0.9998, 0.9998, 0.9995, 0.9995, 0.9994],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 134 | Batch_idx: 0 |  Loss: (0.1239) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 134 | Batch_idx: 10 |  Loss: (0.1506) |  Loss2: (0.0000) | Acc: (95.00%) (1338/1408)
Epoch: 134 | Batch_idx: 20 |  Loss: (0.1726) |  Loss2: (0.0000) | Acc: (94.00%) (2527/2688)
Epoch: 134 | Batch_idx: 30 |  Loss: (0.1784) |  Loss2: (0.0000) | Acc: (93.00%) (3716/3968)
Epoch: 134 | Batch_idx: 40 |  Loss: (0.1849) |  Loss2: (0.0000) | Acc: (93.00%) (4906/5248)
Epoch: 134 | Batch_idx: 50 |  Loss: (0.1907) |  Loss2: (0.0000) | Acc: (93.00%) (6088/6528)
Epoch: 134 | Batch_idx: 60 |  Loss: (0.1919) |  Loss2: (0.0000) | Acc: (93.00%) (7282/7808)
Epoch: 134 | Batch_idx: 70 |  Loss: (0.1950) |  Loss2: (0.0000) | Acc: (93.00%) (8466/9088)
Epoch: 134 | Batch_idx: 80 |  Loss: (0.1946) |  Loss2: (0.0000) | Acc: (93.00%) (9665/10368)
Epoch: 134 | Batch_idx: 90 |  Loss: (0.1943) |  Loss2: (0.0000) | Acc: (93.00%) (10861/11648)
Epoch: 134 | Batch_idx: 100 |  Loss: (0.1938) |  Loss2: (0.0000) | Acc: (93.00%) (12060/12928)
Epoch: 134 | Batch_idx: 110 |  Loss: (0.1933) |  Loss2: (0.0000) | Acc: (93.00%) (13255/14208)
Epoch: 134 | Batch_idx: 120 |  Loss: (0.1920) |  Loss2: (0.0000) | Acc: (93.00%) (14456/15488)
Epoch: 134 | Batch_idx: 130 |  Loss: (0.1892) |  Loss2: (0.0000) | Acc: (93.00%) (15674/16768)
Epoch: 134 | Batch_idx: 140 |  Loss: (0.1898) |  Loss2: (0.0000) | Acc: (93.00%) (16856/18048)
Epoch: 134 | Batch_idx: 150 |  Loss: (0.1889) |  Loss2: (0.0000) | Acc: (93.00%) (18054/19328)
Epoch: 134 | Batch_idx: 160 |  Loss: (0.1879) |  Loss2: (0.0000) | Acc: (93.00%) (19252/20608)
Epoch: 134 | Batch_idx: 170 |  Loss: (0.1873) |  Loss2: (0.0000) | Acc: (93.00%) (20446/21888)
Epoch: 134 | Batch_idx: 180 |  Loss: (0.1857) |  Loss2: (0.0000) | Acc: (93.00%) (21649/23168)
Epoch: 134 | Batch_idx: 190 |  Loss: (0.1866) |  Loss2: (0.0000) | Acc: (93.00%) (22834/24448)
Epoch: 134 | Batch_idx: 200 |  Loss: (0.1863) |  Loss2: (0.0000) | Acc: (93.00%) (24046/25728)
Epoch: 134 | Batch_idx: 210 |  Loss: (0.1864) |  Loss2: (0.0000) | Acc: (93.00%) (25254/27008)
Epoch: 134 | Batch_idx: 220 |  Loss: (0.1846) |  Loss2: (0.0000) | Acc: (93.00%) (26465/28288)
Epoch: 134 | Batch_idx: 230 |  Loss: (0.1842) |  Loss2: (0.0000) | Acc: (93.00%) (27661/29568)
Epoch: 134 | Batch_idx: 240 |  Loss: (0.1839) |  Loss2: (0.0000) | Acc: (93.00%) (28861/30848)
Epoch: 134 | Batch_idx: 250 |  Loss: (0.1835) |  Loss2: (0.0000) | Acc: (93.00%) (30056/32128)
Epoch: 134 | Batch_idx: 260 |  Loss: (0.1834) |  Loss2: (0.0000) | Acc: (93.00%) (31256/33408)
Epoch: 134 | Batch_idx: 270 |  Loss: (0.1820) |  Loss2: (0.0000) | Acc: (93.00%) (32467/34688)
Epoch: 134 | Batch_idx: 280 |  Loss: (0.1821) |  Loss2: (0.0000) | Acc: (93.00%) (33671/35968)
Epoch: 134 | Batch_idx: 290 |  Loss: (0.1816) |  Loss2: (0.0000) | Acc: (93.00%) (34877/37248)
Epoch: 134 | Batch_idx: 300 |  Loss: (0.1809) |  Loss2: (0.0000) | Acc: (93.00%) (36089/38528)
Epoch: 134 | Batch_idx: 310 |  Loss: (0.1796) |  Loss2: (0.0000) | Acc: (93.00%) (37303/39808)
Epoch: 134 | Batch_idx: 320 |  Loss: (0.1796) |  Loss2: (0.0000) | Acc: (93.00%) (38492/41088)
Epoch: 134 | Batch_idx: 330 |  Loss: (0.1795) |  Loss2: (0.0000) | Acc: (93.00%) (39694/42368)
Epoch: 134 | Batch_idx: 340 |  Loss: (0.1786) |  Loss2: (0.0000) | Acc: (93.00%) (40907/43648)
Epoch: 134 | Batch_idx: 350 |  Loss: (0.1784) |  Loss2: (0.0000) | Acc: (93.00%) (42096/44928)
Epoch: 134 | Batch_idx: 360 |  Loss: (0.1777) |  Loss2: (0.0000) | Acc: (93.00%) (43311/46208)
Epoch: 134 | Batch_idx: 370 |  Loss: (0.1767) |  Loss2: (0.0000) | Acc: (93.00%) (44523/47488)
Epoch: 134 | Batch_idx: 380 |  Loss: (0.1761) |  Loss2: (0.0000) | Acc: (93.00%) (45744/48768)
Epoch: 134 | Batch_idx: 390 |  Loss: (0.1757) |  Loss2: (0.0000) | Acc: (93.00%) (46912/50000)
# TEST : Loss: (0.4209) | Acc: (87.00%) (8738/10000)
percent tensor([0.5408, 0.5584, 0.5438, 0.5220, 0.5518, 0.5602, 0.5628, 0.5351, 0.5426,
        0.5532, 0.5557, 0.5537, 0.5422, 0.5281, 0.5596, 0.5404],
       device='cuda:0') torch.Size([16])
percent tensor([0.5394, 0.5394, 0.5449, 0.5491, 0.5454, 0.5386, 0.5405, 0.5475, 0.5420,
        0.5406, 0.5386, 0.5410, 0.5389, 0.5432, 0.5384, 0.5414],
       device='cuda:0') torch.Size([16])
percent tensor([0.6113, 0.6419, 0.4573, 0.4233, 0.4139, 0.5506, 0.5929, 0.4765, 0.5257,
        0.6111, 0.6269, 0.5311, 0.6308, 0.6250, 0.5928, 0.6039],
       device='cuda:0') torch.Size([16])
percent tensor([0.7414, 0.6392, 0.7279, 0.7406, 0.7317, 0.7860, 0.6931, 0.7379, 0.6990,
        0.6688, 0.6940, 0.6806, 0.6440, 0.7253, 0.7258, 0.7756],
       device='cuda:0') torch.Size([16])
percent tensor([0.6588, 0.6668, 0.6759, 0.6024, 0.6375, 0.6120, 0.6813, 0.6735, 0.6633,
        0.6444, 0.6297, 0.6238, 0.6343, 0.6348, 0.6676, 0.6172],
       device='cuda:0') torch.Size([16])
percent tensor([0.5227, 0.6451, 0.6212, 0.6867, 0.6789, 0.7187, 0.6728, 0.5354, 0.6853,
        0.5436, 0.6739, 0.7144, 0.6480, 0.7420, 0.5559, 0.5351],
       device='cuda:0') torch.Size([16])
percent tensor([0.7068, 0.6993, 0.5668, 0.5022, 0.6237, 0.7413, 0.6518, 0.4419, 0.6981,
        0.7355, 0.7143, 0.5020, 0.7141, 0.6735, 0.4897, 0.6609],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9997, 0.9998, 0.9997, 0.9998, 0.9992, 0.9998, 0.9996, 0.9999,
        0.9996, 0.9999, 0.9998, 0.9998, 0.9994, 0.9996, 0.9991],
       device='cuda:0') torch.Size([16])
Epoch: 135 | Batch_idx: 0 |  Loss: (0.1201) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 135 | Batch_idx: 10 |  Loss: (0.1601) |  Loss2: (0.0000) | Acc: (94.00%) (1333/1408)
Epoch: 135 | Batch_idx: 20 |  Loss: (0.1705) |  Loss2: (0.0000) | Acc: (94.00%) (2540/2688)
Epoch: 135 | Batch_idx: 30 |  Loss: (0.1712) |  Loss2: (0.0000) | Acc: (94.00%) (3740/3968)
Epoch: 135 | Batch_idx: 40 |  Loss: (0.1647) |  Loss2: (0.0000) | Acc: (94.00%) (4952/5248)
Epoch: 135 | Batch_idx: 50 |  Loss: (0.1690) |  Loss2: (0.0000) | Acc: (94.00%) (6146/6528)
Epoch: 135 | Batch_idx: 60 |  Loss: (0.1676) |  Loss2: (0.0000) | Acc: (94.00%) (7350/7808)
Epoch: 135 | Batch_idx: 70 |  Loss: (0.1646) |  Loss2: (0.0000) | Acc: (94.00%) (8563/9088)
Epoch: 135 | Batch_idx: 80 |  Loss: (0.1654) |  Loss2: (0.0000) | Acc: (94.00%) (9763/10368)
Epoch: 135 | Batch_idx: 90 |  Loss: (0.1654) |  Loss2: (0.0000) | Acc: (94.00%) (10963/11648)
Epoch: 135 | Batch_idx: 100 |  Loss: (0.1618) |  Loss2: (0.0000) | Acc: (94.00%) (12188/12928)
Epoch: 135 | Batch_idx: 110 |  Loss: (0.1613) |  Loss2: (0.0000) | Acc: (94.00%) (13399/14208)
Epoch: 135 | Batch_idx: 120 |  Loss: (0.1614) |  Loss2: (0.0000) | Acc: (94.00%) (14604/15488)
Epoch: 135 | Batch_idx: 130 |  Loss: (0.1611) |  Loss2: (0.0000) | Acc: (94.00%) (15815/16768)
Epoch: 135 | Batch_idx: 140 |  Loss: (0.1616) |  Loss2: (0.0000) | Acc: (94.00%) (17017/18048)
Epoch: 135 | Batch_idx: 150 |  Loss: (0.1625) |  Loss2: (0.0000) | Acc: (94.00%) (18217/19328)
Epoch: 135 | Batch_idx: 160 |  Loss: (0.1636) |  Loss2: (0.0000) | Acc: (94.00%) (19422/20608)
Epoch: 135 | Batch_idx: 170 |  Loss: (0.1631) |  Loss2: (0.0000) | Acc: (94.00%) (20634/21888)
Epoch: 135 | Batch_idx: 180 |  Loss: (0.1638) |  Loss2: (0.0000) | Acc: (94.00%) (21830/23168)
Epoch: 135 | Batch_idx: 190 |  Loss: (0.1631) |  Loss2: (0.0000) | Acc: (94.00%) (23041/24448)
Epoch: 135 | Batch_idx: 200 |  Loss: (0.1637) |  Loss2: (0.0000) | Acc: (94.00%) (24242/25728)
Epoch: 135 | Batch_idx: 210 |  Loss: (0.1644) |  Loss2: (0.0000) | Acc: (94.00%) (25441/27008)
Epoch: 135 | Batch_idx: 220 |  Loss: (0.1641) |  Loss2: (0.0000) | Acc: (94.00%) (26648/28288)
Epoch: 135 | Batch_idx: 230 |  Loss: (0.1635) |  Loss2: (0.0000) | Acc: (94.00%) (27865/29568)
Epoch: 135 | Batch_idx: 240 |  Loss: (0.1630) |  Loss2: (0.0000) | Acc: (94.00%) (29084/30848)
Epoch: 135 | Batch_idx: 250 |  Loss: (0.1628) |  Loss2: (0.0000) | Acc: (94.00%) (30291/32128)
Epoch: 135 | Batch_idx: 260 |  Loss: (0.1636) |  Loss2: (0.0000) | Acc: (94.00%) (31491/33408)
Epoch: 135 | Batch_idx: 270 |  Loss: (0.1625) |  Loss2: (0.0000) | Acc: (94.00%) (32716/34688)
Epoch: 135 | Batch_idx: 280 |  Loss: (0.1621) |  Loss2: (0.0000) | Acc: (94.00%) (33926/35968)
Epoch: 135 | Batch_idx: 290 |  Loss: (0.1619) |  Loss2: (0.0000) | Acc: (94.00%) (35147/37248)
Epoch: 135 | Batch_idx: 300 |  Loss: (0.1613) |  Loss2: (0.0000) | Acc: (94.00%) (36366/38528)
Epoch: 135 | Batch_idx: 310 |  Loss: (0.1606) |  Loss2: (0.0000) | Acc: (94.00%) (37582/39808)
Epoch: 135 | Batch_idx: 320 |  Loss: (0.1605) |  Loss2: (0.0000) | Acc: (94.00%) (38796/41088)
Epoch: 135 | Batch_idx: 330 |  Loss: (0.1602) |  Loss2: (0.0000) | Acc: (94.00%) (40010/42368)
Epoch: 135 | Batch_idx: 340 |  Loss: (0.1603) |  Loss2: (0.0000) | Acc: (94.00%) (41214/43648)
Epoch: 135 | Batch_idx: 350 |  Loss: (0.1611) |  Loss2: (0.0000) | Acc: (94.00%) (42413/44928)
Epoch: 135 | Batch_idx: 360 |  Loss: (0.1611) |  Loss2: (0.0000) | Acc: (94.00%) (43620/46208)
Epoch: 135 | Batch_idx: 370 |  Loss: (0.1607) |  Loss2: (0.0000) | Acc: (94.00%) (44834/47488)
Epoch: 135 | Batch_idx: 380 |  Loss: (0.1607) |  Loss2: (0.0000) | Acc: (94.00%) (46040/48768)
Epoch: 135 | Batch_idx: 390 |  Loss: (0.1611) |  Loss2: (0.0000) | Acc: (94.00%) (47190/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_135.pth.tar'
# TEST : Loss: (0.4065) | Acc: (87.00%) (8782/10000)
percent tensor([0.5382, 0.5559, 0.5402, 0.5184, 0.5478, 0.5554, 0.5601, 0.5327, 0.5404,
        0.5512, 0.5534, 0.5507, 0.5403, 0.5269, 0.5559, 0.5376],
       device='cuda:0') torch.Size([16])
percent tensor([0.5426, 0.5433, 0.5482, 0.5523, 0.5489, 0.5413, 0.5442, 0.5508, 0.5454,
        0.5442, 0.5423, 0.5445, 0.5425, 0.5467, 0.5417, 0.5445],
       device='cuda:0') torch.Size([16])
percent tensor([0.6114, 0.6454, 0.4585, 0.4226, 0.4174, 0.5444, 0.5958, 0.4826, 0.5261,
        0.6141, 0.6275, 0.5330, 0.6309, 0.6282, 0.5907, 0.6039],
       device='cuda:0') torch.Size([16])
percent tensor([0.7348, 0.6257, 0.7244, 0.7362, 0.7272, 0.7834, 0.6847, 0.7337, 0.6927,
        0.6578, 0.6856, 0.6704, 0.6313, 0.7177, 0.7155, 0.7704],
       device='cuda:0') torch.Size([16])
percent tensor([0.6678, 0.6673, 0.6801, 0.6047, 0.6447, 0.6279, 0.6824, 0.6762, 0.6616,
        0.6471, 0.6290, 0.6232, 0.6353, 0.6306, 0.6743, 0.6309],
       device='cuda:0') torch.Size([16])
percent tensor([0.5079, 0.6433, 0.6140, 0.6825, 0.6762, 0.7046, 0.6683, 0.5306, 0.6848,
        0.5426, 0.6693, 0.7143, 0.6412, 0.7376, 0.5480, 0.5214],
       device='cuda:0') torch.Size([16])
percent tensor([0.7156, 0.7056, 0.5619, 0.5015, 0.6297, 0.7536, 0.6513, 0.4354, 0.6987,
        0.7437, 0.7197, 0.4917, 0.7179, 0.6793, 0.4793, 0.6724],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9997, 0.9997, 0.9997, 0.9997, 0.9992, 0.9998, 0.9996, 0.9999,
        0.9996, 0.9999, 0.9998, 0.9998, 0.9994, 0.9995, 0.9991],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 136 | Batch_idx: 0 |  Loss: (0.1752) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 136 | Batch_idx: 10 |  Loss: (0.1470) |  Loss2: (0.0000) | Acc: (95.00%) (1338/1408)
Epoch: 136 | Batch_idx: 20 |  Loss: (0.1473) |  Loss2: (0.0000) | Acc: (94.00%) (2549/2688)
Epoch: 136 | Batch_idx: 30 |  Loss: (0.1379) |  Loss2: (0.0000) | Acc: (95.00%) (3783/3968)
Epoch: 136 | Batch_idx: 40 |  Loss: (0.1358) |  Loss2: (0.0000) | Acc: (95.00%) (5010/5248)
Epoch: 136 | Batch_idx: 50 |  Loss: (0.1353) |  Loss2: (0.0000) | Acc: (95.00%) (6242/6528)
Epoch: 136 | Batch_idx: 60 |  Loss: (0.1361) |  Loss2: (0.0000) | Acc: (95.00%) (7462/7808)
Epoch: 136 | Batch_idx: 70 |  Loss: (0.1370) |  Loss2: (0.0000) | Acc: (95.00%) (8682/9088)
Epoch: 136 | Batch_idx: 80 |  Loss: (0.1347) |  Loss2: (0.0000) | Acc: (95.00%) (9914/10368)
Epoch: 136 | Batch_idx: 90 |  Loss: (0.1343) |  Loss2: (0.0000) | Acc: (95.00%) (11137/11648)
Epoch: 136 | Batch_idx: 100 |  Loss: (0.1343) |  Loss2: (0.0000) | Acc: (95.00%) (12353/12928)
Epoch: 136 | Batch_idx: 110 |  Loss: (0.1361) |  Loss2: (0.0000) | Acc: (95.00%) (13575/14208)
Epoch: 136 | Batch_idx: 120 |  Loss: (0.1375) |  Loss2: (0.0000) | Acc: (95.00%) (14784/15488)
Epoch: 136 | Batch_idx: 130 |  Loss: (0.1386) |  Loss2: (0.0000) | Acc: (95.00%) (15999/16768)
Epoch: 136 | Batch_idx: 140 |  Loss: (0.1399) |  Loss2: (0.0000) | Acc: (95.00%) (17211/18048)
Epoch: 136 | Batch_idx: 150 |  Loss: (0.1395) |  Loss2: (0.0000) | Acc: (95.00%) (18434/19328)
Epoch: 136 | Batch_idx: 160 |  Loss: (0.1400) |  Loss2: (0.0000) | Acc: (95.00%) (19644/20608)
Epoch: 136 | Batch_idx: 170 |  Loss: (0.1409) |  Loss2: (0.0000) | Acc: (95.00%) (20863/21888)
Epoch: 136 | Batch_idx: 180 |  Loss: (0.1410) |  Loss2: (0.0000) | Acc: (95.00%) (22076/23168)
Epoch: 136 | Batch_idx: 190 |  Loss: (0.1410) |  Loss2: (0.0000) | Acc: (95.00%) (23300/24448)
Epoch: 136 | Batch_idx: 200 |  Loss: (0.1413) |  Loss2: (0.0000) | Acc: (95.00%) (24518/25728)
Epoch: 136 | Batch_idx: 210 |  Loss: (0.1415) |  Loss2: (0.0000) | Acc: (95.00%) (25737/27008)
Epoch: 136 | Batch_idx: 220 |  Loss: (0.1424) |  Loss2: (0.0000) | Acc: (95.00%) (26943/28288)
Epoch: 136 | Batch_idx: 230 |  Loss: (0.1425) |  Loss2: (0.0000) | Acc: (95.00%) (28149/29568)
Epoch: 136 | Batch_idx: 240 |  Loss: (0.1437) |  Loss2: (0.0000) | Acc: (95.00%) (29354/30848)
Epoch: 136 | Batch_idx: 250 |  Loss: (0.1442) |  Loss2: (0.0000) | Acc: (95.00%) (30566/32128)
Epoch: 136 | Batch_idx: 260 |  Loss: (0.1452) |  Loss2: (0.0000) | Acc: (95.00%) (31763/33408)
Epoch: 136 | Batch_idx: 270 |  Loss: (0.1460) |  Loss2: (0.0000) | Acc: (95.00%) (32965/34688)
Epoch: 136 | Batch_idx: 280 |  Loss: (0.1462) |  Loss2: (0.0000) | Acc: (95.00%) (34172/35968)
Epoch: 136 | Batch_idx: 290 |  Loss: (0.1462) |  Loss2: (0.0000) | Acc: (95.00%) (35388/37248)
Epoch: 136 | Batch_idx: 300 |  Loss: (0.1464) |  Loss2: (0.0000) | Acc: (94.00%) (36601/38528)
Epoch: 136 | Batch_idx: 310 |  Loss: (0.1466) |  Loss2: (0.0000) | Acc: (95.00%) (37822/39808)
Epoch: 136 | Batch_idx: 320 |  Loss: (0.1464) |  Loss2: (0.0000) | Acc: (95.00%) (39040/41088)
Epoch: 136 | Batch_idx: 330 |  Loss: (0.1464) |  Loss2: (0.0000) | Acc: (95.00%) (40256/42368)
Epoch: 136 | Batch_idx: 340 |  Loss: (0.1461) |  Loss2: (0.0000) | Acc: (95.00%) (41483/43648)
Epoch: 136 | Batch_idx: 350 |  Loss: (0.1460) |  Loss2: (0.0000) | Acc: (95.00%) (42699/44928)
Epoch: 136 | Batch_idx: 360 |  Loss: (0.1463) |  Loss2: (0.0000) | Acc: (95.00%) (43904/46208)
Epoch: 136 | Batch_idx: 370 |  Loss: (0.1463) |  Loss2: (0.0000) | Acc: (95.00%) (45123/47488)
Epoch: 136 | Batch_idx: 380 |  Loss: (0.1462) |  Loss2: (0.0000) | Acc: (95.00%) (46347/48768)
Epoch: 136 | Batch_idx: 390 |  Loss: (0.1463) |  Loss2: (0.0000) | Acc: (95.00%) (47511/50000)
# TEST : Loss: (0.4395) | Acc: (86.00%) (8663/10000)
percent tensor([0.5415, 0.5566, 0.5426, 0.5189, 0.5499, 0.5573, 0.5611, 0.5331, 0.5429,
        0.5518, 0.5554, 0.5523, 0.5420, 0.5252, 0.5567, 0.5385],
       device='cuda:0') torch.Size([16])
percent tensor([0.5421, 0.5435, 0.5478, 0.5523, 0.5495, 0.5402, 0.5450, 0.5508, 0.5460,
        0.5446, 0.5424, 0.5458, 0.5421, 0.5478, 0.5407, 0.5446],
       device='cuda:0') torch.Size([16])
percent tensor([0.6270, 0.6393, 0.4592, 0.4117, 0.4219, 0.5715, 0.5945, 0.4665, 0.5342,
        0.6071, 0.6340, 0.5166, 0.6408, 0.6003, 0.6037, 0.6040],
       device='cuda:0') torch.Size([16])
percent tensor([0.7330, 0.6313, 0.7176, 0.7186, 0.7203, 0.7788, 0.6864, 0.7273, 0.6874,
        0.6640, 0.6826, 0.6587, 0.6256, 0.7180, 0.7069, 0.7673],
       device='cuda:0') torch.Size([16])
percent tensor([0.6714, 0.6627, 0.6771, 0.6028, 0.6408, 0.6089, 0.6852, 0.6706, 0.6650,
        0.6438, 0.6290, 0.6179, 0.6425, 0.6309, 0.6659, 0.6208],
       device='cuda:0') torch.Size([16])
percent tensor([0.5145, 0.6404, 0.6114, 0.6658, 0.6884, 0.7020, 0.6359, 0.5279, 0.6962,
        0.5586, 0.6905, 0.7219, 0.6467, 0.7260, 0.5578, 0.5601],
       device='cuda:0') torch.Size([16])
percent tensor([0.7054, 0.6836, 0.5756, 0.5405, 0.6741, 0.7510, 0.6266, 0.4689, 0.6536,
        0.7057, 0.6596, 0.4322, 0.6857, 0.6374, 0.5040, 0.6737],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9997, 0.9998, 0.9998, 0.9999, 0.9991, 0.9997, 0.9996, 0.9998,
        0.9996, 0.9998, 0.9999, 0.9998, 0.9996, 0.9995, 0.9995],
       device='cuda:0') torch.Size([16])
Epoch: 137 | Batch_idx: 0 |  Loss: (0.0764) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 137 | Batch_idx: 10 |  Loss: (0.1351) |  Loss2: (0.0000) | Acc: (94.00%) (1337/1408)
Epoch: 137 | Batch_idx: 20 |  Loss: (0.1378) |  Loss2: (0.0000) | Acc: (95.00%) (2555/2688)
Epoch: 137 | Batch_idx: 30 |  Loss: (0.1367) |  Loss2: (0.0000) | Acc: (95.00%) (3770/3968)
Epoch: 137 | Batch_idx: 40 |  Loss: (0.1324) |  Loss2: (0.0000) | Acc: (95.00%) (5001/5248)
Epoch: 137 | Batch_idx: 50 |  Loss: (0.1379) |  Loss2: (0.0000) | Acc: (95.00%) (6217/6528)
Epoch: 137 | Batch_idx: 60 |  Loss: (0.1373) |  Loss2: (0.0000) | Acc: (95.00%) (7438/7808)
Epoch: 137 | Batch_idx: 70 |  Loss: (0.1362) |  Loss2: (0.0000) | Acc: (95.00%) (8659/9088)
Epoch: 137 | Batch_idx: 80 |  Loss: (0.1324) |  Loss2: (0.0000) | Acc: (95.00%) (9899/10368)
Epoch: 137 | Batch_idx: 90 |  Loss: (0.1317) |  Loss2: (0.0000) | Acc: (95.00%) (11131/11648)
Epoch: 137 | Batch_idx: 100 |  Loss: (0.1326) |  Loss2: (0.0000) | Acc: (95.00%) (12345/12928)
Epoch: 137 | Batch_idx: 110 |  Loss: (0.1323) |  Loss2: (0.0000) | Acc: (95.00%) (13570/14208)
Epoch: 137 | Batch_idx: 120 |  Loss: (0.1331) |  Loss2: (0.0000) | Acc: (95.00%) (14789/15488)
Epoch: 137 | Batch_idx: 130 |  Loss: (0.1330) |  Loss2: (0.0000) | Acc: (95.00%) (16017/16768)
Epoch: 137 | Batch_idx: 140 |  Loss: (0.1328) |  Loss2: (0.0000) | Acc: (95.00%) (17236/18048)
Epoch: 137 | Batch_idx: 150 |  Loss: (0.1323) |  Loss2: (0.0000) | Acc: (95.00%) (18459/19328)
Epoch: 137 | Batch_idx: 160 |  Loss: (0.1317) |  Loss2: (0.0000) | Acc: (95.00%) (19683/20608)
Epoch: 137 | Batch_idx: 170 |  Loss: (0.1326) |  Loss2: (0.0000) | Acc: (95.00%) (20898/21888)
Epoch: 137 | Batch_idx: 180 |  Loss: (0.1331) |  Loss2: (0.0000) | Acc: (95.00%) (22118/23168)
Epoch: 137 | Batch_idx: 190 |  Loss: (0.1345) |  Loss2: (0.0000) | Acc: (95.00%) (23322/24448)
Epoch: 137 | Batch_idx: 200 |  Loss: (0.1344) |  Loss2: (0.0000) | Acc: (95.00%) (24541/25728)
Epoch: 137 | Batch_idx: 210 |  Loss: (0.1343) |  Loss2: (0.0000) | Acc: (95.00%) (25757/27008)
Epoch: 137 | Batch_idx: 220 |  Loss: (0.1352) |  Loss2: (0.0000) | Acc: (95.00%) (26965/28288)
Epoch: 137 | Batch_idx: 230 |  Loss: (0.1360) |  Loss2: (0.0000) | Acc: (95.00%) (28173/29568)
Epoch: 137 | Batch_idx: 240 |  Loss: (0.1369) |  Loss2: (0.0000) | Acc: (95.00%) (29384/30848)
Epoch: 137 | Batch_idx: 250 |  Loss: (0.1369) |  Loss2: (0.0000) | Acc: (95.00%) (30603/32128)
Epoch: 137 | Batch_idx: 260 |  Loss: (0.1377) |  Loss2: (0.0000) | Acc: (95.00%) (31813/33408)
Epoch: 137 | Batch_idx: 270 |  Loss: (0.1386) |  Loss2: (0.0000) | Acc: (95.00%) (33025/34688)
Epoch: 137 | Batch_idx: 280 |  Loss: (0.1395) |  Loss2: (0.0000) | Acc: (95.00%) (34235/35968)
Epoch: 137 | Batch_idx: 290 |  Loss: (0.1397) |  Loss2: (0.0000) | Acc: (95.00%) (35446/37248)
Epoch: 137 | Batch_idx: 300 |  Loss: (0.1401) |  Loss2: (0.0000) | Acc: (95.00%) (36661/38528)
Epoch: 137 | Batch_idx: 310 |  Loss: (0.1404) |  Loss2: (0.0000) | Acc: (95.00%) (37862/39808)
Epoch: 137 | Batch_idx: 320 |  Loss: (0.1403) |  Loss2: (0.0000) | Acc: (95.00%) (39088/41088)
Epoch: 137 | Batch_idx: 330 |  Loss: (0.1402) |  Loss2: (0.0000) | Acc: (95.00%) (40314/42368)
Epoch: 137 | Batch_idx: 340 |  Loss: (0.1401) |  Loss2: (0.0000) | Acc: (95.00%) (41532/43648)
Epoch: 137 | Batch_idx: 350 |  Loss: (0.1402) |  Loss2: (0.0000) | Acc: (95.00%) (42750/44928)
Epoch: 137 | Batch_idx: 360 |  Loss: (0.1401) |  Loss2: (0.0000) | Acc: (95.00%) (43978/46208)
Epoch: 137 | Batch_idx: 370 |  Loss: (0.1405) |  Loss2: (0.0000) | Acc: (95.00%) (45192/47488)
Epoch: 137 | Batch_idx: 380 |  Loss: (0.1411) |  Loss2: (0.0000) | Acc: (95.00%) (46404/48768)
Epoch: 137 | Batch_idx: 390 |  Loss: (0.1408) |  Loss2: (0.0000) | Acc: (95.00%) (47578/50000)
# TEST : Loss: (0.4448) | Acc: (87.00%) (8732/10000)
percent tensor([0.5412, 0.5562, 0.5443, 0.5180, 0.5490, 0.5570, 0.5605, 0.5339, 0.5419,
        0.5517, 0.5546, 0.5525, 0.5407, 0.5242, 0.5567, 0.5384],
       device='cuda:0') torch.Size([16])
percent tensor([0.5421, 0.5430, 0.5481, 0.5520, 0.5502, 0.5406, 0.5447, 0.5514, 0.5461,
        0.5442, 0.5421, 0.5459, 0.5421, 0.5468, 0.5408, 0.5441],
       device='cuda:0') torch.Size([16])
percent tensor([0.6192, 0.6400, 0.4624, 0.4223, 0.4318, 0.5693, 0.5997, 0.4632, 0.5187,
        0.6172, 0.6232, 0.5288, 0.6298, 0.6045, 0.6038, 0.6076],
       device='cuda:0') torch.Size([16])
percent tensor([0.7300, 0.6369, 0.7137, 0.7152, 0.7215, 0.7849, 0.6889, 0.7347, 0.6827,
        0.6604, 0.6794, 0.6579, 0.6208, 0.7109, 0.7109, 0.7675],
       device='cuda:0') torch.Size([16])
percent tensor([0.6620, 0.6449, 0.6859, 0.6114, 0.6403, 0.6019, 0.6821, 0.6668, 0.6523,
        0.6318, 0.6146, 0.6251, 0.6295, 0.6459, 0.6609, 0.6268],
       device='cuda:0') torch.Size([16])
percent tensor([0.5423, 0.6494, 0.5985, 0.6647, 0.6756, 0.7146, 0.6612, 0.5116, 0.7128,
        0.5579, 0.7087, 0.7007, 0.6519, 0.7239, 0.5741, 0.5588],
       device='cuda:0') torch.Size([16])
percent tensor([0.7086, 0.7158, 0.5818, 0.5455, 0.6509, 0.7302, 0.6523, 0.4595, 0.6905,
        0.7232, 0.6956, 0.4572, 0.7182, 0.6728, 0.4770, 0.6823],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9998, 0.9999, 0.9998, 0.9999, 0.9989, 0.9998, 0.9997, 0.9998,
        0.9996, 0.9999, 0.9999, 0.9999, 0.9994, 0.9996, 0.9993],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 138 | Batch_idx: 0 |  Loss: (0.1885) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 138 | Batch_idx: 10 |  Loss: (0.1352) |  Loss2: (0.0000) | Acc: (95.00%) (1341/1408)
Epoch: 138 | Batch_idx: 20 |  Loss: (0.1417) |  Loss2: (0.0000) | Acc: (94.00%) (2545/2688)
Epoch: 138 | Batch_idx: 30 |  Loss: (0.1483) |  Loss2: (0.0000) | Acc: (94.00%) (3752/3968)
Epoch: 138 | Batch_idx: 40 |  Loss: (0.1473) |  Loss2: (0.0000) | Acc: (94.00%) (4970/5248)
Epoch: 138 | Batch_idx: 50 |  Loss: (0.1524) |  Loss2: (0.0000) | Acc: (94.00%) (6162/6528)
Epoch: 138 | Batch_idx: 60 |  Loss: (0.1528) |  Loss2: (0.0000) | Acc: (94.00%) (7368/7808)
Epoch: 138 | Batch_idx: 70 |  Loss: (0.1547) |  Loss2: (0.0000) | Acc: (94.00%) (8575/9088)
Epoch: 138 | Batch_idx: 80 |  Loss: (0.1574) |  Loss2: (0.0000) | Acc: (94.00%) (9777/10368)
Epoch: 138 | Batch_idx: 90 |  Loss: (0.1584) |  Loss2: (0.0000) | Acc: (94.00%) (10985/11648)
Epoch: 138 | Batch_idx: 100 |  Loss: (0.1592) |  Loss2: (0.0000) | Acc: (94.00%) (12184/12928)
Epoch: 138 | Batch_idx: 110 |  Loss: (0.1584) |  Loss2: (0.0000) | Acc: (94.00%) (13385/14208)
Epoch: 138 | Batch_idx: 120 |  Loss: (0.1570) |  Loss2: (0.0000) | Acc: (94.00%) (14604/15488)
Epoch: 138 | Batch_idx: 130 |  Loss: (0.1582) |  Loss2: (0.0000) | Acc: (94.00%) (15808/16768)
Epoch: 138 | Batch_idx: 140 |  Loss: (0.1581) |  Loss2: (0.0000) | Acc: (94.00%) (17016/18048)
Epoch: 138 | Batch_idx: 150 |  Loss: (0.1581) |  Loss2: (0.0000) | Acc: (94.00%) (18224/19328)
Epoch: 138 | Batch_idx: 160 |  Loss: (0.1584) |  Loss2: (0.0000) | Acc: (94.00%) (19426/20608)
Epoch: 138 | Batch_idx: 170 |  Loss: (0.1586) |  Loss2: (0.0000) | Acc: (94.00%) (20636/21888)
Epoch: 138 | Batch_idx: 180 |  Loss: (0.1595) |  Loss2: (0.0000) | Acc: (94.00%) (21834/23168)
Epoch: 138 | Batch_idx: 190 |  Loss: (0.1590) |  Loss2: (0.0000) | Acc: (94.00%) (23048/24448)
Epoch: 138 | Batch_idx: 200 |  Loss: (0.1587) |  Loss2: (0.0000) | Acc: (94.00%) (24263/25728)
Epoch: 138 | Batch_idx: 210 |  Loss: (0.1583) |  Loss2: (0.0000) | Acc: (94.00%) (25474/27008)
Epoch: 138 | Batch_idx: 220 |  Loss: (0.1579) |  Loss2: (0.0000) | Acc: (94.00%) (26687/28288)
Epoch: 138 | Batch_idx: 230 |  Loss: (0.1575) |  Loss2: (0.0000) | Acc: (94.00%) (27902/29568)
Epoch: 138 | Batch_idx: 240 |  Loss: (0.1569) |  Loss2: (0.0000) | Acc: (94.00%) (29118/30848)
Epoch: 138 | Batch_idx: 250 |  Loss: (0.1567) |  Loss2: (0.0000) | Acc: (94.00%) (30339/32128)
Epoch: 138 | Batch_idx: 260 |  Loss: (0.1567) |  Loss2: (0.0000) | Acc: (94.00%) (31549/33408)
Epoch: 138 | Batch_idx: 270 |  Loss: (0.1561) |  Loss2: (0.0000) | Acc: (94.00%) (32766/34688)
Epoch: 138 | Batch_idx: 280 |  Loss: (0.1559) |  Loss2: (0.0000) | Acc: (94.00%) (33985/35968)
Epoch: 138 | Batch_idx: 290 |  Loss: (0.1554) |  Loss2: (0.0000) | Acc: (94.00%) (35197/37248)
Epoch: 138 | Batch_idx: 300 |  Loss: (0.1558) |  Loss2: (0.0000) | Acc: (94.00%) (36397/38528)
Epoch: 138 | Batch_idx: 310 |  Loss: (0.1556) |  Loss2: (0.0000) | Acc: (94.00%) (37608/39808)
Epoch: 138 | Batch_idx: 320 |  Loss: (0.1556) |  Loss2: (0.0000) | Acc: (94.00%) (38817/41088)
Epoch: 138 | Batch_idx: 330 |  Loss: (0.1555) |  Loss2: (0.0000) | Acc: (94.00%) (40033/42368)
Epoch: 138 | Batch_idx: 340 |  Loss: (0.1541) |  Loss2: (0.0000) | Acc: (94.00%) (41265/43648)
Epoch: 138 | Batch_idx: 350 |  Loss: (0.1534) |  Loss2: (0.0000) | Acc: (94.00%) (42490/44928)
Epoch: 138 | Batch_idx: 360 |  Loss: (0.1529) |  Loss2: (0.0000) | Acc: (94.00%) (43709/46208)
Epoch: 138 | Batch_idx: 370 |  Loss: (0.1522) |  Loss2: (0.0000) | Acc: (94.00%) (44932/47488)
Epoch: 138 | Batch_idx: 380 |  Loss: (0.1515) |  Loss2: (0.0000) | Acc: (94.00%) (46153/48768)
Epoch: 138 | Batch_idx: 390 |  Loss: (0.1512) |  Loss2: (0.0000) | Acc: (94.00%) (47327/50000)
# TEST : Loss: (0.4014) | Acc: (87.00%) (8799/10000)
percent tensor([0.5430, 0.5614, 0.5445, 0.5183, 0.5502, 0.5578, 0.5646, 0.5355, 0.5445,
        0.5554, 0.5589, 0.5539, 0.5439, 0.5294, 0.5596, 0.5408],
       device='cuda:0') torch.Size([16])
percent tensor([0.5353, 0.5362, 0.5414, 0.5445, 0.5429, 0.5348, 0.5374, 0.5439, 0.5390,
        0.5373, 0.5351, 0.5392, 0.5355, 0.5389, 0.5346, 0.5373],
       device='cuda:0') torch.Size([16])
percent tensor([0.6235, 0.6604, 0.4367, 0.4080, 0.4004, 0.5556, 0.6015, 0.4333, 0.5115,
        0.6339, 0.6437, 0.5261, 0.6512, 0.6167, 0.6082, 0.6115],
       device='cuda:0') torch.Size([16])
percent tensor([0.7238, 0.6350, 0.7048, 0.7087, 0.7048, 0.7770, 0.6780, 0.7197, 0.6739,
        0.6603, 0.6795, 0.6486, 0.6251, 0.6982, 0.7062, 0.7647],
       device='cuda:0') torch.Size([16])
percent tensor([0.6756, 0.6467, 0.6933, 0.6264, 0.6583, 0.6245, 0.6917, 0.6753, 0.6619,
        0.6354, 0.6267, 0.6232, 0.6330, 0.6519, 0.6724, 0.6399],
       device='cuda:0') torch.Size([16])
percent tensor([0.5458, 0.6707, 0.6153, 0.6802, 0.6829, 0.7117, 0.6717, 0.5194, 0.7295,
        0.5718, 0.7322, 0.7169, 0.6735, 0.7487, 0.5876, 0.5353],
       device='cuda:0') torch.Size([16])
percent tensor([0.6665, 0.6730, 0.5559, 0.5247, 0.6441, 0.7073, 0.6157, 0.4506, 0.6484,
        0.6943, 0.6577, 0.4419, 0.6635, 0.6281, 0.4556, 0.6350],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9997, 0.9998, 0.9998, 0.9999, 0.9992, 0.9998, 0.9997, 0.9997,
        0.9996, 0.9999, 0.9999, 0.9998, 0.9996, 0.9996, 0.9992],
       device='cuda:0') torch.Size([16])
Epoch: 139 | Batch_idx: 0 |  Loss: (0.1160) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 139 | Batch_idx: 10 |  Loss: (0.1267) |  Loss2: (0.0000) | Acc: (95.00%) (1351/1408)
Epoch: 139 | Batch_idx: 20 |  Loss: (0.1297) |  Loss2: (0.0000) | Acc: (95.00%) (2575/2688)
Epoch: 139 | Batch_idx: 30 |  Loss: (0.1414) |  Loss2: (0.0000) | Acc: (95.00%) (3776/3968)
Epoch: 139 | Batch_idx: 40 |  Loss: (0.1404) |  Loss2: (0.0000) | Acc: (95.00%) (4996/5248)
Epoch: 139 | Batch_idx: 50 |  Loss: (0.1391) |  Loss2: (0.0000) | Acc: (95.00%) (6225/6528)
Epoch: 139 | Batch_idx: 60 |  Loss: (0.1412) |  Loss2: (0.0000) | Acc: (95.00%) (7433/7808)
Epoch: 139 | Batch_idx: 70 |  Loss: (0.1430) |  Loss2: (0.0000) | Acc: (95.00%) (8646/9088)
Epoch: 139 | Batch_idx: 80 |  Loss: (0.1436) |  Loss2: (0.0000) | Acc: (95.00%) (9858/10368)
Epoch: 139 | Batch_idx: 90 |  Loss: (0.1412) |  Loss2: (0.0000) | Acc: (95.00%) (11085/11648)
Epoch: 139 | Batch_idx: 100 |  Loss: (0.1403) |  Loss2: (0.0000) | Acc: (95.00%) (12302/12928)
Epoch: 139 | Batch_idx: 110 |  Loss: (0.1399) |  Loss2: (0.0000) | Acc: (95.00%) (13519/14208)
Epoch: 139 | Batch_idx: 120 |  Loss: (0.1416) |  Loss2: (0.0000) | Acc: (95.00%) (14728/15488)
Epoch: 139 | Batch_idx: 130 |  Loss: (0.1417) |  Loss2: (0.0000) | Acc: (95.00%) (15939/16768)
Epoch: 139 | Batch_idx: 140 |  Loss: (0.1418) |  Loss2: (0.0000) | Acc: (95.00%) (17148/18048)
Epoch: 139 | Batch_idx: 150 |  Loss: (0.1427) |  Loss2: (0.0000) | Acc: (94.00%) (18358/19328)
Epoch: 139 | Batch_idx: 160 |  Loss: (0.1427) |  Loss2: (0.0000) | Acc: (94.00%) (19572/20608)
Epoch: 139 | Batch_idx: 170 |  Loss: (0.1414) |  Loss2: (0.0000) | Acc: (94.00%) (20785/21888)
Epoch: 139 | Batch_idx: 180 |  Loss: (0.1419) |  Loss2: (0.0000) | Acc: (94.00%) (22009/23168)
Epoch: 139 | Batch_idx: 190 |  Loss: (0.1411) |  Loss2: (0.0000) | Acc: (95.00%) (23232/24448)
Epoch: 139 | Batch_idx: 200 |  Loss: (0.1410) |  Loss2: (0.0000) | Acc: (95.00%) (24447/25728)
Epoch: 139 | Batch_idx: 210 |  Loss: (0.1404) |  Loss2: (0.0000) | Acc: (95.00%) (25670/27008)
Epoch: 139 | Batch_idx: 220 |  Loss: (0.1410) |  Loss2: (0.0000) | Acc: (95.00%) (26883/28288)
Epoch: 139 | Batch_idx: 230 |  Loss: (0.1404) |  Loss2: (0.0000) | Acc: (95.00%) (28109/29568)
Epoch: 139 | Batch_idx: 240 |  Loss: (0.1401) |  Loss2: (0.0000) | Acc: (95.00%) (29332/30848)
Epoch: 139 | Batch_idx: 250 |  Loss: (0.1394) |  Loss2: (0.0000) | Acc: (95.00%) (30555/32128)
Epoch: 139 | Batch_idx: 260 |  Loss: (0.1390) |  Loss2: (0.0000) | Acc: (95.00%) (31779/33408)
Epoch: 139 | Batch_idx: 270 |  Loss: (0.1392) |  Loss2: (0.0000) | Acc: (95.00%) (32993/34688)
Epoch: 139 | Batch_idx: 280 |  Loss: (0.1392) |  Loss2: (0.0000) | Acc: (95.00%) (34215/35968)
Epoch: 139 | Batch_idx: 290 |  Loss: (0.1395) |  Loss2: (0.0000) | Acc: (95.00%) (35426/37248)
Epoch: 139 | Batch_idx: 300 |  Loss: (0.1390) |  Loss2: (0.0000) | Acc: (95.00%) (36651/38528)
Epoch: 139 | Batch_idx: 310 |  Loss: (0.1393) |  Loss2: (0.0000) | Acc: (95.00%) (37870/39808)
Epoch: 139 | Batch_idx: 320 |  Loss: (0.1389) |  Loss2: (0.0000) | Acc: (95.00%) (39100/41088)
Epoch: 139 | Batch_idx: 330 |  Loss: (0.1388) |  Loss2: (0.0000) | Acc: (95.00%) (40317/42368)
Epoch: 139 | Batch_idx: 340 |  Loss: (0.1386) |  Loss2: (0.0000) | Acc: (95.00%) (41548/43648)
Epoch: 139 | Batch_idx: 350 |  Loss: (0.1381) |  Loss2: (0.0000) | Acc: (95.00%) (42782/44928)
Epoch: 139 | Batch_idx: 360 |  Loss: (0.1380) |  Loss2: (0.0000) | Acc: (95.00%) (44006/46208)
Epoch: 139 | Batch_idx: 370 |  Loss: (0.1381) |  Loss2: (0.0000) | Acc: (95.00%) (45225/47488)
Epoch: 139 | Batch_idx: 380 |  Loss: (0.1382) |  Loss2: (0.0000) | Acc: (95.00%) (46439/48768)
Epoch: 139 | Batch_idx: 390 |  Loss: (0.1380) |  Loss2: (0.0000) | Acc: (95.00%) (47613/50000)
# TEST : Loss: (0.3897) | Acc: (88.00%) (8837/10000)
percent tensor([0.5443, 0.5633, 0.5472, 0.5197, 0.5531, 0.5581, 0.5669, 0.5380, 0.5467,
        0.5577, 0.5606, 0.5569, 0.5457, 0.5309, 0.5609, 0.5420],
       device='cuda:0') torch.Size([16])
percent tensor([0.5336, 0.5346, 0.5395, 0.5426, 0.5407, 0.5332, 0.5356, 0.5418, 0.5372,
        0.5357, 0.5336, 0.5374, 0.5339, 0.5372, 0.5329, 0.5357],
       device='cuda:0') torch.Size([16])
percent tensor([0.6248, 0.6630, 0.4440, 0.4155, 0.4056, 0.5528, 0.6053, 0.4398, 0.5231,
        0.6396, 0.6494, 0.5392, 0.6567, 0.6235, 0.6081, 0.6132],
       device='cuda:0') torch.Size([16])
percent tensor([0.7203, 0.6357, 0.6984, 0.6990, 0.6987, 0.7724, 0.6754, 0.7114, 0.6709,
        0.6603, 0.6785, 0.6459, 0.6247, 0.6967, 0.7022, 0.7620],
       device='cuda:0') torch.Size([16])
percent tensor([0.6760, 0.6438, 0.6925, 0.6279, 0.6574, 0.6291, 0.6924, 0.6737, 0.6609,
        0.6333, 0.6280, 0.6186, 0.6309, 0.6506, 0.6747, 0.6372],
       device='cuda:0') torch.Size([16])
percent tensor([0.5560, 0.6819, 0.6322, 0.6945, 0.6923, 0.7154, 0.6826, 0.5340, 0.7410,
        0.5862, 0.7426, 0.7358, 0.6835, 0.7615, 0.5981, 0.5425],
       device='cuda:0') torch.Size([16])
percent tensor([0.6595, 0.6714, 0.5699, 0.5374, 0.6521, 0.7091, 0.6159, 0.4658, 0.6516,
        0.6906, 0.6630, 0.4536, 0.6615, 0.6279, 0.4640, 0.6242],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9997, 0.9998, 0.9998, 0.9999, 0.9992, 0.9999, 0.9997, 0.9998,
        0.9996, 0.9999, 0.9999, 0.9998, 0.9996, 0.9996, 0.9992],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 140 | Batch_idx: 0 |  Loss: (0.1496) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 140 | Batch_idx: 10 |  Loss: (0.1502) |  Loss2: (0.0000) | Acc: (94.00%) (1333/1408)
Epoch: 140 | Batch_idx: 20 |  Loss: (0.1415) |  Loss2: (0.0000) | Acc: (94.00%) (2550/2688)
Epoch: 140 | Batch_idx: 30 |  Loss: (0.1404) |  Loss2: (0.0000) | Acc: (94.00%) (3766/3968)
Epoch: 140 | Batch_idx: 40 |  Loss: (0.1393) |  Loss2: (0.0000) | Acc: (95.00%) (4990/5248)
Epoch: 140 | Batch_idx: 50 |  Loss: (0.1371) |  Loss2: (0.0000) | Acc: (95.00%) (6213/6528)
Epoch: 140 | Batch_idx: 60 |  Loss: (0.1362) |  Loss2: (0.0000) | Acc: (95.00%) (7439/7808)
Epoch: 140 | Batch_idx: 70 |  Loss: (0.1341) |  Loss2: (0.0000) | Acc: (95.00%) (8670/9088)
Epoch: 140 | Batch_idx: 80 |  Loss: (0.1342) |  Loss2: (0.0000) | Acc: (95.00%) (9891/10368)
Epoch: 140 | Batch_idx: 90 |  Loss: (0.1324) |  Loss2: (0.0000) | Acc: (95.00%) (11125/11648)
Epoch: 140 | Batch_idx: 100 |  Loss: (0.1319) |  Loss2: (0.0000) | Acc: (95.00%) (12355/12928)
Epoch: 140 | Batch_idx: 110 |  Loss: (0.1319) |  Loss2: (0.0000) | Acc: (95.00%) (13584/14208)
Epoch: 140 | Batch_idx: 120 |  Loss: (0.1335) |  Loss2: (0.0000) | Acc: (95.00%) (14804/15488)
Epoch: 140 | Batch_idx: 130 |  Loss: (0.1331) |  Loss2: (0.0000) | Acc: (95.00%) (16030/16768)
Epoch: 140 | Batch_idx: 140 |  Loss: (0.1318) |  Loss2: (0.0000) | Acc: (95.00%) (17258/18048)
Epoch: 140 | Batch_idx: 150 |  Loss: (0.1335) |  Loss2: (0.0000) | Acc: (95.00%) (18476/19328)
Epoch: 140 | Batch_idx: 160 |  Loss: (0.1343) |  Loss2: (0.0000) | Acc: (95.00%) (19687/20608)
Epoch: 140 | Batch_idx: 170 |  Loss: (0.1347) |  Loss2: (0.0000) | Acc: (95.00%) (20906/21888)
Epoch: 140 | Batch_idx: 180 |  Loss: (0.1346) |  Loss2: (0.0000) | Acc: (95.00%) (22134/23168)
Epoch: 140 | Batch_idx: 190 |  Loss: (0.1344) |  Loss2: (0.0000) | Acc: (95.00%) (23355/24448)
Epoch: 140 | Batch_idx: 200 |  Loss: (0.1338) |  Loss2: (0.0000) | Acc: (95.00%) (24587/25728)
Epoch: 140 | Batch_idx: 210 |  Loss: (0.1342) |  Loss2: (0.0000) | Acc: (95.00%) (25806/27008)
Epoch: 140 | Batch_idx: 220 |  Loss: (0.1349) |  Loss2: (0.0000) | Acc: (95.00%) (27020/28288)
Epoch: 140 | Batch_idx: 230 |  Loss: (0.1346) |  Loss2: (0.0000) | Acc: (95.00%) (28242/29568)
Epoch: 140 | Batch_idx: 240 |  Loss: (0.1359) |  Loss2: (0.0000) | Acc: (95.00%) (29455/30848)
Epoch: 140 | Batch_idx: 250 |  Loss: (0.1366) |  Loss2: (0.0000) | Acc: (95.00%) (30669/32128)
Epoch: 140 | Batch_idx: 260 |  Loss: (0.1377) |  Loss2: (0.0000) | Acc: (95.00%) (31875/33408)
Epoch: 140 | Batch_idx: 270 |  Loss: (0.1391) |  Loss2: (0.0000) | Acc: (95.00%) (33083/34688)
Epoch: 140 | Batch_idx: 280 |  Loss: (0.1403) |  Loss2: (0.0000) | Acc: (95.00%) (34285/35968)
Epoch: 140 | Batch_idx: 290 |  Loss: (0.1406) |  Loss2: (0.0000) | Acc: (95.00%) (35501/37248)
Epoch: 140 | Batch_idx: 300 |  Loss: (0.1412) |  Loss2: (0.0000) | Acc: (95.00%) (36711/38528)
Epoch: 140 | Batch_idx: 310 |  Loss: (0.1419) |  Loss2: (0.0000) | Acc: (95.00%) (37915/39808)
Epoch: 140 | Batch_idx: 320 |  Loss: (0.1418) |  Loss2: (0.0000) | Acc: (95.00%) (39135/41088)
Epoch: 140 | Batch_idx: 330 |  Loss: (0.1415) |  Loss2: (0.0000) | Acc: (95.00%) (40354/42368)
Epoch: 140 | Batch_idx: 340 |  Loss: (0.1410) |  Loss2: (0.0000) | Acc: (95.00%) (41575/43648)
Epoch: 140 | Batch_idx: 350 |  Loss: (0.1409) |  Loss2: (0.0000) | Acc: (95.00%) (42795/44928)
Epoch: 140 | Batch_idx: 360 |  Loss: (0.1405) |  Loss2: (0.0000) | Acc: (95.00%) (44015/46208)
Epoch: 140 | Batch_idx: 370 |  Loss: (0.1400) |  Loss2: (0.0000) | Acc: (95.00%) (45243/47488)
Epoch: 140 | Batch_idx: 380 |  Loss: (0.1406) |  Loss2: (0.0000) | Acc: (95.00%) (46452/48768)
Epoch: 140 | Batch_idx: 390 |  Loss: (0.1410) |  Loss2: (0.0000) | Acc: (95.00%) (47613/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_140.pth.tar'
# TEST : Loss: (0.4512) | Acc: (86.00%) (8683/10000)
percent tensor([0.5463, 0.5611, 0.5528, 0.5220, 0.5615, 0.5610, 0.5696, 0.5403, 0.5501,
        0.5589, 0.5611, 0.5635, 0.5482, 0.5262, 0.5620, 0.5421],
       device='cuda:0') torch.Size([16])
percent tensor([0.5331, 0.5343, 0.5393, 0.5423, 0.5398, 0.5323, 0.5357, 0.5414, 0.5363,
        0.5359, 0.5332, 0.5372, 0.5334, 0.5373, 0.5321, 0.5357],
       device='cuda:0') torch.Size([16])
percent tensor([0.6290, 0.6652, 0.4256, 0.4000, 0.3924, 0.5494, 0.6000, 0.4586, 0.5473,
        0.6289, 0.6526, 0.5178, 0.6636, 0.6422, 0.6071, 0.6110],
       device='cuda:0') torch.Size([16])
percent tensor([0.7249, 0.6228, 0.7053, 0.7064, 0.7077, 0.7672, 0.6777, 0.7161, 0.6706,
        0.6562, 0.6771, 0.6556, 0.6221, 0.6931, 0.6998, 0.7635],
       device='cuda:0') torch.Size([16])
percent tensor([0.6824, 0.6718, 0.6755, 0.6187, 0.6495, 0.6314, 0.6940, 0.6694, 0.6867,
        0.6519, 0.6451, 0.6299, 0.6539, 0.6651, 0.6855, 0.6408],
       device='cuda:0') torch.Size([16])
percent tensor([0.5191, 0.6647, 0.6431, 0.6899, 0.6724, 0.7176, 0.6666, 0.5594, 0.7120,
        0.5716, 0.7083, 0.7367, 0.6661, 0.7448, 0.5725, 0.5229],
       device='cuda:0') torch.Size([16])
percent tensor([0.6563, 0.6499, 0.5813, 0.5354, 0.6470, 0.7395, 0.6038, 0.4530, 0.6364,
        0.6691, 0.6591, 0.4902, 0.6561, 0.6311, 0.4692, 0.6243],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9998, 0.9998, 0.9997, 0.9999, 0.9993, 0.9998, 0.9996, 0.9998,
        0.9997, 0.9998, 0.9999, 0.9999, 0.9995, 0.9993, 0.9994],
       device='cuda:0') torch.Size([16])
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(182.6852, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(824.4892, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(819.4181, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1514.7446, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(486.1011, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2272.8257, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4252.1089, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1378.3728, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6212.7095, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11736.3428, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3878.5579, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16375.2539, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 141 | Batch_idx: 0 |  Loss: (0.1496) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 141 | Batch_idx: 10 |  Loss: (0.1296) |  Loss2: (0.0000) | Acc: (95.00%) (1347/1408)
Epoch: 141 | Batch_idx: 20 |  Loss: (0.1191) |  Loss2: (0.0000) | Acc: (96.00%) (2588/2688)
Epoch: 141 | Batch_idx: 30 |  Loss: (0.1291) |  Loss2: (0.0000) | Acc: (95.00%) (3792/3968)
Epoch: 141 | Batch_idx: 40 |  Loss: (0.1372) |  Loss2: (0.0000) | Acc: (95.00%) (4993/5248)
Epoch: 141 | Batch_idx: 50 |  Loss: (0.1331) |  Loss2: (0.0000) | Acc: (95.00%) (6220/6528)
Epoch: 141 | Batch_idx: 60 |  Loss: (0.1378) |  Loss2: (0.0000) | Acc: (95.00%) (7425/7808)
Epoch: 141 | Batch_idx: 70 |  Loss: (0.1375) |  Loss2: (0.0000) | Acc: (95.00%) (8647/9088)
Epoch: 141 | Batch_idx: 80 |  Loss: (0.1371) |  Loss2: (0.0000) | Acc: (95.00%) (9857/10368)
Epoch: 141 | Batch_idx: 90 |  Loss: (0.1383) |  Loss2: (0.0000) | Acc: (95.00%) (11070/11648)
Epoch: 141 | Batch_idx: 100 |  Loss: (0.1374) |  Loss2: (0.0000) | Acc: (95.00%) (12294/12928)
Epoch: 141 | Batch_idx: 110 |  Loss: (0.1362) |  Loss2: (0.0000) | Acc: (95.00%) (13522/14208)
Epoch: 141 | Batch_idx: 120 |  Loss: (0.1349) |  Loss2: (0.0000) | Acc: (95.00%) (14749/15488)
Epoch: 141 | Batch_idx: 130 |  Loss: (0.1340) |  Loss2: (0.0000) | Acc: (95.00%) (15975/16768)
Epoch: 141 | Batch_idx: 140 |  Loss: (0.1336) |  Loss2: (0.0000) | Acc: (95.00%) (17196/18048)
Epoch: 141 | Batch_idx: 150 |  Loss: (0.1342) |  Loss2: (0.0000) | Acc: (95.00%) (18410/19328)
Epoch: 141 | Batch_idx: 160 |  Loss: (0.1330) |  Loss2: (0.0000) | Acc: (95.00%) (19646/20608)
Epoch: 141 | Batch_idx: 170 |  Loss: (0.1334) |  Loss2: (0.0000) | Acc: (95.00%) (20853/21888)
Epoch: 141 | Batch_idx: 180 |  Loss: (0.1326) |  Loss2: (0.0000) | Acc: (95.00%) (22084/23168)
Epoch: 141 | Batch_idx: 190 |  Loss: (0.1329) |  Loss2: (0.0000) | Acc: (95.00%) (23313/24448)
Epoch: 141 | Batch_idx: 200 |  Loss: (0.1322) |  Loss2: (0.0000) | Acc: (95.00%) (24542/25728)
Epoch: 141 | Batch_idx: 210 |  Loss: (0.1330) |  Loss2: (0.0000) | Acc: (95.00%) (25753/27008)
Epoch: 141 | Batch_idx: 220 |  Loss: (0.1335) |  Loss2: (0.0000) | Acc: (95.00%) (26968/28288)
Epoch: 141 | Batch_idx: 230 |  Loss: (0.1338) |  Loss2: (0.0000) | Acc: (95.00%) (28187/29568)
Epoch: 141 | Batch_idx: 240 |  Loss: (0.1343) |  Loss2: (0.0000) | Acc: (95.00%) (29402/30848)
Epoch: 141 | Batch_idx: 250 |  Loss: (0.1341) |  Loss2: (0.0000) | Acc: (95.00%) (30623/32128)
Epoch: 141 | Batch_idx: 260 |  Loss: (0.1342) |  Loss2: (0.0000) | Acc: (95.00%) (31839/33408)
Epoch: 141 | Batch_idx: 270 |  Loss: (0.1353) |  Loss2: (0.0000) | Acc: (95.00%) (33046/34688)
Epoch: 141 | Batch_idx: 280 |  Loss: (0.1351) |  Loss2: (0.0000) | Acc: (95.00%) (34272/35968)
Epoch: 141 | Batch_idx: 290 |  Loss: (0.1349) |  Loss2: (0.0000) | Acc: (95.00%) (35497/37248)
Epoch: 141 | Batch_idx: 300 |  Loss: (0.1350) |  Loss2: (0.0000) | Acc: (95.00%) (36710/38528)
Epoch: 141 | Batch_idx: 310 |  Loss: (0.1356) |  Loss2: (0.0000) | Acc: (95.00%) (37927/39808)
Epoch: 141 | Batch_idx: 320 |  Loss: (0.1358) |  Loss2: (0.0000) | Acc: (95.00%) (39144/41088)
Epoch: 141 | Batch_idx: 330 |  Loss: (0.1359) |  Loss2: (0.0000) | Acc: (95.00%) (40362/42368)
Epoch: 141 | Batch_idx: 340 |  Loss: (0.1360) |  Loss2: (0.0000) | Acc: (95.00%) (41585/43648)
Epoch: 141 | Batch_idx: 350 |  Loss: (0.1364) |  Loss2: (0.0000) | Acc: (95.00%) (42799/44928)
Epoch: 141 | Batch_idx: 360 |  Loss: (0.1367) |  Loss2: (0.0000) | Acc: (95.00%) (44009/46208)
Epoch: 141 | Batch_idx: 370 |  Loss: (0.1370) |  Loss2: (0.0000) | Acc: (95.00%) (45213/47488)
Epoch: 141 | Batch_idx: 380 |  Loss: (0.1377) |  Loss2: (0.0000) | Acc: (95.00%) (46415/48768)
Epoch: 141 | Batch_idx: 390 |  Loss: (0.1377) |  Loss2: (0.0000) | Acc: (95.00%) (47589/50000)
# TEST : Loss: (0.4024) | Acc: (87.00%) (8792/10000)
percent tensor([0.5450, 0.5624, 0.5493, 0.5224, 0.5581, 0.5612, 0.5692, 0.5377, 0.5473,
        0.5586, 0.5600, 0.5619, 0.5469, 0.5302, 0.5617, 0.5421],
       device='cuda:0') torch.Size([16])
percent tensor([0.5330, 0.5346, 0.5387, 0.5417, 0.5394, 0.5319, 0.5353, 0.5414, 0.5365,
        0.5356, 0.5336, 0.5368, 0.5333, 0.5365, 0.5325, 0.5353],
       device='cuda:0') torch.Size([16])
percent tensor([0.6215, 0.6572, 0.4527, 0.4191, 0.4063, 0.5648, 0.5918, 0.4590, 0.5243,
        0.6182, 0.6342, 0.5319, 0.6571, 0.6100, 0.6094, 0.6033],
       device='cuda:0') torch.Size([16])
percent tensor([0.7155, 0.6236, 0.6939, 0.6960, 0.7024, 0.7707, 0.6733, 0.7019, 0.6684,
        0.6535, 0.6729, 0.6506, 0.6101, 0.6955, 0.7032, 0.7584],
       device='cuda:0') torch.Size([16])
percent tensor([0.6718, 0.6580, 0.6717, 0.6242, 0.6407, 0.6232, 0.6831, 0.6644, 0.6696,
        0.6350, 0.6238, 0.6171, 0.6464, 0.6500, 0.6686, 0.6367],
       device='cuda:0') torch.Size([16])
percent tensor([0.5151, 0.6515, 0.6704, 0.6847, 0.6957, 0.7268, 0.6785, 0.5365, 0.7254,
        0.6147, 0.7027, 0.7666, 0.6607, 0.7248, 0.5592, 0.5259],
       device='cuda:0') torch.Size([16])
percent tensor([0.6593, 0.6612, 0.5767, 0.5061, 0.6393, 0.7348, 0.6306, 0.4476, 0.6687,
        0.6834, 0.6713, 0.4740, 0.6692, 0.6268, 0.4823, 0.6202],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9996, 0.9999, 0.9997, 0.9998, 0.9994, 0.9998, 0.9996, 0.9998,
        0.9996, 0.9998, 0.9999, 0.9998, 0.9996, 0.9993, 0.9993],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 142 | Batch_idx: 0 |  Loss: (0.1369) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 142 | Batch_idx: 10 |  Loss: (0.1264) |  Loss2: (0.0000) | Acc: (95.00%) (1350/1408)
Epoch: 142 | Batch_idx: 20 |  Loss: (0.1440) |  Loss2: (0.0000) | Acc: (95.00%) (2560/2688)
Epoch: 142 | Batch_idx: 30 |  Loss: (0.1462) |  Loss2: (0.0000) | Acc: (94.00%) (3763/3968)
Epoch: 142 | Batch_idx: 40 |  Loss: (0.1476) |  Loss2: (0.0000) | Acc: (94.00%) (4984/5248)
Epoch: 142 | Batch_idx: 50 |  Loss: (0.1565) |  Loss2: (0.0000) | Acc: (94.00%) (6182/6528)
Epoch: 142 | Batch_idx: 60 |  Loss: (0.1575) |  Loss2: (0.0000) | Acc: (94.00%) (7395/7808)
Epoch: 142 | Batch_idx: 70 |  Loss: (0.1549) |  Loss2: (0.0000) | Acc: (94.00%) (8618/9088)
Epoch: 142 | Batch_idx: 80 |  Loss: (0.1561) |  Loss2: (0.0000) | Acc: (94.00%) (9823/10368)
Epoch: 142 | Batch_idx: 90 |  Loss: (0.1575) |  Loss2: (0.0000) | Acc: (94.00%) (11027/11648)
Epoch: 142 | Batch_idx: 100 |  Loss: (0.1570) |  Loss2: (0.0000) | Acc: (94.00%) (12235/12928)
Epoch: 142 | Batch_idx: 110 |  Loss: (0.1565) |  Loss2: (0.0000) | Acc: (94.00%) (13446/14208)
Epoch: 142 | Batch_idx: 120 |  Loss: (0.1554) |  Loss2: (0.0000) | Acc: (94.00%) (14662/15488)
Epoch: 142 | Batch_idx: 130 |  Loss: (0.1561) |  Loss2: (0.0000) | Acc: (94.00%) (15864/16768)
Epoch: 142 | Batch_idx: 140 |  Loss: (0.1585) |  Loss2: (0.0000) | Acc: (94.00%) (17058/18048)
Epoch: 142 | Batch_idx: 150 |  Loss: (0.1574) |  Loss2: (0.0000) | Acc: (94.00%) (18276/19328)
Epoch: 142 | Batch_idx: 160 |  Loss: (0.1589) |  Loss2: (0.0000) | Acc: (94.00%) (19468/20608)
Epoch: 142 | Batch_idx: 170 |  Loss: (0.1580) |  Loss2: (0.0000) | Acc: (94.00%) (20685/21888)
Epoch: 142 | Batch_idx: 180 |  Loss: (0.1569) |  Loss2: (0.0000) | Acc: (94.00%) (21900/23168)
Epoch: 142 | Batch_idx: 190 |  Loss: (0.1565) |  Loss2: (0.0000) | Acc: (94.00%) (23110/24448)
Epoch: 142 | Batch_idx: 200 |  Loss: (0.1561) |  Loss2: (0.0000) | Acc: (94.00%) (24330/25728)
Epoch: 142 | Batch_idx: 210 |  Loss: (0.1570) |  Loss2: (0.0000) | Acc: (94.00%) (25528/27008)
Epoch: 142 | Batch_idx: 220 |  Loss: (0.1571) |  Loss2: (0.0000) | Acc: (94.00%) (26740/28288)
Epoch: 142 | Batch_idx: 230 |  Loss: (0.1566) |  Loss2: (0.0000) | Acc: (94.00%) (27952/29568)
Epoch: 142 | Batch_idx: 240 |  Loss: (0.1568) |  Loss2: (0.0000) | Acc: (94.00%) (29165/30848)
Epoch: 142 | Batch_idx: 250 |  Loss: (0.1555) |  Loss2: (0.0000) | Acc: (94.00%) (30387/32128)
Epoch: 142 | Batch_idx: 260 |  Loss: (0.1547) |  Loss2: (0.0000) | Acc: (94.00%) (31607/33408)
Epoch: 142 | Batch_idx: 270 |  Loss: (0.1538) |  Loss2: (0.0000) | Acc: (94.00%) (32831/34688)
Epoch: 142 | Batch_idx: 280 |  Loss: (0.1533) |  Loss2: (0.0000) | Acc: (94.00%) (34056/35968)
Epoch: 142 | Batch_idx: 290 |  Loss: (0.1531) |  Loss2: (0.0000) | Acc: (94.00%) (35279/37248)
Epoch: 142 | Batch_idx: 300 |  Loss: (0.1526) |  Loss2: (0.0000) | Acc: (94.00%) (36498/38528)
Epoch: 142 | Batch_idx: 310 |  Loss: (0.1522) |  Loss2: (0.0000) | Acc: (94.00%) (37718/39808)
Epoch: 142 | Batch_idx: 320 |  Loss: (0.1520) |  Loss2: (0.0000) | Acc: (94.00%) (38931/41088)
Epoch: 142 | Batch_idx: 330 |  Loss: (0.1511) |  Loss2: (0.0000) | Acc: (94.00%) (40157/42368)
Epoch: 142 | Batch_idx: 340 |  Loss: (0.1506) |  Loss2: (0.0000) | Acc: (94.00%) (41374/43648)
Epoch: 142 | Batch_idx: 350 |  Loss: (0.1505) |  Loss2: (0.0000) | Acc: (94.00%) (42592/44928)
Epoch: 142 | Batch_idx: 360 |  Loss: (0.1510) |  Loss2: (0.0000) | Acc: (94.00%) (43797/46208)
Epoch: 142 | Batch_idx: 370 |  Loss: (0.1508) |  Loss2: (0.0000) | Acc: (94.00%) (45014/47488)
Epoch: 142 | Batch_idx: 380 |  Loss: (0.1505) |  Loss2: (0.0000) | Acc: (94.00%) (46226/48768)
Epoch: 142 | Batch_idx: 390 |  Loss: (0.1502) |  Loss2: (0.0000) | Acc: (94.00%) (47396/50000)
# TEST : Loss: (0.3971) | Acc: (88.00%) (8820/10000)
percent tensor([0.5388, 0.5544, 0.5418, 0.5163, 0.5499, 0.5553, 0.5609, 0.5286, 0.5406,
        0.5504, 0.5535, 0.5530, 0.5401, 0.5237, 0.5543, 0.5361],
       device='cuda:0') torch.Size([16])
percent tensor([0.5340, 0.5357, 0.5389, 0.5427, 0.5401, 0.5333, 0.5362, 0.5424, 0.5375,
        0.5366, 0.5348, 0.5375, 0.5345, 0.5380, 0.5336, 0.5363],
       device='cuda:0') torch.Size([16])
percent tensor([0.6313, 0.6624, 0.4830, 0.4316, 0.4273, 0.5692, 0.6069, 0.4841, 0.5427,
        0.6330, 0.6433, 0.5529, 0.6657, 0.6273, 0.6154, 0.6148],
       device='cuda:0') torch.Size([16])
percent tensor([0.7347, 0.6453, 0.7154, 0.7114, 0.7214, 0.7779, 0.6939, 0.7184, 0.6897,
        0.6741, 0.6942, 0.6692, 0.6372, 0.7151, 0.7220, 0.7760],
       device='cuda:0') torch.Size([16])
percent tensor([0.6512, 0.6397, 0.6583, 0.5997, 0.6291, 0.6006, 0.6596, 0.6475, 0.6427,
        0.6170, 0.5972, 0.6026, 0.6297, 0.6188, 0.6451, 0.6132],
       device='cuda:0') torch.Size([16])
percent tensor([0.5337, 0.6538, 0.6920, 0.6991, 0.7157, 0.7398, 0.6852, 0.5600, 0.7383,
        0.6094, 0.6957, 0.7663, 0.6620, 0.7379, 0.5611, 0.5481],
       device='cuda:0') torch.Size([16])
percent tensor([0.6457, 0.6534, 0.5779, 0.4914, 0.6260, 0.7287, 0.6186, 0.4285, 0.6871,
        0.6644, 0.6760, 0.4781, 0.6681, 0.6329, 0.4448, 0.5827],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9997, 0.9999, 0.9998, 0.9998, 0.9992, 0.9998, 0.9997, 0.9999,
        0.9997, 0.9999, 0.9999, 0.9999, 0.9997, 0.9993, 0.9993],
       device='cuda:0') torch.Size([16])
Epoch: 143 | Batch_idx: 0 |  Loss: (0.1029) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 143 | Batch_idx: 10 |  Loss: (0.1437) |  Loss2: (0.0000) | Acc: (95.00%) (1339/1408)
Epoch: 143 | Batch_idx: 20 |  Loss: (0.1477) |  Loss2: (0.0000) | Acc: (95.00%) (2554/2688)
Epoch: 143 | Batch_idx: 30 |  Loss: (0.1518) |  Loss2: (0.0000) | Acc: (94.00%) (3762/3968)
Epoch: 143 | Batch_idx: 40 |  Loss: (0.1493) |  Loss2: (0.0000) | Acc: (94.00%) (4984/5248)
Epoch: 143 | Batch_idx: 50 |  Loss: (0.1493) |  Loss2: (0.0000) | Acc: (94.00%) (6190/6528)
Epoch: 143 | Batch_idx: 60 |  Loss: (0.1515) |  Loss2: (0.0000) | Acc: (94.00%) (7401/7808)
Epoch: 143 | Batch_idx: 70 |  Loss: (0.1505) |  Loss2: (0.0000) | Acc: (94.00%) (8621/9088)
Epoch: 143 | Batch_idx: 80 |  Loss: (0.1508) |  Loss2: (0.0000) | Acc: (94.00%) (9836/10368)
Epoch: 143 | Batch_idx: 90 |  Loss: (0.1500) |  Loss2: (0.0000) | Acc: (94.00%) (11050/11648)
Epoch: 143 | Batch_idx: 100 |  Loss: (0.1507) |  Loss2: (0.0000) | Acc: (94.00%) (12260/12928)
Epoch: 143 | Batch_idx: 110 |  Loss: (0.1498) |  Loss2: (0.0000) | Acc: (94.00%) (13476/14208)
Epoch: 143 | Batch_idx: 120 |  Loss: (0.1501) |  Loss2: (0.0000) | Acc: (94.00%) (14684/15488)
Epoch: 143 | Batch_idx: 130 |  Loss: (0.1487) |  Loss2: (0.0000) | Acc: (94.00%) (15902/16768)
Epoch: 143 | Batch_idx: 140 |  Loss: (0.1484) |  Loss2: (0.0000) | Acc: (94.00%) (17119/18048)
Epoch: 143 | Batch_idx: 150 |  Loss: (0.1483) |  Loss2: (0.0000) | Acc: (94.00%) (18336/19328)
Epoch: 143 | Batch_idx: 160 |  Loss: (0.1472) |  Loss2: (0.0000) | Acc: (94.00%) (19562/20608)
Epoch: 143 | Batch_idx: 170 |  Loss: (0.1461) |  Loss2: (0.0000) | Acc: (94.00%) (20784/21888)
Epoch: 143 | Batch_idx: 180 |  Loss: (0.1447) |  Loss2: (0.0000) | Acc: (94.00%) (22009/23168)
Epoch: 143 | Batch_idx: 190 |  Loss: (0.1447) |  Loss2: (0.0000) | Acc: (94.00%) (23219/24448)
Epoch: 143 | Batch_idx: 200 |  Loss: (0.1442) |  Loss2: (0.0000) | Acc: (94.00%) (24438/25728)
Epoch: 143 | Batch_idx: 210 |  Loss: (0.1432) |  Loss2: (0.0000) | Acc: (95.00%) (25669/27008)
Epoch: 143 | Batch_idx: 220 |  Loss: (0.1437) |  Loss2: (0.0000) | Acc: (95.00%) (26880/28288)
Epoch: 143 | Batch_idx: 230 |  Loss: (0.1432) |  Loss2: (0.0000) | Acc: (95.00%) (28096/29568)
Epoch: 143 | Batch_idx: 240 |  Loss: (0.1426) |  Loss2: (0.0000) | Acc: (95.00%) (29323/30848)
Epoch: 143 | Batch_idx: 250 |  Loss: (0.1430) |  Loss2: (0.0000) | Acc: (95.00%) (30535/32128)
Epoch: 143 | Batch_idx: 260 |  Loss: (0.1434) |  Loss2: (0.0000) | Acc: (95.00%) (31743/33408)
Epoch: 143 | Batch_idx: 270 |  Loss: (0.1429) |  Loss2: (0.0000) | Acc: (95.00%) (32963/34688)
Epoch: 143 | Batch_idx: 280 |  Loss: (0.1425) |  Loss2: (0.0000) | Acc: (95.00%) (34183/35968)
Epoch: 143 | Batch_idx: 290 |  Loss: (0.1425) |  Loss2: (0.0000) | Acc: (95.00%) (35405/37248)
Epoch: 143 | Batch_idx: 300 |  Loss: (0.1418) |  Loss2: (0.0000) | Acc: (95.00%) (36628/38528)
Epoch: 143 | Batch_idx: 310 |  Loss: (0.1416) |  Loss2: (0.0000) | Acc: (95.00%) (37850/39808)
Epoch: 143 | Batch_idx: 320 |  Loss: (0.1406) |  Loss2: (0.0000) | Acc: (95.00%) (39082/41088)
Epoch: 143 | Batch_idx: 330 |  Loss: (0.1406) |  Loss2: (0.0000) | Acc: (95.00%) (40303/42368)
Epoch: 143 | Batch_idx: 340 |  Loss: (0.1402) |  Loss2: (0.0000) | Acc: (95.00%) (41526/43648)
Epoch: 143 | Batch_idx: 350 |  Loss: (0.1398) |  Loss2: (0.0000) | Acc: (95.00%) (42750/44928)
Epoch: 143 | Batch_idx: 360 |  Loss: (0.1403) |  Loss2: (0.0000) | Acc: (95.00%) (43962/46208)
Epoch: 143 | Batch_idx: 370 |  Loss: (0.1404) |  Loss2: (0.0000) | Acc: (95.00%) (45187/47488)
Epoch: 143 | Batch_idx: 380 |  Loss: (0.1406) |  Loss2: (0.0000) | Acc: (95.00%) (46399/48768)
Epoch: 143 | Batch_idx: 390 |  Loss: (0.1404) |  Loss2: (0.0000) | Acc: (95.00%) (47578/50000)
# TEST : Loss: (0.3862) | Acc: (88.00%) (8843/10000)
percent tensor([0.5404, 0.5552, 0.5425, 0.5169, 0.5510, 0.5577, 0.5619, 0.5285, 0.5417,
        0.5509, 0.5550, 0.5533, 0.5412, 0.5239, 0.5558, 0.5375],
       device='cuda:0') torch.Size([16])
percent tensor([0.5363, 0.5382, 0.5412, 0.5451, 0.5427, 0.5354, 0.5387, 0.5452, 0.5402,
        0.5390, 0.5372, 0.5400, 0.5368, 0.5406, 0.5358, 0.5386],
       device='cuda:0') torch.Size([16])
percent tensor([0.6316, 0.6650, 0.4806, 0.4329, 0.4244, 0.5701, 0.6068, 0.4810, 0.5425,
        0.6345, 0.6465, 0.5520, 0.6675, 0.6342, 0.6156, 0.6168],
       device='cuda:0') torch.Size([16])
percent tensor([0.7275, 0.6391, 0.7078, 0.7042, 0.7148, 0.7730, 0.6873, 0.7126, 0.6841,
        0.6680, 0.6871, 0.6622, 0.6303, 0.7104, 0.7140, 0.7705],
       device='cuda:0') torch.Size([16])
percent tensor([0.6551, 0.6443, 0.6642, 0.5989, 0.6349, 0.5994, 0.6663, 0.6516, 0.6476,
        0.6261, 0.6015, 0.6060, 0.6356, 0.6192, 0.6496, 0.6167],
       device='cuda:0') torch.Size([16])
percent tensor([0.5418, 0.6576, 0.7023, 0.7042, 0.7257, 0.7461, 0.6917, 0.5690, 0.7432,
        0.6117, 0.6988, 0.7690, 0.6668, 0.7379, 0.5691, 0.5582],
       device='cuda:0') torch.Size([16])
percent tensor([0.6544, 0.6645, 0.5969, 0.5098, 0.6399, 0.7441, 0.6268, 0.4304, 0.7017,
        0.6691, 0.6920, 0.4921, 0.6807, 0.6489, 0.4490, 0.5793],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9997, 0.9999, 0.9998, 0.9999, 0.9992, 0.9999, 0.9997, 0.9999,
        0.9997, 0.9999, 0.9999, 0.9999, 0.9997, 0.9993, 0.9994],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 144 | Batch_idx: 0 |  Loss: (0.1310) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 144 | Batch_idx: 10 |  Loss: (0.1300) |  Loss2: (0.0000) | Acc: (95.00%) (1338/1408)
Epoch: 144 | Batch_idx: 20 |  Loss: (0.1362) |  Loss2: (0.0000) | Acc: (95.00%) (2555/2688)
Epoch: 144 | Batch_idx: 30 |  Loss: (0.1397) |  Loss2: (0.0000) | Acc: (94.00%) (3769/3968)
Epoch: 144 | Batch_idx: 40 |  Loss: (0.1408) |  Loss2: (0.0000) | Acc: (95.00%) (4988/5248)
Epoch: 144 | Batch_idx: 50 |  Loss: (0.1365) |  Loss2: (0.0000) | Acc: (95.00%) (6215/6528)
Epoch: 144 | Batch_idx: 60 |  Loss: (0.1367) |  Loss2: (0.0000) | Acc: (95.00%) (7436/7808)
Epoch: 144 | Batch_idx: 70 |  Loss: (0.1367) |  Loss2: (0.0000) | Acc: (95.00%) (8660/9088)
Epoch: 144 | Batch_idx: 80 |  Loss: (0.1347) |  Loss2: (0.0000) | Acc: (95.00%) (9884/10368)
Epoch: 144 | Batch_idx: 90 |  Loss: (0.1361) |  Loss2: (0.0000) | Acc: (95.00%) (11100/11648)
Epoch: 144 | Batch_idx: 100 |  Loss: (0.1358) |  Loss2: (0.0000) | Acc: (95.00%) (12319/12928)
Epoch: 144 | Batch_idx: 110 |  Loss: (0.1367) |  Loss2: (0.0000) | Acc: (95.00%) (13533/14208)
Epoch: 144 | Batch_idx: 120 |  Loss: (0.1368) |  Loss2: (0.0000) | Acc: (95.00%) (14750/15488)
Epoch: 144 | Batch_idx: 130 |  Loss: (0.1380) |  Loss2: (0.0000) | Acc: (95.00%) (15964/16768)
Epoch: 144 | Batch_idx: 140 |  Loss: (0.1371) |  Loss2: (0.0000) | Acc: (95.00%) (17182/18048)
Epoch: 144 | Batch_idx: 150 |  Loss: (0.1364) |  Loss2: (0.0000) | Acc: (95.00%) (18407/19328)
Epoch: 144 | Batch_idx: 160 |  Loss: (0.1367) |  Loss2: (0.0000) | Acc: (95.00%) (19626/20608)
Epoch: 144 | Batch_idx: 170 |  Loss: (0.1381) |  Loss2: (0.0000) | Acc: (95.00%) (20839/21888)
Epoch: 144 | Batch_idx: 180 |  Loss: (0.1380) |  Loss2: (0.0000) | Acc: (95.00%) (22068/23168)
Epoch: 144 | Batch_idx: 190 |  Loss: (0.1389) |  Loss2: (0.0000) | Acc: (95.00%) (23287/24448)
Epoch: 144 | Batch_idx: 200 |  Loss: (0.1380) |  Loss2: (0.0000) | Acc: (95.00%) (24521/25728)
Epoch: 144 | Batch_idx: 210 |  Loss: (0.1379) |  Loss2: (0.0000) | Acc: (95.00%) (25736/27008)
Epoch: 144 | Batch_idx: 220 |  Loss: (0.1378) |  Loss2: (0.0000) | Acc: (95.00%) (26950/28288)
Epoch: 144 | Batch_idx: 230 |  Loss: (0.1383) |  Loss2: (0.0000) | Acc: (95.00%) (28171/29568)
Epoch: 144 | Batch_idx: 240 |  Loss: (0.1379) |  Loss2: (0.0000) | Acc: (95.00%) (29387/30848)
Epoch: 144 | Batch_idx: 250 |  Loss: (0.1378) |  Loss2: (0.0000) | Acc: (95.00%) (30611/32128)
Epoch: 144 | Batch_idx: 260 |  Loss: (0.1381) |  Loss2: (0.0000) | Acc: (95.00%) (31827/33408)
Epoch: 144 | Batch_idx: 270 |  Loss: (0.1385) |  Loss2: (0.0000) | Acc: (95.00%) (33041/34688)
Epoch: 144 | Batch_idx: 280 |  Loss: (0.1389) |  Loss2: (0.0000) | Acc: (95.00%) (34253/35968)
Epoch: 144 | Batch_idx: 290 |  Loss: (0.1398) |  Loss2: (0.0000) | Acc: (95.00%) (35464/37248)
Epoch: 144 | Batch_idx: 300 |  Loss: (0.1397) |  Loss2: (0.0000) | Acc: (95.00%) (36687/38528)
Epoch: 144 | Batch_idx: 310 |  Loss: (0.1398) |  Loss2: (0.0000) | Acc: (95.00%) (37896/39808)
Epoch: 144 | Batch_idx: 320 |  Loss: (0.1397) |  Loss2: (0.0000) | Acc: (95.00%) (39114/41088)
Epoch: 144 | Batch_idx: 330 |  Loss: (0.1396) |  Loss2: (0.0000) | Acc: (95.00%) (40333/42368)
Epoch: 144 | Batch_idx: 340 |  Loss: (0.1397) |  Loss2: (0.0000) | Acc: (95.00%) (41543/43648)
Epoch: 144 | Batch_idx: 350 |  Loss: (0.1402) |  Loss2: (0.0000) | Acc: (95.00%) (42756/44928)
Epoch: 144 | Batch_idx: 360 |  Loss: (0.1403) |  Loss2: (0.0000) | Acc: (95.00%) (43979/46208)
Epoch: 144 | Batch_idx: 370 |  Loss: (0.1402) |  Loss2: (0.0000) | Acc: (95.00%) (45198/47488)
Epoch: 144 | Batch_idx: 380 |  Loss: (0.1404) |  Loss2: (0.0000) | Acc: (95.00%) (46406/48768)
Epoch: 144 | Batch_idx: 390 |  Loss: (0.1399) |  Loss2: (0.0000) | Acc: (95.00%) (47587/50000)
# TEST : Loss: (0.4059) | Acc: (87.00%) (8786/10000)
percent tensor([0.5432, 0.5524, 0.5447, 0.5153, 0.5521, 0.5584, 0.5604, 0.5286, 0.5425,
        0.5503, 0.5563, 0.5533, 0.5428, 0.5174, 0.5556, 0.5374],
       device='cuda:0') torch.Size([16])
percent tensor([0.5362, 0.5381, 0.5416, 0.5452, 0.5432, 0.5352, 0.5389, 0.5447, 0.5407,
        0.5392, 0.5371, 0.5406, 0.5371, 0.5414, 0.5355, 0.5387],
       device='cuda:0') torch.Size([16])
percent tensor([0.6461, 0.6626, 0.4542, 0.4234, 0.4186, 0.5938, 0.6070, 0.4613, 0.5370,
        0.6296, 0.6575, 0.5238, 0.6680, 0.6271, 0.6259, 0.6277],
       device='cuda:0') torch.Size([16])
percent tensor([0.7261, 0.6396, 0.7003, 0.7023, 0.7085, 0.7791, 0.6863, 0.7150, 0.6808,
        0.6679, 0.6840, 0.6577, 0.6267, 0.7076, 0.7155, 0.7687],
       device='cuda:0') torch.Size([16])
percent tensor([0.6558, 0.6401, 0.6618, 0.5908, 0.6334, 0.5958, 0.6718, 0.6489, 0.6558,
        0.6365, 0.6185, 0.6031, 0.6370, 0.6250, 0.6484, 0.6159],
       device='cuda:0') torch.Size([16])
percent tensor([0.5559, 0.6745, 0.6487, 0.6961, 0.7195, 0.7197, 0.6882, 0.5763, 0.7392,
        0.5996, 0.7224, 0.7271, 0.6711, 0.7508, 0.5769, 0.5482],
       device='cuda:0') torch.Size([16])
percent tensor([0.6739, 0.6824, 0.6089, 0.5301, 0.6627, 0.7446, 0.6440, 0.4572, 0.7034,
        0.6967, 0.6936, 0.4633, 0.6955, 0.6518, 0.4624, 0.6063],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9996, 0.9998, 0.9998, 0.9998, 0.9996, 0.9998, 0.9997, 0.9998,
        0.9997, 0.9999, 0.9999, 0.9999, 0.9994, 0.9995, 0.9991],
       device='cuda:0') torch.Size([16])
Epoch: 145 | Batch_idx: 0 |  Loss: (0.1752) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 145 | Batch_idx: 10 |  Loss: (0.1157) |  Loss2: (0.0000) | Acc: (95.00%) (1349/1408)
Epoch: 145 | Batch_idx: 20 |  Loss: (0.1270) |  Loss2: (0.0000) | Acc: (95.00%) (2559/2688)
Epoch: 145 | Batch_idx: 30 |  Loss: (0.1275) |  Loss2: (0.0000) | Acc: (95.00%) (3786/3968)
Epoch: 145 | Batch_idx: 40 |  Loss: (0.1293) |  Loss2: (0.0000) | Acc: (95.00%) (5008/5248)
Epoch: 145 | Batch_idx: 50 |  Loss: (0.1299) |  Loss2: (0.0000) | Acc: (95.00%) (6231/6528)
Epoch: 145 | Batch_idx: 60 |  Loss: (0.1286) |  Loss2: (0.0000) | Acc: (95.00%) (7456/7808)
Epoch: 145 | Batch_idx: 70 |  Loss: (0.1277) |  Loss2: (0.0000) | Acc: (95.00%) (8686/9088)
Epoch: 145 | Batch_idx: 80 |  Loss: (0.1270) |  Loss2: (0.0000) | Acc: (95.00%) (9914/10368)
Epoch: 145 | Batch_idx: 90 |  Loss: (0.1269) |  Loss2: (0.0000) | Acc: (95.00%) (11141/11648)
Epoch: 145 | Batch_idx: 100 |  Loss: (0.1252) |  Loss2: (0.0000) | Acc: (95.00%) (12374/12928)
Epoch: 145 | Batch_idx: 110 |  Loss: (0.1236) |  Loss2: (0.0000) | Acc: (95.00%) (13604/14208)
Epoch: 145 | Batch_idx: 120 |  Loss: (0.1231) |  Loss2: (0.0000) | Acc: (95.00%) (14836/15488)
Epoch: 145 | Batch_idx: 130 |  Loss: (0.1257) |  Loss2: (0.0000) | Acc: (95.00%) (16041/16768)
Epoch: 145 | Batch_idx: 140 |  Loss: (0.1246) |  Loss2: (0.0000) | Acc: (95.00%) (17269/18048)
Epoch: 145 | Batch_idx: 150 |  Loss: (0.1237) |  Loss2: (0.0000) | Acc: (95.00%) (18498/19328)
Epoch: 145 | Batch_idx: 160 |  Loss: (0.1244) |  Loss2: (0.0000) | Acc: (95.00%) (19723/20608)
Epoch: 145 | Batch_idx: 170 |  Loss: (0.1243) |  Loss2: (0.0000) | Acc: (95.00%) (20938/21888)
Epoch: 145 | Batch_idx: 180 |  Loss: (0.1237) |  Loss2: (0.0000) | Acc: (95.00%) (22168/23168)
Epoch: 145 | Batch_idx: 190 |  Loss: (0.1236) |  Loss2: (0.0000) | Acc: (95.00%) (23405/24448)
Epoch: 145 | Batch_idx: 200 |  Loss: (0.1247) |  Loss2: (0.0000) | Acc: (95.00%) (24628/25728)
Epoch: 145 | Batch_idx: 210 |  Loss: (0.1253) |  Loss2: (0.0000) | Acc: (95.00%) (25850/27008)
Epoch: 145 | Batch_idx: 220 |  Loss: (0.1256) |  Loss2: (0.0000) | Acc: (95.00%) (27073/28288)
Epoch: 145 | Batch_idx: 230 |  Loss: (0.1260) |  Loss2: (0.0000) | Acc: (95.00%) (28290/29568)
Epoch: 145 | Batch_idx: 240 |  Loss: (0.1263) |  Loss2: (0.0000) | Acc: (95.00%) (29513/30848)
Epoch: 145 | Batch_idx: 250 |  Loss: (0.1265) |  Loss2: (0.0000) | Acc: (95.00%) (30737/32128)
Epoch: 145 | Batch_idx: 260 |  Loss: (0.1265) |  Loss2: (0.0000) | Acc: (95.00%) (31965/33408)
Epoch: 145 | Batch_idx: 270 |  Loss: (0.1270) |  Loss2: (0.0000) | Acc: (95.00%) (33182/34688)
Epoch: 145 | Batch_idx: 280 |  Loss: (0.1270) |  Loss2: (0.0000) | Acc: (95.00%) (34408/35968)
Epoch: 145 | Batch_idx: 290 |  Loss: (0.1277) |  Loss2: (0.0000) | Acc: (95.00%) (35625/37248)
Epoch: 145 | Batch_idx: 300 |  Loss: (0.1275) |  Loss2: (0.0000) | Acc: (95.00%) (36845/38528)
Epoch: 145 | Batch_idx: 310 |  Loss: (0.1278) |  Loss2: (0.0000) | Acc: (95.00%) (38070/39808)
Epoch: 145 | Batch_idx: 320 |  Loss: (0.1280) |  Loss2: (0.0000) | Acc: (95.00%) (39288/41088)
Epoch: 145 | Batch_idx: 330 |  Loss: (0.1282) |  Loss2: (0.0000) | Acc: (95.00%) (40506/42368)
Epoch: 145 | Batch_idx: 340 |  Loss: (0.1285) |  Loss2: (0.0000) | Acc: (95.00%) (41726/43648)
Epoch: 145 | Batch_idx: 350 |  Loss: (0.1293) |  Loss2: (0.0000) | Acc: (95.00%) (42943/44928)
Epoch: 145 | Batch_idx: 360 |  Loss: (0.1292) |  Loss2: (0.0000) | Acc: (95.00%) (44173/46208)
Epoch: 145 | Batch_idx: 370 |  Loss: (0.1294) |  Loss2: (0.0000) | Acc: (95.00%) (45388/47488)
Epoch: 145 | Batch_idx: 380 |  Loss: (0.1294) |  Loss2: (0.0000) | Acc: (95.00%) (46614/48768)
Epoch: 145 | Batch_idx: 390 |  Loss: (0.1298) |  Loss2: (0.0000) | Acc: (95.00%) (47785/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_145.pth.tar'
# TEST : Loss: (0.4474) | Acc: (86.00%) (8698/10000)
percent tensor([0.5429, 0.5508, 0.5484, 0.5155, 0.5548, 0.5576, 0.5598, 0.5308, 0.5414,
        0.5508, 0.5549, 0.5567, 0.5427, 0.5139, 0.5548, 0.5365],
       device='cuda:0') torch.Size([16])
percent tensor([0.5357, 0.5376, 0.5406, 0.5445, 0.5417, 0.5356, 0.5389, 0.5442, 0.5403,
        0.5383, 0.5367, 0.5394, 0.5359, 0.5418, 0.5354, 0.5385],
       device='cuda:0') torch.Size([16])
percent tensor([0.6372, 0.6618, 0.4574, 0.4255, 0.4168, 0.5724, 0.6037, 0.4719, 0.5332,
        0.6307, 0.6432, 0.5302, 0.6628, 0.6206, 0.6179, 0.6168],
       device='cuda:0') torch.Size([16])
percent tensor([0.7299, 0.6258, 0.7127, 0.7080, 0.7122, 0.7802, 0.6848, 0.7210, 0.6875,
        0.6590, 0.6801, 0.6662, 0.6249, 0.7019, 0.7106, 0.7656],
       device='cuda:0') torch.Size([16])
percent tensor([0.6597, 0.6408, 0.6558, 0.5812, 0.6272, 0.5907, 0.6684, 0.6466, 0.6454,
        0.6412, 0.6251, 0.5957, 0.6356, 0.6191, 0.6470, 0.6159],
       device='cuda:0') torch.Size([16])
percent tensor([0.5249, 0.6908, 0.6707, 0.7252, 0.7095, 0.7146, 0.6940, 0.5756, 0.7322,
        0.6088, 0.7209, 0.7601, 0.6682, 0.7596, 0.5964, 0.5438],
       device='cuda:0') torch.Size([16])
percent tensor([0.6376, 0.6619, 0.5740, 0.5290, 0.6283, 0.7218, 0.5952, 0.4333, 0.6561,
        0.6566, 0.6642, 0.4409, 0.6719, 0.6138, 0.4684, 0.5821],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9998, 0.9999, 0.9998, 0.9999, 0.9996, 0.9999, 0.9998, 0.9998,
        0.9997, 0.9998, 0.9999, 0.9999, 0.9996, 0.9995, 0.9993],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 146 | Batch_idx: 0 |  Loss: (0.1287) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 146 | Batch_idx: 10 |  Loss: (0.1391) |  Loss2: (0.0000) | Acc: (95.00%) (1339/1408)
Epoch: 146 | Batch_idx: 20 |  Loss: (0.1325) |  Loss2: (0.0000) | Acc: (95.00%) (2573/2688)
Epoch: 146 | Batch_idx: 30 |  Loss: (0.1352) |  Loss2: (0.0000) | Acc: (95.00%) (3800/3968)
Epoch: 146 | Batch_idx: 40 |  Loss: (0.1440) |  Loss2: (0.0000) | Acc: (95.00%) (5010/5248)
Epoch: 146 | Batch_idx: 50 |  Loss: (0.1506) |  Loss2: (0.0000) | Acc: (95.00%) (6215/6528)
Epoch: 146 | Batch_idx: 60 |  Loss: (0.1538) |  Loss2: (0.0000) | Acc: (94.00%) (7411/7808)
Epoch: 146 | Batch_idx: 70 |  Loss: (0.1562) |  Loss2: (0.0000) | Acc: (94.00%) (8608/9088)
Epoch: 146 | Batch_idx: 80 |  Loss: (0.1562) |  Loss2: (0.0000) | Acc: (94.00%) (9808/10368)
Epoch: 146 | Batch_idx: 90 |  Loss: (0.1558) |  Loss2: (0.0000) | Acc: (94.00%) (11029/11648)
Epoch: 146 | Batch_idx: 100 |  Loss: (0.1551) |  Loss2: (0.0000) | Acc: (94.00%) (12242/12928)
Epoch: 146 | Batch_idx: 110 |  Loss: (0.1564) |  Loss2: (0.0000) | Acc: (94.00%) (13445/14208)
Epoch: 146 | Batch_idx: 120 |  Loss: (0.1548) |  Loss2: (0.0000) | Acc: (94.00%) (14667/15488)
Epoch: 146 | Batch_idx: 130 |  Loss: (0.1535) |  Loss2: (0.0000) | Acc: (94.00%) (15887/16768)
Epoch: 146 | Batch_idx: 140 |  Loss: (0.1541) |  Loss2: (0.0000) | Acc: (94.00%) (17091/18048)
Epoch: 146 | Batch_idx: 150 |  Loss: (0.1544) |  Loss2: (0.0000) | Acc: (94.00%) (18302/19328)
Epoch: 146 | Batch_idx: 160 |  Loss: (0.1544) |  Loss2: (0.0000) | Acc: (94.00%) (19508/20608)
Epoch: 146 | Batch_idx: 170 |  Loss: (0.1544) |  Loss2: (0.0000) | Acc: (94.00%) (20724/21888)
Epoch: 146 | Batch_idx: 180 |  Loss: (0.1528) |  Loss2: (0.0000) | Acc: (94.00%) (21956/23168)
Epoch: 146 | Batch_idx: 190 |  Loss: (0.1527) |  Loss2: (0.0000) | Acc: (94.00%) (23177/24448)
Epoch: 146 | Batch_idx: 200 |  Loss: (0.1528) |  Loss2: (0.0000) | Acc: (94.00%) (24382/25728)
Epoch: 146 | Batch_idx: 210 |  Loss: (0.1528) |  Loss2: (0.0000) | Acc: (94.00%) (25584/27008)
Epoch: 146 | Batch_idx: 220 |  Loss: (0.1525) |  Loss2: (0.0000) | Acc: (94.00%) (26793/28288)
Epoch: 146 | Batch_idx: 230 |  Loss: (0.1529) |  Loss2: (0.0000) | Acc: (94.00%) (27996/29568)
Epoch: 146 | Batch_idx: 240 |  Loss: (0.1536) |  Loss2: (0.0000) | Acc: (94.00%) (29201/30848)
Epoch: 146 | Batch_idx: 250 |  Loss: (0.1538) |  Loss2: (0.0000) | Acc: (94.00%) (30407/32128)
Epoch: 146 | Batch_idx: 260 |  Loss: (0.1539) |  Loss2: (0.0000) | Acc: (94.00%) (31616/33408)
Epoch: 146 | Batch_idx: 270 |  Loss: (0.1529) |  Loss2: (0.0000) | Acc: (94.00%) (32842/34688)
Epoch: 146 | Batch_idx: 280 |  Loss: (0.1535) |  Loss2: (0.0000) | Acc: (94.00%) (34048/35968)
Epoch: 146 | Batch_idx: 290 |  Loss: (0.1523) |  Loss2: (0.0000) | Acc: (94.00%) (35271/37248)
Epoch: 146 | Batch_idx: 300 |  Loss: (0.1518) |  Loss2: (0.0000) | Acc: (94.00%) (36493/38528)
Epoch: 146 | Batch_idx: 310 |  Loss: (0.1515) |  Loss2: (0.0000) | Acc: (94.00%) (37707/39808)
Epoch: 146 | Batch_idx: 320 |  Loss: (0.1505) |  Loss2: (0.0000) | Acc: (94.00%) (38930/41088)
Epoch: 146 | Batch_idx: 330 |  Loss: (0.1504) |  Loss2: (0.0000) | Acc: (94.00%) (40147/42368)
Epoch: 146 | Batch_idx: 340 |  Loss: (0.1498) |  Loss2: (0.0000) | Acc: (94.00%) (41372/43648)
Epoch: 146 | Batch_idx: 350 |  Loss: (0.1499) |  Loss2: (0.0000) | Acc: (94.00%) (42591/44928)
Epoch: 146 | Batch_idx: 360 |  Loss: (0.1501) |  Loss2: (0.0000) | Acc: (94.00%) (43793/46208)
Epoch: 146 | Batch_idx: 370 |  Loss: (0.1497) |  Loss2: (0.0000) | Acc: (94.00%) (45007/47488)
Epoch: 146 | Batch_idx: 380 |  Loss: (0.1499) |  Loss2: (0.0000) | Acc: (94.00%) (46220/48768)
Epoch: 146 | Batch_idx: 390 |  Loss: (0.1499) |  Loss2: (0.0000) | Acc: (94.00%) (47383/50000)
# TEST : Loss: (0.4144) | Acc: (87.00%) (8777/10000)
percent tensor([0.5471, 0.5519, 0.5558, 0.5182, 0.5620, 0.5592, 0.5644, 0.5354, 0.5473,
        0.5550, 0.5581, 0.5649, 0.5470, 0.5149, 0.5564, 0.5385],
       device='cuda:0') torch.Size([16])
percent tensor([0.5379, 0.5413, 0.5413, 0.5460, 0.5431, 0.5378, 0.5415, 0.5461, 0.5423,
        0.5411, 0.5394, 0.5408, 0.5390, 0.5445, 0.5382, 0.5410],
       device='cuda:0') torch.Size([16])
percent tensor([0.6562, 0.6833, 0.4443, 0.4292, 0.4032, 0.5929, 0.6150, 0.4784, 0.5522,
        0.6450, 0.6674, 0.5246, 0.6839, 0.6559, 0.6398, 0.6400],
       device='cuda:0') torch.Size([16])
percent tensor([0.7476, 0.6489, 0.7207, 0.7129, 0.7239, 0.7860, 0.7100, 0.7265, 0.7031,
        0.6842, 0.6982, 0.6947, 0.6599, 0.7117, 0.7259, 0.7817],
       device='cuda:0') torch.Size([16])
percent tensor([0.6600, 0.6687, 0.6528, 0.5794, 0.6275, 0.5765, 0.6821, 0.6654, 0.6608,
        0.6599, 0.6408, 0.6066, 0.6454, 0.6512, 0.6649, 0.6119],
       device='cuda:0') torch.Size([16])
percent tensor([0.5096, 0.6791, 0.6514, 0.7083, 0.6847, 0.6929, 0.6730, 0.5399, 0.6940,
        0.5788, 0.6969, 0.7473, 0.6382, 0.7490, 0.5789, 0.5312],
       device='cuda:0') torch.Size([16])
percent tensor([0.6587, 0.6546, 0.6026, 0.5514, 0.6594, 0.7453, 0.5889, 0.4554, 0.6537,
        0.6627, 0.6663, 0.4703, 0.6641, 0.6152, 0.4627, 0.6059],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9998, 0.9999, 0.9998, 0.9998, 0.9997, 0.9999, 0.9997, 0.9998,
        0.9997, 0.9998, 0.9999, 0.9999, 0.9995, 0.9995, 0.9992],
       device='cuda:0') torch.Size([16])
Epoch: 147 | Batch_idx: 0 |  Loss: (0.1253) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 147 | Batch_idx: 10 |  Loss: (0.1457) |  Loss2: (0.0000) | Acc: (94.00%) (1335/1408)
Epoch: 147 | Batch_idx: 20 |  Loss: (0.1479) |  Loss2: (0.0000) | Acc: (94.00%) (2548/2688)
Epoch: 147 | Batch_idx: 30 |  Loss: (0.1459) |  Loss2: (0.0000) | Acc: (95.00%) (3772/3968)
Epoch: 147 | Batch_idx: 40 |  Loss: (0.1428) |  Loss2: (0.0000) | Acc: (95.00%) (4994/5248)
Epoch: 147 | Batch_idx: 50 |  Loss: (0.1425) |  Loss2: (0.0000) | Acc: (95.00%) (6204/6528)
Epoch: 147 | Batch_idx: 60 |  Loss: (0.1451) |  Loss2: (0.0000) | Acc: (94.00%) (7416/7808)
Epoch: 147 | Batch_idx: 70 |  Loss: (0.1455) |  Loss2: (0.0000) | Acc: (95.00%) (8638/9088)
Epoch: 147 | Batch_idx: 80 |  Loss: (0.1425) |  Loss2: (0.0000) | Acc: (95.00%) (9867/10368)
Epoch: 147 | Batch_idx: 90 |  Loss: (0.1390) |  Loss2: (0.0000) | Acc: (95.00%) (11105/11648)
Epoch: 147 | Batch_idx: 100 |  Loss: (0.1380) |  Loss2: (0.0000) | Acc: (95.00%) (12331/12928)
Epoch: 147 | Batch_idx: 110 |  Loss: (0.1388) |  Loss2: (0.0000) | Acc: (95.00%) (13544/14208)
Epoch: 147 | Batch_idx: 120 |  Loss: (0.1388) |  Loss2: (0.0000) | Acc: (95.00%) (14758/15488)
Epoch: 147 | Batch_idx: 130 |  Loss: (0.1380) |  Loss2: (0.0000) | Acc: (95.00%) (15990/16768)
Epoch: 147 | Batch_idx: 140 |  Loss: (0.1368) |  Loss2: (0.0000) | Acc: (95.00%) (17223/18048)
Epoch: 147 | Batch_idx: 150 |  Loss: (0.1370) |  Loss2: (0.0000) | Acc: (95.00%) (18443/19328)
Epoch: 147 | Batch_idx: 160 |  Loss: (0.1372) |  Loss2: (0.0000) | Acc: (95.00%) (19666/20608)
Epoch: 147 | Batch_idx: 170 |  Loss: (0.1359) |  Loss2: (0.0000) | Acc: (95.00%) (20895/21888)
Epoch: 147 | Batch_idx: 180 |  Loss: (0.1365) |  Loss2: (0.0000) | Acc: (95.00%) (22117/23168)
Epoch: 147 | Batch_idx: 190 |  Loss: (0.1357) |  Loss2: (0.0000) | Acc: (95.00%) (23338/24448)
Epoch: 147 | Batch_idx: 200 |  Loss: (0.1354) |  Loss2: (0.0000) | Acc: (95.00%) (24555/25728)
Epoch: 147 | Batch_idx: 210 |  Loss: (0.1362) |  Loss2: (0.0000) | Acc: (95.00%) (25763/27008)
Epoch: 147 | Batch_idx: 220 |  Loss: (0.1372) |  Loss2: (0.0000) | Acc: (95.00%) (26983/28288)
Epoch: 147 | Batch_idx: 230 |  Loss: (0.1376) |  Loss2: (0.0000) | Acc: (95.00%) (28204/29568)
Epoch: 147 | Batch_idx: 240 |  Loss: (0.1368) |  Loss2: (0.0000) | Acc: (95.00%) (29437/30848)
Epoch: 147 | Batch_idx: 250 |  Loss: (0.1362) |  Loss2: (0.0000) | Acc: (95.00%) (30658/32128)
Epoch: 147 | Batch_idx: 260 |  Loss: (0.1358) |  Loss2: (0.0000) | Acc: (95.00%) (31884/33408)
Epoch: 147 | Batch_idx: 270 |  Loss: (0.1352) |  Loss2: (0.0000) | Acc: (95.00%) (33111/34688)
Epoch: 147 | Batch_idx: 280 |  Loss: (0.1349) |  Loss2: (0.0000) | Acc: (95.00%) (34333/35968)
Epoch: 147 | Batch_idx: 290 |  Loss: (0.1347) |  Loss2: (0.0000) | Acc: (95.00%) (35556/37248)
Epoch: 147 | Batch_idx: 300 |  Loss: (0.1345) |  Loss2: (0.0000) | Acc: (95.00%) (36781/38528)
Epoch: 147 | Batch_idx: 310 |  Loss: (0.1349) |  Loss2: (0.0000) | Acc: (95.00%) (37990/39808)
Epoch: 147 | Batch_idx: 320 |  Loss: (0.1350) |  Loss2: (0.0000) | Acc: (95.00%) (39207/41088)
Epoch: 147 | Batch_idx: 330 |  Loss: (0.1347) |  Loss2: (0.0000) | Acc: (95.00%) (40430/42368)
Epoch: 147 | Batch_idx: 340 |  Loss: (0.1345) |  Loss2: (0.0000) | Acc: (95.00%) (41657/43648)
Epoch: 147 | Batch_idx: 350 |  Loss: (0.1344) |  Loss2: (0.0000) | Acc: (95.00%) (42875/44928)
Epoch: 147 | Batch_idx: 360 |  Loss: (0.1344) |  Loss2: (0.0000) | Acc: (95.00%) (44099/46208)
Epoch: 147 | Batch_idx: 370 |  Loss: (0.1339) |  Loss2: (0.0000) | Acc: (95.00%) (45330/47488)
Epoch: 147 | Batch_idx: 380 |  Loss: (0.1335) |  Loss2: (0.0000) | Acc: (95.00%) (46559/48768)
Epoch: 147 | Batch_idx: 390 |  Loss: (0.1333) |  Loss2: (0.0000) | Acc: (95.00%) (47736/50000)
# TEST : Loss: (0.3953) | Acc: (88.00%) (8813/10000)
percent tensor([0.5445, 0.5496, 0.5531, 0.5154, 0.5590, 0.5550, 0.5620, 0.5338, 0.5456,
        0.5530, 0.5560, 0.5627, 0.5450, 0.5134, 0.5531, 0.5357],
       device='cuda:0') torch.Size([16])
percent tensor([0.5398, 0.5436, 0.5432, 0.5480, 0.5450, 0.5395, 0.5436, 0.5481, 0.5443,
        0.5432, 0.5416, 0.5426, 0.5411, 0.5465, 0.5403, 0.5431],
       device='cuda:0') torch.Size([16])
percent tensor([0.6583, 0.6850, 0.4396, 0.4229, 0.4010, 0.5946, 0.6145, 0.4774, 0.5554,
        0.6459, 0.6703, 0.5249, 0.6853, 0.6567, 0.6406, 0.6392],
       device='cuda:0') torch.Size([16])
percent tensor([0.7410, 0.6446, 0.7134, 0.7047, 0.7154, 0.7838, 0.7031, 0.7196, 0.6965,
        0.6797, 0.6920, 0.6867, 0.6543, 0.7088, 0.7186, 0.7784],
       device='cuda:0') torch.Size([16])
percent tensor([0.6554, 0.6686, 0.6509, 0.5776, 0.6261, 0.5706, 0.6797, 0.6638, 0.6576,
        0.6596, 0.6372, 0.6054, 0.6438, 0.6479, 0.6612, 0.6094],
       device='cuda:0') torch.Size([16])
percent tensor([0.5305, 0.6928, 0.6604, 0.7170, 0.6919, 0.7053, 0.6887, 0.5520, 0.7085,
        0.5952, 0.7134, 0.7585, 0.6529, 0.7647, 0.5963, 0.5559],
       device='cuda:0') torch.Size([16])
percent tensor([0.6677, 0.6544, 0.6015, 0.5426, 0.6599, 0.7430, 0.5972, 0.4575, 0.6568,
        0.6685, 0.6693, 0.4683, 0.6652, 0.6203, 0.4604, 0.6129],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9998, 0.9999, 0.9998, 0.9998, 0.9997, 0.9999, 0.9997, 0.9998,
        0.9998, 0.9999, 0.9999, 0.9999, 0.9995, 0.9995, 0.9993],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 148 | Batch_idx: 0 |  Loss: (0.1086) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 148 | Batch_idx: 10 |  Loss: (0.1165) |  Loss2: (0.0000) | Acc: (96.00%) (1354/1408)
Epoch: 148 | Batch_idx: 20 |  Loss: (0.1126) |  Loss2: (0.0000) | Acc: (96.00%) (2583/2688)
Epoch: 148 | Batch_idx: 30 |  Loss: (0.1160) |  Loss2: (0.0000) | Acc: (95.00%) (3805/3968)
Epoch: 148 | Batch_idx: 40 |  Loss: (0.1172) |  Loss2: (0.0000) | Acc: (95.00%) (5035/5248)
Epoch: 148 | Batch_idx: 50 |  Loss: (0.1153) |  Loss2: (0.0000) | Acc: (95.00%) (6265/6528)
Epoch: 148 | Batch_idx: 60 |  Loss: (0.1165) |  Loss2: (0.0000) | Acc: (95.00%) (7493/7808)
Epoch: 148 | Batch_idx: 70 |  Loss: (0.1178) |  Loss2: (0.0000) | Acc: (95.00%) (8711/9088)
Epoch: 148 | Batch_idx: 80 |  Loss: (0.1203) |  Loss2: (0.0000) | Acc: (95.00%) (9931/10368)
Epoch: 148 | Batch_idx: 90 |  Loss: (0.1181) |  Loss2: (0.0000) | Acc: (95.00%) (11170/11648)
Epoch: 148 | Batch_idx: 100 |  Loss: (0.1174) |  Loss2: (0.0000) | Acc: (95.00%) (12402/12928)
Epoch: 148 | Batch_idx: 110 |  Loss: (0.1183) |  Loss2: (0.0000) | Acc: (95.00%) (13636/14208)
Epoch: 148 | Batch_idx: 120 |  Loss: (0.1194) |  Loss2: (0.0000) | Acc: (95.00%) (14850/15488)
Epoch: 148 | Batch_idx: 130 |  Loss: (0.1211) |  Loss2: (0.0000) | Acc: (95.00%) (16062/16768)
Epoch: 148 | Batch_idx: 140 |  Loss: (0.1215) |  Loss2: (0.0000) | Acc: (95.00%) (17289/18048)
Epoch: 148 | Batch_idx: 150 |  Loss: (0.1214) |  Loss2: (0.0000) | Acc: (95.00%) (18513/19328)
Epoch: 148 | Batch_idx: 160 |  Loss: (0.1226) |  Loss2: (0.0000) | Acc: (95.00%) (19727/20608)
Epoch: 148 | Batch_idx: 170 |  Loss: (0.1227) |  Loss2: (0.0000) | Acc: (95.00%) (20953/21888)
Epoch: 148 | Batch_idx: 180 |  Loss: (0.1238) |  Loss2: (0.0000) | Acc: (95.00%) (22170/23168)
Epoch: 148 | Batch_idx: 190 |  Loss: (0.1241) |  Loss2: (0.0000) | Acc: (95.00%) (23401/24448)
Epoch: 148 | Batch_idx: 200 |  Loss: (0.1255) |  Loss2: (0.0000) | Acc: (95.00%) (24612/25728)
Epoch: 148 | Batch_idx: 210 |  Loss: (0.1251) |  Loss2: (0.0000) | Acc: (95.00%) (25843/27008)
Epoch: 148 | Batch_idx: 220 |  Loss: (0.1247) |  Loss2: (0.0000) | Acc: (95.00%) (27068/28288)
Epoch: 148 | Batch_idx: 230 |  Loss: (0.1249) |  Loss2: (0.0000) | Acc: (95.00%) (28293/29568)
Epoch: 148 | Batch_idx: 240 |  Loss: (0.1243) |  Loss2: (0.0000) | Acc: (95.00%) (29526/30848)
Epoch: 148 | Batch_idx: 250 |  Loss: (0.1247) |  Loss2: (0.0000) | Acc: (95.00%) (30751/32128)
Epoch: 148 | Batch_idx: 260 |  Loss: (0.1248) |  Loss2: (0.0000) | Acc: (95.00%) (31983/33408)
Epoch: 148 | Batch_idx: 270 |  Loss: (0.1246) |  Loss2: (0.0000) | Acc: (95.00%) (33205/34688)
Epoch: 148 | Batch_idx: 280 |  Loss: (0.1249) |  Loss2: (0.0000) | Acc: (95.00%) (34427/35968)
Epoch: 148 | Batch_idx: 290 |  Loss: (0.1249) |  Loss2: (0.0000) | Acc: (95.00%) (35659/37248)
Epoch: 148 | Batch_idx: 300 |  Loss: (0.1250) |  Loss2: (0.0000) | Acc: (95.00%) (36894/38528)
Epoch: 148 | Batch_idx: 310 |  Loss: (0.1255) |  Loss2: (0.0000) | Acc: (95.00%) (38109/39808)
Epoch: 148 | Batch_idx: 320 |  Loss: (0.1252) |  Loss2: (0.0000) | Acc: (95.00%) (39336/41088)
Epoch: 148 | Batch_idx: 330 |  Loss: (0.1255) |  Loss2: (0.0000) | Acc: (95.00%) (40566/42368)
Epoch: 148 | Batch_idx: 340 |  Loss: (0.1255) |  Loss2: (0.0000) | Acc: (95.00%) (41793/43648)
Epoch: 148 | Batch_idx: 350 |  Loss: (0.1255) |  Loss2: (0.0000) | Acc: (95.00%) (43015/44928)
Epoch: 148 | Batch_idx: 360 |  Loss: (0.1264) |  Loss2: (0.0000) | Acc: (95.00%) (44222/46208)
Epoch: 148 | Batch_idx: 370 |  Loss: (0.1272) |  Loss2: (0.0000) | Acc: (95.00%) (45431/47488)
Epoch: 148 | Batch_idx: 380 |  Loss: (0.1273) |  Loss2: (0.0000) | Acc: (95.00%) (46650/48768)
Epoch: 148 | Batch_idx: 390 |  Loss: (0.1275) |  Loss2: (0.0000) | Acc: (95.00%) (47823/50000)
# TEST : Loss: (0.4319) | Acc: (87.00%) (8752/10000)
percent tensor([0.5409, 0.5553, 0.5419, 0.5136, 0.5491, 0.5513, 0.5620, 0.5311, 0.5437,
        0.5524, 0.5565, 0.5538, 0.5431, 0.5231, 0.5538, 0.5359],
       device='cuda:0') torch.Size([16])
percent tensor([0.5391, 0.5422, 0.5438, 0.5467, 0.5455, 0.5388, 0.5429, 0.5469, 0.5424,
        0.5423, 0.5405, 0.5422, 0.5398, 0.5444, 0.5398, 0.5419],
       device='cuda:0') torch.Size([16])
percent tensor([0.6703, 0.6913, 0.5026, 0.4561, 0.4373, 0.5910, 0.6407, 0.5131, 0.5887,
        0.6816, 0.6882, 0.5831, 0.7022, 0.6627, 0.6458, 0.6499],
       device='cuda:0') torch.Size([16])
percent tensor([0.7370, 0.6479, 0.7048, 0.6989, 0.7025, 0.7846, 0.6953, 0.7076, 0.6845,
        0.6733, 0.6938, 0.6578, 0.6470, 0.7088, 0.7196, 0.7783],
       device='cuda:0') torch.Size([16])
percent tensor([0.6628, 0.6697, 0.6738, 0.6198, 0.6542, 0.5807, 0.6862, 0.6827, 0.6683,
        0.6642, 0.6345, 0.6343, 0.6543, 0.6480, 0.6675, 0.6247],
       device='cuda:0') torch.Size([16])
percent tensor([0.5515, 0.6629, 0.6364, 0.6931, 0.6853, 0.7220, 0.6742, 0.5448, 0.7136,
        0.5739, 0.7032, 0.7219, 0.6634, 0.7505, 0.5893, 0.5500],
       device='cuda:0') torch.Size([16])
percent tensor([0.6646, 0.6501, 0.5757, 0.5343, 0.6305, 0.7376, 0.5824, 0.4362, 0.6629,
        0.6539, 0.6584, 0.4591, 0.6639, 0.6485, 0.4263, 0.5998],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9997, 0.9999, 0.9998, 0.9999, 0.9993, 0.9998, 0.9998, 0.9998,
        0.9996, 0.9998, 0.9998, 0.9998, 0.9995, 0.9995, 0.9995],
       device='cuda:0') torch.Size([16])
Epoch: 149 | Batch_idx: 0 |  Loss: (0.0918) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 149 | Batch_idx: 10 |  Loss: (0.1154) |  Loss2: (0.0000) | Acc: (96.00%) (1353/1408)
Epoch: 149 | Batch_idx: 20 |  Loss: (0.1187) |  Loss2: (0.0000) | Acc: (95.00%) (2580/2688)
Epoch: 149 | Batch_idx: 30 |  Loss: (0.1229) |  Loss2: (0.0000) | Acc: (95.00%) (3795/3968)
Epoch: 149 | Batch_idx: 40 |  Loss: (0.1262) |  Loss2: (0.0000) | Acc: (95.00%) (5010/5248)
Epoch: 149 | Batch_idx: 50 |  Loss: (0.1263) |  Loss2: (0.0000) | Acc: (95.00%) (6236/6528)
Epoch: 149 | Batch_idx: 60 |  Loss: (0.1234) |  Loss2: (0.0000) | Acc: (95.00%) (7472/7808)
Epoch: 149 | Batch_idx: 70 |  Loss: (0.1230) |  Loss2: (0.0000) | Acc: (95.00%) (8700/9088)
Epoch: 149 | Batch_idx: 80 |  Loss: (0.1226) |  Loss2: (0.0000) | Acc: (95.00%) (9933/10368)
Epoch: 149 | Batch_idx: 90 |  Loss: (0.1194) |  Loss2: (0.0000) | Acc: (95.00%) (11176/11648)
Epoch: 149 | Batch_idx: 100 |  Loss: (0.1193) |  Loss2: (0.0000) | Acc: (95.00%) (12408/12928)
Epoch: 149 | Batch_idx: 110 |  Loss: (0.1192) |  Loss2: (0.0000) | Acc: (96.00%) (13643/14208)
Epoch: 149 | Batch_idx: 120 |  Loss: (0.1184) |  Loss2: (0.0000) | Acc: (96.00%) (14877/15488)
Epoch: 149 | Batch_idx: 130 |  Loss: (0.1186) |  Loss2: (0.0000) | Acc: (96.00%) (16104/16768)
Epoch: 149 | Batch_idx: 140 |  Loss: (0.1191) |  Loss2: (0.0000) | Acc: (96.00%) (17329/18048)
Epoch: 149 | Batch_idx: 150 |  Loss: (0.1190) |  Loss2: (0.0000) | Acc: (96.00%) (18556/19328)
Epoch: 149 | Batch_idx: 160 |  Loss: (0.1195) |  Loss2: (0.0000) | Acc: (95.00%) (19782/20608)
Epoch: 149 | Batch_idx: 170 |  Loss: (0.1206) |  Loss2: (0.0000) | Acc: (95.00%) (21002/21888)
Epoch: 149 | Batch_idx: 180 |  Loss: (0.1217) |  Loss2: (0.0000) | Acc: (95.00%) (22222/23168)
Epoch: 149 | Batch_idx: 190 |  Loss: (0.1206) |  Loss2: (0.0000) | Acc: (95.00%) (23462/24448)
Epoch: 149 | Batch_idx: 200 |  Loss: (0.1203) |  Loss2: (0.0000) | Acc: (95.00%) (24697/25728)
Epoch: 149 | Batch_idx: 210 |  Loss: (0.1203) |  Loss2: (0.0000) | Acc: (95.00%) (25921/27008)
Epoch: 149 | Batch_idx: 220 |  Loss: (0.1209) |  Loss2: (0.0000) | Acc: (95.00%) (27142/28288)
Epoch: 149 | Batch_idx: 230 |  Loss: (0.1204) |  Loss2: (0.0000) | Acc: (95.00%) (28371/29568)
Epoch: 149 | Batch_idx: 240 |  Loss: (0.1206) |  Loss2: (0.0000) | Acc: (95.00%) (29588/30848)
Epoch: 149 | Batch_idx: 250 |  Loss: (0.1214) |  Loss2: (0.0000) | Acc: (95.00%) (30798/32128)
Epoch: 149 | Batch_idx: 260 |  Loss: (0.1218) |  Loss2: (0.0000) | Acc: (95.00%) (32021/33408)
Epoch: 149 | Batch_idx: 270 |  Loss: (0.1216) |  Loss2: (0.0000) | Acc: (95.00%) (33251/34688)
Epoch: 149 | Batch_idx: 280 |  Loss: (0.1228) |  Loss2: (0.0000) | Acc: (95.00%) (34463/35968)
Epoch: 149 | Batch_idx: 290 |  Loss: (0.1232) |  Loss2: (0.0000) | Acc: (95.00%) (35680/37248)
Epoch: 149 | Batch_idx: 300 |  Loss: (0.1229) |  Loss2: (0.0000) | Acc: (95.00%) (36916/38528)
Epoch: 149 | Batch_idx: 310 |  Loss: (0.1234) |  Loss2: (0.0000) | Acc: (95.00%) (38135/39808)
Epoch: 149 | Batch_idx: 320 |  Loss: (0.1233) |  Loss2: (0.0000) | Acc: (95.00%) (39361/41088)
Epoch: 149 | Batch_idx: 330 |  Loss: (0.1232) |  Loss2: (0.0000) | Acc: (95.00%) (40588/42368)
Epoch: 149 | Batch_idx: 340 |  Loss: (0.1228) |  Loss2: (0.0000) | Acc: (95.00%) (41818/43648)
Epoch: 149 | Batch_idx: 350 |  Loss: (0.1230) |  Loss2: (0.0000) | Acc: (95.00%) (43039/44928)
Epoch: 149 | Batch_idx: 360 |  Loss: (0.1232) |  Loss2: (0.0000) | Acc: (95.00%) (44260/46208)
Epoch: 149 | Batch_idx: 370 |  Loss: (0.1234) |  Loss2: (0.0000) | Acc: (95.00%) (45483/47488)
Epoch: 149 | Batch_idx: 380 |  Loss: (0.1238) |  Loss2: (0.0000) | Acc: (95.00%) (46701/48768)
Epoch: 149 | Batch_idx: 390 |  Loss: (0.1237) |  Loss2: (0.0000) | Acc: (95.00%) (47887/50000)
# TEST : Loss: (0.4284) | Acc: (87.00%) (8783/10000)
percent tensor([0.5417, 0.5554, 0.5463, 0.5143, 0.5497, 0.5502, 0.5627, 0.5325, 0.5426,
        0.5536, 0.5560, 0.5566, 0.5434, 0.5217, 0.5533, 0.5359],
       device='cuda:0') torch.Size([16])
percent tensor([0.5400, 0.5426, 0.5441, 0.5474, 0.5455, 0.5395, 0.5429, 0.5481, 0.5432,
        0.5422, 0.5409, 0.5425, 0.5406, 0.5449, 0.5401, 0.5424],
       device='cuda:0') torch.Size([16])
percent tensor([0.6688, 0.6876, 0.4794, 0.4488, 0.4236, 0.5910, 0.6267, 0.4950, 0.5750,
        0.6733, 0.6889, 0.5729, 0.7031, 0.6541, 0.6433, 0.6521],
       device='cuda:0') torch.Size([16])
percent tensor([0.7306, 0.6557, 0.7001, 0.6963, 0.6932, 0.7765, 0.6938, 0.7233, 0.6807,
        0.6764, 0.6894, 0.6603, 0.6415, 0.7173, 0.7153, 0.7767],
       device='cuda:0') torch.Size([16])
percent tensor([0.6581, 0.6631, 0.6657, 0.6034, 0.6486, 0.6026, 0.6808, 0.6558, 0.6710,
        0.6682, 0.6447, 0.6207, 0.6478, 0.6433, 0.6689, 0.6185],
       device='cuda:0') torch.Size([16])
percent tensor([0.5686, 0.6812, 0.6295, 0.6984, 0.6595, 0.7189, 0.7018, 0.5778, 0.7200,
        0.6144, 0.7358, 0.7496, 0.6881, 0.7653, 0.5990, 0.5879],
       device='cuda:0') torch.Size([16])
percent tensor([0.6852, 0.6844, 0.5693, 0.5193, 0.6211, 0.7255, 0.6400, 0.4714, 0.6773,
        0.6979, 0.6884, 0.4358, 0.6817, 0.6550, 0.4445, 0.6344],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9997, 0.9999, 0.9999, 0.9999, 0.9995, 0.9999, 0.9997, 0.9998,
        0.9997, 0.9998, 0.9998, 0.9999, 0.9996, 0.9994, 0.9995],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 150 | Batch_idx: 0 |  Loss: (0.1513) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 150 | Batch_idx: 10 |  Loss: (0.1277) |  Loss2: (0.0000) | Acc: (95.00%) (1342/1408)
Epoch: 150 | Batch_idx: 20 |  Loss: (0.1394) |  Loss2: (0.0000) | Acc: (95.00%) (2555/2688)
Epoch: 150 | Batch_idx: 30 |  Loss: (0.1513) |  Loss2: (0.0000) | Acc: (94.00%) (3756/3968)
Epoch: 150 | Batch_idx: 40 |  Loss: (0.1586) |  Loss2: (0.0000) | Acc: (94.00%) (4965/5248)
Epoch: 150 | Batch_idx: 50 |  Loss: (0.1612) |  Loss2: (0.0000) | Acc: (94.00%) (6166/6528)
Epoch: 150 | Batch_idx: 60 |  Loss: (0.1644) |  Loss2: (0.0000) | Acc: (94.00%) (7367/7808)
Epoch: 150 | Batch_idx: 70 |  Loss: (0.1622) |  Loss2: (0.0000) | Acc: (94.00%) (8579/9088)
Epoch: 150 | Batch_idx: 80 |  Loss: (0.1617) |  Loss2: (0.0000) | Acc: (94.00%) (9788/10368)
Epoch: 150 | Batch_idx: 90 |  Loss: (0.1618) |  Loss2: (0.0000) | Acc: (94.00%) (10994/11648)
Epoch: 150 | Batch_idx: 100 |  Loss: (0.1625) |  Loss2: (0.0000) | Acc: (94.00%) (12198/12928)
Epoch: 150 | Batch_idx: 110 |  Loss: (0.1605) |  Loss2: (0.0000) | Acc: (94.00%) (13408/14208)
Epoch: 150 | Batch_idx: 120 |  Loss: (0.1604) |  Loss2: (0.0000) | Acc: (94.00%) (14619/15488)
Epoch: 150 | Batch_idx: 130 |  Loss: (0.1619) |  Loss2: (0.0000) | Acc: (94.00%) (15810/16768)
Epoch: 150 | Batch_idx: 140 |  Loss: (0.1604) |  Loss2: (0.0000) | Acc: (94.00%) (17024/18048)
Epoch: 150 | Batch_idx: 150 |  Loss: (0.1586) |  Loss2: (0.0000) | Acc: (94.00%) (18245/19328)
Epoch: 150 | Batch_idx: 160 |  Loss: (0.1577) |  Loss2: (0.0000) | Acc: (94.00%) (19462/20608)
Epoch: 150 | Batch_idx: 170 |  Loss: (0.1576) |  Loss2: (0.0000) | Acc: (94.00%) (20680/21888)
Epoch: 150 | Batch_idx: 180 |  Loss: (0.1567) |  Loss2: (0.0000) | Acc: (94.00%) (21893/23168)
Epoch: 150 | Batch_idx: 190 |  Loss: (0.1559) |  Loss2: (0.0000) | Acc: (94.00%) (23115/24448)
Epoch: 150 | Batch_idx: 200 |  Loss: (0.1581) |  Loss2: (0.0000) | Acc: (94.00%) (24313/25728)
Epoch: 150 | Batch_idx: 210 |  Loss: (0.1579) |  Loss2: (0.0000) | Acc: (94.00%) (25515/27008)
Epoch: 150 | Batch_idx: 220 |  Loss: (0.1585) |  Loss2: (0.0000) | Acc: (94.00%) (26722/28288)
Epoch: 150 | Batch_idx: 230 |  Loss: (0.1583) |  Loss2: (0.0000) | Acc: (94.00%) (27937/29568)
Epoch: 150 | Batch_idx: 240 |  Loss: (0.1570) |  Loss2: (0.0000) | Acc: (94.00%) (29152/30848)
Epoch: 150 | Batch_idx: 250 |  Loss: (0.1575) |  Loss2: (0.0000) | Acc: (94.00%) (30349/32128)
Epoch: 150 | Batch_idx: 260 |  Loss: (0.1567) |  Loss2: (0.0000) | Acc: (94.00%) (31569/33408)
Epoch: 150 | Batch_idx: 270 |  Loss: (0.1568) |  Loss2: (0.0000) | Acc: (94.00%) (32778/34688)
Epoch: 150 | Batch_idx: 280 |  Loss: (0.1559) |  Loss2: (0.0000) | Acc: (94.00%) (33997/35968)
Epoch: 150 | Batch_idx: 290 |  Loss: (0.1558) |  Loss2: (0.0000) | Acc: (94.00%) (35211/37248)
Epoch: 150 | Batch_idx: 300 |  Loss: (0.1554) |  Loss2: (0.0000) | Acc: (94.00%) (36424/38528)
Epoch: 150 | Batch_idx: 310 |  Loss: (0.1544) |  Loss2: (0.0000) | Acc: (94.00%) (37659/39808)
Epoch: 150 | Batch_idx: 320 |  Loss: (0.1541) |  Loss2: (0.0000) | Acc: (94.00%) (38881/41088)
Epoch: 150 | Batch_idx: 330 |  Loss: (0.1536) |  Loss2: (0.0000) | Acc: (94.00%) (40103/42368)
Epoch: 150 | Batch_idx: 340 |  Loss: (0.1532) |  Loss2: (0.0000) | Acc: (94.00%) (41328/43648)
Epoch: 150 | Batch_idx: 350 |  Loss: (0.1529) |  Loss2: (0.0000) | Acc: (94.00%) (42545/44928)
Epoch: 150 | Batch_idx: 360 |  Loss: (0.1524) |  Loss2: (0.0000) | Acc: (94.00%) (43765/46208)
Epoch: 150 | Batch_idx: 370 |  Loss: (0.1521) |  Loss2: (0.0000) | Acc: (94.00%) (44984/47488)
Epoch: 150 | Batch_idx: 380 |  Loss: (0.1512) |  Loss2: (0.0000) | Acc: (94.00%) (46214/48768)
Epoch: 150 | Batch_idx: 390 |  Loss: (0.1509) |  Loss2: (0.0000) | Acc: (94.00%) (47384/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_150.pth.tar'
# TEST : Loss: (0.4191) | Acc: (87.00%) (8782/10000)
percent tensor([0.5517, 0.5638, 0.5572, 0.5230, 0.5618, 0.5625, 0.5726, 0.5417, 0.5521,
        0.5620, 0.5648, 0.5667, 0.5525, 0.5279, 0.5638, 0.5451],
       device='cuda:0') torch.Size([16])
percent tensor([0.5493, 0.5524, 0.5537, 0.5576, 0.5558, 0.5480, 0.5526, 0.5588, 0.5529,
        0.5521, 0.5505, 0.5520, 0.5502, 0.5542, 0.5497, 0.5518],
       device='cuda:0') torch.Size([16])
percent tensor([0.6536, 0.6699, 0.4657, 0.4389, 0.4164, 0.5890, 0.6093, 0.4895, 0.5544,
        0.6496, 0.6643, 0.5495, 0.6784, 0.6391, 0.6352, 0.6345],
       device='cuda:0') torch.Size([16])
percent tensor([0.7316, 0.6464, 0.6995, 0.7089, 0.6924, 0.7778, 0.6845, 0.7324, 0.6759,
        0.6714, 0.6831, 0.6512, 0.6364, 0.7060, 0.7157, 0.7761],
       device='cuda:0') torch.Size([16])
percent tensor([0.6610, 0.6684, 0.6629, 0.6036, 0.6459, 0.6081, 0.6833, 0.6475, 0.6757,
        0.6700, 0.6489, 0.6274, 0.6547, 0.6592, 0.6728, 0.6264],
       device='cuda:0') torch.Size([16])
percent tensor([0.5311, 0.6553, 0.6039, 0.6819, 0.6314, 0.7022, 0.6822, 0.5420, 0.6919,
        0.5884, 0.7150, 0.7478, 0.6604, 0.7371, 0.5749, 0.5415],
       device='cuda:0') torch.Size([16])
percent tensor([0.6716, 0.6937, 0.6086, 0.5278, 0.6585, 0.7397, 0.6534, 0.4906, 0.6885,
        0.7064, 0.6999, 0.4413, 0.6933, 0.6548, 0.4566, 0.6183],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9998, 0.9999, 0.9999, 0.9999, 0.9994, 0.9999, 0.9997, 0.9999,
        0.9998, 0.9999, 0.9998, 0.9999, 0.9997, 0.9994, 0.9994],
       device='cuda:0') torch.Size([16])
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(183.0915, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(826.4390, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(821.6455, device='cuda:0')
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1513.6543, device='cuda:0')
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(484.4503, device='cuda:0')
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2279.3003, device='cuda:0')
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4249.2754, device='cuda:0')
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1373.3890, device='cuda:0')
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6225.5933, device='cuda:0')
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11703.5508, device='cuda:0')
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3863.6135, device='cuda:0')
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16309.7471, device='cuda:0')
Epoch: 151 | Batch_idx: 0 |  Loss: (0.1500) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 151 | Batch_idx: 10 |  Loss: (0.1287) |  Loss2: (0.0000) | Acc: (95.00%) (1347/1408)
Epoch: 151 | Batch_idx: 20 |  Loss: (0.1320) |  Loss2: (0.0000) | Acc: (95.00%) (2573/2688)
Epoch: 151 | Batch_idx: 30 |  Loss: (0.1323) |  Loss2: (0.0000) | Acc: (95.00%) (3803/3968)
Epoch: 151 | Batch_idx: 40 |  Loss: (0.1336) |  Loss2: (0.0000) | Acc: (95.00%) (5016/5248)
Epoch: 151 | Batch_idx: 50 |  Loss: (0.1322) |  Loss2: (0.0000) | Acc: (95.00%) (6239/6528)
Epoch: 151 | Batch_idx: 60 |  Loss: (0.1323) |  Loss2: (0.0000) | Acc: (95.00%) (7463/7808)
Epoch: 151 | Batch_idx: 70 |  Loss: (0.1330) |  Loss2: (0.0000) | Acc: (95.00%) (8684/9088)
Epoch: 151 | Batch_idx: 80 |  Loss: (0.1331) |  Loss2: (0.0000) | Acc: (95.00%) (9906/10368)
Epoch: 151 | Batch_idx: 90 |  Loss: (0.1330) |  Loss2: (0.0000) | Acc: (95.00%) (11125/11648)
Epoch: 151 | Batch_idx: 100 |  Loss: (0.1352) |  Loss2: (0.0000) | Acc: (95.00%) (12334/12928)
Epoch: 151 | Batch_idx: 110 |  Loss: (0.1365) |  Loss2: (0.0000) | Acc: (95.00%) (13552/14208)
Epoch: 151 | Batch_idx: 120 |  Loss: (0.1361) |  Loss2: (0.0000) | Acc: (95.00%) (14776/15488)
Epoch: 151 | Batch_idx: 130 |  Loss: (0.1352) |  Loss2: (0.0000) | Acc: (95.00%) (16005/16768)
Epoch: 151 | Batch_idx: 140 |  Loss: (0.1367) |  Loss2: (0.0000) | Acc: (95.00%) (17223/18048)
Epoch: 151 | Batch_idx: 150 |  Loss: (0.1355) |  Loss2: (0.0000) | Acc: (95.00%) (18453/19328)
Epoch: 151 | Batch_idx: 160 |  Loss: (0.1351) |  Loss2: (0.0000) | Acc: (95.00%) (19671/20608)
Epoch: 151 | Batch_idx: 170 |  Loss: (0.1357) |  Loss2: (0.0000) | Acc: (95.00%) (20890/21888)
Epoch: 151 | Batch_idx: 180 |  Loss: (0.1356) |  Loss2: (0.0000) | Acc: (95.00%) (22115/23168)
Epoch: 151 | Batch_idx: 190 |  Loss: (0.1354) |  Loss2: (0.0000) | Acc: (95.00%) (23332/24448)
Epoch: 151 | Batch_idx: 200 |  Loss: (0.1351) |  Loss2: (0.0000) | Acc: (95.00%) (24552/25728)
Epoch: 151 | Batch_idx: 210 |  Loss: (0.1343) |  Loss2: (0.0000) | Acc: (95.00%) (25773/27008)
Epoch: 151 | Batch_idx: 220 |  Loss: (0.1330) |  Loss2: (0.0000) | Acc: (95.00%) (27004/28288)
Epoch: 151 | Batch_idx: 230 |  Loss: (0.1324) |  Loss2: (0.0000) | Acc: (95.00%) (28235/29568)
Epoch: 151 | Batch_idx: 240 |  Loss: (0.1325) |  Loss2: (0.0000) | Acc: (95.00%) (29450/30848)
Epoch: 151 | Batch_idx: 250 |  Loss: (0.1327) |  Loss2: (0.0000) | Acc: (95.00%) (30671/32128)
Epoch: 151 | Batch_idx: 260 |  Loss: (0.1325) |  Loss2: (0.0000) | Acc: (95.00%) (31894/33408)
Epoch: 151 | Batch_idx: 270 |  Loss: (0.1322) |  Loss2: (0.0000) | Acc: (95.00%) (33113/34688)
Epoch: 151 | Batch_idx: 280 |  Loss: (0.1325) |  Loss2: (0.0000) | Acc: (95.00%) (34329/35968)
Epoch: 151 | Batch_idx: 290 |  Loss: (0.1329) |  Loss2: (0.0000) | Acc: (95.00%) (35550/37248)
Epoch: 151 | Batch_idx: 300 |  Loss: (0.1323) |  Loss2: (0.0000) | Acc: (95.00%) (36777/38528)
Epoch: 151 | Batch_idx: 310 |  Loss: (0.1322) |  Loss2: (0.0000) | Acc: (95.00%) (38001/39808)
Epoch: 151 | Batch_idx: 320 |  Loss: (0.1322) |  Loss2: (0.0000) | Acc: (95.00%) (39227/41088)
Epoch: 151 | Batch_idx: 330 |  Loss: (0.1319) |  Loss2: (0.0000) | Acc: (95.00%) (40452/42368)
Epoch: 151 | Batch_idx: 340 |  Loss: (0.1318) |  Loss2: (0.0000) | Acc: (95.00%) (41677/43648)
Epoch: 151 | Batch_idx: 350 |  Loss: (0.1318) |  Loss2: (0.0000) | Acc: (95.00%) (42896/44928)
Epoch: 151 | Batch_idx: 360 |  Loss: (0.1319) |  Loss2: (0.0000) | Acc: (95.00%) (44111/46208)
Epoch: 151 | Batch_idx: 370 |  Loss: (0.1317) |  Loss2: (0.0000) | Acc: (95.00%) (45338/47488)
Epoch: 151 | Batch_idx: 380 |  Loss: (0.1319) |  Loss2: (0.0000) | Acc: (95.00%) (46543/48768)
Epoch: 151 | Batch_idx: 390 |  Loss: (0.1317) |  Loss2: (0.0000) | Acc: (95.00%) (47717/50000)
# TEST : Loss: (0.4032) | Acc: (88.00%) (8812/10000)
percent tensor([0.5460, 0.5563, 0.5512, 0.5177, 0.5552, 0.5579, 0.5649, 0.5354, 0.5457,
        0.5546, 0.5584, 0.5594, 0.5462, 0.5208, 0.5577, 0.5392],
       device='cuda:0') torch.Size([16])
percent tensor([0.5512, 0.5545, 0.5557, 0.5598, 0.5584, 0.5497, 0.5552, 0.5612, 0.5551,
        0.5541, 0.5524, 0.5542, 0.5521, 0.5567, 0.5517, 0.5537],
       device='cuda:0') torch.Size([16])
percent tensor([0.6540, 0.6672, 0.4711, 0.4396, 0.4215, 0.5932, 0.6089, 0.4936, 0.5521,
        0.6449, 0.6574, 0.5469, 0.6749, 0.6364, 0.6370, 0.6352],
       device='cuda:0') torch.Size([16])
percent tensor([0.7275, 0.6364, 0.6990, 0.7054, 0.6904, 0.7739, 0.6776, 0.7278, 0.6717,
        0.6619, 0.6729, 0.6449, 0.6296, 0.6967, 0.7088, 0.7701],
       device='cuda:0') torch.Size([16])
percent tensor([0.6679, 0.6789, 0.6655, 0.6100, 0.6493, 0.6145, 0.6902, 0.6515, 0.6812,
        0.6770, 0.6587, 0.6365, 0.6651, 0.6697, 0.6805, 0.6377],
       device='cuda:0') torch.Size([16])
percent tensor([0.5341, 0.6625, 0.6124, 0.6908, 0.6430, 0.7149, 0.6848, 0.5459, 0.7019,
        0.5903, 0.7166, 0.7550, 0.6679, 0.7449, 0.5771, 0.5425],
       device='cuda:0') torch.Size([16])
percent tensor([0.6641, 0.6899, 0.6111, 0.5250, 0.6620, 0.7447, 0.6458, 0.4826, 0.6874,
        0.7007, 0.6980, 0.4561, 0.6959, 0.6491, 0.4527, 0.6093],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9998, 0.9999, 0.9999, 0.9999, 0.9995, 0.9999, 0.9997, 0.9999,
        0.9998, 0.9999, 0.9998, 0.9998, 0.9997, 0.9994, 0.9994],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 152 | Batch_idx: 0 |  Loss: (0.1767) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 152 | Batch_idx: 10 |  Loss: (0.1210) |  Loss2: (0.0000) | Acc: (95.00%) (1348/1408)
Epoch: 152 | Batch_idx: 20 |  Loss: (0.1076) |  Loss2: (0.0000) | Acc: (96.00%) (2594/2688)
Epoch: 152 | Batch_idx: 30 |  Loss: (0.1170) |  Loss2: (0.0000) | Acc: (96.00%) (3820/3968)
Epoch: 152 | Batch_idx: 40 |  Loss: (0.1128) |  Loss2: (0.0000) | Acc: (96.00%) (5053/5248)
Epoch: 152 | Batch_idx: 50 |  Loss: (0.1163) |  Loss2: (0.0000) | Acc: (96.00%) (6281/6528)
Epoch: 152 | Batch_idx: 60 |  Loss: (0.1166) |  Loss2: (0.0000) | Acc: (96.00%) (7512/7808)
Epoch: 152 | Batch_idx: 70 |  Loss: (0.1173) |  Loss2: (0.0000) | Acc: (96.00%) (8738/9088)
Epoch: 152 | Batch_idx: 80 |  Loss: (0.1178) |  Loss2: (0.0000) | Acc: (96.00%) (9961/10368)
Epoch: 152 | Batch_idx: 90 |  Loss: (0.1184) |  Loss2: (0.0000) | Acc: (96.00%) (11186/11648)
Epoch: 152 | Batch_idx: 100 |  Loss: (0.1177) |  Loss2: (0.0000) | Acc: (96.00%) (12420/12928)
Epoch: 152 | Batch_idx: 110 |  Loss: (0.1175) |  Loss2: (0.0000) | Acc: (96.00%) (13650/14208)
Epoch: 152 | Batch_idx: 120 |  Loss: (0.1166) |  Loss2: (0.0000) | Acc: (96.00%) (14885/15488)
Epoch: 152 | Batch_idx: 130 |  Loss: (0.1167) |  Loss2: (0.0000) | Acc: (96.00%) (16107/16768)
Epoch: 152 | Batch_idx: 140 |  Loss: (0.1161) |  Loss2: (0.0000) | Acc: (96.00%) (17336/18048)
Epoch: 152 | Batch_idx: 150 |  Loss: (0.1179) |  Loss2: (0.0000) | Acc: (95.00%) (18546/19328)
Epoch: 152 | Batch_idx: 160 |  Loss: (0.1182) |  Loss2: (0.0000) | Acc: (95.00%) (19772/20608)
Epoch: 152 | Batch_idx: 170 |  Loss: (0.1186) |  Loss2: (0.0000) | Acc: (95.00%) (20997/21888)
Epoch: 152 | Batch_idx: 180 |  Loss: (0.1198) |  Loss2: (0.0000) | Acc: (95.00%) (22217/23168)
Epoch: 152 | Batch_idx: 190 |  Loss: (0.1203) |  Loss2: (0.0000) | Acc: (95.00%) (23444/24448)
Epoch: 152 | Batch_idx: 200 |  Loss: (0.1190) |  Loss2: (0.0000) | Acc: (95.00%) (24684/25728)
Epoch: 152 | Batch_idx: 210 |  Loss: (0.1193) |  Loss2: (0.0000) | Acc: (95.00%) (25898/27008)
Epoch: 152 | Batch_idx: 220 |  Loss: (0.1192) |  Loss2: (0.0000) | Acc: (95.00%) (27129/28288)
Epoch: 152 | Batch_idx: 230 |  Loss: (0.1190) |  Loss2: (0.0000) | Acc: (95.00%) (28353/29568)
Epoch: 152 | Batch_idx: 240 |  Loss: (0.1198) |  Loss2: (0.0000) | Acc: (95.00%) (29573/30848)
Epoch: 152 | Batch_idx: 250 |  Loss: (0.1204) |  Loss2: (0.0000) | Acc: (95.00%) (30793/32128)
Epoch: 152 | Batch_idx: 260 |  Loss: (0.1204) |  Loss2: (0.0000) | Acc: (95.00%) (32021/33408)
Epoch: 152 | Batch_idx: 270 |  Loss: (0.1200) |  Loss2: (0.0000) | Acc: (95.00%) (33256/34688)
Epoch: 152 | Batch_idx: 280 |  Loss: (0.1194) |  Loss2: (0.0000) | Acc: (95.00%) (34485/35968)
Epoch: 152 | Batch_idx: 290 |  Loss: (0.1201) |  Loss2: (0.0000) | Acc: (95.00%) (35706/37248)
Epoch: 152 | Batch_idx: 300 |  Loss: (0.1210) |  Loss2: (0.0000) | Acc: (95.00%) (36920/38528)
Epoch: 152 | Batch_idx: 310 |  Loss: (0.1208) |  Loss2: (0.0000) | Acc: (95.00%) (38146/39808)
Epoch: 152 | Batch_idx: 320 |  Loss: (0.1215) |  Loss2: (0.0000) | Acc: (95.00%) (39355/41088)
Epoch: 152 | Batch_idx: 330 |  Loss: (0.1220) |  Loss2: (0.0000) | Acc: (95.00%) (40576/42368)
Epoch: 152 | Batch_idx: 340 |  Loss: (0.1221) |  Loss2: (0.0000) | Acc: (95.00%) (41802/43648)
Epoch: 152 | Batch_idx: 350 |  Loss: (0.1228) |  Loss2: (0.0000) | Acc: (95.00%) (43017/44928)
Epoch: 152 | Batch_idx: 360 |  Loss: (0.1226) |  Loss2: (0.0000) | Acc: (95.00%) (44246/46208)
Epoch: 152 | Batch_idx: 370 |  Loss: (0.1227) |  Loss2: (0.0000) | Acc: (95.00%) (45471/47488)
Epoch: 152 | Batch_idx: 380 |  Loss: (0.1229) |  Loss2: (0.0000) | Acc: (95.00%) (46697/48768)
Epoch: 152 | Batch_idx: 390 |  Loss: (0.1231) |  Loss2: (0.0000) | Acc: (95.00%) (47868/50000)
# TEST : Loss: (0.4035) | Acc: (87.00%) (8788/10000)
percent tensor([0.5451, 0.5568, 0.5453, 0.5165, 0.5527, 0.5583, 0.5638, 0.5319, 0.5463,
        0.5527, 0.5591, 0.5552, 0.5459, 0.5239, 0.5574, 0.5393],
       device='cuda:0') torch.Size([16])
percent tensor([0.5501, 0.5545, 0.5552, 0.5602, 0.5582, 0.5487, 0.5550, 0.5605, 0.5544,
        0.5537, 0.5512, 0.5537, 0.5510, 0.5574, 0.5511, 0.5532],
       device='cuda:0') torch.Size([16])
percent tensor([0.6516, 0.6670, 0.4686, 0.4241, 0.4228, 0.5834, 0.6137, 0.4891, 0.5564,
        0.6429, 0.6565, 0.5438, 0.6725, 0.6401, 0.6310, 0.6312],
       device='cuda:0') torch.Size([16])
percent tensor([0.7281, 0.6463, 0.7090, 0.7097, 0.7047, 0.7733, 0.6885, 0.7192, 0.6768,
        0.6672, 0.6816, 0.6604, 0.6316, 0.7134, 0.7090, 0.7714],
       device='cuda:0') torch.Size([16])
percent tensor([0.6753, 0.6787, 0.6540, 0.6005, 0.6333, 0.6222, 0.6939, 0.6614, 0.6774,
        0.6708, 0.6553, 0.6283, 0.6691, 0.6607, 0.6811, 0.6469],
       device='cuda:0') torch.Size([16])
percent tensor([0.5350, 0.6632, 0.6479, 0.7041, 0.6890, 0.7257, 0.6583, 0.5344, 0.7169,
        0.5731, 0.7151, 0.7571, 0.6683, 0.7436, 0.5879, 0.5172],
       device='cuda:0') torch.Size([16])
percent tensor([0.6643, 0.6732, 0.6354, 0.5422, 0.6838, 0.7490, 0.6080, 0.4798, 0.7110,
        0.6777, 0.6958, 0.4745, 0.6744, 0.6389, 0.4597, 0.5958],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9997, 0.9999, 0.9998, 0.9999, 0.9995, 0.9998, 0.9996, 0.9998,
        0.9995, 0.9998, 0.9999, 0.9999, 0.9992, 0.9994, 0.9996],
       device='cuda:0') torch.Size([16])
Epoch: 153 | Batch_idx: 0 |  Loss: (0.1196) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 153 | Batch_idx: 10 |  Loss: (0.1026) |  Loss2: (0.0000) | Acc: (96.00%) (1356/1408)
Epoch: 153 | Batch_idx: 20 |  Loss: (0.1036) |  Loss2: (0.0000) | Acc: (96.00%) (2594/2688)
Epoch: 153 | Batch_idx: 30 |  Loss: (0.1055) |  Loss2: (0.0000) | Acc: (96.00%) (3831/3968)
Epoch: 153 | Batch_idx: 40 |  Loss: (0.1085) |  Loss2: (0.0000) | Acc: (96.00%) (5056/5248)
Epoch: 153 | Batch_idx: 50 |  Loss: (0.1108) |  Loss2: (0.0000) | Acc: (96.00%) (6279/6528)
Epoch: 153 | Batch_idx: 60 |  Loss: (0.1126) |  Loss2: (0.0000) | Acc: (96.00%) (7501/7808)
Epoch: 153 | Batch_idx: 70 |  Loss: (0.1099) |  Loss2: (0.0000) | Acc: (96.00%) (8740/9088)
Epoch: 153 | Batch_idx: 80 |  Loss: (0.1096) |  Loss2: (0.0000) | Acc: (96.00%) (9978/10368)
Epoch: 153 | Batch_idx: 90 |  Loss: (0.1101) |  Loss2: (0.0000) | Acc: (96.00%) (11205/11648)
Epoch: 153 | Batch_idx: 100 |  Loss: (0.1106) |  Loss2: (0.0000) | Acc: (96.00%) (12433/12928)
Epoch: 153 | Batch_idx: 110 |  Loss: (0.1111) |  Loss2: (0.0000) | Acc: (96.00%) (13663/14208)
Epoch: 153 | Batch_idx: 120 |  Loss: (0.1131) |  Loss2: (0.0000) | Acc: (96.00%) (14890/15488)
Epoch: 153 | Batch_idx: 130 |  Loss: (0.1141) |  Loss2: (0.0000) | Acc: (96.00%) (16116/16768)
Epoch: 153 | Batch_idx: 140 |  Loss: (0.1151) |  Loss2: (0.0000) | Acc: (96.00%) (17340/18048)
Epoch: 153 | Batch_idx: 150 |  Loss: (0.1157) |  Loss2: (0.0000) | Acc: (96.00%) (18566/19328)
Epoch: 153 | Batch_idx: 160 |  Loss: (0.1171) |  Loss2: (0.0000) | Acc: (96.00%) (19785/20608)
Epoch: 153 | Batch_idx: 170 |  Loss: (0.1171) |  Loss2: (0.0000) | Acc: (95.00%) (21009/21888)
Epoch: 153 | Batch_idx: 180 |  Loss: (0.1169) |  Loss2: (0.0000) | Acc: (95.00%) (22237/23168)
Epoch: 153 | Batch_idx: 190 |  Loss: (0.1177) |  Loss2: (0.0000) | Acc: (95.00%) (23460/24448)
Epoch: 153 | Batch_idx: 200 |  Loss: (0.1195) |  Loss2: (0.0000) | Acc: (95.00%) (24667/25728)
Epoch: 153 | Batch_idx: 210 |  Loss: (0.1194) |  Loss2: (0.0000) | Acc: (95.00%) (25898/27008)
Epoch: 153 | Batch_idx: 220 |  Loss: (0.1191) |  Loss2: (0.0000) | Acc: (95.00%) (27128/28288)
Epoch: 153 | Batch_idx: 230 |  Loss: (0.1192) |  Loss2: (0.0000) | Acc: (95.00%) (28353/29568)
Epoch: 153 | Batch_idx: 240 |  Loss: (0.1189) |  Loss2: (0.0000) | Acc: (95.00%) (29578/30848)
Epoch: 153 | Batch_idx: 250 |  Loss: (0.1196) |  Loss2: (0.0000) | Acc: (95.00%) (30800/32128)
Epoch: 153 | Batch_idx: 260 |  Loss: (0.1203) |  Loss2: (0.0000) | Acc: (95.00%) (32015/33408)
Epoch: 153 | Batch_idx: 270 |  Loss: (0.1208) |  Loss2: (0.0000) | Acc: (95.00%) (33242/34688)
Epoch: 153 | Batch_idx: 280 |  Loss: (0.1208) |  Loss2: (0.0000) | Acc: (95.00%) (34467/35968)
Epoch: 153 | Batch_idx: 290 |  Loss: (0.1208) |  Loss2: (0.0000) | Acc: (95.00%) (35692/37248)
Epoch: 153 | Batch_idx: 300 |  Loss: (0.1215) |  Loss2: (0.0000) | Acc: (95.00%) (36914/38528)
Epoch: 153 | Batch_idx: 310 |  Loss: (0.1220) |  Loss2: (0.0000) | Acc: (95.00%) (38141/39808)
Epoch: 153 | Batch_idx: 320 |  Loss: (0.1220) |  Loss2: (0.0000) | Acc: (95.00%) (39371/41088)
Epoch: 153 | Batch_idx: 330 |  Loss: (0.1224) |  Loss2: (0.0000) | Acc: (95.00%) (40590/42368)
Epoch: 153 | Batch_idx: 340 |  Loss: (0.1220) |  Loss2: (0.0000) | Acc: (95.00%) (41816/43648)
Epoch: 153 | Batch_idx: 350 |  Loss: (0.1221) |  Loss2: (0.0000) | Acc: (95.00%) (43035/44928)
Epoch: 153 | Batch_idx: 360 |  Loss: (0.1223) |  Loss2: (0.0000) | Acc: (95.00%) (44268/46208)
Epoch: 153 | Batch_idx: 370 |  Loss: (0.1222) |  Loss2: (0.0000) | Acc: (95.00%) (45492/47488)
Epoch: 153 | Batch_idx: 380 |  Loss: (0.1222) |  Loss2: (0.0000) | Acc: (95.00%) (46720/48768)
Epoch: 153 | Batch_idx: 390 |  Loss: (0.1223) |  Loss2: (0.0000) | Acc: (95.00%) (47901/50000)
# TEST : Loss: (0.4394) | Acc: (87.00%) (8756/10000)
percent tensor([0.5454, 0.5560, 0.5437, 0.5169, 0.5503, 0.5600, 0.5612, 0.5325, 0.5449,
        0.5510, 0.5588, 0.5521, 0.5452, 0.5203, 0.5578, 0.5394],
       device='cuda:0') torch.Size([16])
percent tensor([0.5499, 0.5531, 0.5578, 0.5595, 0.5596, 0.5481, 0.5554, 0.5610, 0.5547,
        0.5546, 0.5505, 0.5561, 0.5512, 0.5564, 0.5506, 0.5522],
       device='cuda:0') torch.Size([16])
percent tensor([0.6548, 0.6649, 0.4696, 0.4487, 0.4228, 0.5955, 0.6143, 0.4972, 0.5593,
        0.6345, 0.6535, 0.5350, 0.6705, 0.6380, 0.6286, 0.6397],
       device='cuda:0') torch.Size([16])
percent tensor([0.7267, 0.6293, 0.7079, 0.7081, 0.7000, 0.7752, 0.6833, 0.7133, 0.6781,
        0.6548, 0.6712, 0.6577, 0.6246, 0.7011, 0.7016, 0.7683],
       device='cuda:0') torch.Size([16])
percent tensor([0.6758, 0.6766, 0.6680, 0.6093, 0.6454, 0.6157, 0.6977, 0.6657, 0.6766,
        0.6729, 0.6553, 0.6374, 0.6725, 0.6706, 0.6882, 0.6432],
       device='cuda:0') torch.Size([16])
percent tensor([0.5187, 0.6961, 0.6250, 0.6992, 0.6811, 0.7330, 0.6636, 0.5228, 0.7226,
        0.5518, 0.7179, 0.7459, 0.6623, 0.7665, 0.5714, 0.5431],
       device='cuda:0') torch.Size([16])
percent tensor([0.6321, 0.6737, 0.5894, 0.5136, 0.6826, 0.7728, 0.6014, 0.4489, 0.6963,
        0.6656, 0.6875, 0.4572, 0.6848, 0.6311, 0.4458, 0.6049],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9997, 0.9999, 0.9998, 0.9999, 0.9997, 0.9998, 0.9996, 0.9997,
        0.9995, 0.9999, 0.9999, 0.9999, 0.9996, 0.9994, 0.9994],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 154 | Batch_idx: 0 |  Loss: (0.1260) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 154 | Batch_idx: 10 |  Loss: (0.1363) |  Loss2: (0.0000) | Acc: (95.00%) (1340/1408)
Epoch: 154 | Batch_idx: 20 |  Loss: (0.1412) |  Loss2: (0.0000) | Acc: (94.00%) (2553/2688)
Epoch: 154 | Batch_idx: 30 |  Loss: (0.1430) |  Loss2: (0.0000) | Acc: (94.00%) (3768/3968)
Epoch: 154 | Batch_idx: 40 |  Loss: (0.1458) |  Loss2: (0.0000) | Acc: (95.00%) (4986/5248)
Epoch: 154 | Batch_idx: 50 |  Loss: (0.1544) |  Loss2: (0.0000) | Acc: (94.00%) (6176/6528)
Epoch: 154 | Batch_idx: 60 |  Loss: (0.1543) |  Loss2: (0.0000) | Acc: (94.00%) (7392/7808)
Epoch: 154 | Batch_idx: 70 |  Loss: (0.1554) |  Loss2: (0.0000) | Acc: (94.00%) (8601/9088)
Epoch: 154 | Batch_idx: 80 |  Loss: (0.1554) |  Loss2: (0.0000) | Acc: (94.00%) (9809/10368)
Epoch: 154 | Batch_idx: 90 |  Loss: (0.1548) |  Loss2: (0.0000) | Acc: (94.00%) (11015/11648)
Epoch: 154 | Batch_idx: 100 |  Loss: (0.1536) |  Loss2: (0.0000) | Acc: (94.00%) (12227/12928)
Epoch: 154 | Batch_idx: 110 |  Loss: (0.1515) |  Loss2: (0.0000) | Acc: (94.00%) (13443/14208)
Epoch: 154 | Batch_idx: 120 |  Loss: (0.1533) |  Loss2: (0.0000) | Acc: (94.00%) (14645/15488)
Epoch: 154 | Batch_idx: 130 |  Loss: (0.1538) |  Loss2: (0.0000) | Acc: (94.00%) (15852/16768)
Epoch: 154 | Batch_idx: 140 |  Loss: (0.1540) |  Loss2: (0.0000) | Acc: (94.00%) (17063/18048)
Epoch: 154 | Batch_idx: 150 |  Loss: (0.1529) |  Loss2: (0.0000) | Acc: (94.00%) (18283/19328)
Epoch: 154 | Batch_idx: 160 |  Loss: (0.1513) |  Loss2: (0.0000) | Acc: (94.00%) (19509/20608)
Epoch: 154 | Batch_idx: 170 |  Loss: (0.1508) |  Loss2: (0.0000) | Acc: (94.00%) (20726/21888)
Epoch: 154 | Batch_idx: 180 |  Loss: (0.1511) |  Loss2: (0.0000) | Acc: (94.00%) (21934/23168)
Epoch: 154 | Batch_idx: 190 |  Loss: (0.1489) |  Loss2: (0.0000) | Acc: (94.00%) (23168/24448)
Epoch: 154 | Batch_idx: 200 |  Loss: (0.1476) |  Loss2: (0.0000) | Acc: (94.00%) (24388/25728)
Epoch: 154 | Batch_idx: 210 |  Loss: (0.1484) |  Loss2: (0.0000) | Acc: (94.00%) (25588/27008)
Epoch: 154 | Batch_idx: 220 |  Loss: (0.1474) |  Loss2: (0.0000) | Acc: (94.00%) (26811/28288)
Epoch: 154 | Batch_idx: 230 |  Loss: (0.1468) |  Loss2: (0.0000) | Acc: (94.00%) (28030/29568)
Epoch: 154 | Batch_idx: 240 |  Loss: (0.1457) |  Loss2: (0.0000) | Acc: (94.00%) (29257/30848)
Epoch: 154 | Batch_idx: 250 |  Loss: (0.1459) |  Loss2: (0.0000) | Acc: (94.00%) (30473/32128)
Epoch: 154 | Batch_idx: 260 |  Loss: (0.1456) |  Loss2: (0.0000) | Acc: (94.00%) (31679/33408)
Epoch: 154 | Batch_idx: 270 |  Loss: (0.1450) |  Loss2: (0.0000) | Acc: (94.00%) (32903/34688)
Epoch: 154 | Batch_idx: 280 |  Loss: (0.1448) |  Loss2: (0.0000) | Acc: (94.00%) (34124/35968)
Epoch: 154 | Batch_idx: 290 |  Loss: (0.1440) |  Loss2: (0.0000) | Acc: (94.00%) (35353/37248)
Epoch: 154 | Batch_idx: 300 |  Loss: (0.1450) |  Loss2: (0.0000) | Acc: (94.00%) (36547/38528)
Epoch: 154 | Batch_idx: 310 |  Loss: (0.1440) |  Loss2: (0.0000) | Acc: (94.00%) (37777/39808)
Epoch: 154 | Batch_idx: 320 |  Loss: (0.1435) |  Loss2: (0.0000) | Acc: (94.00%) (38996/41088)
Epoch: 154 | Batch_idx: 330 |  Loss: (0.1438) |  Loss2: (0.0000) | Acc: (94.00%) (40209/42368)
Epoch: 154 | Batch_idx: 340 |  Loss: (0.1435) |  Loss2: (0.0000) | Acc: (94.00%) (41424/43648)
Epoch: 154 | Batch_idx: 350 |  Loss: (0.1435) |  Loss2: (0.0000) | Acc: (94.00%) (42635/44928)
Epoch: 154 | Batch_idx: 360 |  Loss: (0.1433) |  Loss2: (0.0000) | Acc: (94.00%) (43853/46208)
Epoch: 154 | Batch_idx: 370 |  Loss: (0.1429) |  Loss2: (0.0000) | Acc: (94.00%) (45078/47488)
Epoch: 154 | Batch_idx: 380 |  Loss: (0.1426) |  Loss2: (0.0000) | Acc: (94.00%) (46292/48768)
Epoch: 154 | Batch_idx: 390 |  Loss: (0.1430) |  Loss2: (0.0000) | Acc: (94.00%) (47458/50000)
# TEST : Loss: (0.4180) | Acc: (87.00%) (8788/10000)
percent tensor([0.5507, 0.5625, 0.5500, 0.5226, 0.5574, 0.5650, 0.5684, 0.5386, 0.5511,
        0.5576, 0.5647, 0.5596, 0.5509, 0.5274, 0.5637, 0.5446],
       device='cuda:0') torch.Size([16])
percent tensor([0.5487, 0.5513, 0.5557, 0.5579, 0.5574, 0.5480, 0.5534, 0.5586, 0.5528,
        0.5523, 0.5487, 0.5536, 0.5496, 0.5547, 0.5491, 0.5510],
       device='cuda:0') torch.Size([16])
percent tensor([0.6552, 0.6677, 0.4975, 0.4501, 0.4435, 0.5934, 0.6165, 0.5047, 0.5579,
        0.6367, 0.6506, 0.5552, 0.6710, 0.6324, 0.6279, 0.6334],
       device='cuda:0') torch.Size([16])
percent tensor([0.7636, 0.6549, 0.7458, 0.7471, 0.7387, 0.8138, 0.7161, 0.7498, 0.7074,
        0.6853, 0.7053, 0.6955, 0.6552, 0.7267, 0.7392, 0.8033],
       device='cuda:0') torch.Size([16])
percent tensor([0.6346, 0.6400, 0.6250, 0.5692, 0.6059, 0.5645, 0.6546, 0.6285, 0.6368,
        0.6316, 0.6074, 0.5938, 0.6288, 0.6373, 0.6498, 0.5949],
       device='cuda:0') torch.Size([16])
percent tensor([0.5399, 0.7188, 0.6460, 0.7090, 0.7028, 0.7357, 0.6951, 0.5598, 0.7385,
        0.6011, 0.7309, 0.7663, 0.6809, 0.7812, 0.5929, 0.5673],
       device='cuda:0') torch.Size([16])
percent tensor([0.6412, 0.6984, 0.5866, 0.5227, 0.6768, 0.7736, 0.6270, 0.4404, 0.7240,
        0.6883, 0.7186, 0.4806, 0.7081, 0.6803, 0.4443, 0.6023],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9997, 0.9999, 0.9998, 0.9999, 0.9997, 0.9998, 0.9997, 0.9998,
        0.9995, 0.9999, 0.9999, 0.9999, 0.9995, 0.9994, 0.9994],
       device='cuda:0') torch.Size([16])
Epoch: 155 | Batch_idx: 0 |  Loss: (0.1398) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 155 | Batch_idx: 10 |  Loss: (0.1301) |  Loss2: (0.0000) | Acc: (95.00%) (1338/1408)
Epoch: 155 | Batch_idx: 20 |  Loss: (0.1388) |  Loss2: (0.0000) | Acc: (94.00%) (2552/2688)
Epoch: 155 | Batch_idx: 30 |  Loss: (0.1375) |  Loss2: (0.0000) | Acc: (95.00%) (3774/3968)
Epoch: 155 | Batch_idx: 40 |  Loss: (0.1324) |  Loss2: (0.0000) | Acc: (95.00%) (5006/5248)
Epoch: 155 | Batch_idx: 50 |  Loss: (0.1318) |  Loss2: (0.0000) | Acc: (95.00%) (6230/6528)
Epoch: 155 | Batch_idx: 60 |  Loss: (0.1294) |  Loss2: (0.0000) | Acc: (95.00%) (7463/7808)
Epoch: 155 | Batch_idx: 70 |  Loss: (0.1283) |  Loss2: (0.0000) | Acc: (95.00%) (8696/9088)
Epoch: 155 | Batch_idx: 80 |  Loss: (0.1292) |  Loss2: (0.0000) | Acc: (95.00%) (9918/10368)
Epoch: 155 | Batch_idx: 90 |  Loss: (0.1307) |  Loss2: (0.0000) | Acc: (95.00%) (11137/11648)
Epoch: 155 | Batch_idx: 100 |  Loss: (0.1300) |  Loss2: (0.0000) | Acc: (95.00%) (12365/12928)
Epoch: 155 | Batch_idx: 110 |  Loss: (0.1297) |  Loss2: (0.0000) | Acc: (95.00%) (13594/14208)
Epoch: 155 | Batch_idx: 120 |  Loss: (0.1283) |  Loss2: (0.0000) | Acc: (95.00%) (14823/15488)
Epoch: 155 | Batch_idx: 130 |  Loss: (0.1265) |  Loss2: (0.0000) | Acc: (95.00%) (16053/16768)
Epoch: 155 | Batch_idx: 140 |  Loss: (0.1259) |  Loss2: (0.0000) | Acc: (95.00%) (17283/18048)
Epoch: 155 | Batch_idx: 150 |  Loss: (0.1258) |  Loss2: (0.0000) | Acc: (95.00%) (18511/19328)
Epoch: 155 | Batch_idx: 160 |  Loss: (0.1252) |  Loss2: (0.0000) | Acc: (95.00%) (19741/20608)
Epoch: 155 | Batch_idx: 170 |  Loss: (0.1257) |  Loss2: (0.0000) | Acc: (95.00%) (20962/21888)
Epoch: 155 | Batch_idx: 180 |  Loss: (0.1257) |  Loss2: (0.0000) | Acc: (95.00%) (22187/23168)
Epoch: 155 | Batch_idx: 190 |  Loss: (0.1259) |  Loss2: (0.0000) | Acc: (95.00%) (23404/24448)
Epoch: 155 | Batch_idx: 200 |  Loss: (0.1253) |  Loss2: (0.0000) | Acc: (95.00%) (24634/25728)
Epoch: 155 | Batch_idx: 210 |  Loss: (0.1250) |  Loss2: (0.0000) | Acc: (95.00%) (25864/27008)
Epoch: 155 | Batch_idx: 220 |  Loss: (0.1258) |  Loss2: (0.0000) | Acc: (95.00%) (27082/28288)
Epoch: 155 | Batch_idx: 230 |  Loss: (0.1254) |  Loss2: (0.0000) | Acc: (95.00%) (28313/29568)
Epoch: 155 | Batch_idx: 240 |  Loss: (0.1261) |  Loss2: (0.0000) | Acc: (95.00%) (29527/30848)
Epoch: 155 | Batch_idx: 250 |  Loss: (0.1252) |  Loss2: (0.0000) | Acc: (95.00%) (30766/32128)
Epoch: 155 | Batch_idx: 260 |  Loss: (0.1244) |  Loss2: (0.0000) | Acc: (95.00%) (32007/33408)
Epoch: 155 | Batch_idx: 270 |  Loss: (0.1241) |  Loss2: (0.0000) | Acc: (95.00%) (33240/34688)
Epoch: 155 | Batch_idx: 280 |  Loss: (0.1237) |  Loss2: (0.0000) | Acc: (95.00%) (34467/35968)
Epoch: 155 | Batch_idx: 290 |  Loss: (0.1234) |  Loss2: (0.0000) | Acc: (95.00%) (35700/37248)
Epoch: 155 | Batch_idx: 300 |  Loss: (0.1231) |  Loss2: (0.0000) | Acc: (95.00%) (36926/38528)
Epoch: 155 | Batch_idx: 310 |  Loss: (0.1235) |  Loss2: (0.0000) | Acc: (95.00%) (38150/39808)
Epoch: 155 | Batch_idx: 320 |  Loss: (0.1237) |  Loss2: (0.0000) | Acc: (95.00%) (39376/41088)
Epoch: 155 | Batch_idx: 330 |  Loss: (0.1238) |  Loss2: (0.0000) | Acc: (95.00%) (40598/42368)
Epoch: 155 | Batch_idx: 340 |  Loss: (0.1243) |  Loss2: (0.0000) | Acc: (95.00%) (41820/43648)
Epoch: 155 | Batch_idx: 350 |  Loss: (0.1242) |  Loss2: (0.0000) | Acc: (95.00%) (43046/44928)
Epoch: 155 | Batch_idx: 360 |  Loss: (0.1244) |  Loss2: (0.0000) | Acc: (95.00%) (44274/46208)
Epoch: 155 | Batch_idx: 370 |  Loss: (0.1242) |  Loss2: (0.0000) | Acc: (95.00%) (45506/47488)
Epoch: 155 | Batch_idx: 380 |  Loss: (0.1240) |  Loss2: (0.0000) | Acc: (95.00%) (46730/48768)
Epoch: 155 | Batch_idx: 390 |  Loss: (0.1239) |  Loss2: (0.0000) | Acc: (95.00%) (47919/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_155.pth.tar'
# TEST : Loss: (0.4067) | Acc: (88.00%) (8815/10000)
percent tensor([0.5531, 0.5648, 0.5533, 0.5244, 0.5606, 0.5669, 0.5714, 0.5412, 0.5542,
        0.5605, 0.5674, 0.5633, 0.5535, 0.5293, 0.5658, 0.5465],
       device='cuda:0') torch.Size([16])
percent tensor([0.5497, 0.5523, 0.5565, 0.5588, 0.5583, 0.5491, 0.5544, 0.5595, 0.5538,
        0.5533, 0.5499, 0.5544, 0.5505, 0.5558, 0.5501, 0.5521],
       device='cuda:0') torch.Size([16])
percent tensor([0.6526, 0.6645, 0.4906, 0.4434, 0.4389, 0.5944, 0.6115, 0.5005, 0.5491,
        0.6294, 0.6426, 0.5461, 0.6630, 0.6305, 0.6271, 0.6311],
       device='cuda:0') torch.Size([16])
percent tensor([0.7658, 0.6544, 0.7478, 0.7509, 0.7422, 0.8237, 0.7184, 0.7532, 0.7089,
        0.6869, 0.7071, 0.6959, 0.6511, 0.7323, 0.7434, 0.8103],
       device='cuda:0') torch.Size([16])
percent tensor([0.6383, 0.6422, 0.6280, 0.5701, 0.6092, 0.5642, 0.6581, 0.6341, 0.6389,
        0.6351, 0.6093, 0.5974, 0.6300, 0.6382, 0.6552, 0.5973],
       device='cuda:0') torch.Size([16])
percent tensor([0.5228, 0.7036, 0.6414, 0.7046, 0.6984, 0.7243, 0.6829, 0.5522, 0.7298,
        0.5829, 0.7175, 0.7579, 0.6654, 0.7684, 0.5756, 0.5440],
       device='cuda:0') torch.Size([16])
percent tensor([0.6394, 0.6978, 0.5892, 0.5252, 0.6758, 0.7713, 0.6248, 0.4355, 0.7290,
        0.6843, 0.7227, 0.4848, 0.7100, 0.6832, 0.4395, 0.5903],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9997, 0.9999, 0.9998, 0.9999, 0.9997, 0.9998, 0.9996, 0.9998,
        0.9995, 0.9999, 0.9999, 0.9999, 0.9996, 0.9994, 0.9993],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 156 | Batch_idx: 0 |  Loss: (0.0959) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 156 | Batch_idx: 10 |  Loss: (0.1179) |  Loss2: (0.0000) | Acc: (95.00%) (1350/1408)
Epoch: 156 | Batch_idx: 20 |  Loss: (0.1071) |  Loss2: (0.0000) | Acc: (96.00%) (2596/2688)
Epoch: 156 | Batch_idx: 30 |  Loss: (0.1155) |  Loss2: (0.0000) | Acc: (96.00%) (3823/3968)
Epoch: 156 | Batch_idx: 40 |  Loss: (0.1124) |  Loss2: (0.0000) | Acc: (96.00%) (5060/5248)
Epoch: 156 | Batch_idx: 50 |  Loss: (0.1123) |  Loss2: (0.0000) | Acc: (96.00%) (6288/6528)
Epoch: 156 | Batch_idx: 60 |  Loss: (0.1086) |  Loss2: (0.0000) | Acc: (96.00%) (7528/7808)
Epoch: 156 | Batch_idx: 70 |  Loss: (0.1111) |  Loss2: (0.0000) | Acc: (96.00%) (8755/9088)
Epoch: 156 | Batch_idx: 80 |  Loss: (0.1117) |  Loss2: (0.0000) | Acc: (96.00%) (9984/10368)
Epoch: 156 | Batch_idx: 90 |  Loss: (0.1130) |  Loss2: (0.0000) | Acc: (96.00%) (11212/11648)
Epoch: 156 | Batch_idx: 100 |  Loss: (0.1119) |  Loss2: (0.0000) | Acc: (96.00%) (12447/12928)
Epoch: 156 | Batch_idx: 110 |  Loss: (0.1120) |  Loss2: (0.0000) | Acc: (96.00%) (13675/14208)
Epoch: 156 | Batch_idx: 120 |  Loss: (0.1110) |  Loss2: (0.0000) | Acc: (96.00%) (14913/15488)
Epoch: 156 | Batch_idx: 130 |  Loss: (0.1102) |  Loss2: (0.0000) | Acc: (96.00%) (16147/16768)
Epoch: 156 | Batch_idx: 140 |  Loss: (0.1112) |  Loss2: (0.0000) | Acc: (96.00%) (17373/18048)
Epoch: 156 | Batch_idx: 150 |  Loss: (0.1121) |  Loss2: (0.0000) | Acc: (96.00%) (18598/19328)
Epoch: 156 | Batch_idx: 160 |  Loss: (0.1128) |  Loss2: (0.0000) | Acc: (96.00%) (19819/20608)
Epoch: 156 | Batch_idx: 170 |  Loss: (0.1133) |  Loss2: (0.0000) | Acc: (96.00%) (21042/21888)
Epoch: 156 | Batch_idx: 180 |  Loss: (0.1142) |  Loss2: (0.0000) | Acc: (96.00%) (22260/23168)
Epoch: 156 | Batch_idx: 190 |  Loss: (0.1136) |  Loss2: (0.0000) | Acc: (96.00%) (23500/24448)
Epoch: 156 | Batch_idx: 200 |  Loss: (0.1129) |  Loss2: (0.0000) | Acc: (96.00%) (24735/25728)
Epoch: 156 | Batch_idx: 210 |  Loss: (0.1134) |  Loss2: (0.0000) | Acc: (96.00%) (25959/27008)
Epoch: 156 | Batch_idx: 220 |  Loss: (0.1139) |  Loss2: (0.0000) | Acc: (96.00%) (27182/28288)
Epoch: 156 | Batch_idx: 230 |  Loss: (0.1143) |  Loss2: (0.0000) | Acc: (96.00%) (28414/29568)
Epoch: 156 | Batch_idx: 240 |  Loss: (0.1145) |  Loss2: (0.0000) | Acc: (96.00%) (29641/30848)
Epoch: 156 | Batch_idx: 250 |  Loss: (0.1144) |  Loss2: (0.0000) | Acc: (96.00%) (30872/32128)
Epoch: 156 | Batch_idx: 260 |  Loss: (0.1146) |  Loss2: (0.0000) | Acc: (96.00%) (32097/33408)
Epoch: 156 | Batch_idx: 270 |  Loss: (0.1150) |  Loss2: (0.0000) | Acc: (96.00%) (33328/34688)
Epoch: 156 | Batch_idx: 280 |  Loss: (0.1153) |  Loss2: (0.0000) | Acc: (96.00%) (34554/35968)
Epoch: 156 | Batch_idx: 290 |  Loss: (0.1160) |  Loss2: (0.0000) | Acc: (96.00%) (35779/37248)
Epoch: 156 | Batch_idx: 300 |  Loss: (0.1161) |  Loss2: (0.0000) | Acc: (96.00%) (36999/38528)
Epoch: 156 | Batch_idx: 310 |  Loss: (0.1158) |  Loss2: (0.0000) | Acc: (96.00%) (38228/39808)
Epoch: 156 | Batch_idx: 320 |  Loss: (0.1153) |  Loss2: (0.0000) | Acc: (96.00%) (39464/41088)
Epoch: 156 | Batch_idx: 330 |  Loss: (0.1153) |  Loss2: (0.0000) | Acc: (96.00%) (40693/42368)
Epoch: 156 | Batch_idx: 340 |  Loss: (0.1162) |  Loss2: (0.0000) | Acc: (96.00%) (41918/43648)
Epoch: 156 | Batch_idx: 350 |  Loss: (0.1164) |  Loss2: (0.0000) | Acc: (96.00%) (43143/44928)
Epoch: 156 | Batch_idx: 360 |  Loss: (0.1172) |  Loss2: (0.0000) | Acc: (96.00%) (44363/46208)
Epoch: 156 | Batch_idx: 370 |  Loss: (0.1173) |  Loss2: (0.0000) | Acc: (95.00%) (45588/47488)
Epoch: 156 | Batch_idx: 380 |  Loss: (0.1175) |  Loss2: (0.0000) | Acc: (95.00%) (46811/48768)
Epoch: 156 | Batch_idx: 390 |  Loss: (0.1174) |  Loss2: (0.0000) | Acc: (95.00%) (47996/50000)
# TEST : Loss: (0.4099) | Acc: (88.00%) (8832/10000)
percent tensor([0.5520, 0.5657, 0.5513, 0.5230, 0.5593, 0.5664, 0.5725, 0.5398, 0.5532,
        0.5604, 0.5665, 0.5614, 0.5523, 0.5320, 0.5659, 0.5466],
       device='cuda:0') torch.Size([16])
percent tensor([0.5490, 0.5530, 0.5553, 0.5587, 0.5580, 0.5480, 0.5537, 0.5596, 0.5540,
        0.5524, 0.5506, 0.5532, 0.5500, 0.5563, 0.5498, 0.5523],
       device='cuda:0') torch.Size([16])
percent tensor([0.6617, 0.6766, 0.4629, 0.4476, 0.4245, 0.6144, 0.6181, 0.4835, 0.5550,
        0.6377, 0.6611, 0.5296, 0.6755, 0.6422, 0.6467, 0.6465],
       device='cuda:0') torch.Size([16])
percent tensor([0.7617, 0.6642, 0.7388, 0.7497, 0.7375, 0.8230, 0.7149, 0.7427, 0.7094,
        0.6940, 0.7206, 0.6845, 0.6528, 0.7349, 0.7531, 0.8134],
       device='cuda:0') torch.Size([16])
percent tensor([0.6474, 0.6523, 0.6327, 0.5684, 0.6073, 0.5655, 0.6663, 0.6412, 0.6379,
        0.6461, 0.6169, 0.6040, 0.6312, 0.6312, 0.6506, 0.6017],
       device='cuda:0') torch.Size([16])
percent tensor([0.5371, 0.6867, 0.6617, 0.7114, 0.7049, 0.7326, 0.6895, 0.5878, 0.7218,
        0.5886, 0.7118, 0.7545, 0.6752, 0.7610, 0.5850, 0.5381],
       device='cuda:0') torch.Size([16])
percent tensor([0.6770, 0.7070, 0.6407, 0.5645, 0.6904, 0.7756, 0.6423, 0.4602, 0.7233,
        0.7060, 0.7318, 0.5254, 0.7228, 0.6639, 0.5002, 0.6181],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9997, 0.9999, 0.9998, 0.9999, 0.9998, 0.9999, 0.9997, 0.9998,
        0.9997, 0.9999, 0.9999, 0.9999, 0.9997, 0.9995, 0.9997],
       device='cuda:0') torch.Size([16])
Epoch: 157 | Batch_idx: 0 |  Loss: (0.0794) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 157 | Batch_idx: 10 |  Loss: (0.1191) |  Loss2: (0.0000) | Acc: (95.00%) (1342/1408)
Epoch: 157 | Batch_idx: 20 |  Loss: (0.1202) |  Loss2: (0.0000) | Acc: (95.00%) (2570/2688)
Epoch: 157 | Batch_idx: 30 |  Loss: (0.1210) |  Loss2: (0.0000) | Acc: (95.00%) (3790/3968)
Epoch: 157 | Batch_idx: 40 |  Loss: (0.1189) |  Loss2: (0.0000) | Acc: (95.00%) (5022/5248)
Epoch: 157 | Batch_idx: 50 |  Loss: (0.1161) |  Loss2: (0.0000) | Acc: (95.00%) (6259/6528)
Epoch: 157 | Batch_idx: 60 |  Loss: (0.1153) |  Loss2: (0.0000) | Acc: (95.00%) (7488/7808)
Epoch: 157 | Batch_idx: 70 |  Loss: (0.1150) |  Loss2: (0.0000) | Acc: (95.00%) (8716/9088)
Epoch: 157 | Batch_idx: 80 |  Loss: (0.1150) |  Loss2: (0.0000) | Acc: (95.00%) (9944/10368)
Epoch: 157 | Batch_idx: 90 |  Loss: (0.1146) |  Loss2: (0.0000) | Acc: (95.00%) (11179/11648)
Epoch: 157 | Batch_idx: 100 |  Loss: (0.1140) |  Loss2: (0.0000) | Acc: (96.00%) (12413/12928)
Epoch: 157 | Batch_idx: 110 |  Loss: (0.1130) |  Loss2: (0.0000) | Acc: (96.00%) (13644/14208)
Epoch: 157 | Batch_idx: 120 |  Loss: (0.1126) |  Loss2: (0.0000) | Acc: (96.00%) (14876/15488)
Epoch: 157 | Batch_idx: 130 |  Loss: (0.1132) |  Loss2: (0.0000) | Acc: (96.00%) (16106/16768)
Epoch: 157 | Batch_idx: 140 |  Loss: (0.1129) |  Loss2: (0.0000) | Acc: (96.00%) (17347/18048)
Epoch: 157 | Batch_idx: 150 |  Loss: (0.1139) |  Loss2: (0.0000) | Acc: (96.00%) (18573/19328)
Epoch: 157 | Batch_idx: 160 |  Loss: (0.1138) |  Loss2: (0.0000) | Acc: (96.00%) (19805/20608)
Epoch: 157 | Batch_idx: 170 |  Loss: (0.1140) |  Loss2: (0.0000) | Acc: (96.00%) (21041/21888)
Epoch: 157 | Batch_idx: 180 |  Loss: (0.1131) |  Loss2: (0.0000) | Acc: (96.00%) (22276/23168)
Epoch: 157 | Batch_idx: 190 |  Loss: (0.1130) |  Loss2: (0.0000) | Acc: (96.00%) (23513/24448)
Epoch: 157 | Batch_idx: 200 |  Loss: (0.1126) |  Loss2: (0.0000) | Acc: (96.00%) (24747/25728)
Epoch: 157 | Batch_idx: 210 |  Loss: (0.1121) |  Loss2: (0.0000) | Acc: (96.00%) (25985/27008)
Epoch: 157 | Batch_idx: 220 |  Loss: (0.1126) |  Loss2: (0.0000) | Acc: (96.00%) (27213/28288)
Epoch: 157 | Batch_idx: 230 |  Loss: (0.1136) |  Loss2: (0.0000) | Acc: (96.00%) (28432/29568)
Epoch: 157 | Batch_idx: 240 |  Loss: (0.1138) |  Loss2: (0.0000) | Acc: (96.00%) (29662/30848)
Epoch: 157 | Batch_idx: 250 |  Loss: (0.1139) |  Loss2: (0.0000) | Acc: (96.00%) (30889/32128)
Epoch: 157 | Batch_idx: 260 |  Loss: (0.1140) |  Loss2: (0.0000) | Acc: (96.00%) (32118/33408)
Epoch: 157 | Batch_idx: 270 |  Loss: (0.1141) |  Loss2: (0.0000) | Acc: (96.00%) (33348/34688)
Epoch: 157 | Batch_idx: 280 |  Loss: (0.1143) |  Loss2: (0.0000) | Acc: (96.00%) (34576/35968)
Epoch: 157 | Batch_idx: 290 |  Loss: (0.1152) |  Loss2: (0.0000) | Acc: (96.00%) (35786/37248)
Epoch: 157 | Batch_idx: 300 |  Loss: (0.1156) |  Loss2: (0.0000) | Acc: (96.00%) (37006/38528)
Epoch: 157 | Batch_idx: 310 |  Loss: (0.1163) |  Loss2: (0.0000) | Acc: (96.00%) (38228/39808)
Epoch: 157 | Batch_idx: 320 |  Loss: (0.1169) |  Loss2: (0.0000) | Acc: (95.00%) (39441/41088)
Epoch: 157 | Batch_idx: 330 |  Loss: (0.1170) |  Loss2: (0.0000) | Acc: (95.00%) (40670/42368)
Epoch: 157 | Batch_idx: 340 |  Loss: (0.1173) |  Loss2: (0.0000) | Acc: (95.00%) (41895/43648)
Epoch: 157 | Batch_idx: 350 |  Loss: (0.1170) |  Loss2: (0.0000) | Acc: (95.00%) (43126/44928)
Epoch: 157 | Batch_idx: 360 |  Loss: (0.1174) |  Loss2: (0.0000) | Acc: (95.00%) (44352/46208)
Epoch: 157 | Batch_idx: 370 |  Loss: (0.1168) |  Loss2: (0.0000) | Acc: (96.00%) (45593/47488)
Epoch: 157 | Batch_idx: 380 |  Loss: (0.1167) |  Loss2: (0.0000) | Acc: (96.00%) (46821/48768)
Epoch: 157 | Batch_idx: 390 |  Loss: (0.1167) |  Loss2: (0.0000) | Acc: (96.00%) (48000/50000)
# TEST : Loss: (0.4419) | Acc: (87.00%) (8765/10000)
percent tensor([0.5516, 0.5676, 0.5502, 0.5243, 0.5593, 0.5669, 0.5729, 0.5416, 0.5531,
        0.5615, 0.5670, 0.5611, 0.5528, 0.5335, 0.5674, 0.5478],
       device='cuda:0') torch.Size([16])
percent tensor([0.5486, 0.5513, 0.5546, 0.5569, 0.5565, 0.5470, 0.5527, 0.5586, 0.5537,
        0.5514, 0.5500, 0.5521, 0.5494, 0.5552, 0.5487, 0.5511],
       device='cuda:0') torch.Size([16])
percent tensor([0.6681, 0.6807, 0.4700, 0.4461, 0.4401, 0.6129, 0.6273, 0.4957, 0.5767,
        0.6497, 0.6696, 0.5461, 0.6794, 0.6568, 0.6471, 0.6479],
       device='cuda:0') torch.Size([16])
percent tensor([0.7580, 0.6506, 0.7409, 0.7418, 0.7388, 0.8143, 0.7101, 0.7486, 0.7023,
        0.6949, 0.7044, 0.6814, 0.6378, 0.7382, 0.7383, 0.8027],
       device='cuda:0') torch.Size([16])
percent tensor([0.6555, 0.6577, 0.6310, 0.5795, 0.6123, 0.5848, 0.6707, 0.6436, 0.6486,
        0.6450, 0.6272, 0.6082, 0.6455, 0.6327, 0.6647, 0.6138],
       device='cuda:0') torch.Size([16])
percent tensor([0.5276, 0.6685, 0.6865, 0.7207, 0.7147, 0.7245, 0.6815, 0.6007, 0.7135,
        0.5767, 0.6854, 0.7586, 0.6479, 0.7287, 0.5886, 0.5050],
       device='cuda:0') torch.Size([16])
percent tensor([0.6514, 0.6880, 0.6438, 0.5732, 0.6793, 0.7742, 0.6148, 0.4860, 0.6869,
        0.6809, 0.6926, 0.4974, 0.6882, 0.6445, 0.4765, 0.5789],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9999, 0.9999, 0.9998, 0.9999, 0.9995, 0.9999, 0.9997, 0.9998,
        0.9997, 0.9999, 0.9999, 0.9999, 0.9997, 0.9995, 0.9991],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 158 | Batch_idx: 0 |  Loss: (0.0919) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 158 | Batch_idx: 10 |  Loss: (0.1151) |  Loss2: (0.0000) | Acc: (96.00%) (1354/1408)
Epoch: 158 | Batch_idx: 20 |  Loss: (0.1179) |  Loss2: (0.0000) | Acc: (95.00%) (2579/2688)
Epoch: 158 | Batch_idx: 30 |  Loss: (0.1151) |  Loss2: (0.0000) | Acc: (96.00%) (3810/3968)
Epoch: 158 | Batch_idx: 40 |  Loss: (0.1237) |  Loss2: (0.0000) | Acc: (95.00%) (5030/5248)
Epoch: 158 | Batch_idx: 50 |  Loss: (0.1231) |  Loss2: (0.0000) | Acc: (95.00%) (6260/6528)
Epoch: 158 | Batch_idx: 60 |  Loss: (0.1247) |  Loss2: (0.0000) | Acc: (95.00%) (7481/7808)
Epoch: 158 | Batch_idx: 70 |  Loss: (0.1266) |  Loss2: (0.0000) | Acc: (95.00%) (8702/9088)
Epoch: 158 | Batch_idx: 80 |  Loss: (0.1276) |  Loss2: (0.0000) | Acc: (95.00%) (9925/10368)
Epoch: 158 | Batch_idx: 90 |  Loss: (0.1261) |  Loss2: (0.0000) | Acc: (95.00%) (11156/11648)
Epoch: 158 | Batch_idx: 100 |  Loss: (0.1253) |  Loss2: (0.0000) | Acc: (95.00%) (12388/12928)
Epoch: 158 | Batch_idx: 110 |  Loss: (0.1262) |  Loss2: (0.0000) | Acc: (95.00%) (13601/14208)
Epoch: 158 | Batch_idx: 120 |  Loss: (0.1259) |  Loss2: (0.0000) | Acc: (95.00%) (14828/15488)
Epoch: 158 | Batch_idx: 130 |  Loss: (0.1260) |  Loss2: (0.0000) | Acc: (95.00%) (16051/16768)
Epoch: 158 | Batch_idx: 140 |  Loss: (0.1276) |  Loss2: (0.0000) | Acc: (95.00%) (17261/18048)
Epoch: 158 | Batch_idx: 150 |  Loss: (0.1271) |  Loss2: (0.0000) | Acc: (95.00%) (18487/19328)
Epoch: 158 | Batch_idx: 160 |  Loss: (0.1262) |  Loss2: (0.0000) | Acc: (95.00%) (19713/20608)
Epoch: 158 | Batch_idx: 170 |  Loss: (0.1267) |  Loss2: (0.0000) | Acc: (95.00%) (20932/21888)
Epoch: 158 | Batch_idx: 180 |  Loss: (0.1266) |  Loss2: (0.0000) | Acc: (95.00%) (22154/23168)
Epoch: 158 | Batch_idx: 190 |  Loss: (0.1272) |  Loss2: (0.0000) | Acc: (95.00%) (23369/24448)
Epoch: 158 | Batch_idx: 200 |  Loss: (0.1280) |  Loss2: (0.0000) | Acc: (95.00%) (24582/25728)
Epoch: 158 | Batch_idx: 210 |  Loss: (0.1285) |  Loss2: (0.0000) | Acc: (95.00%) (25799/27008)
Epoch: 158 | Batch_idx: 220 |  Loss: (0.1283) |  Loss2: (0.0000) | Acc: (95.00%) (27026/28288)
Epoch: 158 | Batch_idx: 230 |  Loss: (0.1284) |  Loss2: (0.0000) | Acc: (95.00%) (28244/29568)
Epoch: 158 | Batch_idx: 240 |  Loss: (0.1276) |  Loss2: (0.0000) | Acc: (95.00%) (29476/30848)
Epoch: 158 | Batch_idx: 250 |  Loss: (0.1273) |  Loss2: (0.0000) | Acc: (95.00%) (30693/32128)
Epoch: 158 | Batch_idx: 260 |  Loss: (0.1277) |  Loss2: (0.0000) | Acc: (95.00%) (31915/33408)
Epoch: 158 | Batch_idx: 270 |  Loss: (0.1274) |  Loss2: (0.0000) | Acc: (95.00%) (33135/34688)
Epoch: 158 | Batch_idx: 280 |  Loss: (0.1279) |  Loss2: (0.0000) | Acc: (95.00%) (34345/35968)
Epoch: 158 | Batch_idx: 290 |  Loss: (0.1280) |  Loss2: (0.0000) | Acc: (95.00%) (35568/37248)
Epoch: 158 | Batch_idx: 300 |  Loss: (0.1274) |  Loss2: (0.0000) | Acc: (95.00%) (36808/38528)
Epoch: 158 | Batch_idx: 310 |  Loss: (0.1277) |  Loss2: (0.0000) | Acc: (95.00%) (38024/39808)
Epoch: 158 | Batch_idx: 320 |  Loss: (0.1275) |  Loss2: (0.0000) | Acc: (95.00%) (39246/41088)
Epoch: 158 | Batch_idx: 330 |  Loss: (0.1262) |  Loss2: (0.0000) | Acc: (95.00%) (40497/42368)
Epoch: 158 | Batch_idx: 340 |  Loss: (0.1261) |  Loss2: (0.0000) | Acc: (95.00%) (41731/43648)
Epoch: 158 | Batch_idx: 350 |  Loss: (0.1259) |  Loss2: (0.0000) | Acc: (95.00%) (42956/44928)
Epoch: 158 | Batch_idx: 360 |  Loss: (0.1259) |  Loss2: (0.0000) | Acc: (95.00%) (44185/46208)
Epoch: 158 | Batch_idx: 370 |  Loss: (0.1249) |  Loss2: (0.0000) | Acc: (95.00%) (45426/47488)
Epoch: 158 | Batch_idx: 380 |  Loss: (0.1246) |  Loss2: (0.0000) | Acc: (95.00%) (46664/48768)
Epoch: 158 | Batch_idx: 390 |  Loss: (0.1255) |  Loss2: (0.0000) | Acc: (95.00%) (47829/50000)
# TEST : Loss: (0.3967) | Acc: (88.00%) (8854/10000)
percent tensor([0.5456, 0.5599, 0.5433, 0.5168, 0.5523, 0.5608, 0.5653, 0.5335, 0.5459,
        0.5539, 0.5605, 0.5534, 0.5459, 0.5247, 0.5604, 0.5410],
       device='cuda:0') torch.Size([16])
percent tensor([0.5452, 0.5483, 0.5510, 0.5535, 0.5530, 0.5439, 0.5498, 0.5554, 0.5505,
        0.5484, 0.5469, 0.5485, 0.5464, 0.5521, 0.5459, 0.5480],
       device='cuda:0') torch.Size([16])
percent tensor([0.6443, 0.6605, 0.4370, 0.4078, 0.4134, 0.5924, 0.6001, 0.4688, 0.5435,
        0.6125, 0.6368, 0.5101, 0.6523, 0.6252, 0.6237, 0.6163],
       device='cuda:0') torch.Size([16])
percent tensor([0.7324, 0.6084, 0.7234, 0.7200, 0.7210, 0.7988, 0.6783, 0.7369, 0.6840,
        0.6547, 0.6656, 0.6534, 0.5992, 0.7126, 0.7059, 0.7734],
       device='cuda:0') torch.Size([16])
percent tensor([0.6554, 0.6576, 0.6392, 0.5892, 0.6206, 0.5860, 0.6766, 0.6495, 0.6569,
        0.6429, 0.6264, 0.6191, 0.6482, 0.6378, 0.6700, 0.6101],
       device='cuda:0') torch.Size([16])
percent tensor([0.5502, 0.6996, 0.7037, 0.7317, 0.7201, 0.7425, 0.7088, 0.6026, 0.7407,
        0.6133, 0.7196, 0.7724, 0.6908, 0.7512, 0.6111, 0.5209],
       device='cuda:0') torch.Size([16])
percent tensor([0.6159, 0.6591, 0.6168, 0.5263, 0.6678, 0.7355, 0.6018, 0.4607, 0.6392,
        0.6499, 0.6470, 0.4455, 0.6339, 0.5892, 0.4484, 0.5500],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9998, 0.9999, 0.9999, 0.9999, 0.9996, 0.9999, 0.9997, 0.9998,
        0.9997, 0.9998, 0.9998, 0.9999, 0.9994, 0.9995, 0.9995],
       device='cuda:0') torch.Size([16])
Epoch: 159 | Batch_idx: 0 |  Loss: (0.1561) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 159 | Batch_idx: 10 |  Loss: (0.1194) |  Loss2: (0.0000) | Acc: (96.00%) (1353/1408)
Epoch: 159 | Batch_idx: 20 |  Loss: (0.1129) |  Loss2: (0.0000) | Acc: (96.00%) (2590/2688)
Epoch: 159 | Batch_idx: 30 |  Loss: (0.1113) |  Loss2: (0.0000) | Acc: (96.00%) (3827/3968)
Epoch: 159 | Batch_idx: 40 |  Loss: (0.1138) |  Loss2: (0.0000) | Acc: (96.00%) (5054/5248)
Epoch: 159 | Batch_idx: 50 |  Loss: (0.1124) |  Loss2: (0.0000) | Acc: (96.00%) (6292/6528)
Epoch: 159 | Batch_idx: 60 |  Loss: (0.1135) |  Loss2: (0.0000) | Acc: (96.00%) (7508/7808)
Epoch: 159 | Batch_idx: 70 |  Loss: (0.1155) |  Loss2: (0.0000) | Acc: (96.00%) (8727/9088)
Epoch: 159 | Batch_idx: 80 |  Loss: (0.1137) |  Loss2: (0.0000) | Acc: (96.00%) (9964/10368)
Epoch: 159 | Batch_idx: 90 |  Loss: (0.1135) |  Loss2: (0.0000) | Acc: (96.00%) (11192/11648)
Epoch: 159 | Batch_idx: 100 |  Loss: (0.1141) |  Loss2: (0.0000) | Acc: (96.00%) (12424/12928)
Epoch: 159 | Batch_idx: 110 |  Loss: (0.1122) |  Loss2: (0.0000) | Acc: (96.00%) (13668/14208)
Epoch: 159 | Batch_idx: 120 |  Loss: (0.1117) |  Loss2: (0.0000) | Acc: (96.00%) (14897/15488)
Epoch: 159 | Batch_idx: 130 |  Loss: (0.1115) |  Loss2: (0.0000) | Acc: (96.00%) (16122/16768)
Epoch: 159 | Batch_idx: 140 |  Loss: (0.1123) |  Loss2: (0.0000) | Acc: (96.00%) (17347/18048)
Epoch: 159 | Batch_idx: 150 |  Loss: (0.1109) |  Loss2: (0.0000) | Acc: (96.00%) (18593/19328)
Epoch: 159 | Batch_idx: 160 |  Loss: (0.1121) |  Loss2: (0.0000) | Acc: (96.00%) (19826/20608)
Epoch: 159 | Batch_idx: 170 |  Loss: (0.1112) |  Loss2: (0.0000) | Acc: (96.00%) (21064/21888)
Epoch: 159 | Batch_idx: 180 |  Loss: (0.1124) |  Loss2: (0.0000) | Acc: (96.00%) (22284/23168)
Epoch: 159 | Batch_idx: 190 |  Loss: (0.1119) |  Loss2: (0.0000) | Acc: (96.00%) (23515/24448)
Epoch: 159 | Batch_idx: 200 |  Loss: (0.1125) |  Loss2: (0.0000) | Acc: (96.00%) (24737/25728)
Epoch: 159 | Batch_idx: 210 |  Loss: (0.1121) |  Loss2: (0.0000) | Acc: (96.00%) (25965/27008)
Epoch: 159 | Batch_idx: 220 |  Loss: (0.1125) |  Loss2: (0.0000) | Acc: (96.00%) (27189/28288)
Epoch: 159 | Batch_idx: 230 |  Loss: (0.1125) |  Loss2: (0.0000) | Acc: (96.00%) (28421/29568)
Epoch: 159 | Batch_idx: 240 |  Loss: (0.1117) |  Loss2: (0.0000) | Acc: (96.00%) (29664/30848)
Epoch: 159 | Batch_idx: 250 |  Loss: (0.1114) |  Loss2: (0.0000) | Acc: (96.00%) (30897/32128)
Epoch: 159 | Batch_idx: 260 |  Loss: (0.1119) |  Loss2: (0.0000) | Acc: (96.00%) (32119/33408)
Epoch: 159 | Batch_idx: 270 |  Loss: (0.1116) |  Loss2: (0.0000) | Acc: (96.00%) (33358/34688)
Epoch: 159 | Batch_idx: 280 |  Loss: (0.1116) |  Loss2: (0.0000) | Acc: (96.00%) (34594/35968)
Epoch: 159 | Batch_idx: 290 |  Loss: (0.1118) |  Loss2: (0.0000) | Acc: (96.00%) (35826/37248)
Epoch: 159 | Batch_idx: 300 |  Loss: (0.1121) |  Loss2: (0.0000) | Acc: (96.00%) (37054/38528)
Epoch: 159 | Batch_idx: 310 |  Loss: (0.1119) |  Loss2: (0.0000) | Acc: (96.00%) (38280/39808)
Epoch: 159 | Batch_idx: 320 |  Loss: (0.1124) |  Loss2: (0.0000) | Acc: (96.00%) (39492/41088)
Epoch: 159 | Batch_idx: 330 |  Loss: (0.1127) |  Loss2: (0.0000) | Acc: (96.00%) (40718/42368)
Epoch: 159 | Batch_idx: 340 |  Loss: (0.1125) |  Loss2: (0.0000) | Acc: (96.00%) (41950/43648)
Epoch: 159 | Batch_idx: 350 |  Loss: (0.1122) |  Loss2: (0.0000) | Acc: (96.00%) (43187/44928)
Epoch: 159 | Batch_idx: 360 |  Loss: (0.1123) |  Loss2: (0.0000) | Acc: (96.00%) (44417/46208)
Epoch: 159 | Batch_idx: 370 |  Loss: (0.1120) |  Loss2: (0.0000) | Acc: (96.00%) (45655/47488)
Epoch: 159 | Batch_idx: 380 |  Loss: (0.1118) |  Loss2: (0.0000) | Acc: (96.00%) (46885/48768)
Epoch: 159 | Batch_idx: 390 |  Loss: (0.1118) |  Loss2: (0.0000) | Acc: (96.00%) (48072/50000)
# TEST : Loss: (0.3854) | Acc: (88.00%) (8895/10000)
percent tensor([0.5503, 0.5652, 0.5466, 0.5203, 0.5566, 0.5656, 0.5706, 0.5375, 0.5504,
        0.5584, 0.5655, 0.5573, 0.5505, 0.5294, 0.5655, 0.5458],
       device='cuda:0') torch.Size([16])
percent tensor([0.5443, 0.5477, 0.5502, 0.5525, 0.5523, 0.5432, 0.5492, 0.5546, 0.5495,
        0.5476, 0.5461, 0.5477, 0.5457, 0.5509, 0.5454, 0.5471],
       device='cuda:0') torch.Size([16])
percent tensor([0.6622, 0.6734, 0.4583, 0.4277, 0.4326, 0.6128, 0.6184, 0.4876, 0.5601,
        0.6304, 0.6523, 0.5296, 0.6700, 0.6411, 0.6433, 0.6362],
       device='cuda:0') torch.Size([16])
percent tensor([0.7301, 0.6006, 0.7218, 0.7176, 0.7216, 0.7979, 0.6738, 0.7358, 0.6802,
        0.6466, 0.6578, 0.6483, 0.5921, 0.7073, 0.7035, 0.7699],
       device='cuda:0') torch.Size([16])
percent tensor([0.6506, 0.6564, 0.6402, 0.5860, 0.6168, 0.5808, 0.6735, 0.6467, 0.6575,
        0.6384, 0.6234, 0.6160, 0.6477, 0.6342, 0.6633, 0.6017],
       device='cuda:0') torch.Size([16])
percent tensor([0.5463, 0.7089, 0.6957, 0.7311, 0.7115, 0.7452, 0.7094, 0.5872, 0.7358,
        0.6155, 0.7247, 0.7721, 0.6982, 0.7585, 0.6108, 0.5200],
       device='cuda:0') torch.Size([16])
percent tensor([0.6249, 0.6683, 0.6208, 0.5367, 0.6793, 0.7351, 0.6188, 0.4758, 0.6382,
        0.6625, 0.6584, 0.4495, 0.6364, 0.5970, 0.4568, 0.5626],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9998, 0.9999, 0.9999, 0.9999, 0.9996, 0.9999, 0.9997, 0.9998,
        0.9997, 0.9998, 0.9999, 0.9999, 0.9995, 0.9995, 0.9994],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 160 | Batch_idx: 0 |  Loss: (0.0887) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 160 | Batch_idx: 10 |  Loss: (0.1038) |  Loss2: (0.0000) | Acc: (96.00%) (1359/1408)
Epoch: 160 | Batch_idx: 20 |  Loss: (0.1026) |  Loss2: (0.0000) | Acc: (96.00%) (2593/2688)
Epoch: 160 | Batch_idx: 30 |  Loss: (0.1045) |  Loss2: (0.0000) | Acc: (96.00%) (3823/3968)
Epoch: 160 | Batch_idx: 40 |  Loss: (0.1039) |  Loss2: (0.0000) | Acc: (96.00%) (5061/5248)
Epoch: 160 | Batch_idx: 50 |  Loss: (0.1020) |  Loss2: (0.0000) | Acc: (96.00%) (6311/6528)
Epoch: 160 | Batch_idx: 60 |  Loss: (0.1041) |  Loss2: (0.0000) | Acc: (96.00%) (7540/7808)
Epoch: 160 | Batch_idx: 70 |  Loss: (0.1049) |  Loss2: (0.0000) | Acc: (96.00%) (8774/9088)
Epoch: 160 | Batch_idx: 80 |  Loss: (0.1068) |  Loss2: (0.0000) | Acc: (96.00%) (9999/10368)
Epoch: 160 | Batch_idx: 90 |  Loss: (0.1081) |  Loss2: (0.0000) | Acc: (96.00%) (11224/11648)
Epoch: 160 | Batch_idx: 100 |  Loss: (0.1073) |  Loss2: (0.0000) | Acc: (96.00%) (12459/12928)
Epoch: 160 | Batch_idx: 110 |  Loss: (0.1081) |  Loss2: (0.0000) | Acc: (96.00%) (13693/14208)
Epoch: 160 | Batch_idx: 120 |  Loss: (0.1076) |  Loss2: (0.0000) | Acc: (96.00%) (14930/15488)
Epoch: 160 | Batch_idx: 130 |  Loss: (0.1062) |  Loss2: (0.0000) | Acc: (96.00%) (16173/16768)
Epoch: 160 | Batch_idx: 140 |  Loss: (0.1059) |  Loss2: (0.0000) | Acc: (96.00%) (17412/18048)
Epoch: 160 | Batch_idx: 150 |  Loss: (0.1045) |  Loss2: (0.0000) | Acc: (96.00%) (18656/19328)
Epoch: 160 | Batch_idx: 160 |  Loss: (0.1049) |  Loss2: (0.0000) | Acc: (96.00%) (19893/20608)
Epoch: 160 | Batch_idx: 170 |  Loss: (0.1056) |  Loss2: (0.0000) | Acc: (96.00%) (21118/21888)
Epoch: 160 | Batch_idx: 180 |  Loss: (0.1060) |  Loss2: (0.0000) | Acc: (96.00%) (22350/23168)
Epoch: 160 | Batch_idx: 190 |  Loss: (0.1059) |  Loss2: (0.0000) | Acc: (96.00%) (23587/24448)
Epoch: 160 | Batch_idx: 200 |  Loss: (0.1059) |  Loss2: (0.0000) | Acc: (96.00%) (24822/25728)
Epoch: 160 | Batch_idx: 210 |  Loss: (0.1073) |  Loss2: (0.0000) | Acc: (96.00%) (26044/27008)
Epoch: 160 | Batch_idx: 220 |  Loss: (0.1091) |  Loss2: (0.0000) | Acc: (96.00%) (27254/28288)
Epoch: 160 | Batch_idx: 230 |  Loss: (0.1098) |  Loss2: (0.0000) | Acc: (96.00%) (28479/29568)
Epoch: 160 | Batch_idx: 240 |  Loss: (0.1113) |  Loss2: (0.0000) | Acc: (96.00%) (29695/30848)
Epoch: 160 | Batch_idx: 250 |  Loss: (0.1125) |  Loss2: (0.0000) | Acc: (96.00%) (30911/32128)
Epoch: 160 | Batch_idx: 260 |  Loss: (0.1118) |  Loss2: (0.0000) | Acc: (96.00%) (32149/33408)
Epoch: 160 | Batch_idx: 270 |  Loss: (0.1123) |  Loss2: (0.0000) | Acc: (96.00%) (33366/34688)
Epoch: 160 | Batch_idx: 280 |  Loss: (0.1127) |  Loss2: (0.0000) | Acc: (96.00%) (34590/35968)
Epoch: 160 | Batch_idx: 290 |  Loss: (0.1129) |  Loss2: (0.0000) | Acc: (96.00%) (35828/37248)
Epoch: 160 | Batch_idx: 300 |  Loss: (0.1133) |  Loss2: (0.0000) | Acc: (96.00%) (37051/38528)
Epoch: 160 | Batch_idx: 310 |  Loss: (0.1128) |  Loss2: (0.0000) | Acc: (96.00%) (38283/39808)
Epoch: 160 | Batch_idx: 320 |  Loss: (0.1127) |  Loss2: (0.0000) | Acc: (96.00%) (39510/41088)
Epoch: 160 | Batch_idx: 330 |  Loss: (0.1130) |  Loss2: (0.0000) | Acc: (96.00%) (40735/42368)
Epoch: 160 | Batch_idx: 340 |  Loss: (0.1127) |  Loss2: (0.0000) | Acc: (96.00%) (41964/43648)
Epoch: 160 | Batch_idx: 350 |  Loss: (0.1125) |  Loss2: (0.0000) | Acc: (96.00%) (43201/44928)
Epoch: 160 | Batch_idx: 360 |  Loss: (0.1123) |  Loss2: (0.0000) | Acc: (96.00%) (44438/46208)
Epoch: 160 | Batch_idx: 370 |  Loss: (0.1125) |  Loss2: (0.0000) | Acc: (96.00%) (45663/47488)
Epoch: 160 | Batch_idx: 380 |  Loss: (0.1129) |  Loss2: (0.0000) | Acc: (96.00%) (46882/48768)
Epoch: 160 | Batch_idx: 390 |  Loss: (0.1134) |  Loss2: (0.0000) | Acc: (96.00%) (48051/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_160.pth.tar'
# TEST : Loss: (0.4225) | Acc: (87.00%) (8770/10000)
percent tensor([0.5525, 0.5642, 0.5529, 0.5230, 0.5627, 0.5689, 0.5728, 0.5387, 0.5514,
        0.5600, 0.5663, 0.5632, 0.5516, 0.5283, 0.5666, 0.5468],
       device='cuda:0') torch.Size([16])
percent tensor([0.5449, 0.5470, 0.5498, 0.5527, 0.5512, 0.5438, 0.5485, 0.5546, 0.5484,
        0.5476, 0.5450, 0.5477, 0.5458, 0.5493, 0.5455, 0.5475],
       device='cuda:0') torch.Size([16])
percent tensor([0.6620, 0.6726, 0.4772, 0.4334, 0.4414, 0.6079, 0.6152, 0.4909, 0.5592,
        0.6407, 0.6609, 0.5347, 0.6781, 0.6335, 0.6478, 0.6403],
       device='cuda:0') torch.Size([16])
percent tensor([0.7329, 0.6013, 0.7077, 0.7136, 0.7164, 0.7997, 0.6683, 0.7302, 0.6806,
        0.6426, 0.6658, 0.6463, 0.5964, 0.7046, 0.7086, 0.7761],
       device='cuda:0') torch.Size([16])
percent tensor([0.6497, 0.6612, 0.6503, 0.5961, 0.6229, 0.5842, 0.6773, 0.6491, 0.6497,
        0.6494, 0.6284, 0.6181, 0.6486, 0.6274, 0.6645, 0.6032],
       device='cuda:0') torch.Size([16])
percent tensor([0.5461, 0.7217, 0.6541, 0.7150, 0.6984, 0.7324, 0.7025, 0.5715, 0.7454,
        0.6395, 0.7504, 0.7761, 0.6986, 0.7608, 0.5981, 0.5624],
       device='cuda:0') torch.Size([16])
percent tensor([0.6355, 0.6746, 0.6114, 0.5386, 0.6768, 0.7183, 0.6301, 0.4936, 0.6656,
        0.6754, 0.6747, 0.4364, 0.6461, 0.5804, 0.4320, 0.5831],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9997, 0.9999, 0.9994, 0.9998,
        0.9998, 0.9999, 0.9999, 0.9999, 0.9996, 0.9996, 0.9996],
       device='cuda:0') torch.Size([16])
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(183.4336, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(827.9303, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(823.5935, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1512.3660, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(482.8665, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2285.2124, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4246.1006, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1368.5304, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6237.6313, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11671.0586, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3848.6958, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16244.8564, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 161 | Batch_idx: 0 |  Loss: (0.1303) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 161 | Batch_idx: 10 |  Loss: (0.1026) |  Loss2: (0.0000) | Acc: (96.00%) (1362/1408)
Epoch: 161 | Batch_idx: 20 |  Loss: (0.1114) |  Loss2: (0.0000) | Acc: (96.00%) (2594/2688)
Epoch: 161 | Batch_idx: 30 |  Loss: (0.1089) |  Loss2: (0.0000) | Acc: (96.00%) (3826/3968)
Epoch: 161 | Batch_idx: 40 |  Loss: (0.1084) |  Loss2: (0.0000) | Acc: (96.00%) (5054/5248)
Epoch: 161 | Batch_idx: 50 |  Loss: (0.1081) |  Loss2: (0.0000) | Acc: (96.00%) (6287/6528)
Epoch: 161 | Batch_idx: 60 |  Loss: (0.1053) |  Loss2: (0.0000) | Acc: (96.00%) (7523/7808)
Epoch: 161 | Batch_idx: 70 |  Loss: (0.1083) |  Loss2: (0.0000) | Acc: (96.00%) (8749/9088)
Epoch: 161 | Batch_idx: 80 |  Loss: (0.1087) |  Loss2: (0.0000) | Acc: (96.00%) (9976/10368)
Epoch: 161 | Batch_idx: 90 |  Loss: (0.1074) |  Loss2: (0.0000) | Acc: (96.00%) (11213/11648)
Epoch: 161 | Batch_idx: 100 |  Loss: (0.1084) |  Loss2: (0.0000) | Acc: (96.00%) (12438/12928)
Epoch: 161 | Batch_idx: 110 |  Loss: (0.1083) |  Loss2: (0.0000) | Acc: (96.00%) (13662/14208)
Epoch: 161 | Batch_idx: 120 |  Loss: (0.1083) |  Loss2: (0.0000) | Acc: (96.00%) (14891/15488)
Epoch: 161 | Batch_idx: 130 |  Loss: (0.1076) |  Loss2: (0.0000) | Acc: (96.00%) (16131/16768)
Epoch: 161 | Batch_idx: 140 |  Loss: (0.1073) |  Loss2: (0.0000) | Acc: (96.00%) (17369/18048)
Epoch: 161 | Batch_idx: 150 |  Loss: (0.1065) |  Loss2: (0.0000) | Acc: (96.00%) (18612/19328)
Epoch: 161 | Batch_idx: 160 |  Loss: (0.1061) |  Loss2: (0.0000) | Acc: (96.00%) (19844/20608)
Epoch: 161 | Batch_idx: 170 |  Loss: (0.1060) |  Loss2: (0.0000) | Acc: (96.00%) (21084/21888)
Epoch: 161 | Batch_idx: 180 |  Loss: (0.1065) |  Loss2: (0.0000) | Acc: (96.00%) (22313/23168)
Epoch: 161 | Batch_idx: 190 |  Loss: (0.1073) |  Loss2: (0.0000) | Acc: (96.00%) (23539/24448)
Epoch: 161 | Batch_idx: 200 |  Loss: (0.1076) |  Loss2: (0.0000) | Acc: (96.00%) (24771/25728)
Epoch: 161 | Batch_idx: 210 |  Loss: (0.1069) |  Loss2: (0.0000) | Acc: (96.00%) (26007/27008)
Epoch: 161 | Batch_idx: 220 |  Loss: (0.1071) |  Loss2: (0.0000) | Acc: (96.00%) (27241/28288)
Epoch: 161 | Batch_idx: 230 |  Loss: (0.1068) |  Loss2: (0.0000) | Acc: (96.00%) (28481/29568)
Epoch: 161 | Batch_idx: 240 |  Loss: (0.1071) |  Loss2: (0.0000) | Acc: (96.00%) (29711/30848)
Epoch: 161 | Batch_idx: 250 |  Loss: (0.1066) |  Loss2: (0.0000) | Acc: (96.00%) (30949/32128)
Epoch: 161 | Batch_idx: 260 |  Loss: (0.1069) |  Loss2: (0.0000) | Acc: (96.00%) (32180/33408)
Epoch: 161 | Batch_idx: 270 |  Loss: (0.1073) |  Loss2: (0.0000) | Acc: (96.00%) (33406/34688)
Epoch: 161 | Batch_idx: 280 |  Loss: (0.1072) |  Loss2: (0.0000) | Acc: (96.00%) (34643/35968)
Epoch: 161 | Batch_idx: 290 |  Loss: (0.1067) |  Loss2: (0.0000) | Acc: (96.00%) (35884/37248)
Epoch: 161 | Batch_idx: 300 |  Loss: (0.1069) |  Loss2: (0.0000) | Acc: (96.00%) (37121/38528)
Epoch: 161 | Batch_idx: 310 |  Loss: (0.1070) |  Loss2: (0.0000) | Acc: (96.00%) (38355/39808)
Epoch: 161 | Batch_idx: 320 |  Loss: (0.1071) |  Loss2: (0.0000) | Acc: (96.00%) (39592/41088)
Epoch: 161 | Batch_idx: 330 |  Loss: (0.1075) |  Loss2: (0.0000) | Acc: (96.00%) (40820/42368)
Epoch: 161 | Batch_idx: 340 |  Loss: (0.1079) |  Loss2: (0.0000) | Acc: (96.00%) (42043/43648)
Epoch: 161 | Batch_idx: 350 |  Loss: (0.1090) |  Loss2: (0.0000) | Acc: (96.00%) (43262/44928)
Epoch: 161 | Batch_idx: 360 |  Loss: (0.1090) |  Loss2: (0.0000) | Acc: (96.00%) (44496/46208)
Epoch: 161 | Batch_idx: 370 |  Loss: (0.1093) |  Loss2: (0.0000) | Acc: (96.00%) (45719/47488)
Epoch: 161 | Batch_idx: 380 |  Loss: (0.1099) |  Loss2: (0.0000) | Acc: (96.00%) (46936/48768)
Epoch: 161 | Batch_idx: 390 |  Loss: (0.1102) |  Loss2: (0.0000) | Acc: (96.00%) (48117/50000)
# TEST : Loss: (0.4029) | Acc: (88.00%) (8831/10000)
percent tensor([0.5522, 0.5631, 0.5555, 0.5249, 0.5628, 0.5654, 0.5715, 0.5403, 0.5519,
        0.5605, 0.5650, 0.5641, 0.5520, 0.5267, 0.5650, 0.5466],
       device='cuda:0') torch.Size([16])
percent tensor([0.5447, 0.5474, 0.5504, 0.5532, 0.5529, 0.5445, 0.5492, 0.5546, 0.5488,
        0.5482, 0.5455, 0.5488, 0.5455, 0.5491, 0.5463, 0.5474],
       device='cuda:0') torch.Size([16])
percent tensor([0.6628, 0.6745, 0.4845, 0.4339, 0.4444, 0.5988, 0.6186, 0.4977, 0.5459,
        0.6461, 0.6541, 0.5462, 0.6777, 0.6394, 0.6450, 0.6380],
       device='cuda:0') torch.Size([16])
percent tensor([0.7351, 0.6110, 0.7125, 0.7213, 0.7205, 0.8048, 0.6712, 0.7376, 0.6844,
        0.6529, 0.6641, 0.6464, 0.6060, 0.7017, 0.7132, 0.7763],
       device='cuda:0') torch.Size([16])
percent tensor([0.6375, 0.6517, 0.6372, 0.5810, 0.6144, 0.5605, 0.6792, 0.6366, 0.6463,
        0.6401, 0.6251, 0.6154, 0.6384, 0.6378, 0.6520, 0.5925],
       device='cuda:0') torch.Size([16])
percent tensor([0.5536, 0.7016, 0.6792, 0.7279, 0.7191, 0.7318, 0.7065, 0.5638, 0.7629,
        0.6556, 0.7403, 0.7836, 0.7044, 0.7760, 0.5990, 0.5420],
       device='cuda:0') torch.Size([16])
percent tensor([0.6387, 0.6875, 0.5771, 0.4900, 0.6311, 0.6934, 0.6379, 0.4521, 0.6824,
        0.6861, 0.7021, 0.4569, 0.6787, 0.6439, 0.4384, 0.5692],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9997, 0.9999, 0.9998, 0.9999, 0.9994, 0.9999, 0.9996, 0.9998,
        0.9996, 0.9998, 0.9999, 0.9998, 0.9995, 0.9995, 0.9992],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 162 | Batch_idx: 0 |  Loss: (0.1009) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 162 | Batch_idx: 10 |  Loss: (0.1110) |  Loss2: (0.0000) | Acc: (95.00%) (1349/1408)
Epoch: 162 | Batch_idx: 20 |  Loss: (0.1290) |  Loss2: (0.0000) | Acc: (95.00%) (2558/2688)
Epoch: 162 | Batch_idx: 30 |  Loss: (0.1284) |  Loss2: (0.0000) | Acc: (95.00%) (3787/3968)
Epoch: 162 | Batch_idx: 40 |  Loss: (0.1285) |  Loss2: (0.0000) | Acc: (95.00%) (5017/5248)
Epoch: 162 | Batch_idx: 50 |  Loss: (0.1270) |  Loss2: (0.0000) | Acc: (95.00%) (6240/6528)
Epoch: 162 | Batch_idx: 60 |  Loss: (0.1288) |  Loss2: (0.0000) | Acc: (95.00%) (7459/7808)
Epoch: 162 | Batch_idx: 70 |  Loss: (0.1282) |  Loss2: (0.0000) | Acc: (95.00%) (8675/9088)
Epoch: 162 | Batch_idx: 80 |  Loss: (0.1291) |  Loss2: (0.0000) | Acc: (95.00%) (9889/10368)
Epoch: 162 | Batch_idx: 90 |  Loss: (0.1304) |  Loss2: (0.0000) | Acc: (95.00%) (11101/11648)
Epoch: 162 | Batch_idx: 100 |  Loss: (0.1311) |  Loss2: (0.0000) | Acc: (95.00%) (12324/12928)
Epoch: 162 | Batch_idx: 110 |  Loss: (0.1315) |  Loss2: (0.0000) | Acc: (95.00%) (13546/14208)
Epoch: 162 | Batch_idx: 120 |  Loss: (0.1303) |  Loss2: (0.0000) | Acc: (95.00%) (14772/15488)
Epoch: 162 | Batch_idx: 130 |  Loss: (0.1314) |  Loss2: (0.0000) | Acc: (95.00%) (15982/16768)
Epoch: 162 | Batch_idx: 140 |  Loss: (0.1309) |  Loss2: (0.0000) | Acc: (95.00%) (17206/18048)
Epoch: 162 | Batch_idx: 150 |  Loss: (0.1303) |  Loss2: (0.0000) | Acc: (95.00%) (18431/19328)
Epoch: 162 | Batch_idx: 160 |  Loss: (0.1291) |  Loss2: (0.0000) | Acc: (95.00%) (19665/20608)
Epoch: 162 | Batch_idx: 170 |  Loss: (0.1294) |  Loss2: (0.0000) | Acc: (95.00%) (20886/21888)
Epoch: 162 | Batch_idx: 180 |  Loss: (0.1291) |  Loss2: (0.0000) | Acc: (95.00%) (22114/23168)
Epoch: 162 | Batch_idx: 190 |  Loss: (0.1281) |  Loss2: (0.0000) | Acc: (95.00%) (23347/24448)
Epoch: 162 | Batch_idx: 200 |  Loss: (0.1272) |  Loss2: (0.0000) | Acc: (95.00%) (24587/25728)
Epoch: 162 | Batch_idx: 210 |  Loss: (0.1282) |  Loss2: (0.0000) | Acc: (95.00%) (25792/27008)
Epoch: 162 | Batch_idx: 220 |  Loss: (0.1291) |  Loss2: (0.0000) | Acc: (95.00%) (27000/28288)
Epoch: 162 | Batch_idx: 230 |  Loss: (0.1277) |  Loss2: (0.0000) | Acc: (95.00%) (28237/29568)
Epoch: 162 | Batch_idx: 240 |  Loss: (0.1274) |  Loss2: (0.0000) | Acc: (95.00%) (29467/30848)
Epoch: 162 | Batch_idx: 250 |  Loss: (0.1275) |  Loss2: (0.0000) | Acc: (95.00%) (30692/32128)
Epoch: 162 | Batch_idx: 260 |  Loss: (0.1273) |  Loss2: (0.0000) | Acc: (95.00%) (31923/33408)
Epoch: 162 | Batch_idx: 270 |  Loss: (0.1270) |  Loss2: (0.0000) | Acc: (95.00%) (33153/34688)
Epoch: 162 | Batch_idx: 280 |  Loss: (0.1265) |  Loss2: (0.0000) | Acc: (95.00%) (34379/35968)
Epoch: 162 | Batch_idx: 290 |  Loss: (0.1260) |  Loss2: (0.0000) | Acc: (95.00%) (35612/37248)
Epoch: 162 | Batch_idx: 300 |  Loss: (0.1256) |  Loss2: (0.0000) | Acc: (95.00%) (36843/38528)
Epoch: 162 | Batch_idx: 310 |  Loss: (0.1260) |  Loss2: (0.0000) | Acc: (95.00%) (38065/39808)
Epoch: 162 | Batch_idx: 320 |  Loss: (0.1256) |  Loss2: (0.0000) | Acc: (95.00%) (39292/41088)
Epoch: 162 | Batch_idx: 330 |  Loss: (0.1251) |  Loss2: (0.0000) | Acc: (95.00%) (40524/42368)
Epoch: 162 | Batch_idx: 340 |  Loss: (0.1253) |  Loss2: (0.0000) | Acc: (95.00%) (41745/43648)
Epoch: 162 | Batch_idx: 350 |  Loss: (0.1254) |  Loss2: (0.0000) | Acc: (95.00%) (42971/44928)
Epoch: 162 | Batch_idx: 360 |  Loss: (0.1252) |  Loss2: (0.0000) | Acc: (95.00%) (44195/46208)
Epoch: 162 | Batch_idx: 370 |  Loss: (0.1250) |  Loss2: (0.0000) | Acc: (95.00%) (45430/47488)
Epoch: 162 | Batch_idx: 380 |  Loss: (0.1251) |  Loss2: (0.0000) | Acc: (95.00%) (46654/48768)
Epoch: 162 | Batch_idx: 390 |  Loss: (0.1251) |  Loss2: (0.0000) | Acc: (95.00%) (47829/50000)
# TEST : Loss: (0.4023) | Acc: (88.00%) (8852/10000)
percent tensor([0.5552, 0.5637, 0.5599, 0.5270, 0.5673, 0.5698, 0.5735, 0.5424, 0.5544,
        0.5627, 0.5674, 0.5672, 0.5544, 0.5254, 0.5673, 0.5490],
       device='cuda:0') torch.Size([16])
percent tensor([0.5443, 0.5473, 0.5495, 0.5531, 0.5521, 0.5437, 0.5487, 0.5538, 0.5484,
        0.5477, 0.5451, 0.5484, 0.5450, 0.5495, 0.5457, 0.5470],
       device='cuda:0') torch.Size([16])
percent tensor([0.6724, 0.6767, 0.5159, 0.4528, 0.4659, 0.6084, 0.6270, 0.5144, 0.5594,
        0.6596, 0.6656, 0.5725, 0.6902, 0.6414, 0.6499, 0.6464],
       device='cuda:0') torch.Size([16])
percent tensor([0.7405, 0.6230, 0.7226, 0.7238, 0.7248, 0.8084, 0.6790, 0.7454, 0.6979,
        0.6664, 0.6816, 0.6582, 0.6202, 0.7101, 0.7222, 0.7874],
       device='cuda:0') torch.Size([16])
percent tensor([0.6168, 0.6193, 0.6144, 0.5747, 0.6011, 0.5608, 0.6461, 0.6139, 0.6186,
        0.6090, 0.5933, 0.5875, 0.6067, 0.6129, 0.6236, 0.5725],
       device='cuda:0') torch.Size([16])
percent tensor([0.5742, 0.7197, 0.6968, 0.7390, 0.7307, 0.7509, 0.7277, 0.5930, 0.7857,
        0.6899, 0.7647, 0.7971, 0.7182, 0.7943, 0.6268, 0.5628],
       device='cuda:0') torch.Size([16])
percent tensor([0.6467, 0.6868, 0.5534, 0.4569, 0.5930, 0.6824, 0.6504, 0.4386, 0.6898,
        0.7030, 0.7044, 0.4326, 0.6647, 0.6443, 0.4288, 0.5738],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9997, 0.9999, 0.9998, 0.9999, 0.9994, 0.9999, 0.9997, 0.9999,
        0.9997, 0.9999, 0.9999, 0.9999, 0.9995, 0.9995, 0.9994],
       device='cuda:0') torch.Size([16])
Epoch: 163 | Batch_idx: 0 |  Loss: (0.0578) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 163 | Batch_idx: 10 |  Loss: (0.0932) |  Loss2: (0.0000) | Acc: (97.00%) (1366/1408)
Epoch: 163 | Batch_idx: 20 |  Loss: (0.0988) |  Loss2: (0.0000) | Acc: (97.00%) (2608/2688)
Epoch: 163 | Batch_idx: 30 |  Loss: (0.1088) |  Loss2: (0.0000) | Acc: (96.00%) (3826/3968)
Epoch: 163 | Batch_idx: 40 |  Loss: (0.1128) |  Loss2: (0.0000) | Acc: (96.00%) (5053/5248)
Epoch: 163 | Batch_idx: 50 |  Loss: (0.1149) |  Loss2: (0.0000) | Acc: (96.00%) (6275/6528)
Epoch: 163 | Batch_idx: 60 |  Loss: (0.1131) |  Loss2: (0.0000) | Acc: (96.00%) (7510/7808)
Epoch: 163 | Batch_idx: 70 |  Loss: (0.1099) |  Loss2: (0.0000) | Acc: (96.00%) (8752/9088)
Epoch: 163 | Batch_idx: 80 |  Loss: (0.1134) |  Loss2: (0.0000) | Acc: (96.00%) (9968/10368)
Epoch: 163 | Batch_idx: 90 |  Loss: (0.1130) |  Loss2: (0.0000) | Acc: (96.00%) (11190/11648)
Epoch: 163 | Batch_idx: 100 |  Loss: (0.1140) |  Loss2: (0.0000) | Acc: (96.00%) (12412/12928)
Epoch: 163 | Batch_idx: 110 |  Loss: (0.1139) |  Loss2: (0.0000) | Acc: (96.00%) (13640/14208)
Epoch: 163 | Batch_idx: 120 |  Loss: (0.1148) |  Loss2: (0.0000) | Acc: (95.00%) (14868/15488)
Epoch: 163 | Batch_idx: 130 |  Loss: (0.1151) |  Loss2: (0.0000) | Acc: (95.00%) (16095/16768)
Epoch: 163 | Batch_idx: 140 |  Loss: (0.1149) |  Loss2: (0.0000) | Acc: (96.00%) (17330/18048)
Epoch: 163 | Batch_idx: 150 |  Loss: (0.1159) |  Loss2: (0.0000) | Acc: (95.00%) (18550/19328)
Epoch: 163 | Batch_idx: 160 |  Loss: (0.1163) |  Loss2: (0.0000) | Acc: (95.00%) (19779/20608)
Epoch: 163 | Batch_idx: 170 |  Loss: (0.1156) |  Loss2: (0.0000) | Acc: (96.00%) (21013/21888)
Epoch: 163 | Batch_idx: 180 |  Loss: (0.1150) |  Loss2: (0.0000) | Acc: (96.00%) (22249/23168)
Epoch: 163 | Batch_idx: 190 |  Loss: (0.1147) |  Loss2: (0.0000) | Acc: (96.00%) (23492/24448)
Epoch: 163 | Batch_idx: 200 |  Loss: (0.1139) |  Loss2: (0.0000) | Acc: (96.00%) (24733/25728)
Epoch: 163 | Batch_idx: 210 |  Loss: (0.1139) |  Loss2: (0.0000) | Acc: (96.00%) (25961/27008)
Epoch: 163 | Batch_idx: 220 |  Loss: (0.1143) |  Loss2: (0.0000) | Acc: (96.00%) (27185/28288)
Epoch: 163 | Batch_idx: 230 |  Loss: (0.1143) |  Loss2: (0.0000) | Acc: (96.00%) (28410/29568)
Epoch: 163 | Batch_idx: 240 |  Loss: (0.1142) |  Loss2: (0.0000) | Acc: (96.00%) (29645/30848)
Epoch: 163 | Batch_idx: 250 |  Loss: (0.1141) |  Loss2: (0.0000) | Acc: (96.00%) (30877/32128)
Epoch: 163 | Batch_idx: 260 |  Loss: (0.1138) |  Loss2: (0.0000) | Acc: (96.00%) (32110/33408)
Epoch: 163 | Batch_idx: 270 |  Loss: (0.1138) |  Loss2: (0.0000) | Acc: (96.00%) (33336/34688)
Epoch: 163 | Batch_idx: 280 |  Loss: (0.1129) |  Loss2: (0.0000) | Acc: (96.00%) (34577/35968)
Epoch: 163 | Batch_idx: 290 |  Loss: (0.1125) |  Loss2: (0.0000) | Acc: (96.00%) (35808/37248)
Epoch: 163 | Batch_idx: 300 |  Loss: (0.1129) |  Loss2: (0.0000) | Acc: (96.00%) (37037/38528)
Epoch: 163 | Batch_idx: 310 |  Loss: (0.1131) |  Loss2: (0.0000) | Acc: (96.00%) (38269/39808)
Epoch: 163 | Batch_idx: 320 |  Loss: (0.1129) |  Loss2: (0.0000) | Acc: (96.00%) (39496/41088)
Epoch: 163 | Batch_idx: 330 |  Loss: (0.1130) |  Loss2: (0.0000) | Acc: (96.00%) (40721/42368)
Epoch: 163 | Batch_idx: 340 |  Loss: (0.1126) |  Loss2: (0.0000) | Acc: (96.00%) (41963/43648)
Epoch: 163 | Batch_idx: 350 |  Loss: (0.1124) |  Loss2: (0.0000) | Acc: (96.00%) (43204/44928)
Epoch: 163 | Batch_idx: 360 |  Loss: (0.1128) |  Loss2: (0.0000) | Acc: (96.00%) (44433/46208)
Epoch: 163 | Batch_idx: 370 |  Loss: (0.1128) |  Loss2: (0.0000) | Acc: (96.00%) (45671/47488)
Epoch: 163 | Batch_idx: 380 |  Loss: (0.1128) |  Loss2: (0.0000) | Acc: (96.00%) (46907/48768)
Epoch: 163 | Batch_idx: 390 |  Loss: (0.1124) |  Loss2: (0.0000) | Acc: (96.00%) (48102/50000)
# TEST : Loss: (0.3875) | Acc: (88.00%) (8869/10000)
percent tensor([0.5583, 0.5665, 0.5624, 0.5297, 0.5706, 0.5736, 0.5765, 0.5450, 0.5576,
        0.5654, 0.5705, 0.5698, 0.5571, 0.5281, 0.5705, 0.5520],
       device='cuda:0') torch.Size([16])
percent tensor([0.5440, 0.5466, 0.5494, 0.5533, 0.5520, 0.5434, 0.5481, 0.5536, 0.5480,
        0.5472, 0.5446, 0.5481, 0.5444, 0.5491, 0.5451, 0.5467],
       device='cuda:0') torch.Size([16])
percent tensor([0.6668, 0.6737, 0.5102, 0.4504, 0.4590, 0.6050, 0.6220, 0.5071, 0.5530,
        0.6566, 0.6629, 0.5691, 0.6856, 0.6397, 0.6452, 0.6416],
       device='cuda:0') torch.Size([16])
percent tensor([0.7430, 0.6281, 0.7277, 0.7283, 0.7264, 0.8076, 0.6822, 0.7455, 0.7023,
        0.6723, 0.6890, 0.6651, 0.6282, 0.7127, 0.7237, 0.7885],
       device='cuda:0') torch.Size([16])
percent tensor([0.6256, 0.6261, 0.6209, 0.5808, 0.6103, 0.5753, 0.6531, 0.6215, 0.6247,
        0.6174, 0.5993, 0.5928, 0.6125, 0.6196, 0.6344, 0.5856],
       device='cuda:0') torch.Size([16])
percent tensor([0.5856, 0.7291, 0.7047, 0.7487, 0.7378, 0.7598, 0.7360, 0.5973, 0.7992,
        0.6983, 0.7778, 0.8074, 0.7270, 0.8093, 0.6343, 0.5653],
       device='cuda:0') torch.Size([16])
percent tensor([0.6571, 0.6866, 0.5523, 0.4526, 0.5956, 0.6891, 0.6502, 0.4401, 0.6950,
        0.7055, 0.7018, 0.4285, 0.6685, 0.6388, 0.4247, 0.5772],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9997, 0.9999, 0.9998, 0.9999, 0.9994, 0.9999, 0.9997, 0.9999,
        0.9997, 0.9999, 0.9999, 0.9999, 0.9995, 0.9995, 0.9994],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 164 | Batch_idx: 0 |  Loss: (0.0680) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 164 | Batch_idx: 10 |  Loss: (0.0924) |  Loss2: (0.0000) | Acc: (96.00%) (1361/1408)
Epoch: 164 | Batch_idx: 20 |  Loss: (0.0930) |  Loss2: (0.0000) | Acc: (97.00%) (2609/2688)
Epoch: 164 | Batch_idx: 30 |  Loss: (0.0947) |  Loss2: (0.0000) | Acc: (96.00%) (3847/3968)
Epoch: 164 | Batch_idx: 40 |  Loss: (0.0979) |  Loss2: (0.0000) | Acc: (96.00%) (5087/5248)
Epoch: 164 | Batch_idx: 50 |  Loss: (0.0960) |  Loss2: (0.0000) | Acc: (96.00%) (6331/6528)
Epoch: 164 | Batch_idx: 60 |  Loss: (0.0940) |  Loss2: (0.0000) | Acc: (97.00%) (7576/7808)
Epoch: 164 | Batch_idx: 70 |  Loss: (0.0952) |  Loss2: (0.0000) | Acc: (96.00%) (8809/9088)
Epoch: 164 | Batch_idx: 80 |  Loss: (0.0961) |  Loss2: (0.0000) | Acc: (96.00%) (10046/10368)
Epoch: 164 | Batch_idx: 90 |  Loss: (0.0983) |  Loss2: (0.0000) | Acc: (96.00%) (11274/11648)
Epoch: 164 | Batch_idx: 100 |  Loss: (0.0986) |  Loss2: (0.0000) | Acc: (96.00%) (12516/12928)
Epoch: 164 | Batch_idx: 110 |  Loss: (0.0981) |  Loss2: (0.0000) | Acc: (96.00%) (13759/14208)
Epoch: 164 | Batch_idx: 120 |  Loss: (0.1004) |  Loss2: (0.0000) | Acc: (96.00%) (14985/15488)
Epoch: 164 | Batch_idx: 130 |  Loss: (0.1013) |  Loss2: (0.0000) | Acc: (96.00%) (16217/16768)
Epoch: 164 | Batch_idx: 140 |  Loss: (0.1010) |  Loss2: (0.0000) | Acc: (96.00%) (17461/18048)
Epoch: 164 | Batch_idx: 150 |  Loss: (0.1025) |  Loss2: (0.0000) | Acc: (96.00%) (18680/19328)
Epoch: 164 | Batch_idx: 160 |  Loss: (0.1024) |  Loss2: (0.0000) | Acc: (96.00%) (19924/20608)
Epoch: 164 | Batch_idx: 170 |  Loss: (0.1022) |  Loss2: (0.0000) | Acc: (96.00%) (21167/21888)
Epoch: 164 | Batch_idx: 180 |  Loss: (0.1026) |  Loss2: (0.0000) | Acc: (96.00%) (22398/23168)
Epoch: 164 | Batch_idx: 190 |  Loss: (0.1026) |  Loss2: (0.0000) | Acc: (96.00%) (23632/24448)
Epoch: 164 | Batch_idx: 200 |  Loss: (0.1028) |  Loss2: (0.0000) | Acc: (96.00%) (24869/25728)
Epoch: 164 | Batch_idx: 210 |  Loss: (0.1033) |  Loss2: (0.0000) | Acc: (96.00%) (26101/27008)
Epoch: 164 | Batch_idx: 220 |  Loss: (0.1036) |  Loss2: (0.0000) | Acc: (96.00%) (27330/28288)
Epoch: 164 | Batch_idx: 230 |  Loss: (0.1033) |  Loss2: (0.0000) | Acc: (96.00%) (28568/29568)
Epoch: 164 | Batch_idx: 240 |  Loss: (0.1038) |  Loss2: (0.0000) | Acc: (96.00%) (29800/30848)
Epoch: 164 | Batch_idx: 250 |  Loss: (0.1040) |  Loss2: (0.0000) | Acc: (96.00%) (31031/32128)
Epoch: 164 | Batch_idx: 260 |  Loss: (0.1047) |  Loss2: (0.0000) | Acc: (96.00%) (32259/33408)
Epoch: 164 | Batch_idx: 270 |  Loss: (0.1053) |  Loss2: (0.0000) | Acc: (96.00%) (33485/34688)
Epoch: 164 | Batch_idx: 280 |  Loss: (0.1052) |  Loss2: (0.0000) | Acc: (96.00%) (34716/35968)
Epoch: 164 | Batch_idx: 290 |  Loss: (0.1059) |  Loss2: (0.0000) | Acc: (96.00%) (35944/37248)
Epoch: 164 | Batch_idx: 300 |  Loss: (0.1061) |  Loss2: (0.0000) | Acc: (96.00%) (37176/38528)
Epoch: 164 | Batch_idx: 310 |  Loss: (0.1059) |  Loss2: (0.0000) | Acc: (96.00%) (38411/39808)
Epoch: 164 | Batch_idx: 320 |  Loss: (0.1061) |  Loss2: (0.0000) | Acc: (96.00%) (39638/41088)
Epoch: 164 | Batch_idx: 330 |  Loss: (0.1061) |  Loss2: (0.0000) | Acc: (96.00%) (40872/42368)
Epoch: 164 | Batch_idx: 340 |  Loss: (0.1065) |  Loss2: (0.0000) | Acc: (96.00%) (42091/43648)
Epoch: 164 | Batch_idx: 350 |  Loss: (0.1070) |  Loss2: (0.0000) | Acc: (96.00%) (43309/44928)
Epoch: 164 | Batch_idx: 360 |  Loss: (0.1072) |  Loss2: (0.0000) | Acc: (96.00%) (44541/46208)
Epoch: 164 | Batch_idx: 370 |  Loss: (0.1076) |  Loss2: (0.0000) | Acc: (96.00%) (45765/47488)
Epoch: 164 | Batch_idx: 380 |  Loss: (0.1080) |  Loss2: (0.0000) | Acc: (96.00%) (46991/48768)
Epoch: 164 | Batch_idx: 390 |  Loss: (0.1083) |  Loss2: (0.0000) | Acc: (96.00%) (48171/50000)
# TEST : Loss: (0.4262) | Acc: (87.00%) (8766/10000)
percent tensor([0.5606, 0.5660, 0.5660, 0.5297, 0.5751, 0.5757, 0.5784, 0.5447, 0.5613,
        0.5658, 0.5723, 0.5740, 0.5591, 0.5264, 0.5712, 0.5525],
       device='cuda:0') torch.Size([16])
percent tensor([0.5442, 0.5471, 0.5479, 0.5537, 0.5515, 0.5442, 0.5485, 0.5534, 0.5473,
        0.5474, 0.5446, 0.5476, 0.5443, 0.5509, 0.5452, 0.5471],
       device='cuda:0') torch.Size([16])
percent tensor([0.6698, 0.6797, 0.5275, 0.4510, 0.4672, 0.6034, 0.6337, 0.5105, 0.5839,
        0.6555, 0.6757, 0.5717, 0.6931, 0.6417, 0.6458, 0.6461],
       device='cuda:0') torch.Size([16])
percent tensor([0.7465, 0.6255, 0.7297, 0.7256, 0.7291, 0.8034, 0.6950, 0.7377, 0.6911,
        0.6673, 0.6850, 0.6734, 0.6268, 0.7077, 0.7184, 0.7854],
       device='cuda:0') torch.Size([16])
percent tensor([0.6249, 0.6329, 0.6281, 0.5810, 0.6149, 0.5804, 0.6448, 0.6281, 0.6287,
        0.6113, 0.5908, 0.5892, 0.6125, 0.6145, 0.6439, 0.5914],
       device='cuda:0') torch.Size([16])
percent tensor([0.6060, 0.7563, 0.7117, 0.7739, 0.7367, 0.7831, 0.7431, 0.5973, 0.7843,
        0.6766, 0.7714, 0.8014, 0.7303, 0.8289, 0.6502, 0.6164],
       device='cuda:0') torch.Size([16])
percent tensor([0.6422, 0.6688, 0.6106, 0.4502, 0.6536, 0.7239, 0.6284, 0.4629, 0.6659,
        0.6848, 0.6861, 0.4661, 0.6586, 0.6169, 0.4359, 0.5779],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9997, 0.9999, 0.9998, 0.9999, 0.9996, 0.9999, 0.9997, 0.9999,
        0.9997, 0.9999, 0.9999, 0.9999, 0.9994, 0.9995, 0.9992],
       device='cuda:0') torch.Size([16])
Epoch: 165 | Batch_idx: 0 |  Loss: (0.0840) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 165 | Batch_idx: 10 |  Loss: (0.1090) |  Loss2: (0.0000) | Acc: (96.00%) (1362/1408)
Epoch: 165 | Batch_idx: 20 |  Loss: (0.1100) |  Loss2: (0.0000) | Acc: (96.00%) (2594/2688)
Epoch: 165 | Batch_idx: 30 |  Loss: (0.1034) |  Loss2: (0.0000) | Acc: (96.00%) (3841/3968)
Epoch: 165 | Batch_idx: 40 |  Loss: (0.1037) |  Loss2: (0.0000) | Acc: (96.00%) (5078/5248)
Epoch: 165 | Batch_idx: 50 |  Loss: (0.1041) |  Loss2: (0.0000) | Acc: (96.00%) (6307/6528)
Epoch: 165 | Batch_idx: 60 |  Loss: (0.1033) |  Loss2: (0.0000) | Acc: (96.00%) (7546/7808)
Epoch: 165 | Batch_idx: 70 |  Loss: (0.1023) |  Loss2: (0.0000) | Acc: (96.00%) (8787/9088)
Epoch: 165 | Batch_idx: 80 |  Loss: (0.1014) |  Loss2: (0.0000) | Acc: (96.00%) (10028/10368)
Epoch: 165 | Batch_idx: 90 |  Loss: (0.1019) |  Loss2: (0.0000) | Acc: (96.00%) (11257/11648)
Epoch: 165 | Batch_idx: 100 |  Loss: (0.1020) |  Loss2: (0.0000) | Acc: (96.00%) (12493/12928)
Epoch: 165 | Batch_idx: 110 |  Loss: (0.1009) |  Loss2: (0.0000) | Acc: (96.00%) (13736/14208)
Epoch: 165 | Batch_idx: 120 |  Loss: (0.1000) |  Loss2: (0.0000) | Acc: (96.00%) (14976/15488)
Epoch: 165 | Batch_idx: 130 |  Loss: (0.1015) |  Loss2: (0.0000) | Acc: (96.00%) (16196/16768)
Epoch: 165 | Batch_idx: 140 |  Loss: (0.1021) |  Loss2: (0.0000) | Acc: (96.00%) (17428/18048)
Epoch: 165 | Batch_idx: 150 |  Loss: (0.1035) |  Loss2: (0.0000) | Acc: (96.00%) (18652/19328)
Epoch: 165 | Batch_idx: 160 |  Loss: (0.1038) |  Loss2: (0.0000) | Acc: (96.00%) (19882/20608)
Epoch: 165 | Batch_idx: 170 |  Loss: (0.1044) |  Loss2: (0.0000) | Acc: (96.00%) (21110/21888)
Epoch: 165 | Batch_idx: 180 |  Loss: (0.1046) |  Loss2: (0.0000) | Acc: (96.00%) (22345/23168)
Epoch: 165 | Batch_idx: 190 |  Loss: (0.1033) |  Loss2: (0.0000) | Acc: (96.00%) (23590/24448)
Epoch: 165 | Batch_idx: 200 |  Loss: (0.1036) |  Loss2: (0.0000) | Acc: (96.00%) (24825/25728)
Epoch: 165 | Batch_idx: 210 |  Loss: (0.1042) |  Loss2: (0.0000) | Acc: (96.00%) (26056/27008)
Epoch: 165 | Batch_idx: 220 |  Loss: (0.1035) |  Loss2: (0.0000) | Acc: (96.00%) (27302/28288)
Epoch: 165 | Batch_idx: 230 |  Loss: (0.1033) |  Loss2: (0.0000) | Acc: (96.00%) (28538/29568)
Epoch: 165 | Batch_idx: 240 |  Loss: (0.1030) |  Loss2: (0.0000) | Acc: (96.00%) (29770/30848)
Epoch: 165 | Batch_idx: 250 |  Loss: (0.1028) |  Loss2: (0.0000) | Acc: (96.00%) (31005/32128)
Epoch: 165 | Batch_idx: 260 |  Loss: (0.1029) |  Loss2: (0.0000) | Acc: (96.00%) (32236/33408)
Epoch: 165 | Batch_idx: 270 |  Loss: (0.1029) |  Loss2: (0.0000) | Acc: (96.00%) (33470/34688)
Epoch: 165 | Batch_idx: 280 |  Loss: (0.1029) |  Loss2: (0.0000) | Acc: (96.00%) (34706/35968)
Epoch: 165 | Batch_idx: 290 |  Loss: (0.1031) |  Loss2: (0.0000) | Acc: (96.00%) (35938/37248)
Epoch: 165 | Batch_idx: 300 |  Loss: (0.1030) |  Loss2: (0.0000) | Acc: (96.00%) (37172/38528)
Epoch: 165 | Batch_idx: 310 |  Loss: (0.1029) |  Loss2: (0.0000) | Acc: (96.00%) (38406/39808)
Epoch: 165 | Batch_idx: 320 |  Loss: (0.1034) |  Loss2: (0.0000) | Acc: (96.00%) (39629/41088)
Epoch: 165 | Batch_idx: 330 |  Loss: (0.1036) |  Loss2: (0.0000) | Acc: (96.00%) (40857/42368)
Epoch: 165 | Batch_idx: 340 |  Loss: (0.1039) |  Loss2: (0.0000) | Acc: (96.00%) (42090/43648)
Epoch: 165 | Batch_idx: 350 |  Loss: (0.1038) |  Loss2: (0.0000) | Acc: (96.00%) (43325/44928)
Epoch: 165 | Batch_idx: 360 |  Loss: (0.1039) |  Loss2: (0.0000) | Acc: (96.00%) (44564/46208)
Epoch: 165 | Batch_idx: 370 |  Loss: (0.1047) |  Loss2: (0.0000) | Acc: (96.00%) (45788/47488)
Epoch: 165 | Batch_idx: 380 |  Loss: (0.1046) |  Loss2: (0.0000) | Acc: (96.00%) (47026/48768)
Epoch: 165 | Batch_idx: 390 |  Loss: (0.1050) |  Loss2: (0.0000) | Acc: (96.00%) (48217/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_165.pth.tar'
# TEST : Loss: (0.4008) | Acc: (88.00%) (8863/10000)
percent tensor([0.5582, 0.5700, 0.5541, 0.5272, 0.5655, 0.5752, 0.5773, 0.5421, 0.5596,
        0.5633, 0.5733, 0.5637, 0.5571, 0.5370, 0.5717, 0.5532],
       device='cuda:0') torch.Size([16])
percent tensor([0.5443, 0.5463, 0.5526, 0.5539, 0.5542, 0.5434, 0.5482, 0.5550, 0.5487,
        0.5477, 0.5446, 0.5494, 0.5448, 0.5493, 0.5443, 0.5466],
       device='cuda:0') torch.Size([16])
percent tensor([0.6657, 0.6843, 0.4627, 0.4413, 0.4255, 0.6032, 0.6263, 0.4805, 0.5600,
        0.6418, 0.6659, 0.5329, 0.6810, 0.6587, 0.6497, 0.6483],
       device='cuda:0') torch.Size([16])
percent tensor([0.7478, 0.6288, 0.7401, 0.7383, 0.7396, 0.8045, 0.6940, 0.7371, 0.6927,
        0.6651, 0.6832, 0.6775, 0.6331, 0.7186, 0.7192, 0.7874],
       device='cuda:0') torch.Size([16])
percent tensor([0.6296, 0.6364, 0.6227, 0.5735, 0.5993, 0.5747, 0.6506, 0.6349, 0.6343,
        0.6153, 0.6006, 0.5916, 0.6131, 0.6130, 0.6449, 0.5920],
       device='cuda:0') torch.Size([16])
percent tensor([0.6042, 0.7355, 0.6940, 0.7615, 0.7527, 0.7780, 0.7351, 0.5970, 0.7751,
        0.6423, 0.7594, 0.8010, 0.7379, 0.8041, 0.6170, 0.5864],
       device='cuda:0') torch.Size([16])
percent tensor([0.6334, 0.6577, 0.5840, 0.4920, 0.6376, 0.6867, 0.6206, 0.4550, 0.6407,
        0.6671, 0.6692, 0.4503, 0.6606, 0.5739, 0.4051, 0.5560],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9997, 0.9999, 0.9997, 0.9999, 0.9997, 0.9999, 0.9996, 0.9999,
        0.9997, 0.9999, 0.9999, 0.9999, 0.9996, 0.9995, 0.9992],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 166 | Batch_idx: 0 |  Loss: (0.1021) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 166 | Batch_idx: 10 |  Loss: (0.1096) |  Loss2: (0.0000) | Acc: (96.00%) (1360/1408)
Epoch: 166 | Batch_idx: 20 |  Loss: (0.1157) |  Loss2: (0.0000) | Acc: (96.00%) (2585/2688)
Epoch: 166 | Batch_idx: 30 |  Loss: (0.1140) |  Loss2: (0.0000) | Acc: (96.00%) (3819/3968)
Epoch: 166 | Batch_idx: 40 |  Loss: (0.1123) |  Loss2: (0.0000) | Acc: (96.00%) (5045/5248)
Epoch: 166 | Batch_idx: 50 |  Loss: (0.1145) |  Loss2: (0.0000) | Acc: (96.00%) (6272/6528)
Epoch: 166 | Batch_idx: 60 |  Loss: (0.1169) |  Loss2: (0.0000) | Acc: (95.00%) (7491/7808)
Epoch: 166 | Batch_idx: 70 |  Loss: (0.1178) |  Loss2: (0.0000) | Acc: (95.00%) (8716/9088)
Epoch: 166 | Batch_idx: 80 |  Loss: (0.1175) |  Loss2: (0.0000) | Acc: (95.00%) (9944/10368)
Epoch: 166 | Batch_idx: 90 |  Loss: (0.1207) |  Loss2: (0.0000) | Acc: (95.00%) (11153/11648)
Epoch: 166 | Batch_idx: 100 |  Loss: (0.1209) |  Loss2: (0.0000) | Acc: (95.00%) (12384/12928)
Epoch: 166 | Batch_idx: 110 |  Loss: (0.1197) |  Loss2: (0.0000) | Acc: (95.00%) (13617/14208)
Epoch: 166 | Batch_idx: 120 |  Loss: (0.1197) |  Loss2: (0.0000) | Acc: (95.00%) (14844/15488)
Epoch: 166 | Batch_idx: 130 |  Loss: (0.1187) |  Loss2: (0.0000) | Acc: (95.00%) (16077/16768)
Epoch: 166 | Batch_idx: 140 |  Loss: (0.1183) |  Loss2: (0.0000) | Acc: (95.00%) (17313/18048)
Epoch: 166 | Batch_idx: 150 |  Loss: (0.1175) |  Loss2: (0.0000) | Acc: (95.00%) (18550/19328)
Epoch: 166 | Batch_idx: 160 |  Loss: (0.1173) |  Loss2: (0.0000) | Acc: (96.00%) (19784/20608)
Epoch: 166 | Batch_idx: 170 |  Loss: (0.1162) |  Loss2: (0.0000) | Acc: (96.00%) (21026/21888)
Epoch: 166 | Batch_idx: 180 |  Loss: (0.1165) |  Loss2: (0.0000) | Acc: (96.00%) (22257/23168)
Epoch: 166 | Batch_idx: 190 |  Loss: (0.1160) |  Loss2: (0.0000) | Acc: (96.00%) (23480/24448)
Epoch: 166 | Batch_idx: 200 |  Loss: (0.1147) |  Loss2: (0.0000) | Acc: (96.00%) (24724/25728)
Epoch: 166 | Batch_idx: 210 |  Loss: (0.1144) |  Loss2: (0.0000) | Acc: (96.00%) (25955/27008)
Epoch: 166 | Batch_idx: 220 |  Loss: (0.1154) |  Loss2: (0.0000) | Acc: (96.00%) (27160/28288)
Epoch: 166 | Batch_idx: 230 |  Loss: (0.1163) |  Loss2: (0.0000) | Acc: (95.00%) (28380/29568)
Epoch: 166 | Batch_idx: 240 |  Loss: (0.1168) |  Loss2: (0.0000) | Acc: (95.00%) (29593/30848)
Epoch: 166 | Batch_idx: 250 |  Loss: (0.1166) |  Loss2: (0.0000) | Acc: (95.00%) (30814/32128)
Epoch: 166 | Batch_idx: 260 |  Loss: (0.1168) |  Loss2: (0.0000) | Acc: (95.00%) (32041/33408)
Epoch: 166 | Batch_idx: 270 |  Loss: (0.1167) |  Loss2: (0.0000) | Acc: (95.00%) (33275/34688)
Epoch: 166 | Batch_idx: 280 |  Loss: (0.1169) |  Loss2: (0.0000) | Acc: (95.00%) (34500/35968)
Epoch: 166 | Batch_idx: 290 |  Loss: (0.1170) |  Loss2: (0.0000) | Acc: (95.00%) (35715/37248)
Epoch: 166 | Batch_idx: 300 |  Loss: (0.1164) |  Loss2: (0.0000) | Acc: (95.00%) (36960/38528)
Epoch: 166 | Batch_idx: 310 |  Loss: (0.1169) |  Loss2: (0.0000) | Acc: (95.00%) (38181/39808)
Epoch: 166 | Batch_idx: 320 |  Loss: (0.1168) |  Loss2: (0.0000) | Acc: (95.00%) (39412/41088)
Epoch: 166 | Batch_idx: 330 |  Loss: (0.1164) |  Loss2: (0.0000) | Acc: (95.00%) (40644/42368)
Epoch: 166 | Batch_idx: 340 |  Loss: (0.1167) |  Loss2: (0.0000) | Acc: (95.00%) (41871/43648)
Epoch: 166 | Batch_idx: 350 |  Loss: (0.1166) |  Loss2: (0.0000) | Acc: (95.00%) (43104/44928)
Epoch: 166 | Batch_idx: 360 |  Loss: (0.1167) |  Loss2: (0.0000) | Acc: (95.00%) (44328/46208)
Epoch: 166 | Batch_idx: 370 |  Loss: (0.1167) |  Loss2: (0.0000) | Acc: (95.00%) (45557/47488)
Epoch: 166 | Batch_idx: 380 |  Loss: (0.1162) |  Loss2: (0.0000) | Acc: (95.00%) (46798/48768)
Epoch: 166 | Batch_idx: 390 |  Loss: (0.1159) |  Loss2: (0.0000) | Acc: (95.00%) (47987/50000)
# TEST : Loss: (0.4009) | Acc: (88.00%) (8869/10000)
percent tensor([0.5546, 0.5684, 0.5480, 0.5209, 0.5602, 0.5730, 0.5751, 0.5366, 0.5565,
        0.5594, 0.5715, 0.5586, 0.5534, 0.5352, 0.5697, 0.5495],
       device='cuda:0') torch.Size([16])
percent tensor([0.5378, 0.5388, 0.5456, 0.5472, 0.5464, 0.5367, 0.5407, 0.5475, 0.5410,
        0.5404, 0.5374, 0.5423, 0.5379, 0.5417, 0.5372, 0.5399],
       device='cuda:0') torch.Size([16])
percent tensor([0.6718, 0.6892, 0.4598, 0.4276, 0.4230, 0.6236, 0.6251, 0.4710, 0.5595,
        0.6374, 0.6697, 0.5274, 0.6873, 0.6530, 0.6615, 0.6525],
       device='cuda:0') torch.Size([16])
percent tensor([0.7272, 0.6107, 0.7129, 0.7047, 0.7085, 0.7954, 0.6730, 0.7019, 0.6652,
        0.6474, 0.6613, 0.6520, 0.6165, 0.6929, 0.6975, 0.7739],
       device='cuda:0') torch.Size([16])
percent tensor([0.6336, 0.6409, 0.6247, 0.5839, 0.6116, 0.5687, 0.6552, 0.6371, 0.6431,
        0.6210, 0.6135, 0.6005, 0.6206, 0.6245, 0.6467, 0.5926],
       device='cuda:0') torch.Size([16])
percent tensor([0.6041, 0.7397, 0.6968, 0.7566, 0.7519, 0.7694, 0.7342, 0.6113, 0.7722,
        0.6510, 0.7620, 0.8182, 0.7364, 0.8122, 0.6397, 0.5743],
       device='cuda:0') torch.Size([16])
percent tensor([0.6439, 0.6728, 0.6165, 0.5278, 0.6542, 0.6955, 0.6396, 0.5003, 0.6702,
        0.6882, 0.6917, 0.5095, 0.6774, 0.6126, 0.4307, 0.5562],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9997, 0.9999, 0.9998, 0.9999, 0.9997, 0.9999, 0.9997, 0.9999,
        0.9997, 0.9999, 0.9999, 0.9999, 0.9997, 0.9995, 0.9994],
       device='cuda:0') torch.Size([16])
Epoch: 167 | Batch_idx: 0 |  Loss: (0.0982) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 167 | Batch_idx: 10 |  Loss: (0.0947) |  Loss2: (0.0000) | Acc: (96.00%) (1361/1408)
Epoch: 167 | Batch_idx: 20 |  Loss: (0.1003) |  Loss2: (0.0000) | Acc: (96.00%) (2593/2688)
Epoch: 167 | Batch_idx: 30 |  Loss: (0.1014) |  Loss2: (0.0000) | Acc: (96.00%) (3827/3968)
Epoch: 167 | Batch_idx: 40 |  Loss: (0.1042) |  Loss2: (0.0000) | Acc: (96.00%) (5059/5248)
Epoch: 167 | Batch_idx: 50 |  Loss: (0.1061) |  Loss2: (0.0000) | Acc: (96.00%) (6288/6528)
Epoch: 167 | Batch_idx: 60 |  Loss: (0.1053) |  Loss2: (0.0000) | Acc: (96.00%) (7530/7808)
Epoch: 167 | Batch_idx: 70 |  Loss: (0.1067) |  Loss2: (0.0000) | Acc: (96.00%) (8764/9088)
Epoch: 167 | Batch_idx: 80 |  Loss: (0.1078) |  Loss2: (0.0000) | Acc: (96.00%) (9994/10368)
Epoch: 167 | Batch_idx: 90 |  Loss: (0.1090) |  Loss2: (0.0000) | Acc: (96.00%) (11222/11648)
Epoch: 167 | Batch_idx: 100 |  Loss: (0.1104) |  Loss2: (0.0000) | Acc: (96.00%) (12443/12928)
Epoch: 167 | Batch_idx: 110 |  Loss: (0.1081) |  Loss2: (0.0000) | Acc: (96.00%) (13687/14208)
Epoch: 167 | Batch_idx: 120 |  Loss: (0.1094) |  Loss2: (0.0000) | Acc: (96.00%) (14915/15488)
Epoch: 167 | Batch_idx: 130 |  Loss: (0.1108) |  Loss2: (0.0000) | Acc: (96.00%) (16131/16768)
Epoch: 167 | Batch_idx: 140 |  Loss: (0.1102) |  Loss2: (0.0000) | Acc: (96.00%) (17361/18048)
Epoch: 167 | Batch_idx: 150 |  Loss: (0.1103) |  Loss2: (0.0000) | Acc: (96.00%) (18589/19328)
Epoch: 167 | Batch_idx: 160 |  Loss: (0.1083) |  Loss2: (0.0000) | Acc: (96.00%) (19835/20608)
Epoch: 167 | Batch_idx: 170 |  Loss: (0.1090) |  Loss2: (0.0000) | Acc: (96.00%) (21058/21888)
Epoch: 167 | Batch_idx: 180 |  Loss: (0.1082) |  Loss2: (0.0000) | Acc: (96.00%) (22303/23168)
Epoch: 167 | Batch_idx: 190 |  Loss: (0.1077) |  Loss2: (0.0000) | Acc: (96.00%) (23543/24448)
Epoch: 167 | Batch_idx: 200 |  Loss: (0.1079) |  Loss2: (0.0000) | Acc: (96.00%) (24772/25728)
Epoch: 167 | Batch_idx: 210 |  Loss: (0.1083) |  Loss2: (0.0000) | Acc: (96.00%) (26002/27008)
Epoch: 167 | Batch_idx: 220 |  Loss: (0.1078) |  Loss2: (0.0000) | Acc: (96.00%) (27241/28288)
Epoch: 167 | Batch_idx: 230 |  Loss: (0.1083) |  Loss2: (0.0000) | Acc: (96.00%) (28463/29568)
Epoch: 167 | Batch_idx: 240 |  Loss: (0.1079) |  Loss2: (0.0000) | Acc: (96.00%) (29698/30848)
Epoch: 167 | Batch_idx: 250 |  Loss: (0.1084) |  Loss2: (0.0000) | Acc: (96.00%) (30918/32128)
Epoch: 167 | Batch_idx: 260 |  Loss: (0.1081) |  Loss2: (0.0000) | Acc: (96.00%) (32164/33408)
Epoch: 167 | Batch_idx: 270 |  Loss: (0.1074) |  Loss2: (0.0000) | Acc: (96.00%) (33407/34688)
Epoch: 167 | Batch_idx: 280 |  Loss: (0.1073) |  Loss2: (0.0000) | Acc: (96.00%) (34637/35968)
Epoch: 167 | Batch_idx: 290 |  Loss: (0.1074) |  Loss2: (0.0000) | Acc: (96.00%) (35871/37248)
Epoch: 167 | Batch_idx: 300 |  Loss: (0.1073) |  Loss2: (0.0000) | Acc: (96.00%) (37107/38528)
Epoch: 167 | Batch_idx: 310 |  Loss: (0.1070) |  Loss2: (0.0000) | Acc: (96.00%) (38351/39808)
Epoch: 167 | Batch_idx: 320 |  Loss: (0.1067) |  Loss2: (0.0000) | Acc: (96.00%) (39587/41088)
Epoch: 167 | Batch_idx: 330 |  Loss: (0.1059) |  Loss2: (0.0000) | Acc: (96.00%) (40837/42368)
Epoch: 167 | Batch_idx: 340 |  Loss: (0.1058) |  Loss2: (0.0000) | Acc: (96.00%) (42074/43648)
Epoch: 167 | Batch_idx: 350 |  Loss: (0.1058) |  Loss2: (0.0000) | Acc: (96.00%) (43311/44928)
Epoch: 167 | Batch_idx: 360 |  Loss: (0.1056) |  Loss2: (0.0000) | Acc: (96.00%) (44551/46208)
Epoch: 167 | Batch_idx: 370 |  Loss: (0.1060) |  Loss2: (0.0000) | Acc: (96.00%) (45773/47488)
Epoch: 167 | Batch_idx: 380 |  Loss: (0.1060) |  Loss2: (0.0000) | Acc: (96.00%) (47008/48768)
Epoch: 167 | Batch_idx: 390 |  Loss: (0.1053) |  Loss2: (0.0000) | Acc: (96.00%) (48217/50000)
# TEST : Loss: (0.3891) | Acc: (88.00%) (8873/10000)
percent tensor([0.5491, 0.5629, 0.5416, 0.5149, 0.5537, 0.5685, 0.5691, 0.5300, 0.5508,
        0.5536, 0.5663, 0.5525, 0.5478, 0.5296, 0.5646, 0.5440],
       device='cuda:0') torch.Size([16])
percent tensor([0.5367, 0.5375, 0.5444, 0.5462, 0.5451, 0.5356, 0.5393, 0.5462, 0.5397,
        0.5391, 0.5362, 0.5409, 0.5367, 0.5408, 0.5359, 0.5389],
       device='cuda:0') torch.Size([16])
percent tensor([0.6701, 0.6904, 0.4527, 0.4197, 0.4153, 0.6297, 0.6208, 0.4656, 0.5598,
        0.6336, 0.6705, 0.5226, 0.6884, 0.6513, 0.6640, 0.6521],
       device='cuda:0') torch.Size([16])
percent tensor([0.7361, 0.6264, 0.7119, 0.7033, 0.7079, 0.8056, 0.6816, 0.7014, 0.6713,
        0.6582, 0.6734, 0.6534, 0.6285, 0.7038, 0.7110, 0.7862],
       device='cuda:0') torch.Size([16])
percent tensor([0.6356, 0.6438, 0.6293, 0.5925, 0.6192, 0.5665, 0.6589, 0.6413, 0.6486,
        0.6281, 0.6216, 0.6087, 0.6240, 0.6317, 0.6494, 0.5916],
       device='cuda:0') torch.Size([16])
percent tensor([0.5974, 0.7363, 0.7001, 0.7562, 0.7529, 0.7712, 0.7348, 0.6176, 0.7726,
        0.6423, 0.7600, 0.8185, 0.7332, 0.8103, 0.6363, 0.5670],
       device='cuda:0') torch.Size([16])
percent tensor([0.6439, 0.6804, 0.6152, 0.5300, 0.6486, 0.6961, 0.6435, 0.5053, 0.6754,
        0.6887, 0.6985, 0.5189, 0.6816, 0.6226, 0.4382, 0.5539],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9997, 0.9999, 0.9998, 0.9999, 0.9997, 0.9999, 0.9997, 0.9999,
        0.9997, 0.9999, 0.9999, 0.9999, 0.9997, 0.9995, 0.9993],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 168 | Batch_idx: 0 |  Loss: (0.1076) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 168 | Batch_idx: 10 |  Loss: (0.0950) |  Loss2: (0.0000) | Acc: (96.00%) (1363/1408)
Epoch: 168 | Batch_idx: 20 |  Loss: (0.0970) |  Loss2: (0.0000) | Acc: (96.00%) (2599/2688)
Epoch: 168 | Batch_idx: 30 |  Loss: (0.0968) |  Loss2: (0.0000) | Acc: (96.00%) (3842/3968)
Epoch: 168 | Batch_idx: 40 |  Loss: (0.0993) |  Loss2: (0.0000) | Acc: (96.00%) (5069/5248)
Epoch: 168 | Batch_idx: 50 |  Loss: (0.0979) |  Loss2: (0.0000) | Acc: (96.00%) (6318/6528)
Epoch: 168 | Batch_idx: 60 |  Loss: (0.0984) |  Loss2: (0.0000) | Acc: (96.00%) (7553/7808)
Epoch: 168 | Batch_idx: 70 |  Loss: (0.0990) |  Loss2: (0.0000) | Acc: (96.00%) (8789/9088)
Epoch: 168 | Batch_idx: 80 |  Loss: (0.0982) |  Loss2: (0.0000) | Acc: (96.00%) (10029/10368)
Epoch: 168 | Batch_idx: 90 |  Loss: (0.0996) |  Loss2: (0.0000) | Acc: (96.00%) (11260/11648)
Epoch: 168 | Batch_idx: 100 |  Loss: (0.1015) |  Loss2: (0.0000) | Acc: (96.00%) (12484/12928)
Epoch: 168 | Batch_idx: 110 |  Loss: (0.1014) |  Loss2: (0.0000) | Acc: (96.00%) (13722/14208)
Epoch: 168 | Batch_idx: 120 |  Loss: (0.1015) |  Loss2: (0.0000) | Acc: (96.00%) (14950/15488)
Epoch: 168 | Batch_idx: 130 |  Loss: (0.1012) |  Loss2: (0.0000) | Acc: (96.00%) (16191/16768)
Epoch: 168 | Batch_idx: 140 |  Loss: (0.1013) |  Loss2: (0.0000) | Acc: (96.00%) (17424/18048)
Epoch: 168 | Batch_idx: 150 |  Loss: (0.1008) |  Loss2: (0.0000) | Acc: (96.00%) (18667/19328)
Epoch: 168 | Batch_idx: 160 |  Loss: (0.1001) |  Loss2: (0.0000) | Acc: (96.00%) (19910/20608)
Epoch: 168 | Batch_idx: 170 |  Loss: (0.0994) |  Loss2: (0.0000) | Acc: (96.00%) (21147/21888)
Epoch: 168 | Batch_idx: 180 |  Loss: (0.0998) |  Loss2: (0.0000) | Acc: (96.00%) (22381/23168)
Epoch: 168 | Batch_idx: 190 |  Loss: (0.1000) |  Loss2: (0.0000) | Acc: (96.00%) (23616/24448)
Epoch: 168 | Batch_idx: 200 |  Loss: (0.1008) |  Loss2: (0.0000) | Acc: (96.00%) (24837/25728)
Epoch: 168 | Batch_idx: 210 |  Loss: (0.1012) |  Loss2: (0.0000) | Acc: (96.00%) (26070/27008)
Epoch: 168 | Batch_idx: 220 |  Loss: (0.1009) |  Loss2: (0.0000) | Acc: (96.00%) (27305/28288)
Epoch: 168 | Batch_idx: 230 |  Loss: (0.1015) |  Loss2: (0.0000) | Acc: (96.00%) (28532/29568)
Epoch: 168 | Batch_idx: 240 |  Loss: (0.1023) |  Loss2: (0.0000) | Acc: (96.00%) (29757/30848)
Epoch: 168 | Batch_idx: 250 |  Loss: (0.1031) |  Loss2: (0.0000) | Acc: (96.00%) (30979/32128)
Epoch: 168 | Batch_idx: 260 |  Loss: (0.1029) |  Loss2: (0.0000) | Acc: (96.00%) (32218/33408)
Epoch: 168 | Batch_idx: 270 |  Loss: (0.1023) |  Loss2: (0.0000) | Acc: (96.00%) (33459/34688)
Epoch: 168 | Batch_idx: 280 |  Loss: (0.1030) |  Loss2: (0.0000) | Acc: (96.00%) (34686/35968)
Epoch: 168 | Batch_idx: 290 |  Loss: (0.1033) |  Loss2: (0.0000) | Acc: (96.00%) (35907/37248)
Epoch: 168 | Batch_idx: 300 |  Loss: (0.1033) |  Loss2: (0.0000) | Acc: (96.00%) (37146/38528)
Epoch: 168 | Batch_idx: 310 |  Loss: (0.1038) |  Loss2: (0.0000) | Acc: (96.00%) (38371/39808)
Epoch: 168 | Batch_idx: 320 |  Loss: (0.1039) |  Loss2: (0.0000) | Acc: (96.00%) (39604/41088)
Epoch: 168 | Batch_idx: 330 |  Loss: (0.1044) |  Loss2: (0.0000) | Acc: (96.00%) (40832/42368)
Epoch: 168 | Batch_idx: 340 |  Loss: (0.1049) |  Loss2: (0.0000) | Acc: (96.00%) (42058/43648)
Epoch: 168 | Batch_idx: 350 |  Loss: (0.1051) |  Loss2: (0.0000) | Acc: (96.00%) (43280/44928)
Epoch: 168 | Batch_idx: 360 |  Loss: (0.1047) |  Loss2: (0.0000) | Acc: (96.00%) (44526/46208)
Epoch: 168 | Batch_idx: 370 |  Loss: (0.1046) |  Loss2: (0.0000) | Acc: (96.00%) (45762/47488)
Epoch: 168 | Batch_idx: 380 |  Loss: (0.1045) |  Loss2: (0.0000) | Acc: (96.00%) (47003/48768)
Epoch: 168 | Batch_idx: 390 |  Loss: (0.1045) |  Loss2: (0.0000) | Acc: (96.00%) (48193/50000)
# TEST : Loss: (0.4369) | Acc: (87.00%) (8784/10000)
percent tensor([0.5489, 0.5595, 0.5462, 0.5184, 0.5575, 0.5674, 0.5673, 0.5313, 0.5474,
        0.5543, 0.5627, 0.5566, 0.5472, 0.5216, 0.5636, 0.5428],
       device='cuda:0') torch.Size([16])
percent tensor([0.5361, 0.5376, 0.5415, 0.5451, 0.5425, 0.5354, 0.5393, 0.5446, 0.5387,
        0.5387, 0.5364, 0.5393, 0.5359, 0.5412, 0.5363, 0.5387],
       device='cuda:0') torch.Size([16])
percent tensor([0.6592, 0.6832, 0.4865, 0.4476, 0.4398, 0.6142, 0.6209, 0.4956, 0.5610,
        0.6449, 0.6538, 0.5544, 0.6840, 0.6443, 0.6512, 0.6461],
       device='cuda:0') torch.Size([16])
percent tensor([0.7203, 0.6188, 0.6886, 0.6987, 0.6942, 0.7933, 0.6678, 0.7007, 0.6710,
        0.6472, 0.6679, 0.6314, 0.6043, 0.6967, 0.7001, 0.7742],
       device='cuda:0') torch.Size([16])
percent tensor([0.6397, 0.6353, 0.6388, 0.5957, 0.6193, 0.5801, 0.6589, 0.6399, 0.6404,
        0.6328, 0.6123, 0.6048, 0.6254, 0.6340, 0.6501, 0.6007],
       device='cuda:0') torch.Size([16])
percent tensor([0.5926, 0.7705, 0.7208, 0.7830, 0.7485, 0.7978, 0.7489, 0.6432, 0.7885,
        0.6723, 0.7989, 0.8265, 0.7309, 0.8324, 0.6881, 0.5773],
       device='cuda:0') torch.Size([16])
percent tensor([0.6584, 0.6989, 0.6395, 0.5195, 0.6614, 0.7093, 0.6530, 0.5039, 0.6778,
        0.7119, 0.7224, 0.4641, 0.6864, 0.6238, 0.4512, 0.5568],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9998, 0.9999, 0.9999, 0.9999, 0.9996, 0.9999, 0.9998, 0.9999,
        0.9999, 0.9999, 0.9999, 0.9999, 0.9995, 0.9996, 0.9994],
       device='cuda:0') torch.Size([16])
Epoch: 169 | Batch_idx: 0 |  Loss: (0.1220) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 169 | Batch_idx: 10 |  Loss: (0.0932) |  Loss2: (0.0000) | Acc: (97.00%) (1366/1408)
Epoch: 169 | Batch_idx: 20 |  Loss: (0.1052) |  Loss2: (0.0000) | Acc: (96.00%) (2591/2688)
Epoch: 169 | Batch_idx: 30 |  Loss: (0.1088) |  Loss2: (0.0000) | Acc: (96.00%) (3820/3968)
Epoch: 169 | Batch_idx: 40 |  Loss: (0.1084) |  Loss2: (0.0000) | Acc: (96.00%) (5051/5248)
Epoch: 169 | Batch_idx: 50 |  Loss: (0.1020) |  Loss2: (0.0000) | Acc: (96.00%) (6301/6528)
Epoch: 169 | Batch_idx: 60 |  Loss: (0.1016) |  Loss2: (0.0000) | Acc: (96.00%) (7541/7808)
Epoch: 169 | Batch_idx: 70 |  Loss: (0.1025) |  Loss2: (0.0000) | Acc: (96.00%) (8774/9088)
Epoch: 169 | Batch_idx: 80 |  Loss: (0.1008) |  Loss2: (0.0000) | Acc: (96.00%) (10016/10368)
Epoch: 169 | Batch_idx: 90 |  Loss: (0.0975) |  Loss2: (0.0000) | Acc: (96.00%) (11270/11648)
Epoch: 169 | Batch_idx: 100 |  Loss: (0.0979) |  Loss2: (0.0000) | Acc: (96.00%) (12498/12928)
Epoch: 169 | Batch_idx: 110 |  Loss: (0.0968) |  Loss2: (0.0000) | Acc: (96.00%) (13742/14208)
Epoch: 169 | Batch_idx: 120 |  Loss: (0.0969) |  Loss2: (0.0000) | Acc: (96.00%) (14989/15488)
Epoch: 169 | Batch_idx: 130 |  Loss: (0.0966) |  Loss2: (0.0000) | Acc: (96.00%) (16221/16768)
Epoch: 169 | Batch_idx: 140 |  Loss: (0.0968) |  Loss2: (0.0000) | Acc: (96.00%) (17453/18048)
Epoch: 169 | Batch_idx: 150 |  Loss: (0.0965) |  Loss2: (0.0000) | Acc: (96.00%) (18691/19328)
Epoch: 169 | Batch_idx: 160 |  Loss: (0.0955) |  Loss2: (0.0000) | Acc: (96.00%) (19936/20608)
Epoch: 169 | Batch_idx: 170 |  Loss: (0.0946) |  Loss2: (0.0000) | Acc: (96.00%) (21183/21888)
Epoch: 169 | Batch_idx: 180 |  Loss: (0.0941) |  Loss2: (0.0000) | Acc: (96.00%) (22420/23168)
Epoch: 169 | Batch_idx: 190 |  Loss: (0.0943) |  Loss2: (0.0000) | Acc: (96.00%) (23657/24448)
Epoch: 169 | Batch_idx: 200 |  Loss: (0.0939) |  Loss2: (0.0000) | Acc: (96.00%) (24901/25728)
Epoch: 169 | Batch_idx: 210 |  Loss: (0.0936) |  Loss2: (0.0000) | Acc: (96.00%) (26141/27008)
Epoch: 169 | Batch_idx: 220 |  Loss: (0.0945) |  Loss2: (0.0000) | Acc: (96.00%) (27368/28288)
Epoch: 169 | Batch_idx: 230 |  Loss: (0.0941) |  Loss2: (0.0000) | Acc: (96.00%) (28606/29568)
Epoch: 169 | Batch_idx: 240 |  Loss: (0.0946) |  Loss2: (0.0000) | Acc: (96.00%) (29845/30848)
Epoch: 169 | Batch_idx: 250 |  Loss: (0.0943) |  Loss2: (0.0000) | Acc: (96.00%) (31086/32128)
Epoch: 169 | Batch_idx: 260 |  Loss: (0.0938) |  Loss2: (0.0000) | Acc: (96.00%) (32331/33408)
Epoch: 169 | Batch_idx: 270 |  Loss: (0.0936) |  Loss2: (0.0000) | Acc: (96.00%) (33571/34688)
Epoch: 169 | Batch_idx: 280 |  Loss: (0.0943) |  Loss2: (0.0000) | Acc: (96.00%) (34807/35968)
Epoch: 169 | Batch_idx: 290 |  Loss: (0.0950) |  Loss2: (0.0000) | Acc: (96.00%) (36039/37248)
Epoch: 169 | Batch_idx: 300 |  Loss: (0.0954) |  Loss2: (0.0000) | Acc: (96.00%) (37263/38528)
Epoch: 169 | Batch_idx: 310 |  Loss: (0.0962) |  Loss2: (0.0000) | Acc: (96.00%) (38498/39808)
Epoch: 169 | Batch_idx: 320 |  Loss: (0.0964) |  Loss2: (0.0000) | Acc: (96.00%) (39725/41088)
Epoch: 169 | Batch_idx: 330 |  Loss: (0.0961) |  Loss2: (0.0000) | Acc: (96.00%) (40966/42368)
Epoch: 169 | Batch_idx: 340 |  Loss: (0.0963) |  Loss2: (0.0000) | Acc: (96.00%) (42196/43648)
Epoch: 169 | Batch_idx: 350 |  Loss: (0.0966) |  Loss2: (0.0000) | Acc: (96.00%) (43428/44928)
Epoch: 169 | Batch_idx: 360 |  Loss: (0.0968) |  Loss2: (0.0000) | Acc: (96.00%) (44653/46208)
Epoch: 169 | Batch_idx: 370 |  Loss: (0.0972) |  Loss2: (0.0000) | Acc: (96.00%) (45890/47488)
Epoch: 169 | Batch_idx: 380 |  Loss: (0.0970) |  Loss2: (0.0000) | Acc: (96.00%) (47130/48768)
Epoch: 169 | Batch_idx: 390 |  Loss: (0.0973) |  Loss2: (0.0000) | Acc: (96.00%) (48312/50000)
# TEST : Loss: (0.4710) | Acc: (87.00%) (8714/10000)
percent tensor([0.5503, 0.5605, 0.5476, 0.5182, 0.5582, 0.5686, 0.5688, 0.5333, 0.5498,
        0.5558, 0.5652, 0.5586, 0.5490, 0.5235, 0.5648, 0.5437],
       device='cuda:0') torch.Size([16])
percent tensor([0.5366, 0.5378, 0.5424, 0.5458, 0.5434, 0.5355, 0.5395, 0.5450, 0.5390,
        0.5391, 0.5368, 0.5399, 0.5363, 0.5410, 0.5365, 0.5392],
       device='cuda:0') torch.Size([16])
percent tensor([0.6648, 0.6800, 0.4896, 0.4423, 0.4447, 0.6185, 0.6183, 0.4973, 0.5637,
        0.6416, 0.6585, 0.5586, 0.6894, 0.6398, 0.6510, 0.6419],
       device='cuda:0') torch.Size([16])
percent tensor([0.7252, 0.6210, 0.7089, 0.7021, 0.7058, 0.7990, 0.6731, 0.7005, 0.6824,
        0.6586, 0.6695, 0.6552, 0.6103, 0.7074, 0.6993, 0.7748],
       device='cuda:0') torch.Size([16])
percent tensor([0.6344, 0.6425, 0.6382, 0.5993, 0.6302, 0.5802, 0.6616, 0.6456, 0.6443,
        0.6358, 0.6103, 0.6135, 0.6235, 0.6335, 0.6529, 0.5988],
       device='cuda:0') torch.Size([16])
percent tensor([0.5743, 0.7468, 0.6938, 0.7628, 0.7188, 0.7615, 0.7168, 0.5776, 0.7850,
        0.6610, 0.7726, 0.8068, 0.7392, 0.8152, 0.6342, 0.5509],
       device='cuda:0') torch.Size([16])
percent tensor([0.6624, 0.7068, 0.6307, 0.5338, 0.6634, 0.7352, 0.6474, 0.4818, 0.7011,
        0.7141, 0.7103, 0.5254, 0.7099, 0.6143, 0.4663, 0.5735],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9998, 0.9999, 0.9998, 0.9999, 0.9996, 0.9999, 0.9995, 0.9998,
        0.9998, 0.9999, 0.9999, 0.9999, 0.9998, 0.9996, 0.9990],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 170 | Batch_idx: 0 |  Loss: (0.0687) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 170 | Batch_idx: 10 |  Loss: (0.1040) |  Loss2: (0.0000) | Acc: (96.00%) (1359/1408)
Epoch: 170 | Batch_idx: 20 |  Loss: (0.1187) |  Loss2: (0.0000) | Acc: (95.00%) (2577/2688)
Epoch: 170 | Batch_idx: 30 |  Loss: (0.1215) |  Loss2: (0.0000) | Acc: (96.00%) (3811/3968)
Epoch: 170 | Batch_idx: 40 |  Loss: (0.1182) |  Loss2: (0.0000) | Acc: (96.00%) (5041/5248)
Epoch: 170 | Batch_idx: 50 |  Loss: (0.1202) |  Loss2: (0.0000) | Acc: (95.00%) (6257/6528)
Epoch: 170 | Batch_idx: 60 |  Loss: (0.1250) |  Loss2: (0.0000) | Acc: (95.00%) (7468/7808)
Epoch: 170 | Batch_idx: 70 |  Loss: (0.1259) |  Loss2: (0.0000) | Acc: (95.00%) (8689/9088)
Epoch: 170 | Batch_idx: 80 |  Loss: (0.1296) |  Loss2: (0.0000) | Acc: (95.00%) (9899/10368)
Epoch: 170 | Batch_idx: 90 |  Loss: (0.1321) |  Loss2: (0.0000) | Acc: (95.00%) (11106/11648)
Epoch: 170 | Batch_idx: 100 |  Loss: (0.1324) |  Loss2: (0.0000) | Acc: (95.00%) (12318/12928)
Epoch: 170 | Batch_idx: 110 |  Loss: (0.1314) |  Loss2: (0.0000) | Acc: (95.00%) (13540/14208)
Epoch: 170 | Batch_idx: 120 |  Loss: (0.1316) |  Loss2: (0.0000) | Acc: (95.00%) (14761/15488)
Epoch: 170 | Batch_idx: 130 |  Loss: (0.1308) |  Loss2: (0.0000) | Acc: (95.00%) (15995/16768)
Epoch: 170 | Batch_idx: 140 |  Loss: (0.1310) |  Loss2: (0.0000) | Acc: (95.00%) (17216/18048)
Epoch: 170 | Batch_idx: 150 |  Loss: (0.1300) |  Loss2: (0.0000) | Acc: (95.00%) (18448/19328)
Epoch: 170 | Batch_idx: 160 |  Loss: (0.1288) |  Loss2: (0.0000) | Acc: (95.00%) (19683/20608)
Epoch: 170 | Batch_idx: 170 |  Loss: (0.1290) |  Loss2: (0.0000) | Acc: (95.00%) (20900/21888)
Epoch: 170 | Batch_idx: 180 |  Loss: (0.1284) |  Loss2: (0.0000) | Acc: (95.00%) (22120/23168)
Epoch: 170 | Batch_idx: 190 |  Loss: (0.1279) |  Loss2: (0.0000) | Acc: (95.00%) (23345/24448)
Epoch: 170 | Batch_idx: 200 |  Loss: (0.1298) |  Loss2: (0.0000) | Acc: (95.00%) (24554/25728)
Epoch: 170 | Batch_idx: 210 |  Loss: (0.1296) |  Loss2: (0.0000) | Acc: (95.00%) (25779/27008)
Epoch: 170 | Batch_idx: 220 |  Loss: (0.1297) |  Loss2: (0.0000) | Acc: (95.00%) (26995/28288)
Epoch: 170 | Batch_idx: 230 |  Loss: (0.1299) |  Loss2: (0.0000) | Acc: (95.00%) (28209/29568)
Epoch: 170 | Batch_idx: 240 |  Loss: (0.1298) |  Loss2: (0.0000) | Acc: (95.00%) (29431/30848)
Epoch: 170 | Batch_idx: 250 |  Loss: (0.1290) |  Loss2: (0.0000) | Acc: (95.00%) (30659/32128)
Epoch: 170 | Batch_idx: 260 |  Loss: (0.1286) |  Loss2: (0.0000) | Acc: (95.00%) (31887/33408)
Epoch: 170 | Batch_idx: 270 |  Loss: (0.1281) |  Loss2: (0.0000) | Acc: (95.00%) (33123/34688)
Epoch: 170 | Batch_idx: 280 |  Loss: (0.1271) |  Loss2: (0.0000) | Acc: (95.00%) (34361/35968)
Epoch: 170 | Batch_idx: 290 |  Loss: (0.1271) |  Loss2: (0.0000) | Acc: (95.00%) (35584/37248)
Epoch: 170 | Batch_idx: 300 |  Loss: (0.1261) |  Loss2: (0.0000) | Acc: (95.00%) (36818/38528)
Epoch: 170 | Batch_idx: 310 |  Loss: (0.1256) |  Loss2: (0.0000) | Acc: (95.00%) (38050/39808)
Epoch: 170 | Batch_idx: 320 |  Loss: (0.1257) |  Loss2: (0.0000) | Acc: (95.00%) (39270/41088)
Epoch: 170 | Batch_idx: 330 |  Loss: (0.1251) |  Loss2: (0.0000) | Acc: (95.00%) (40499/42368)
Epoch: 170 | Batch_idx: 340 |  Loss: (0.1248) |  Loss2: (0.0000) | Acc: (95.00%) (41720/43648)
Epoch: 170 | Batch_idx: 350 |  Loss: (0.1249) |  Loss2: (0.0000) | Acc: (95.00%) (42940/44928)
Epoch: 170 | Batch_idx: 360 |  Loss: (0.1247) |  Loss2: (0.0000) | Acc: (95.00%) (44168/46208)
Epoch: 170 | Batch_idx: 370 |  Loss: (0.1242) |  Loss2: (0.0000) | Acc: (95.00%) (45397/47488)
Epoch: 170 | Batch_idx: 380 |  Loss: (0.1244) |  Loss2: (0.0000) | Acc: (95.00%) (46627/48768)
Epoch: 170 | Batch_idx: 390 |  Loss: (0.1241) |  Loss2: (0.0000) | Acc: (95.00%) (47807/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_170.pth.tar'
# TEST : Loss: (0.4224) | Acc: (87.00%) (8783/10000)
percent tensor([0.5480, 0.5620, 0.5502, 0.5185, 0.5608, 0.5690, 0.5705, 0.5337, 0.5479,
        0.5559, 0.5642, 0.5605, 0.5467, 0.5227, 0.5662, 0.5418],
       device='cuda:0') torch.Size([16])
percent tensor([0.5346, 0.5358, 0.5407, 0.5430, 0.5416, 0.5330, 0.5382, 0.5434, 0.5375,
        0.5371, 0.5343, 0.5385, 0.5344, 0.5386, 0.5342, 0.5368],
       device='cuda:0') torch.Size([16])
percent tensor([0.6711, 0.6947, 0.4855, 0.4321, 0.4506, 0.6103, 0.6351, 0.4986, 0.5524,
        0.6487, 0.6652, 0.5544, 0.6935, 0.6321, 0.6693, 0.6480],
       device='cuda:0') torch.Size([16])
percent tensor([0.7007, 0.6068, 0.6796, 0.6720, 0.6867, 0.7778, 0.6557, 0.6740, 0.6575,
        0.6391, 0.6473, 0.6293, 0.5931, 0.6817, 0.6782, 0.7536],
       device='cuda:0') torch.Size([16])
percent tensor([0.6490, 0.6578, 0.6477, 0.6119, 0.6343, 0.6152, 0.6718, 0.6505, 0.6586,
        0.6491, 0.6298, 0.6250, 0.6454, 0.6543, 0.6580, 0.6235],
       device='cuda:0') torch.Size([16])
percent tensor([0.6039, 0.7614, 0.6940, 0.7489, 0.7112, 0.7669, 0.7253, 0.5686, 0.7998,
        0.6880, 0.7888, 0.8211, 0.7604, 0.8280, 0.6459, 0.5571],
       device='cuda:0') torch.Size([16])
percent tensor([0.6388, 0.6784, 0.5928, 0.5140, 0.6331, 0.7142, 0.6097, 0.4439, 0.6796,
        0.7023, 0.6895, 0.4930, 0.6873, 0.5762, 0.4285, 0.5321],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9998, 0.9999, 0.9998, 0.9999, 0.9997, 0.9999, 0.9995, 0.9998,
        0.9998, 0.9999, 0.9999, 0.9999, 0.9997, 0.9996, 0.9990],
       device='cuda:0') torch.Size([16])
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(183.8405, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(829.2949, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(825.2684, device='cuda:0')
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1511.1451, device='cuda:0')
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(481.1688, device='cuda:0')
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2290.6477, device='cuda:0')
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4241.8374, device='cuda:0')
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1363.6211, device='cuda:0')
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6249.1699, device='cuda:0')
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11637.4941, device='cuda:0')
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3833.9312, device='cuda:0')
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16179.8906, device='cuda:0')
Epoch: 171 | Batch_idx: 0 |  Loss: (0.1203) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 171 | Batch_idx: 10 |  Loss: (0.1169) |  Loss2: (0.0000) | Acc: (96.00%) (1353/1408)
Epoch: 171 | Batch_idx: 20 |  Loss: (0.1065) |  Loss2: (0.0000) | Acc: (96.00%) (2593/2688)
Epoch: 171 | Batch_idx: 30 |  Loss: (0.1046) |  Loss2: (0.0000) | Acc: (96.00%) (3828/3968)
Epoch: 171 | Batch_idx: 40 |  Loss: (0.1137) |  Loss2: (0.0000) | Acc: (96.00%) (5044/5248)
Epoch: 171 | Batch_idx: 50 |  Loss: (0.1121) |  Loss2: (0.0000) | Acc: (96.00%) (6275/6528)
Epoch: 171 | Batch_idx: 60 |  Loss: (0.1130) |  Loss2: (0.0000) | Acc: (96.00%) (7511/7808)
Epoch: 171 | Batch_idx: 70 |  Loss: (0.1122) |  Loss2: (0.0000) | Acc: (96.00%) (8739/9088)
Epoch: 171 | Batch_idx: 80 |  Loss: (0.1136) |  Loss2: (0.0000) | Acc: (96.00%) (9963/10368)
Epoch: 171 | Batch_idx: 90 |  Loss: (0.1132) |  Loss2: (0.0000) | Acc: (96.00%) (11184/11648)
Epoch: 171 | Batch_idx: 100 |  Loss: (0.1136) |  Loss2: (0.0000) | Acc: (96.00%) (12414/12928)
Epoch: 171 | Batch_idx: 110 |  Loss: (0.1118) |  Loss2: (0.0000) | Acc: (96.00%) (13657/14208)
Epoch: 171 | Batch_idx: 120 |  Loss: (0.1106) |  Loss2: (0.0000) | Acc: (96.00%) (14896/15488)
Epoch: 171 | Batch_idx: 130 |  Loss: (0.1110) |  Loss2: (0.0000) | Acc: (96.00%) (16116/16768)
Epoch: 171 | Batch_idx: 140 |  Loss: (0.1107) |  Loss2: (0.0000) | Acc: (96.00%) (17340/18048)
Epoch: 171 | Batch_idx: 150 |  Loss: (0.1111) |  Loss2: (0.0000) | Acc: (96.00%) (18564/19328)
Epoch: 171 | Batch_idx: 160 |  Loss: (0.1107) |  Loss2: (0.0000) | Acc: (96.00%) (19801/20608)
Epoch: 171 | Batch_idx: 170 |  Loss: (0.1107) |  Loss2: (0.0000) | Acc: (96.00%) (21025/21888)
Epoch: 171 | Batch_idx: 180 |  Loss: (0.1113) |  Loss2: (0.0000) | Acc: (96.00%) (22247/23168)
Epoch: 171 | Batch_idx: 190 |  Loss: (0.1106) |  Loss2: (0.0000) | Acc: (96.00%) (23490/24448)
Epoch: 171 | Batch_idx: 200 |  Loss: (0.1107) |  Loss2: (0.0000) | Acc: (96.00%) (24722/25728)
Epoch: 171 | Batch_idx: 210 |  Loss: (0.1103) |  Loss2: (0.0000) | Acc: (96.00%) (25954/27008)
Epoch: 171 | Batch_idx: 220 |  Loss: (0.1113) |  Loss2: (0.0000) | Acc: (96.00%) (27175/28288)
Epoch: 171 | Batch_idx: 230 |  Loss: (0.1115) |  Loss2: (0.0000) | Acc: (96.00%) (28404/29568)
Epoch: 171 | Batch_idx: 240 |  Loss: (0.1115) |  Loss2: (0.0000) | Acc: (96.00%) (29638/30848)
Epoch: 171 | Batch_idx: 250 |  Loss: (0.1110) |  Loss2: (0.0000) | Acc: (96.00%) (30879/32128)
Epoch: 171 | Batch_idx: 260 |  Loss: (0.1107) |  Loss2: (0.0000) | Acc: (96.00%) (32116/33408)
Epoch: 171 | Batch_idx: 270 |  Loss: (0.1109) |  Loss2: (0.0000) | Acc: (96.00%) (33343/34688)
Epoch: 171 | Batch_idx: 280 |  Loss: (0.1109) |  Loss2: (0.0000) | Acc: (96.00%) (34576/35968)
Epoch: 171 | Batch_idx: 290 |  Loss: (0.1099) |  Loss2: (0.0000) | Acc: (96.00%) (35822/37248)
Epoch: 171 | Batch_idx: 300 |  Loss: (0.1095) |  Loss2: (0.0000) | Acc: (96.00%) (37055/38528)
Epoch: 171 | Batch_idx: 310 |  Loss: (0.1089) |  Loss2: (0.0000) | Acc: (96.00%) (38293/39808)
Epoch: 171 | Batch_idx: 320 |  Loss: (0.1083) |  Loss2: (0.0000) | Acc: (96.00%) (39532/41088)
Epoch: 171 | Batch_idx: 330 |  Loss: (0.1084) |  Loss2: (0.0000) | Acc: (96.00%) (40763/42368)
Epoch: 171 | Batch_idx: 340 |  Loss: (0.1079) |  Loss2: (0.0000) | Acc: (96.00%) (42002/43648)
Epoch: 171 | Batch_idx: 350 |  Loss: (0.1077) |  Loss2: (0.0000) | Acc: (96.00%) (43232/44928)
Epoch: 171 | Batch_idx: 360 |  Loss: (0.1081) |  Loss2: (0.0000) | Acc: (96.00%) (44459/46208)
Epoch: 171 | Batch_idx: 370 |  Loss: (0.1084) |  Loss2: (0.0000) | Acc: (96.00%) (45687/47488)
Epoch: 171 | Batch_idx: 380 |  Loss: (0.1076) |  Loss2: (0.0000) | Acc: (96.00%) (46929/48768)
Epoch: 171 | Batch_idx: 390 |  Loss: (0.1075) |  Loss2: (0.0000) | Acc: (96.00%) (48121/50000)
# TEST : Loss: (0.4090) | Acc: (88.00%) (8839/10000)
percent tensor([0.5448, 0.5601, 0.5504, 0.5175, 0.5604, 0.5677, 0.5692, 0.5331, 0.5452,
        0.5542, 0.5613, 0.5602, 0.5437, 0.5195, 0.5649, 0.5389],
       device='cuda:0') torch.Size([16])
percent tensor([0.5344, 0.5361, 0.5413, 0.5433, 0.5423, 0.5325, 0.5386, 0.5441, 0.5376,
        0.5373, 0.5341, 0.5390, 0.5344, 0.5386, 0.5343, 0.5367],
       device='cuda:0') torch.Size([16])
percent tensor([0.6809, 0.6969, 0.4904, 0.4355, 0.4568, 0.6202, 0.6417, 0.5060, 0.5565,
        0.6537, 0.6725, 0.5561, 0.6966, 0.6370, 0.6773, 0.6576],
       device='cuda:0') torch.Size([16])
percent tensor([0.7010, 0.6112, 0.6787, 0.6685, 0.6876, 0.7772, 0.6581, 0.6750, 0.6558,
        0.6422, 0.6459, 0.6288, 0.5937, 0.6820, 0.6793, 0.7557],
       device='cuda:0') torch.Size([16])
percent tensor([0.6365, 0.6523, 0.6356, 0.5987, 0.6183, 0.6125, 0.6613, 0.6366, 0.6533,
        0.6417, 0.6237, 0.6195, 0.6423, 0.6515, 0.6441, 0.6142],
       device='cuda:0') torch.Size([16])
percent tensor([0.6086, 0.7520, 0.6891, 0.7430, 0.7046, 0.7630, 0.7266, 0.5685, 0.7958,
        0.6861, 0.7876, 0.8201, 0.7539, 0.8278, 0.6515, 0.5579],
       device='cuda:0') torch.Size([16])
percent tensor([0.6414, 0.6863, 0.5967, 0.5128, 0.6412, 0.7198, 0.6172, 0.4433, 0.6911,
        0.7172, 0.7042, 0.5027, 0.6948, 0.5944, 0.4316, 0.5280],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9998, 0.9999, 0.9999, 0.9999, 0.9997, 0.9999, 0.9996, 0.9998,
        0.9998, 0.9999, 0.9999, 0.9998, 0.9997, 0.9996, 0.9990],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 172 | Batch_idx: 0 |  Loss: (0.1009) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 172 | Batch_idx: 10 |  Loss: (0.1162) |  Loss2: (0.0000) | Acc: (96.00%) (1355/1408)
Epoch: 172 | Batch_idx: 20 |  Loss: (0.0990) |  Loss2: (0.0000) | Acc: (96.00%) (2605/2688)
Epoch: 172 | Batch_idx: 30 |  Loss: (0.0940) |  Loss2: (0.0000) | Acc: (97.00%) (3849/3968)
Epoch: 172 | Batch_idx: 40 |  Loss: (0.0939) |  Loss2: (0.0000) | Acc: (97.00%) (5093/5248)
Epoch: 172 | Batch_idx: 50 |  Loss: (0.0929) |  Loss2: (0.0000) | Acc: (96.00%) (6332/6528)
Epoch: 172 | Batch_idx: 60 |  Loss: (0.0945) |  Loss2: (0.0000) | Acc: (96.00%) (7571/7808)
Epoch: 172 | Batch_idx: 70 |  Loss: (0.0936) |  Loss2: (0.0000) | Acc: (96.00%) (8814/9088)
Epoch: 172 | Batch_idx: 80 |  Loss: (0.0924) |  Loss2: (0.0000) | Acc: (96.00%) (10056/10368)
Epoch: 172 | Batch_idx: 90 |  Loss: (0.0950) |  Loss2: (0.0000) | Acc: (96.00%) (11280/11648)
Epoch: 172 | Batch_idx: 100 |  Loss: (0.0963) |  Loss2: (0.0000) | Acc: (96.00%) (12511/12928)
Epoch: 172 | Batch_idx: 110 |  Loss: (0.0971) |  Loss2: (0.0000) | Acc: (96.00%) (13744/14208)
Epoch: 172 | Batch_idx: 120 |  Loss: (0.0978) |  Loss2: (0.0000) | Acc: (96.00%) (14978/15488)
Epoch: 172 | Batch_idx: 130 |  Loss: (0.0986) |  Loss2: (0.0000) | Acc: (96.00%) (16206/16768)
Epoch: 172 | Batch_idx: 140 |  Loss: (0.0984) |  Loss2: (0.0000) | Acc: (96.00%) (17443/18048)
Epoch: 172 | Batch_idx: 150 |  Loss: (0.0989) |  Loss2: (0.0000) | Acc: (96.00%) (18679/19328)
Epoch: 172 | Batch_idx: 160 |  Loss: (0.0997) |  Loss2: (0.0000) | Acc: (96.00%) (19909/20608)
Epoch: 172 | Batch_idx: 170 |  Loss: (0.0998) |  Loss2: (0.0000) | Acc: (96.00%) (21151/21888)
Epoch: 172 | Batch_idx: 180 |  Loss: (0.1000) |  Loss2: (0.0000) | Acc: (96.00%) (22386/23168)
Epoch: 172 | Batch_idx: 190 |  Loss: (0.0998) |  Loss2: (0.0000) | Acc: (96.00%) (23626/24448)
Epoch: 172 | Batch_idx: 200 |  Loss: (0.1000) |  Loss2: (0.0000) | Acc: (96.00%) (24858/25728)
Epoch: 172 | Batch_idx: 210 |  Loss: (0.1003) |  Loss2: (0.0000) | Acc: (96.00%) (26089/27008)
Epoch: 172 | Batch_idx: 220 |  Loss: (0.1006) |  Loss2: (0.0000) | Acc: (96.00%) (27326/28288)
Epoch: 172 | Batch_idx: 230 |  Loss: (0.1002) |  Loss2: (0.0000) | Acc: (96.00%) (28562/29568)
Epoch: 172 | Batch_idx: 240 |  Loss: (0.1007) |  Loss2: (0.0000) | Acc: (96.00%) (29789/30848)
Epoch: 172 | Batch_idx: 250 |  Loss: (0.1010) |  Loss2: (0.0000) | Acc: (96.00%) (31022/32128)
Epoch: 172 | Batch_idx: 260 |  Loss: (0.1010) |  Loss2: (0.0000) | Acc: (96.00%) (32259/33408)
Epoch: 172 | Batch_idx: 270 |  Loss: (0.1011) |  Loss2: (0.0000) | Acc: (96.00%) (33494/34688)
Epoch: 172 | Batch_idx: 280 |  Loss: (0.1009) |  Loss2: (0.0000) | Acc: (96.00%) (34732/35968)
Epoch: 172 | Batch_idx: 290 |  Loss: (0.1014) |  Loss2: (0.0000) | Acc: (96.00%) (35951/37248)
Epoch: 172 | Batch_idx: 300 |  Loss: (0.1017) |  Loss2: (0.0000) | Acc: (96.00%) (37180/38528)
Epoch: 172 | Batch_idx: 310 |  Loss: (0.1020) |  Loss2: (0.0000) | Acc: (96.00%) (38416/39808)
Epoch: 172 | Batch_idx: 320 |  Loss: (0.1022) |  Loss2: (0.0000) | Acc: (96.00%) (39648/41088)
Epoch: 172 | Batch_idx: 330 |  Loss: (0.1024) |  Loss2: (0.0000) | Acc: (96.00%) (40880/42368)
Epoch: 172 | Batch_idx: 340 |  Loss: (0.1024) |  Loss2: (0.0000) | Acc: (96.00%) (42116/43648)
Epoch: 172 | Batch_idx: 350 |  Loss: (0.1026) |  Loss2: (0.0000) | Acc: (96.00%) (43355/44928)
Epoch: 172 | Batch_idx: 360 |  Loss: (0.1029) |  Loss2: (0.0000) | Acc: (96.00%) (44593/46208)
Epoch: 172 | Batch_idx: 370 |  Loss: (0.1028) |  Loss2: (0.0000) | Acc: (96.00%) (45828/47488)
Epoch: 172 | Batch_idx: 380 |  Loss: (0.1026) |  Loss2: (0.0000) | Acc: (96.00%) (47071/48768)
Epoch: 172 | Batch_idx: 390 |  Loss: (0.1026) |  Loss2: (0.0000) | Acc: (96.00%) (48251/50000)
# TEST : Loss: (0.4011) | Acc: (88.00%) (8879/10000)
percent tensor([0.5435, 0.5616, 0.5445, 0.5178, 0.5549, 0.5687, 0.5677, 0.5316, 0.5439,
        0.5528, 0.5610, 0.5541, 0.5425, 0.5238, 0.5654, 0.5404],
       device='cuda:0') torch.Size([16])
percent tensor([0.5340, 0.5366, 0.5404, 0.5418, 0.5415, 0.5322, 0.5382, 0.5434, 0.5370,
        0.5367, 0.5340, 0.5377, 0.5342, 0.5388, 0.5344, 0.5362],
       device='cuda:0') torch.Size([16])
percent tensor([0.6891, 0.6995, 0.4790, 0.4559, 0.4479, 0.6295, 0.6403, 0.5043, 0.5816,
        0.6622, 0.6934, 0.5495, 0.7054, 0.6507, 0.6786, 0.6662],
       device='cuda:0') torch.Size([16])
percent tensor([0.7004, 0.6139, 0.6822, 0.6773, 0.6798, 0.7737, 0.6528, 0.6881, 0.6496,
        0.6452, 0.6480, 0.6165, 0.5895, 0.6841, 0.6780, 0.7613],
       device='cuda:0') torch.Size([16])
percent tensor([0.6316, 0.6530, 0.6359, 0.5901, 0.6179, 0.6014, 0.6619, 0.6314, 0.6592,
        0.6470, 0.6292, 0.6211, 0.6419, 0.6433, 0.6468, 0.6010],
       device='cuda:0') torch.Size([16])
percent tensor([0.6026, 0.7344, 0.6855, 0.7528, 0.7089, 0.7749, 0.7421, 0.5950, 0.7613,
        0.6120, 0.7633, 0.7937, 0.7336, 0.8129, 0.6376, 0.5941],
       device='cuda:0') torch.Size([16])
percent tensor([0.6179, 0.6573, 0.6158, 0.4949, 0.6517, 0.6914, 0.6131, 0.4876, 0.6737,
        0.6668, 0.6868, 0.4589, 0.6901, 0.5612, 0.4233, 0.5352],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9998, 0.9999, 0.9998, 0.9999, 0.9996, 0.9999, 0.9997, 0.9999,
        0.9998, 0.9999, 0.9999, 0.9999, 0.9994, 0.9995, 0.9993],
       device='cuda:0') torch.Size([16])
Epoch: 173 | Batch_idx: 0 |  Loss: (0.1380) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 173 | Batch_idx: 10 |  Loss: (0.0796) |  Loss2: (0.0000) | Acc: (97.00%) (1369/1408)
Epoch: 173 | Batch_idx: 20 |  Loss: (0.0834) |  Loss2: (0.0000) | Acc: (97.00%) (2609/2688)
Epoch: 173 | Batch_idx: 30 |  Loss: (0.0849) |  Loss2: (0.0000) | Acc: (97.00%) (3852/3968)
Epoch: 173 | Batch_idx: 40 |  Loss: (0.0849) |  Loss2: (0.0000) | Acc: (97.00%) (5100/5248)
Epoch: 173 | Batch_idx: 50 |  Loss: (0.0841) |  Loss2: (0.0000) | Acc: (97.00%) (6346/6528)
Epoch: 173 | Batch_idx: 60 |  Loss: (0.0828) |  Loss2: (0.0000) | Acc: (97.00%) (7598/7808)
Epoch: 173 | Batch_idx: 70 |  Loss: (0.0815) |  Loss2: (0.0000) | Acc: (97.00%) (8851/9088)
Epoch: 173 | Batch_idx: 80 |  Loss: (0.0823) |  Loss2: (0.0000) | Acc: (97.00%) (10092/10368)
Epoch: 173 | Batch_idx: 90 |  Loss: (0.0831) |  Loss2: (0.0000) | Acc: (97.00%) (11334/11648)
Epoch: 173 | Batch_idx: 100 |  Loss: (0.0847) |  Loss2: (0.0000) | Acc: (97.00%) (12576/12928)
Epoch: 173 | Batch_idx: 110 |  Loss: (0.0849) |  Loss2: (0.0000) | Acc: (97.00%) (13824/14208)
Epoch: 173 | Batch_idx: 120 |  Loss: (0.0859) |  Loss2: (0.0000) | Acc: (97.00%) (15067/15488)
Epoch: 173 | Batch_idx: 130 |  Loss: (0.0866) |  Loss2: (0.0000) | Acc: (97.00%) (16308/16768)
Epoch: 173 | Batch_idx: 140 |  Loss: (0.0861) |  Loss2: (0.0000) | Acc: (97.00%) (17555/18048)
Epoch: 173 | Batch_idx: 150 |  Loss: (0.0879) |  Loss2: (0.0000) | Acc: (97.00%) (18788/19328)
Epoch: 173 | Batch_idx: 160 |  Loss: (0.0882) |  Loss2: (0.0000) | Acc: (97.00%) (20031/20608)
Epoch: 173 | Batch_idx: 170 |  Loss: (0.0886) |  Loss2: (0.0000) | Acc: (97.00%) (21272/21888)
Epoch: 173 | Batch_idx: 180 |  Loss: (0.0897) |  Loss2: (0.0000) | Acc: (97.00%) (22505/23168)
Epoch: 173 | Batch_idx: 190 |  Loss: (0.0897) |  Loss2: (0.0000) | Acc: (97.00%) (23744/24448)
Epoch: 173 | Batch_idx: 200 |  Loss: (0.0902) |  Loss2: (0.0000) | Acc: (97.00%) (24985/25728)
Epoch: 173 | Batch_idx: 210 |  Loss: (0.0900) |  Loss2: (0.0000) | Acc: (97.00%) (26233/27008)
Epoch: 173 | Batch_idx: 220 |  Loss: (0.0902) |  Loss2: (0.0000) | Acc: (97.00%) (27470/28288)
Epoch: 173 | Batch_idx: 230 |  Loss: (0.0914) |  Loss2: (0.0000) | Acc: (97.00%) (28698/29568)
Epoch: 173 | Batch_idx: 240 |  Loss: (0.0913) |  Loss2: (0.0000) | Acc: (97.00%) (29933/30848)
Epoch: 173 | Batch_idx: 250 |  Loss: (0.0911) |  Loss2: (0.0000) | Acc: (97.00%) (31177/32128)
Epoch: 173 | Batch_idx: 260 |  Loss: (0.0916) |  Loss2: (0.0000) | Acc: (97.00%) (32410/33408)
Epoch: 173 | Batch_idx: 270 |  Loss: (0.0917) |  Loss2: (0.0000) | Acc: (97.00%) (33652/34688)
Epoch: 173 | Batch_idx: 280 |  Loss: (0.0925) |  Loss2: (0.0000) | Acc: (96.00%) (34884/35968)
Epoch: 173 | Batch_idx: 290 |  Loss: (0.0927) |  Loss2: (0.0000) | Acc: (96.00%) (36119/37248)
Epoch: 173 | Batch_idx: 300 |  Loss: (0.0927) |  Loss2: (0.0000) | Acc: (96.00%) (37354/38528)
Epoch: 173 | Batch_idx: 310 |  Loss: (0.0930) |  Loss2: (0.0000) | Acc: (96.00%) (38582/39808)
Epoch: 173 | Batch_idx: 320 |  Loss: (0.0937) |  Loss2: (0.0000) | Acc: (96.00%) (39816/41088)
Epoch: 173 | Batch_idx: 330 |  Loss: (0.0942) |  Loss2: (0.0000) | Acc: (96.00%) (41044/42368)
Epoch: 173 | Batch_idx: 340 |  Loss: (0.0946) |  Loss2: (0.0000) | Acc: (96.00%) (42278/43648)
Epoch: 173 | Batch_idx: 350 |  Loss: (0.0948) |  Loss2: (0.0000) | Acc: (96.00%) (43512/44928)
Epoch: 173 | Batch_idx: 360 |  Loss: (0.0948) |  Loss2: (0.0000) | Acc: (96.00%) (44758/46208)
Epoch: 173 | Batch_idx: 370 |  Loss: (0.0946) |  Loss2: (0.0000) | Acc: (96.00%) (46002/47488)
Epoch: 173 | Batch_idx: 380 |  Loss: (0.0951) |  Loss2: (0.0000) | Acc: (96.00%) (47228/48768)
Epoch: 173 | Batch_idx: 390 |  Loss: (0.0957) |  Loss2: (0.0000) | Acc: (96.00%) (48411/50000)
# TEST : Loss: (0.4205) | Acc: (87.00%) (8788/10000)
percent tensor([0.5448, 0.5612, 0.5479, 0.5170, 0.5589, 0.5687, 0.5694, 0.5309, 0.5448,
        0.5545, 0.5613, 0.5589, 0.5434, 0.5219, 0.5646, 0.5395],
       device='cuda:0') torch.Size([16])
percent tensor([0.5338, 0.5355, 0.5413, 0.5433, 0.5425, 0.5328, 0.5380, 0.5448, 0.5373,
        0.5368, 0.5336, 0.5385, 0.5343, 0.5376, 0.5342, 0.5362],
       device='cuda:0') torch.Size([16])
percent tensor([0.6855, 0.6947, 0.4859, 0.4347, 0.4443, 0.6309, 0.6328, 0.4818, 0.5735,
        0.6490, 0.6868, 0.5516, 0.6975, 0.6415, 0.6707, 0.6567],
       device='cuda:0') torch.Size([16])
percent tensor([0.6962, 0.5981, 0.6773, 0.6712, 0.6838, 0.7778, 0.6514, 0.6768, 0.6477,
        0.6412, 0.6403, 0.6182, 0.5852, 0.6751, 0.6661, 0.7509],
       device='cuda:0') torch.Size([16])
percent tensor([0.6459, 0.6539, 0.6371, 0.5875, 0.6165, 0.6035, 0.6652, 0.6341, 0.6660,
        0.6429, 0.6348, 0.6307, 0.6564, 0.6501, 0.6498, 0.6105],
       device='cuda:0') torch.Size([16])
percent tensor([0.5796, 0.7332, 0.6996, 0.7584, 0.7506, 0.7524, 0.7446, 0.6067, 0.7743,
        0.6304, 0.7626, 0.8151, 0.7261, 0.8040, 0.6346, 0.5547],
       device='cuda:0') torch.Size([16])
percent tensor([0.6058, 0.6486, 0.6146, 0.4996, 0.6572, 0.6952, 0.6245, 0.4508, 0.6819,
        0.6497, 0.6784, 0.4600, 0.6774, 0.5764, 0.4212, 0.5016],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9997, 0.9999, 0.9998, 0.9999, 0.9998, 0.9998, 0.9996, 0.9998,
        0.9995, 0.9999, 0.9998, 0.9999, 0.9997, 0.9996, 0.9992],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 174 | Batch_idx: 0 |  Loss: (0.0682) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 174 | Batch_idx: 10 |  Loss: (0.0766) |  Loss2: (0.0000) | Acc: (97.00%) (1375/1408)
Epoch: 174 | Batch_idx: 20 |  Loss: (0.0953) |  Loss2: (0.0000) | Acc: (97.00%) (2610/2688)
Epoch: 174 | Batch_idx: 30 |  Loss: (0.1055) |  Loss2: (0.0000) | Acc: (96.00%) (3832/3968)
Epoch: 174 | Batch_idx: 40 |  Loss: (0.1138) |  Loss2: (0.0000) | Acc: (96.00%) (5046/5248)
Epoch: 174 | Batch_idx: 50 |  Loss: (0.1134) |  Loss2: (0.0000) | Acc: (96.00%) (6275/6528)
Epoch: 174 | Batch_idx: 60 |  Loss: (0.1178) |  Loss2: (0.0000) | Acc: (95.00%) (7492/7808)
Epoch: 174 | Batch_idx: 70 |  Loss: (0.1201) |  Loss2: (0.0000) | Acc: (95.00%) (8709/9088)
Epoch: 174 | Batch_idx: 80 |  Loss: (0.1260) |  Loss2: (0.0000) | Acc: (95.00%) (9913/10368)
Epoch: 174 | Batch_idx: 90 |  Loss: (0.1300) |  Loss2: (0.0000) | Acc: (95.00%) (11119/11648)
Epoch: 174 | Batch_idx: 100 |  Loss: (0.1320) |  Loss2: (0.0000) | Acc: (95.00%) (12330/12928)
Epoch: 174 | Batch_idx: 110 |  Loss: (0.1319) |  Loss2: (0.0000) | Acc: (95.00%) (13552/14208)
Epoch: 174 | Batch_idx: 120 |  Loss: (0.1317) |  Loss2: (0.0000) | Acc: (95.00%) (14773/15488)
Epoch: 174 | Batch_idx: 130 |  Loss: (0.1298) |  Loss2: (0.0000) | Acc: (95.00%) (16009/16768)
Epoch: 174 | Batch_idx: 140 |  Loss: (0.1300) |  Loss2: (0.0000) | Acc: (95.00%) (17240/18048)
Epoch: 174 | Batch_idx: 150 |  Loss: (0.1303) |  Loss2: (0.0000) | Acc: (95.00%) (18454/19328)
Epoch: 174 | Batch_idx: 160 |  Loss: (0.1309) |  Loss2: (0.0000) | Acc: (95.00%) (19670/20608)
Epoch: 174 | Batch_idx: 170 |  Loss: (0.1295) |  Loss2: (0.0000) | Acc: (95.00%) (20906/21888)
Epoch: 174 | Batch_idx: 180 |  Loss: (0.1305) |  Loss2: (0.0000) | Acc: (95.00%) (22123/23168)
Epoch: 174 | Batch_idx: 190 |  Loss: (0.1304) |  Loss2: (0.0000) | Acc: (95.00%) (23339/24448)
Epoch: 174 | Batch_idx: 200 |  Loss: (0.1291) |  Loss2: (0.0000) | Acc: (95.00%) (24567/25728)
Epoch: 174 | Batch_idx: 210 |  Loss: (0.1293) |  Loss2: (0.0000) | Acc: (95.00%) (25787/27008)
Epoch: 174 | Batch_idx: 220 |  Loss: (0.1300) |  Loss2: (0.0000) | Acc: (95.00%) (27005/28288)
Epoch: 174 | Batch_idx: 230 |  Loss: (0.1294) |  Loss2: (0.0000) | Acc: (95.00%) (28236/29568)
Epoch: 174 | Batch_idx: 240 |  Loss: (0.1288) |  Loss2: (0.0000) | Acc: (95.00%) (29461/30848)
Epoch: 174 | Batch_idx: 250 |  Loss: (0.1282) |  Loss2: (0.0000) | Acc: (95.00%) (30695/32128)
Epoch: 174 | Batch_idx: 260 |  Loss: (0.1281) |  Loss2: (0.0000) | Acc: (95.00%) (31914/33408)
Epoch: 174 | Batch_idx: 270 |  Loss: (0.1281) |  Loss2: (0.0000) | Acc: (95.00%) (33129/34688)
Epoch: 174 | Batch_idx: 280 |  Loss: (0.1282) |  Loss2: (0.0000) | Acc: (95.00%) (34350/35968)
Epoch: 174 | Batch_idx: 290 |  Loss: (0.1281) |  Loss2: (0.0000) | Acc: (95.00%) (35575/37248)
Epoch: 174 | Batch_idx: 300 |  Loss: (0.1276) |  Loss2: (0.0000) | Acc: (95.00%) (36795/38528)
Epoch: 174 | Batch_idx: 310 |  Loss: (0.1266) |  Loss2: (0.0000) | Acc: (95.00%) (38030/39808)
Epoch: 174 | Batch_idx: 320 |  Loss: (0.1261) |  Loss2: (0.0000) | Acc: (95.00%) (39253/41088)
Epoch: 174 | Batch_idx: 330 |  Loss: (0.1253) |  Loss2: (0.0000) | Acc: (95.00%) (40492/42368)
Epoch: 174 | Batch_idx: 340 |  Loss: (0.1249) |  Loss2: (0.0000) | Acc: (95.00%) (41722/43648)
Epoch: 174 | Batch_idx: 350 |  Loss: (0.1245) |  Loss2: (0.0000) | Acc: (95.00%) (42954/44928)
Epoch: 174 | Batch_idx: 360 |  Loss: (0.1253) |  Loss2: (0.0000) | Acc: (95.00%) (44166/46208)
Epoch: 174 | Batch_idx: 370 |  Loss: (0.1248) |  Loss2: (0.0000) | Acc: (95.00%) (45392/47488)
Epoch: 174 | Batch_idx: 380 |  Loss: (0.1246) |  Loss2: (0.0000) | Acc: (95.00%) (46627/48768)
Epoch: 174 | Batch_idx: 390 |  Loss: (0.1244) |  Loss2: (0.0000) | Acc: (95.00%) (47800/50000)
# TEST : Loss: (0.4117) | Acc: (88.00%) (8817/10000)
percent tensor([0.5444, 0.5584, 0.5471, 0.5155, 0.5581, 0.5682, 0.5671, 0.5285, 0.5437,
        0.5528, 0.5602, 0.5580, 0.5434, 0.5203, 0.5628, 0.5384],
       device='cuda:0') torch.Size([16])
percent tensor([0.5315, 0.5329, 0.5375, 0.5406, 0.5393, 0.5306, 0.5350, 0.5421, 0.5349,
        0.5339, 0.5314, 0.5352, 0.5317, 0.5355, 0.5318, 0.5339],
       device='cuda:0') torch.Size([16])
percent tensor([0.7000, 0.7081, 0.5256, 0.4584, 0.4697, 0.6397, 0.6499, 0.4880, 0.5748,
        0.6766, 0.6955, 0.5921, 0.7168, 0.6476, 0.6862, 0.6672],
       device='cuda:0') torch.Size([16])
percent tensor([0.7036, 0.6022, 0.6772, 0.6708, 0.6847, 0.7732, 0.6555, 0.6810, 0.6516,
        0.6398, 0.6482, 0.6201, 0.5933, 0.6793, 0.6722, 0.7521],
       device='cuda:0') torch.Size([16])
percent tensor([0.6293, 0.6269, 0.6237, 0.5770, 0.6062, 0.5819, 0.6450, 0.6220, 0.6403,
        0.6219, 0.6059, 0.6039, 0.6253, 0.6277, 0.6207, 0.5881],
       device='cuda:0') torch.Size([16])
percent tensor([0.5836, 0.7284, 0.7080, 0.7447, 0.7345, 0.7505, 0.7427, 0.6104, 0.7814,
        0.6421, 0.7670, 0.8108, 0.7399, 0.8065, 0.6347, 0.5514],
       device='cuda:0') torch.Size([16])
percent tensor([0.5836, 0.6299, 0.5574, 0.4539, 0.5880, 0.6767, 0.6115, 0.3973, 0.6866,
        0.6305, 0.6681, 0.4262, 0.6783, 0.5877, 0.3934, 0.4818],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9997, 0.9999, 0.9999, 0.9998, 0.9997, 0.9998, 0.9996, 0.9998,
        0.9994, 0.9999, 0.9998, 0.9999, 0.9996, 0.9996, 0.9995],
       device='cuda:0') torch.Size([16])
Epoch: 175 | Batch_idx: 0 |  Loss: (0.1177) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 175 | Batch_idx: 10 |  Loss: (0.1042) |  Loss2: (0.0000) | Acc: (96.00%) (1354/1408)
Epoch: 175 | Batch_idx: 20 |  Loss: (0.1176) |  Loss2: (0.0000) | Acc: (95.00%) (2567/2688)
Epoch: 175 | Batch_idx: 30 |  Loss: (0.1178) |  Loss2: (0.0000) | Acc: (95.00%) (3801/3968)
Epoch: 175 | Batch_idx: 40 |  Loss: (0.1150) |  Loss2: (0.0000) | Acc: (95.00%) (5030/5248)
Epoch: 175 | Batch_idx: 50 |  Loss: (0.1162) |  Loss2: (0.0000) | Acc: (95.00%) (6247/6528)
Epoch: 175 | Batch_idx: 60 |  Loss: (0.1128) |  Loss2: (0.0000) | Acc: (95.00%) (7489/7808)
Epoch: 175 | Batch_idx: 70 |  Loss: (0.1116) |  Loss2: (0.0000) | Acc: (96.00%) (8726/9088)
Epoch: 175 | Batch_idx: 80 |  Loss: (0.1109) |  Loss2: (0.0000) | Acc: (96.00%) (9960/10368)
Epoch: 175 | Batch_idx: 90 |  Loss: (0.1128) |  Loss2: (0.0000) | Acc: (95.00%) (11180/11648)
Epoch: 175 | Batch_idx: 100 |  Loss: (0.1136) |  Loss2: (0.0000) | Acc: (96.00%) (12412/12928)
Epoch: 175 | Batch_idx: 110 |  Loss: (0.1120) |  Loss2: (0.0000) | Acc: (96.00%) (13653/14208)
Epoch: 175 | Batch_idx: 120 |  Loss: (0.1109) |  Loss2: (0.0000) | Acc: (96.00%) (14891/15488)
Epoch: 175 | Batch_idx: 130 |  Loss: (0.1110) |  Loss2: (0.0000) | Acc: (96.00%) (16125/16768)
Epoch: 175 | Batch_idx: 140 |  Loss: (0.1116) |  Loss2: (0.0000) | Acc: (96.00%) (17355/18048)
Epoch: 175 | Batch_idx: 150 |  Loss: (0.1110) |  Loss2: (0.0000) | Acc: (96.00%) (18585/19328)
Epoch: 175 | Batch_idx: 160 |  Loss: (0.1109) |  Loss2: (0.0000) | Acc: (96.00%) (19815/20608)
Epoch: 175 | Batch_idx: 170 |  Loss: (0.1107) |  Loss2: (0.0000) | Acc: (96.00%) (21050/21888)
Epoch: 175 | Batch_idx: 180 |  Loss: (0.1110) |  Loss2: (0.0000) | Acc: (96.00%) (22280/23168)
Epoch: 175 | Batch_idx: 190 |  Loss: (0.1116) |  Loss2: (0.0000) | Acc: (96.00%) (23500/24448)
Epoch: 175 | Batch_idx: 200 |  Loss: (0.1107) |  Loss2: (0.0000) | Acc: (96.00%) (24734/25728)
Epoch: 175 | Batch_idx: 210 |  Loss: (0.1110) |  Loss2: (0.0000) | Acc: (96.00%) (25963/27008)
Epoch: 175 | Batch_idx: 220 |  Loss: (0.1105) |  Loss2: (0.0000) | Acc: (96.00%) (27200/28288)
Epoch: 175 | Batch_idx: 230 |  Loss: (0.1101) |  Loss2: (0.0000) | Acc: (96.00%) (28434/29568)
Epoch: 175 | Batch_idx: 240 |  Loss: (0.1103) |  Loss2: (0.0000) | Acc: (96.00%) (29656/30848)
Epoch: 175 | Batch_idx: 250 |  Loss: (0.1101) |  Loss2: (0.0000) | Acc: (96.00%) (30887/32128)
Epoch: 175 | Batch_idx: 260 |  Loss: (0.1096) |  Loss2: (0.0000) | Acc: (96.00%) (32123/33408)
Epoch: 175 | Batch_idx: 270 |  Loss: (0.1105) |  Loss2: (0.0000) | Acc: (96.00%) (33348/34688)
Epoch: 175 | Batch_idx: 280 |  Loss: (0.1101) |  Loss2: (0.0000) | Acc: (96.00%) (34585/35968)
Epoch: 175 | Batch_idx: 290 |  Loss: (0.1097) |  Loss2: (0.0000) | Acc: (96.00%) (35825/37248)
Epoch: 175 | Batch_idx: 300 |  Loss: (0.1089) |  Loss2: (0.0000) | Acc: (96.00%) (37075/38528)
Epoch: 175 | Batch_idx: 310 |  Loss: (0.1082) |  Loss2: (0.0000) | Acc: (96.00%) (38319/39808)
Epoch: 175 | Batch_idx: 320 |  Loss: (0.1081) |  Loss2: (0.0000) | Acc: (96.00%) (39556/41088)
Epoch: 175 | Batch_idx: 330 |  Loss: (0.1082) |  Loss2: (0.0000) | Acc: (96.00%) (40777/42368)
Epoch: 175 | Batch_idx: 340 |  Loss: (0.1076) |  Loss2: (0.0000) | Acc: (96.00%) (42025/43648)
Epoch: 175 | Batch_idx: 350 |  Loss: (0.1073) |  Loss2: (0.0000) | Acc: (96.00%) (43263/44928)
Epoch: 175 | Batch_idx: 360 |  Loss: (0.1070) |  Loss2: (0.0000) | Acc: (96.00%) (44498/46208)
Epoch: 175 | Batch_idx: 370 |  Loss: (0.1070) |  Loss2: (0.0000) | Acc: (96.00%) (45732/47488)
Epoch: 175 | Batch_idx: 380 |  Loss: (0.1064) |  Loss2: (0.0000) | Acc: (96.00%) (46975/48768)
Epoch: 175 | Batch_idx: 390 |  Loss: (0.1061) |  Loss2: (0.0000) | Acc: (96.00%) (48167/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_175.pth.tar'
# TEST : Loss: (0.4008) | Acc: (88.00%) (8828/10000)
percent tensor([0.5446, 0.5584, 0.5474, 0.5169, 0.5580, 0.5677, 0.5665, 0.5300, 0.5441,
        0.5530, 0.5602, 0.5579, 0.5438, 0.5212, 0.5626, 0.5389],
       device='cuda:0') torch.Size([16])
percent tensor([0.5326, 0.5340, 0.5387, 0.5420, 0.5405, 0.5318, 0.5361, 0.5433, 0.5361,
        0.5350, 0.5325, 0.5364, 0.5328, 0.5366, 0.5329, 0.5351],
       device='cuda:0') torch.Size([16])
percent tensor([0.7052, 0.7168, 0.5306, 0.4693, 0.4727, 0.6430, 0.6581, 0.4975, 0.5801,
        0.6845, 0.7028, 0.5980, 0.7222, 0.6585, 0.6955, 0.6735],
       device='cuda:0') torch.Size([16])
percent tensor([0.7083, 0.6030, 0.6820, 0.6749, 0.6876, 0.7715, 0.6593, 0.6869, 0.6557,
        0.6405, 0.6514, 0.6220, 0.5953, 0.6802, 0.6777, 0.7531],
       device='cuda:0') torch.Size([16])
percent tensor([0.6402, 0.6318, 0.6328, 0.5840, 0.6146, 0.5886, 0.6551, 0.6349, 0.6493,
        0.6286, 0.6138, 0.6066, 0.6292, 0.6379, 0.6248, 0.5963],
       device='cuda:0') torch.Size([16])
percent tensor([0.5897, 0.7394, 0.7157, 0.7516, 0.7422, 0.7606, 0.7483, 0.6213, 0.7914,
        0.6530, 0.7763, 0.8177, 0.7474, 0.8148, 0.6453, 0.5644],
       device='cuda:0') torch.Size([16])
percent tensor([0.5928, 0.6375, 0.5627, 0.4579, 0.5876, 0.6879, 0.6089, 0.3943, 0.6940,
        0.6400, 0.6777, 0.4323, 0.6905, 0.6020, 0.3886, 0.4900],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9997, 0.9999, 0.9999, 0.9999, 0.9997, 0.9998, 0.9996, 0.9999,
        0.9995, 0.9999, 0.9999, 0.9999, 0.9997, 0.9996, 0.9995],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 176 | Batch_idx: 0 |  Loss: (0.0825) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 176 | Batch_idx: 10 |  Loss: (0.0982) |  Loss2: (0.0000) | Acc: (96.00%) (1358/1408)
Epoch: 176 | Batch_idx: 20 |  Loss: (0.0903) |  Loss2: (0.0000) | Acc: (96.00%) (2596/2688)
Epoch: 176 | Batch_idx: 30 |  Loss: (0.0961) |  Loss2: (0.0000) | Acc: (96.00%) (3833/3968)
Epoch: 176 | Batch_idx: 40 |  Loss: (0.0946) |  Loss2: (0.0000) | Acc: (96.00%) (5075/5248)
Epoch: 176 | Batch_idx: 50 |  Loss: (0.0916) |  Loss2: (0.0000) | Acc: (96.00%) (6319/6528)
Epoch: 176 | Batch_idx: 60 |  Loss: (0.0917) |  Loss2: (0.0000) | Acc: (96.00%) (7555/7808)
Epoch: 176 | Batch_idx: 70 |  Loss: (0.0907) |  Loss2: (0.0000) | Acc: (96.00%) (8799/9088)
Epoch: 176 | Batch_idx: 80 |  Loss: (0.0911) |  Loss2: (0.0000) | Acc: (96.00%) (10031/10368)
Epoch: 176 | Batch_idx: 90 |  Loss: (0.0910) |  Loss2: (0.0000) | Acc: (96.00%) (11273/11648)
Epoch: 176 | Batch_idx: 100 |  Loss: (0.0913) |  Loss2: (0.0000) | Acc: (96.00%) (12514/12928)
Epoch: 176 | Batch_idx: 110 |  Loss: (0.0908) |  Loss2: (0.0000) | Acc: (96.00%) (13755/14208)
Epoch: 176 | Batch_idx: 120 |  Loss: (0.0908) |  Loss2: (0.0000) | Acc: (96.00%) (14993/15488)
Epoch: 176 | Batch_idx: 130 |  Loss: (0.0918) |  Loss2: (0.0000) | Acc: (96.00%) (16225/16768)
Epoch: 176 | Batch_idx: 140 |  Loss: (0.0922) |  Loss2: (0.0000) | Acc: (96.00%) (17464/18048)
Epoch: 176 | Batch_idx: 150 |  Loss: (0.0928) |  Loss2: (0.0000) | Acc: (96.00%) (18698/19328)
Epoch: 176 | Batch_idx: 160 |  Loss: (0.0921) |  Loss2: (0.0000) | Acc: (96.00%) (19945/20608)
Epoch: 176 | Batch_idx: 170 |  Loss: (0.0918) |  Loss2: (0.0000) | Acc: (96.00%) (21190/21888)
Epoch: 176 | Batch_idx: 180 |  Loss: (0.0914) |  Loss2: (0.0000) | Acc: (96.00%) (22432/23168)
Epoch: 176 | Batch_idx: 190 |  Loss: (0.0914) |  Loss2: (0.0000) | Acc: (96.00%) (23672/24448)
Epoch: 176 | Batch_idx: 200 |  Loss: (0.0911) |  Loss2: (0.0000) | Acc: (96.00%) (24912/25728)
Epoch: 176 | Batch_idx: 210 |  Loss: (0.0908) |  Loss2: (0.0000) | Acc: (96.00%) (26154/27008)
Epoch: 176 | Batch_idx: 220 |  Loss: (0.0906) |  Loss2: (0.0000) | Acc: (96.00%) (27390/28288)
Epoch: 176 | Batch_idx: 230 |  Loss: (0.0905) |  Loss2: (0.0000) | Acc: (96.00%) (28624/29568)
Epoch: 176 | Batch_idx: 240 |  Loss: (0.0911) |  Loss2: (0.0000) | Acc: (96.00%) (29857/30848)
Epoch: 176 | Batch_idx: 250 |  Loss: (0.0913) |  Loss2: (0.0000) | Acc: (96.00%) (31097/32128)
Epoch: 176 | Batch_idx: 260 |  Loss: (0.0918) |  Loss2: (0.0000) | Acc: (96.00%) (32328/33408)
Epoch: 176 | Batch_idx: 270 |  Loss: (0.0924) |  Loss2: (0.0000) | Acc: (96.00%) (33559/34688)
Epoch: 176 | Batch_idx: 280 |  Loss: (0.0928) |  Loss2: (0.0000) | Acc: (96.00%) (34793/35968)
Epoch: 176 | Batch_idx: 290 |  Loss: (0.0928) |  Loss2: (0.0000) | Acc: (96.00%) (36029/37248)
Epoch: 176 | Batch_idx: 300 |  Loss: (0.0929) |  Loss2: (0.0000) | Acc: (96.00%) (37274/38528)
Epoch: 176 | Batch_idx: 310 |  Loss: (0.0934) |  Loss2: (0.0000) | Acc: (96.00%) (38517/39808)
Epoch: 176 | Batch_idx: 320 |  Loss: (0.0936) |  Loss2: (0.0000) | Acc: (96.00%) (39750/41088)
Epoch: 176 | Batch_idx: 330 |  Loss: (0.0945) |  Loss2: (0.0000) | Acc: (96.00%) (40975/42368)
Epoch: 176 | Batch_idx: 340 |  Loss: (0.0953) |  Loss2: (0.0000) | Acc: (96.00%) (42195/43648)
Epoch: 176 | Batch_idx: 350 |  Loss: (0.0955) |  Loss2: (0.0000) | Acc: (96.00%) (43426/44928)
Epoch: 176 | Batch_idx: 360 |  Loss: (0.0954) |  Loss2: (0.0000) | Acc: (96.00%) (44664/46208)
Epoch: 176 | Batch_idx: 370 |  Loss: (0.0962) |  Loss2: (0.0000) | Acc: (96.00%) (45888/47488)
Epoch: 176 | Batch_idx: 380 |  Loss: (0.0963) |  Loss2: (0.0000) | Acc: (96.00%) (47124/48768)
Epoch: 176 | Batch_idx: 390 |  Loss: (0.0966) |  Loss2: (0.0000) | Acc: (96.00%) (48313/50000)
# TEST : Loss: (0.4035) | Acc: (88.00%) (8855/10000)
percent tensor([0.5455, 0.5592, 0.5481, 0.5195, 0.5560, 0.5689, 0.5647, 0.5346, 0.5441,
        0.5522, 0.5598, 0.5549, 0.5434, 0.5198, 0.5643, 0.5407],
       device='cuda:0') torch.Size([16])
percent tensor([0.5324, 0.5349, 0.5384, 0.5409, 0.5405, 0.5319, 0.5367, 0.5419, 0.5351,
        0.5354, 0.5324, 0.5368, 0.5325, 0.5369, 0.5336, 0.5347],
       device='cuda:0') torch.Size([16])
percent tensor([0.6963, 0.7135, 0.5134, 0.4716, 0.4506, 0.6332, 0.6557, 0.5123, 0.5915,
        0.6748, 0.7025, 0.5686, 0.7155, 0.6662, 0.6850, 0.6763],
       device='cuda:0') torch.Size([16])
percent tensor([0.7086, 0.5975, 0.6947, 0.6780, 0.6921, 0.7723, 0.6569, 0.6924, 0.6508,
        0.6350, 0.6420, 0.6299, 0.5883, 0.6713, 0.6813, 0.7472],
       device='cuda:0') torch.Size([16])
percent tensor([0.6351, 0.6323, 0.6373, 0.5959, 0.6147, 0.5887, 0.6522, 0.6422, 0.6554,
        0.6276, 0.6167, 0.5928, 0.6288, 0.6420, 0.6255, 0.6010],
       device='cuda:0') torch.Size([16])
percent tensor([0.5892, 0.7429, 0.7205, 0.7470, 0.7338, 0.7542, 0.7566, 0.6126, 0.7675,
        0.6624, 0.7818, 0.8142, 0.7297, 0.8243, 0.6652, 0.5856],
       device='cuda:0') torch.Size([16])
percent tensor([0.5885, 0.6447, 0.5531, 0.4654, 0.5967, 0.6783, 0.5869, 0.4016, 0.6565,
        0.6490, 0.6669, 0.4684, 0.6733, 0.6142, 0.4003, 0.4994],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9998, 0.9999, 0.9999, 0.9999, 0.9995, 0.9999, 0.9996, 0.9998,
        0.9998, 0.9999, 0.9999, 0.9999, 0.9996, 0.9997, 0.9994],
       device='cuda:0') torch.Size([16])
Epoch: 177 | Batch_idx: 0 |  Loss: (0.1091) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 177 | Batch_idx: 10 |  Loss: (0.1058) |  Loss2: (0.0000) | Acc: (96.00%) (1354/1408)
Epoch: 177 | Batch_idx: 20 |  Loss: (0.1062) |  Loss2: (0.0000) | Acc: (96.00%) (2583/2688)
Epoch: 177 | Batch_idx: 30 |  Loss: (0.1059) |  Loss2: (0.0000) | Acc: (96.00%) (3816/3968)
Epoch: 177 | Batch_idx: 40 |  Loss: (0.1040) |  Loss2: (0.0000) | Acc: (96.00%) (5056/5248)
Epoch: 177 | Batch_idx: 50 |  Loss: (0.1015) |  Loss2: (0.0000) | Acc: (96.00%) (6294/6528)
Epoch: 177 | Batch_idx: 60 |  Loss: (0.1020) |  Loss2: (0.0000) | Acc: (96.00%) (7524/7808)
Epoch: 177 | Batch_idx: 70 |  Loss: (0.1009) |  Loss2: (0.0000) | Acc: (96.00%) (8765/9088)
Epoch: 177 | Batch_idx: 80 |  Loss: (0.0996) |  Loss2: (0.0000) | Acc: (96.00%) (10005/10368)
Epoch: 177 | Batch_idx: 90 |  Loss: (0.0988) |  Loss2: (0.0000) | Acc: (96.00%) (11240/11648)
Epoch: 177 | Batch_idx: 100 |  Loss: (0.0974) |  Loss2: (0.0000) | Acc: (96.00%) (12482/12928)
Epoch: 177 | Batch_idx: 110 |  Loss: (0.0958) |  Loss2: (0.0000) | Acc: (96.00%) (13727/14208)
Epoch: 177 | Batch_idx: 120 |  Loss: (0.0943) |  Loss2: (0.0000) | Acc: (96.00%) (14975/15488)
Epoch: 177 | Batch_idx: 130 |  Loss: (0.0927) |  Loss2: (0.0000) | Acc: (96.00%) (16226/16768)
Epoch: 177 | Batch_idx: 140 |  Loss: (0.0923) |  Loss2: (0.0000) | Acc: (96.00%) (17468/18048)
Epoch: 177 | Batch_idx: 150 |  Loss: (0.0931) |  Loss2: (0.0000) | Acc: (96.00%) (18705/19328)
Epoch: 177 | Batch_idx: 160 |  Loss: (0.0931) |  Loss2: (0.0000) | Acc: (96.00%) (19950/20608)
Epoch: 177 | Batch_idx: 170 |  Loss: (0.0922) |  Loss2: (0.0000) | Acc: (96.00%) (21204/21888)
Epoch: 177 | Batch_idx: 180 |  Loss: (0.0919) |  Loss2: (0.0000) | Acc: (96.00%) (22447/23168)
Epoch: 177 | Batch_idx: 190 |  Loss: (0.0920) |  Loss2: (0.0000) | Acc: (96.00%) (23682/24448)
Epoch: 177 | Batch_idx: 200 |  Loss: (0.0928) |  Loss2: (0.0000) | Acc: (96.00%) (24911/25728)
Epoch: 177 | Batch_idx: 210 |  Loss: (0.0924) |  Loss2: (0.0000) | Acc: (96.00%) (26155/27008)
Epoch: 177 | Batch_idx: 220 |  Loss: (0.0925) |  Loss2: (0.0000) | Acc: (96.00%) (27396/28288)
Epoch: 177 | Batch_idx: 230 |  Loss: (0.0932) |  Loss2: (0.0000) | Acc: (96.00%) (28618/29568)
Epoch: 177 | Batch_idx: 240 |  Loss: (0.0934) |  Loss2: (0.0000) | Acc: (96.00%) (29853/30848)
Epoch: 177 | Batch_idx: 250 |  Loss: (0.0940) |  Loss2: (0.0000) | Acc: (96.00%) (31090/32128)
Epoch: 177 | Batch_idx: 260 |  Loss: (0.0939) |  Loss2: (0.0000) | Acc: (96.00%) (32327/33408)
Epoch: 177 | Batch_idx: 270 |  Loss: (0.0939) |  Loss2: (0.0000) | Acc: (96.00%) (33559/34688)
Epoch: 177 | Batch_idx: 280 |  Loss: (0.0939) |  Loss2: (0.0000) | Acc: (96.00%) (34802/35968)
Epoch: 177 | Batch_idx: 290 |  Loss: (0.0943) |  Loss2: (0.0000) | Acc: (96.00%) (36035/37248)
Epoch: 177 | Batch_idx: 300 |  Loss: (0.0940) |  Loss2: (0.0000) | Acc: (96.00%) (37278/38528)
Epoch: 177 | Batch_idx: 310 |  Loss: (0.0942) |  Loss2: (0.0000) | Acc: (96.00%) (38510/39808)
Epoch: 177 | Batch_idx: 320 |  Loss: (0.0942) |  Loss2: (0.0000) | Acc: (96.00%) (39743/41088)
Epoch: 177 | Batch_idx: 330 |  Loss: (0.0943) |  Loss2: (0.0000) | Acc: (96.00%) (40975/42368)
Epoch: 177 | Batch_idx: 340 |  Loss: (0.0940) |  Loss2: (0.0000) | Acc: (96.00%) (42218/43648)
Epoch: 177 | Batch_idx: 350 |  Loss: (0.0941) |  Loss2: (0.0000) | Acc: (96.00%) (43457/44928)
Epoch: 177 | Batch_idx: 360 |  Loss: (0.0938) |  Loss2: (0.0000) | Acc: (96.00%) (44703/46208)
Epoch: 177 | Batch_idx: 370 |  Loss: (0.0936) |  Loss2: (0.0000) | Acc: (96.00%) (45949/47488)
Epoch: 177 | Batch_idx: 380 |  Loss: (0.0936) |  Loss2: (0.0000) | Acc: (96.00%) (47190/48768)
Epoch: 177 | Batch_idx: 390 |  Loss: (0.0937) |  Loss2: (0.0000) | Acc: (96.00%) (48383/50000)
# TEST : Loss: (0.4150) | Acc: (88.00%) (8814/10000)
percent tensor([0.5446, 0.5591, 0.5462, 0.5172, 0.5564, 0.5692, 0.5657, 0.5322, 0.5451,
        0.5522, 0.5604, 0.5552, 0.5436, 0.5208, 0.5638, 0.5393],
       device='cuda:0') torch.Size([16])
percent tensor([0.5328, 0.5346, 0.5385, 0.5406, 0.5405, 0.5316, 0.5364, 0.5422, 0.5361,
        0.5351, 0.5325, 0.5360, 0.5330, 0.5373, 0.5330, 0.5348],
       device='cuda:0') torch.Size([16])
percent tensor([0.7070, 0.7207, 0.5227, 0.4885, 0.4539, 0.6418, 0.6616, 0.5293, 0.6027,
        0.6906, 0.7078, 0.5874, 0.7271, 0.6710, 0.6996, 0.6839],
       device='cuda:0') torch.Size([16])
percent tensor([0.7146, 0.6083, 0.6952, 0.6781, 0.6923, 0.7736, 0.6621, 0.6984, 0.6574,
        0.6414, 0.6524, 0.6293, 0.5958, 0.6788, 0.6894, 0.7516],
       device='cuda:0') torch.Size([16])
percent tensor([0.6397, 0.6497, 0.6399, 0.5940, 0.6224, 0.5979, 0.6598, 0.6305, 0.6514,
        0.6285, 0.6234, 0.6026, 0.6364, 0.6401, 0.6339, 0.6040],
       device='cuda:0') torch.Size([16])
percent tensor([0.5489, 0.7197, 0.7062, 0.7404, 0.7364, 0.7641, 0.7338, 0.5894, 0.7731,
        0.6397, 0.7563, 0.8183, 0.7209, 0.8000, 0.6384, 0.5595],
       device='cuda:0') torch.Size([16])
percent tensor([0.6001, 0.6798, 0.5511, 0.4498, 0.6258, 0.7113, 0.6216, 0.4169, 0.6695,
        0.6705, 0.6705, 0.4643, 0.6830, 0.6087, 0.4348, 0.5100],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9997, 0.9998, 0.9999, 0.9999, 0.9996, 0.9999, 0.9996, 0.9998,
        0.9998, 0.9999, 0.9999, 0.9999, 0.9996, 0.9995, 0.9994],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 178 | Batch_idx: 0 |  Loss: (0.0861) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 178 | Batch_idx: 10 |  Loss: (0.1023) |  Loss2: (0.0000) | Acc: (96.00%) (1352/1408)
Epoch: 178 | Batch_idx: 20 |  Loss: (0.1134) |  Loss2: (0.0000) | Acc: (95.00%) (2574/2688)
Epoch: 178 | Batch_idx: 30 |  Loss: (0.1171) |  Loss2: (0.0000) | Acc: (95.00%) (3805/3968)
Epoch: 178 | Batch_idx: 40 |  Loss: (0.1204) |  Loss2: (0.0000) | Acc: (95.00%) (5030/5248)
Epoch: 178 | Batch_idx: 50 |  Loss: (0.1170) |  Loss2: (0.0000) | Acc: (95.00%) (6256/6528)
Epoch: 178 | Batch_idx: 60 |  Loss: (0.1188) |  Loss2: (0.0000) | Acc: (95.00%) (7476/7808)
Epoch: 178 | Batch_idx: 70 |  Loss: (0.1165) |  Loss2: (0.0000) | Acc: (95.00%) (8710/9088)
Epoch: 178 | Batch_idx: 80 |  Loss: (0.1178) |  Loss2: (0.0000) | Acc: (95.00%) (9926/10368)
Epoch: 178 | Batch_idx: 90 |  Loss: (0.1203) |  Loss2: (0.0000) | Acc: (95.00%) (11148/11648)
Epoch: 178 | Batch_idx: 100 |  Loss: (0.1202) |  Loss2: (0.0000) | Acc: (95.00%) (12377/12928)
Epoch: 178 | Batch_idx: 110 |  Loss: (0.1208) |  Loss2: (0.0000) | Acc: (95.00%) (13593/14208)
Epoch: 178 | Batch_idx: 120 |  Loss: (0.1201) |  Loss2: (0.0000) | Acc: (95.00%) (14813/15488)
Epoch: 178 | Batch_idx: 130 |  Loss: (0.1191) |  Loss2: (0.0000) | Acc: (95.00%) (16047/16768)
Epoch: 178 | Batch_idx: 140 |  Loss: (0.1181) |  Loss2: (0.0000) | Acc: (95.00%) (17273/18048)
Epoch: 178 | Batch_idx: 150 |  Loss: (0.1178) |  Loss2: (0.0000) | Acc: (95.00%) (18501/19328)
Epoch: 178 | Batch_idx: 160 |  Loss: (0.1181) |  Loss2: (0.0000) | Acc: (95.00%) (19727/20608)
Epoch: 178 | Batch_idx: 170 |  Loss: (0.1169) |  Loss2: (0.0000) | Acc: (95.00%) (20960/21888)
Epoch: 178 | Batch_idx: 180 |  Loss: (0.1169) |  Loss2: (0.0000) | Acc: (95.00%) (22191/23168)
Epoch: 178 | Batch_idx: 190 |  Loss: (0.1168) |  Loss2: (0.0000) | Acc: (95.00%) (23416/24448)
Epoch: 178 | Batch_idx: 200 |  Loss: (0.1167) |  Loss2: (0.0000) | Acc: (95.00%) (24643/25728)
Epoch: 178 | Batch_idx: 210 |  Loss: (0.1158) |  Loss2: (0.0000) | Acc: (95.00%) (25879/27008)
Epoch: 178 | Batch_idx: 220 |  Loss: (0.1157) |  Loss2: (0.0000) | Acc: (95.00%) (27101/28288)
Epoch: 178 | Batch_idx: 230 |  Loss: (0.1155) |  Loss2: (0.0000) | Acc: (95.00%) (28334/29568)
Epoch: 178 | Batch_idx: 240 |  Loss: (0.1151) |  Loss2: (0.0000) | Acc: (95.00%) (29567/30848)
Epoch: 178 | Batch_idx: 250 |  Loss: (0.1149) |  Loss2: (0.0000) | Acc: (95.00%) (30794/32128)
Epoch: 178 | Batch_idx: 260 |  Loss: (0.1146) |  Loss2: (0.0000) | Acc: (95.00%) (32027/33408)
Epoch: 178 | Batch_idx: 270 |  Loss: (0.1144) |  Loss2: (0.0000) | Acc: (95.00%) (33261/34688)
Epoch: 178 | Batch_idx: 280 |  Loss: (0.1136) |  Loss2: (0.0000) | Acc: (95.00%) (34501/35968)
Epoch: 178 | Batch_idx: 290 |  Loss: (0.1135) |  Loss2: (0.0000) | Acc: (95.00%) (35730/37248)
Epoch: 178 | Batch_idx: 300 |  Loss: (0.1133) |  Loss2: (0.0000) | Acc: (95.00%) (36962/38528)
Epoch: 178 | Batch_idx: 310 |  Loss: (0.1127) |  Loss2: (0.0000) | Acc: (95.00%) (38200/39808)
Epoch: 178 | Batch_idx: 320 |  Loss: (0.1131) |  Loss2: (0.0000) | Acc: (95.00%) (39416/41088)
Epoch: 178 | Batch_idx: 330 |  Loss: (0.1123) |  Loss2: (0.0000) | Acc: (95.00%) (40658/42368)
Epoch: 178 | Batch_idx: 340 |  Loss: (0.1119) |  Loss2: (0.0000) | Acc: (95.00%) (41893/43648)
Epoch: 178 | Batch_idx: 350 |  Loss: (0.1119) |  Loss2: (0.0000) | Acc: (95.00%) (43119/44928)
Epoch: 178 | Batch_idx: 360 |  Loss: (0.1115) |  Loss2: (0.0000) | Acc: (95.00%) (44352/46208)
Epoch: 178 | Batch_idx: 370 |  Loss: (0.1115) |  Loss2: (0.0000) | Acc: (95.00%) (45581/47488)
Epoch: 178 | Batch_idx: 380 |  Loss: (0.1112) |  Loss2: (0.0000) | Acc: (95.00%) (46813/48768)
Epoch: 178 | Batch_idx: 390 |  Loss: (0.1104) |  Loss2: (0.0000) | Acc: (96.00%) (48015/50000)
# TEST : Loss: (0.4030) | Acc: (88.00%) (8864/10000)
percent tensor([0.5387, 0.5516, 0.5427, 0.5139, 0.5507, 0.5638, 0.5584, 0.5274, 0.5389,
        0.5448, 0.5534, 0.5488, 0.5364, 0.5143, 0.5573, 0.5335],
       device='cuda:0') torch.Size([16])
percent tensor([0.5387, 0.5410, 0.5452, 0.5472, 0.5476, 0.5369, 0.5433, 0.5494, 0.5424,
        0.5416, 0.5385, 0.5427, 0.5393, 0.5438, 0.5389, 0.5407],
       device='cuda:0') torch.Size([16])
percent tensor([0.7157, 0.7239, 0.5275, 0.4880, 0.4660, 0.6542, 0.6716, 0.5431, 0.6165,
        0.6941, 0.7170, 0.5895, 0.7301, 0.6790, 0.7069, 0.6911],
       device='cuda:0') torch.Size([16])
percent tensor([0.7285, 0.6282, 0.6961, 0.6845, 0.7007, 0.7808, 0.6763, 0.7159, 0.6723,
        0.6616, 0.6705, 0.6373, 0.6122, 0.6954, 0.7070, 0.7671],
       device='cuda:0') torch.Size([16])
percent tensor([0.6218, 0.6379, 0.6280, 0.5817, 0.6087, 0.5888, 0.6432, 0.6125, 0.6355,
        0.6145, 0.6132, 0.5998, 0.6250, 0.6302, 0.6150, 0.5928],
       device='cuda:0') torch.Size([16])
percent tensor([0.5580, 0.7251, 0.6898, 0.7463, 0.7342, 0.7717, 0.7395, 0.5983, 0.7799,
        0.6480, 0.7607, 0.8176, 0.7287, 0.8103, 0.6399, 0.5952],
       device='cuda:0') torch.Size([16])
percent tensor([0.5844, 0.6709, 0.5337, 0.4396, 0.6023, 0.6947, 0.6179, 0.4108, 0.6703,
        0.6505, 0.6780, 0.4441, 0.6699, 0.6106, 0.4180, 0.4959],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9997, 0.9999, 0.9999, 0.9999, 0.9995, 0.9999, 0.9996, 0.9999,
        0.9998, 0.9999, 0.9999, 0.9999, 0.9996, 0.9995, 0.9995],
       device='cuda:0') torch.Size([16])
Epoch: 179 | Batch_idx: 0 |  Loss: (0.1271) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 179 | Batch_idx: 10 |  Loss: (0.1233) |  Loss2: (0.0000) | Acc: (95.00%) (1347/1408)
Epoch: 179 | Batch_idx: 20 |  Loss: (0.1153) |  Loss2: (0.0000) | Acc: (96.00%) (2584/2688)
Epoch: 179 | Batch_idx: 30 |  Loss: (0.1165) |  Loss2: (0.0000) | Acc: (95.00%) (3809/3968)
Epoch: 179 | Batch_idx: 40 |  Loss: (0.1127) |  Loss2: (0.0000) | Acc: (96.00%) (5046/5248)
Epoch: 179 | Batch_idx: 50 |  Loss: (0.1094) |  Loss2: (0.0000) | Acc: (96.00%) (6282/6528)
Epoch: 179 | Batch_idx: 60 |  Loss: (0.1095) |  Loss2: (0.0000) | Acc: (96.00%) (7509/7808)
Epoch: 179 | Batch_idx: 70 |  Loss: (0.1078) |  Loss2: (0.0000) | Acc: (96.00%) (8742/9088)
Epoch: 179 | Batch_idx: 80 |  Loss: (0.1086) |  Loss2: (0.0000) | Acc: (96.00%) (9962/10368)
Epoch: 179 | Batch_idx: 90 |  Loss: (0.1074) |  Loss2: (0.0000) | Acc: (96.00%) (11193/11648)
Epoch: 179 | Batch_idx: 100 |  Loss: (0.1072) |  Loss2: (0.0000) | Acc: (96.00%) (12432/12928)
Epoch: 179 | Batch_idx: 110 |  Loss: (0.1080) |  Loss2: (0.0000) | Acc: (96.00%) (13657/14208)
Epoch: 179 | Batch_idx: 120 |  Loss: (0.1058) |  Loss2: (0.0000) | Acc: (96.00%) (14899/15488)
Epoch: 179 | Batch_idx: 130 |  Loss: (0.1044) |  Loss2: (0.0000) | Acc: (96.00%) (16146/16768)
Epoch: 179 | Batch_idx: 140 |  Loss: (0.1048) |  Loss2: (0.0000) | Acc: (96.00%) (17380/18048)
Epoch: 179 | Batch_idx: 150 |  Loss: (0.1042) |  Loss2: (0.0000) | Acc: (96.00%) (18620/19328)
Epoch: 179 | Batch_idx: 160 |  Loss: (0.1048) |  Loss2: (0.0000) | Acc: (96.00%) (19851/20608)
Epoch: 179 | Batch_idx: 170 |  Loss: (0.1034) |  Loss2: (0.0000) | Acc: (96.00%) (21098/21888)
Epoch: 179 | Batch_idx: 180 |  Loss: (0.1027) |  Loss2: (0.0000) | Acc: (96.00%) (22336/23168)
Epoch: 179 | Batch_idx: 190 |  Loss: (0.1013) |  Loss2: (0.0000) | Acc: (96.00%) (23585/24448)
Epoch: 179 | Batch_idx: 200 |  Loss: (0.1002) |  Loss2: (0.0000) | Acc: (96.00%) (24829/25728)
Epoch: 179 | Batch_idx: 210 |  Loss: (0.1002) |  Loss2: (0.0000) | Acc: (96.00%) (26067/27008)
Epoch: 179 | Batch_idx: 220 |  Loss: (0.1002) |  Loss2: (0.0000) | Acc: (96.00%) (27295/28288)
Epoch: 179 | Batch_idx: 230 |  Loss: (0.0999) |  Loss2: (0.0000) | Acc: (96.00%) (28534/29568)
Epoch: 179 | Batch_idx: 240 |  Loss: (0.0993) |  Loss2: (0.0000) | Acc: (96.00%) (29775/30848)
Epoch: 179 | Batch_idx: 250 |  Loss: (0.0987) |  Loss2: (0.0000) | Acc: (96.00%) (31024/32128)
Epoch: 179 | Batch_idx: 260 |  Loss: (0.0985) |  Loss2: (0.0000) | Acc: (96.00%) (32267/33408)
Epoch: 179 | Batch_idx: 270 |  Loss: (0.0985) |  Loss2: (0.0000) | Acc: (96.00%) (33506/34688)
Epoch: 179 | Batch_idx: 280 |  Loss: (0.0984) |  Loss2: (0.0000) | Acc: (96.00%) (34746/35968)
Epoch: 179 | Batch_idx: 290 |  Loss: (0.0982) |  Loss2: (0.0000) | Acc: (96.00%) (35984/37248)
Epoch: 179 | Batch_idx: 300 |  Loss: (0.0977) |  Loss2: (0.0000) | Acc: (96.00%) (37232/38528)
Epoch: 179 | Batch_idx: 310 |  Loss: (0.0977) |  Loss2: (0.0000) | Acc: (96.00%) (38467/39808)
Epoch: 179 | Batch_idx: 320 |  Loss: (0.0973) |  Loss2: (0.0000) | Acc: (96.00%) (39711/41088)
Epoch: 179 | Batch_idx: 330 |  Loss: (0.0965) |  Loss2: (0.0000) | Acc: (96.00%) (40960/42368)
Epoch: 179 | Batch_idx: 340 |  Loss: (0.0966) |  Loss2: (0.0000) | Acc: (96.00%) (42199/43648)
Epoch: 179 | Batch_idx: 350 |  Loss: (0.0974) |  Loss2: (0.0000) | Acc: (96.00%) (43427/44928)
Epoch: 179 | Batch_idx: 360 |  Loss: (0.0972) |  Loss2: (0.0000) | Acc: (96.00%) (44666/46208)
Epoch: 179 | Batch_idx: 370 |  Loss: (0.0970) |  Loss2: (0.0000) | Acc: (96.00%) (45906/47488)
Epoch: 179 | Batch_idx: 380 |  Loss: (0.0971) |  Loss2: (0.0000) | Acc: (96.00%) (47143/48768)
Epoch: 179 | Batch_idx: 390 |  Loss: (0.0971) |  Loss2: (0.0000) | Acc: (96.00%) (48333/50000)
# TEST : Loss: (0.3919) | Acc: (89.00%) (8900/10000)
percent tensor([0.5385, 0.5504, 0.5422, 0.5130, 0.5504, 0.5638, 0.5576, 0.5264, 0.5387,
        0.5437, 0.5529, 0.5478, 0.5358, 0.5134, 0.5567, 0.5328],
       device='cuda:0') torch.Size([16])
percent tensor([0.5394, 0.5421, 0.5465, 0.5481, 0.5490, 0.5373, 0.5446, 0.5508, 0.5435,
        0.5427, 0.5392, 0.5439, 0.5402, 0.5450, 0.5398, 0.5413],
       device='cuda:0') torch.Size([16])
percent tensor([0.7097, 0.7189, 0.5158, 0.4760, 0.4592, 0.6468, 0.6659, 0.5341, 0.6059,
        0.6873, 0.7127, 0.5809, 0.7232, 0.6744, 0.6995, 0.6841],
       device='cuda:0') torch.Size([16])
percent tensor([0.7240, 0.6249, 0.6933, 0.6813, 0.6996, 0.7802, 0.6729, 0.7120, 0.6683,
        0.6588, 0.6652, 0.6336, 0.6074, 0.6905, 0.7032, 0.7641],
       device='cuda:0') torch.Size([16])
percent tensor([0.6202, 0.6375, 0.6286, 0.5854, 0.6073, 0.5921, 0.6405, 0.6116, 0.6321,
        0.6142, 0.6094, 0.6015, 0.6236, 0.6282, 0.6127, 0.5967],
       device='cuda:0') torch.Size([16])
percent tensor([0.5605, 0.7217, 0.6849, 0.7430, 0.7313, 0.7703, 0.7421, 0.5953, 0.7821,
        0.6443, 0.7635, 0.8190, 0.7301, 0.8107, 0.6401, 0.5891],
       device='cuda:0') torch.Size([16])
percent tensor([0.5963, 0.6763, 0.5423, 0.4504, 0.6089, 0.7029, 0.6325, 0.4153, 0.6841,
        0.6519, 0.6951, 0.4588, 0.6820, 0.6186, 0.4244, 0.5002],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9997, 0.9998, 0.9999, 0.9999, 0.9995, 0.9999, 0.9996, 0.9999,
        0.9998, 0.9999, 0.9999, 0.9999, 0.9995, 0.9995, 0.9994],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 180 | Batch_idx: 0 |  Loss: (0.0799) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 180 | Batch_idx: 10 |  Loss: (0.0919) |  Loss2: (0.0000) | Acc: (96.00%) (1365/1408)
Epoch: 180 | Batch_idx: 20 |  Loss: (0.0848) |  Loss2: (0.0000) | Acc: (97.00%) (2613/2688)
Epoch: 180 | Batch_idx: 30 |  Loss: (0.0809) |  Loss2: (0.0000) | Acc: (97.00%) (3862/3968)
Epoch: 180 | Batch_idx: 40 |  Loss: (0.0853) |  Loss2: (0.0000) | Acc: (97.00%) (5098/5248)
Epoch: 180 | Batch_idx: 50 |  Loss: (0.0881) |  Loss2: (0.0000) | Acc: (97.00%) (6339/6528)
Epoch: 180 | Batch_idx: 60 |  Loss: (0.0889) |  Loss2: (0.0000) | Acc: (97.00%) (7576/7808)
Epoch: 180 | Batch_idx: 70 |  Loss: (0.0885) |  Loss2: (0.0000) | Acc: (97.00%) (8819/9088)
Epoch: 180 | Batch_idx: 80 |  Loss: (0.0873) |  Loss2: (0.0000) | Acc: (97.00%) (10065/10368)
Epoch: 180 | Batch_idx: 90 |  Loss: (0.0870) |  Loss2: (0.0000) | Acc: (97.00%) (11305/11648)
Epoch: 180 | Batch_idx: 100 |  Loss: (0.0866) |  Loss2: (0.0000) | Acc: (97.00%) (12545/12928)
Epoch: 180 | Batch_idx: 110 |  Loss: (0.0853) |  Loss2: (0.0000) | Acc: (97.00%) (13797/14208)
Epoch: 180 | Batch_idx: 120 |  Loss: (0.0861) |  Loss2: (0.0000) | Acc: (97.00%) (15030/15488)
Epoch: 180 | Batch_idx: 130 |  Loss: (0.0857) |  Loss2: (0.0000) | Acc: (97.00%) (16270/16768)
Epoch: 180 | Batch_idx: 140 |  Loss: (0.0858) |  Loss2: (0.0000) | Acc: (97.00%) (17508/18048)
Epoch: 180 | Batch_idx: 150 |  Loss: (0.0867) |  Loss2: (0.0000) | Acc: (96.00%) (18747/19328)
Epoch: 180 | Batch_idx: 160 |  Loss: (0.0871) |  Loss2: (0.0000) | Acc: (97.00%) (19991/20608)
Epoch: 180 | Batch_idx: 170 |  Loss: (0.0873) |  Loss2: (0.0000) | Acc: (96.00%) (21229/21888)
Epoch: 180 | Batch_idx: 180 |  Loss: (0.0876) |  Loss2: (0.0000) | Acc: (96.00%) (22472/23168)
Epoch: 180 | Batch_idx: 190 |  Loss: (0.0877) |  Loss2: (0.0000) | Acc: (97.00%) (23715/24448)
Epoch: 180 | Batch_idx: 200 |  Loss: (0.0876) |  Loss2: (0.0000) | Acc: (97.00%) (24961/25728)
Epoch: 180 | Batch_idx: 210 |  Loss: (0.0875) |  Loss2: (0.0000) | Acc: (96.00%) (26197/27008)
Epoch: 180 | Batch_idx: 220 |  Loss: (0.0876) |  Loss2: (0.0000) | Acc: (96.00%) (27433/28288)
Epoch: 180 | Batch_idx: 230 |  Loss: (0.0883) |  Loss2: (0.0000) | Acc: (96.00%) (28674/29568)
Epoch: 180 | Batch_idx: 240 |  Loss: (0.0885) |  Loss2: (0.0000) | Acc: (96.00%) (29913/30848)
Epoch: 180 | Batch_idx: 250 |  Loss: (0.0883) |  Loss2: (0.0000) | Acc: (96.00%) (31156/32128)
Epoch: 180 | Batch_idx: 260 |  Loss: (0.0884) |  Loss2: (0.0000) | Acc: (96.00%) (32402/33408)
Epoch: 180 | Batch_idx: 270 |  Loss: (0.0882) |  Loss2: (0.0000) | Acc: (96.00%) (33644/34688)
Epoch: 180 | Batch_idx: 280 |  Loss: (0.0886) |  Loss2: (0.0000) | Acc: (96.00%) (34880/35968)
Epoch: 180 | Batch_idx: 290 |  Loss: (0.0889) |  Loss2: (0.0000) | Acc: (96.00%) (36123/37248)
Epoch: 180 | Batch_idx: 300 |  Loss: (0.0888) |  Loss2: (0.0000) | Acc: (96.00%) (37364/38528)
Epoch: 180 | Batch_idx: 310 |  Loss: (0.0891) |  Loss2: (0.0000) | Acc: (96.00%) (38606/39808)
Epoch: 180 | Batch_idx: 320 |  Loss: (0.0894) |  Loss2: (0.0000) | Acc: (96.00%) (39844/41088)
Epoch: 180 | Batch_idx: 330 |  Loss: (0.0897) |  Loss2: (0.0000) | Acc: (96.00%) (41088/42368)
Epoch: 180 | Batch_idx: 340 |  Loss: (0.0899) |  Loss2: (0.0000) | Acc: (96.00%) (42324/43648)
Epoch: 180 | Batch_idx: 350 |  Loss: (0.0903) |  Loss2: (0.0000) | Acc: (96.00%) (43556/44928)
Epoch: 180 | Batch_idx: 360 |  Loss: (0.0903) |  Loss2: (0.0000) | Acc: (96.00%) (44793/46208)
Epoch: 180 | Batch_idx: 370 |  Loss: (0.0910) |  Loss2: (0.0000) | Acc: (96.00%) (46023/47488)
Epoch: 180 | Batch_idx: 380 |  Loss: (0.0914) |  Loss2: (0.0000) | Acc: (96.00%) (47254/48768)
Epoch: 180 | Batch_idx: 390 |  Loss: (0.0916) |  Loss2: (0.0000) | Acc: (96.00%) (48444/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_180.pth.tar'
# TEST : Loss: (0.4289) | Acc: (88.00%) (8849/10000)
percent tensor([0.5401, 0.5498, 0.5433, 0.5126, 0.5507, 0.5625, 0.5574, 0.5266, 0.5384,
        0.5443, 0.5535, 0.5491, 0.5369, 0.5141, 0.5560, 0.5327],
       device='cuda:0') torch.Size([16])
percent tensor([0.5388, 0.5415, 0.5464, 0.5488, 0.5481, 0.5396, 0.5436, 0.5508, 0.5436,
        0.5419, 0.5395, 0.5433, 0.5396, 0.5438, 0.5407, 0.5419],
       device='cuda:0') torch.Size([16])
percent tensor([0.7065, 0.7137, 0.5115, 0.4629, 0.4622, 0.6330, 0.6617, 0.5209, 0.5844,
        0.6863, 0.7032, 0.5889, 0.7210, 0.6704, 0.6831, 0.6781],
       device='cuda:0') torch.Size([16])
percent tensor([0.7214, 0.6345, 0.7081, 0.6851, 0.7067, 0.7797, 0.6754, 0.7087, 0.6686,
        0.6608, 0.6722, 0.6437, 0.6081, 0.6916, 0.6981, 0.7662],
       device='cuda:0') torch.Size([16])
percent tensor([0.6219, 0.6300, 0.6222, 0.5928, 0.6026, 0.5934, 0.6451, 0.6193, 0.6317,
        0.6161, 0.6028, 0.5984, 0.6196, 0.6293, 0.6214, 0.5987],
       device='cuda:0') torch.Size([16])
percent tensor([0.5868, 0.7429, 0.6982, 0.7571, 0.7376, 0.7542, 0.7631, 0.6252, 0.7967,
        0.6680, 0.7704, 0.8118, 0.7437, 0.8256, 0.6457, 0.5837],
       device='cuda:0') torch.Size([16])
percent tensor([0.6189, 0.6928, 0.5772, 0.4677, 0.6285, 0.6907, 0.6343, 0.4494, 0.6845,
        0.6660, 0.7135, 0.4664, 0.7049, 0.6388, 0.4329, 0.5246],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9998, 0.9998, 0.9998, 0.9999, 0.9995, 0.9999, 0.9997, 0.9999,
        0.9998, 1.0000, 0.9999, 0.9999, 0.9995, 0.9995, 0.9993],
       device='cuda:0') torch.Size([16])
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(184.1135, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(830.4788, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(826.5908, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1510.0192, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(479.4938, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2294.8403, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4237.5405, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1358.7065, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6259.8696, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11603.7979, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3819.2263, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16115.3301, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 181 | Batch_idx: 0 |  Loss: (0.1234) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 181 | Batch_idx: 10 |  Loss: (0.0901) |  Loss2: (0.0000) | Acc: (96.00%) (1358/1408)
Epoch: 181 | Batch_idx: 20 |  Loss: (0.0911) |  Loss2: (0.0000) | Acc: (96.00%) (2594/2688)
Epoch: 181 | Batch_idx: 30 |  Loss: (0.0857) |  Loss2: (0.0000) | Acc: (96.00%) (3846/3968)
Epoch: 181 | Batch_idx: 40 |  Loss: (0.0856) |  Loss2: (0.0000) | Acc: (97.00%) (5092/5248)
Epoch: 181 | Batch_idx: 50 |  Loss: (0.0866) |  Loss2: (0.0000) | Acc: (96.00%) (6331/6528)
Epoch: 181 | Batch_idx: 60 |  Loss: (0.0871) |  Loss2: (0.0000) | Acc: (96.00%) (7568/7808)
Epoch: 181 | Batch_idx: 70 |  Loss: (0.0868) |  Loss2: (0.0000) | Acc: (96.00%) (8810/9088)
Epoch: 181 | Batch_idx: 80 |  Loss: (0.0844) |  Loss2: (0.0000) | Acc: (97.00%) (10060/10368)
Epoch: 181 | Batch_idx: 90 |  Loss: (0.0849) |  Loss2: (0.0000) | Acc: (97.00%) (11302/11648)
Epoch: 181 | Batch_idx: 100 |  Loss: (0.0855) |  Loss2: (0.0000) | Acc: (97.00%) (12541/12928)
Epoch: 181 | Batch_idx: 110 |  Loss: (0.0848) |  Loss2: (0.0000) | Acc: (97.00%) (13789/14208)
Epoch: 181 | Batch_idx: 120 |  Loss: (0.0842) |  Loss2: (0.0000) | Acc: (97.00%) (15035/15488)
Epoch: 181 | Batch_idx: 130 |  Loss: (0.0840) |  Loss2: (0.0000) | Acc: (97.00%) (16279/16768)
Epoch: 181 | Batch_idx: 140 |  Loss: (0.0841) |  Loss2: (0.0000) | Acc: (97.00%) (17519/18048)
Epoch: 181 | Batch_idx: 150 |  Loss: (0.0845) |  Loss2: (0.0000) | Acc: (97.00%) (18762/19328)
Epoch: 181 | Batch_idx: 160 |  Loss: (0.0846) |  Loss2: (0.0000) | Acc: (97.00%) (20001/20608)
Epoch: 181 | Batch_idx: 170 |  Loss: (0.0843) |  Loss2: (0.0000) | Acc: (97.00%) (21248/21888)
Epoch: 181 | Batch_idx: 180 |  Loss: (0.0843) |  Loss2: (0.0000) | Acc: (97.00%) (22499/23168)
Epoch: 181 | Batch_idx: 190 |  Loss: (0.0843) |  Loss2: (0.0000) | Acc: (97.00%) (23739/24448)
Epoch: 181 | Batch_idx: 200 |  Loss: (0.0846) |  Loss2: (0.0000) | Acc: (97.00%) (24979/25728)
Epoch: 181 | Batch_idx: 210 |  Loss: (0.0852) |  Loss2: (0.0000) | Acc: (97.00%) (26218/27008)
Epoch: 181 | Batch_idx: 220 |  Loss: (0.0863) |  Loss2: (0.0000) | Acc: (97.00%) (27456/28288)
Epoch: 181 | Batch_idx: 230 |  Loss: (0.0864) |  Loss2: (0.0000) | Acc: (97.00%) (28696/29568)
Epoch: 181 | Batch_idx: 240 |  Loss: (0.0863) |  Loss2: (0.0000) | Acc: (97.00%) (29944/30848)
Epoch: 181 | Batch_idx: 250 |  Loss: (0.0864) |  Loss2: (0.0000) | Acc: (97.00%) (31181/32128)
Epoch: 181 | Batch_idx: 260 |  Loss: (0.0863) |  Loss2: (0.0000) | Acc: (97.00%) (32427/33408)
Epoch: 181 | Batch_idx: 270 |  Loss: (0.0867) |  Loss2: (0.0000) | Acc: (97.00%) (33661/34688)
Epoch: 181 | Batch_idx: 280 |  Loss: (0.0872) |  Loss2: (0.0000) | Acc: (97.00%) (34897/35968)
Epoch: 181 | Batch_idx: 290 |  Loss: (0.0873) |  Loss2: (0.0000) | Acc: (97.00%) (36136/37248)
Epoch: 181 | Batch_idx: 300 |  Loss: (0.0877) |  Loss2: (0.0000) | Acc: (96.00%) (37367/38528)
Epoch: 181 | Batch_idx: 310 |  Loss: (0.0885) |  Loss2: (0.0000) | Acc: (96.00%) (38597/39808)
Epoch: 181 | Batch_idx: 320 |  Loss: (0.0890) |  Loss2: (0.0000) | Acc: (96.00%) (39837/41088)
Epoch: 181 | Batch_idx: 330 |  Loss: (0.0893) |  Loss2: (0.0000) | Acc: (96.00%) (41077/42368)
Epoch: 181 | Batch_idx: 340 |  Loss: (0.0894) |  Loss2: (0.0000) | Acc: (96.00%) (42313/43648)
Epoch: 181 | Batch_idx: 350 |  Loss: (0.0898) |  Loss2: (0.0000) | Acc: (96.00%) (43546/44928)
Epoch: 181 | Batch_idx: 360 |  Loss: (0.0899) |  Loss2: (0.0000) | Acc: (96.00%) (44785/46208)
Epoch: 181 | Batch_idx: 370 |  Loss: (0.0898) |  Loss2: (0.0000) | Acc: (96.00%) (46028/47488)
Epoch: 181 | Batch_idx: 380 |  Loss: (0.0899) |  Loss2: (0.0000) | Acc: (96.00%) (47269/48768)
Epoch: 181 | Batch_idx: 390 |  Loss: (0.0900) |  Loss2: (0.0000) | Acc: (96.00%) (48460/50000)
# TEST : Loss: (0.4006) | Acc: (88.00%) (8879/10000)
percent tensor([0.5374, 0.5504, 0.5415, 0.5123, 0.5503, 0.5611, 0.5576, 0.5259, 0.5376,
        0.5438, 0.5524, 0.5488, 0.5355, 0.5145, 0.5558, 0.5317],
       device='cuda:0') torch.Size([16])
percent tensor([0.5397, 0.5417, 0.5457, 0.5475, 0.5483, 0.5384, 0.5444, 0.5501, 0.5436,
        0.5422, 0.5395, 0.5436, 0.5400, 0.5447, 0.5399, 0.5416],
       device='cuda:0') torch.Size([16])
percent tensor([0.6964, 0.7095, 0.5212, 0.4675, 0.4627, 0.6285, 0.6538, 0.5202, 0.5777,
        0.6781, 0.6898, 0.5936, 0.7135, 0.6572, 0.6863, 0.6645],
       device='cuda:0') torch.Size([16])
percent tensor([0.7175, 0.6197, 0.7064, 0.6908, 0.7104, 0.7823, 0.6747, 0.7065, 0.6683,
        0.6497, 0.6568, 0.6410, 0.6010, 0.6845, 0.7019, 0.7630],
       device='cuda:0') torch.Size([16])
percent tensor([0.6256, 0.6310, 0.6263, 0.5862, 0.5982, 0.5797, 0.6450, 0.6163, 0.6397,
        0.6229, 0.6159, 0.6039, 0.6277, 0.6319, 0.6161, 0.5958],
       device='cuda:0') torch.Size([16])
percent tensor([0.5943, 0.7075, 0.7155, 0.7842, 0.7485, 0.7897, 0.7227, 0.5973, 0.7779,
        0.6372, 0.7723, 0.8227, 0.7250, 0.7993, 0.6549, 0.5823],
       device='cuda:0') torch.Size([16])
percent tensor([0.6185, 0.6695, 0.5567, 0.4917, 0.6084, 0.6962, 0.6288, 0.4140, 0.6660,
        0.6551, 0.7224, 0.4868, 0.6820, 0.6375, 0.4350, 0.5144],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9998, 0.9999, 0.9998, 0.9999, 0.9993, 0.9999, 0.9997, 0.9999,
        0.9998, 1.0000, 0.9998, 0.9999, 0.9998, 0.9996, 0.9995],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 182 | Batch_idx: 0 |  Loss: (0.0614) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 182 | Batch_idx: 10 |  Loss: (0.0786) |  Loss2: (0.0000) | Acc: (96.00%) (1363/1408)
Epoch: 182 | Batch_idx: 20 |  Loss: (0.1026) |  Loss2: (0.0000) | Acc: (96.00%) (2585/2688)
Epoch: 182 | Batch_idx: 30 |  Loss: (0.1024) |  Loss2: (0.0000) | Acc: (96.00%) (3814/3968)
Epoch: 182 | Batch_idx: 40 |  Loss: (0.1034) |  Loss2: (0.0000) | Acc: (96.00%) (5048/5248)
Epoch: 182 | Batch_idx: 50 |  Loss: (0.1036) |  Loss2: (0.0000) | Acc: (96.00%) (6283/6528)
Epoch: 182 | Batch_idx: 60 |  Loss: (0.1095) |  Loss2: (0.0000) | Acc: (96.00%) (7502/7808)
Epoch: 182 | Batch_idx: 70 |  Loss: (0.1095) |  Loss2: (0.0000) | Acc: (96.00%) (8735/9088)
Epoch: 182 | Batch_idx: 80 |  Loss: (0.1112) |  Loss2: (0.0000) | Acc: (96.00%) (9961/10368)
Epoch: 182 | Batch_idx: 90 |  Loss: (0.1106) |  Loss2: (0.0000) | Acc: (96.00%) (11193/11648)
Epoch: 182 | Batch_idx: 100 |  Loss: (0.1112) |  Loss2: (0.0000) | Acc: (96.00%) (12413/12928)
Epoch: 182 | Batch_idx: 110 |  Loss: (0.1101) |  Loss2: (0.0000) | Acc: (96.00%) (13644/14208)
Epoch: 182 | Batch_idx: 120 |  Loss: (0.1088) |  Loss2: (0.0000) | Acc: (96.00%) (14885/15488)
Epoch: 182 | Batch_idx: 130 |  Loss: (0.1096) |  Loss2: (0.0000) | Acc: (96.00%) (16103/16768)
Epoch: 182 | Batch_idx: 140 |  Loss: (0.1108) |  Loss2: (0.0000) | Acc: (95.00%) (17326/18048)
Epoch: 182 | Batch_idx: 150 |  Loss: (0.1118) |  Loss2: (0.0000) | Acc: (95.00%) (18550/19328)
Epoch: 182 | Batch_idx: 160 |  Loss: (0.1108) |  Loss2: (0.0000) | Acc: (96.00%) (19792/20608)
Epoch: 182 | Batch_idx: 170 |  Loss: (0.1094) |  Loss2: (0.0000) | Acc: (96.00%) (21033/21888)
Epoch: 182 | Batch_idx: 180 |  Loss: (0.1103) |  Loss2: (0.0000) | Acc: (96.00%) (22250/23168)
Epoch: 182 | Batch_idx: 190 |  Loss: (0.1103) |  Loss2: (0.0000) | Acc: (96.00%) (23476/24448)
Epoch: 182 | Batch_idx: 200 |  Loss: (0.1093) |  Loss2: (0.0000) | Acc: (96.00%) (24722/25728)
Epoch: 182 | Batch_idx: 210 |  Loss: (0.1088) |  Loss2: (0.0000) | Acc: (96.00%) (25956/27008)
Epoch: 182 | Batch_idx: 220 |  Loss: (0.1082) |  Loss2: (0.0000) | Acc: (96.00%) (27202/28288)
Epoch: 182 | Batch_idx: 230 |  Loss: (0.1082) |  Loss2: (0.0000) | Acc: (96.00%) (28441/29568)
Epoch: 182 | Batch_idx: 240 |  Loss: (0.1081) |  Loss2: (0.0000) | Acc: (96.00%) (29678/30848)
Epoch: 182 | Batch_idx: 250 |  Loss: (0.1079) |  Loss2: (0.0000) | Acc: (96.00%) (30911/32128)
Epoch: 182 | Batch_idx: 260 |  Loss: (0.1077) |  Loss2: (0.0000) | Acc: (96.00%) (32147/33408)
Epoch: 182 | Batch_idx: 270 |  Loss: (0.1075) |  Loss2: (0.0000) | Acc: (96.00%) (33379/34688)
Epoch: 182 | Batch_idx: 280 |  Loss: (0.1078) |  Loss2: (0.0000) | Acc: (96.00%) (34607/35968)
Epoch: 182 | Batch_idx: 290 |  Loss: (0.1072) |  Loss2: (0.0000) | Acc: (96.00%) (35845/37248)
Epoch: 182 | Batch_idx: 300 |  Loss: (0.1072) |  Loss2: (0.0000) | Acc: (96.00%) (37081/38528)
Epoch: 182 | Batch_idx: 310 |  Loss: (0.1064) |  Loss2: (0.0000) | Acc: (96.00%) (38332/39808)
Epoch: 182 | Batch_idx: 320 |  Loss: (0.1057) |  Loss2: (0.0000) | Acc: (96.00%) (39572/41088)
Epoch: 182 | Batch_idx: 330 |  Loss: (0.1053) |  Loss2: (0.0000) | Acc: (96.00%) (40807/42368)
Epoch: 182 | Batch_idx: 340 |  Loss: (0.1047) |  Loss2: (0.0000) | Acc: (96.00%) (42055/43648)
Epoch: 182 | Batch_idx: 350 |  Loss: (0.1044) |  Loss2: (0.0000) | Acc: (96.00%) (43295/44928)
Epoch: 182 | Batch_idx: 360 |  Loss: (0.1043) |  Loss2: (0.0000) | Acc: (96.00%) (44528/46208)
Epoch: 182 | Batch_idx: 370 |  Loss: (0.1041) |  Loss2: (0.0000) | Acc: (96.00%) (45765/47488)
Epoch: 182 | Batch_idx: 380 |  Loss: (0.1044) |  Loss2: (0.0000) | Acc: (96.00%) (46996/48768)
Epoch: 182 | Batch_idx: 390 |  Loss: (0.1047) |  Loss2: (0.0000) | Acc: (96.00%) (48179/50000)
# TEST : Loss: (0.4059) | Acc: (89.00%) (8900/10000)
percent tensor([0.5404, 0.5529, 0.5484, 0.5159, 0.5554, 0.5638, 0.5615, 0.5312, 0.5402,
        0.5474, 0.5538, 0.5545, 0.5381, 0.5165, 0.5585, 0.5341],
       device='cuda:0') torch.Size([16])
percent tensor([0.5432, 0.5442, 0.5493, 0.5516, 0.5526, 0.5412, 0.5477, 0.5542, 0.5470,
        0.5454, 0.5425, 0.5472, 0.5429, 0.5472, 0.5425, 0.5450],
       device='cuda:0') torch.Size([16])
percent tensor([0.6857, 0.6961, 0.5217, 0.4560, 0.4569, 0.6219, 0.6446, 0.5087, 0.5630,
        0.6664, 0.6738, 0.5900, 0.7034, 0.6552, 0.6746, 0.6498],
       device='cuda:0') torch.Size([16])
percent tensor([0.7073, 0.6018, 0.6992, 0.6848, 0.7010, 0.7835, 0.6585, 0.7027, 0.6595,
        0.6326, 0.6428, 0.6289, 0.5800, 0.6756, 0.6909, 0.7564],
       device='cuda:0') torch.Size([16])
percent tensor([0.6382, 0.6401, 0.6500, 0.6026, 0.6175, 0.5916, 0.6542, 0.6303, 0.6518,
        0.6301, 0.6205, 0.6155, 0.6344, 0.6458, 0.6230, 0.6045],
       device='cuda:0') torch.Size([16])
percent tensor([0.5739, 0.7081, 0.7132, 0.7670, 0.7330, 0.7760, 0.7255, 0.5761, 0.7754,
        0.6454, 0.7668, 0.8247, 0.7310, 0.7943, 0.6458, 0.5646],
       device='cuda:0') torch.Size([16])
percent tensor([0.6045, 0.6806, 0.5096, 0.4592, 0.5859, 0.6813, 0.6320, 0.3878, 0.6665,
        0.6664, 0.7170, 0.4565, 0.6956, 0.6415, 0.4251, 0.5148],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9997, 0.9999, 0.9998, 0.9999, 0.9993, 0.9999, 0.9997, 0.9999,
        0.9998, 1.0000, 0.9998, 0.9999, 0.9997, 0.9995, 0.9994],
       device='cuda:0') torch.Size([16])
Epoch: 183 | Batch_idx: 0 |  Loss: (0.1147) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 183 | Batch_idx: 10 |  Loss: (0.1086) |  Loss2: (0.0000) | Acc: (95.00%) (1350/1408)
Epoch: 183 | Batch_idx: 20 |  Loss: (0.1009) |  Loss2: (0.0000) | Acc: (96.00%) (2591/2688)
Epoch: 183 | Batch_idx: 30 |  Loss: (0.0993) |  Loss2: (0.0000) | Acc: (96.00%) (3826/3968)
Epoch: 183 | Batch_idx: 40 |  Loss: (0.1029) |  Loss2: (0.0000) | Acc: (96.00%) (5054/5248)
Epoch: 183 | Batch_idx: 50 |  Loss: (0.1016) |  Loss2: (0.0000) | Acc: (96.00%) (6290/6528)
Epoch: 183 | Batch_idx: 60 |  Loss: (0.1023) |  Loss2: (0.0000) | Acc: (96.00%) (7527/7808)
Epoch: 183 | Batch_idx: 70 |  Loss: (0.1000) |  Loss2: (0.0000) | Acc: (96.00%) (8770/9088)
Epoch: 183 | Batch_idx: 80 |  Loss: (0.1016) |  Loss2: (0.0000) | Acc: (96.00%) (10003/10368)
Epoch: 183 | Batch_idx: 90 |  Loss: (0.1012) |  Loss2: (0.0000) | Acc: (96.00%) (11242/11648)
Epoch: 183 | Batch_idx: 100 |  Loss: (0.1009) |  Loss2: (0.0000) | Acc: (96.00%) (12478/12928)
Epoch: 183 | Batch_idx: 110 |  Loss: (0.0996) |  Loss2: (0.0000) | Acc: (96.00%) (13722/14208)
Epoch: 183 | Batch_idx: 120 |  Loss: (0.0996) |  Loss2: (0.0000) | Acc: (96.00%) (14959/15488)
Epoch: 183 | Batch_idx: 130 |  Loss: (0.0989) |  Loss2: (0.0000) | Acc: (96.00%) (16200/16768)
Epoch: 183 | Batch_idx: 140 |  Loss: (0.0985) |  Loss2: (0.0000) | Acc: (96.00%) (17431/18048)
Epoch: 183 | Batch_idx: 150 |  Loss: (0.0973) |  Loss2: (0.0000) | Acc: (96.00%) (18677/19328)
Epoch: 183 | Batch_idx: 160 |  Loss: (0.0969) |  Loss2: (0.0000) | Acc: (96.00%) (19923/20608)
Epoch: 183 | Batch_idx: 170 |  Loss: (0.0959) |  Loss2: (0.0000) | Acc: (96.00%) (21168/21888)
Epoch: 183 | Batch_idx: 180 |  Loss: (0.0944) |  Loss2: (0.0000) | Acc: (96.00%) (22417/23168)
Epoch: 183 | Batch_idx: 190 |  Loss: (0.0935) |  Loss2: (0.0000) | Acc: (96.00%) (23662/24448)
Epoch: 183 | Batch_idx: 200 |  Loss: (0.0930) |  Loss2: (0.0000) | Acc: (96.00%) (24911/25728)
Epoch: 183 | Batch_idx: 210 |  Loss: (0.0932) |  Loss2: (0.0000) | Acc: (96.00%) (26146/27008)
Epoch: 183 | Batch_idx: 220 |  Loss: (0.0928) |  Loss2: (0.0000) | Acc: (96.00%) (27384/28288)
Epoch: 183 | Batch_idx: 230 |  Loss: (0.0924) |  Loss2: (0.0000) | Acc: (96.00%) (28620/29568)
Epoch: 183 | Batch_idx: 240 |  Loss: (0.0923) |  Loss2: (0.0000) | Acc: (96.00%) (29864/30848)
Epoch: 183 | Batch_idx: 250 |  Loss: (0.0921) |  Loss2: (0.0000) | Acc: (96.00%) (31108/32128)
Epoch: 183 | Batch_idx: 260 |  Loss: (0.0922) |  Loss2: (0.0000) | Acc: (96.00%) (32345/33408)
Epoch: 183 | Batch_idx: 270 |  Loss: (0.0927) |  Loss2: (0.0000) | Acc: (96.00%) (33567/34688)
Epoch: 183 | Batch_idx: 280 |  Loss: (0.0925) |  Loss2: (0.0000) | Acc: (96.00%) (34809/35968)
Epoch: 183 | Batch_idx: 290 |  Loss: (0.0921) |  Loss2: (0.0000) | Acc: (96.00%) (36054/37248)
Epoch: 183 | Batch_idx: 300 |  Loss: (0.0917) |  Loss2: (0.0000) | Acc: (96.00%) (37299/38528)
Epoch: 183 | Batch_idx: 310 |  Loss: (0.0919) |  Loss2: (0.0000) | Acc: (96.00%) (38531/39808)
Epoch: 183 | Batch_idx: 320 |  Loss: (0.0916) |  Loss2: (0.0000) | Acc: (96.00%) (39778/41088)
Epoch: 183 | Batch_idx: 330 |  Loss: (0.0912) |  Loss2: (0.0000) | Acc: (96.00%) (41029/42368)
Epoch: 183 | Batch_idx: 340 |  Loss: (0.0911) |  Loss2: (0.0000) | Acc: (96.00%) (42274/43648)
Epoch: 183 | Batch_idx: 350 |  Loss: (0.0909) |  Loss2: (0.0000) | Acc: (96.00%) (43516/44928)
Epoch: 183 | Batch_idx: 360 |  Loss: (0.0907) |  Loss2: (0.0000) | Acc: (96.00%) (44760/46208)
Epoch: 183 | Batch_idx: 370 |  Loss: (0.0912) |  Loss2: (0.0000) | Acc: (96.00%) (46001/47488)
Epoch: 183 | Batch_idx: 380 |  Loss: (0.0912) |  Loss2: (0.0000) | Acc: (96.00%) (47242/48768)
Epoch: 183 | Batch_idx: 390 |  Loss: (0.0914) |  Loss2: (0.0000) | Acc: (96.00%) (48429/50000)
# TEST : Loss: (0.3966) | Acc: (89.00%) (8914/10000)
percent tensor([0.5408, 0.5530, 0.5508, 0.5177, 0.5574, 0.5650, 0.5620, 0.5328, 0.5405,
        0.5480, 0.5535, 0.5561, 0.5382, 0.5165, 0.5592, 0.5346],
       device='cuda:0') torch.Size([16])
percent tensor([0.5436, 0.5443, 0.5508, 0.5530, 0.5538, 0.5417, 0.5482, 0.5556, 0.5477,
        0.5460, 0.5426, 0.5483, 0.5432, 0.5474, 0.5427, 0.5456],
       device='cuda:0') torch.Size([16])
percent tensor([0.6877, 0.6986, 0.5190, 0.4555, 0.4539, 0.6263, 0.6469, 0.5085, 0.5642,
        0.6675, 0.6758, 0.5889, 0.7051, 0.6632, 0.6776, 0.6534],
       device='cuda:0') torch.Size([16])
percent tensor([0.7150, 0.6033, 0.7106, 0.6934, 0.7087, 0.7897, 0.6622, 0.7116, 0.6633,
        0.6357, 0.6449, 0.6348, 0.5814, 0.6786, 0.6977, 0.7635],
       device='cuda:0') torch.Size([16])
percent tensor([0.6461, 0.6462, 0.6589, 0.6120, 0.6251, 0.6010, 0.6608, 0.6396, 0.6577,
        0.6363, 0.6240, 0.6190, 0.6382, 0.6520, 0.6288, 0.6138],
       device='cuda:0') torch.Size([16])
percent tensor([0.5619, 0.7102, 0.7021, 0.7570, 0.7227, 0.7685, 0.7196, 0.5633, 0.7696,
        0.6432, 0.7645, 0.8192, 0.7281, 0.7917, 0.6358, 0.5523],
       device='cuda:0') torch.Size([16])
percent tensor([0.5985, 0.6820, 0.5251, 0.4711, 0.6010, 0.6852, 0.6285, 0.3930, 0.6669,
        0.6666, 0.7143, 0.4537, 0.6967, 0.6393, 0.4169, 0.5128],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9998, 0.9999, 0.9998, 0.9999, 0.9994, 0.9999, 0.9997, 0.9999,
        0.9998, 1.0000, 0.9998, 0.9999, 0.9997, 0.9995, 0.9994],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 184 | Batch_idx: 0 |  Loss: (0.0583) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 184 | Batch_idx: 10 |  Loss: (0.0693) |  Loss2: (0.0000) | Acc: (97.00%) (1379/1408)
Epoch: 184 | Batch_idx: 20 |  Loss: (0.0741) |  Loss2: (0.0000) | Acc: (97.00%) (2622/2688)
Epoch: 184 | Batch_idx: 30 |  Loss: (0.0759) |  Loss2: (0.0000) | Acc: (97.00%) (3870/3968)
Epoch: 184 | Batch_idx: 40 |  Loss: (0.0763) |  Loss2: (0.0000) | Acc: (97.00%) (5120/5248)
Epoch: 184 | Batch_idx: 50 |  Loss: (0.0769) |  Loss2: (0.0000) | Acc: (97.00%) (6361/6528)
Epoch: 184 | Batch_idx: 60 |  Loss: (0.0786) |  Loss2: (0.0000) | Acc: (97.00%) (7600/7808)
Epoch: 184 | Batch_idx: 70 |  Loss: (0.0790) |  Loss2: (0.0000) | Acc: (97.00%) (8849/9088)
Epoch: 184 | Batch_idx: 80 |  Loss: (0.0798) |  Loss2: (0.0000) | Acc: (97.00%) (10089/10368)
Epoch: 184 | Batch_idx: 90 |  Loss: (0.0802) |  Loss2: (0.0000) | Acc: (97.00%) (11328/11648)
Epoch: 184 | Batch_idx: 100 |  Loss: (0.0798) |  Loss2: (0.0000) | Acc: (97.00%) (12578/12928)
Epoch: 184 | Batch_idx: 110 |  Loss: (0.0805) |  Loss2: (0.0000) | Acc: (97.00%) (13823/14208)
Epoch: 184 | Batch_idx: 120 |  Loss: (0.0799) |  Loss2: (0.0000) | Acc: (97.00%) (15072/15488)
Epoch: 184 | Batch_idx: 130 |  Loss: (0.0807) |  Loss2: (0.0000) | Acc: (97.00%) (16309/16768)
Epoch: 184 | Batch_idx: 140 |  Loss: (0.0812) |  Loss2: (0.0000) | Acc: (97.00%) (17550/18048)
Epoch: 184 | Batch_idx: 150 |  Loss: (0.0815) |  Loss2: (0.0000) | Acc: (97.00%) (18794/19328)
Epoch: 184 | Batch_idx: 160 |  Loss: (0.0829) |  Loss2: (0.0000) | Acc: (97.00%) (20018/20608)
Epoch: 184 | Batch_idx: 170 |  Loss: (0.0831) |  Loss2: (0.0000) | Acc: (97.00%) (21264/21888)
Epoch: 184 | Batch_idx: 180 |  Loss: (0.0833) |  Loss2: (0.0000) | Acc: (97.00%) (22504/23168)
Epoch: 184 | Batch_idx: 190 |  Loss: (0.0827) |  Loss2: (0.0000) | Acc: (97.00%) (23753/24448)
Epoch: 184 | Batch_idx: 200 |  Loss: (0.0822) |  Loss2: (0.0000) | Acc: (97.00%) (25006/25728)
Epoch: 184 | Batch_idx: 210 |  Loss: (0.0815) |  Loss2: (0.0000) | Acc: (97.00%) (26260/27008)
Epoch: 184 | Batch_idx: 220 |  Loss: (0.0813) |  Loss2: (0.0000) | Acc: (97.00%) (27505/28288)
Epoch: 184 | Batch_idx: 230 |  Loss: (0.0822) |  Loss2: (0.0000) | Acc: (97.00%) (28739/29568)
Epoch: 184 | Batch_idx: 240 |  Loss: (0.0820) |  Loss2: (0.0000) | Acc: (97.00%) (29984/30848)
Epoch: 184 | Batch_idx: 250 |  Loss: (0.0825) |  Loss2: (0.0000) | Acc: (97.00%) (31229/32128)
Epoch: 184 | Batch_idx: 260 |  Loss: (0.0826) |  Loss2: (0.0000) | Acc: (97.00%) (32476/33408)
Epoch: 184 | Batch_idx: 270 |  Loss: (0.0825) |  Loss2: (0.0000) | Acc: (97.00%) (33715/34688)
Epoch: 184 | Batch_idx: 280 |  Loss: (0.0827) |  Loss2: (0.0000) | Acc: (97.00%) (34950/35968)
Epoch: 184 | Batch_idx: 290 |  Loss: (0.0833) |  Loss2: (0.0000) | Acc: (97.00%) (36184/37248)
Epoch: 184 | Batch_idx: 300 |  Loss: (0.0830) |  Loss2: (0.0000) | Acc: (97.00%) (37430/38528)
Epoch: 184 | Batch_idx: 310 |  Loss: (0.0836) |  Loss2: (0.0000) | Acc: (97.00%) (38664/39808)
Epoch: 184 | Batch_idx: 320 |  Loss: (0.0834) |  Loss2: (0.0000) | Acc: (97.00%) (39904/41088)
Epoch: 184 | Batch_idx: 330 |  Loss: (0.0831) |  Loss2: (0.0000) | Acc: (97.00%) (41153/42368)
Epoch: 184 | Batch_idx: 340 |  Loss: (0.0835) |  Loss2: (0.0000) | Acc: (97.00%) (42391/43648)
Epoch: 184 | Batch_idx: 350 |  Loss: (0.0838) |  Loss2: (0.0000) | Acc: (97.00%) (43632/44928)
Epoch: 184 | Batch_idx: 360 |  Loss: (0.0842) |  Loss2: (0.0000) | Acc: (97.00%) (44867/46208)
Epoch: 184 | Batch_idx: 370 |  Loss: (0.0842) |  Loss2: (0.0000) | Acc: (97.00%) (46109/47488)
Epoch: 184 | Batch_idx: 380 |  Loss: (0.0843) |  Loss2: (0.0000) | Acc: (97.00%) (47344/48768)
Epoch: 184 | Batch_idx: 390 |  Loss: (0.0845) |  Loss2: (0.0000) | Acc: (97.00%) (48537/50000)
# TEST : Loss: (0.4109) | Acc: (88.00%) (8844/10000)
percent tensor([0.5411, 0.5530, 0.5465, 0.5169, 0.5557, 0.5655, 0.5612, 0.5299, 0.5399,
        0.5477, 0.5539, 0.5529, 0.5381, 0.5161, 0.5593, 0.5354],
       device='cuda:0') torch.Size([16])
percent tensor([0.5426, 0.5441, 0.5515, 0.5541, 0.5536, 0.5419, 0.5478, 0.5564, 0.5474,
        0.5459, 0.5417, 0.5491, 0.5426, 0.5478, 0.5430, 0.5450],
       device='cuda:0') torch.Size([16])
percent tensor([0.6846, 0.7010, 0.5032, 0.4542, 0.4437, 0.6151, 0.6466, 0.5082, 0.5723,
        0.6684, 0.6796, 0.5737, 0.7033, 0.6568, 0.6717, 0.6599],
       device='cuda:0') torch.Size([16])
percent tensor([0.7248, 0.6073, 0.7241, 0.7049, 0.7221, 0.7905, 0.6699, 0.7193, 0.6682,
        0.6423, 0.6536, 0.6435, 0.5914, 0.6823, 0.7007, 0.7668],
       device='cuda:0') torch.Size([16])
percent tensor([0.6418, 0.6408, 0.6498, 0.6020, 0.6140, 0.6078, 0.6532, 0.6407, 0.6532,
        0.6344, 0.6171, 0.6104, 0.6369, 0.6361, 0.6315, 0.6117],
       device='cuda:0') torch.Size([16])
percent tensor([0.5915, 0.7383, 0.6681, 0.7456, 0.7188, 0.7490, 0.7338, 0.5694, 0.7898,
        0.6718, 0.7830, 0.8078, 0.7372, 0.8248, 0.6227, 0.5363],
       device='cuda:0') torch.Size([16])
percent tensor([0.6035, 0.6871, 0.5615, 0.4469, 0.6236, 0.6966, 0.6069, 0.4001, 0.6739,
        0.6797, 0.7093, 0.4710, 0.7137, 0.5903, 0.3861, 0.4892],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9998, 0.9999, 0.9998, 0.9999, 0.9993, 0.9999, 0.9996, 0.9999,
        0.9998, 1.0000, 0.9999, 0.9999, 0.9997, 0.9995, 0.9988],
       device='cuda:0') torch.Size([16])
Epoch: 185 | Batch_idx: 0 |  Loss: (0.0364) |  Loss2: (0.0000) | Acc: (100.00%) (128/128)
Epoch: 185 | Batch_idx: 10 |  Loss: (0.0707) |  Loss2: (0.0000) | Acc: (97.00%) (1374/1408)
Epoch: 185 | Batch_idx: 20 |  Loss: (0.0750) |  Loss2: (0.0000) | Acc: (97.00%) (2612/2688)
Epoch: 185 | Batch_idx: 30 |  Loss: (0.0753) |  Loss2: (0.0000) | Acc: (97.00%) (3860/3968)
Epoch: 185 | Batch_idx: 40 |  Loss: (0.0742) |  Loss2: (0.0000) | Acc: (97.00%) (5111/5248)
Epoch: 185 | Batch_idx: 50 |  Loss: (0.0737) |  Loss2: (0.0000) | Acc: (97.00%) (6363/6528)
Epoch: 185 | Batch_idx: 60 |  Loss: (0.0727) |  Loss2: (0.0000) | Acc: (97.00%) (7615/7808)
Epoch: 185 | Batch_idx: 70 |  Loss: (0.0721) |  Loss2: (0.0000) | Acc: (97.00%) (8869/9088)
Epoch: 185 | Batch_idx: 80 |  Loss: (0.0728) |  Loss2: (0.0000) | Acc: (97.00%) (10110/10368)
Epoch: 185 | Batch_idx: 90 |  Loss: (0.0723) |  Loss2: (0.0000) | Acc: (97.00%) (11365/11648)
Epoch: 185 | Batch_idx: 100 |  Loss: (0.0719) |  Loss2: (0.0000) | Acc: (97.00%) (12611/12928)
Epoch: 185 | Batch_idx: 110 |  Loss: (0.0727) |  Loss2: (0.0000) | Acc: (97.00%) (13854/14208)
Epoch: 185 | Batch_idx: 120 |  Loss: (0.0759) |  Loss2: (0.0000) | Acc: (97.00%) (15078/15488)
Epoch: 185 | Batch_idx: 130 |  Loss: (0.0754) |  Loss2: (0.0000) | Acc: (97.00%) (16335/16768)
Epoch: 185 | Batch_idx: 140 |  Loss: (0.0757) |  Loss2: (0.0000) | Acc: (97.00%) (17588/18048)
Epoch: 185 | Batch_idx: 150 |  Loss: (0.0750) |  Loss2: (0.0000) | Acc: (97.00%) (18837/19328)
Epoch: 185 | Batch_idx: 160 |  Loss: (0.0756) |  Loss2: (0.0000) | Acc: (97.00%) (20076/20608)
Epoch: 185 | Batch_idx: 170 |  Loss: (0.0763) |  Loss2: (0.0000) | Acc: (97.00%) (21319/21888)
Epoch: 185 | Batch_idx: 180 |  Loss: (0.0771) |  Loss2: (0.0000) | Acc: (97.00%) (22561/23168)
Epoch: 185 | Batch_idx: 190 |  Loss: (0.0777) |  Loss2: (0.0000) | Acc: (97.00%) (23802/24448)
Epoch: 185 | Batch_idx: 200 |  Loss: (0.0781) |  Loss2: (0.0000) | Acc: (97.00%) (25046/25728)
Epoch: 185 | Batch_idx: 210 |  Loss: (0.0786) |  Loss2: (0.0000) | Acc: (97.00%) (26286/27008)
Epoch: 185 | Batch_idx: 220 |  Loss: (0.0795) |  Loss2: (0.0000) | Acc: (97.00%) (27526/28288)
Epoch: 185 | Batch_idx: 230 |  Loss: (0.0796) |  Loss2: (0.0000) | Acc: (97.00%) (28770/29568)
Epoch: 185 | Batch_idx: 240 |  Loss: (0.0799) |  Loss2: (0.0000) | Acc: (97.00%) (30010/30848)
Epoch: 185 | Batch_idx: 250 |  Loss: (0.0799) |  Loss2: (0.0000) | Acc: (97.00%) (31256/32128)
Epoch: 185 | Batch_idx: 260 |  Loss: (0.0803) |  Loss2: (0.0000) | Acc: (97.00%) (32490/33408)
Epoch: 185 | Batch_idx: 270 |  Loss: (0.0807) |  Loss2: (0.0000) | Acc: (97.00%) (33731/34688)
Epoch: 185 | Batch_idx: 280 |  Loss: (0.0810) |  Loss2: (0.0000) | Acc: (97.00%) (34977/35968)
Epoch: 185 | Batch_idx: 290 |  Loss: (0.0816) |  Loss2: (0.0000) | Acc: (97.00%) (36211/37248)
Epoch: 185 | Batch_idx: 300 |  Loss: (0.0820) |  Loss2: (0.0000) | Acc: (97.00%) (37450/38528)
Epoch: 185 | Batch_idx: 310 |  Loss: (0.0822) |  Loss2: (0.0000) | Acc: (97.00%) (38690/39808)
Epoch: 185 | Batch_idx: 320 |  Loss: (0.0824) |  Loss2: (0.0000) | Acc: (97.00%) (39927/41088)
Epoch: 185 | Batch_idx: 330 |  Loss: (0.0830) |  Loss2: (0.0000) | Acc: (97.00%) (41161/42368)
Epoch: 185 | Batch_idx: 340 |  Loss: (0.0830) |  Loss2: (0.0000) | Acc: (97.00%) (42403/43648)
Epoch: 185 | Batch_idx: 350 |  Loss: (0.0833) |  Loss2: (0.0000) | Acc: (97.00%) (43637/44928)
Epoch: 185 | Batch_idx: 360 |  Loss: (0.0831) |  Loss2: (0.0000) | Acc: (97.00%) (44885/46208)
Epoch: 185 | Batch_idx: 370 |  Loss: (0.0835) |  Loss2: (0.0000) | Acc: (97.00%) (46121/47488)
Epoch: 185 | Batch_idx: 380 |  Loss: (0.0835) |  Loss2: (0.0000) | Acc: (97.00%) (47364/48768)
Epoch: 185 | Batch_idx: 390 |  Loss: (0.0835) |  Loss2: (0.0000) | Acc: (97.00%) (48564/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_185.pth.tar'
# TEST : Loss: (0.4064) | Acc: (88.00%) (8873/10000)
percent tensor([0.5405, 0.5526, 0.5409, 0.5128, 0.5516, 0.5651, 0.5602, 0.5262, 0.5391,
        0.5459, 0.5546, 0.5491, 0.5380, 0.5194, 0.5586, 0.5346],
       device='cuda:0') torch.Size([16])
percent tensor([0.5417, 0.5449, 0.5511, 0.5547, 0.5530, 0.5406, 0.5473, 0.5570, 0.5467,
        0.5455, 0.5414, 0.5480, 0.5423, 0.5472, 0.5430, 0.5448],
       device='cuda:0') torch.Size([16])
percent tensor([0.6908, 0.7048, 0.5068, 0.4519, 0.4525, 0.6288, 0.6493, 0.5075, 0.5688,
        0.6791, 0.6883, 0.5829, 0.7093, 0.6606, 0.6781, 0.6638],
       device='cuda:0') torch.Size([16])
percent tensor([0.7077, 0.6046, 0.7025, 0.6950, 0.7088, 0.7796, 0.6542, 0.7167, 0.6577,
        0.6339, 0.6448, 0.6318, 0.5799, 0.6765, 0.6942, 0.7594],
       device='cuda:0') torch.Size([16])
percent tensor([0.6499, 0.6453, 0.6496, 0.6041, 0.6153, 0.6150, 0.6680, 0.6372, 0.6558,
        0.6337, 0.6177, 0.6129, 0.6370, 0.6415, 0.6303, 0.6255],
       device='cuda:0') torch.Size([16])
percent tensor([0.5432, 0.6812, 0.6658, 0.7416, 0.7048, 0.7490, 0.6979, 0.5452, 0.7670,
        0.6081, 0.7572, 0.7832, 0.7149, 0.7720, 0.5990, 0.5412],
       device='cuda:0') torch.Size([16])
percent tensor([0.5936, 0.6674, 0.5415, 0.4652, 0.6022, 0.6934, 0.6152, 0.3981, 0.6553,
        0.6658, 0.7018, 0.4510, 0.6954, 0.5961, 0.4032, 0.5223],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9997, 0.9999, 0.9998, 0.9999, 0.9997, 0.9999, 0.9997, 0.9998,
        0.9998, 1.0000, 0.9999, 0.9999, 0.9997, 0.9995, 0.9992],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 186 | Batch_idx: 0 |  Loss: (0.0767) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 186 | Batch_idx: 10 |  Loss: (0.0839) |  Loss2: (0.0000) | Acc: (97.00%) (1374/1408)
Epoch: 186 | Batch_idx: 20 |  Loss: (0.0857) |  Loss2: (0.0000) | Acc: (97.00%) (2614/2688)
Epoch: 186 | Batch_idx: 30 |  Loss: (0.0880) |  Loss2: (0.0000) | Acc: (97.00%) (3849/3968)
Epoch: 186 | Batch_idx: 40 |  Loss: (0.0851) |  Loss2: (0.0000) | Acc: (97.00%) (5101/5248)
Epoch: 186 | Batch_idx: 50 |  Loss: (0.0895) |  Loss2: (0.0000) | Acc: (97.00%) (6333/6528)
Epoch: 186 | Batch_idx: 60 |  Loss: (0.0897) |  Loss2: (0.0000) | Acc: (96.00%) (7571/7808)
Epoch: 186 | Batch_idx: 70 |  Loss: (0.0900) |  Loss2: (0.0000) | Acc: (96.00%) (8808/9088)
Epoch: 186 | Batch_idx: 80 |  Loss: (0.0895) |  Loss2: (0.0000) | Acc: (96.00%) (10054/10368)
Epoch: 186 | Batch_idx: 90 |  Loss: (0.0905) |  Loss2: (0.0000) | Acc: (96.00%) (11290/11648)
Epoch: 186 | Batch_idx: 100 |  Loss: (0.0915) |  Loss2: (0.0000) | Acc: (96.00%) (12522/12928)
Epoch: 186 | Batch_idx: 110 |  Loss: (0.0908) |  Loss2: (0.0000) | Acc: (96.00%) (13769/14208)
Epoch: 186 | Batch_idx: 120 |  Loss: (0.0906) |  Loss2: (0.0000) | Acc: (96.00%) (15009/15488)
Epoch: 186 | Batch_idx: 130 |  Loss: (0.0923) |  Loss2: (0.0000) | Acc: (96.00%) (16238/16768)
Epoch: 186 | Batch_idx: 140 |  Loss: (0.0926) |  Loss2: (0.0000) | Acc: (96.00%) (17477/18048)
Epoch: 186 | Batch_idx: 150 |  Loss: (0.0937) |  Loss2: (0.0000) | Acc: (96.00%) (18711/19328)
Epoch: 186 | Batch_idx: 160 |  Loss: (0.0930) |  Loss2: (0.0000) | Acc: (96.00%) (19953/20608)
Epoch: 186 | Batch_idx: 170 |  Loss: (0.0931) |  Loss2: (0.0000) | Acc: (96.00%) (21187/21888)
Epoch: 186 | Batch_idx: 180 |  Loss: (0.0930) |  Loss2: (0.0000) | Acc: (96.00%) (22420/23168)
Epoch: 186 | Batch_idx: 190 |  Loss: (0.0928) |  Loss2: (0.0000) | Acc: (96.00%) (23658/24448)
Epoch: 186 | Batch_idx: 200 |  Loss: (0.0934) |  Loss2: (0.0000) | Acc: (96.00%) (24885/25728)
Epoch: 186 | Batch_idx: 210 |  Loss: (0.0922) |  Loss2: (0.0000) | Acc: (96.00%) (26138/27008)
Epoch: 186 | Batch_idx: 220 |  Loss: (0.0922) |  Loss2: (0.0000) | Acc: (96.00%) (27377/28288)
Epoch: 186 | Batch_idx: 230 |  Loss: (0.0921) |  Loss2: (0.0000) | Acc: (96.00%) (28617/29568)
Epoch: 186 | Batch_idx: 240 |  Loss: (0.0929) |  Loss2: (0.0000) | Acc: (96.00%) (29850/30848)
Epoch: 186 | Batch_idx: 250 |  Loss: (0.0932) |  Loss2: (0.0000) | Acc: (96.00%) (31080/32128)
Epoch: 186 | Batch_idx: 260 |  Loss: (0.0931) |  Loss2: (0.0000) | Acc: (96.00%) (32322/33408)
Epoch: 186 | Batch_idx: 270 |  Loss: (0.0926) |  Loss2: (0.0000) | Acc: (96.00%) (33568/34688)
Epoch: 186 | Batch_idx: 280 |  Loss: (0.0921) |  Loss2: (0.0000) | Acc: (96.00%) (34814/35968)
Epoch: 186 | Batch_idx: 290 |  Loss: (0.0923) |  Loss2: (0.0000) | Acc: (96.00%) (36051/37248)
Epoch: 186 | Batch_idx: 300 |  Loss: (0.0922) |  Loss2: (0.0000) | Acc: (96.00%) (37290/38528)
Epoch: 186 | Batch_idx: 310 |  Loss: (0.0919) |  Loss2: (0.0000) | Acc: (96.00%) (38535/39808)
Epoch: 186 | Batch_idx: 320 |  Loss: (0.0915) |  Loss2: (0.0000) | Acc: (96.00%) (39781/41088)
Epoch: 186 | Batch_idx: 330 |  Loss: (0.0916) |  Loss2: (0.0000) | Acc: (96.00%) (41016/42368)
Epoch: 186 | Batch_idx: 340 |  Loss: (0.0919) |  Loss2: (0.0000) | Acc: (96.00%) (42248/43648)
Epoch: 186 | Batch_idx: 350 |  Loss: (0.0917) |  Loss2: (0.0000) | Acc: (96.00%) (43492/44928)
Epoch: 186 | Batch_idx: 360 |  Loss: (0.0915) |  Loss2: (0.0000) | Acc: (96.00%) (44732/46208)
Epoch: 186 | Batch_idx: 370 |  Loss: (0.0912) |  Loss2: (0.0000) | Acc: (96.00%) (45978/47488)
Epoch: 186 | Batch_idx: 380 |  Loss: (0.0913) |  Loss2: (0.0000) | Acc: (96.00%) (47217/48768)
Epoch: 186 | Batch_idx: 390 |  Loss: (0.0912) |  Loss2: (0.0000) | Acc: (96.00%) (48412/50000)
# TEST : Loss: (0.3977) | Acc: (88.00%) (8883/10000)
percent tensor([0.5566, 0.5693, 0.5570, 0.5263, 0.5696, 0.5809, 0.5781, 0.5417, 0.5556,
        0.5619, 0.5711, 0.5656, 0.5540, 0.5340, 0.5749, 0.5496],
       device='cuda:0') torch.Size([16])
percent tensor([0.5400, 0.5430, 0.5484, 0.5521, 0.5504, 0.5389, 0.5454, 0.5546, 0.5448,
        0.5438, 0.5396, 0.5459, 0.5406, 0.5454, 0.5413, 0.5432],
       device='cuda:0') torch.Size([16])
percent tensor([0.6858, 0.7099, 0.4989, 0.4414, 0.4403, 0.6294, 0.6452, 0.4857, 0.5594,
        0.6779, 0.6913, 0.5822, 0.7123, 0.6562, 0.6768, 0.6607],
       device='cuda:0') torch.Size([16])
percent tensor([0.6908, 0.5867, 0.6801, 0.6746, 0.6888, 0.7695, 0.6365, 0.6994, 0.6432,
        0.6122, 0.6282, 0.6146, 0.5595, 0.6624, 0.6777, 0.7435],
       device='cuda:0') torch.Size([16])
percent tensor([0.6515, 0.6380, 0.6592, 0.6177, 0.6281, 0.6156, 0.6644, 0.6503, 0.6584,
        0.6264, 0.6133, 0.6158, 0.6278, 0.6431, 0.6309, 0.6226],
       device='cuda:0') torch.Size([16])
percent tensor([0.5418, 0.7079, 0.6578, 0.7397, 0.6949, 0.7534, 0.6981, 0.5271, 0.7651,
        0.6332, 0.7674, 0.7839, 0.7258, 0.7792, 0.5999, 0.5428],
       device='cuda:0') torch.Size([16])
percent tensor([0.5951, 0.6697, 0.5511, 0.4601, 0.6093, 0.6869, 0.6165, 0.3980, 0.6489,
        0.6850, 0.7001, 0.4276, 0.7042, 0.5964, 0.3939, 0.5198],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9997, 0.9999, 0.9999, 0.9999, 0.9997, 0.9999, 0.9997, 0.9998,
        0.9998, 1.0000, 0.9998, 0.9999, 0.9996, 0.9995, 0.9991],
       device='cuda:0') torch.Size([16])
Epoch: 187 | Batch_idx: 0 |  Loss: (0.1029) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 187 | Batch_idx: 10 |  Loss: (0.0941) |  Loss2: (0.0000) | Acc: (96.00%) (1363/1408)
Epoch: 187 | Batch_idx: 20 |  Loss: (0.0842) |  Loss2: (0.0000) | Acc: (97.00%) (2613/2688)
Epoch: 187 | Batch_idx: 30 |  Loss: (0.0878) |  Loss2: (0.0000) | Acc: (96.00%) (3848/3968)
Epoch: 187 | Batch_idx: 40 |  Loss: (0.0889) |  Loss2: (0.0000) | Acc: (96.00%) (5090/5248)
Epoch: 187 | Batch_idx: 50 |  Loss: (0.0892) |  Loss2: (0.0000) | Acc: (96.00%) (6325/6528)
Epoch: 187 | Batch_idx: 60 |  Loss: (0.0920) |  Loss2: (0.0000) | Acc: (96.00%) (7561/7808)
Epoch: 187 | Batch_idx: 70 |  Loss: (0.0913) |  Loss2: (0.0000) | Acc: (96.00%) (8801/9088)
Epoch: 187 | Batch_idx: 80 |  Loss: (0.0886) |  Loss2: (0.0000) | Acc: (96.00%) (10054/10368)
Epoch: 187 | Batch_idx: 90 |  Loss: (0.0893) |  Loss2: (0.0000) | Acc: (96.00%) (11295/11648)
Epoch: 187 | Batch_idx: 100 |  Loss: (0.0886) |  Loss2: (0.0000) | Acc: (96.00%) (12531/12928)
Epoch: 187 | Batch_idx: 110 |  Loss: (0.0892) |  Loss2: (0.0000) | Acc: (96.00%) (13773/14208)
Epoch: 187 | Batch_idx: 120 |  Loss: (0.0889) |  Loss2: (0.0000) | Acc: (96.00%) (15015/15488)
Epoch: 187 | Batch_idx: 130 |  Loss: (0.0890) |  Loss2: (0.0000) | Acc: (96.00%) (16256/16768)
Epoch: 187 | Batch_idx: 140 |  Loss: (0.0900) |  Loss2: (0.0000) | Acc: (96.00%) (17483/18048)
Epoch: 187 | Batch_idx: 150 |  Loss: (0.0900) |  Loss2: (0.0000) | Acc: (96.00%) (18720/19328)
Epoch: 187 | Batch_idx: 160 |  Loss: (0.0886) |  Loss2: (0.0000) | Acc: (96.00%) (19975/20608)
Epoch: 187 | Batch_idx: 170 |  Loss: (0.0874) |  Loss2: (0.0000) | Acc: (96.00%) (21226/21888)
Epoch: 187 | Batch_idx: 180 |  Loss: (0.0870) |  Loss2: (0.0000) | Acc: (96.00%) (22469/23168)
Epoch: 187 | Batch_idx: 190 |  Loss: (0.0871) |  Loss2: (0.0000) | Acc: (96.00%) (23709/24448)
Epoch: 187 | Batch_idx: 200 |  Loss: (0.0871) |  Loss2: (0.0000) | Acc: (96.00%) (24953/25728)
Epoch: 187 | Batch_idx: 210 |  Loss: (0.0873) |  Loss2: (0.0000) | Acc: (96.00%) (26186/27008)
Epoch: 187 | Batch_idx: 220 |  Loss: (0.0873) |  Loss2: (0.0000) | Acc: (96.00%) (27428/28288)
Epoch: 187 | Batch_idx: 230 |  Loss: (0.0875) |  Loss2: (0.0000) | Acc: (96.00%) (28665/29568)
Epoch: 187 | Batch_idx: 240 |  Loss: (0.0872) |  Loss2: (0.0000) | Acc: (96.00%) (29903/30848)
Epoch: 187 | Batch_idx: 250 |  Loss: (0.0874) |  Loss2: (0.0000) | Acc: (96.00%) (31147/32128)
Epoch: 187 | Batch_idx: 260 |  Loss: (0.0868) |  Loss2: (0.0000) | Acc: (96.00%) (32398/33408)
Epoch: 187 | Batch_idx: 270 |  Loss: (0.0869) |  Loss2: (0.0000) | Acc: (96.00%) (33638/34688)
Epoch: 187 | Batch_idx: 280 |  Loss: (0.0868) |  Loss2: (0.0000) | Acc: (96.00%) (34883/35968)
Epoch: 187 | Batch_idx: 290 |  Loss: (0.0867) |  Loss2: (0.0000) | Acc: (96.00%) (36120/37248)
Epoch: 187 | Batch_idx: 300 |  Loss: (0.0864) |  Loss2: (0.0000) | Acc: (96.00%) (37368/38528)
Epoch: 187 | Batch_idx: 310 |  Loss: (0.0862) |  Loss2: (0.0000) | Acc: (97.00%) (38617/39808)
Epoch: 187 | Batch_idx: 320 |  Loss: (0.0854) |  Loss2: (0.0000) | Acc: (97.00%) (39871/41088)
Epoch: 187 | Batch_idx: 330 |  Loss: (0.0849) |  Loss2: (0.0000) | Acc: (97.00%) (41129/42368)
Epoch: 187 | Batch_idx: 340 |  Loss: (0.0849) |  Loss2: (0.0000) | Acc: (97.00%) (42374/43648)
Epoch: 187 | Batch_idx: 350 |  Loss: (0.0849) |  Loss2: (0.0000) | Acc: (97.00%) (43613/44928)
Epoch: 187 | Batch_idx: 360 |  Loss: (0.0848) |  Loss2: (0.0000) | Acc: (97.00%) (44857/46208)
Epoch: 187 | Batch_idx: 370 |  Loss: (0.0847) |  Loss2: (0.0000) | Acc: (97.00%) (46106/47488)
Epoch: 187 | Batch_idx: 380 |  Loss: (0.0846) |  Loss2: (0.0000) | Acc: (97.00%) (47354/48768)
Epoch: 187 | Batch_idx: 390 |  Loss: (0.0845) |  Loss2: (0.0000) | Acc: (97.00%) (48549/50000)
# TEST : Loss: (0.3900) | Acc: (88.00%) (8899/10000)
percent tensor([0.5575, 0.5698, 0.5576, 0.5270, 0.5703, 0.5811, 0.5787, 0.5425, 0.5565,
        0.5625, 0.5719, 0.5662, 0.5549, 0.5346, 0.5753, 0.5505],
       device='cuda:0') torch.Size([16])
percent tensor([0.5395, 0.5427, 0.5479, 0.5516, 0.5498, 0.5384, 0.5449, 0.5540, 0.5444,
        0.5433, 0.5392, 0.5454, 0.5402, 0.5453, 0.5408, 0.5427],
       device='cuda:0') torch.Size([16])
percent tensor([0.6885, 0.7121, 0.5002, 0.4453, 0.4420, 0.6351, 0.6484, 0.4873, 0.5596,
        0.6790, 0.6932, 0.5859, 0.7129, 0.6577, 0.6798, 0.6634],
       device='cuda:0') torch.Size([16])
percent tensor([0.6970, 0.5953, 0.6853, 0.6827, 0.6940, 0.7717, 0.6454, 0.7056, 0.6513,
        0.6201, 0.6374, 0.6264, 0.5691, 0.6720, 0.6853, 0.7466],
       device='cuda:0') torch.Size([16])
percent tensor([0.6526, 0.6407, 0.6642, 0.6229, 0.6353, 0.6195, 0.6668, 0.6576, 0.6615,
        0.6287, 0.6144, 0.6176, 0.6271, 0.6457, 0.6326, 0.6255],
       device='cuda:0') torch.Size([16])
percent tensor([0.5660, 0.7211, 0.6712, 0.7527, 0.7126, 0.7663, 0.7110, 0.5353, 0.7775,
        0.6476, 0.7778, 0.7915, 0.7368, 0.7902, 0.6146, 0.5604],
       device='cuda:0') torch.Size([16])
percent tensor([0.6058, 0.6681, 0.5538, 0.4668, 0.6177, 0.6940, 0.6139, 0.3954, 0.6542,
        0.6921, 0.6989, 0.4188, 0.7032, 0.5993, 0.3871, 0.5279],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9997, 0.9999, 0.9998, 0.9999, 0.9997, 0.9999, 0.9997, 0.9998,
        0.9998, 1.0000, 0.9998, 0.9999, 0.9997, 0.9995, 0.9992],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 188 | Batch_idx: 0 |  Loss: (0.0839) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 188 | Batch_idx: 10 |  Loss: (0.0830) |  Loss2: (0.0000) | Acc: (97.00%) (1366/1408)
Epoch: 188 | Batch_idx: 20 |  Loss: (0.0794) |  Loss2: (0.0000) | Acc: (97.00%) (2622/2688)
Epoch: 188 | Batch_idx: 30 |  Loss: (0.0775) |  Loss2: (0.0000) | Acc: (97.00%) (3874/3968)
Epoch: 188 | Batch_idx: 40 |  Loss: (0.0826) |  Loss2: (0.0000) | Acc: (97.00%) (5114/5248)
Epoch: 188 | Batch_idx: 50 |  Loss: (0.0832) |  Loss2: (0.0000) | Acc: (97.00%) (6353/6528)
Epoch: 188 | Batch_idx: 60 |  Loss: (0.0812) |  Loss2: (0.0000) | Acc: (97.00%) (7601/7808)
Epoch: 188 | Batch_idx: 70 |  Loss: (0.0819) |  Loss2: (0.0000) | Acc: (97.00%) (8849/9088)
Epoch: 188 | Batch_idx: 80 |  Loss: (0.0816) |  Loss2: (0.0000) | Acc: (97.00%) (10085/10368)
Epoch: 188 | Batch_idx: 90 |  Loss: (0.0815) |  Loss2: (0.0000) | Acc: (97.00%) (11333/11648)
Epoch: 188 | Batch_idx: 100 |  Loss: (0.0806) |  Loss2: (0.0000) | Acc: (97.00%) (12581/12928)
Epoch: 188 | Batch_idx: 110 |  Loss: (0.0804) |  Loss2: (0.0000) | Acc: (97.00%) (13824/14208)
Epoch: 188 | Batch_idx: 120 |  Loss: (0.0807) |  Loss2: (0.0000) | Acc: (97.00%) (15063/15488)
Epoch: 188 | Batch_idx: 130 |  Loss: (0.0815) |  Loss2: (0.0000) | Acc: (97.00%) (16306/16768)
Epoch: 188 | Batch_idx: 140 |  Loss: (0.0816) |  Loss2: (0.0000) | Acc: (97.00%) (17557/18048)
Epoch: 188 | Batch_idx: 150 |  Loss: (0.0806) |  Loss2: (0.0000) | Acc: (97.00%) (18808/19328)
Epoch: 188 | Batch_idx: 160 |  Loss: (0.0812) |  Loss2: (0.0000) | Acc: (97.00%) (20048/20608)
Epoch: 188 | Batch_idx: 170 |  Loss: (0.0807) |  Loss2: (0.0000) | Acc: (97.00%) (21296/21888)
Epoch: 188 | Batch_idx: 180 |  Loss: (0.0810) |  Loss2: (0.0000) | Acc: (97.00%) (22534/23168)
Epoch: 188 | Batch_idx: 190 |  Loss: (0.0809) |  Loss2: (0.0000) | Acc: (97.00%) (23782/24448)
Epoch: 188 | Batch_idx: 200 |  Loss: (0.0821) |  Loss2: (0.0000) | Acc: (97.00%) (25013/25728)
Epoch: 188 | Batch_idx: 210 |  Loss: (0.0821) |  Loss2: (0.0000) | Acc: (97.00%) (26254/27008)
Epoch: 188 | Batch_idx: 220 |  Loss: (0.0828) |  Loss2: (0.0000) | Acc: (97.00%) (27485/28288)
Epoch: 188 | Batch_idx: 230 |  Loss: (0.0830) |  Loss2: (0.0000) | Acc: (97.00%) (28720/29568)
Epoch: 188 | Batch_idx: 240 |  Loss: (0.0829) |  Loss2: (0.0000) | Acc: (97.00%) (29965/30848)
Epoch: 188 | Batch_idx: 250 |  Loss: (0.0831) |  Loss2: (0.0000) | Acc: (97.00%) (31205/32128)
Epoch: 188 | Batch_idx: 260 |  Loss: (0.0838) |  Loss2: (0.0000) | Acc: (97.00%) (32436/33408)
Epoch: 188 | Batch_idx: 270 |  Loss: (0.0842) |  Loss2: (0.0000) | Acc: (97.00%) (33674/34688)
Epoch: 188 | Batch_idx: 280 |  Loss: (0.0846) |  Loss2: (0.0000) | Acc: (97.00%) (34914/35968)
Epoch: 188 | Batch_idx: 290 |  Loss: (0.0843) |  Loss2: (0.0000) | Acc: (97.00%) (36157/37248)
Epoch: 188 | Batch_idx: 300 |  Loss: (0.0844) |  Loss2: (0.0000) | Acc: (97.00%) (37399/38528)
Epoch: 188 | Batch_idx: 310 |  Loss: (0.0842) |  Loss2: (0.0000) | Acc: (97.00%) (38649/39808)
Epoch: 188 | Batch_idx: 320 |  Loss: (0.0844) |  Loss2: (0.0000) | Acc: (97.00%) (39884/41088)
Epoch: 188 | Batch_idx: 330 |  Loss: (0.0847) |  Loss2: (0.0000) | Acc: (97.00%) (41119/42368)
Epoch: 188 | Batch_idx: 340 |  Loss: (0.0848) |  Loss2: (0.0000) | Acc: (97.00%) (42359/43648)
Epoch: 188 | Batch_idx: 350 |  Loss: (0.0846) |  Loss2: (0.0000) | Acc: (97.00%) (43608/44928)
Epoch: 188 | Batch_idx: 360 |  Loss: (0.0846) |  Loss2: (0.0000) | Acc: (97.00%) (44846/46208)
Epoch: 188 | Batch_idx: 370 |  Loss: (0.0845) |  Loss2: (0.0000) | Acc: (97.00%) (46085/47488)
Epoch: 188 | Batch_idx: 380 |  Loss: (0.0845) |  Loss2: (0.0000) | Acc: (97.00%) (47331/48768)
Epoch: 188 | Batch_idx: 390 |  Loss: (0.0846) |  Loss2: (0.0000) | Acc: (97.00%) (48526/50000)
# TEST : Loss: (0.4156) | Acc: (88.00%) (8845/10000)
percent tensor([0.5584, 0.5736, 0.5591, 0.5299, 0.5697, 0.5813, 0.5806, 0.5458, 0.5585,
        0.5654, 0.5733, 0.5680, 0.5559, 0.5382, 0.5770, 0.5518],
       device='cuda:0') torch.Size([16])
percent tensor([0.5397, 0.5431, 0.5492, 0.5508, 0.5520, 0.5382, 0.5468, 0.5532, 0.5442,
        0.5439, 0.5391, 0.5464, 0.5401, 0.5463, 0.5405, 0.5427],
       device='cuda:0') torch.Size([16])
percent tensor([0.6885, 0.6997, 0.4906, 0.4499, 0.4335, 0.6272, 0.6394, 0.4908, 0.5792,
        0.6654, 0.6930, 0.5710, 0.7122, 0.6586, 0.6708, 0.6643],
       device='cuda:0') torch.Size([16])
percent tensor([0.7021, 0.5905, 0.6930, 0.6815, 0.6924, 0.7754, 0.6530, 0.7012, 0.6550,
        0.6147, 0.6371, 0.6260, 0.5730, 0.6696, 0.6779, 0.7465],
       device='cuda:0') torch.Size([16])
percent tensor([0.6467, 0.6374, 0.6628, 0.6174, 0.6397, 0.6262, 0.6575, 0.6587, 0.6551,
        0.6320, 0.6094, 0.6098, 0.6264, 0.6475, 0.6334, 0.6179],
       device='cuda:0') torch.Size([16])
percent tensor([0.5873, 0.7285, 0.6880, 0.7450, 0.7304, 0.7811, 0.7416, 0.5819, 0.7701,
        0.6517, 0.7809, 0.8086, 0.7373, 0.8132, 0.6328, 0.5716],
       device='cuda:0') torch.Size([16])
percent tensor([0.5924, 0.6480, 0.5382, 0.4357, 0.6125, 0.6887, 0.6112, 0.3772, 0.6628,
        0.6604, 0.6969, 0.4438, 0.6993, 0.5827, 0.3694, 0.5147],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9997, 0.9999, 0.9998, 0.9999, 0.9997, 0.9999, 0.9996, 0.9999,
        0.9998, 0.9999, 0.9999, 0.9999, 0.9996, 0.9995, 0.9995],
       device='cuda:0') torch.Size([16])
Epoch: 189 | Batch_idx: 0 |  Loss: (0.1003) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 189 | Batch_idx: 10 |  Loss: (0.0701) |  Loss2: (0.0000) | Acc: (97.00%) (1371/1408)
Epoch: 189 | Batch_idx: 20 |  Loss: (0.0792) |  Loss2: (0.0000) | Acc: (97.00%) (2611/2688)
Epoch: 189 | Batch_idx: 30 |  Loss: (0.0776) |  Loss2: (0.0000) | Acc: (97.00%) (3864/3968)
Epoch: 189 | Batch_idx: 40 |  Loss: (0.0735) |  Loss2: (0.0000) | Acc: (97.00%) (5124/5248)
Epoch: 189 | Batch_idx: 50 |  Loss: (0.0758) |  Loss2: (0.0000) | Acc: (97.00%) (6365/6528)
Epoch: 189 | Batch_idx: 60 |  Loss: (0.0775) |  Loss2: (0.0000) | Acc: (97.00%) (7605/7808)
Epoch: 189 | Batch_idx: 70 |  Loss: (0.0774) |  Loss2: (0.0000) | Acc: (97.00%) (8848/9088)
Epoch: 189 | Batch_idx: 80 |  Loss: (0.0760) |  Loss2: (0.0000) | Acc: (97.00%) (10103/10368)
Epoch: 189 | Batch_idx: 90 |  Loss: (0.0759) |  Loss2: (0.0000) | Acc: (97.00%) (11352/11648)
Epoch: 189 | Batch_idx: 100 |  Loss: (0.0771) |  Loss2: (0.0000) | Acc: (97.00%) (12589/12928)
Epoch: 189 | Batch_idx: 110 |  Loss: (0.0778) |  Loss2: (0.0000) | Acc: (97.00%) (13831/14208)
Epoch: 189 | Batch_idx: 120 |  Loss: (0.0781) |  Loss2: (0.0000) | Acc: (97.00%) (15082/15488)
Epoch: 189 | Batch_idx: 130 |  Loss: (0.0788) |  Loss2: (0.0000) | Acc: (97.00%) (16323/16768)
Epoch: 189 | Batch_idx: 140 |  Loss: (0.0785) |  Loss2: (0.0000) | Acc: (97.00%) (17571/18048)
Epoch: 189 | Batch_idx: 150 |  Loss: (0.0794) |  Loss2: (0.0000) | Acc: (97.00%) (18805/19328)
Epoch: 189 | Batch_idx: 160 |  Loss: (0.0789) |  Loss2: (0.0000) | Acc: (97.00%) (20052/20608)
Epoch: 189 | Batch_idx: 170 |  Loss: (0.0793) |  Loss2: (0.0000) | Acc: (97.00%) (21296/21888)
Epoch: 189 | Batch_idx: 180 |  Loss: (0.0796) |  Loss2: (0.0000) | Acc: (97.00%) (22538/23168)
Epoch: 189 | Batch_idx: 190 |  Loss: (0.0799) |  Loss2: (0.0000) | Acc: (97.00%) (23776/24448)
Epoch: 189 | Batch_idx: 200 |  Loss: (0.0800) |  Loss2: (0.0000) | Acc: (97.00%) (25014/25728)
Epoch: 189 | Batch_idx: 210 |  Loss: (0.0797) |  Loss2: (0.0000) | Acc: (97.00%) (26268/27008)
Epoch: 189 | Batch_idx: 220 |  Loss: (0.0797) |  Loss2: (0.0000) | Acc: (97.00%) (27512/28288)
Epoch: 189 | Batch_idx: 230 |  Loss: (0.0796) |  Loss2: (0.0000) | Acc: (97.00%) (28761/29568)
Epoch: 189 | Batch_idx: 240 |  Loss: (0.0803) |  Loss2: (0.0000) | Acc: (97.00%) (29996/30848)
Epoch: 189 | Batch_idx: 250 |  Loss: (0.0806) |  Loss2: (0.0000) | Acc: (97.00%) (31239/32128)
Epoch: 189 | Batch_idx: 260 |  Loss: (0.0799) |  Loss2: (0.0000) | Acc: (97.00%) (32492/33408)
Epoch: 189 | Batch_idx: 270 |  Loss: (0.0800) |  Loss2: (0.0000) | Acc: (97.00%) (33737/34688)
Epoch: 189 | Batch_idx: 280 |  Loss: (0.0801) |  Loss2: (0.0000) | Acc: (97.00%) (34979/35968)
Epoch: 189 | Batch_idx: 290 |  Loss: (0.0803) |  Loss2: (0.0000) | Acc: (97.00%) (36213/37248)
Epoch: 189 | Batch_idx: 300 |  Loss: (0.0805) |  Loss2: (0.0000) | Acc: (97.00%) (37456/38528)
Epoch: 189 | Batch_idx: 310 |  Loss: (0.0807) |  Loss2: (0.0000) | Acc: (97.00%) (38697/39808)
Epoch: 189 | Batch_idx: 320 |  Loss: (0.0805) |  Loss2: (0.0000) | Acc: (97.00%) (39950/41088)
Epoch: 189 | Batch_idx: 330 |  Loss: (0.0811) |  Loss2: (0.0000) | Acc: (97.00%) (41194/42368)
Epoch: 189 | Batch_idx: 340 |  Loss: (0.0810) |  Loss2: (0.0000) | Acc: (97.00%) (42441/43648)
Epoch: 189 | Batch_idx: 350 |  Loss: (0.0811) |  Loss2: (0.0000) | Acc: (97.00%) (43685/44928)
Epoch: 189 | Batch_idx: 360 |  Loss: (0.0813) |  Loss2: (0.0000) | Acc: (97.00%) (44924/46208)
Epoch: 189 | Batch_idx: 370 |  Loss: (0.0816) |  Loss2: (0.0000) | Acc: (97.00%) (46163/47488)
Epoch: 189 | Batch_idx: 380 |  Loss: (0.0814) |  Loss2: (0.0000) | Acc: (97.00%) (47411/48768)
Epoch: 189 | Batch_idx: 390 |  Loss: (0.0813) |  Loss2: (0.0000) | Acc: (97.00%) (48610/50000)
# TEST : Loss: (0.4489) | Acc: (88.00%) (8839/10000)
percent tensor([0.5575, 0.5717, 0.5585, 0.5299, 0.5700, 0.5817, 0.5791, 0.5468, 0.5578,
        0.5645, 0.5725, 0.5678, 0.5564, 0.5352, 0.5766, 0.5512],
       device='cuda:0') torch.Size([16])
percent tensor([0.5400, 0.5429, 0.5487, 0.5517, 0.5505, 0.5386, 0.5455, 0.5529, 0.5448,
        0.5436, 0.5398, 0.5458, 0.5402, 0.5462, 0.5406, 0.5435],
       device='cuda:0') torch.Size([16])
percent tensor([0.6913, 0.7150, 0.4732, 0.4485, 0.4303, 0.6425, 0.6484, 0.4918, 0.5756,
        0.6729, 0.6945, 0.5693, 0.7168, 0.6726, 0.6849, 0.6678],
       device='cuda:0') torch.Size([16])
percent tensor([0.6992, 0.5915, 0.6775, 0.6850, 0.6889, 0.7677, 0.6520, 0.6964, 0.6499,
        0.6195, 0.6374, 0.6198, 0.5769, 0.6738, 0.6795, 0.7473],
       device='cuda:0') torch.Size([16])
percent tensor([0.6465, 0.6437, 0.6678, 0.6111, 0.6405, 0.6183, 0.6643, 0.6621, 0.6607,
        0.6358, 0.6063, 0.6215, 0.6272, 0.6445, 0.6394, 0.6150],
       device='cuda:0') torch.Size([16])
percent tensor([0.5944, 0.7291, 0.7038, 0.7656, 0.7327, 0.7622, 0.7387, 0.5927, 0.7763,
        0.6484, 0.7813, 0.8182, 0.7497, 0.8113, 0.6214, 0.5806],
       device='cuda:0') torch.Size([16])
percent tensor([0.6106, 0.6775, 0.5507, 0.4603, 0.6231, 0.7002, 0.6185, 0.4278, 0.6603,
        0.6805, 0.7128, 0.4458, 0.6969, 0.6249, 0.3813, 0.5250],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9998, 0.9999, 0.9998, 0.9999, 0.9995, 0.9999, 0.9994, 0.9999,
        0.9999, 1.0000, 0.9999, 0.9999, 0.9998, 0.9995, 0.9993],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 190 | Batch_idx: 0 |  Loss: (0.0475) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 190 | Batch_idx: 10 |  Loss: (0.0972) |  Loss2: (0.0000) | Acc: (96.00%) (1360/1408)
Epoch: 190 | Batch_idx: 20 |  Loss: (0.0857) |  Loss2: (0.0000) | Acc: (96.00%) (2603/2688)
Epoch: 190 | Batch_idx: 30 |  Loss: (0.0982) |  Loss2: (0.0000) | Acc: (96.00%) (3827/3968)
Epoch: 190 | Batch_idx: 40 |  Loss: (0.0954) |  Loss2: (0.0000) | Acc: (96.00%) (5069/5248)
Epoch: 190 | Batch_idx: 50 |  Loss: (0.0956) |  Loss2: (0.0000) | Acc: (96.00%) (6304/6528)
Epoch: 190 | Batch_idx: 60 |  Loss: (0.0952) |  Loss2: (0.0000) | Acc: (96.00%) (7542/7808)
Epoch: 190 | Batch_idx: 70 |  Loss: (0.0930) |  Loss2: (0.0000) | Acc: (96.00%) (8786/9088)
Epoch: 190 | Batch_idx: 80 |  Loss: (0.0950) |  Loss2: (0.0000) | Acc: (96.00%) (10020/10368)
Epoch: 190 | Batch_idx: 90 |  Loss: (0.0928) |  Loss2: (0.0000) | Acc: (96.00%) (11269/11648)
Epoch: 190 | Batch_idx: 100 |  Loss: (0.0934) |  Loss2: (0.0000) | Acc: (96.00%) (12506/12928)
Epoch: 190 | Batch_idx: 110 |  Loss: (0.0935) |  Loss2: (0.0000) | Acc: (96.00%) (13744/14208)
Epoch: 190 | Batch_idx: 120 |  Loss: (0.0934) |  Loss2: (0.0000) | Acc: (96.00%) (14980/15488)
Epoch: 190 | Batch_idx: 130 |  Loss: (0.0939) |  Loss2: (0.0000) | Acc: (96.00%) (16219/16768)
Epoch: 190 | Batch_idx: 140 |  Loss: (0.0953) |  Loss2: (0.0000) | Acc: (96.00%) (17446/18048)
Epoch: 190 | Batch_idx: 150 |  Loss: (0.0959) |  Loss2: (0.0000) | Acc: (96.00%) (18689/19328)
Epoch: 190 | Batch_idx: 160 |  Loss: (0.0953) |  Loss2: (0.0000) | Acc: (96.00%) (19935/20608)
Epoch: 190 | Batch_idx: 170 |  Loss: (0.0952) |  Loss2: (0.0000) | Acc: (96.00%) (21170/21888)
Epoch: 190 | Batch_idx: 180 |  Loss: (0.0960) |  Loss2: (0.0000) | Acc: (96.00%) (22394/23168)
Epoch: 190 | Batch_idx: 190 |  Loss: (0.0968) |  Loss2: (0.0000) | Acc: (96.00%) (23623/24448)
Epoch: 190 | Batch_idx: 200 |  Loss: (0.0967) |  Loss2: (0.0000) | Acc: (96.00%) (24857/25728)
Epoch: 190 | Batch_idx: 210 |  Loss: (0.0954) |  Loss2: (0.0000) | Acc: (96.00%) (26104/27008)
Epoch: 190 | Batch_idx: 220 |  Loss: (0.0957) |  Loss2: (0.0000) | Acc: (96.00%) (27344/28288)
Epoch: 190 | Batch_idx: 230 |  Loss: (0.0964) |  Loss2: (0.0000) | Acc: (96.00%) (28568/29568)
Epoch: 190 | Batch_idx: 240 |  Loss: (0.0961) |  Loss2: (0.0000) | Acc: (96.00%) (29803/30848)
Epoch: 190 | Batch_idx: 250 |  Loss: (0.0956) |  Loss2: (0.0000) | Acc: (96.00%) (31039/32128)
Epoch: 190 | Batch_idx: 260 |  Loss: (0.0965) |  Loss2: (0.0000) | Acc: (96.00%) (32270/33408)
Epoch: 190 | Batch_idx: 270 |  Loss: (0.0961) |  Loss2: (0.0000) | Acc: (96.00%) (33515/34688)
Epoch: 190 | Batch_idx: 280 |  Loss: (0.0962) |  Loss2: (0.0000) | Acc: (96.00%) (34748/35968)
Epoch: 190 | Batch_idx: 290 |  Loss: (0.0959) |  Loss2: (0.0000) | Acc: (96.00%) (35986/37248)
Epoch: 190 | Batch_idx: 300 |  Loss: (0.0954) |  Loss2: (0.0000) | Acc: (96.00%) (37231/38528)
Epoch: 190 | Batch_idx: 310 |  Loss: (0.0950) |  Loss2: (0.0000) | Acc: (96.00%) (38473/39808)
Epoch: 190 | Batch_idx: 320 |  Loss: (0.0946) |  Loss2: (0.0000) | Acc: (96.00%) (39715/41088)
Epoch: 190 | Batch_idx: 330 |  Loss: (0.0942) |  Loss2: (0.0000) | Acc: (96.00%) (40952/42368)
Epoch: 190 | Batch_idx: 340 |  Loss: (0.0949) |  Loss2: (0.0000) | Acc: (96.00%) (42179/43648)
Epoch: 190 | Batch_idx: 350 |  Loss: (0.0945) |  Loss2: (0.0000) | Acc: (96.00%) (43426/44928)
Epoch: 190 | Batch_idx: 360 |  Loss: (0.0944) |  Loss2: (0.0000) | Acc: (96.00%) (44669/46208)
Epoch: 190 | Batch_idx: 370 |  Loss: (0.0947) |  Loss2: (0.0000) | Acc: (96.00%) (45898/47488)
Epoch: 190 | Batch_idx: 380 |  Loss: (0.0946) |  Loss2: (0.0000) | Acc: (96.00%) (47131/48768)
Epoch: 190 | Batch_idx: 390 |  Loss: (0.0942) |  Loss2: (0.0000) | Acc: (96.00%) (48328/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_190.pth.tar'
# TEST : Loss: (0.4125) | Acc: (88.00%) (8881/10000)
percent tensor([0.5630, 0.5793, 0.5642, 0.5360, 0.5771, 0.5884, 0.5866, 0.5524, 0.5643,
        0.5709, 0.5790, 0.5742, 0.5620, 0.5428, 0.5839, 0.5574],
       device='cuda:0') torch.Size([16])
percent tensor([0.5318, 0.5343, 0.5393, 0.5419, 0.5408, 0.5310, 0.5366, 0.5434, 0.5358,
        0.5351, 0.5314, 0.5367, 0.5321, 0.5374, 0.5325, 0.5349],
       device='cuda:0') torch.Size([16])
percent tensor([0.6692, 0.7140, 0.4488, 0.4198, 0.3996, 0.6175, 0.6299, 0.4585, 0.5599,
        0.6622, 0.6876, 0.5550, 0.7061, 0.6549, 0.6652, 0.6517],
       device='cuda:0') torch.Size([16])
percent tensor([0.6862, 0.5945, 0.6586, 0.6687, 0.6668, 0.7631, 0.6408, 0.6664, 0.6358,
        0.6186, 0.6336, 0.6087, 0.5775, 0.6628, 0.6672, 0.7449],
       device='cuda:0') torch.Size([16])
percent tensor([0.6377, 0.6322, 0.6645, 0.6143, 0.6416, 0.6102, 0.6612, 0.6572, 0.6583,
        0.6237, 0.6022, 0.6225, 0.6196, 0.6463, 0.6370, 0.5999],
       device='cuda:0') torch.Size([16])
percent tensor([0.5983, 0.7317, 0.7097, 0.7642, 0.7419, 0.7541, 0.7429, 0.6071, 0.7754,
        0.6509, 0.7819, 0.8132, 0.7466, 0.8173, 0.6229, 0.5739],
       device='cuda:0') torch.Size([16])
percent tensor([0.5878, 0.6550, 0.5619, 0.4661, 0.6430, 0.6980, 0.5925, 0.4404, 0.6200,
        0.6517, 0.6910, 0.4445, 0.6678, 0.5953, 0.3666, 0.5034],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9998, 0.9999, 0.9998, 0.9998, 0.9996, 0.9999, 0.9996, 0.9999,
        0.9998, 0.9999, 0.9999, 0.9999, 0.9998, 0.9995, 0.9991],
       device='cuda:0') torch.Size([16])
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(184.2849, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(831.3793, device='cuda:0')
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(827.6912, device='cuda:0')
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1508.4860, device='cuda:0')
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(477.7050, device='cuda:0')
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2298.5742, device='cuda:0')
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4233.0127, device='cuda:0')
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1353.7720, device='cuda:0')
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6268.6362, device='cuda:0')
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11570.0225, device='cuda:0')
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3804.3391, device='cuda:0')
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16051.0762, device='cuda:0')
Epoch: 191 | Batch_idx: 0 |  Loss: (0.0470) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 191 | Batch_idx: 10 |  Loss: (0.0797) |  Loss2: (0.0000) | Acc: (97.00%) (1366/1408)
Epoch: 191 | Batch_idx: 20 |  Loss: (0.0794) |  Loss2: (0.0000) | Acc: (96.00%) (2605/2688)
Epoch: 191 | Batch_idx: 30 |  Loss: (0.0796) |  Loss2: (0.0000) | Acc: (97.00%) (3856/3968)
Epoch: 191 | Batch_idx: 40 |  Loss: (0.0818) |  Loss2: (0.0000) | Acc: (97.00%) (5097/5248)
Epoch: 191 | Batch_idx: 50 |  Loss: (0.0836) |  Loss2: (0.0000) | Acc: (97.00%) (6336/6528)
Epoch: 191 | Batch_idx: 60 |  Loss: (0.0866) |  Loss2: (0.0000) | Acc: (96.00%) (7570/7808)
Epoch: 191 | Batch_idx: 70 |  Loss: (0.0872) |  Loss2: (0.0000) | Acc: (96.00%) (8808/9088)
Epoch: 191 | Batch_idx: 80 |  Loss: (0.0881) |  Loss2: (0.0000) | Acc: (96.00%) (10044/10368)
Epoch: 191 | Batch_idx: 90 |  Loss: (0.0881) |  Loss2: (0.0000) | Acc: (96.00%) (11289/11648)
Epoch: 191 | Batch_idx: 100 |  Loss: (0.0890) |  Loss2: (0.0000) | Acc: (96.00%) (12526/12928)
Epoch: 191 | Batch_idx: 110 |  Loss: (0.0890) |  Loss2: (0.0000) | Acc: (96.00%) (13763/14208)
Epoch: 191 | Batch_idx: 120 |  Loss: (0.0905) |  Loss2: (0.0000) | Acc: (96.00%) (14995/15488)
Epoch: 191 | Batch_idx: 130 |  Loss: (0.0897) |  Loss2: (0.0000) | Acc: (96.00%) (16235/16768)
Epoch: 191 | Batch_idx: 140 |  Loss: (0.0896) |  Loss2: (0.0000) | Acc: (96.00%) (17471/18048)
Epoch: 191 | Batch_idx: 150 |  Loss: (0.0894) |  Loss2: (0.0000) | Acc: (96.00%) (18710/19328)
Epoch: 191 | Batch_idx: 160 |  Loss: (0.0891) |  Loss2: (0.0000) | Acc: (96.00%) (19947/20608)
Epoch: 191 | Batch_idx: 170 |  Loss: (0.0896) |  Loss2: (0.0000) | Acc: (96.00%) (21184/21888)
Epoch: 191 | Batch_idx: 180 |  Loss: (0.0885) |  Loss2: (0.0000) | Acc: (96.00%) (22435/23168)
Epoch: 191 | Batch_idx: 190 |  Loss: (0.0879) |  Loss2: (0.0000) | Acc: (96.00%) (23684/24448)
Epoch: 191 | Batch_idx: 200 |  Loss: (0.0875) |  Loss2: (0.0000) | Acc: (96.00%) (24932/25728)
Epoch: 191 | Batch_idx: 210 |  Loss: (0.0866) |  Loss2: (0.0000) | Acc: (96.00%) (26187/27008)
Epoch: 191 | Batch_idx: 220 |  Loss: (0.0860) |  Loss2: (0.0000) | Acc: (96.00%) (27436/28288)
Epoch: 191 | Batch_idx: 230 |  Loss: (0.0853) |  Loss2: (0.0000) | Acc: (97.00%) (28687/29568)
Epoch: 191 | Batch_idx: 240 |  Loss: (0.0852) |  Loss2: (0.0000) | Acc: (97.00%) (29936/30848)
Epoch: 191 | Batch_idx: 250 |  Loss: (0.0851) |  Loss2: (0.0000) | Acc: (97.00%) (31179/32128)
Epoch: 191 | Batch_idx: 260 |  Loss: (0.0847) |  Loss2: (0.0000) | Acc: (97.00%) (32428/33408)
Epoch: 191 | Batch_idx: 270 |  Loss: (0.0847) |  Loss2: (0.0000) | Acc: (97.00%) (33671/34688)
Epoch: 191 | Batch_idx: 280 |  Loss: (0.0845) |  Loss2: (0.0000) | Acc: (97.00%) (34912/35968)
Epoch: 191 | Batch_idx: 290 |  Loss: (0.0842) |  Loss2: (0.0000) | Acc: (97.00%) (36162/37248)
Epoch: 191 | Batch_idx: 300 |  Loss: (0.0839) |  Loss2: (0.0000) | Acc: (97.00%) (37406/38528)
Epoch: 191 | Batch_idx: 310 |  Loss: (0.0842) |  Loss2: (0.0000) | Acc: (97.00%) (38640/39808)
Epoch: 191 | Batch_idx: 320 |  Loss: (0.0837) |  Loss2: (0.0000) | Acc: (97.00%) (39891/41088)
Epoch: 191 | Batch_idx: 330 |  Loss: (0.0837) |  Loss2: (0.0000) | Acc: (97.00%) (41131/42368)
Epoch: 191 | Batch_idx: 340 |  Loss: (0.0840) |  Loss2: (0.0000) | Acc: (97.00%) (42372/43648)
Epoch: 191 | Batch_idx: 350 |  Loss: (0.0837) |  Loss2: (0.0000) | Acc: (97.00%) (43621/44928)
Epoch: 191 | Batch_idx: 360 |  Loss: (0.0844) |  Loss2: (0.0000) | Acc: (97.00%) (44847/46208)
Epoch: 191 | Batch_idx: 370 |  Loss: (0.0840) |  Loss2: (0.0000) | Acc: (97.00%) (46096/47488)
Epoch: 191 | Batch_idx: 380 |  Loss: (0.0837) |  Loss2: (0.0000) | Acc: (97.00%) (47344/48768)
Epoch: 191 | Batch_idx: 390 |  Loss: (0.0835) |  Loss2: (0.0000) | Acc: (97.00%) (48541/50000)
# TEST : Loss: (0.3938) | Acc: (89.00%) (8940/10000)
percent tensor([0.5632, 0.5800, 0.5644, 0.5362, 0.5773, 0.5883, 0.5873, 0.5530, 0.5650,
        0.5716, 0.5795, 0.5749, 0.5625, 0.5438, 0.5841, 0.5578],
       device='cuda:0') torch.Size([16])
percent tensor([0.5294, 0.5317, 0.5368, 0.5392, 0.5382, 0.5286, 0.5340, 0.5407, 0.5333,
        0.5324, 0.5289, 0.5341, 0.5296, 0.5349, 0.5301, 0.5323],
       device='cuda:0') torch.Size([16])
percent tensor([0.6664, 0.7195, 0.4481, 0.4228, 0.3955, 0.6142, 0.6292, 0.4557, 0.5625,
        0.6716, 0.6940, 0.5625, 0.7132, 0.6566, 0.6642, 0.6540],
       device='cuda:0') torch.Size([16])
percent tensor([0.6853, 0.5971, 0.6575, 0.6684, 0.6626, 0.7645, 0.6402, 0.6625, 0.6375,
        0.6203, 0.6366, 0.6110, 0.5820, 0.6642, 0.6672, 0.7469],
       device='cuda:0') torch.Size([16])
percent tensor([0.6439, 0.6368, 0.6730, 0.6254, 0.6505, 0.6125, 0.6677, 0.6663, 0.6657,
        0.6286, 0.6089, 0.6307, 0.6253, 0.6522, 0.6461, 0.6003],
       device='cuda:0') torch.Size([16])
percent tensor([0.5887, 0.7276, 0.7067, 0.7626, 0.7412, 0.7449, 0.7404, 0.6055, 0.7733,
        0.6443, 0.7800, 0.8101, 0.7419, 0.8158, 0.6123, 0.5616],
       device='cuda:0') torch.Size([16])
percent tensor([0.5846, 0.6547, 0.5673, 0.4850, 0.6567, 0.7042, 0.5949, 0.4478, 0.6123,
        0.6427, 0.6905, 0.4463, 0.6641, 0.5895, 0.3652, 0.5033],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9997, 0.9999, 0.9998, 0.9998, 0.9995, 0.9999, 0.9996, 0.9999,
        0.9998, 0.9999, 0.9999, 0.9999, 0.9997, 0.9995, 0.9991],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 192 | Batch_idx: 0 |  Loss: (0.0530) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 192 | Batch_idx: 10 |  Loss: (0.0619) |  Loss2: (0.0000) | Acc: (98.00%) (1384/1408)
Epoch: 192 | Batch_idx: 20 |  Loss: (0.0639) |  Loss2: (0.0000) | Acc: (98.00%) (2637/2688)
Epoch: 192 | Batch_idx: 30 |  Loss: (0.0657) |  Loss2: (0.0000) | Acc: (98.00%) (3891/3968)
Epoch: 192 | Batch_idx: 40 |  Loss: (0.0684) |  Loss2: (0.0000) | Acc: (97.00%) (5131/5248)
Epoch: 192 | Batch_idx: 50 |  Loss: (0.0714) |  Loss2: (0.0000) | Acc: (97.00%) (6370/6528)
Epoch: 192 | Batch_idx: 60 |  Loss: (0.0710) |  Loss2: (0.0000) | Acc: (97.00%) (7622/7808)
Epoch: 192 | Batch_idx: 70 |  Loss: (0.0716) |  Loss2: (0.0000) | Acc: (97.00%) (8872/9088)
Epoch: 192 | Batch_idx: 80 |  Loss: (0.0734) |  Loss2: (0.0000) | Acc: (97.00%) (10112/10368)
Epoch: 192 | Batch_idx: 90 |  Loss: (0.0741) |  Loss2: (0.0000) | Acc: (97.00%) (11353/11648)
Epoch: 192 | Batch_idx: 100 |  Loss: (0.0762) |  Loss2: (0.0000) | Acc: (97.00%) (12591/12928)
Epoch: 192 | Batch_idx: 110 |  Loss: (0.0762) |  Loss2: (0.0000) | Acc: (97.00%) (13836/14208)
Epoch: 192 | Batch_idx: 120 |  Loss: (0.0771) |  Loss2: (0.0000) | Acc: (97.00%) (15075/15488)
Epoch: 192 | Batch_idx: 130 |  Loss: (0.0768) |  Loss2: (0.0000) | Acc: (97.00%) (16326/16768)
Epoch: 192 | Batch_idx: 140 |  Loss: (0.0761) |  Loss2: (0.0000) | Acc: (97.00%) (17583/18048)
Epoch: 192 | Batch_idx: 150 |  Loss: (0.0766) |  Loss2: (0.0000) | Acc: (97.00%) (18828/19328)
Epoch: 192 | Batch_idx: 160 |  Loss: (0.0766) |  Loss2: (0.0000) | Acc: (97.00%) (20072/20608)
Epoch: 192 | Batch_idx: 170 |  Loss: (0.0765) |  Loss2: (0.0000) | Acc: (97.00%) (21318/21888)
Epoch: 192 | Batch_idx: 180 |  Loss: (0.0767) |  Loss2: (0.0000) | Acc: (97.00%) (22565/23168)
Epoch: 192 | Batch_idx: 190 |  Loss: (0.0763) |  Loss2: (0.0000) | Acc: (97.00%) (23818/24448)
Epoch: 192 | Batch_idx: 200 |  Loss: (0.0763) |  Loss2: (0.0000) | Acc: (97.00%) (25061/25728)
Epoch: 192 | Batch_idx: 210 |  Loss: (0.0760) |  Loss2: (0.0000) | Acc: (97.00%) (26312/27008)
Epoch: 192 | Batch_idx: 220 |  Loss: (0.0760) |  Loss2: (0.0000) | Acc: (97.00%) (27557/28288)
Epoch: 192 | Batch_idx: 230 |  Loss: (0.0766) |  Loss2: (0.0000) | Acc: (97.00%) (28799/29568)
Epoch: 192 | Batch_idx: 240 |  Loss: (0.0770) |  Loss2: (0.0000) | Acc: (97.00%) (30035/30848)
Epoch: 192 | Batch_idx: 250 |  Loss: (0.0773) |  Loss2: (0.0000) | Acc: (97.00%) (31274/32128)
Epoch: 192 | Batch_idx: 260 |  Loss: (0.0777) |  Loss2: (0.0000) | Acc: (97.00%) (32514/33408)
Epoch: 192 | Batch_idx: 270 |  Loss: (0.0779) |  Loss2: (0.0000) | Acc: (97.00%) (33760/34688)
Epoch: 192 | Batch_idx: 280 |  Loss: (0.0778) |  Loss2: (0.0000) | Acc: (97.00%) (35009/35968)
Epoch: 192 | Batch_idx: 290 |  Loss: (0.0776) |  Loss2: (0.0000) | Acc: (97.00%) (36259/37248)
Epoch: 192 | Batch_idx: 300 |  Loss: (0.0781) |  Loss2: (0.0000) | Acc: (97.00%) (37506/38528)
Epoch: 192 | Batch_idx: 310 |  Loss: (0.0785) |  Loss2: (0.0000) | Acc: (97.00%) (38751/39808)
Epoch: 192 | Batch_idx: 320 |  Loss: (0.0785) |  Loss2: (0.0000) | Acc: (97.00%) (39998/41088)
Epoch: 192 | Batch_idx: 330 |  Loss: (0.0790) |  Loss2: (0.0000) | Acc: (97.00%) (41230/42368)
Epoch: 192 | Batch_idx: 340 |  Loss: (0.0793) |  Loss2: (0.0000) | Acc: (97.00%) (42468/43648)
Epoch: 192 | Batch_idx: 350 |  Loss: (0.0792) |  Loss2: (0.0000) | Acc: (97.00%) (43715/44928)
Epoch: 192 | Batch_idx: 360 |  Loss: (0.0789) |  Loss2: (0.0000) | Acc: (97.00%) (44968/46208)
Epoch: 192 | Batch_idx: 370 |  Loss: (0.0794) |  Loss2: (0.0000) | Acc: (97.00%) (46205/47488)
Epoch: 192 | Batch_idx: 380 |  Loss: (0.0799) |  Loss2: (0.0000) | Acc: (97.00%) (47440/48768)
Epoch: 192 | Batch_idx: 390 |  Loss: (0.0801) |  Loss2: (0.0000) | Acc: (97.00%) (48630/50000)
# TEST : Loss: (0.4432) | Acc: (87.00%) (8779/10000)
percent tensor([0.5641, 0.5801, 0.5671, 0.5377, 0.5790, 0.5886, 0.5880, 0.5535, 0.5642,
        0.5723, 0.5790, 0.5768, 0.5630, 0.5421, 0.5845, 0.5582],
       device='cuda:0') torch.Size([16])
percent tensor([0.5286, 0.5307, 0.5352, 0.5373, 0.5368, 0.5271, 0.5332, 0.5395, 0.5327,
        0.5313, 0.5285, 0.5328, 0.5286, 0.5341, 0.5289, 0.5313],
       device='cuda:0') torch.Size([16])
percent tensor([0.6760, 0.7182, 0.4796, 0.4449, 0.4253, 0.6167, 0.6379, 0.4816, 0.5605,
        0.6845, 0.6912, 0.5838, 0.7223, 0.6518, 0.6715, 0.6638],
       device='cuda:0') torch.Size([16])
percent tensor([0.6841, 0.5825, 0.6735, 0.6627, 0.6673, 0.7677, 0.6330, 0.6712, 0.6387,
        0.6114, 0.6300, 0.6094, 0.5693, 0.6531, 0.6668, 0.7423],
       device='cuda:0') torch.Size([16])
percent tensor([0.6401, 0.6346, 0.6606, 0.6344, 0.6394, 0.6161, 0.6588, 0.6519, 0.6579,
        0.6236, 0.6070, 0.6252, 0.6236, 0.6500, 0.6369, 0.6120],
       device='cuda:0') torch.Size([16])
percent tensor([0.5565, 0.7271, 0.6678, 0.7345, 0.7148, 0.7621, 0.7225, 0.5672, 0.7645,
        0.6494, 0.7771, 0.7989, 0.7260, 0.8108, 0.6256, 0.5375],
       device='cuda:0') torch.Size([16])
percent tensor([0.5978, 0.7065, 0.5792, 0.4826, 0.6347, 0.7294, 0.6213, 0.4320, 0.6581,
        0.6688, 0.7191, 0.4602, 0.6826, 0.6168, 0.3986, 0.5050],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9996, 0.9999, 0.9998, 0.9999, 0.9997, 0.9999, 0.9997, 0.9999,
        0.9998, 1.0000, 0.9998, 0.9999, 0.9996, 0.9994, 0.9995],
       device='cuda:0') torch.Size([16])
Epoch: 193 | Batch_idx: 0 |  Loss: (0.1436) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 193 | Batch_idx: 10 |  Loss: (0.0841) |  Loss2: (0.0000) | Acc: (97.00%) (1366/1408)
Epoch: 193 | Batch_idx: 20 |  Loss: (0.0855) |  Loss2: (0.0000) | Acc: (97.00%) (2609/2688)
Epoch: 193 | Batch_idx: 30 |  Loss: (0.0835) |  Loss2: (0.0000) | Acc: (97.00%) (3857/3968)
Epoch: 193 | Batch_idx: 40 |  Loss: (0.0805) |  Loss2: (0.0000) | Acc: (97.00%) (5103/5248)
Epoch: 193 | Batch_idx: 50 |  Loss: (0.0782) |  Loss2: (0.0000) | Acc: (97.00%) (6352/6528)
Epoch: 193 | Batch_idx: 60 |  Loss: (0.0779) |  Loss2: (0.0000) | Acc: (97.00%) (7593/7808)
Epoch: 193 | Batch_idx: 70 |  Loss: (0.0792) |  Loss2: (0.0000) | Acc: (97.00%) (8831/9088)
Epoch: 193 | Batch_idx: 80 |  Loss: (0.0813) |  Loss2: (0.0000) | Acc: (97.00%) (10069/10368)
Epoch: 193 | Batch_idx: 90 |  Loss: (0.0836) |  Loss2: (0.0000) | Acc: (97.00%) (11301/11648)
Epoch: 193 | Batch_idx: 100 |  Loss: (0.0824) |  Loss2: (0.0000) | Acc: (97.00%) (12550/12928)
Epoch: 193 | Batch_idx: 110 |  Loss: (0.0812) |  Loss2: (0.0000) | Acc: (97.00%) (13806/14208)
Epoch: 193 | Batch_idx: 120 |  Loss: (0.0809) |  Loss2: (0.0000) | Acc: (97.00%) (15055/15488)
Epoch: 193 | Batch_idx: 130 |  Loss: (0.0813) |  Loss2: (0.0000) | Acc: (97.00%) (16293/16768)
Epoch: 193 | Batch_idx: 140 |  Loss: (0.0820) |  Loss2: (0.0000) | Acc: (97.00%) (17539/18048)
Epoch: 193 | Batch_idx: 150 |  Loss: (0.0816) |  Loss2: (0.0000) | Acc: (97.00%) (18793/19328)
Epoch: 193 | Batch_idx: 160 |  Loss: (0.0814) |  Loss2: (0.0000) | Acc: (97.00%) (20043/20608)
Epoch: 193 | Batch_idx: 170 |  Loss: (0.0809) |  Loss2: (0.0000) | Acc: (97.00%) (21292/21888)
Epoch: 193 | Batch_idx: 180 |  Loss: (0.0811) |  Loss2: (0.0000) | Acc: (97.00%) (22536/23168)
Epoch: 193 | Batch_idx: 190 |  Loss: (0.0802) |  Loss2: (0.0000) | Acc: (97.00%) (23790/24448)
Epoch: 193 | Batch_idx: 200 |  Loss: (0.0796) |  Loss2: (0.0000) | Acc: (97.00%) (25042/25728)
Epoch: 193 | Batch_idx: 210 |  Loss: (0.0799) |  Loss2: (0.0000) | Acc: (97.00%) (26285/27008)
Epoch: 193 | Batch_idx: 220 |  Loss: (0.0796) |  Loss2: (0.0000) | Acc: (97.00%) (27539/28288)
Epoch: 193 | Batch_idx: 230 |  Loss: (0.0795) |  Loss2: (0.0000) | Acc: (97.00%) (28783/29568)
Epoch: 193 | Batch_idx: 240 |  Loss: (0.0793) |  Loss2: (0.0000) | Acc: (97.00%) (30036/30848)
Epoch: 193 | Batch_idx: 250 |  Loss: (0.0799) |  Loss2: (0.0000) | Acc: (97.00%) (31271/32128)
Epoch: 193 | Batch_idx: 260 |  Loss: (0.0798) |  Loss2: (0.0000) | Acc: (97.00%) (32518/33408)
Epoch: 193 | Batch_idx: 270 |  Loss: (0.0796) |  Loss2: (0.0000) | Acc: (97.00%) (33765/34688)
Epoch: 193 | Batch_idx: 280 |  Loss: (0.0798) |  Loss2: (0.0000) | Acc: (97.00%) (35010/35968)
Epoch: 193 | Batch_idx: 290 |  Loss: (0.0796) |  Loss2: (0.0000) | Acc: (97.00%) (36255/37248)
Epoch: 193 | Batch_idx: 300 |  Loss: (0.0796) |  Loss2: (0.0000) | Acc: (97.00%) (37497/38528)
Epoch: 193 | Batch_idx: 310 |  Loss: (0.0795) |  Loss2: (0.0000) | Acc: (97.00%) (38741/39808)
Epoch: 193 | Batch_idx: 320 |  Loss: (0.0792) |  Loss2: (0.0000) | Acc: (97.00%) (39990/41088)
Epoch: 193 | Batch_idx: 330 |  Loss: (0.0791) |  Loss2: (0.0000) | Acc: (97.00%) (41236/42368)
Epoch: 193 | Batch_idx: 340 |  Loss: (0.0788) |  Loss2: (0.0000) | Acc: (97.00%) (42491/43648)
Epoch: 193 | Batch_idx: 350 |  Loss: (0.0783) |  Loss2: (0.0000) | Acc: (97.00%) (43744/44928)
Epoch: 193 | Batch_idx: 360 |  Loss: (0.0781) |  Loss2: (0.0000) | Acc: (97.00%) (44999/46208)
Epoch: 193 | Batch_idx: 370 |  Loss: (0.0778) |  Loss2: (0.0000) | Acc: (97.00%) (46250/47488)
Epoch: 193 | Batch_idx: 380 |  Loss: (0.0775) |  Loss2: (0.0000) | Acc: (97.00%) (47509/48768)
Epoch: 193 | Batch_idx: 390 |  Loss: (0.0773) |  Loss2: (0.0000) | Acc: (97.00%) (48712/50000)
# TEST : Loss: (0.3871) | Acc: (89.00%) (8949/10000)
percent tensor([0.5617, 0.5799, 0.5648, 0.5366, 0.5765, 0.5862, 0.5869, 0.5531, 0.5635,
        0.5713, 0.5783, 0.5744, 0.5610, 0.5444, 0.5833, 0.5578],
       device='cuda:0') torch.Size([16])
percent tensor([0.5297, 0.5304, 0.5369, 0.5384, 0.5385, 0.5291, 0.5338, 0.5400, 0.5333,
        0.5319, 0.5285, 0.5344, 0.5292, 0.5337, 0.5299, 0.5321],
       device='cuda:0') torch.Size([16])
percent tensor([0.6726, 0.7109, 0.4624, 0.4318, 0.4106, 0.6102, 0.6278, 0.4678, 0.5684,
        0.6657, 0.6916, 0.5590, 0.7098, 0.6490, 0.6626, 0.6514],
       device='cuda:0') torch.Size([16])
percent tensor([0.6875, 0.5813, 0.6723, 0.6740, 0.6701, 0.7715, 0.6371, 0.6695, 0.6398,
        0.6080, 0.6246, 0.6068, 0.5671, 0.6588, 0.6640, 0.7443],
       device='cuda:0') torch.Size([16])
percent tensor([0.6426, 0.6346, 0.6610, 0.6304, 0.6365, 0.6049, 0.6594, 0.6620, 0.6672,
        0.6243, 0.6137, 0.6268, 0.6305, 0.6508, 0.6385, 0.5999],
       device='cuda:0') torch.Size([16])
percent tensor([0.5581, 0.7697, 0.6890, 0.7395, 0.7220, 0.7674, 0.7400, 0.5724, 0.7765,
        0.6750, 0.7944, 0.8157, 0.7397, 0.8177, 0.6402, 0.5445],
       device='cuda:0') torch.Size([16])
percent tensor([0.5729, 0.6877, 0.5585, 0.4889, 0.6358, 0.7257, 0.6179, 0.4228, 0.6481,
        0.6644, 0.7324, 0.4563, 0.6764, 0.6160, 0.3855, 0.5092],
       device='cuda:0') torch.Size([16])
percent tensor([0.9997, 0.9997, 0.9999, 0.9997, 0.9999, 0.9995, 0.9999, 0.9996, 0.9999,
        0.9997, 1.0000, 0.9999, 0.9998, 0.9997, 0.9995, 0.9991],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 194 | Batch_idx: 0 |  Loss: (0.1127) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 194 | Batch_idx: 10 |  Loss: (0.0852) |  Loss2: (0.0000) | Acc: (97.00%) (1369/1408)
Epoch: 194 | Batch_idx: 20 |  Loss: (0.0853) |  Loss2: (0.0000) | Acc: (96.00%) (2605/2688)
Epoch: 194 | Batch_idx: 30 |  Loss: (0.0867) |  Loss2: (0.0000) | Acc: (96.00%) (3844/3968)
Epoch: 194 | Batch_idx: 40 |  Loss: (0.0872) |  Loss2: (0.0000) | Acc: (96.00%) (5090/5248)
Epoch: 194 | Batch_idx: 50 |  Loss: (0.0877) |  Loss2: (0.0000) | Acc: (96.00%) (6327/6528)
Epoch: 194 | Batch_idx: 60 |  Loss: (0.0873) |  Loss2: (0.0000) | Acc: (96.00%) (7564/7808)
Epoch: 194 | Batch_idx: 70 |  Loss: (0.0856) |  Loss2: (0.0000) | Acc: (96.00%) (8812/9088)
Epoch: 194 | Batch_idx: 80 |  Loss: (0.0856) |  Loss2: (0.0000) | Acc: (96.00%) (10048/10368)
Epoch: 194 | Batch_idx: 90 |  Loss: (0.0860) |  Loss2: (0.0000) | Acc: (96.00%) (11284/11648)
Epoch: 194 | Batch_idx: 100 |  Loss: (0.0865) |  Loss2: (0.0000) | Acc: (96.00%) (12522/12928)
Epoch: 194 | Batch_idx: 110 |  Loss: (0.0861) |  Loss2: (0.0000) | Acc: (96.00%) (13770/14208)
Epoch: 194 | Batch_idx: 120 |  Loss: (0.0859) |  Loss2: (0.0000) | Acc: (96.00%) (15016/15488)
Epoch: 194 | Batch_idx: 130 |  Loss: (0.0855) |  Loss2: (0.0000) | Acc: (96.00%) (16260/16768)
Epoch: 194 | Batch_idx: 140 |  Loss: (0.0843) |  Loss2: (0.0000) | Acc: (96.00%) (17506/18048)
Epoch: 194 | Batch_idx: 150 |  Loss: (0.0829) |  Loss2: (0.0000) | Acc: (97.00%) (18759/19328)
Epoch: 194 | Batch_idx: 160 |  Loss: (0.0829) |  Loss2: (0.0000) | Acc: (97.00%) (20000/20608)
Epoch: 194 | Batch_idx: 170 |  Loss: (0.0816) |  Loss2: (0.0000) | Acc: (97.00%) (21253/21888)
Epoch: 194 | Batch_idx: 180 |  Loss: (0.0813) |  Loss2: (0.0000) | Acc: (97.00%) (22501/23168)
Epoch: 194 | Batch_idx: 190 |  Loss: (0.0800) |  Loss2: (0.0000) | Acc: (97.00%) (23760/24448)
Epoch: 194 | Batch_idx: 200 |  Loss: (0.0798) |  Loss2: (0.0000) | Acc: (97.00%) (25007/25728)
Epoch: 194 | Batch_idx: 210 |  Loss: (0.0809) |  Loss2: (0.0000) | Acc: (97.00%) (26245/27008)
Epoch: 194 | Batch_idx: 220 |  Loss: (0.0812) |  Loss2: (0.0000) | Acc: (97.00%) (27484/28288)
Epoch: 194 | Batch_idx: 230 |  Loss: (0.0808) |  Loss2: (0.0000) | Acc: (97.00%) (28732/29568)
Epoch: 194 | Batch_idx: 240 |  Loss: (0.0810) |  Loss2: (0.0000) | Acc: (97.00%) (29972/30848)
Epoch: 194 | Batch_idx: 250 |  Loss: (0.0810) |  Loss2: (0.0000) | Acc: (97.00%) (31214/32128)
Epoch: 194 | Batch_idx: 260 |  Loss: (0.0807) |  Loss2: (0.0000) | Acc: (97.00%) (32464/33408)
Epoch: 194 | Batch_idx: 270 |  Loss: (0.0809) |  Loss2: (0.0000) | Acc: (97.00%) (33710/34688)
Epoch: 194 | Batch_idx: 280 |  Loss: (0.0804) |  Loss2: (0.0000) | Acc: (97.00%) (34961/35968)
Epoch: 194 | Batch_idx: 290 |  Loss: (0.0798) |  Loss2: (0.0000) | Acc: (97.00%) (36215/37248)
Epoch: 194 | Batch_idx: 300 |  Loss: (0.0793) |  Loss2: (0.0000) | Acc: (97.00%) (37470/38528)
Epoch: 194 | Batch_idx: 310 |  Loss: (0.0791) |  Loss2: (0.0000) | Acc: (97.00%) (38716/39808)
Epoch: 194 | Batch_idx: 320 |  Loss: (0.0790) |  Loss2: (0.0000) | Acc: (97.00%) (39965/41088)
Epoch: 194 | Batch_idx: 330 |  Loss: (0.0788) |  Loss2: (0.0000) | Acc: (97.00%) (41219/42368)
Epoch: 194 | Batch_idx: 340 |  Loss: (0.0783) |  Loss2: (0.0000) | Acc: (97.00%) (42476/43648)
Epoch: 194 | Batch_idx: 350 |  Loss: (0.0781) |  Loss2: (0.0000) | Acc: (97.00%) (43728/44928)
Epoch: 194 | Batch_idx: 360 |  Loss: (0.0780) |  Loss2: (0.0000) | Acc: (97.00%) (44980/46208)
Epoch: 194 | Batch_idx: 370 |  Loss: (0.0777) |  Loss2: (0.0000) | Acc: (97.00%) (46230/47488)
Epoch: 194 | Batch_idx: 380 |  Loss: (0.0775) |  Loss2: (0.0000) | Acc: (97.00%) (47480/48768)
Epoch: 194 | Batch_idx: 390 |  Loss: (0.0772) |  Loss2: (0.0000) | Acc: (97.00%) (48688/50000)
# TEST : Loss: (0.4040) | Acc: (88.00%) (8894/10000)
percent tensor([0.5679, 0.5890, 0.5730, 0.5444, 0.5836, 0.5903, 0.5955, 0.5631, 0.5709,
        0.5800, 0.5849, 0.5829, 0.5673, 0.5545, 0.5904, 0.5643],
       device='cuda:0') torch.Size([16])
percent tensor([0.5303, 0.5309, 0.5382, 0.5397, 0.5399, 0.5295, 0.5345, 0.5411, 0.5338,
        0.5325, 0.5285, 0.5351, 0.5296, 0.5340, 0.5302, 0.5327],
       device='cuda:0') torch.Size([16])
percent tensor([0.6833, 0.7170, 0.4635, 0.4442, 0.4138, 0.6265, 0.6394, 0.4816, 0.5853,
        0.6714, 0.7016, 0.5610, 0.7159, 0.6676, 0.6757, 0.6647],
       device='cuda:0') torch.Size([16])
percent tensor([0.6959, 0.5978, 0.6728, 0.6716, 0.6726, 0.7729, 0.6482, 0.6707, 0.6432,
        0.6235, 0.6352, 0.6082, 0.5814, 0.6676, 0.6731, 0.7544],
       device='cuda:0') torch.Size([16])
percent tensor([0.6188, 0.6249, 0.6373, 0.6018, 0.6111, 0.5761, 0.6419, 0.6390, 0.6487,
        0.6128, 0.5994, 0.6106, 0.6167, 0.6316, 0.6213, 0.5760],
       device='cuda:0') torch.Size([16])
percent tensor([0.5334, 0.7459, 0.6526, 0.7153, 0.6913, 0.7463, 0.7167, 0.5358, 0.7533,
        0.6378, 0.7763, 0.8061, 0.7174, 0.8080, 0.6076, 0.5146],
       device='cuda:0') torch.Size([16])
percent tensor([0.6185, 0.7309, 0.5612, 0.4619, 0.6256, 0.7319, 0.6324, 0.4295, 0.6863,
        0.7062, 0.7633, 0.4841, 0.7401, 0.6447, 0.4082, 0.5393],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9998, 0.9999, 0.9997, 0.9999, 0.9995, 0.9999, 0.9996, 0.9999,
        0.9998, 1.0000, 0.9999, 0.9999, 0.9996, 0.9995, 0.9991],
       device='cuda:0') torch.Size([16])
Epoch: 195 | Batch_idx: 0 |  Loss: (0.1196) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 195 | Batch_idx: 10 |  Loss: (0.0643) |  Loss2: (0.0000) | Acc: (98.00%) (1385/1408)
Epoch: 195 | Batch_idx: 20 |  Loss: (0.0663) |  Loss2: (0.0000) | Acc: (97.00%) (2634/2688)
Epoch: 195 | Batch_idx: 30 |  Loss: (0.0649) |  Loss2: (0.0000) | Acc: (98.00%) (3894/3968)
Epoch: 195 | Batch_idx: 40 |  Loss: (0.0657) |  Loss2: (0.0000) | Acc: (98.00%) (5144/5248)
Epoch: 195 | Batch_idx: 50 |  Loss: (0.0651) |  Loss2: (0.0000) | Acc: (97.00%) (6395/6528)
Epoch: 195 | Batch_idx: 60 |  Loss: (0.0664) |  Loss2: (0.0000) | Acc: (97.00%) (7641/7808)
Epoch: 195 | Batch_idx: 70 |  Loss: (0.0689) |  Loss2: (0.0000) | Acc: (97.00%) (8885/9088)
Epoch: 195 | Batch_idx: 80 |  Loss: (0.0702) |  Loss2: (0.0000) | Acc: (97.00%) (10131/10368)
Epoch: 195 | Batch_idx: 90 |  Loss: (0.0711) |  Loss2: (0.0000) | Acc: (97.00%) (11375/11648)
Epoch: 195 | Batch_idx: 100 |  Loss: (0.0713) |  Loss2: (0.0000) | Acc: (97.00%) (12628/12928)
Epoch: 195 | Batch_idx: 110 |  Loss: (0.0717) |  Loss2: (0.0000) | Acc: (97.00%) (13878/14208)
Epoch: 195 | Batch_idx: 120 |  Loss: (0.0717) |  Loss2: (0.0000) | Acc: (97.00%) (15127/15488)
Epoch: 195 | Batch_idx: 130 |  Loss: (0.0710) |  Loss2: (0.0000) | Acc: (97.00%) (16384/16768)
Epoch: 195 | Batch_idx: 140 |  Loss: (0.0704) |  Loss2: (0.0000) | Acc: (97.00%) (17642/18048)
Epoch: 195 | Batch_idx: 150 |  Loss: (0.0702) |  Loss2: (0.0000) | Acc: (97.00%) (18897/19328)
Epoch: 195 | Batch_idx: 160 |  Loss: (0.0712) |  Loss2: (0.0000) | Acc: (97.00%) (20144/20608)
Epoch: 195 | Batch_idx: 170 |  Loss: (0.0708) |  Loss2: (0.0000) | Acc: (97.00%) (21398/21888)
Epoch: 195 | Batch_idx: 180 |  Loss: (0.0705) |  Loss2: (0.0000) | Acc: (97.00%) (22649/23168)
Epoch: 195 | Batch_idx: 190 |  Loss: (0.0716) |  Loss2: (0.0000) | Acc: (97.00%) (23892/24448)
Epoch: 195 | Batch_idx: 200 |  Loss: (0.0713) |  Loss2: (0.0000) | Acc: (97.00%) (25146/25728)
Epoch: 195 | Batch_idx: 210 |  Loss: (0.0712) |  Loss2: (0.0000) | Acc: (97.00%) (26397/27008)
Epoch: 195 | Batch_idx: 220 |  Loss: (0.0711) |  Loss2: (0.0000) | Acc: (97.00%) (27646/28288)
Epoch: 195 | Batch_idx: 230 |  Loss: (0.0710) |  Loss2: (0.0000) | Acc: (97.00%) (28900/29568)
Epoch: 195 | Batch_idx: 240 |  Loss: (0.0711) |  Loss2: (0.0000) | Acc: (97.00%) (30147/30848)
Epoch: 195 | Batch_idx: 250 |  Loss: (0.0709) |  Loss2: (0.0000) | Acc: (97.00%) (31397/32128)
Epoch: 195 | Batch_idx: 260 |  Loss: (0.0708) |  Loss2: (0.0000) | Acc: (97.00%) (32648/33408)
Epoch: 195 | Batch_idx: 270 |  Loss: (0.0714) |  Loss2: (0.0000) | Acc: (97.00%) (33887/34688)
Epoch: 195 | Batch_idx: 280 |  Loss: (0.0712) |  Loss2: (0.0000) | Acc: (97.00%) (35136/35968)
Epoch: 195 | Batch_idx: 290 |  Loss: (0.0713) |  Loss2: (0.0000) | Acc: (97.00%) (36380/37248)
Epoch: 195 | Batch_idx: 300 |  Loss: (0.0719) |  Loss2: (0.0000) | Acc: (97.00%) (37619/38528)
Epoch: 195 | Batch_idx: 310 |  Loss: (0.0718) |  Loss2: (0.0000) | Acc: (97.00%) (38869/39808)
Epoch: 195 | Batch_idx: 320 |  Loss: (0.0712) |  Loss2: (0.0000) | Acc: (97.00%) (40131/41088)
Epoch: 195 | Batch_idx: 330 |  Loss: (0.0718) |  Loss2: (0.0000) | Acc: (97.00%) (41374/42368)
Epoch: 195 | Batch_idx: 340 |  Loss: (0.0718) |  Loss2: (0.0000) | Acc: (97.00%) (42626/43648)
Epoch: 195 | Batch_idx: 350 |  Loss: (0.0715) |  Loss2: (0.0000) | Acc: (97.00%) (43884/44928)
Epoch: 195 | Batch_idx: 360 |  Loss: (0.0712) |  Loss2: (0.0000) | Acc: (97.00%) (45143/46208)
Epoch: 195 | Batch_idx: 370 |  Loss: (0.0711) |  Loss2: (0.0000) | Acc: (97.00%) (46392/47488)
Epoch: 195 | Batch_idx: 380 |  Loss: (0.0711) |  Loss2: (0.0000) | Acc: (97.00%) (47643/48768)
Epoch: 195 | Batch_idx: 390 |  Loss: (0.0711) |  Loss2: (0.0000) | Acc: (97.00%) (48845/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_195.pth.tar'
# TEST : Loss: (0.3909) | Acc: (89.00%) (8921/10000)
percent tensor([0.5625, 0.5835, 0.5684, 0.5402, 0.5784, 0.5839, 0.5900, 0.5592, 0.5661,
        0.5752, 0.5793, 0.5784, 0.5624, 0.5506, 0.5846, 0.5591],
       device='cuda:0') torch.Size([16])
percent tensor([0.5296, 0.5302, 0.5382, 0.5394, 0.5397, 0.5287, 0.5340, 0.5409, 0.5334,
        0.5319, 0.5277, 0.5348, 0.5289, 0.5336, 0.5293, 0.5320],
       device='cuda:0') torch.Size([16])
percent tensor([0.6823, 0.7110, 0.4638, 0.4437, 0.4162, 0.6256, 0.6361, 0.4845, 0.5854,
        0.6671, 0.6979, 0.5605, 0.7121, 0.6640, 0.6730, 0.6626],
       device='cuda:0') torch.Size([16])
percent tensor([0.6927, 0.5980, 0.6683, 0.6654, 0.6691, 0.7715, 0.6463, 0.6651, 0.6388,
        0.6222, 0.6328, 0.6042, 0.5761, 0.6660, 0.6713, 0.7534],
       device='cuda:0') torch.Size([16])
percent tensor([0.6256, 0.6323, 0.6404, 0.6047, 0.6143, 0.5777, 0.6493, 0.6437, 0.6532,
        0.6215, 0.6091, 0.6149, 0.6243, 0.6357, 0.6281, 0.5832],
       device='cuda:0') torch.Size([16])
percent tensor([0.5427, 0.7503, 0.6586, 0.7230, 0.6954, 0.7540, 0.7225, 0.5494, 0.7572,
        0.6434, 0.7824, 0.8129, 0.7229, 0.8148, 0.6110, 0.5230],
       device='cuda:0') torch.Size([16])
percent tensor([0.6173, 0.7361, 0.5527, 0.4544, 0.6176, 0.7291, 0.6362, 0.4200, 0.6856,
        0.7065, 0.7656, 0.4838, 0.7422, 0.6511, 0.4027, 0.5363],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9998, 0.9999, 0.9997, 0.9999, 0.9995, 0.9999, 0.9996, 0.9999,
        0.9998, 1.0000, 0.9999, 0.9999, 0.9997, 0.9995, 0.9991],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 196 | Batch_idx: 0 |  Loss: (0.0763) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 196 | Batch_idx: 10 |  Loss: (0.0560) |  Loss2: (0.0000) | Acc: (98.00%) (1386/1408)
Epoch: 196 | Batch_idx: 20 |  Loss: (0.0557) |  Loss2: (0.0000) | Acc: (98.00%) (2644/2688)
Epoch: 196 | Batch_idx: 30 |  Loss: (0.0582) |  Loss2: (0.0000) | Acc: (98.00%) (3895/3968)
Epoch: 196 | Batch_idx: 40 |  Loss: (0.0594) |  Loss2: (0.0000) | Acc: (98.00%) (5146/5248)
Epoch: 196 | Batch_idx: 50 |  Loss: (0.0603) |  Loss2: (0.0000) | Acc: (98.00%) (6402/6528)
Epoch: 196 | Batch_idx: 60 |  Loss: (0.0652) |  Loss2: (0.0000) | Acc: (97.00%) (7642/7808)
Epoch: 196 | Batch_idx: 70 |  Loss: (0.0662) |  Loss2: (0.0000) | Acc: (97.00%) (8894/9088)
Epoch: 196 | Batch_idx: 80 |  Loss: (0.0662) |  Loss2: (0.0000) | Acc: (97.00%) (10150/10368)
Epoch: 196 | Batch_idx: 90 |  Loss: (0.0676) |  Loss2: (0.0000) | Acc: (97.00%) (11399/11648)
Epoch: 196 | Batch_idx: 100 |  Loss: (0.0682) |  Loss2: (0.0000) | Acc: (97.00%) (12644/12928)
Epoch: 196 | Batch_idx: 110 |  Loss: (0.0677) |  Loss2: (0.0000) | Acc: (97.00%) (13901/14208)
Epoch: 196 | Batch_idx: 120 |  Loss: (0.0679) |  Loss2: (0.0000) | Acc: (97.00%) (15153/15488)
Epoch: 196 | Batch_idx: 130 |  Loss: (0.0681) |  Loss2: (0.0000) | Acc: (97.00%) (16403/16768)
Epoch: 196 | Batch_idx: 140 |  Loss: (0.0689) |  Loss2: (0.0000) | Acc: (97.00%) (17645/18048)
Epoch: 196 | Batch_idx: 150 |  Loss: (0.0694) |  Loss2: (0.0000) | Acc: (97.00%) (18890/19328)
Epoch: 196 | Batch_idx: 160 |  Loss: (0.0695) |  Loss2: (0.0000) | Acc: (97.00%) (20145/20608)
Epoch: 196 | Batch_idx: 170 |  Loss: (0.0695) |  Loss2: (0.0000) | Acc: (97.00%) (21402/21888)
Epoch: 196 | Batch_idx: 180 |  Loss: (0.0697) |  Loss2: (0.0000) | Acc: (97.00%) (22654/23168)
Epoch: 196 | Batch_idx: 190 |  Loss: (0.0701) |  Loss2: (0.0000) | Acc: (97.00%) (23900/24448)
Epoch: 196 | Batch_idx: 200 |  Loss: (0.0707) |  Loss2: (0.0000) | Acc: (97.00%) (25142/25728)
Epoch: 196 | Batch_idx: 210 |  Loss: (0.0719) |  Loss2: (0.0000) | Acc: (97.00%) (26372/27008)
Epoch: 196 | Batch_idx: 220 |  Loss: (0.0719) |  Loss2: (0.0000) | Acc: (97.00%) (27614/28288)
Epoch: 196 | Batch_idx: 230 |  Loss: (0.0716) |  Loss2: (0.0000) | Acc: (97.00%) (28867/29568)
Epoch: 196 | Batch_idx: 240 |  Loss: (0.0716) |  Loss2: (0.0000) | Acc: (97.00%) (30123/30848)
Epoch: 196 | Batch_idx: 250 |  Loss: (0.0716) |  Loss2: (0.0000) | Acc: (97.00%) (31372/32128)
Epoch: 196 | Batch_idx: 260 |  Loss: (0.0717) |  Loss2: (0.0000) | Acc: (97.00%) (32618/33408)
Epoch: 196 | Batch_idx: 270 |  Loss: (0.0720) |  Loss2: (0.0000) | Acc: (97.00%) (33862/34688)
Epoch: 196 | Batch_idx: 280 |  Loss: (0.0722) |  Loss2: (0.0000) | Acc: (97.00%) (35112/35968)
Epoch: 196 | Batch_idx: 290 |  Loss: (0.0730) |  Loss2: (0.0000) | Acc: (97.00%) (36351/37248)
Epoch: 196 | Batch_idx: 300 |  Loss: (0.0736) |  Loss2: (0.0000) | Acc: (97.00%) (37589/38528)
Epoch: 196 | Batch_idx: 310 |  Loss: (0.0742) |  Loss2: (0.0000) | Acc: (97.00%) (38825/39808)
Epoch: 196 | Batch_idx: 320 |  Loss: (0.0745) |  Loss2: (0.0000) | Acc: (97.00%) (40070/41088)
Epoch: 196 | Batch_idx: 330 |  Loss: (0.0743) |  Loss2: (0.0000) | Acc: (97.00%) (41317/42368)
Epoch: 196 | Batch_idx: 340 |  Loss: (0.0746) |  Loss2: (0.0000) | Acc: (97.00%) (42558/43648)
Epoch: 196 | Batch_idx: 350 |  Loss: (0.0745) |  Loss2: (0.0000) | Acc: (97.00%) (43806/44928)
Epoch: 196 | Batch_idx: 360 |  Loss: (0.0747) |  Loss2: (0.0000) | Acc: (97.00%) (45044/46208)
Epoch: 196 | Batch_idx: 370 |  Loss: (0.0746) |  Loss2: (0.0000) | Acc: (97.00%) (46297/47488)
Epoch: 196 | Batch_idx: 380 |  Loss: (0.0748) |  Loss2: (0.0000) | Acc: (97.00%) (47541/48768)
Epoch: 196 | Batch_idx: 390 |  Loss: (0.0748) |  Loss2: (0.0000) | Acc: (97.00%) (48742/50000)
# TEST : Loss: (0.4359) | Acc: (88.00%) (8811/10000)
percent tensor([0.5645, 0.5838, 0.5681, 0.5402, 0.5790, 0.5860, 0.5904, 0.5595, 0.5672,
        0.5753, 0.5810, 0.5775, 0.5647, 0.5493, 0.5857, 0.5605],
       device='cuda:0') torch.Size([16])
percent tensor([0.5295, 0.5304, 0.5368, 0.5391, 0.5387, 0.5287, 0.5340, 0.5405, 0.5334,
        0.5317, 0.5283, 0.5345, 0.5284, 0.5348, 0.5295, 0.5320],
       device='cuda:0') torch.Size([16])
percent tensor([0.6884, 0.7098, 0.5033, 0.4438, 0.4389, 0.6370, 0.6422, 0.4982, 0.5764,
        0.6759, 0.6916, 0.5794, 0.7195, 0.6459, 0.6756, 0.6662],
       device='cuda:0') torch.Size([16])
percent tensor([0.6933, 0.5993, 0.6781, 0.6670, 0.6708, 0.7735, 0.6486, 0.6768, 0.6428,
        0.6359, 0.6423, 0.6133, 0.5863, 0.6725, 0.6774, 0.7585],
       device='cuda:0') torch.Size([16])
percent tensor([0.6274, 0.6324, 0.6425, 0.6137, 0.6210, 0.5825, 0.6500, 0.6400, 0.6447,
        0.6155, 0.6026, 0.6068, 0.6138, 0.6262, 0.6255, 0.5883],
       device='cuda:0') torch.Size([16])
percent tensor([0.5666, 0.7446, 0.6511, 0.7255, 0.6907, 0.7364, 0.7166, 0.5220, 0.7700,
        0.6501, 0.7990, 0.7948, 0.7366, 0.8115, 0.6112, 0.5230],
       device='cuda:0') torch.Size([16])
percent tensor([0.6144, 0.7099, 0.5720, 0.4778, 0.6196, 0.7304, 0.6398, 0.4071, 0.6714,
        0.7026, 0.7532, 0.4622, 0.7430, 0.6601, 0.4118, 0.5346],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9998, 0.9999, 0.9998, 0.9999, 0.9993, 0.9999, 0.9995, 0.9999,
        0.9998, 0.9999, 0.9999, 0.9999, 0.9997, 0.9995, 0.9995],
       device='cuda:0') torch.Size([16])
Epoch: 197 | Batch_idx: 0 |  Loss: (0.0758) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 197 | Batch_idx: 10 |  Loss: (0.0843) |  Loss2: (0.0000) | Acc: (97.00%) (1369/1408)
Epoch: 197 | Batch_idx: 20 |  Loss: (0.0819) |  Loss2: (0.0000) | Acc: (97.00%) (2610/2688)
Epoch: 197 | Batch_idx: 30 |  Loss: (0.0776) |  Loss2: (0.0000) | Acc: (97.00%) (3865/3968)
Epoch: 197 | Batch_idx: 40 |  Loss: (0.0791) |  Loss2: (0.0000) | Acc: (97.00%) (5100/5248)
Epoch: 197 | Batch_idx: 50 |  Loss: (0.0789) |  Loss2: (0.0000) | Acc: (97.00%) (6353/6528)
Epoch: 197 | Batch_idx: 60 |  Loss: (0.0794) |  Loss2: (0.0000) | Acc: (97.00%) (7601/7808)
Epoch: 197 | Batch_idx: 70 |  Loss: (0.0809) |  Loss2: (0.0000) | Acc: (97.00%) (8847/9088)
Epoch: 197 | Batch_idx: 80 |  Loss: (0.0810) |  Loss2: (0.0000) | Acc: (97.00%) (10094/10368)
Epoch: 197 | Batch_idx: 90 |  Loss: (0.0817) |  Loss2: (0.0000) | Acc: (97.00%) (11339/11648)
Epoch: 197 | Batch_idx: 100 |  Loss: (0.0817) |  Loss2: (0.0000) | Acc: (97.00%) (12585/12928)
Epoch: 197 | Batch_idx: 110 |  Loss: (0.0812) |  Loss2: (0.0000) | Acc: (97.00%) (13831/14208)
Epoch: 197 | Batch_idx: 120 |  Loss: (0.0808) |  Loss2: (0.0000) | Acc: (97.00%) (15080/15488)
Epoch: 197 | Batch_idx: 130 |  Loss: (0.0814) |  Loss2: (0.0000) | Acc: (97.00%) (16323/16768)
Epoch: 197 | Batch_idx: 140 |  Loss: (0.0813) |  Loss2: (0.0000) | Acc: (97.00%) (17563/18048)
Epoch: 197 | Batch_idx: 150 |  Loss: (0.0805) |  Loss2: (0.0000) | Acc: (97.00%) (18810/19328)
Epoch: 197 | Batch_idx: 160 |  Loss: (0.0807) |  Loss2: (0.0000) | Acc: (97.00%) (20045/20608)
Epoch: 197 | Batch_idx: 170 |  Loss: (0.0805) |  Loss2: (0.0000) | Acc: (97.00%) (21288/21888)
Epoch: 197 | Batch_idx: 180 |  Loss: (0.0805) |  Loss2: (0.0000) | Acc: (97.00%) (22531/23168)
Epoch: 197 | Batch_idx: 190 |  Loss: (0.0802) |  Loss2: (0.0000) | Acc: (97.00%) (23776/24448)
Epoch: 197 | Batch_idx: 200 |  Loss: (0.0798) |  Loss2: (0.0000) | Acc: (97.00%) (25028/25728)
Epoch: 197 | Batch_idx: 210 |  Loss: (0.0797) |  Loss2: (0.0000) | Acc: (97.00%) (26275/27008)
Epoch: 197 | Batch_idx: 220 |  Loss: (0.0801) |  Loss2: (0.0000) | Acc: (97.00%) (27513/28288)
Epoch: 197 | Batch_idx: 230 |  Loss: (0.0798) |  Loss2: (0.0000) | Acc: (97.00%) (28765/29568)
Epoch: 197 | Batch_idx: 240 |  Loss: (0.0794) |  Loss2: (0.0000) | Acc: (97.00%) (30012/30848)
Epoch: 197 | Batch_idx: 250 |  Loss: (0.0794) |  Loss2: (0.0000) | Acc: (97.00%) (31257/32128)
Epoch: 197 | Batch_idx: 260 |  Loss: (0.0796) |  Loss2: (0.0000) | Acc: (97.00%) (32493/33408)
Epoch: 197 | Batch_idx: 270 |  Loss: (0.0792) |  Loss2: (0.0000) | Acc: (97.00%) (33750/34688)
Epoch: 197 | Batch_idx: 280 |  Loss: (0.0787) |  Loss2: (0.0000) | Acc: (97.00%) (35004/35968)
Epoch: 197 | Batch_idx: 290 |  Loss: (0.0794) |  Loss2: (0.0000) | Acc: (97.00%) (36240/37248)
Epoch: 197 | Batch_idx: 300 |  Loss: (0.0793) |  Loss2: (0.0000) | Acc: (97.00%) (37489/38528)
Epoch: 197 | Batch_idx: 310 |  Loss: (0.0786) |  Loss2: (0.0000) | Acc: (97.00%) (38742/39808)
Epoch: 197 | Batch_idx: 320 |  Loss: (0.0787) |  Loss2: (0.0000) | Acc: (97.00%) (39985/41088)
Epoch: 197 | Batch_idx: 330 |  Loss: (0.0792) |  Loss2: (0.0000) | Acc: (97.00%) (41214/42368)
Epoch: 197 | Batch_idx: 340 |  Loss: (0.0795) |  Loss2: (0.0000) | Acc: (97.00%) (42449/43648)
Epoch: 197 | Batch_idx: 350 |  Loss: (0.0792) |  Loss2: (0.0000) | Acc: (97.00%) (43700/44928)
Epoch: 197 | Batch_idx: 360 |  Loss: (0.0790) |  Loss2: (0.0000) | Acc: (97.00%) (44952/46208)
Epoch: 197 | Batch_idx: 370 |  Loss: (0.0789) |  Loss2: (0.0000) | Acc: (97.00%) (46199/47488)
Epoch: 197 | Batch_idx: 380 |  Loss: (0.0784) |  Loss2: (0.0000) | Acc: (97.00%) (47448/48768)
Epoch: 197 | Batch_idx: 390 |  Loss: (0.0786) |  Loss2: (0.0000) | Acc: (97.00%) (48634/50000)
# TEST : Loss: (0.4151) | Acc: (88.00%) (8861/10000)
percent tensor([0.5636, 0.5832, 0.5707, 0.5422, 0.5814, 0.5860, 0.5908, 0.5593, 0.5655,
        0.5756, 0.5779, 0.5795, 0.5626, 0.5482, 0.5851, 0.5598],
       device='cuda:0') torch.Size([16])
percent tensor([0.5290, 0.5305, 0.5373, 0.5386, 0.5393, 0.5279, 0.5338, 0.5412, 0.5334,
        0.5314, 0.5277, 0.5341, 0.5285, 0.5345, 0.5288, 0.5314],
       device='cuda:0') torch.Size([16])
percent tensor([0.6771, 0.7062, 0.4763, 0.4417, 0.4166, 0.6088, 0.6313, 0.4911, 0.5726,
        0.6712, 0.6908, 0.5692, 0.7083, 0.6486, 0.6648, 0.6569],
       device='cuda:0') torch.Size([16])
percent tensor([0.6920, 0.6027, 0.6790, 0.6680, 0.6716, 0.7676, 0.6513, 0.6775, 0.6458,
        0.6250, 0.6408, 0.6221, 0.5819, 0.6617, 0.6740, 0.7543],
       device='cuda:0') torch.Size([16])
percent tensor([0.6305, 0.6361, 0.6398, 0.6018, 0.6149, 0.5917, 0.6475, 0.6337, 0.6477,
        0.6252, 0.6100, 0.6044, 0.6230, 0.6352, 0.6281, 0.5887],
       device='cuda:0') torch.Size([16])
percent tensor([0.5725, 0.7327, 0.6571, 0.7268, 0.6846, 0.7690, 0.7332, 0.5633, 0.7546,
        0.6261, 0.7810, 0.8110, 0.7133, 0.8155, 0.6325, 0.5451],
       device='cuda:0') torch.Size([16])
percent tensor([0.6341, 0.7227, 0.5826, 0.4601, 0.6191, 0.7337, 0.6708, 0.4294, 0.6925,
        0.6842, 0.7400, 0.4920, 0.7213, 0.6358, 0.4054, 0.5393],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9998, 0.9999, 0.9997, 0.9999, 0.9996, 0.9999, 0.9996, 0.9998,
        0.9998, 0.9999, 0.9999, 0.9999, 0.9996, 0.9996, 0.9992],
       device='cuda:0') torch.Size([16])
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 198 | Batch_idx: 0 |  Loss: (0.0875) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 198 | Batch_idx: 10 |  Loss: (0.0861) |  Loss2: (0.0000) | Acc: (96.00%) (1361/1408)
Epoch: 198 | Batch_idx: 20 |  Loss: (0.0864) |  Loss2: (0.0000) | Acc: (96.00%) (2598/2688)
Epoch: 198 | Batch_idx: 30 |  Loss: (0.0907) |  Loss2: (0.0000) | Acc: (96.00%) (3834/3968)
Epoch: 198 | Batch_idx: 40 |  Loss: (0.0934) |  Loss2: (0.0000) | Acc: (96.00%) (5067/5248)
Epoch: 198 | Batch_idx: 50 |  Loss: (0.0928) |  Loss2: (0.0000) | Acc: (96.00%) (6311/6528)
Epoch: 198 | Batch_idx: 60 |  Loss: (0.0958) |  Loss2: (0.0000) | Acc: (96.00%) (7536/7808)
Epoch: 198 | Batch_idx: 70 |  Loss: (0.0963) |  Loss2: (0.0000) | Acc: (96.00%) (8775/9088)
Epoch: 198 | Batch_idx: 80 |  Loss: (0.0985) |  Loss2: (0.0000) | Acc: (96.00%) (10005/10368)
Epoch: 198 | Batch_idx: 90 |  Loss: (0.0981) |  Loss2: (0.0000) | Acc: (96.00%) (11247/11648)
Epoch: 198 | Batch_idx: 100 |  Loss: (0.0977) |  Loss2: (0.0000) | Acc: (96.00%) (12482/12928)
Epoch: 198 | Batch_idx: 110 |  Loss: (0.0997) |  Loss2: (0.0000) | Acc: (96.00%) (13706/14208)
Epoch: 198 | Batch_idx: 120 |  Loss: (0.0993) |  Loss2: (0.0000) | Acc: (96.00%) (14945/15488)
Epoch: 198 | Batch_idx: 130 |  Loss: (0.1014) |  Loss2: (0.0000) | Acc: (96.00%) (16168/16768)
Epoch: 198 | Batch_idx: 140 |  Loss: (0.1011) |  Loss2: (0.0000) | Acc: (96.00%) (17404/18048)
Epoch: 198 | Batch_idx: 150 |  Loss: (0.1011) |  Loss2: (0.0000) | Acc: (96.00%) (18635/19328)
Epoch: 198 | Batch_idx: 160 |  Loss: (0.1022) |  Loss2: (0.0000) | Acc: (96.00%) (19861/20608)
Epoch: 198 | Batch_idx: 170 |  Loss: (0.1030) |  Loss2: (0.0000) | Acc: (96.00%) (21095/21888)
Epoch: 198 | Batch_idx: 180 |  Loss: (0.1028) |  Loss2: (0.0000) | Acc: (96.00%) (22331/23168)
Epoch: 198 | Batch_idx: 190 |  Loss: (0.1029) |  Loss2: (0.0000) | Acc: (96.00%) (23568/24448)
Epoch: 198 | Batch_idx: 200 |  Loss: (0.1020) |  Loss2: (0.0000) | Acc: (96.00%) (24800/25728)
Epoch: 198 | Batch_idx: 210 |  Loss: (0.1013) |  Loss2: (0.0000) | Acc: (96.00%) (26036/27008)
Epoch: 198 | Batch_idx: 220 |  Loss: (0.1010) |  Loss2: (0.0000) | Acc: (96.00%) (27271/28288)
Epoch: 198 | Batch_idx: 230 |  Loss: (0.1011) |  Loss2: (0.0000) | Acc: (96.00%) (28502/29568)
Epoch: 198 | Batch_idx: 240 |  Loss: (0.1018) |  Loss2: (0.0000) | Acc: (96.00%) (29730/30848)
Epoch: 198 | Batch_idx: 250 |  Loss: (0.1010) |  Loss2: (0.0000) | Acc: (96.00%) (30973/32128)
Epoch: 198 | Batch_idx: 260 |  Loss: (0.1006) |  Loss2: (0.0000) | Acc: (96.00%) (32206/33408)
Epoch: 198 | Batch_idx: 270 |  Loss: (0.1004) |  Loss2: (0.0000) | Acc: (96.00%) (33439/34688)
Epoch: 198 | Batch_idx: 280 |  Loss: (0.1004) |  Loss2: (0.0000) | Acc: (96.00%) (34672/35968)
Epoch: 198 | Batch_idx: 290 |  Loss: (0.1001) |  Loss2: (0.0000) | Acc: (96.00%) (35916/37248)
Epoch: 198 | Batch_idx: 300 |  Loss: (0.0998) |  Loss2: (0.0000) | Acc: (96.00%) (37159/38528)
Epoch: 198 | Batch_idx: 310 |  Loss: (0.0991) |  Loss2: (0.0000) | Acc: (96.00%) (38405/39808)
Epoch: 198 | Batch_idx: 320 |  Loss: (0.0991) |  Loss2: (0.0000) | Acc: (96.00%) (39640/41088)
Epoch: 198 | Batch_idx: 330 |  Loss: (0.0991) |  Loss2: (0.0000) | Acc: (96.00%) (40867/42368)
Epoch: 198 | Batch_idx: 340 |  Loss: (0.0995) |  Loss2: (0.0000) | Acc: (96.00%) (42090/43648)
Epoch: 198 | Batch_idx: 350 |  Loss: (0.0993) |  Loss2: (0.0000) | Acc: (96.00%) (43330/44928)
Epoch: 198 | Batch_idx: 360 |  Loss: (0.0990) |  Loss2: (0.0000) | Acc: (96.00%) (44564/46208)
Epoch: 198 | Batch_idx: 370 |  Loss: (0.0988) |  Loss2: (0.0000) | Acc: (96.00%) (45799/47488)
Epoch: 198 | Batch_idx: 380 |  Loss: (0.0983) |  Loss2: (0.0000) | Acc: (96.00%) (47046/48768)
Epoch: 198 | Batch_idx: 390 |  Loss: (0.0984) |  Loss2: (0.0000) | Acc: (96.00%) (48236/50000)
# TEST : Loss: (0.4153) | Acc: (88.00%) (8888/10000)
percent tensor([0.5689, 0.5902, 0.5731, 0.5456, 0.5848, 0.5891, 0.5968, 0.5639, 0.5713,
        0.5817, 0.5847, 0.5837, 0.5693, 0.5552, 0.5905, 0.5653],
       device='cuda:0') torch.Size([16])
percent tensor([0.5376, 0.5401, 0.5448, 0.5474, 0.5472, 0.5352, 0.5424, 0.5500, 0.5420,
        0.5404, 0.5374, 0.5418, 0.5377, 0.5436, 0.5374, 0.5401],
       device='cuda:0') torch.Size([16])
percent tensor([0.6786, 0.7008, 0.4740, 0.4492, 0.4204, 0.6084, 0.6317, 0.4909, 0.5738,
        0.6695, 0.6901, 0.5725, 0.7085, 0.6562, 0.6571, 0.6543],
       device='cuda:0') torch.Size([16])
percent tensor([0.6915, 0.6126, 0.6754, 0.6721, 0.6726, 0.7614, 0.6532, 0.6801, 0.6500,
        0.6366, 0.6500, 0.6240, 0.5950, 0.6688, 0.6715, 0.7561],
       device='cuda:0') torch.Size([16])
percent tensor([0.6580, 0.6685, 0.6667, 0.6253, 0.6375, 0.6270, 0.6749, 0.6540, 0.6738,
        0.6540, 0.6332, 0.6321, 0.6506, 0.6600, 0.6578, 0.6226],
       device='cuda:0') torch.Size([16])
percent tensor([0.5425, 0.7038, 0.6634, 0.7201, 0.6702, 0.7548, 0.7267, 0.5505, 0.7421,
        0.6146, 0.7712, 0.8026, 0.6930, 0.8061, 0.6168, 0.5182],
       device='cuda:0') torch.Size([16])
percent tensor([0.6371, 0.7051, 0.6063, 0.4979, 0.6337, 0.7493, 0.6765, 0.4941, 0.6752,
        0.6842, 0.7198, 0.5021, 0.6868, 0.6318, 0.4726, 0.5625],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9998, 0.9999, 0.9997, 0.9999, 0.9996, 0.9999, 0.9997, 0.9999,
        0.9997, 0.9999, 0.9999, 0.9998, 0.9996, 0.9996, 0.9992],
       device='cuda:0') torch.Size([16])
Epoch: 199 | Batch_idx: 0 |  Loss: (0.1012) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 199 | Batch_idx: 10 |  Loss: (0.0859) |  Loss2: (0.0000) | Acc: (96.00%) (1365/1408)
Epoch: 199 | Batch_idx: 20 |  Loss: (0.0898) |  Loss2: (0.0000) | Acc: (96.00%) (2601/2688)
Epoch: 199 | Batch_idx: 30 |  Loss: (0.0880) |  Loss2: (0.0000) | Acc: (96.00%) (3841/3968)
Epoch: 199 | Batch_idx: 40 |  Loss: (0.0893) |  Loss2: (0.0000) | Acc: (96.00%) (5080/5248)
Epoch: 199 | Batch_idx: 50 |  Loss: (0.0906) |  Loss2: (0.0000) | Acc: (96.00%) (6313/6528)
Epoch: 199 | Batch_idx: 60 |  Loss: (0.0892) |  Loss2: (0.0000) | Acc: (96.00%) (7553/7808)
Epoch: 199 | Batch_idx: 70 |  Loss: (0.0897) |  Loss2: (0.0000) | Acc: (96.00%) (8789/9088)
Epoch: 199 | Batch_idx: 80 |  Loss: (0.0872) |  Loss2: (0.0000) | Acc: (96.00%) (10038/10368)
Epoch: 199 | Batch_idx: 90 |  Loss: (0.0876) |  Loss2: (0.0000) | Acc: (96.00%) (11277/11648)
Epoch: 199 | Batch_idx: 100 |  Loss: (0.0893) |  Loss2: (0.0000) | Acc: (96.00%) (12505/12928)
Epoch: 199 | Batch_idx: 110 |  Loss: (0.0902) |  Loss2: (0.0000) | Acc: (96.00%) (13734/14208)
Epoch: 199 | Batch_idx: 120 |  Loss: (0.0887) |  Loss2: (0.0000) | Acc: (96.00%) (14980/15488)
Epoch: 199 | Batch_idx: 130 |  Loss: (0.0876) |  Loss2: (0.0000) | Acc: (96.00%) (16225/16768)
Epoch: 199 | Batch_idx: 140 |  Loss: (0.0871) |  Loss2: (0.0000) | Acc: (96.00%) (17477/18048)
Epoch: 199 | Batch_idx: 150 |  Loss: (0.0861) |  Loss2: (0.0000) | Acc: (96.00%) (18727/19328)
Epoch: 199 | Batch_idx: 160 |  Loss: (0.0856) |  Loss2: (0.0000) | Acc: (96.00%) (19972/20608)
Epoch: 199 | Batch_idx: 170 |  Loss: (0.0858) |  Loss2: (0.0000) | Acc: (96.00%) (21207/21888)
Epoch: 199 | Batch_idx: 180 |  Loss: (0.0858) |  Loss2: (0.0000) | Acc: (96.00%) (22444/23168)
Epoch: 199 | Batch_idx: 190 |  Loss: (0.0844) |  Loss2: (0.0000) | Acc: (96.00%) (23703/24448)
Epoch: 199 | Batch_idx: 200 |  Loss: (0.0837) |  Loss2: (0.0000) | Acc: (96.00%) (24953/25728)
Epoch: 199 | Batch_idx: 210 |  Loss: (0.0834) |  Loss2: (0.0000) | Acc: (97.00%) (26202/27008)
Epoch: 199 | Batch_idx: 220 |  Loss: (0.0834) |  Loss2: (0.0000) | Acc: (97.00%) (27448/28288)
Epoch: 199 | Batch_idx: 230 |  Loss: (0.0827) |  Loss2: (0.0000) | Acc: (97.00%) (28702/29568)
Epoch: 199 | Batch_idx: 240 |  Loss: (0.0824) |  Loss2: (0.0000) | Acc: (97.00%) (29948/30848)
Epoch: 199 | Batch_idx: 250 |  Loss: (0.0823) |  Loss2: (0.0000) | Acc: (97.00%) (31188/32128)
Epoch: 199 | Batch_idx: 260 |  Loss: (0.0823) |  Loss2: (0.0000) | Acc: (97.00%) (32438/33408)
Epoch: 199 | Batch_idx: 270 |  Loss: (0.0820) |  Loss2: (0.0000) | Acc: (97.00%) (33688/34688)
Epoch: 199 | Batch_idx: 280 |  Loss: (0.0816) |  Loss2: (0.0000) | Acc: (97.00%) (34938/35968)
Epoch: 199 | Batch_idx: 290 |  Loss: (0.0817) |  Loss2: (0.0000) | Acc: (97.00%) (36183/37248)
Epoch: 199 | Batch_idx: 300 |  Loss: (0.0816) |  Loss2: (0.0000) | Acc: (97.00%) (37429/38528)
Epoch: 199 | Batch_idx: 310 |  Loss: (0.0817) |  Loss2: (0.0000) | Acc: (97.00%) (38667/39808)
Epoch: 199 | Batch_idx: 320 |  Loss: (0.0811) |  Loss2: (0.0000) | Acc: (97.00%) (39923/41088)
Epoch: 199 | Batch_idx: 330 |  Loss: (0.0812) |  Loss2: (0.0000) | Acc: (97.00%) (41171/42368)
Epoch: 199 | Batch_idx: 340 |  Loss: (0.0808) |  Loss2: (0.0000) | Acc: (97.00%) (42420/43648)
Epoch: 199 | Batch_idx: 350 |  Loss: (0.0807) |  Loss2: (0.0000) | Acc: (97.00%) (43669/44928)
Epoch: 199 | Batch_idx: 360 |  Loss: (0.0805) |  Loss2: (0.0000) | Acc: (97.00%) (44920/46208)
Epoch: 199 | Batch_idx: 370 |  Loss: (0.0806) |  Loss2: (0.0000) | Acc: (97.00%) (46161/47488)
Epoch: 199 | Batch_idx: 380 |  Loss: (0.0806) |  Loss2: (0.0000) | Acc: (97.00%) (47403/48768)
Epoch: 199 | Batch_idx: 390 |  Loss: (0.0807) |  Loss2: (0.0000) | Acc: (97.00%) (48599/50000)
# TEST : Loss: (0.3979) | Acc: (89.00%) (8917/10000)
percent tensor([0.5697, 0.5923, 0.5733, 0.5460, 0.5851, 0.5882, 0.5983, 0.5651, 0.5728,
        0.5834, 0.5864, 0.5844, 0.5708, 0.5578, 0.5913, 0.5664],
       device='cuda:0') torch.Size([16])
percent tensor([0.5423, 0.5451, 0.5498, 0.5522, 0.5523, 0.5394, 0.5473, 0.5552, 0.5470,
        0.5453, 0.5423, 0.5467, 0.5426, 0.5484, 0.5420, 0.5447],
       device='cuda:0') torch.Size([16])
percent tensor([0.6889, 0.7069, 0.4832, 0.4614, 0.4315, 0.6165, 0.6443, 0.5024, 0.5835,
        0.6805, 0.7002, 0.5841, 0.7153, 0.6680, 0.6648, 0.6664],
       device='cuda:0') torch.Size([16])
percent tensor([0.6811, 0.6041, 0.6663, 0.6608, 0.6651, 0.7535, 0.6438, 0.6697, 0.6407,
        0.6276, 0.6394, 0.6116, 0.5844, 0.6602, 0.6580, 0.7480],
       device='cuda:0') torch.Size([16])
percent tensor([0.6448, 0.6601, 0.6554, 0.6152, 0.6275, 0.6216, 0.6634, 0.6409, 0.6609,
        0.6417, 0.6203, 0.6198, 0.6386, 0.6494, 0.6462, 0.6135],
       device='cuda:0') torch.Size([16])
percent tensor([0.5531, 0.7136, 0.6701, 0.7290, 0.6789, 0.7587, 0.7349, 0.5613, 0.7518,
        0.6245, 0.7798, 0.8057, 0.7028, 0.8134, 0.6295, 0.5264],
       device='cuda:0') torch.Size([16])
percent tensor([0.6302, 0.7009, 0.6007, 0.5017, 0.6240, 0.7376, 0.6674, 0.4958, 0.6672,
        0.6828, 0.7135, 0.4992, 0.6784, 0.6295, 0.4837, 0.5560],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9997, 0.9999, 0.9997, 0.9999, 0.9995, 0.9999, 0.9997, 0.9998,
        0.9997, 0.9999, 0.9999, 0.9998, 0.9996, 0.9996, 0.9992],
       device='cuda:0') torch.Size([16])
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 200 | Batch_idx: 0 |  Loss: (0.0481) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 200 | Batch_idx: 10 |  Loss: (0.0742) |  Loss2: (0.0000) | Acc: (97.00%) (1375/1408)
Epoch: 200 | Batch_idx: 20 |  Loss: (0.0714) |  Loss2: (0.0000) | Acc: (97.00%) (2627/2688)
Epoch: 200 | Batch_idx: 30 |  Loss: (0.0724) |  Loss2: (0.0000) | Acc: (97.00%) (3874/3968)
Epoch: 200 | Batch_idx: 40 |  Loss: (0.0709) |  Loss2: (0.0000) | Acc: (97.00%) (5129/5248)
Epoch: 200 | Batch_idx: 50 |  Loss: (0.0697) |  Loss2: (0.0000) | Acc: (97.00%) (6379/6528)
Epoch: 200 | Batch_idx: 60 |  Loss: (0.0699) |  Loss2: (0.0000) | Acc: (97.00%) (7629/7808)
Epoch: 200 | Batch_idx: 70 |  Loss: (0.0708) |  Loss2: (0.0000) | Acc: (97.00%) (8880/9088)
Epoch: 200 | Batch_idx: 80 |  Loss: (0.0710) |  Loss2: (0.0000) | Acc: (97.00%) (10124/10368)
Epoch: 200 | Batch_idx: 90 |  Loss: (0.0711) |  Loss2: (0.0000) | Acc: (97.00%) (11372/11648)
Epoch: 200 | Batch_idx: 100 |  Loss: (0.0711) |  Loss2: (0.0000) | Acc: (97.00%) (12623/12928)
Epoch: 200 | Batch_idx: 110 |  Loss: (0.0707) |  Loss2: (0.0000) | Acc: (97.00%) (13876/14208)
Epoch: 200 | Batch_idx: 120 |  Loss: (0.0707) |  Loss2: (0.0000) | Acc: (97.00%) (15126/15488)
Epoch: 200 | Batch_idx: 130 |  Loss: (0.0704) |  Loss2: (0.0000) | Acc: (97.00%) (16383/16768)
Epoch: 200 | Batch_idx: 140 |  Loss: (0.0704) |  Loss2: (0.0000) | Acc: (97.00%) (17631/18048)
Epoch: 200 | Batch_idx: 150 |  Loss: (0.0696) |  Loss2: (0.0000) | Acc: (97.00%) (18888/19328)
Epoch: 200 | Batch_idx: 160 |  Loss: (0.0700) |  Loss2: (0.0000) | Acc: (97.00%) (20135/20608)
Epoch: 200 | Batch_idx: 170 |  Loss: (0.0700) |  Loss2: (0.0000) | Acc: (97.00%) (21389/21888)
Epoch: 200 | Batch_idx: 180 |  Loss: (0.0704) |  Loss2: (0.0000) | Acc: (97.00%) (22636/23168)
Epoch: 200 | Batch_idx: 190 |  Loss: (0.0699) |  Loss2: (0.0000) | Acc: (97.00%) (23893/24448)
Epoch: 200 | Batch_idx: 200 |  Loss: (0.0696) |  Loss2: (0.0000) | Acc: (97.00%) (25147/25728)
Epoch: 200 | Batch_idx: 210 |  Loss: (0.0698) |  Loss2: (0.0000) | Acc: (97.00%) (26392/27008)
Epoch: 200 | Batch_idx: 220 |  Loss: (0.0694) |  Loss2: (0.0000) | Acc: (97.00%) (27648/28288)
Epoch: 200 | Batch_idx: 230 |  Loss: (0.0688) |  Loss2: (0.0000) | Acc: (97.00%) (28903/29568)
Epoch: 200 | Batch_idx: 240 |  Loss: (0.0690) |  Loss2: (0.0000) | Acc: (97.00%) (30153/30848)
Epoch: 200 | Batch_idx: 250 |  Loss: (0.0691) |  Loss2: (0.0000) | Acc: (97.00%) (31405/32128)
Epoch: 200 | Batch_idx: 260 |  Loss: (0.0696) |  Loss2: (0.0000) | Acc: (97.00%) (32652/33408)
Epoch: 200 | Batch_idx: 270 |  Loss: (0.0701) |  Loss2: (0.0000) | Acc: (97.00%) (33897/34688)
Epoch: 200 | Batch_idx: 280 |  Loss: (0.0706) |  Loss2: (0.0000) | Acc: (97.00%) (35141/35968)
Epoch: 200 | Batch_idx: 290 |  Loss: (0.0713) |  Loss2: (0.0000) | Acc: (97.00%) (36383/37248)
Epoch: 200 | Batch_idx: 300 |  Loss: (0.0722) |  Loss2: (0.0000) | Acc: (97.00%) (37622/38528)
Epoch: 200 | Batch_idx: 310 |  Loss: (0.0725) |  Loss2: (0.0000) | Acc: (97.00%) (38867/39808)
Epoch: 200 | Batch_idx: 320 |  Loss: (0.0728) |  Loss2: (0.0000) | Acc: (97.00%) (40110/41088)
Epoch: 200 | Batch_idx: 330 |  Loss: (0.0730) |  Loss2: (0.0000) | Acc: (97.00%) (41358/42368)
Epoch: 200 | Batch_idx: 340 |  Loss: (0.0734) |  Loss2: (0.0000) | Acc: (97.00%) (42593/43648)
Epoch: 200 | Batch_idx: 350 |  Loss: (0.0738) |  Loss2: (0.0000) | Acc: (97.00%) (43833/44928)
Epoch: 200 | Batch_idx: 360 |  Loss: (0.0738) |  Loss2: (0.0000) | Acc: (97.00%) (45081/46208)
Epoch: 200 | Batch_idx: 370 |  Loss: (0.0737) |  Loss2: (0.0000) | Acc: (97.00%) (46329/47488)
Epoch: 200 | Batch_idx: 380 |  Loss: (0.0736) |  Loss2: (0.0000) | Acc: (97.00%) (47578/48768)
Epoch: 200 | Batch_idx: 390 |  Loss: (0.0736) |  Loss2: (0.0000) | Acc: (97.00%) (48780/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_200.pth.tar'
# TEST : Loss: (0.4182) | Acc: (88.00%) (8869/10000)
percent tensor([0.5677, 0.5901, 0.5740, 0.5446, 0.5849, 0.5853, 0.5965, 0.5648, 0.5722,
        0.5821, 0.5845, 0.5846, 0.5688, 0.5559, 0.5883, 0.5646],
       device='cuda:0') torch.Size([16])
percent tensor([0.5417, 0.5425, 0.5503, 0.5517, 0.5522, 0.5387, 0.5464, 0.5549, 0.5460,
        0.5441, 0.5403, 0.5464, 0.5413, 0.5458, 0.5405, 0.5438],
       device='cuda:0') torch.Size([16])
percent tensor([0.6846, 0.7140, 0.4741, 0.4480, 0.4241, 0.6201, 0.6441, 0.5031, 0.5851,
        0.6787, 0.6998, 0.5788, 0.7127, 0.6662, 0.6730, 0.6672],
       device='cuda:0') torch.Size([16])
percent tensor([0.6985, 0.6131, 0.6782, 0.6673, 0.6773, 0.7667, 0.6561, 0.6800, 0.6565,
        0.6363, 0.6573, 0.6260, 0.5942, 0.6755, 0.6707, 0.7573],
       device='cuda:0') torch.Size([16])
percent tensor([0.6410, 0.6581, 0.6599, 0.6094, 0.6315, 0.6201, 0.6684, 0.6545, 0.6653,
        0.6370, 0.6181, 0.6218, 0.6357, 0.6486, 0.6493, 0.6072],
       device='cuda:0') torch.Size([16])
percent tensor([0.6108, 0.7448, 0.6878, 0.7520, 0.7248, 0.7764, 0.7472, 0.5919, 0.7872,
        0.6730, 0.8039, 0.8182, 0.7567, 0.8196, 0.6388, 0.5731],
       device='cuda:0') torch.Size([16])
percent tensor([0.6314, 0.6848, 0.5805, 0.4878, 0.6352, 0.7102, 0.6394, 0.4752, 0.6813,
        0.6970, 0.7125, 0.4948, 0.7222, 0.6282, 0.4281, 0.5558],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9997, 0.9999, 0.9997, 0.9999, 0.9996, 0.9999, 0.9996, 0.9999,
        0.9999, 0.9999, 0.9999, 0.9999, 0.9996, 0.9997, 0.9994],
       device='cuda:0') torch.Size([16])
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(184.4586, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(832.3791, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(828.5728, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1506.7755, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(476.1077, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2302.2869, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4228.5039, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1348.8827, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6276.4854, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11535.9648, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3789.6345, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(15987.0098, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 201 | Batch_idx: 0 |  Loss: (0.0642) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 201 | Batch_idx: 10 |  Loss: (0.0720) |  Loss2: (0.0000) | Acc: (97.00%) (1374/1408)
Epoch: 201 | Batch_idx: 20 |  Loss: (0.0719) |  Loss2: (0.0000) | Acc: (97.00%) (2623/2688)
Epoch: 201 | Batch_idx: 30 |  Loss: (0.0679) |  Loss2: (0.0000) | Acc: (97.00%) (3880/3968)
Epoch: 201 | Batch_idx: 40 |  Loss: (0.0721) |  Loss2: (0.0000) | Acc: (97.00%) (5122/5248)
Epoch: 201 | Batch_idx: 50 |  Loss: (0.0721) |  Loss2: (0.0000) | Acc: (97.00%) (6366/6528)
Epoch: 201 | Batch_idx: 60 |  Loss: (0.0723) |  Loss2: (0.0000) | Acc: (97.00%) (7608/7808)
Epoch: 201 | Batch_idx: 70 |  Loss: (0.0718) |  Loss2: (0.0000) | Acc: (97.00%) (8864/9088)
Epoch: 201 | Batch_idx: 80 |  Loss: (0.0721) |  Loss2: (0.0000) | Acc: (97.00%) (10112/10368)
Epoch: 201 | Batch_idx: 90 |  Loss: (0.0717) |  Loss2: (0.0000) | Acc: (97.00%) (11365/11648)
Epoch: 201 | Batch_idx: 100 |  Loss: (0.0745) |  Loss2: (0.0000) | Acc: (97.00%) (12600/12928)
Epoch: 201 | Batch_idx: 110 |  Loss: (0.0740) |  Loss2: (0.0000) | Acc: (97.00%) (13857/14208)
Epoch: 201 | Batch_idx: 120 |  Loss: (0.0738) |  Loss2: (0.0000) | Acc: (97.00%) (15109/15488)
Epoch: 201 | Batch_idx: 130 |  Loss: (0.0754) |  Loss2: (0.0000) | Acc: (97.00%) (16345/16768)
Epoch: 201 | Batch_idx: 140 |  Loss: (0.0754) |  Loss2: (0.0000) | Acc: (97.00%) (17592/18048)
Epoch: 201 | Batch_idx: 150 |  Loss: (0.0753) |  Loss2: (0.0000) | Acc: (97.00%) (18839/19328)
Epoch: 201 | Batch_idx: 160 |  Loss: (0.0752) |  Loss2: (0.0000) | Acc: (97.00%) (20087/20608)
Epoch: 201 | Batch_idx: 170 |  Loss: (0.0757) |  Loss2: (0.0000) | Acc: (97.00%) (21337/21888)
Epoch: 201 | Batch_idx: 180 |  Loss: (0.0763) |  Loss2: (0.0000) | Acc: (97.00%) (22574/23168)
Epoch: 201 | Batch_idx: 190 |  Loss: (0.0773) |  Loss2: (0.0000) | Acc: (97.00%) (23812/24448)
Epoch: 201 | Batch_idx: 200 |  Loss: (0.0775) |  Loss2: (0.0000) | Acc: (97.00%) (25064/25728)
Epoch: 201 | Batch_idx: 210 |  Loss: (0.0778) |  Loss2: (0.0000) | Acc: (97.00%) (26313/27008)
Epoch: 201 | Batch_idx: 220 |  Loss: (0.0771) |  Loss2: (0.0000) | Acc: (97.00%) (27567/28288)
Epoch: 201 | Batch_idx: 230 |  Loss: (0.0767) |  Loss2: (0.0000) | Acc: (97.00%) (28821/29568)
Epoch: 201 | Batch_idx: 240 |  Loss: (0.0763) |  Loss2: (0.0000) | Acc: (97.00%) (30073/30848)
Epoch: 201 | Batch_idx: 250 |  Loss: (0.0766) |  Loss2: (0.0000) | Acc: (97.00%) (31318/32128)
Epoch: 201 | Batch_idx: 260 |  Loss: (0.0765) |  Loss2: (0.0000) | Acc: (97.00%) (32562/33408)
Epoch: 201 | Batch_idx: 270 |  Loss: (0.0767) |  Loss2: (0.0000) | Acc: (97.00%) (33807/34688)
Epoch: 201 | Batch_idx: 280 |  Loss: (0.0765) |  Loss2: (0.0000) | Acc: (97.00%) (35057/35968)
Epoch: 201 | Batch_idx: 290 |  Loss: (0.0763) |  Loss2: (0.0000) | Acc: (97.00%) (36307/37248)
Epoch: 201 | Batch_idx: 300 |  Loss: (0.0762) |  Loss2: (0.0000) | Acc: (97.00%) (37558/38528)
Epoch: 201 | Batch_idx: 310 |  Loss: (0.0760) |  Loss2: (0.0000) | Acc: (97.00%) (38806/39808)
Epoch: 201 | Batch_idx: 320 |  Loss: (0.0756) |  Loss2: (0.0000) | Acc: (97.00%) (40060/41088)
Epoch: 201 | Batch_idx: 330 |  Loss: (0.0754) |  Loss2: (0.0000) | Acc: (97.00%) (41306/42368)
Epoch: 201 | Batch_idx: 340 |  Loss: (0.0755) |  Loss2: (0.0000) | Acc: (97.00%) (42546/43648)
Epoch: 201 | Batch_idx: 350 |  Loss: (0.0755) |  Loss2: (0.0000) | Acc: (97.00%) (43789/44928)
Epoch: 201 | Batch_idx: 360 |  Loss: (0.0757) |  Loss2: (0.0000) | Acc: (97.00%) (45036/46208)
Epoch: 201 | Batch_idx: 370 |  Loss: (0.0759) |  Loss2: (0.0000) | Acc: (97.00%) (46278/47488)
Epoch: 201 | Batch_idx: 380 |  Loss: (0.0758) |  Loss2: (0.0000) | Acc: (97.00%) (47520/48768)
Epoch: 201 | Batch_idx: 390 |  Loss: (0.0760) |  Loss2: (0.0000) | Acc: (97.00%) (48715/50000)
# TEST : Loss: (0.4184) | Acc: (88.00%) (8861/10000)
percent tensor([0.5676, 0.5887, 0.5735, 0.5453, 0.5849, 0.5861, 0.5964, 0.5632, 0.5711,
        0.5812, 0.5838, 0.5843, 0.5688, 0.5554, 0.5881, 0.5644],
       device='cuda:0') torch.Size([16])
percent tensor([0.5411, 0.5432, 0.5481, 0.5511, 0.5508, 0.5387, 0.5461, 0.5540, 0.5451,
        0.5440, 0.5402, 0.5458, 0.5407, 0.5466, 0.5410, 0.5435],
       device='cuda:0') torch.Size([16])
percent tensor([0.6911, 0.7051, 0.5124, 0.4636, 0.4374, 0.6214, 0.6358, 0.5030, 0.5748,
        0.6806, 0.6978, 0.5920, 0.7203, 0.6551, 0.6621, 0.6698],
       device='cuda:0') torch.Size([16])
percent tensor([0.7073, 0.6221, 0.6820, 0.6721, 0.6850, 0.7716, 0.6607, 0.6885, 0.6544,
        0.6477, 0.6562, 0.6313, 0.5948, 0.6843, 0.6828, 0.7674],
       device='cuda:0') torch.Size([16])
percent tensor([0.6549, 0.6641, 0.6714, 0.6300, 0.6382, 0.6225, 0.6787, 0.6657, 0.6744,
        0.6501, 0.6363, 0.6324, 0.6484, 0.6460, 0.6574, 0.6215],
       device='cuda:0') torch.Size([16])
percent tensor([0.5995, 0.7406, 0.6948, 0.7425, 0.7210, 0.7773, 0.7504, 0.5916, 0.7836,
        0.6593, 0.8081, 0.8134, 0.7463, 0.8348, 0.6458, 0.5691],
       device='cuda:0') torch.Size([16])
percent tensor([0.6197, 0.6805, 0.5755, 0.4798, 0.6314, 0.7182, 0.6599, 0.4592, 0.6578,
        0.6741, 0.7150, 0.4438, 0.6916, 0.6484, 0.4248, 0.5386],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9998, 0.9999, 0.9998, 0.9999, 0.9995, 1.0000, 0.9996, 0.9999,
        0.9998, 0.9999, 0.9999, 0.9999, 0.9997, 0.9995, 0.9995],
       device='cuda:0') torch.Size([16])
Epoch: 202 | Batch_idx: 0 |  Loss: (0.0671) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 202 | Batch_idx: 10 |  Loss: (0.0737) |  Loss2: (0.0000) | Acc: (97.00%) (1373/1408)
Epoch: 202 | Batch_idx: 20 |  Loss: (0.0730) |  Loss2: (0.0000) | Acc: (97.00%) (2617/2688)
Epoch: 202 | Batch_idx: 30 |  Loss: (0.0689) |  Loss2: (0.0000) | Acc: (97.00%) (3871/3968)
Epoch: 202 | Batch_idx: 40 |  Loss: (0.0680) |  Loss2: (0.0000) | Acc: (97.00%) (5123/5248)
Epoch: 202 | Batch_idx: 50 |  Loss: (0.0674) |  Loss2: (0.0000) | Acc: (97.00%) (6381/6528)
Epoch: 202 | Batch_idx: 60 |  Loss: (0.0681) |  Loss2: (0.0000) | Acc: (97.00%) (7629/7808)
Epoch: 202 | Batch_idx: 70 |  Loss: (0.0685) |  Loss2: (0.0000) | Acc: (97.00%) (8876/9088)
Epoch: 202 | Batch_idx: 80 |  Loss: (0.0701) |  Loss2: (0.0000) | Acc: (97.00%) (10121/10368)
Epoch: 202 | Batch_idx: 90 |  Loss: (0.0718) |  Loss2: (0.0000) | Acc: (97.00%) (11368/11648)
Epoch: 202 | Batch_idx: 100 |  Loss: (0.0725) |  Loss2: (0.0000) | Acc: (97.00%) (12612/12928)
Epoch: 202 | Batch_idx: 110 |  Loss: (0.0719) |  Loss2: (0.0000) | Acc: (97.00%) (13862/14208)
Epoch: 202 | Batch_idx: 120 |  Loss: (0.0716) |  Loss2: (0.0000) | Acc: (97.00%) (15106/15488)
Epoch: 202 | Batch_idx: 130 |  Loss: (0.0727) |  Loss2: (0.0000) | Acc: (97.00%) (16349/16768)
Epoch: 202 | Batch_idx: 140 |  Loss: (0.0722) |  Loss2: (0.0000) | Acc: (97.00%) (17603/18048)
Epoch: 202 | Batch_idx: 150 |  Loss: (0.0715) |  Loss2: (0.0000) | Acc: (97.00%) (18859/19328)
Epoch: 202 | Batch_idx: 160 |  Loss: (0.0716) |  Loss2: (0.0000) | Acc: (97.00%) (20103/20608)
Epoch: 202 | Batch_idx: 170 |  Loss: (0.0711) |  Loss2: (0.0000) | Acc: (97.00%) (21359/21888)
Epoch: 202 | Batch_idx: 180 |  Loss: (0.0713) |  Loss2: (0.0000) | Acc: (97.00%) (22608/23168)
Epoch: 202 | Batch_idx: 190 |  Loss: (0.0709) |  Loss2: (0.0000) | Acc: (97.00%) (23863/24448)
Epoch: 202 | Batch_idx: 200 |  Loss: (0.0708) |  Loss2: (0.0000) | Acc: (97.00%) (25118/25728)
Epoch: 202 | Batch_idx: 210 |  Loss: (0.0700) |  Loss2: (0.0000) | Acc: (97.00%) (26375/27008)
Epoch: 202 | Batch_idx: 220 |  Loss: (0.0694) |  Loss2: (0.0000) | Acc: (97.00%) (27636/28288)
Epoch: 202 | Batch_idx: 230 |  Loss: (0.0692) |  Loss2: (0.0000) | Acc: (97.00%) (28885/29568)
Epoch: 202 | Batch_idx: 240 |  Loss: (0.0695) |  Loss2: (0.0000) | Acc: (97.00%) (30135/30848)
Epoch: 202 | Batch_idx: 250 |  Loss: (0.0695) |  Loss2: (0.0000) | Acc: (97.00%) (31388/32128)
Epoch: 202 | Batch_idx: 260 |  Loss: (0.0697) |  Loss2: (0.0000) | Acc: (97.00%) (32640/33408)
Epoch: 202 | Batch_idx: 270 |  Loss: (0.0708) |  Loss2: (0.0000) | Acc: (97.00%) (33872/34688)
Epoch: 202 | Batch_idx: 280 |  Loss: (0.0708) |  Loss2: (0.0000) | Acc: (97.00%) (35123/35968)
Epoch: 202 | Batch_idx: 290 |  Loss: (0.0706) |  Loss2: (0.0000) | Acc: (97.00%) (36375/37248)
Epoch: 202 | Batch_idx: 300 |  Loss: (0.0706) |  Loss2: (0.0000) | Acc: (97.00%) (37620/38528)
Epoch: 202 | Batch_idx: 310 |  Loss: (0.0704) |  Loss2: (0.0000) | Acc: (97.00%) (38873/39808)
Epoch: 202 | Batch_idx: 320 |  Loss: (0.0707) |  Loss2: (0.0000) | Acc: (97.00%) (40121/41088)
Epoch: 202 | Batch_idx: 330 |  Loss: (0.0708) |  Loss2: (0.0000) | Acc: (97.00%) (41366/42368)
Epoch: 202 | Batch_idx: 340 |  Loss: (0.0710) |  Loss2: (0.0000) | Acc: (97.00%) (42623/43648)
Epoch: 202 | Batch_idx: 350 |  Loss: (0.0709) |  Loss2: (0.0000) | Acc: (97.00%) (43868/44928)
Epoch: 202 | Batch_idx: 360 |  Loss: (0.0707) |  Loss2: (0.0000) | Acc: (97.00%) (45125/46208)
Epoch: 202 | Batch_idx: 370 |  Loss: (0.0707) |  Loss2: (0.0000) | Acc: (97.00%) (46374/47488)
Epoch: 202 | Batch_idx: 380 |  Loss: (0.0705) |  Loss2: (0.0000) | Acc: (97.00%) (47632/48768)
Epoch: 202 | Batch_idx: 390 |  Loss: (0.0704) |  Loss2: (0.0000) | Acc: (97.00%) (48839/50000)
# TEST : Loss: (0.4109) | Acc: (88.00%) (8875/10000)
percent tensor([0.5667, 0.5857, 0.5707, 0.5420, 0.5807, 0.5825, 0.5922, 0.5619, 0.5694,
        0.5789, 0.5825, 0.5805, 0.5673, 0.5510, 0.5851, 0.5627],
       device='cuda:0') torch.Size([16])
percent tensor([0.5407, 0.5421, 0.5488, 0.5518, 0.5508, 0.5386, 0.5453, 0.5536, 0.5448,
        0.5429, 0.5393, 0.5453, 0.5403, 0.5458, 0.5405, 0.5432],
       device='cuda:0') torch.Size([16])
percent tensor([0.6887, 0.7076, 0.4897, 0.4458, 0.4238, 0.6205, 0.6414, 0.5048, 0.5973,
        0.6792, 0.7061, 0.5784, 0.7220, 0.6640, 0.6668, 0.6683],
       device='cuda:0') torch.Size([16])
percent tensor([0.7140, 0.6266, 0.6815, 0.6748, 0.6817, 0.7729, 0.6669, 0.6874, 0.6646,
        0.6493, 0.6699, 0.6241, 0.6089, 0.6862, 0.6859, 0.7694],
       device='cuda:0') torch.Size([16])
percent tensor([0.6483, 0.6593, 0.6675, 0.6317, 0.6389, 0.6238, 0.6758, 0.6668, 0.6711,
        0.6452, 0.6235, 0.6365, 0.6418, 0.6619, 0.6568, 0.6173],
       device='cuda:0') torch.Size([16])
percent tensor([0.5972, 0.7221, 0.6741, 0.7488, 0.7183, 0.7722, 0.7229, 0.6052, 0.7739,
        0.6439, 0.7981, 0.7961, 0.7275, 0.8211, 0.6436, 0.5768],
       device='cuda:0') torch.Size([16])
percent tensor([0.6093, 0.6927, 0.5645, 0.4816, 0.6369, 0.7227, 0.6432, 0.4631, 0.6497,
        0.6780, 0.6994, 0.4364, 0.7002, 0.6308, 0.4194, 0.5451],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9998, 0.9999, 0.9998, 0.9999, 0.9997, 0.9999, 0.9997, 0.9998,
        0.9999, 0.9999, 0.9999, 0.9999, 0.9994, 0.9995, 0.9995],
       device='cuda:0') torch.Size([16])
Epoch: 203 | Batch_idx: 0 |  Loss: (0.1383) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 203 | Batch_idx: 10 |  Loss: (0.0723) |  Loss2: (0.0000) | Acc: (97.00%) (1378/1408)
Epoch: 203 | Batch_idx: 20 |  Loss: (0.0639) |  Loss2: (0.0000) | Acc: (97.00%) (2631/2688)
Epoch: 203 | Batch_idx: 30 |  Loss: (0.0676) |  Loss2: (0.0000) | Acc: (97.00%) (3877/3968)
Epoch: 203 | Batch_idx: 40 |  Loss: (0.0654) |  Loss2: (0.0000) | Acc: (97.00%) (5138/5248)
Epoch: 203 | Batch_idx: 50 |  Loss: (0.0629) |  Loss2: (0.0000) | Acc: (98.00%) (6400/6528)
Epoch: 203 | Batch_idx: 60 |  Loss: (0.0621) |  Loss2: (0.0000) | Acc: (98.00%) (7656/7808)
Epoch: 203 | Batch_idx: 70 |  Loss: (0.0630) |  Loss2: (0.0000) | Acc: (97.00%) (8903/9088)
Epoch: 203 | Batch_idx: 80 |  Loss: (0.0631) |  Loss2: (0.0000) | Acc: (97.00%) (10151/10368)
Epoch: 203 | Batch_idx: 90 |  Loss: (0.0631) |  Loss2: (0.0000) | Acc: (97.00%) (11404/11648)
Epoch: 203 | Batch_idx: 100 |  Loss: (0.0626) |  Loss2: (0.0000) | Acc: (97.00%) (12663/12928)
Epoch: 203 | Batch_idx: 110 |  Loss: (0.0626) |  Loss2: (0.0000) | Acc: (97.00%) (13913/14208)
Epoch: 203 | Batch_idx: 120 |  Loss: (0.0630) |  Loss2: (0.0000) | Acc: (97.00%) (15167/15488)
Epoch: 203 | Batch_idx: 130 |  Loss: (0.0632) |  Loss2: (0.0000) | Acc: (97.00%) (16422/16768)
Epoch: 203 | Batch_idx: 140 |  Loss: (0.0629) |  Loss2: (0.0000) | Acc: (97.00%) (17680/18048)
Epoch: 203 | Batch_idx: 150 |  Loss: (0.0627) |  Loss2: (0.0000) | Acc: (97.00%) (18934/19328)
Epoch: 203 | Batch_idx: 160 |  Loss: (0.0637) |  Loss2: (0.0000) | Acc: (97.00%) (20177/20608)
Epoch: 203 | Batch_idx: 170 |  Loss: (0.0636) |  Loss2: (0.0000) | Acc: (97.00%) (21435/21888)
Epoch: 203 | Batch_idx: 180 |  Loss: (0.0639) |  Loss2: (0.0000) | Acc: (97.00%) (22687/23168)
Epoch: 203 | Batch_idx: 190 |  Loss: (0.0641) |  Loss2: (0.0000) | Acc: (97.00%) (23936/24448)
Epoch: 203 | Batch_idx: 200 |  Loss: (0.0646) |  Loss2: (0.0000) | Acc: (97.00%) (25185/25728)
Epoch: 203 | Batch_idx: 210 |  Loss: (0.0649) |  Loss2: (0.0000) | Acc: (97.00%) (26437/27008)
Epoch: 203 | Batch_idx: 220 |  Loss: (0.0646) |  Loss2: (0.0000) | Acc: (97.00%) (27692/28288)
Epoch: 203 | Batch_idx: 230 |  Loss: (0.0654) |  Loss2: (0.0000) | Acc: (97.00%) (28936/29568)
Epoch: 203 | Batch_idx: 240 |  Loss: (0.0653) |  Loss2: (0.0000) | Acc: (97.00%) (30193/30848)
Epoch: 203 | Batch_idx: 250 |  Loss: (0.0656) |  Loss2: (0.0000) | Acc: (97.00%) (31439/32128)
Epoch: 203 | Batch_idx: 260 |  Loss: (0.0657) |  Loss2: (0.0000) | Acc: (97.00%) (32689/33408)
Epoch: 203 | Batch_idx: 270 |  Loss: (0.0661) |  Loss2: (0.0000) | Acc: (97.00%) (33938/34688)
Epoch: 203 | Batch_idx: 280 |  Loss: (0.0662) |  Loss2: (0.0000) | Acc: (97.00%) (35193/35968)
Epoch: 203 | Batch_idx: 290 |  Loss: (0.0663) |  Loss2: (0.0000) | Acc: (97.00%) (36444/37248)
Epoch: 203 | Batch_idx: 300 |  Loss: (0.0667) |  Loss2: (0.0000) | Acc: (97.00%) (37694/38528)
Epoch: 203 | Batch_idx: 310 |  Loss: (0.0667) |  Loss2: (0.0000) | Acc: (97.00%) (38948/39808)
Epoch: 203 | Batch_idx: 320 |  Loss: (0.0665) |  Loss2: (0.0000) | Acc: (97.00%) (40200/41088)
Epoch: 203 | Batch_idx: 330 |  Loss: (0.0664) |  Loss2: (0.0000) | Acc: (97.00%) (41452/42368)
Epoch: 203 | Batch_idx: 340 |  Loss: (0.0668) |  Loss2: (0.0000) | Acc: (97.00%) (42704/43648)
Epoch: 203 | Batch_idx: 350 |  Loss: (0.0672) |  Loss2: (0.0000) | Acc: (97.00%) (43948/44928)
Epoch: 203 | Batch_idx: 360 |  Loss: (0.0671) |  Loss2: (0.0000) | Acc: (97.00%) (45202/46208)
Epoch: 203 | Batch_idx: 370 |  Loss: (0.0671) |  Loss2: (0.0000) | Acc: (97.00%) (46452/47488)
Epoch: 203 | Batch_idx: 380 |  Loss: (0.0676) |  Loss2: (0.0000) | Acc: (97.00%) (47697/48768)
Epoch: 203 | Batch_idx: 390 |  Loss: (0.0676) |  Loss2: (0.0000) | Acc: (97.00%) (48905/50000)
# TEST : Loss: (0.4264) | Acc: (88.00%) (8817/10000)
percent tensor([0.5683, 0.5865, 0.5735, 0.5433, 0.5841, 0.5838, 0.5948, 0.5623, 0.5713,
        0.5796, 0.5834, 0.5837, 0.5691, 0.5521, 0.5858, 0.5632],
       device='cuda:0') torch.Size([16])
percent tensor([0.5406, 0.5419, 0.5487, 0.5509, 0.5502, 0.5378, 0.5453, 0.5532, 0.5444,
        0.5434, 0.5398, 0.5455, 0.5400, 0.5458, 0.5402, 0.5426],
       device='cuda:0') torch.Size([16])
percent tensor([0.7060, 0.7140, 0.5116, 0.4766, 0.4524, 0.6546, 0.6574, 0.5186, 0.5989,
        0.6868, 0.7087, 0.5942, 0.7289, 0.6689, 0.6854, 0.6889],
       device='cuda:0') torch.Size([16])
percent tensor([0.7201, 0.6322, 0.7044, 0.7013, 0.7018, 0.7826, 0.6734, 0.6999, 0.6732,
        0.6614, 0.6692, 0.6567, 0.6189, 0.6954, 0.6906, 0.7762],
       device='cuda:0') torch.Size([16])
percent tensor([0.6458, 0.6576, 0.6556, 0.6108, 0.6289, 0.6222, 0.6748, 0.6596, 0.6782,
        0.6409, 0.6295, 0.6227, 0.6458, 0.6555, 0.6536, 0.6169],
       device='cuda:0') torch.Size([16])
percent tensor([0.6285, 0.7512, 0.6965, 0.7793, 0.7370, 0.7815, 0.7533, 0.6133, 0.7977,
        0.6853, 0.8063, 0.8280, 0.7592, 0.8399, 0.6862, 0.5862],
       device='cuda:0') torch.Size([16])
percent tensor([0.6056, 0.7104, 0.5653, 0.4875, 0.6445, 0.7182, 0.6489, 0.4199, 0.6822,
        0.7153, 0.7231, 0.4916, 0.7081, 0.6546, 0.4286, 0.5363],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9999, 0.9999, 0.9998, 0.9999, 0.9997, 0.9999, 0.9995, 0.9999,
        0.9999, 1.0000, 0.9999, 0.9999, 0.9997, 0.9996, 0.9993],
       device='cuda:0') torch.Size([16])
Epoch: 204 | Batch_idx: 0 |  Loss: (0.1080) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 204 | Batch_idx: 10 |  Loss: (0.0623) |  Loss2: (0.0000) | Acc: (97.00%) (1378/1408)
Epoch: 204 | Batch_idx: 20 |  Loss: (0.0601) |  Loss2: (0.0000) | Acc: (98.00%) (2635/2688)
Epoch: 204 | Batch_idx: 30 |  Loss: (0.0660) |  Loss2: (0.0000) | Acc: (97.00%) (3875/3968)
Epoch: 204 | Batch_idx: 40 |  Loss: (0.0647) |  Loss2: (0.0000) | Acc: (97.00%) (5128/5248)
Epoch: 204 | Batch_idx: 50 |  Loss: (0.0654) |  Loss2: (0.0000) | Acc: (97.00%) (6384/6528)
Epoch: 204 | Batch_idx: 60 |  Loss: (0.0638) |  Loss2: (0.0000) | Acc: (97.00%) (7641/7808)
Epoch: 204 | Batch_idx: 70 |  Loss: (0.0640) |  Loss2: (0.0000) | Acc: (97.00%) (8897/9088)
Epoch: 204 | Batch_idx: 80 |  Loss: (0.0638) |  Loss2: (0.0000) | Acc: (97.00%) (10152/10368)
Epoch: 204 | Batch_idx: 90 |  Loss: (0.0621) |  Loss2: (0.0000) | Acc: (97.00%) (11415/11648)
Epoch: 204 | Batch_idx: 100 |  Loss: (0.0624) |  Loss2: (0.0000) | Acc: (97.00%) (12668/12928)
Epoch: 204 | Batch_idx: 110 |  Loss: (0.0644) |  Loss2: (0.0000) | Acc: (97.00%) (13909/14208)
Epoch: 204 | Batch_idx: 120 |  Loss: (0.0635) |  Loss2: (0.0000) | Acc: (97.00%) (15170/15488)
Epoch: 204 | Batch_idx: 130 |  Loss: (0.0644) |  Loss2: (0.0000) | Acc: (97.00%) (16416/16768)
Epoch: 204 | Batch_idx: 140 |  Loss: (0.0655) |  Loss2: (0.0000) | Acc: (97.00%) (17662/18048)
Epoch: 204 | Batch_idx: 150 |  Loss: (0.0654) |  Loss2: (0.0000) | Acc: (97.00%) (18916/19328)
Epoch: 204 | Batch_idx: 160 |  Loss: (0.0654) |  Loss2: (0.0000) | Acc: (97.00%) (20173/20608)
Epoch: 204 | Batch_idx: 170 |  Loss: (0.0654) |  Loss2: (0.0000) | Acc: (97.00%) (21430/21888)
Epoch: 204 | Batch_idx: 180 |  Loss: (0.0658) |  Loss2: (0.0000) | Acc: (97.00%) (22677/23168)
Epoch: 204 | Batch_idx: 190 |  Loss: (0.0661) |  Loss2: (0.0000) | Acc: (97.00%) (23929/24448)
Epoch: 204 | Batch_idx: 200 |  Loss: (0.0654) |  Loss2: (0.0000) | Acc: (97.00%) (25181/25728)
Epoch: 204 | Batch_idx: 210 |  Loss: (0.0654) |  Loss2: (0.0000) | Acc: (97.00%) (26427/27008)
Epoch: 204 | Batch_idx: 220 |  Loss: (0.0654) |  Loss2: (0.0000) | Acc: (97.00%) (27682/28288)
Epoch: 204 | Batch_idx: 230 |  Loss: (0.0656) |  Loss2: (0.0000) | Acc: (97.00%) (28932/29568)
Epoch: 204 | Batch_idx: 240 |  Loss: (0.0655) |  Loss2: (0.0000) | Acc: (97.00%) (30191/30848)
Epoch: 204 | Batch_idx: 250 |  Loss: (0.0660) |  Loss2: (0.0000) | Acc: (97.00%) (31433/32128)
Epoch: 204 | Batch_idx: 260 |  Loss: (0.0661) |  Loss2: (0.0000) | Acc: (97.00%) (32684/33408)
Epoch: 204 | Batch_idx: 270 |  Loss: (0.0660) |  Loss2: (0.0000) | Acc: (97.00%) (33941/34688)
Epoch: 204 | Batch_idx: 280 |  Loss: (0.0661) |  Loss2: (0.0000) | Acc: (97.00%) (35194/35968)
Epoch: 204 | Batch_idx: 290 |  Loss: (0.0662) |  Loss2: (0.0000) | Acc: (97.00%) (36445/37248)
Epoch: 204 | Batch_idx: 300 |  Loss: (0.0661) |  Loss2: (0.0000) | Acc: (97.00%) (37697/38528)
Epoch: 204 | Batch_idx: 310 |  Loss: (0.0661) |  Loss2: (0.0000) | Acc: (97.00%) (38951/39808)
Epoch: 204 | Batch_idx: 320 |  Loss: (0.0660) |  Loss2: (0.0000) | Acc: (97.00%) (40206/41088)
Epoch: 204 | Batch_idx: 330 |  Loss: (0.0662) |  Loss2: (0.0000) | Acc: (97.00%) (41451/42368)
Epoch: 204 | Batch_idx: 340 |  Loss: (0.0662) |  Loss2: (0.0000) | Acc: (97.00%) (42706/43648)
Epoch: 204 | Batch_idx: 350 |  Loss: (0.0662) |  Loss2: (0.0000) | Acc: (97.00%) (43961/44928)
Epoch: 204 | Batch_idx: 360 |  Loss: (0.0662) |  Loss2: (0.0000) | Acc: (97.00%) (45216/46208)
Epoch: 204 | Batch_idx: 370 |  Loss: (0.0661) |  Loss2: (0.0000) | Acc: (97.00%) (46470/47488)
Epoch: 204 | Batch_idx: 380 |  Loss: (0.0663) |  Loss2: (0.0000) | Acc: (97.00%) (47715/48768)
Epoch: 204 | Batch_idx: 390 |  Loss: (0.0664) |  Loss2: (0.0000) | Acc: (97.00%) (48916/50000)
# TEST : Loss: (0.4311) | Acc: (88.00%) (8838/10000)
percent tensor([0.5699, 0.5905, 0.5726, 0.5453, 0.5834, 0.5871, 0.5974, 0.5640, 0.5723,
        0.5822, 0.5860, 0.5833, 0.5710, 0.5565, 0.5896, 0.5663],
       device='cuda:0') torch.Size([16])
percent tensor([0.5431, 0.5437, 0.5518, 0.5546, 0.5531, 0.5409, 0.5471, 0.5561, 0.5468,
        0.5454, 0.5419, 0.5480, 0.5419, 0.5468, 0.5429, 0.5457],
       device='cuda:0') torch.Size([16])
percent tensor([0.6920, 0.7122, 0.5144, 0.4623, 0.4521, 0.6242, 0.6552, 0.5106, 0.5825,
        0.6959, 0.7033, 0.6007, 0.7263, 0.6586, 0.6736, 0.6750],
       device='cuda:0') torch.Size([16])
percent tensor([0.7121, 0.6263, 0.6934, 0.6938, 0.6931, 0.7796, 0.6654, 0.6978, 0.6637,
        0.6571, 0.6678, 0.6407, 0.6042, 0.6896, 0.6887, 0.7739],
       device='cuda:0') torch.Size([16])
percent tensor([0.6523, 0.6616, 0.6630, 0.6184, 0.6359, 0.6213, 0.6776, 0.6589, 0.6755,
        0.6457, 0.6297, 0.6235, 0.6495, 0.6569, 0.6560, 0.6185],
       device='cuda:0') torch.Size([16])
percent tensor([0.5592, 0.7199, 0.6822, 0.7404, 0.7125, 0.7550, 0.7373, 0.5884, 0.7668,
        0.6674, 0.7860, 0.8247, 0.7394, 0.8123, 0.6353, 0.5382],
       device='cuda:0') torch.Size([16])
percent tensor([0.5965, 0.7070, 0.5458, 0.4796, 0.6389, 0.7076, 0.6679, 0.4414, 0.6811,
        0.6932, 0.7390, 0.4717, 0.7021, 0.6452, 0.4352, 0.5322],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9999, 0.9999, 0.9997, 0.9999, 0.9995, 0.9999, 0.9997, 0.9999,
        0.9999, 1.0000, 0.9999, 0.9999, 0.9998, 0.9996, 0.9991],
       device='cuda:0') torch.Size([16])
Epoch: 205 | Batch_idx: 0 |  Loss: (0.0475) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 205 | Batch_idx: 10 |  Loss: (0.0531) |  Loss2: (0.0000) | Acc: (98.00%) (1383/1408)
Epoch: 205 | Batch_idx: 20 |  Loss: (0.0646) |  Loss2: (0.0000) | Acc: (97.00%) (2634/2688)
Epoch: 205 | Batch_idx: 30 |  Loss: (0.0604) |  Loss2: (0.0000) | Acc: (98.00%) (3897/3968)
Epoch: 205 | Batch_idx: 40 |  Loss: (0.0587) |  Loss2: (0.0000) | Acc: (98.00%) (5153/5248)
Epoch: 205 | Batch_idx: 50 |  Loss: (0.0596) |  Loss2: (0.0000) | Acc: (98.00%) (6408/6528)
Epoch: 205 | Batch_idx: 60 |  Loss: (0.0605) |  Loss2: (0.0000) | Acc: (98.00%) (7659/7808)
Epoch: 205 | Batch_idx: 70 |  Loss: (0.0605) |  Loss2: (0.0000) | Acc: (98.00%) (8914/9088)
Epoch: 205 | Batch_idx: 80 |  Loss: (0.0606) |  Loss2: (0.0000) | Acc: (98.00%) (10168/10368)
Epoch: 205 | Batch_idx: 90 |  Loss: (0.0608) |  Loss2: (0.0000) | Acc: (98.00%) (11423/11648)
Epoch: 205 | Batch_idx: 100 |  Loss: (0.0614) |  Loss2: (0.0000) | Acc: (98.00%) (12674/12928)
Epoch: 205 | Batch_idx: 110 |  Loss: (0.0616) |  Loss2: (0.0000) | Acc: (98.00%) (13926/14208)
Epoch: 205 | Batch_idx: 120 |  Loss: (0.0613) |  Loss2: (0.0000) | Acc: (97.00%) (15178/15488)
Epoch: 205 | Batch_idx: 130 |  Loss: (0.0627) |  Loss2: (0.0000) | Acc: (97.00%) (16420/16768)
Epoch: 205 | Batch_idx: 140 |  Loss: (0.0624) |  Loss2: (0.0000) | Acc: (97.00%) (17679/18048)
Epoch: 205 | Batch_idx: 150 |  Loss: (0.0621) |  Loss2: (0.0000) | Acc: (97.00%) (18930/19328)
Epoch: 205 | Batch_idx: 160 |  Loss: (0.0627) |  Loss2: (0.0000) | Acc: (97.00%) (20175/20608)
Epoch: 205 | Batch_idx: 170 |  Loss: (0.0630) |  Loss2: (0.0000) | Acc: (97.00%) (21422/21888)
Epoch: 205 | Batch_idx: 180 |  Loss: (0.0627) |  Loss2: (0.0000) | Acc: (97.00%) (22674/23168)
Epoch: 205 | Batch_idx: 190 |  Loss: (0.0628) |  Loss2: (0.0000) | Acc: (97.00%) (23925/24448)
Epoch: 205 | Batch_idx: 200 |  Loss: (0.0628) |  Loss2: (0.0000) | Acc: (97.00%) (25180/25728)
Epoch: 205 | Batch_idx: 210 |  Loss: (0.0628) |  Loss2: (0.0000) | Acc: (97.00%) (26435/27008)
Epoch: 205 | Batch_idx: 220 |  Loss: (0.0623) |  Loss2: (0.0000) | Acc: (97.00%) (27699/28288)
Epoch: 205 | Batch_idx: 230 |  Loss: (0.0621) |  Loss2: (0.0000) | Acc: (97.00%) (28952/29568)
Epoch: 205 | Batch_idx: 240 |  Loss: (0.0624) |  Loss2: (0.0000) | Acc: (97.00%) (30204/30848)
Epoch: 205 | Batch_idx: 250 |  Loss: (0.0630) |  Loss2: (0.0000) | Acc: (97.00%) (31453/32128)
Epoch: 205 | Batch_idx: 260 |  Loss: (0.0629) |  Loss2: (0.0000) | Acc: (97.00%) (32705/33408)
Epoch: 205 | Batch_idx: 270 |  Loss: (0.0629) |  Loss2: (0.0000) | Acc: (97.00%) (33955/34688)
Epoch: 205 | Batch_idx: 280 |  Loss: (0.0630) |  Loss2: (0.0000) | Acc: (97.00%) (35207/35968)
Epoch: 205 | Batch_idx: 290 |  Loss: (0.0635) |  Loss2: (0.0000) | Acc: (97.00%) (36453/37248)
Epoch: 205 | Batch_idx: 300 |  Loss: (0.0635) |  Loss2: (0.0000) | Acc: (97.00%) (37703/38528)
Epoch: 205 | Batch_idx: 310 |  Loss: (0.0638) |  Loss2: (0.0000) | Acc: (97.00%) (38949/39808)
Epoch: 205 | Batch_idx: 320 |  Loss: (0.0644) |  Loss2: (0.0000) | Acc: (97.00%) (40185/41088)
Epoch: 205 | Batch_idx: 330 |  Loss: (0.0652) |  Loss2: (0.0000) | Acc: (97.00%) (41427/42368)
Epoch: 205 | Batch_idx: 340 |  Loss: (0.0653) |  Loss2: (0.0000) | Acc: (97.00%) (42678/43648)
Epoch: 205 | Batch_idx: 350 |  Loss: (0.0658) |  Loss2: (0.0000) | Acc: (97.00%) (43923/44928)
Epoch: 205 | Batch_idx: 360 |  Loss: (0.0663) |  Loss2: (0.0000) | Acc: (97.00%) (45168/46208)
Epoch: 205 | Batch_idx: 370 |  Loss: (0.0666) |  Loss2: (0.0000) | Acc: (97.00%) (46409/47488)
Epoch: 205 | Batch_idx: 380 |  Loss: (0.0670) |  Loss2: (0.0000) | Acc: (97.00%) (47654/48768)
Epoch: 205 | Batch_idx: 390 |  Loss: (0.0672) |  Loss2: (0.0000) | Acc: (97.00%) (48852/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_205.pth.tar'
# TEST : Loss: (0.4294) | Acc: (88.00%) (8837/10000)
percent tensor([0.5699, 0.5880, 0.5775, 0.5455, 0.5881, 0.5884, 0.5972, 0.5650, 0.5738,
        0.5819, 0.5857, 0.5875, 0.5703, 0.5529, 0.5887, 0.5650],
       device='cuda:0') torch.Size([16])
percent tensor([0.5415, 0.5432, 0.5485, 0.5524, 0.5508, 0.5395, 0.5466, 0.5543, 0.5458,
        0.5442, 0.5413, 0.5461, 0.5410, 0.5475, 0.5414, 0.5445],
       device='cuda:0') torch.Size([16])
percent tensor([0.6813, 0.7093, 0.5177, 0.4555, 0.4476, 0.6266, 0.6425, 0.5091, 0.5719,
        0.6753, 0.6824, 0.5904, 0.7133, 0.6504, 0.6703, 0.6617],
       device='cuda:0') torch.Size([16])
percent tensor([0.7207, 0.6308, 0.7019, 0.6971, 0.7032, 0.7823, 0.6771, 0.7064, 0.6735,
        0.6596, 0.6683, 0.6451, 0.6081, 0.6972, 0.6902, 0.7747],
       device='cuda:0') torch.Size([16])
percent tensor([0.6504, 0.6637, 0.6717, 0.6181, 0.6482, 0.6266, 0.6745, 0.6572, 0.6650,
        0.6449, 0.6250, 0.6274, 0.6453, 0.6540, 0.6630, 0.6110],
       device='cuda:0') torch.Size([16])
percent tensor([0.6077, 0.7523, 0.7305, 0.7506, 0.7145, 0.7764, 0.7510, 0.6124, 0.7703,
        0.6775, 0.8015, 0.8314, 0.7511, 0.8238, 0.6808, 0.5769],
       device='cuda:0') torch.Size([16])
percent tensor([0.6154, 0.6916, 0.5834, 0.4941, 0.6329, 0.7399, 0.6705, 0.4496, 0.6741,
        0.6801, 0.7035, 0.4913, 0.7133, 0.6303, 0.4280, 0.5312],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9998, 0.9999, 0.9998, 0.9998, 0.9996, 0.9999, 0.9996, 0.9999,
        0.9999, 0.9999, 0.9999, 0.9999, 0.9998, 0.9996, 0.9993],
       device='cuda:0') torch.Size([16])
Epoch: 206 | Batch_idx: 0 |  Loss: (0.0686) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 206 | Batch_idx: 10 |  Loss: (0.0643) |  Loss2: (0.0000) | Acc: (97.00%) (1372/1408)
Epoch: 206 | Batch_idx: 20 |  Loss: (0.0582) |  Loss2: (0.0000) | Acc: (97.00%) (2634/2688)
Epoch: 206 | Batch_idx: 30 |  Loss: (0.0605) |  Loss2: (0.0000) | Acc: (97.00%) (3883/3968)
Epoch: 206 | Batch_idx: 40 |  Loss: (0.0581) |  Loss2: (0.0000) | Acc: (97.00%) (5138/5248)
Epoch: 206 | Batch_idx: 50 |  Loss: (0.0577) |  Loss2: (0.0000) | Acc: (97.00%) (6396/6528)
Epoch: 206 | Batch_idx: 60 |  Loss: (0.0585) |  Loss2: (0.0000) | Acc: (97.00%) (7645/7808)
Epoch: 206 | Batch_idx: 70 |  Loss: (0.0575) |  Loss2: (0.0000) | Acc: (97.00%) (8903/9088)
Epoch: 206 | Batch_idx: 80 |  Loss: (0.0574) |  Loss2: (0.0000) | Acc: (97.00%) (10158/10368)
Epoch: 206 | Batch_idx: 90 |  Loss: (0.0577) |  Loss2: (0.0000) | Acc: (97.00%) (11412/11648)
Epoch: 206 | Batch_idx: 100 |  Loss: (0.0590) |  Loss2: (0.0000) | Acc: (97.00%) (12659/12928)
Epoch: 206 | Batch_idx: 110 |  Loss: (0.0592) |  Loss2: (0.0000) | Acc: (97.00%) (13917/14208)
Epoch: 206 | Batch_idx: 120 |  Loss: (0.0593) |  Loss2: (0.0000) | Acc: (97.00%) (15169/15488)
Epoch: 206 | Batch_idx: 130 |  Loss: (0.0598) |  Loss2: (0.0000) | Acc: (97.00%) (16424/16768)
Epoch: 206 | Batch_idx: 140 |  Loss: (0.0616) |  Loss2: (0.0000) | Acc: (97.00%) (17663/18048)
Epoch: 206 | Batch_idx: 150 |  Loss: (0.0614) |  Loss2: (0.0000) | Acc: (97.00%) (18920/19328)
Epoch: 206 | Batch_idx: 160 |  Loss: (0.0609) |  Loss2: (0.0000) | Acc: (97.00%) (20176/20608)
Epoch: 206 | Batch_idx: 170 |  Loss: (0.0614) |  Loss2: (0.0000) | Acc: (97.00%) (21429/21888)
Epoch: 206 | Batch_idx: 180 |  Loss: (0.0614) |  Loss2: (0.0000) | Acc: (97.00%) (22682/23168)
Epoch: 206 | Batch_idx: 190 |  Loss: (0.0616) |  Loss2: (0.0000) | Acc: (97.00%) (23937/24448)
Epoch: 206 | Batch_idx: 200 |  Loss: (0.0608) |  Loss2: (0.0000) | Acc: (97.00%) (25199/25728)
Epoch: 206 | Batch_idx: 210 |  Loss: (0.0610) |  Loss2: (0.0000) | Acc: (97.00%) (26450/27008)
Epoch: 206 | Batch_idx: 220 |  Loss: (0.0606) |  Loss2: (0.0000) | Acc: (97.00%) (27712/28288)
Epoch: 206 | Batch_idx: 230 |  Loss: (0.0603) |  Loss2: (0.0000) | Acc: (97.00%) (28970/29568)
Epoch: 206 | Batch_idx: 240 |  Loss: (0.0609) |  Loss2: (0.0000) | Acc: (97.00%) (30216/30848)
Epoch: 206 | Batch_idx: 250 |  Loss: (0.0611) |  Loss2: (0.0000) | Acc: (97.00%) (31466/32128)
Epoch: 206 | Batch_idx: 260 |  Loss: (0.0615) |  Loss2: (0.0000) | Acc: (97.00%) (32716/33408)
Epoch: 206 | Batch_idx: 270 |  Loss: (0.0613) |  Loss2: (0.0000) | Acc: (97.00%) (33974/34688)
Epoch: 206 | Batch_idx: 280 |  Loss: (0.0612) |  Loss2: (0.0000) | Acc: (97.00%) (35226/35968)
Epoch: 206 | Batch_idx: 290 |  Loss: (0.0615) |  Loss2: (0.0000) | Acc: (97.00%) (36479/37248)
Epoch: 206 | Batch_idx: 300 |  Loss: (0.0613) |  Loss2: (0.0000) | Acc: (97.00%) (37735/38528)
Epoch: 206 | Batch_idx: 310 |  Loss: (0.0616) |  Loss2: (0.0000) | Acc: (97.00%) (38984/39808)
Epoch: 206 | Batch_idx: 320 |  Loss: (0.0617) |  Loss2: (0.0000) | Acc: (97.00%) (40234/41088)
Epoch: 206 | Batch_idx: 330 |  Loss: (0.0620) |  Loss2: (0.0000) | Acc: (97.00%) (41481/42368)
Epoch: 206 | Batch_idx: 340 |  Loss: (0.0624) |  Loss2: (0.0000) | Acc: (97.00%) (42730/43648)
Epoch: 206 | Batch_idx: 350 |  Loss: (0.0624) |  Loss2: (0.0000) | Acc: (97.00%) (43987/44928)
Epoch: 206 | Batch_idx: 360 |  Loss: (0.0625) |  Loss2: (0.0000) | Acc: (97.00%) (45238/46208)
Epoch: 206 | Batch_idx: 370 |  Loss: (0.0624) |  Loss2: (0.0000) | Acc: (97.00%) (46491/47488)
Epoch: 206 | Batch_idx: 380 |  Loss: (0.0630) |  Loss2: (0.0000) | Acc: (97.00%) (47738/48768)
Epoch: 206 | Batch_idx: 390 |  Loss: (0.0628) |  Loss2: (0.0000) | Acc: (97.00%) (48949/50000)
# TEST : Loss: (0.4165) | Acc: (88.00%) (8898/10000)
percent tensor([0.5701, 0.5931, 0.5711, 0.5451, 0.5832, 0.5879, 0.5986, 0.5647, 0.5742,
        0.5827, 0.5878, 0.5833, 0.5716, 0.5606, 0.5910, 0.5672],
       device='cuda:0') torch.Size([16])
percent tensor([0.5421, 0.5448, 0.5513, 0.5525, 0.5525, 0.5384, 0.5480, 0.5559, 0.5471,
        0.5459, 0.5419, 0.5485, 0.5427, 0.5478, 0.5421, 0.5446],
       device='cuda:0') torch.Size([16])
percent tensor([0.7002, 0.7175, 0.5026, 0.4922, 0.4520, 0.6549, 0.6577, 0.5091, 0.5911,
        0.6921, 0.7104, 0.5922, 0.7252, 0.6808, 0.6910, 0.6887],
       device='cuda:0') torch.Size([16])
percent tensor([0.7179, 0.6343, 0.7045, 0.7039, 0.7023, 0.7862, 0.6789, 0.7016, 0.6767,
        0.6684, 0.6796, 0.6542, 0.6161, 0.6998, 0.7040, 0.7827],
       device='cuda:0') torch.Size([16])
percent tensor([0.6590, 0.6831, 0.6719, 0.6313, 0.6426, 0.6329, 0.6823, 0.6677, 0.6725,
        0.6504, 0.6330, 0.6334, 0.6511, 0.6693, 0.6677, 0.6237],
       device='cuda:0') torch.Size([16])
percent tensor([0.6049, 0.7630, 0.6889, 0.7648, 0.7141, 0.7829, 0.7624, 0.5930, 0.7840,
        0.6855, 0.8002, 0.8229, 0.7643, 0.8208, 0.6657, 0.5762],
       device='cuda:0') torch.Size([16])
percent tensor([0.6109, 0.6857, 0.5622, 0.5243, 0.6363, 0.7329, 0.6477, 0.4593, 0.6825,
        0.6893, 0.7017, 0.4823, 0.7333, 0.6177, 0.4396, 0.5286],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9999, 0.9999, 0.9998, 0.9999, 0.9995, 0.9999, 0.9996, 0.9998,
        0.9998, 0.9999, 0.9999, 0.9999, 0.9996, 0.9996, 0.9994],
       device='cuda:0') torch.Size([16])
Epoch: 207 | Batch_idx: 0 |  Loss: (0.0722) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 207 | Batch_idx: 10 |  Loss: (0.0546) |  Loss2: (0.0000) | Acc: (98.00%) (1384/1408)
Epoch: 207 | Batch_idx: 20 |  Loss: (0.0610) |  Loss2: (0.0000) | Acc: (97.00%) (2630/2688)
Epoch: 207 | Batch_idx: 30 |  Loss: (0.0602) |  Loss2: (0.0000) | Acc: (97.00%) (3887/3968)
Epoch: 207 | Batch_idx: 40 |  Loss: (0.0588) |  Loss2: (0.0000) | Acc: (98.00%) (5144/5248)
Epoch: 207 | Batch_idx: 50 |  Loss: (0.0585) |  Loss2: (0.0000) | Acc: (98.00%) (6402/6528)
Epoch: 207 | Batch_idx: 60 |  Loss: (0.0594) |  Loss2: (0.0000) | Acc: (98.00%) (7655/7808)
Epoch: 207 | Batch_idx: 70 |  Loss: (0.0588) |  Loss2: (0.0000) | Acc: (98.00%) (8917/9088)
Epoch: 207 | Batch_idx: 80 |  Loss: (0.0580) |  Loss2: (0.0000) | Acc: (98.00%) (10179/10368)
Epoch: 207 | Batch_idx: 90 |  Loss: (0.0606) |  Loss2: (0.0000) | Acc: (98.00%) (11426/11648)
Epoch: 207 | Batch_idx: 100 |  Loss: (0.0618) |  Loss2: (0.0000) | Acc: (98.00%) (12676/12928)
Epoch: 207 | Batch_idx: 110 |  Loss: (0.0617) |  Loss2: (0.0000) | Acc: (98.00%) (13935/14208)
Epoch: 207 | Batch_idx: 120 |  Loss: (0.0619) |  Loss2: (0.0000) | Acc: (98.00%) (15189/15488)
Epoch: 207 | Batch_idx: 130 |  Loss: (0.0634) |  Loss2: (0.0000) | Acc: (97.00%) (16430/16768)
Epoch: 207 | Batch_idx: 140 |  Loss: (0.0633) |  Loss2: (0.0000) | Acc: (97.00%) (17684/18048)
Epoch: 207 | Batch_idx: 150 |  Loss: (0.0635) |  Loss2: (0.0000) | Acc: (97.00%) (18939/19328)
Epoch: 207 | Batch_idx: 160 |  Loss: (0.0634) |  Loss2: (0.0000) | Acc: (97.00%) (20195/20608)
Epoch: 207 | Batch_idx: 170 |  Loss: (0.0635) |  Loss2: (0.0000) | Acc: (97.00%) (21445/21888)
Epoch: 207 | Batch_idx: 180 |  Loss: (0.0638) |  Loss2: (0.0000) | Acc: (97.00%) (22694/23168)
Epoch: 207 | Batch_idx: 190 |  Loss: (0.0635) |  Loss2: (0.0000) | Acc: (97.00%) (23952/24448)
Epoch: 207 | Batch_idx: 200 |  Loss: (0.0633) |  Loss2: (0.0000) | Acc: (97.00%) (25212/25728)
Epoch: 207 | Batch_idx: 210 |  Loss: (0.0632) |  Loss2: (0.0000) | Acc: (97.00%) (26464/27008)
Epoch: 207 | Batch_idx: 220 |  Loss: (0.0627) |  Loss2: (0.0000) | Acc: (98.00%) (27726/28288)
Epoch: 207 | Batch_idx: 230 |  Loss: (0.0623) |  Loss2: (0.0000) | Acc: (98.00%) (28984/29568)
Epoch: 207 | Batch_idx: 240 |  Loss: (0.0619) |  Loss2: (0.0000) | Acc: (98.00%) (30241/30848)
Epoch: 207 | Batch_idx: 250 |  Loss: (0.0618) |  Loss2: (0.0000) | Acc: (98.00%) (31500/32128)
Epoch: 207 | Batch_idx: 260 |  Loss: (0.0618) |  Loss2: (0.0000) | Acc: (98.00%) (32753/33408)
Epoch: 207 | Batch_idx: 270 |  Loss: (0.0615) |  Loss2: (0.0000) | Acc: (98.00%) (34014/34688)
Epoch: 207 | Batch_idx: 280 |  Loss: (0.0613) |  Loss2: (0.0000) | Acc: (98.00%) (35273/35968)
Epoch: 207 | Batch_idx: 290 |  Loss: (0.0614) |  Loss2: (0.0000) | Acc: (98.00%) (36524/37248)
Epoch: 207 | Batch_idx: 300 |  Loss: (0.0617) |  Loss2: (0.0000) | Acc: (98.00%) (37765/38528)
Epoch: 207 | Batch_idx: 310 |  Loss: (0.0619) |  Loss2: (0.0000) | Acc: (98.00%) (39013/39808)
Epoch: 207 | Batch_idx: 320 |  Loss: (0.0625) |  Loss2: (0.0000) | Acc: (97.00%) (40254/41088)
Epoch: 207 | Batch_idx: 330 |  Loss: (0.0624) |  Loss2: (0.0000) | Acc: (97.00%) (41509/42368)
Epoch: 207 | Batch_idx: 340 |  Loss: (0.0621) |  Loss2: (0.0000) | Acc: (97.00%) (42767/43648)
Epoch: 207 | Batch_idx: 350 |  Loss: (0.0622) |  Loss2: (0.0000) | Acc: (97.00%) (44018/44928)
Epoch: 207 | Batch_idx: 360 |  Loss: (0.0623) |  Loss2: (0.0000) | Acc: (97.00%) (45274/46208)
Epoch: 207 | Batch_idx: 370 |  Loss: (0.0620) |  Loss2: (0.0000) | Acc: (97.00%) (46532/47488)
Epoch: 207 | Batch_idx: 380 |  Loss: (0.0624) |  Loss2: (0.0000) | Acc: (97.00%) (47772/48768)
Epoch: 207 | Batch_idx: 390 |  Loss: (0.0621) |  Loss2: (0.0000) | Acc: (97.00%) (48983/50000)
# TEST : Loss: (0.4408) | Acc: (88.00%) (8849/10000)
percent tensor([0.5694, 0.5886, 0.5746, 0.5448, 0.5848, 0.5865, 0.5961, 0.5655, 0.5737,
        0.5811, 0.5854, 0.5847, 0.5702, 0.5561, 0.5882, 0.5651],
       device='cuda:0') torch.Size([16])
percent tensor([0.5428, 0.5446, 0.5519, 0.5544, 0.5531, 0.5401, 0.5477, 0.5562, 0.5465,
        0.5460, 0.5415, 0.5480, 0.5425, 0.5475, 0.5428, 0.5455],
       device='cuda:0') torch.Size([16])
percent tensor([0.6918, 0.7088, 0.5148, 0.4789, 0.4505, 0.6419, 0.6449, 0.5103, 0.5846,
        0.6820, 0.6949, 0.5947, 0.7194, 0.6555, 0.6760, 0.6747],
       device='cuda:0') torch.Size([16])
percent tensor([0.7158, 0.6126, 0.6972, 0.7009, 0.6970, 0.7876, 0.6658, 0.6995, 0.6602,
        0.6494, 0.6656, 0.6352, 0.5969, 0.6878, 0.6944, 0.7763],
       device='cuda:0') torch.Size([16])
percent tensor([0.6516, 0.6770, 0.6793, 0.6161, 0.6451, 0.6160, 0.6886, 0.6660, 0.6827,
        0.6511, 0.6337, 0.6405, 0.6513, 0.6640, 0.6619, 0.6151],
       device='cuda:0') torch.Size([16])
percent tensor([0.5767, 0.7419, 0.7050, 0.7639, 0.7347, 0.7791, 0.7456, 0.5908, 0.7855,
        0.6511, 0.7842, 0.8123, 0.7357, 0.8060, 0.6364, 0.5441],
       device='cuda:0') torch.Size([16])
percent tensor([0.6163, 0.6829, 0.5775, 0.4814, 0.6615, 0.7405, 0.6503, 0.4545, 0.6768,
        0.6867, 0.7205, 0.4763, 0.7396, 0.6065, 0.4235, 0.5413],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9999, 0.9999, 0.9998, 0.9999, 0.9996, 0.9999, 0.9996, 0.9999,
        0.9998, 1.0000, 0.9999, 0.9999, 0.9998, 0.9996, 0.9996],
       device='cuda:0') torch.Size([16])
Epoch: 208 | Batch_idx: 0 |  Loss: (0.0814) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 208 | Batch_idx: 10 |  Loss: (0.0433) |  Loss2: (0.0000) | Acc: (98.00%) (1392/1408)
Epoch: 208 | Batch_idx: 20 |  Loss: (0.0616) |  Loss2: (0.0000) | Acc: (98.00%) (2636/2688)
Epoch: 208 | Batch_idx: 30 |  Loss: (0.0591) |  Loss2: (0.0000) | Acc: (98.00%) (3896/3968)
Epoch: 208 | Batch_idx: 40 |  Loss: (0.0572) |  Loss2: (0.0000) | Acc: (98.00%) (5152/5248)
Epoch: 208 | Batch_idx: 50 |  Loss: (0.0571) |  Loss2: (0.0000) | Acc: (98.00%) (6413/6528)
Epoch: 208 | Batch_idx: 60 |  Loss: (0.0564) |  Loss2: (0.0000) | Acc: (98.00%) (7676/7808)
Epoch: 208 | Batch_idx: 70 |  Loss: (0.0574) |  Loss2: (0.0000) | Acc: (98.00%) (8928/9088)
Epoch: 208 | Batch_idx: 80 |  Loss: (0.0597) |  Loss2: (0.0000) | Acc: (98.00%) (10172/10368)
Epoch: 208 | Batch_idx: 90 |  Loss: (0.0580) |  Loss2: (0.0000) | Acc: (98.00%) (11437/11648)
Epoch: 208 | Batch_idx: 100 |  Loss: (0.0577) |  Loss2: (0.0000) | Acc: (98.00%) (12687/12928)
Epoch: 208 | Batch_idx: 110 |  Loss: (0.0587) |  Loss2: (0.0000) | Acc: (98.00%) (13939/14208)
Epoch: 208 | Batch_idx: 120 |  Loss: (0.0578) |  Loss2: (0.0000) | Acc: (98.00%) (15204/15488)
Epoch: 208 | Batch_idx: 130 |  Loss: (0.0576) |  Loss2: (0.0000) | Acc: (98.00%) (16467/16768)
Epoch: 208 | Batch_idx: 140 |  Loss: (0.0572) |  Loss2: (0.0000) | Acc: (98.00%) (17724/18048)
Epoch: 208 | Batch_idx: 150 |  Loss: (0.0577) |  Loss2: (0.0000) | Acc: (98.00%) (18972/19328)
Epoch: 208 | Batch_idx: 160 |  Loss: (0.0574) |  Loss2: (0.0000) | Acc: (98.00%) (20228/20608)
Epoch: 208 | Batch_idx: 170 |  Loss: (0.0580) |  Loss2: (0.0000) | Acc: (98.00%) (21483/21888)
Epoch: 208 | Batch_idx: 180 |  Loss: (0.0576) |  Loss2: (0.0000) | Acc: (98.00%) (22745/23168)
Epoch: 208 | Batch_idx: 190 |  Loss: (0.0574) |  Loss2: (0.0000) | Acc: (98.00%) (24005/24448)
Epoch: 208 | Batch_idx: 200 |  Loss: (0.0569) |  Loss2: (0.0000) | Acc: (98.00%) (25268/25728)
Epoch: 208 | Batch_idx: 210 |  Loss: (0.0570) |  Loss2: (0.0000) | Acc: (98.00%) (26516/27008)
Epoch: 208 | Batch_idx: 220 |  Loss: (0.0568) |  Loss2: (0.0000) | Acc: (98.00%) (27769/28288)
Epoch: 208 | Batch_idx: 230 |  Loss: (0.0572) |  Loss2: (0.0000) | Acc: (98.00%) (29019/29568)
Epoch: 208 | Batch_idx: 240 |  Loss: (0.0572) |  Loss2: (0.0000) | Acc: (98.00%) (30278/30848)
Epoch: 208 | Batch_idx: 250 |  Loss: (0.0570) |  Loss2: (0.0000) | Acc: (98.00%) (31537/32128)
Epoch: 208 | Batch_idx: 260 |  Loss: (0.0573) |  Loss2: (0.0000) | Acc: (98.00%) (32794/33408)
Epoch: 208 | Batch_idx: 270 |  Loss: (0.0571) |  Loss2: (0.0000) | Acc: (98.00%) (34050/34688)
Epoch: 208 | Batch_idx: 280 |  Loss: (0.0570) |  Loss2: (0.0000) | Acc: (98.00%) (35306/35968)
Epoch: 208 | Batch_idx: 290 |  Loss: (0.0571) |  Loss2: (0.0000) | Acc: (98.00%) (36560/37248)
Epoch: 208 | Batch_idx: 300 |  Loss: (0.0575) |  Loss2: (0.0000) | Acc: (98.00%) (37812/38528)
Epoch: 208 | Batch_idx: 310 |  Loss: (0.0576) |  Loss2: (0.0000) | Acc: (98.00%) (39061/39808)
Epoch: 208 | Batch_idx: 320 |  Loss: (0.0576) |  Loss2: (0.0000) | Acc: (98.00%) (40316/41088)
Epoch: 208 | Batch_idx: 330 |  Loss: (0.0579) |  Loss2: (0.0000) | Acc: (98.00%) (41566/42368)
Epoch: 208 | Batch_idx: 340 |  Loss: (0.0583) |  Loss2: (0.0000) | Acc: (98.00%) (42818/43648)
Epoch: 208 | Batch_idx: 350 |  Loss: (0.0589) |  Loss2: (0.0000) | Acc: (98.00%) (44056/44928)
Epoch: 208 | Batch_idx: 360 |  Loss: (0.0590) |  Loss2: (0.0000) | Acc: (98.00%) (45307/46208)
Epoch: 208 | Batch_idx: 370 |  Loss: (0.0594) |  Loss2: (0.0000) | Acc: (98.00%) (46552/47488)
Epoch: 208 | Batch_idx: 380 |  Loss: (0.0597) |  Loss2: (0.0000) | Acc: (98.00%) (47799/48768)
Epoch: 208 | Batch_idx: 390 |  Loss: (0.0604) |  Loss2: (0.0000) | Acc: (97.00%) (48991/50000)
# TEST : Loss: (0.4891) | Acc: (87.00%) (8751/10000)
percent tensor([0.5701, 0.5874, 0.5773, 0.5443, 0.5875, 0.5872, 0.5964, 0.5651, 0.5739,
        0.5821, 0.5852, 0.5879, 0.5710, 0.5518, 0.5883, 0.5642],
       device='cuda:0') torch.Size([16])
percent tensor([0.5429, 0.5457, 0.5511, 0.5540, 0.5532, 0.5410, 0.5486, 0.5562, 0.5472,
        0.5462, 0.5427, 0.5480, 0.5428, 0.5496, 0.5435, 0.5461],
       device='cuda:0') torch.Size([16])
percent tensor([0.6947, 0.7161, 0.5108, 0.4666, 0.4452, 0.6344, 0.6514, 0.5205, 0.5916,
        0.6905, 0.7032, 0.5913, 0.7271, 0.6746, 0.6802, 0.6757],
       device='cuda:0') torch.Size([16])
percent tensor([0.7150, 0.6208, 0.6985, 0.6931, 0.6937, 0.7900, 0.6670, 0.6988, 0.6692,
        0.6535, 0.6712, 0.6429, 0.6026, 0.7017, 0.6968, 0.7789],
       device='cuda:0') torch.Size([16])
percent tensor([0.6577, 0.6789, 0.6916, 0.6321, 0.6554, 0.6176, 0.6901, 0.6760, 0.6830,
        0.6526, 0.6423, 0.6434, 0.6548, 0.6653, 0.6649, 0.6209],
       device='cuda:0') torch.Size([16])
percent tensor([0.6029, 0.7466, 0.6948, 0.7299, 0.7240, 0.7868, 0.7565, 0.5826, 0.7864,
        0.6690, 0.8109, 0.8240, 0.7610, 0.8266, 0.6360, 0.5751],
       device='cuda:0') torch.Size([16])
percent tensor([0.6310, 0.7037, 0.5709, 0.4822, 0.6685, 0.7294, 0.6771, 0.4644, 0.6956,
        0.7002, 0.7375, 0.4767, 0.7513, 0.6324, 0.4396, 0.5474],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9998, 0.9999, 0.9998, 0.9999, 0.9995, 0.9999, 0.9995, 0.9999,
        0.9999, 0.9999, 0.9999, 0.9999, 0.9998, 0.9996, 0.9992],
       device='cuda:0') torch.Size([16])
Epoch: 209 | Batch_idx: 0 |  Loss: (0.0416) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 209 | Batch_idx: 10 |  Loss: (0.0512) |  Loss2: (0.0000) | Acc: (98.00%) (1382/1408)
Epoch: 209 | Batch_idx: 20 |  Loss: (0.0524) |  Loss2: (0.0000) | Acc: (98.00%) (2639/2688)
Epoch: 209 | Batch_idx: 30 |  Loss: (0.0527) |  Loss2: (0.0000) | Acc: (98.00%) (3897/3968)
Epoch: 209 | Batch_idx: 40 |  Loss: (0.0549) |  Loss2: (0.0000) | Acc: (98.00%) (5147/5248)
Epoch: 209 | Batch_idx: 50 |  Loss: (0.0556) |  Loss2: (0.0000) | Acc: (98.00%) (6405/6528)
Epoch: 209 | Batch_idx: 60 |  Loss: (0.0568) |  Loss2: (0.0000) | Acc: (98.00%) (7652/7808)
Epoch: 209 | Batch_idx: 70 |  Loss: (0.0556) |  Loss2: (0.0000) | Acc: (98.00%) (8914/9088)
Epoch: 209 | Batch_idx: 80 |  Loss: (0.0576) |  Loss2: (0.0000) | Acc: (98.00%) (10162/10368)
Epoch: 209 | Batch_idx: 90 |  Loss: (0.0586) |  Loss2: (0.0000) | Acc: (97.00%) (11412/11648)
Epoch: 209 | Batch_idx: 100 |  Loss: (0.0587) |  Loss2: (0.0000) | Acc: (97.00%) (12666/12928)
Epoch: 209 | Batch_idx: 110 |  Loss: (0.0582) |  Loss2: (0.0000) | Acc: (98.00%) (13929/14208)
Epoch: 209 | Batch_idx: 120 |  Loss: (0.0581) |  Loss2: (0.0000) | Acc: (98.00%) (15184/15488)
Epoch: 209 | Batch_idx: 130 |  Loss: (0.0576) |  Loss2: (0.0000) | Acc: (98.00%) (16437/16768)
Epoch: 209 | Batch_idx: 140 |  Loss: (0.0577) |  Loss2: (0.0000) | Acc: (98.00%) (17694/18048)
Epoch: 209 | Batch_idx: 150 |  Loss: (0.0584) |  Loss2: (0.0000) | Acc: (98.00%) (18945/19328)
Epoch: 209 | Batch_idx: 160 |  Loss: (0.0581) |  Loss2: (0.0000) | Acc: (98.00%) (20206/20608)
Epoch: 209 | Batch_idx: 170 |  Loss: (0.0583) |  Loss2: (0.0000) | Acc: (98.00%) (21462/21888)
Epoch: 209 | Batch_idx: 180 |  Loss: (0.0583) |  Loss2: (0.0000) | Acc: (98.00%) (22720/23168)
Epoch: 209 | Batch_idx: 190 |  Loss: (0.0579) |  Loss2: (0.0000) | Acc: (98.00%) (23985/24448)
Epoch: 209 | Batch_idx: 200 |  Loss: (0.0583) |  Loss2: (0.0000) | Acc: (98.00%) (25232/25728)
Epoch: 209 | Batch_idx: 210 |  Loss: (0.0595) |  Loss2: (0.0000) | Acc: (98.00%) (26471/27008)
Epoch: 209 | Batch_idx: 220 |  Loss: (0.0603) |  Loss2: (0.0000) | Acc: (97.00%) (27721/28288)
Epoch: 209 | Batch_idx: 230 |  Loss: (0.0603) |  Loss2: (0.0000) | Acc: (97.00%) (28976/29568)
Epoch: 209 | Batch_idx: 240 |  Loss: (0.0598) |  Loss2: (0.0000) | Acc: (98.00%) (30236/30848)
Epoch: 209 | Batch_idx: 250 |  Loss: (0.0599) |  Loss2: (0.0000) | Acc: (98.00%) (31494/32128)
Epoch: 209 | Batch_idx: 260 |  Loss: (0.0606) |  Loss2: (0.0000) | Acc: (97.00%) (32738/33408)
Epoch: 209 | Batch_idx: 270 |  Loss: (0.0604) |  Loss2: (0.0000) | Acc: (97.00%) (33994/34688)
Epoch: 209 | Batch_idx: 280 |  Loss: (0.0612) |  Loss2: (0.0000) | Acc: (97.00%) (35242/35968)
Epoch: 209 | Batch_idx: 290 |  Loss: (0.0621) |  Loss2: (0.0000) | Acc: (97.00%) (36489/37248)
Epoch: 209 | Batch_idx: 300 |  Loss: (0.0626) |  Loss2: (0.0000) | Acc: (97.00%) (37733/38528)
Epoch: 209 | Batch_idx: 310 |  Loss: (0.0632) |  Loss2: (0.0000) | Acc: (97.00%) (38978/39808)
Epoch: 209 | Batch_idx: 320 |  Loss: (0.0630) |  Loss2: (0.0000) | Acc: (97.00%) (40235/41088)
Epoch: 209 | Batch_idx: 330 |  Loss: (0.0627) |  Loss2: (0.0000) | Acc: (97.00%) (41497/42368)
Epoch: 209 | Batch_idx: 340 |  Loss: (0.0627) |  Loss2: (0.0000) | Acc: (97.00%) (42747/43648)
Epoch: 209 | Batch_idx: 350 |  Loss: (0.0627) |  Loss2: (0.0000) | Acc: (97.00%) (43995/44928)
Epoch: 209 | Batch_idx: 360 |  Loss: (0.0627) |  Loss2: (0.0000) | Acc: (97.00%) (45241/46208)
Epoch: 209 | Batch_idx: 370 |  Loss: (0.0628) |  Loss2: (0.0000) | Acc: (97.00%) (46494/47488)
Epoch: 209 | Batch_idx: 380 |  Loss: (0.0628) |  Loss2: (0.0000) | Acc: (97.00%) (47746/48768)
Epoch: 209 | Batch_idx: 390 |  Loss: (0.0627) |  Loss2: (0.0000) | Acc: (97.00%) (48949/50000)
# TEST : Loss: (0.5045) | Acc: (87.00%) (8747/10000)
percent tensor([0.5701, 0.5885, 0.5786, 0.5461, 0.5878, 0.5873, 0.5970, 0.5664, 0.5723,
        0.5831, 0.5848, 0.5882, 0.5703, 0.5525, 0.5890, 0.5650],
       device='cuda:0') torch.Size([16])
percent tensor([0.5418, 0.5441, 0.5517, 0.5531, 0.5524, 0.5384, 0.5475, 0.5563, 0.5468,
        0.5454, 0.5415, 0.5479, 0.5423, 0.5480, 0.5415, 0.5446],
       device='cuda:0') torch.Size([16])
percent tensor([0.6919, 0.7097, 0.5024, 0.4694, 0.4473, 0.6475, 0.6467, 0.4929, 0.5743,
        0.6756, 0.6953, 0.5846, 0.7167, 0.6587, 0.6782, 0.6766],
       device='cuda:0') torch.Size([16])
percent tensor([0.7121, 0.6207, 0.6991, 0.6913, 0.6952, 0.7815, 0.6670, 0.7003, 0.6752,
        0.6508, 0.6697, 0.6456, 0.6101, 0.6913, 0.6873, 0.7729],
       device='cuda:0') torch.Size([16])
percent tensor([0.6574, 0.6813, 0.6840, 0.6176, 0.6440, 0.6304, 0.6921, 0.6718, 0.6812,
        0.6599, 0.6403, 0.6370, 0.6546, 0.6687, 0.6681, 0.6197],
       device='cuda:0') torch.Size([16])
percent tensor([0.5922, 0.7557, 0.7053, 0.7311, 0.7157, 0.7755, 0.7571, 0.5981, 0.7820,
        0.6581, 0.8156, 0.8269, 0.7493, 0.8244, 0.6602, 0.5595],
       device='cuda:0') torch.Size([16])
percent tensor([0.6316, 0.7011, 0.5859, 0.4702, 0.6495, 0.7309, 0.6806, 0.4469, 0.6804,
        0.6830, 0.7352, 0.4715, 0.7360, 0.6316, 0.4461, 0.5617],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9999, 0.9999, 0.9997, 0.9999, 0.9996, 0.9999, 0.9997, 0.9999,
        0.9998, 1.0000, 0.9999, 0.9999, 0.9998, 0.9997, 0.9993],
       device='cuda:0') torch.Size([16])
Epoch: 210 | Batch_idx: 0 |  Loss: (0.0736) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 210 | Batch_idx: 10 |  Loss: (0.0561) |  Loss2: (0.0000) | Acc: (98.00%) (1385/1408)
Epoch: 210 | Batch_idx: 20 |  Loss: (0.0530) |  Loss2: (0.0000) | Acc: (98.00%) (2644/2688)
Epoch: 210 | Batch_idx: 30 |  Loss: (0.0562) |  Loss2: (0.0000) | Acc: (98.00%) (3896/3968)
Epoch: 210 | Batch_idx: 40 |  Loss: (0.0582) |  Loss2: (0.0000) | Acc: (98.00%) (5145/5248)
Epoch: 210 | Batch_idx: 50 |  Loss: (0.0590) |  Loss2: (0.0000) | Acc: (98.00%) (6401/6528)
Epoch: 210 | Batch_idx: 60 |  Loss: (0.0577) |  Loss2: (0.0000) | Acc: (98.00%) (7664/7808)
Epoch: 210 | Batch_idx: 70 |  Loss: (0.0576) |  Loss2: (0.0000) | Acc: (98.00%) (8918/9088)
Epoch: 210 | Batch_idx: 80 |  Loss: (0.0566) |  Loss2: (0.0000) | Acc: (98.00%) (10182/10368)
Epoch: 210 | Batch_idx: 90 |  Loss: (0.0563) |  Loss2: (0.0000) | Acc: (98.00%) (11439/11648)
Epoch: 210 | Batch_idx: 100 |  Loss: (0.0567) |  Loss2: (0.0000) | Acc: (98.00%) (12696/12928)
Epoch: 210 | Batch_idx: 110 |  Loss: (0.0562) |  Loss2: (0.0000) | Acc: (98.00%) (13951/14208)
Epoch: 210 | Batch_idx: 120 |  Loss: (0.0560) |  Loss2: (0.0000) | Acc: (98.00%) (15208/15488)
Epoch: 210 | Batch_idx: 130 |  Loss: (0.0556) |  Loss2: (0.0000) | Acc: (98.00%) (16469/16768)
Epoch: 210 | Batch_idx: 140 |  Loss: (0.0550) |  Loss2: (0.0000) | Acc: (98.00%) (17729/18048)
Epoch: 210 | Batch_idx: 150 |  Loss: (0.0561) |  Loss2: (0.0000) | Acc: (98.00%) (18979/19328)
Epoch: 210 | Batch_idx: 160 |  Loss: (0.0567) |  Loss2: (0.0000) | Acc: (98.00%) (20229/20608)
Epoch: 210 | Batch_idx: 170 |  Loss: (0.0566) |  Loss2: (0.0000) | Acc: (98.00%) (21484/21888)
Epoch: 210 | Batch_idx: 180 |  Loss: (0.0569) |  Loss2: (0.0000) | Acc: (98.00%) (22734/23168)
Epoch: 210 | Batch_idx: 190 |  Loss: (0.0570) |  Loss2: (0.0000) | Acc: (98.00%) (23986/24448)
Epoch: 210 | Batch_idx: 200 |  Loss: (0.0573) |  Loss2: (0.0000) | Acc: (98.00%) (25238/25728)
Epoch: 210 | Batch_idx: 210 |  Loss: (0.0571) |  Loss2: (0.0000) | Acc: (98.00%) (26497/27008)
Epoch: 210 | Batch_idx: 220 |  Loss: (0.0576) |  Loss2: (0.0000) | Acc: (98.00%) (27743/28288)
Epoch: 210 | Batch_idx: 230 |  Loss: (0.0575) |  Loss2: (0.0000) | Acc: (98.00%) (29000/29568)
Epoch: 210 | Batch_idx: 240 |  Loss: (0.0579) |  Loss2: (0.0000) | Acc: (98.00%) (30257/30848)
Epoch: 210 | Batch_idx: 250 |  Loss: (0.0583) |  Loss2: (0.0000) | Acc: (98.00%) (31506/32128)
Epoch: 210 | Batch_idx: 260 |  Loss: (0.0588) |  Loss2: (0.0000) | Acc: (98.00%) (32753/33408)
Epoch: 210 | Batch_idx: 270 |  Loss: (0.0589) |  Loss2: (0.0000) | Acc: (98.00%) (34007/34688)
Epoch: 210 | Batch_idx: 280 |  Loss: (0.0593) |  Loss2: (0.0000) | Acc: (98.00%) (35258/35968)
Epoch: 210 | Batch_idx: 290 |  Loss: (0.0594) |  Loss2: (0.0000) | Acc: (98.00%) (36513/37248)
Epoch: 210 | Batch_idx: 300 |  Loss: (0.0593) |  Loss2: (0.0000) | Acc: (98.00%) (37769/38528)
Epoch: 210 | Batch_idx: 310 |  Loss: (0.0593) |  Loss2: (0.0000) | Acc: (98.00%) (39024/39808)
Epoch: 210 | Batch_idx: 320 |  Loss: (0.0591) |  Loss2: (0.0000) | Acc: (98.00%) (40281/41088)
Epoch: 210 | Batch_idx: 330 |  Loss: (0.0591) |  Loss2: (0.0000) | Acc: (98.00%) (41535/42368)
Epoch: 210 | Batch_idx: 340 |  Loss: (0.0589) |  Loss2: (0.0000) | Acc: (98.00%) (42795/43648)
Epoch: 210 | Batch_idx: 350 |  Loss: (0.0590) |  Loss2: (0.0000) | Acc: (98.00%) (44049/44928)
Epoch: 210 | Batch_idx: 360 |  Loss: (0.0590) |  Loss2: (0.0000) | Acc: (98.00%) (45307/46208)
Epoch: 210 | Batch_idx: 370 |  Loss: (0.0589) |  Loss2: (0.0000) | Acc: (98.00%) (46562/47488)
Epoch: 210 | Batch_idx: 380 |  Loss: (0.0589) |  Loss2: (0.0000) | Acc: (98.00%) (47817/48768)
Epoch: 210 | Batch_idx: 390 |  Loss: (0.0593) |  Loss2: (0.0000) | Acc: (98.00%) (49024/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_210.pth.tar'
# TEST : Loss: (0.4174) | Acc: (89.00%) (8930/10000)
percent tensor([0.5716, 0.5904, 0.5820, 0.5492, 0.5922, 0.5883, 0.5991, 0.5690, 0.5755,
        0.5852, 0.5858, 0.5922, 0.5720, 0.5543, 0.5904, 0.5663],
       device='cuda:0') torch.Size([16])
percent tensor([0.5414, 0.5436, 0.5495, 0.5526, 0.5513, 0.5392, 0.5475, 0.5550, 0.5460,
        0.5446, 0.5413, 0.5466, 0.5414, 0.5480, 0.5418, 0.5446],
       device='cuda:0') torch.Size([16])
percent tensor([0.6856, 0.7044, 0.5085, 0.4492, 0.4480, 0.6242, 0.6455, 0.4913, 0.5808,
        0.6779, 0.6904, 0.5924, 0.7107, 0.6597, 0.6659, 0.6623],
       device='cuda:0') torch.Size([16])
percent tensor([0.7183, 0.6180, 0.6895, 0.6934, 0.6965, 0.7848, 0.6708, 0.6985, 0.6753,
        0.6520, 0.6736, 0.6345, 0.6028, 0.6944, 0.6922, 0.7743],
       device='cuda:0') torch.Size([16])
percent tensor([0.6671, 0.6821, 0.6828, 0.6186, 0.6510, 0.6230, 0.6896, 0.6632, 0.6848,
        0.6581, 0.6516, 0.6401, 0.6669, 0.6739, 0.6688, 0.6263],
       device='cuda:0') torch.Size([16])
percent tensor([0.6103, 0.7562, 0.7146, 0.7676, 0.7288, 0.7751, 0.7675, 0.6363, 0.7949,
        0.6694, 0.8133, 0.8366, 0.7596, 0.8259, 0.6528, 0.5598],
       device='cuda:0') torch.Size([16])
percent tensor([0.6181, 0.6991, 0.6084, 0.4840, 0.6443, 0.7239, 0.6789, 0.4671, 0.6834,
        0.6805, 0.7192, 0.4482, 0.7366, 0.6172, 0.4300, 0.5361],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9999, 0.9999, 0.9998, 0.9999, 0.9997, 0.9999, 0.9998, 0.9999,
        0.9999, 0.9999, 0.9999, 0.9999, 0.9998, 0.9996, 0.9995],
       device='cuda:0') torch.Size([16])
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(185.4476, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(836.5211, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(832.6008, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1508.5042, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(474.4810, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2315.6191, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4233.8999, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1344.2539, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6312.2183, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11511.6934, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3775.1570, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(15922.1934, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 211 | Batch_idx: 0 |  Loss: (0.0730) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 211 | Batch_idx: 10 |  Loss: (0.0747) |  Loss2: (0.0000) | Acc: (97.00%) (1366/1408)
Epoch: 211 | Batch_idx: 20 |  Loss: (0.0669) |  Loss2: (0.0000) | Acc: (97.00%) (2617/2688)
Epoch: 211 | Batch_idx: 30 |  Loss: (0.0629) |  Loss2: (0.0000) | Acc: (97.00%) (3872/3968)
Epoch: 211 | Batch_idx: 40 |  Loss: (0.0618) |  Loss2: (0.0000) | Acc: (97.00%) (5124/5248)
Epoch: 211 | Batch_idx: 50 |  Loss: (0.0610) |  Loss2: (0.0000) | Acc: (97.00%) (6378/6528)
Epoch: 211 | Batch_idx: 60 |  Loss: (0.0603) |  Loss2: (0.0000) | Acc: (97.00%) (7633/7808)
Epoch: 211 | Batch_idx: 70 |  Loss: (0.0606) |  Loss2: (0.0000) | Acc: (97.00%) (8885/9088)
Epoch: 211 | Batch_idx: 80 |  Loss: (0.0610) |  Loss2: (0.0000) | Acc: (97.00%) (10141/10368)
Epoch: 211 | Batch_idx: 90 |  Loss: (0.0606) |  Loss2: (0.0000) | Acc: (97.00%) (11396/11648)
Epoch: 211 | Batch_idx: 100 |  Loss: (0.0587) |  Loss2: (0.0000) | Acc: (97.00%) (12660/12928)
Epoch: 211 | Batch_idx: 110 |  Loss: (0.0572) |  Loss2: (0.0000) | Acc: (97.00%) (13922/14208)
Epoch: 211 | Batch_idx: 120 |  Loss: (0.0564) |  Loss2: (0.0000) | Acc: (98.00%) (15182/15488)
Epoch: 211 | Batch_idx: 130 |  Loss: (0.0574) |  Loss2: (0.0000) | Acc: (97.00%) (16432/16768)
Epoch: 211 | Batch_idx: 140 |  Loss: (0.0568) |  Loss2: (0.0000) | Acc: (98.00%) (17693/18048)
Epoch: 211 | Batch_idx: 150 |  Loss: (0.0566) |  Loss2: (0.0000) | Acc: (98.00%) (18949/19328)
Epoch: 211 | Batch_idx: 160 |  Loss: (0.0565) |  Loss2: (0.0000) | Acc: (98.00%) (20206/20608)
Epoch: 211 | Batch_idx: 170 |  Loss: (0.0566) |  Loss2: (0.0000) | Acc: (98.00%) (21464/21888)
Epoch: 211 | Batch_idx: 180 |  Loss: (0.0576) |  Loss2: (0.0000) | Acc: (98.00%) (22712/23168)
Epoch: 211 | Batch_idx: 190 |  Loss: (0.0574) |  Loss2: (0.0000) | Acc: (98.00%) (23972/24448)
Epoch: 211 | Batch_idx: 200 |  Loss: (0.0576) |  Loss2: (0.0000) | Acc: (98.00%) (25226/25728)
Epoch: 211 | Batch_idx: 210 |  Loss: (0.0581) |  Loss2: (0.0000) | Acc: (98.00%) (26476/27008)
Epoch: 211 | Batch_idx: 220 |  Loss: (0.0584) |  Loss2: (0.0000) | Acc: (98.00%) (27724/28288)
Epoch: 211 | Batch_idx: 230 |  Loss: (0.0583) |  Loss2: (0.0000) | Acc: (98.00%) (28981/29568)
Epoch: 211 | Batch_idx: 240 |  Loss: (0.0580) |  Loss2: (0.0000) | Acc: (98.00%) (30239/30848)
Epoch: 211 | Batch_idx: 250 |  Loss: (0.0578) |  Loss2: (0.0000) | Acc: (98.00%) (31500/32128)
Epoch: 211 | Batch_idx: 260 |  Loss: (0.0579) |  Loss2: (0.0000) | Acc: (98.00%) (32752/33408)
Epoch: 211 | Batch_idx: 270 |  Loss: (0.0582) |  Loss2: (0.0000) | Acc: (98.00%) (34004/34688)
Epoch: 211 | Batch_idx: 280 |  Loss: (0.0586) |  Loss2: (0.0000) | Acc: (98.00%) (35252/35968)
Epoch: 211 | Batch_idx: 290 |  Loss: (0.0587) |  Loss2: (0.0000) | Acc: (97.00%) (36499/37248)
Epoch: 211 | Batch_idx: 300 |  Loss: (0.0583) |  Loss2: (0.0000) | Acc: (98.00%) (37759/38528)
Epoch: 211 | Batch_idx: 310 |  Loss: (0.0588) |  Loss2: (0.0000) | Acc: (97.00%) (39007/39808)
Epoch: 211 | Batch_idx: 320 |  Loss: (0.0588) |  Loss2: (0.0000) | Acc: (97.00%) (40261/41088)
Epoch: 211 | Batch_idx: 330 |  Loss: (0.0589) |  Loss2: (0.0000) | Acc: (97.00%) (41516/42368)
Epoch: 211 | Batch_idx: 340 |  Loss: (0.0590) |  Loss2: (0.0000) | Acc: (97.00%) (42770/43648)
Epoch: 211 | Batch_idx: 350 |  Loss: (0.0591) |  Loss2: (0.0000) | Acc: (97.00%) (44027/44928)
Epoch: 211 | Batch_idx: 360 |  Loss: (0.0593) |  Loss2: (0.0000) | Acc: (97.00%) (45277/46208)
Epoch: 211 | Batch_idx: 370 |  Loss: (0.0595) |  Loss2: (0.0000) | Acc: (97.00%) (46531/47488)
Epoch: 211 | Batch_idx: 380 |  Loss: (0.0593) |  Loss2: (0.0000) | Acc: (97.00%) (47790/48768)
Epoch: 211 | Batch_idx: 390 |  Loss: (0.0598) |  Loss2: (0.0000) | Acc: (97.00%) (48986/50000)
# TEST : Loss: (0.4360) | Acc: (88.00%) (8878/10000)
percent tensor([0.5706, 0.5905, 0.5790, 0.5457, 0.5895, 0.5884, 0.5995, 0.5657, 0.5757,
        0.5837, 0.5869, 0.5901, 0.5709, 0.5565, 0.5894, 0.5658],
       device='cuda:0') torch.Size([16])
percent tensor([0.5423, 0.5443, 0.5520, 0.5546, 0.5534, 0.5395, 0.5478, 0.5569, 0.5465,
        0.5454, 0.5412, 0.5483, 0.5421, 0.5478, 0.5422, 0.5450],
       device='cuda:0') torch.Size([16])
percent tensor([0.6995, 0.7118, 0.5157, 0.4775, 0.4583, 0.6518, 0.6556, 0.5122, 0.6017,
        0.6868, 0.7088, 0.5985, 0.7242, 0.6671, 0.6850, 0.6863],
       device='cuda:0') torch.Size([16])
percent tensor([0.7244, 0.6219, 0.7125, 0.6995, 0.7101, 0.7905, 0.6716, 0.7117, 0.6789,
        0.6530, 0.6702, 0.6496, 0.6051, 0.6980, 0.6962, 0.7799],
       device='cuda:0') torch.Size([16])
percent tensor([0.6577, 0.6852, 0.6689, 0.6320, 0.6445, 0.6217, 0.6930, 0.6592, 0.6840,
        0.6524, 0.6425, 0.6312, 0.6573, 0.6671, 0.6673, 0.6296],
       device='cuda:0') torch.Size([16])
percent tensor([0.5889, 0.7258, 0.7205, 0.7473, 0.7312, 0.7781, 0.7519, 0.5932, 0.7839,
        0.6806, 0.8011, 0.8285, 0.7526, 0.8103, 0.6384, 0.5654],
       device='cuda:0') torch.Size([16])
percent tensor([0.6143, 0.6966, 0.6137, 0.5111, 0.6434, 0.7333, 0.6806, 0.4746, 0.6839,
        0.7068, 0.7414, 0.5040, 0.7448, 0.6142, 0.4527, 0.5296],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9998, 0.9999, 0.9999, 0.9999, 0.9995, 1.0000, 0.9996, 0.9999,
        0.9999, 1.0000, 1.0000, 0.9999, 0.9997, 0.9996, 0.9995],
       device='cuda:0') torch.Size([16])
Epoch: 212 | Batch_idx: 0 |  Loss: (0.0321) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 212 | Batch_idx: 10 |  Loss: (0.0620) |  Loss2: (0.0000) | Acc: (97.00%) (1379/1408)
Epoch: 212 | Batch_idx: 20 |  Loss: (0.0576) |  Loss2: (0.0000) | Acc: (97.00%) (2632/2688)
Epoch: 212 | Batch_idx: 30 |  Loss: (0.0563) |  Loss2: (0.0000) | Acc: (97.00%) (3887/3968)
Epoch: 212 | Batch_idx: 40 |  Loss: (0.0565) |  Loss2: (0.0000) | Acc: (97.00%) (5143/5248)
Epoch: 212 | Batch_idx: 50 |  Loss: (0.0565) |  Loss2: (0.0000) | Acc: (98.00%) (6399/6528)
Epoch: 212 | Batch_idx: 60 |  Loss: (0.0567) |  Loss2: (0.0000) | Acc: (98.00%) (7652/7808)
Epoch: 212 | Batch_idx: 70 |  Loss: (0.0586) |  Loss2: (0.0000) | Acc: (97.00%) (8903/9088)
Epoch: 212 | Batch_idx: 80 |  Loss: (0.0596) |  Loss2: (0.0000) | Acc: (97.00%) (10153/10368)
Epoch: 212 | Batch_idx: 90 |  Loss: (0.0602) |  Loss2: (0.0000) | Acc: (97.00%) (11404/11648)
Epoch: 212 | Batch_idx: 100 |  Loss: (0.0603) |  Loss2: (0.0000) | Acc: (97.00%) (12653/12928)
Epoch: 212 | Batch_idx: 110 |  Loss: (0.0599) |  Loss2: (0.0000) | Acc: (97.00%) (13908/14208)
Epoch: 212 | Batch_idx: 120 |  Loss: (0.0591) |  Loss2: (0.0000) | Acc: (97.00%) (15169/15488)
Epoch: 212 | Batch_idx: 130 |  Loss: (0.0588) |  Loss2: (0.0000) | Acc: (97.00%) (16427/16768)
Epoch: 212 | Batch_idx: 140 |  Loss: (0.0584) |  Loss2: (0.0000) | Acc: (97.00%) (17686/18048)
Epoch: 212 | Batch_idx: 150 |  Loss: (0.0581) |  Loss2: (0.0000) | Acc: (97.00%) (18940/19328)
Epoch: 212 | Batch_idx: 160 |  Loss: (0.0579) |  Loss2: (0.0000) | Acc: (98.00%) (20200/20608)
Epoch: 212 | Batch_idx: 170 |  Loss: (0.0576) |  Loss2: (0.0000) | Acc: (98.00%) (21456/21888)
Epoch: 212 | Batch_idx: 180 |  Loss: (0.0574) |  Loss2: (0.0000) | Acc: (98.00%) (22713/23168)
Epoch: 212 | Batch_idx: 190 |  Loss: (0.0578) |  Loss2: (0.0000) | Acc: (98.00%) (23965/24448)
Epoch: 212 | Batch_idx: 200 |  Loss: (0.0584) |  Loss2: (0.0000) | Acc: (97.00%) (25209/25728)
Epoch: 212 | Batch_idx: 210 |  Loss: (0.0582) |  Loss2: (0.0000) | Acc: (97.00%) (26466/27008)
Epoch: 212 | Batch_idx: 220 |  Loss: (0.0582) |  Loss2: (0.0000) | Acc: (97.00%) (27712/28288)
Epoch: 212 | Batch_idx: 230 |  Loss: (0.0577) |  Loss2: (0.0000) | Acc: (97.00%) (28976/29568)
Epoch: 212 | Batch_idx: 240 |  Loss: (0.0579) |  Loss2: (0.0000) | Acc: (97.00%) (30231/30848)
Epoch: 212 | Batch_idx: 250 |  Loss: (0.0576) |  Loss2: (0.0000) | Acc: (98.00%) (31491/32128)
Epoch: 212 | Batch_idx: 260 |  Loss: (0.0575) |  Loss2: (0.0000) | Acc: (98.00%) (32743/33408)
Epoch: 212 | Batch_idx: 270 |  Loss: (0.0577) |  Loss2: (0.0000) | Acc: (97.00%) (33994/34688)
Epoch: 212 | Batch_idx: 280 |  Loss: (0.0575) |  Loss2: (0.0000) | Acc: (98.00%) (35253/35968)
Epoch: 212 | Batch_idx: 290 |  Loss: (0.0580) |  Loss2: (0.0000) | Acc: (97.00%) (36499/37248)
Epoch: 212 | Batch_idx: 300 |  Loss: (0.0580) |  Loss2: (0.0000) | Acc: (97.00%) (37755/38528)
Epoch: 212 | Batch_idx: 310 |  Loss: (0.0579) |  Loss2: (0.0000) | Acc: (97.00%) (39006/39808)
Epoch: 212 | Batch_idx: 320 |  Loss: (0.0578) |  Loss2: (0.0000) | Acc: (97.00%) (40265/41088)
Epoch: 212 | Batch_idx: 330 |  Loss: (0.0577) |  Loss2: (0.0000) | Acc: (98.00%) (41528/42368)
Epoch: 212 | Batch_idx: 340 |  Loss: (0.0577) |  Loss2: (0.0000) | Acc: (98.00%) (42787/43648)
Epoch: 212 | Batch_idx: 350 |  Loss: (0.0576) |  Loss2: (0.0000) | Acc: (98.00%) (44042/44928)
Epoch: 212 | Batch_idx: 360 |  Loss: (0.0578) |  Loss2: (0.0000) | Acc: (98.00%) (45294/46208)
Epoch: 212 | Batch_idx: 370 |  Loss: (0.0578) |  Loss2: (0.0000) | Acc: (98.00%) (46542/47488)
Epoch: 212 | Batch_idx: 380 |  Loss: (0.0575) |  Loss2: (0.0000) | Acc: (98.00%) (47806/48768)
Epoch: 212 | Batch_idx: 390 |  Loss: (0.0577) |  Loss2: (0.0000) | Acc: (98.00%) (49011/50000)
# TEST : Loss: (0.4432) | Acc: (89.00%) (8916/10000)
percent tensor([0.5754, 0.5963, 0.5805, 0.5521, 0.5922, 0.5960, 0.6032, 0.5704, 0.5785,
        0.5885, 0.5917, 0.5912, 0.5762, 0.5600, 0.5966, 0.5717],
       device='cuda:0') torch.Size([16])
percent tensor([0.5420, 0.5439, 0.5495, 0.5522, 0.5513, 0.5394, 0.5472, 0.5543, 0.5463,
        0.5447, 0.5419, 0.5465, 0.5416, 0.5483, 0.5419, 0.5446],
       device='cuda:0') torch.Size([16])
percent tensor([0.6969, 0.7109, 0.5204, 0.4886, 0.4622, 0.6498, 0.6497, 0.5263, 0.5892,
        0.6871, 0.6944, 0.6086, 0.7252, 0.6550, 0.6842, 0.6795],
       device='cuda:0') torch.Size([16])
percent tensor([0.7255, 0.6322, 0.7027, 0.7006, 0.7012, 0.7853, 0.6730, 0.7027, 0.6821,
        0.6593, 0.6819, 0.6494, 0.6148, 0.6979, 0.6999, 0.7818],
       device='cuda:0') torch.Size([16])
percent tensor([0.6571, 0.6790, 0.6772, 0.6227, 0.6440, 0.6193, 0.6854, 0.6733, 0.6832,
        0.6597, 0.6383, 0.6430, 0.6613, 0.6549, 0.6660, 0.6195],
       device='cuda:0') torch.Size([16])
percent tensor([0.5960, 0.7399, 0.7122, 0.7527, 0.7206, 0.7824, 0.7383, 0.5891, 0.7852,
        0.6625, 0.7946, 0.8308, 0.7483, 0.8309, 0.6549, 0.5707],
       device='cuda:0') torch.Size([16])
percent tensor([0.6106, 0.7184, 0.5791, 0.4859, 0.6549, 0.7269, 0.6772, 0.4600, 0.6852,
        0.6924, 0.7278, 0.4975, 0.7333, 0.6003, 0.4541, 0.5507],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9998, 1.0000, 0.9998, 0.9999, 0.9997, 0.9999, 0.9997, 1.0000,
        0.9999, 0.9999, 0.9999, 0.9999, 0.9995, 0.9996, 0.9995],
       device='cuda:0') torch.Size([16])
Epoch: 213 | Batch_idx: 0 |  Loss: (0.0334) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 213 | Batch_idx: 10 |  Loss: (0.0474) |  Loss2: (0.0000) | Acc: (98.00%) (1388/1408)
Epoch: 213 | Batch_idx: 20 |  Loss: (0.0533) |  Loss2: (0.0000) | Acc: (98.00%) (2647/2688)
Epoch: 213 | Batch_idx: 30 |  Loss: (0.0561) |  Loss2: (0.0000) | Acc: (98.00%) (3902/3968)
Epoch: 213 | Batch_idx: 40 |  Loss: (0.0554) |  Loss2: (0.0000) | Acc: (98.00%) (5164/5248)
Epoch: 213 | Batch_idx: 50 |  Loss: (0.0544) |  Loss2: (0.0000) | Acc: (98.00%) (6419/6528)
Epoch: 213 | Batch_idx: 60 |  Loss: (0.0531) |  Loss2: (0.0000) | Acc: (98.00%) (7676/7808)
Epoch: 213 | Batch_idx: 70 |  Loss: (0.0526) |  Loss2: (0.0000) | Acc: (98.00%) (8940/9088)
Epoch: 213 | Batch_idx: 80 |  Loss: (0.0519) |  Loss2: (0.0000) | Acc: (98.00%) (10202/10368)
Epoch: 213 | Batch_idx: 90 |  Loss: (0.0532) |  Loss2: (0.0000) | Acc: (98.00%) (11460/11648)
Epoch: 213 | Batch_idx: 100 |  Loss: (0.0529) |  Loss2: (0.0000) | Acc: (98.00%) (12716/12928)
Epoch: 213 | Batch_idx: 110 |  Loss: (0.0529) |  Loss2: (0.0000) | Acc: (98.00%) (13979/14208)
Epoch: 213 | Batch_idx: 120 |  Loss: (0.0521) |  Loss2: (0.0000) | Acc: (98.00%) (15242/15488)
Epoch: 213 | Batch_idx: 130 |  Loss: (0.0518) |  Loss2: (0.0000) | Acc: (98.00%) (16507/16768)
Epoch: 213 | Batch_idx: 140 |  Loss: (0.0519) |  Loss2: (0.0000) | Acc: (98.00%) (17766/18048)
Epoch: 213 | Batch_idx: 150 |  Loss: (0.0519) |  Loss2: (0.0000) | Acc: (98.00%) (19026/19328)
Epoch: 213 | Batch_idx: 160 |  Loss: (0.0518) |  Loss2: (0.0000) | Acc: (98.00%) (20284/20608)
Epoch: 213 | Batch_idx: 170 |  Loss: (0.0516) |  Loss2: (0.0000) | Acc: (98.00%) (21547/21888)
Epoch: 213 | Batch_idx: 180 |  Loss: (0.0514) |  Loss2: (0.0000) | Acc: (98.00%) (22807/23168)
Epoch: 213 | Batch_idx: 190 |  Loss: (0.0515) |  Loss2: (0.0000) | Acc: (98.00%) (24065/24448)
Epoch: 213 | Batch_idx: 200 |  Loss: (0.0519) |  Loss2: (0.0000) | Acc: (98.00%) (25317/25728)
Epoch: 213 | Batch_idx: 210 |  Loss: (0.0523) |  Loss2: (0.0000) | Acc: (98.00%) (26570/27008)
Epoch: 213 | Batch_idx: 220 |  Loss: (0.0528) |  Loss2: (0.0000) | Acc: (98.00%) (27828/28288)
Epoch: 213 | Batch_idx: 230 |  Loss: (0.0526) |  Loss2: (0.0000) | Acc: (98.00%) (29090/29568)
Epoch: 213 | Batch_idx: 240 |  Loss: (0.0529) |  Loss2: (0.0000) | Acc: (98.00%) (30344/30848)
Epoch: 213 | Batch_idx: 250 |  Loss: (0.0532) |  Loss2: (0.0000) | Acc: (98.00%) (31603/32128)
Epoch: 213 | Batch_idx: 260 |  Loss: (0.0531) |  Loss2: (0.0000) | Acc: (98.00%) (32862/33408)
Epoch: 213 | Batch_idx: 270 |  Loss: (0.0530) |  Loss2: (0.0000) | Acc: (98.00%) (34125/34688)
Epoch: 213 | Batch_idx: 280 |  Loss: (0.0532) |  Loss2: (0.0000) | Acc: (98.00%) (35384/35968)
Epoch: 213 | Batch_idx: 290 |  Loss: (0.0536) |  Loss2: (0.0000) | Acc: (98.00%) (36634/37248)
Epoch: 213 | Batch_idx: 300 |  Loss: (0.0544) |  Loss2: (0.0000) | Acc: (98.00%) (37881/38528)
Epoch: 213 | Batch_idx: 310 |  Loss: (0.0543) |  Loss2: (0.0000) | Acc: (98.00%) (39136/39808)
Epoch: 213 | Batch_idx: 320 |  Loss: (0.0545) |  Loss2: (0.0000) | Acc: (98.00%) (40387/41088)
Epoch: 213 | Batch_idx: 330 |  Loss: (0.0543) |  Loss2: (0.0000) | Acc: (98.00%) (41651/42368)
Epoch: 213 | Batch_idx: 340 |  Loss: (0.0543) |  Loss2: (0.0000) | Acc: (98.00%) (42903/43648)
Epoch: 213 | Batch_idx: 350 |  Loss: (0.0543) |  Loss2: (0.0000) | Acc: (98.00%) (44163/44928)
Epoch: 213 | Batch_idx: 360 |  Loss: (0.0543) |  Loss2: (0.0000) | Acc: (98.00%) (45422/46208)
Epoch: 213 | Batch_idx: 370 |  Loss: (0.0549) |  Loss2: (0.0000) | Acc: (98.00%) (46664/47488)
Epoch: 213 | Batch_idx: 380 |  Loss: (0.0549) |  Loss2: (0.0000) | Acc: (98.00%) (47923/48768)
Epoch: 213 | Batch_idx: 390 |  Loss: (0.0550) |  Loss2: (0.0000) | Acc: (98.00%) (49134/50000)
# TEST : Loss: (0.4327) | Acc: (88.00%) (8882/10000)
percent tensor([0.5741, 0.5930, 0.5813, 0.5505, 0.5925, 0.5936, 0.6015, 0.5704, 0.5776,
        0.5869, 0.5892, 0.5918, 0.5750, 0.5571, 0.5938, 0.5692],
       device='cuda:0') torch.Size([16])
percent tensor([0.5417, 0.5437, 0.5498, 0.5536, 0.5517, 0.5390, 0.5469, 0.5550, 0.5459,
        0.5447, 0.5411, 0.5469, 0.5411, 0.5481, 0.5419, 0.5447],
       device='cuda:0') torch.Size([16])
percent tensor([0.7053, 0.7105, 0.5254, 0.4726, 0.4688, 0.6642, 0.6589, 0.5189, 0.5938,
        0.6822, 0.7047, 0.6045, 0.7342, 0.6553, 0.6866, 0.6842],
       device='cuda:0') torch.Size([16])
percent tensor([0.7203, 0.6175, 0.7037, 0.6974, 0.7021, 0.7880, 0.6748, 0.7122, 0.6750,
        0.6600, 0.6747, 0.6397, 0.6098, 0.6957, 0.6988, 0.7820],
       device='cuda:0') torch.Size([16])
percent tensor([0.6762, 0.6917, 0.6843, 0.6361, 0.6547, 0.6309, 0.6916, 0.6717, 0.6950,
        0.6680, 0.6540, 0.6551, 0.6715, 0.6673, 0.6753, 0.6391],
       device='cuda:0') torch.Size([16])
percent tensor([0.6078, 0.7549, 0.7166, 0.7371, 0.7343, 0.7822, 0.7662, 0.5882, 0.8022,
        0.7014, 0.8265, 0.8326, 0.7792, 0.8336, 0.6631, 0.5869],
       device='cuda:0') torch.Size([16])
percent tensor([0.5959, 0.7025, 0.5914, 0.4468, 0.6407, 0.7153, 0.6716, 0.4842, 0.6981,
        0.6927, 0.7247, 0.4789, 0.7293, 0.6126, 0.4007, 0.5303],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9999, 0.9999, 0.9998, 0.9999, 0.9996, 1.0000, 0.9996, 0.9999,
        0.9999, 1.0000, 0.9999, 0.9999, 0.9998, 0.9996, 0.9995],
       device='cuda:0') torch.Size([16])
Epoch: 214 | Batch_idx: 0 |  Loss: (0.0362) |  Loss2: (0.0000) | Acc: (100.00%) (128/128)
Epoch: 214 | Batch_idx: 10 |  Loss: (0.0440) |  Loss2: (0.0000) | Acc: (98.00%) (1391/1408)
Epoch: 214 | Batch_idx: 20 |  Loss: (0.0468) |  Loss2: (0.0000) | Acc: (98.00%) (2650/2688)
Epoch: 214 | Batch_idx: 30 |  Loss: (0.0408) |  Loss2: (0.0000) | Acc: (98.00%) (3921/3968)
Epoch: 214 | Batch_idx: 40 |  Loss: (0.0436) |  Loss2: (0.0000) | Acc: (98.00%) (5181/5248)
Epoch: 214 | Batch_idx: 50 |  Loss: (0.0438) |  Loss2: (0.0000) | Acc: (98.00%) (6449/6528)
Epoch: 214 | Batch_idx: 60 |  Loss: (0.0460) |  Loss2: (0.0000) | Acc: (98.00%) (7706/7808)
Epoch: 214 | Batch_idx: 70 |  Loss: (0.0468) |  Loss2: (0.0000) | Acc: (98.00%) (8960/9088)
Epoch: 214 | Batch_idx: 80 |  Loss: (0.0468) |  Loss2: (0.0000) | Acc: (98.00%) (10221/10368)
Epoch: 214 | Batch_idx: 90 |  Loss: (0.0483) |  Loss2: (0.0000) | Acc: (98.00%) (11473/11648)
Epoch: 214 | Batch_idx: 100 |  Loss: (0.0486) |  Loss2: (0.0000) | Acc: (98.00%) (12726/12928)
Epoch: 214 | Batch_idx: 110 |  Loss: (0.0491) |  Loss2: (0.0000) | Acc: (98.00%) (13986/14208)
Epoch: 214 | Batch_idx: 120 |  Loss: (0.0497) |  Loss2: (0.0000) | Acc: (98.00%) (15245/15488)
Epoch: 214 | Batch_idx: 130 |  Loss: (0.0490) |  Loss2: (0.0000) | Acc: (98.00%) (16512/16768)
Epoch: 214 | Batch_idx: 140 |  Loss: (0.0486) |  Loss2: (0.0000) | Acc: (98.00%) (17774/18048)
Epoch: 214 | Batch_idx: 150 |  Loss: (0.0480) |  Loss2: (0.0000) | Acc: (98.00%) (19040/19328)
Epoch: 214 | Batch_idx: 160 |  Loss: (0.0477) |  Loss2: (0.0000) | Acc: (98.00%) (20306/20608)
Epoch: 214 | Batch_idx: 170 |  Loss: (0.0479) |  Loss2: (0.0000) | Acc: (98.00%) (21566/21888)
Epoch: 214 | Batch_idx: 180 |  Loss: (0.0483) |  Loss2: (0.0000) | Acc: (98.00%) (22822/23168)
Epoch: 214 | Batch_idx: 190 |  Loss: (0.0478) |  Loss2: (0.0000) | Acc: (98.00%) (24093/24448)
Epoch: 214 | Batch_idx: 200 |  Loss: (0.0486) |  Loss2: (0.0000) | Acc: (98.00%) (25343/25728)
Epoch: 214 | Batch_idx: 210 |  Loss: (0.0489) |  Loss2: (0.0000) | Acc: (98.00%) (26604/27008)
Epoch: 214 | Batch_idx: 220 |  Loss: (0.0490) |  Loss2: (0.0000) | Acc: (98.00%) (27864/28288)
Epoch: 214 | Batch_idx: 230 |  Loss: (0.0492) |  Loss2: (0.0000) | Acc: (98.00%) (29123/29568)
Epoch: 214 | Batch_idx: 240 |  Loss: (0.0493) |  Loss2: (0.0000) | Acc: (98.00%) (30383/30848)
Epoch: 214 | Batch_idx: 250 |  Loss: (0.0494) |  Loss2: (0.0000) | Acc: (98.00%) (31643/32128)
Epoch: 214 | Batch_idx: 260 |  Loss: (0.0495) |  Loss2: (0.0000) | Acc: (98.00%) (32899/33408)
Epoch: 214 | Batch_idx: 270 |  Loss: (0.0496) |  Loss2: (0.0000) | Acc: (98.00%) (34160/34688)
Epoch: 214 | Batch_idx: 280 |  Loss: (0.0499) |  Loss2: (0.0000) | Acc: (98.00%) (35416/35968)
Epoch: 214 | Batch_idx: 290 |  Loss: (0.0503) |  Loss2: (0.0000) | Acc: (98.00%) (36671/37248)
Epoch: 214 | Batch_idx: 300 |  Loss: (0.0503) |  Loss2: (0.0000) | Acc: (98.00%) (37923/38528)
Epoch: 214 | Batch_idx: 310 |  Loss: (0.0506) |  Loss2: (0.0000) | Acc: (98.00%) (39175/39808)
Epoch: 214 | Batch_idx: 320 |  Loss: (0.0504) |  Loss2: (0.0000) | Acc: (98.00%) (40434/41088)
Epoch: 214 | Batch_idx: 330 |  Loss: (0.0503) |  Loss2: (0.0000) | Acc: (98.00%) (41695/42368)
Epoch: 214 | Batch_idx: 340 |  Loss: (0.0509) |  Loss2: (0.0000) | Acc: (98.00%) (42946/43648)
Epoch: 214 | Batch_idx: 350 |  Loss: (0.0510) |  Loss2: (0.0000) | Acc: (98.00%) (44203/44928)
Epoch: 214 | Batch_idx: 360 |  Loss: (0.0514) |  Loss2: (0.0000) | Acc: (98.00%) (45457/46208)
Epoch: 214 | Batch_idx: 370 |  Loss: (0.0517) |  Loss2: (0.0000) | Acc: (98.00%) (46712/47488)
Epoch: 214 | Batch_idx: 380 |  Loss: (0.0520) |  Loss2: (0.0000) | Acc: (98.00%) (47963/48768)
Epoch: 214 | Batch_idx: 390 |  Loss: (0.0522) |  Loss2: (0.0000) | Acc: (98.00%) (49171/50000)
# TEST : Loss: (0.4102) | Acc: (89.00%) (8925/10000)
percent tensor([0.5736, 0.5924, 0.5821, 0.5514, 0.5920, 0.5926, 0.6006, 0.5706, 0.5762,
        0.5864, 0.5886, 0.5910, 0.5742, 0.5559, 0.5931, 0.5696],
       device='cuda:0') torch.Size([16])
percent tensor([0.5412, 0.5432, 0.5497, 0.5526, 0.5513, 0.5387, 0.5468, 0.5545, 0.5456,
        0.5446, 0.5408, 0.5471, 0.5408, 0.5469, 0.5416, 0.5445],
       device='cuda:0') torch.Size([16])
percent tensor([0.6987, 0.7164, 0.5105, 0.4796, 0.4605, 0.6428, 0.6561, 0.5188, 0.5902,
        0.6895, 0.7001, 0.5987, 0.7306, 0.6721, 0.6800, 0.6777],
       device='cuda:0') torch.Size([16])
percent tensor([0.7249, 0.6215, 0.7071, 0.6973, 0.6987, 0.7857, 0.6715, 0.7124, 0.6747,
        0.6587, 0.6716, 0.6496, 0.6118, 0.6926, 0.7017, 0.7804],
       device='cuda:0') torch.Size([16])
percent tensor([0.6717, 0.6927, 0.6856, 0.6457, 0.6549, 0.6440, 0.6935, 0.6753, 0.6914,
        0.6631, 0.6464, 0.6496, 0.6649, 0.6827, 0.6714, 0.6361],
       device='cuda:0') torch.Size([16])
percent tensor([0.5735, 0.7412, 0.6926, 0.7447, 0.6945, 0.7748, 0.7444, 0.5560, 0.7834,
        0.6733, 0.8032, 0.8212, 0.7448, 0.8095, 0.6331, 0.5539],
       device='cuda:0') torch.Size([16])
percent tensor([0.5692, 0.6925, 0.5686, 0.4673, 0.6252, 0.7202, 0.6409, 0.4310, 0.6550,
        0.6662, 0.7124, 0.4848, 0.6983, 0.5973, 0.4103, 0.5236],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9998, 0.9999, 0.9998, 0.9999, 0.9996, 1.0000, 0.9997, 0.9999,
        0.9999, 0.9999, 0.9999, 0.9999, 0.9997, 0.9996, 0.9993],
       device='cuda:0') torch.Size([16])
Epoch: 215 | Batch_idx: 0 |  Loss: (0.0582) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 215 | Batch_idx: 10 |  Loss: (0.0456) |  Loss2: (0.0000) | Acc: (98.00%) (1391/1408)
Epoch: 215 | Batch_idx: 20 |  Loss: (0.0475) |  Loss2: (0.0000) | Acc: (98.00%) (2650/2688)
Epoch: 215 | Batch_idx: 30 |  Loss: (0.0460) |  Loss2: (0.0000) | Acc: (98.00%) (3908/3968)
Epoch: 215 | Batch_idx: 40 |  Loss: (0.0524) |  Loss2: (0.0000) | Acc: (98.00%) (5157/5248)
Epoch: 215 | Batch_idx: 50 |  Loss: (0.0513) |  Loss2: (0.0000) | Acc: (98.00%) (6416/6528)
Epoch: 215 | Batch_idx: 60 |  Loss: (0.0519) |  Loss2: (0.0000) | Acc: (98.00%) (7670/7808)
Epoch: 215 | Batch_idx: 70 |  Loss: (0.0518) |  Loss2: (0.0000) | Acc: (98.00%) (8924/9088)
Epoch: 215 | Batch_idx: 80 |  Loss: (0.0523) |  Loss2: (0.0000) | Acc: (98.00%) (10177/10368)
Epoch: 215 | Batch_idx: 90 |  Loss: (0.0532) |  Loss2: (0.0000) | Acc: (98.00%) (11429/11648)
Epoch: 215 | Batch_idx: 100 |  Loss: (0.0535) |  Loss2: (0.0000) | Acc: (98.00%) (12684/12928)
Epoch: 215 | Batch_idx: 110 |  Loss: (0.0541) |  Loss2: (0.0000) | Acc: (98.00%) (13942/14208)
Epoch: 215 | Batch_idx: 120 |  Loss: (0.0542) |  Loss2: (0.0000) | Acc: (98.00%) (15201/15488)
Epoch: 215 | Batch_idx: 130 |  Loss: (0.0547) |  Loss2: (0.0000) | Acc: (98.00%) (16454/16768)
Epoch: 215 | Batch_idx: 140 |  Loss: (0.0538) |  Loss2: (0.0000) | Acc: (98.00%) (17716/18048)
Epoch: 215 | Batch_idx: 150 |  Loss: (0.0547) |  Loss2: (0.0000) | Acc: (98.00%) (18961/19328)
Epoch: 215 | Batch_idx: 160 |  Loss: (0.0544) |  Loss2: (0.0000) | Acc: (98.00%) (20219/20608)
Epoch: 215 | Batch_idx: 170 |  Loss: (0.0544) |  Loss2: (0.0000) | Acc: (98.00%) (21473/21888)
Epoch: 215 | Batch_idx: 180 |  Loss: (0.0542) |  Loss2: (0.0000) | Acc: (98.00%) (22727/23168)
Epoch: 215 | Batch_idx: 190 |  Loss: (0.0540) |  Loss2: (0.0000) | Acc: (98.00%) (23989/24448)
Epoch: 215 | Batch_idx: 200 |  Loss: (0.0542) |  Loss2: (0.0000) | Acc: (98.00%) (25242/25728)
Epoch: 215 | Batch_idx: 210 |  Loss: (0.0545) |  Loss2: (0.0000) | Acc: (98.00%) (26504/27008)
Epoch: 215 | Batch_idx: 220 |  Loss: (0.0544) |  Loss2: (0.0000) | Acc: (98.00%) (27761/28288)
Epoch: 215 | Batch_idx: 230 |  Loss: (0.0548) |  Loss2: (0.0000) | Acc: (98.00%) (29016/29568)
Epoch: 215 | Batch_idx: 240 |  Loss: (0.0542) |  Loss2: (0.0000) | Acc: (98.00%) (30278/30848)
Epoch: 215 | Batch_idx: 250 |  Loss: (0.0542) |  Loss2: (0.0000) | Acc: (98.00%) (31537/32128)
Epoch: 215 | Batch_idx: 260 |  Loss: (0.0546) |  Loss2: (0.0000) | Acc: (98.00%) (32784/33408)
Epoch: 215 | Batch_idx: 270 |  Loss: (0.0541) |  Loss2: (0.0000) | Acc: (98.00%) (34051/34688)
Epoch: 215 | Batch_idx: 280 |  Loss: (0.0539) |  Loss2: (0.0000) | Acc: (98.00%) (35310/35968)
Epoch: 215 | Batch_idx: 290 |  Loss: (0.0538) |  Loss2: (0.0000) | Acc: (98.00%) (36570/37248)
Epoch: 215 | Batch_idx: 300 |  Loss: (0.0541) |  Loss2: (0.0000) | Acc: (98.00%) (37820/38528)
Epoch: 215 | Batch_idx: 310 |  Loss: (0.0543) |  Loss2: (0.0000) | Acc: (98.00%) (39069/39808)
Epoch: 215 | Batch_idx: 320 |  Loss: (0.0543) |  Loss2: (0.0000) | Acc: (98.00%) (40319/41088)
Epoch: 215 | Batch_idx: 330 |  Loss: (0.0541) |  Loss2: (0.0000) | Acc: (98.00%) (41570/42368)
Epoch: 215 | Batch_idx: 340 |  Loss: (0.0543) |  Loss2: (0.0000) | Acc: (98.00%) (42821/43648)
Epoch: 215 | Batch_idx: 350 |  Loss: (0.0542) |  Loss2: (0.0000) | Acc: (98.00%) (44082/44928)
Epoch: 215 | Batch_idx: 360 |  Loss: (0.0547) |  Loss2: (0.0000) | Acc: (98.00%) (45328/46208)
Epoch: 215 | Batch_idx: 370 |  Loss: (0.0548) |  Loss2: (0.0000) | Acc: (98.00%) (46585/47488)
Epoch: 215 | Batch_idx: 380 |  Loss: (0.0548) |  Loss2: (0.0000) | Acc: (98.00%) (47834/48768)
Epoch: 215 | Batch_idx: 390 |  Loss: (0.0552) |  Loss2: (0.0000) | Acc: (98.00%) (49034/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_215.pth.tar'
# TEST : Loss: (0.4242) | Acc: (89.00%) (8906/10000)
percent tensor([0.5752, 0.5935, 0.5860, 0.5533, 0.5952, 0.5951, 0.6020, 0.5723, 0.5773,
        0.5872, 0.5891, 0.5941, 0.5748, 0.5559, 0.5949, 0.5706],
       device='cuda:0') torch.Size([16])
percent tensor([0.5424, 0.5439, 0.5511, 0.5537, 0.5532, 0.5405, 0.5479, 0.5553, 0.5467,
        0.5457, 0.5417, 0.5484, 0.5419, 0.5480, 0.5424, 0.5454],
       device='cuda:0') torch.Size([16])
percent tensor([0.6923, 0.7066, 0.5168, 0.4819, 0.4509, 0.6387, 0.6488, 0.5197, 0.5848,
        0.6754, 0.6933, 0.5958, 0.7169, 0.6593, 0.6734, 0.6744],
       device='cuda:0') torch.Size([16])
percent tensor([0.7355, 0.6324, 0.7225, 0.7102, 0.7185, 0.7994, 0.6842, 0.7206, 0.6877,
        0.6702, 0.6817, 0.6644, 0.6130, 0.7029, 0.7149, 0.7877],
       device='cuda:0') torch.Size([16])
percent tensor([0.6723, 0.6869, 0.6849, 0.6401, 0.6487, 0.6153, 0.6962, 0.6723, 0.6919,
        0.6703, 0.6474, 0.6389, 0.6757, 0.6791, 0.6686, 0.6304],
       device='cuda:0') torch.Size([16])
percent tensor([0.5902, 0.7469, 0.6977, 0.7649, 0.7215, 0.7739, 0.7570, 0.6013, 0.7851,
        0.6669, 0.8049, 0.8184, 0.7399, 0.8365, 0.6523, 0.5656],
       device='cuda:0') torch.Size([16])
percent tensor([0.5868, 0.7138, 0.5844, 0.4980, 0.6619, 0.7220, 0.6883, 0.4803, 0.6883,
        0.6870, 0.7446, 0.4987, 0.7169, 0.6430, 0.4064, 0.5261],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9999, 0.9999, 0.9998, 0.9999, 0.9992, 1.0000, 0.9997, 0.9999,
        0.9998, 1.0000, 0.9999, 0.9999, 0.9998, 0.9996, 0.9994],
       device='cuda:0') torch.Size([16])
Epoch: 216 | Batch_idx: 0 |  Loss: (0.0468) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 216 | Batch_idx: 10 |  Loss: (0.0534) |  Loss2: (0.0000) | Acc: (98.00%) (1385/1408)
Epoch: 216 | Batch_idx: 20 |  Loss: (0.0508) |  Loss2: (0.0000) | Acc: (98.00%) (2644/2688)
Epoch: 216 | Batch_idx: 30 |  Loss: (0.0543) |  Loss2: (0.0000) | Acc: (98.00%) (3901/3968)
Epoch: 216 | Batch_idx: 40 |  Loss: (0.0546) |  Loss2: (0.0000) | Acc: (98.00%) (5157/5248)
Epoch: 216 | Batch_idx: 50 |  Loss: (0.0521) |  Loss2: (0.0000) | Acc: (98.00%) (6417/6528)
Epoch: 216 | Batch_idx: 60 |  Loss: (0.0529) |  Loss2: (0.0000) | Acc: (98.00%) (7671/7808)
Epoch: 216 | Batch_idx: 70 |  Loss: (0.0507) |  Loss2: (0.0000) | Acc: (98.00%) (8935/9088)
Epoch: 216 | Batch_idx: 80 |  Loss: (0.0491) |  Loss2: (0.0000) | Acc: (98.00%) (10204/10368)
Epoch: 216 | Batch_idx: 90 |  Loss: (0.0491) |  Loss2: (0.0000) | Acc: (98.00%) (11461/11648)
Epoch: 216 | Batch_idx: 100 |  Loss: (0.0498) |  Loss2: (0.0000) | Acc: (98.00%) (12717/12928)
Epoch: 216 | Batch_idx: 110 |  Loss: (0.0495) |  Loss2: (0.0000) | Acc: (98.00%) (13983/14208)
Epoch: 216 | Batch_idx: 120 |  Loss: (0.0506) |  Loss2: (0.0000) | Acc: (98.00%) (15238/15488)
Epoch: 216 | Batch_idx: 130 |  Loss: (0.0503) |  Loss2: (0.0000) | Acc: (98.00%) (16495/16768)
Epoch: 216 | Batch_idx: 140 |  Loss: (0.0511) |  Loss2: (0.0000) | Acc: (98.00%) (17750/18048)
Epoch: 216 | Batch_idx: 150 |  Loss: (0.0512) |  Loss2: (0.0000) | Acc: (98.00%) (19011/19328)
Epoch: 216 | Batch_idx: 160 |  Loss: (0.0527) |  Loss2: (0.0000) | Acc: (98.00%) (20261/20608)
Epoch: 216 | Batch_idx: 170 |  Loss: (0.0526) |  Loss2: (0.0000) | Acc: (98.00%) (21517/21888)
Epoch: 216 | Batch_idx: 180 |  Loss: (0.0528) |  Loss2: (0.0000) | Acc: (98.00%) (22768/23168)
Epoch: 216 | Batch_idx: 190 |  Loss: (0.0530) |  Loss2: (0.0000) | Acc: (98.00%) (24028/24448)
Epoch: 216 | Batch_idx: 200 |  Loss: (0.0529) |  Loss2: (0.0000) | Acc: (98.00%) (25281/25728)
Epoch: 216 | Batch_idx: 210 |  Loss: (0.0526) |  Loss2: (0.0000) | Acc: (98.00%) (26539/27008)
Epoch: 216 | Batch_idx: 220 |  Loss: (0.0528) |  Loss2: (0.0000) | Acc: (98.00%) (27794/28288)
Epoch: 216 | Batch_idx: 230 |  Loss: (0.0528) |  Loss2: (0.0000) | Acc: (98.00%) (29052/29568)
Epoch: 216 | Batch_idx: 240 |  Loss: (0.0526) |  Loss2: (0.0000) | Acc: (98.00%) (30314/30848)
Epoch: 216 | Batch_idx: 250 |  Loss: (0.0528) |  Loss2: (0.0000) | Acc: (98.00%) (31567/32128)
Epoch: 216 | Batch_idx: 260 |  Loss: (0.0536) |  Loss2: (0.0000) | Acc: (98.00%) (32809/33408)
Epoch: 216 | Batch_idx: 270 |  Loss: (0.0538) |  Loss2: (0.0000) | Acc: (98.00%) (34060/34688)
Epoch: 216 | Batch_idx: 280 |  Loss: (0.0539) |  Loss2: (0.0000) | Acc: (98.00%) (35316/35968)
Epoch: 216 | Batch_idx: 290 |  Loss: (0.0541) |  Loss2: (0.0000) | Acc: (98.00%) (36568/37248)
Epoch: 216 | Batch_idx: 300 |  Loss: (0.0540) |  Loss2: (0.0000) | Acc: (98.00%) (37825/38528)
Epoch: 216 | Batch_idx: 310 |  Loss: (0.0542) |  Loss2: (0.0000) | Acc: (98.00%) (39080/39808)
Epoch: 216 | Batch_idx: 320 |  Loss: (0.0543) |  Loss2: (0.0000) | Acc: (98.00%) (40337/41088)
Epoch: 216 | Batch_idx: 330 |  Loss: (0.0542) |  Loss2: (0.0000) | Acc: (98.00%) (41596/42368)
Epoch: 216 | Batch_idx: 340 |  Loss: (0.0539) |  Loss2: (0.0000) | Acc: (98.00%) (42856/43648)
Epoch: 216 | Batch_idx: 350 |  Loss: (0.0537) |  Loss2: (0.0000) | Acc: (98.00%) (44116/44928)
Epoch: 216 | Batch_idx: 360 |  Loss: (0.0537) |  Loss2: (0.0000) | Acc: (98.00%) (45373/46208)
Epoch: 216 | Batch_idx: 370 |  Loss: (0.0537) |  Loss2: (0.0000) | Acc: (98.00%) (46629/47488)
Epoch: 216 | Batch_idx: 380 |  Loss: (0.0539) |  Loss2: (0.0000) | Acc: (98.00%) (47880/48768)
Epoch: 216 | Batch_idx: 390 |  Loss: (0.0538) |  Loss2: (0.0000) | Acc: (98.00%) (49092/50000)
# TEST : Loss: (0.4356) | Acc: (88.00%) (8881/10000)
percent tensor([0.5769, 0.5954, 0.5853, 0.5530, 0.5955, 0.5962, 0.6042, 0.5726, 0.5797,
        0.5890, 0.5922, 0.5947, 0.5772, 0.5587, 0.5966, 0.5725],
       device='cuda:0') torch.Size([16])
percent tensor([0.5420, 0.5431, 0.5510, 0.5543, 0.5528, 0.5407, 0.5469, 0.5558, 0.5464,
        0.5448, 0.5406, 0.5472, 0.5411, 0.5471, 0.5419, 0.5453],
       device='cuda:0') torch.Size([16])
percent tensor([0.6923, 0.7137, 0.5220, 0.4644, 0.4508, 0.6272, 0.6600, 0.5187, 0.5872,
        0.6853, 0.6944, 0.6062, 0.7231, 0.6699, 0.6760, 0.6656],
       device='cuda:0') torch.Size([16])
percent tensor([0.7297, 0.6259, 0.7049, 0.7091, 0.7059, 0.7935, 0.6776, 0.7133, 0.6816,
        0.6574, 0.6697, 0.6516, 0.6064, 0.7045, 0.7090, 0.7855],
       device='cuda:0') torch.Size([16])
percent tensor([0.6716, 0.6973, 0.6859, 0.6311, 0.6515, 0.6200, 0.7003, 0.6835, 0.6922,
        0.6722, 0.6494, 0.6506, 0.6744, 0.6817, 0.6717, 0.6319],
       device='cuda:0') torch.Size([16])
percent tensor([0.5807, 0.7336, 0.6882, 0.7318, 0.6846, 0.7599, 0.7437, 0.5785, 0.7785,
        0.6729, 0.7997, 0.8213, 0.7483, 0.8185, 0.6415, 0.5434],
       device='cuda:0') torch.Size([16])
percent tensor([0.5885, 0.7140, 0.5674, 0.4428, 0.6206, 0.6937, 0.6730, 0.4499, 0.6862,
        0.6799, 0.7319, 0.4834, 0.7375, 0.5943, 0.4069, 0.5017],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9998, 0.9999, 0.9998, 0.9999, 0.9993, 1.0000, 0.9997, 0.9999,
        0.9998, 1.0000, 0.9999, 0.9999, 0.9998, 0.9996, 0.9995],
       device='cuda:0') torch.Size([16])
Epoch: 217 | Batch_idx: 0 |  Loss: (0.0633) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 217 | Batch_idx: 10 |  Loss: (0.0430) |  Loss2: (0.0000) | Acc: (98.00%) (1389/1408)
Epoch: 217 | Batch_idx: 20 |  Loss: (0.0401) |  Loss2: (0.0000) | Acc: (98.00%) (2653/2688)
Epoch: 217 | Batch_idx: 30 |  Loss: (0.0401) |  Loss2: (0.0000) | Acc: (98.00%) (3918/3968)
Epoch: 217 | Batch_idx: 40 |  Loss: (0.0411) |  Loss2: (0.0000) | Acc: (98.00%) (5183/5248)
Epoch: 217 | Batch_idx: 50 |  Loss: (0.0414) |  Loss2: (0.0000) | Acc: (98.00%) (6446/6528)
Epoch: 217 | Batch_idx: 60 |  Loss: (0.0412) |  Loss2: (0.0000) | Acc: (98.00%) (7710/7808)
Epoch: 217 | Batch_idx: 70 |  Loss: (0.0431) |  Loss2: (0.0000) | Acc: (98.00%) (8963/9088)
Epoch: 217 | Batch_idx: 80 |  Loss: (0.0454) |  Loss2: (0.0000) | Acc: (98.00%) (10221/10368)
Epoch: 217 | Batch_idx: 90 |  Loss: (0.0462) |  Loss2: (0.0000) | Acc: (98.00%) (11479/11648)
Epoch: 217 | Batch_idx: 100 |  Loss: (0.0458) |  Loss2: (0.0000) | Acc: (98.00%) (12743/12928)
Epoch: 217 | Batch_idx: 110 |  Loss: (0.0456) |  Loss2: (0.0000) | Acc: (98.00%) (14006/14208)
Epoch: 217 | Batch_idx: 120 |  Loss: (0.0457) |  Loss2: (0.0000) | Acc: (98.00%) (15265/15488)
Epoch: 217 | Batch_idx: 130 |  Loss: (0.0464) |  Loss2: (0.0000) | Acc: (98.00%) (16522/16768)
Epoch: 217 | Batch_idx: 140 |  Loss: (0.0465) |  Loss2: (0.0000) | Acc: (98.00%) (17781/18048)
Epoch: 217 | Batch_idx: 150 |  Loss: (0.0468) |  Loss2: (0.0000) | Acc: (98.00%) (19045/19328)
Epoch: 217 | Batch_idx: 160 |  Loss: (0.0468) |  Loss2: (0.0000) | Acc: (98.00%) (20307/20608)
Epoch: 217 | Batch_idx: 170 |  Loss: (0.0464) |  Loss2: (0.0000) | Acc: (98.00%) (21575/21888)
Epoch: 217 | Batch_idx: 180 |  Loss: (0.0465) |  Loss2: (0.0000) | Acc: (98.00%) (22832/23168)
Epoch: 217 | Batch_idx: 190 |  Loss: (0.0467) |  Loss2: (0.0000) | Acc: (98.00%) (24093/24448)
Epoch: 217 | Batch_idx: 200 |  Loss: (0.0466) |  Loss2: (0.0000) | Acc: (98.00%) (25356/25728)
Epoch: 217 | Batch_idx: 210 |  Loss: (0.0471) |  Loss2: (0.0000) | Acc: (98.00%) (26617/27008)
Epoch: 217 | Batch_idx: 220 |  Loss: (0.0467) |  Loss2: (0.0000) | Acc: (98.00%) (27884/28288)
Epoch: 217 | Batch_idx: 230 |  Loss: (0.0470) |  Loss2: (0.0000) | Acc: (98.00%) (29141/29568)
Epoch: 217 | Batch_idx: 240 |  Loss: (0.0476) |  Loss2: (0.0000) | Acc: (98.00%) (30393/30848)
Epoch: 217 | Batch_idx: 250 |  Loss: (0.0477) |  Loss2: (0.0000) | Acc: (98.00%) (31649/32128)
Epoch: 217 | Batch_idx: 260 |  Loss: (0.0480) |  Loss2: (0.0000) | Acc: (98.00%) (32910/33408)
Epoch: 217 | Batch_idx: 270 |  Loss: (0.0479) |  Loss2: (0.0000) | Acc: (98.00%) (34178/34688)
Epoch: 217 | Batch_idx: 280 |  Loss: (0.0479) |  Loss2: (0.0000) | Acc: (98.00%) (35437/35968)
Epoch: 217 | Batch_idx: 290 |  Loss: (0.0480) |  Loss2: (0.0000) | Acc: (98.00%) (36694/37248)
Epoch: 217 | Batch_idx: 300 |  Loss: (0.0483) |  Loss2: (0.0000) | Acc: (98.00%) (37951/38528)
Epoch: 217 | Batch_idx: 310 |  Loss: (0.0487) |  Loss2: (0.0000) | Acc: (98.00%) (39201/39808)
Epoch: 217 | Batch_idx: 320 |  Loss: (0.0490) |  Loss2: (0.0000) | Acc: (98.00%) (40454/41088)
Epoch: 217 | Batch_idx: 330 |  Loss: (0.0498) |  Loss2: (0.0000) | Acc: (98.00%) (41697/42368)
Epoch: 217 | Batch_idx: 340 |  Loss: (0.0503) |  Loss2: (0.0000) | Acc: (98.00%) (42946/43648)
Epoch: 217 | Batch_idx: 350 |  Loss: (0.0509) |  Loss2: (0.0000) | Acc: (98.00%) (44192/44928)
Epoch: 217 | Batch_idx: 360 |  Loss: (0.0511) |  Loss2: (0.0000) | Acc: (98.00%) (45447/46208)
Epoch: 217 | Batch_idx: 370 |  Loss: (0.0511) |  Loss2: (0.0000) | Acc: (98.00%) (46706/47488)
Epoch: 217 | Batch_idx: 380 |  Loss: (0.0511) |  Loss2: (0.0000) | Acc: (98.00%) (47964/48768)
Epoch: 217 | Batch_idx: 390 |  Loss: (0.0513) |  Loss2: (0.0000) | Acc: (98.00%) (49171/50000)
# TEST : Loss: (0.4903) | Acc: (88.00%) (8804/10000)
percent tensor([0.5741, 0.5950, 0.5783, 0.5514, 0.5898, 0.5937, 0.6017, 0.5695, 0.5776,
        0.5864, 0.5901, 0.5887, 0.5749, 0.5607, 0.5951, 0.5708],
       device='cuda:0') torch.Size([16])
percent tensor([0.5426, 0.5443, 0.5530, 0.5546, 0.5537, 0.5414, 0.5479, 0.5569, 0.5467,
        0.5463, 0.5416, 0.5489, 0.5421, 0.5480, 0.5428, 0.5453],
       device='cuda:0') torch.Size([16])
percent tensor([0.6972, 0.7114, 0.4904, 0.4564, 0.4375, 0.6313, 0.6560, 0.5046, 0.5930,
        0.6779, 0.7013, 0.5845, 0.7263, 0.6746, 0.6704, 0.6753],
       device='cuda:0') torch.Size([16])
percent tensor([0.7258, 0.6285, 0.7131, 0.7102, 0.7149, 0.7945, 0.6814, 0.7120, 0.6785,
        0.6674, 0.6721, 0.6583, 0.6147, 0.6964, 0.7090, 0.7810],
       device='cuda:0') torch.Size([16])
percent tensor([0.6634, 0.6830, 0.6705, 0.6181, 0.6356, 0.6085, 0.6853, 0.6641, 0.6886,
        0.6540, 0.6375, 0.6321, 0.6607, 0.6737, 0.6574, 0.6192],
       device='cuda:0') torch.Size([16])
percent tensor([0.6064, 0.7706, 0.7002, 0.7537, 0.7003, 0.7645, 0.7770, 0.5797, 0.7920,
        0.7203, 0.8245, 0.8259, 0.7579, 0.8418, 0.6634, 0.5823],
       device='cuda:0') torch.Size([16])
percent tensor([0.6220, 0.7422, 0.6137, 0.4728, 0.6096, 0.7164, 0.6967, 0.4408, 0.6883,
        0.7109, 0.7481, 0.5349, 0.7419, 0.6357, 0.4333, 0.5338],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9999, 0.9999, 0.9998, 0.9999, 0.9997, 1.0000, 0.9997, 0.9999,
        0.9999, 0.9999, 1.0000, 0.9999, 0.9997, 0.9997, 0.9997],
       device='cuda:0') torch.Size([16])
Epoch: 218 | Batch_idx: 0 |  Loss: (0.0353) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 218 | Batch_idx: 10 |  Loss: (0.0488) |  Loss2: (0.0000) | Acc: (98.00%) (1384/1408)
Epoch: 218 | Batch_idx: 20 |  Loss: (0.0488) |  Loss2: (0.0000) | Acc: (98.00%) (2641/2688)
Epoch: 218 | Batch_idx: 30 |  Loss: (0.0461) |  Loss2: (0.0000) | Acc: (98.00%) (3907/3968)
Epoch: 218 | Batch_idx: 40 |  Loss: (0.0443) |  Loss2: (0.0000) | Acc: (98.00%) (5169/5248)
Epoch: 218 | Batch_idx: 50 |  Loss: (0.0462) |  Loss2: (0.0000) | Acc: (98.00%) (6426/6528)
Epoch: 218 | Batch_idx: 60 |  Loss: (0.0459) |  Loss2: (0.0000) | Acc: (98.00%) (7685/7808)
Epoch: 218 | Batch_idx: 70 |  Loss: (0.0460) |  Loss2: (0.0000) | Acc: (98.00%) (8946/9088)
Epoch: 218 | Batch_idx: 80 |  Loss: (0.0476) |  Loss2: (0.0000) | Acc: (98.00%) (10202/10368)
Epoch: 218 | Batch_idx: 90 |  Loss: (0.0489) |  Loss2: (0.0000) | Acc: (98.00%) (11455/11648)
Epoch: 218 | Batch_idx: 100 |  Loss: (0.0491) |  Loss2: (0.0000) | Acc: (98.00%) (12714/12928)
Epoch: 218 | Batch_idx: 110 |  Loss: (0.0495) |  Loss2: (0.0000) | Acc: (98.00%) (13969/14208)
Epoch: 218 | Batch_idx: 120 |  Loss: (0.0499) |  Loss2: (0.0000) | Acc: (98.00%) (15226/15488)
Epoch: 218 | Batch_idx: 130 |  Loss: (0.0500) |  Loss2: (0.0000) | Acc: (98.00%) (16488/16768)
Epoch: 218 | Batch_idx: 140 |  Loss: (0.0501) |  Loss2: (0.0000) | Acc: (98.00%) (17745/18048)
Epoch: 218 | Batch_idx: 150 |  Loss: (0.0498) |  Loss2: (0.0000) | Acc: (98.00%) (19007/19328)
Epoch: 218 | Batch_idx: 160 |  Loss: (0.0501) |  Loss2: (0.0000) | Acc: (98.00%) (20261/20608)
Epoch: 218 | Batch_idx: 170 |  Loss: (0.0500) |  Loss2: (0.0000) | Acc: (98.00%) (21521/21888)
Epoch: 218 | Batch_idx: 180 |  Loss: (0.0506) |  Loss2: (0.0000) | Acc: (98.00%) (22773/23168)
Epoch: 218 | Batch_idx: 190 |  Loss: (0.0506) |  Loss2: (0.0000) | Acc: (98.00%) (24031/24448)
Epoch: 218 | Batch_idx: 200 |  Loss: (0.0511) |  Loss2: (0.0000) | Acc: (98.00%) (25288/25728)
Epoch: 218 | Batch_idx: 210 |  Loss: (0.0516) |  Loss2: (0.0000) | Acc: (98.00%) (26538/27008)
Epoch: 218 | Batch_idx: 220 |  Loss: (0.0517) |  Loss2: (0.0000) | Acc: (98.00%) (27794/28288)
Epoch: 218 | Batch_idx: 230 |  Loss: (0.0515) |  Loss2: (0.0000) | Acc: (98.00%) (29056/29568)
Epoch: 218 | Batch_idx: 240 |  Loss: (0.0513) |  Loss2: (0.0000) | Acc: (98.00%) (30317/30848)
Epoch: 218 | Batch_idx: 250 |  Loss: (0.0514) |  Loss2: (0.0000) | Acc: (98.00%) (31578/32128)
Epoch: 218 | Batch_idx: 260 |  Loss: (0.0522) |  Loss2: (0.0000) | Acc: (98.00%) (32826/33408)
Epoch: 218 | Batch_idx: 270 |  Loss: (0.0520) |  Loss2: (0.0000) | Acc: (98.00%) (34084/34688)
Epoch: 218 | Batch_idx: 280 |  Loss: (0.0522) |  Loss2: (0.0000) | Acc: (98.00%) (35336/35968)
Epoch: 218 | Batch_idx: 290 |  Loss: (0.0524) |  Loss2: (0.0000) | Acc: (98.00%) (36592/37248)
Epoch: 218 | Batch_idx: 300 |  Loss: (0.0526) |  Loss2: (0.0000) | Acc: (98.00%) (37846/38528)
Epoch: 218 | Batch_idx: 310 |  Loss: (0.0527) |  Loss2: (0.0000) | Acc: (98.00%) (39106/39808)
Epoch: 218 | Batch_idx: 320 |  Loss: (0.0529) |  Loss2: (0.0000) | Acc: (98.00%) (40361/41088)
Epoch: 218 | Batch_idx: 330 |  Loss: (0.0528) |  Loss2: (0.0000) | Acc: (98.00%) (41628/42368)
Epoch: 218 | Batch_idx: 340 |  Loss: (0.0527) |  Loss2: (0.0000) | Acc: (98.00%) (42885/43648)
Epoch: 218 | Batch_idx: 350 |  Loss: (0.0525) |  Loss2: (0.0000) | Acc: (98.00%) (44149/44928)
Epoch: 218 | Batch_idx: 360 |  Loss: (0.0526) |  Loss2: (0.0000) | Acc: (98.00%) (45400/46208)
Epoch: 218 | Batch_idx: 370 |  Loss: (0.0528) |  Loss2: (0.0000) | Acc: (98.00%) (46660/47488)
Epoch: 218 | Batch_idx: 380 |  Loss: (0.0526) |  Loss2: (0.0000) | Acc: (98.00%) (47922/48768)
Epoch: 218 | Batch_idx: 390 |  Loss: (0.0524) |  Loss2: (0.0000) | Acc: (98.00%) (49133/50000)
# TEST : Loss: (0.4293) | Acc: (88.00%) (8891/10000)
percent tensor([0.5766, 0.5960, 0.5867, 0.5556, 0.5970, 0.5963, 0.6049, 0.5734, 0.5790,
        0.5894, 0.5906, 0.5967, 0.5770, 0.5601, 0.5967, 0.5724],
       device='cuda:0') torch.Size([16])
percent tensor([0.5429, 0.5456, 0.5533, 0.5557, 0.5548, 0.5406, 0.5493, 0.5586, 0.5486,
        0.5483, 0.5426, 0.5503, 0.5431, 0.5499, 0.5435, 0.5462],
       device='cuda:0') torch.Size([16])
percent tensor([0.7060, 0.7257, 0.5329, 0.4846, 0.4668, 0.6418, 0.6726, 0.5226, 0.5978,
        0.6955, 0.7096, 0.6099, 0.7342, 0.6793, 0.6871, 0.6867],
       device='cuda:0') torch.Size([16])
percent tensor([0.7360, 0.6294, 0.7142, 0.7155, 0.7199, 0.7967, 0.6880, 0.7174, 0.6844,
        0.6632, 0.6784, 0.6533, 0.6153, 0.7065, 0.7102, 0.7904],
       device='cuda:0') torch.Size([16])
percent tensor([0.6668, 0.6854, 0.6831, 0.6275, 0.6526, 0.6205, 0.6930, 0.6722, 0.6995,
        0.6655, 0.6495, 0.6436, 0.6721, 0.6760, 0.6659, 0.6166],
       device='cuda:0') torch.Size([16])
percent tensor([0.6040, 0.7396, 0.7166, 0.7505, 0.7336, 0.7716, 0.7550, 0.5859, 0.7753,
        0.6653, 0.8017, 0.8149, 0.7516, 0.8328, 0.6335, 0.5732],
       device='cuda:0') torch.Size([16])
percent tensor([0.5960, 0.7222, 0.6055, 0.4738, 0.6515, 0.7222, 0.6932, 0.4649, 0.6641,
        0.6859, 0.7411, 0.4867, 0.7349, 0.6500, 0.4135, 0.5196],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9999, 1.0000, 0.9998, 0.9999, 0.9994, 1.0000, 0.9995, 0.9999,
        0.9998, 1.0000, 0.9999, 0.9999, 0.9997, 0.9996, 0.9994],
       device='cuda:0') torch.Size([16])
Epoch: 219 | Batch_idx: 0 |  Loss: (0.0597) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 219 | Batch_idx: 10 |  Loss: (0.0581) |  Loss2: (0.0000) | Acc: (97.00%) (1376/1408)
Epoch: 219 | Batch_idx: 20 |  Loss: (0.0506) |  Loss2: (0.0000) | Acc: (98.00%) (2640/2688)
Epoch: 219 | Batch_idx: 30 |  Loss: (0.0505) |  Loss2: (0.0000) | Acc: (98.00%) (3897/3968)
Epoch: 219 | Batch_idx: 40 |  Loss: (0.0487) |  Loss2: (0.0000) | Acc: (98.00%) (5162/5248)
Epoch: 219 | Batch_idx: 50 |  Loss: (0.0478) |  Loss2: (0.0000) | Acc: (98.00%) (6424/6528)
Epoch: 219 | Batch_idx: 60 |  Loss: (0.0497) |  Loss2: (0.0000) | Acc: (98.00%) (7677/7808)
Epoch: 219 | Batch_idx: 70 |  Loss: (0.0491) |  Loss2: (0.0000) | Acc: (98.00%) (8940/9088)
Epoch: 219 | Batch_idx: 80 |  Loss: (0.0505) |  Loss2: (0.0000) | Acc: (98.00%) (10195/10368)
Epoch: 219 | Batch_idx: 90 |  Loss: (0.0493) |  Loss2: (0.0000) | Acc: (98.00%) (11460/11648)
Epoch: 219 | Batch_idx: 100 |  Loss: (0.0491) |  Loss2: (0.0000) | Acc: (98.00%) (12718/12928)
Epoch: 219 | Batch_idx: 110 |  Loss: (0.0483) |  Loss2: (0.0000) | Acc: (98.00%) (13982/14208)
Epoch: 219 | Batch_idx: 120 |  Loss: (0.0489) |  Loss2: (0.0000) | Acc: (98.00%) (15240/15488)
Epoch: 219 | Batch_idx: 130 |  Loss: (0.0490) |  Loss2: (0.0000) | Acc: (98.00%) (16498/16768)
Epoch: 219 | Batch_idx: 140 |  Loss: (0.0494) |  Loss2: (0.0000) | Acc: (98.00%) (17754/18048)
Epoch: 219 | Batch_idx: 150 |  Loss: (0.0491) |  Loss2: (0.0000) | Acc: (98.00%) (19019/19328)
Epoch: 219 | Batch_idx: 160 |  Loss: (0.0491) |  Loss2: (0.0000) | Acc: (98.00%) (20278/20608)
Epoch: 219 | Batch_idx: 170 |  Loss: (0.0492) |  Loss2: (0.0000) | Acc: (98.00%) (21536/21888)
Epoch: 219 | Batch_idx: 180 |  Loss: (0.0493) |  Loss2: (0.0000) | Acc: (98.00%) (22791/23168)
Epoch: 219 | Batch_idx: 190 |  Loss: (0.0494) |  Loss2: (0.0000) | Acc: (98.00%) (24051/24448)
Epoch: 219 | Batch_idx: 200 |  Loss: (0.0495) |  Loss2: (0.0000) | Acc: (98.00%) (25309/25728)
Epoch: 219 | Batch_idx: 210 |  Loss: (0.0502) |  Loss2: (0.0000) | Acc: (98.00%) (26555/27008)
Epoch: 219 | Batch_idx: 220 |  Loss: (0.0503) |  Loss2: (0.0000) | Acc: (98.00%) (27813/28288)
Epoch: 219 | Batch_idx: 230 |  Loss: (0.0505) |  Loss2: (0.0000) | Acc: (98.00%) (29068/29568)
Epoch: 219 | Batch_idx: 240 |  Loss: (0.0505) |  Loss2: (0.0000) | Acc: (98.00%) (30330/30848)
Epoch: 219 | Batch_idx: 250 |  Loss: (0.0505) |  Loss2: (0.0000) | Acc: (98.00%) (31592/32128)
Epoch: 219 | Batch_idx: 260 |  Loss: (0.0503) |  Loss2: (0.0000) | Acc: (98.00%) (32854/33408)
Epoch: 219 | Batch_idx: 270 |  Loss: (0.0507) |  Loss2: (0.0000) | Acc: (98.00%) (34111/34688)
Epoch: 219 | Batch_idx: 280 |  Loss: (0.0517) |  Loss2: (0.0000) | Acc: (98.00%) (35358/35968)
Epoch: 219 | Batch_idx: 290 |  Loss: (0.0516) |  Loss2: (0.0000) | Acc: (98.00%) (36613/37248)
Epoch: 219 | Batch_idx: 300 |  Loss: (0.0517) |  Loss2: (0.0000) | Acc: (98.00%) (37869/38528)
Epoch: 219 | Batch_idx: 310 |  Loss: (0.0518) |  Loss2: (0.0000) | Acc: (98.00%) (39125/39808)
Epoch: 219 | Batch_idx: 320 |  Loss: (0.0521) |  Loss2: (0.0000) | Acc: (98.00%) (40373/41088)
Epoch: 219 | Batch_idx: 330 |  Loss: (0.0526) |  Loss2: (0.0000) | Acc: (98.00%) (41622/42368)
Epoch: 219 | Batch_idx: 340 |  Loss: (0.0524) |  Loss2: (0.0000) | Acc: (98.00%) (42881/43648)
Epoch: 219 | Batch_idx: 350 |  Loss: (0.0529) |  Loss2: (0.0000) | Acc: (98.00%) (44131/44928)
Epoch: 219 | Batch_idx: 360 |  Loss: (0.0530) |  Loss2: (0.0000) | Acc: (98.00%) (45383/46208)
Epoch: 219 | Batch_idx: 370 |  Loss: (0.0527) |  Loss2: (0.0000) | Acc: (98.00%) (46649/47488)
Epoch: 219 | Batch_idx: 380 |  Loss: (0.0529) |  Loss2: (0.0000) | Acc: (98.00%) (47903/48768)
Epoch: 219 | Batch_idx: 390 |  Loss: (0.0534) |  Loss2: (0.0000) | Acc: (98.00%) (49107/50000)
# TEST : Loss: (0.4967) | Acc: (87.00%) (8780/10000)
percent tensor([0.5749, 0.5936, 0.5817, 0.5515, 0.5932, 0.5933, 0.6026, 0.5700, 0.5774,
        0.5865, 0.5891, 0.5924, 0.5749, 0.5591, 0.5939, 0.5703],
       device='cuda:0') torch.Size([16])
percent tensor([0.5428, 0.5447, 0.5528, 0.5547, 0.5542, 0.5409, 0.5481, 0.5570, 0.5472,
        0.5467, 0.5418, 0.5489, 0.5425, 0.5486, 0.5428, 0.5456],
       device='cuda:0') torch.Size([16])
percent tensor([0.6954, 0.7166, 0.5240, 0.4751, 0.4553, 0.6305, 0.6632, 0.5246, 0.5966,
        0.6878, 0.7012, 0.6056, 0.7254, 0.6687, 0.6781, 0.6796],
       device='cuda:0') torch.Size([16])
percent tensor([0.7321, 0.6302, 0.7155, 0.7096, 0.7184, 0.7968, 0.6831, 0.7156, 0.6852,
        0.6689, 0.6768, 0.6643, 0.6148, 0.7128, 0.7097, 0.7878],
       device='cuda:0') torch.Size([16])
percent tensor([0.6693, 0.6950, 0.6756, 0.6355, 0.6498, 0.6283, 0.7001, 0.6821, 0.6915,
        0.6671, 0.6492, 0.6429, 0.6669, 0.6758, 0.6744, 0.6324],
       device='cuda:0') torch.Size([16])
percent tensor([0.6055, 0.7368, 0.7220, 0.7642, 0.7298, 0.7953, 0.7624, 0.5858, 0.7943,
        0.6827, 0.7955, 0.8197, 0.7492, 0.8300, 0.6265, 0.5596],
       device='cuda:0') torch.Size([16])
percent tensor([0.5916, 0.6979, 0.5810, 0.4672, 0.6209, 0.6878, 0.6836, 0.4356, 0.6870,
        0.6780, 0.7167, 0.4968, 0.7275, 0.6187, 0.3853, 0.4957],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9999, 1.0000, 0.9998, 0.9999, 0.9996, 1.0000, 0.9997, 0.9999,
        0.9999, 0.9999, 0.9999, 1.0000, 0.9998, 0.9996, 0.9994],
       device='cuda:0') torch.Size([16])
Epoch: 220 | Batch_idx: 0 |  Loss: (0.0776) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 220 | Batch_idx: 10 |  Loss: (0.0555) |  Loss2: (0.0000) | Acc: (97.00%) (1376/1408)
Epoch: 220 | Batch_idx: 20 |  Loss: (0.0547) |  Loss2: (0.0000) | Acc: (98.00%) (2636/2688)
Epoch: 220 | Batch_idx: 30 |  Loss: (0.0535) |  Loss2: (0.0000) | Acc: (98.00%) (3897/3968)
Epoch: 220 | Batch_idx: 40 |  Loss: (0.0538) |  Loss2: (0.0000) | Acc: (98.00%) (5158/5248)
Epoch: 220 | Batch_idx: 50 |  Loss: (0.0523) |  Loss2: (0.0000) | Acc: (98.00%) (6417/6528)
Epoch: 220 | Batch_idx: 60 |  Loss: (0.0512) |  Loss2: (0.0000) | Acc: (98.00%) (7679/7808)
Epoch: 220 | Batch_idx: 70 |  Loss: (0.0495) |  Loss2: (0.0000) | Acc: (98.00%) (8948/9088)
Epoch: 220 | Batch_idx: 80 |  Loss: (0.0500) |  Loss2: (0.0000) | Acc: (98.00%) (10202/10368)
Epoch: 220 | Batch_idx: 90 |  Loss: (0.0496) |  Loss2: (0.0000) | Acc: (98.00%) (11458/11648)
Epoch: 220 | Batch_idx: 100 |  Loss: (0.0490) |  Loss2: (0.0000) | Acc: (98.00%) (12718/12928)
Epoch: 220 | Batch_idx: 110 |  Loss: (0.0489) |  Loss2: (0.0000) | Acc: (98.00%) (13979/14208)
Epoch: 220 | Batch_idx: 120 |  Loss: (0.0476) |  Loss2: (0.0000) | Acc: (98.00%) (15243/15488)
Epoch: 220 | Batch_idx: 130 |  Loss: (0.0484) |  Loss2: (0.0000) | Acc: (98.00%) (16490/16768)
Epoch: 220 | Batch_idx: 140 |  Loss: (0.0489) |  Loss2: (0.0000) | Acc: (98.00%) (17743/18048)
Epoch: 220 | Batch_idx: 150 |  Loss: (0.0486) |  Loss2: (0.0000) | Acc: (98.00%) (19005/19328)
Epoch: 220 | Batch_idx: 160 |  Loss: (0.0486) |  Loss2: (0.0000) | Acc: (98.00%) (20262/20608)
Epoch: 220 | Batch_idx: 170 |  Loss: (0.0486) |  Loss2: (0.0000) | Acc: (98.00%) (21521/21888)
Epoch: 220 | Batch_idx: 180 |  Loss: (0.0484) |  Loss2: (0.0000) | Acc: (98.00%) (22784/23168)
Epoch: 220 | Batch_idx: 190 |  Loss: (0.0484) |  Loss2: (0.0000) | Acc: (98.00%) (24042/24448)
Epoch: 220 | Batch_idx: 200 |  Loss: (0.0483) |  Loss2: (0.0000) | Acc: (98.00%) (25306/25728)
Epoch: 220 | Batch_idx: 210 |  Loss: (0.0485) |  Loss2: (0.0000) | Acc: (98.00%) (26566/27008)
Epoch: 220 | Batch_idx: 220 |  Loss: (0.0486) |  Loss2: (0.0000) | Acc: (98.00%) (27825/28288)
Epoch: 220 | Batch_idx: 230 |  Loss: (0.0485) |  Loss2: (0.0000) | Acc: (98.00%) (29086/29568)
Epoch: 220 | Batch_idx: 240 |  Loss: (0.0485) |  Loss2: (0.0000) | Acc: (98.00%) (30346/30848)
Epoch: 220 | Batch_idx: 250 |  Loss: (0.0485) |  Loss2: (0.0000) | Acc: (98.00%) (31607/32128)
Epoch: 220 | Batch_idx: 260 |  Loss: (0.0484) |  Loss2: (0.0000) | Acc: (98.00%) (32870/33408)
Epoch: 220 | Batch_idx: 270 |  Loss: (0.0486) |  Loss2: (0.0000) | Acc: (98.00%) (34122/34688)
Epoch: 220 | Batch_idx: 280 |  Loss: (0.0486) |  Loss2: (0.0000) | Acc: (98.00%) (35383/35968)
Epoch: 220 | Batch_idx: 290 |  Loss: (0.0486) |  Loss2: (0.0000) | Acc: (98.00%) (36644/37248)
Epoch: 220 | Batch_idx: 300 |  Loss: (0.0488) |  Loss2: (0.0000) | Acc: (98.00%) (37906/38528)
Epoch: 220 | Batch_idx: 310 |  Loss: (0.0490) |  Loss2: (0.0000) | Acc: (98.00%) (39161/39808)
Epoch: 220 | Batch_idx: 320 |  Loss: (0.0490) |  Loss2: (0.0000) | Acc: (98.00%) (40421/41088)
Epoch: 220 | Batch_idx: 330 |  Loss: (0.0493) |  Loss2: (0.0000) | Acc: (98.00%) (41679/42368)
Epoch: 220 | Batch_idx: 340 |  Loss: (0.0494) |  Loss2: (0.0000) | Acc: (98.00%) (42939/43648)
Epoch: 220 | Batch_idx: 350 |  Loss: (0.0495) |  Loss2: (0.0000) | Acc: (98.00%) (44200/44928)
Epoch: 220 | Batch_idx: 360 |  Loss: (0.0498) |  Loss2: (0.0000) | Acc: (98.00%) (45454/46208)
Epoch: 220 | Batch_idx: 370 |  Loss: (0.0499) |  Loss2: (0.0000) | Acc: (98.00%) (46710/47488)
Epoch: 220 | Batch_idx: 380 |  Loss: (0.0499) |  Loss2: (0.0000) | Acc: (98.00%) (47969/48768)
Epoch: 220 | Batch_idx: 390 |  Loss: (0.0500) |  Loss2: (0.0000) | Acc: (98.00%) (49182/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_220.pth.tar'
# TEST : Loss: (0.4241) | Acc: (89.00%) (8925/10000)
percent tensor([0.5744, 0.5941, 0.5855, 0.5532, 0.5951, 0.5943, 0.6035, 0.5726, 0.5778,
        0.5879, 0.5890, 0.5948, 0.5750, 0.5589, 0.5948, 0.5703],
       device='cuda:0') torch.Size([16])
percent tensor([0.5440, 0.5454, 0.5540, 0.5547, 0.5552, 0.5411, 0.5494, 0.5577, 0.5491,
        0.5474, 0.5437, 0.5506, 0.5435, 0.5493, 0.5431, 0.5467],
       device='cuda:0') torch.Size([16])
percent tensor([0.7045, 0.7179, 0.5214, 0.4978, 0.4564, 0.6424, 0.6619, 0.5261, 0.5915,
        0.6916, 0.6975, 0.5983, 0.7300, 0.6780, 0.6852, 0.6872],
       device='cuda:0') torch.Size([16])
percent tensor([0.7263, 0.6067, 0.7080, 0.7036, 0.7074, 0.7892, 0.6706, 0.7119, 0.6867,
        0.6504, 0.6695, 0.6493, 0.6023, 0.7063, 0.6960, 0.7777],
       device='cuda:0') torch.Size([16])
percent tensor([0.6688, 0.6934, 0.6849, 0.6360, 0.6504, 0.6405, 0.7005, 0.6777, 0.6902,
        0.6656, 0.6450, 0.6415, 0.6674, 0.6747, 0.6779, 0.6305],
       device='cuda:0') torch.Size([16])
percent tensor([0.6016, 0.7649, 0.7117, 0.7744, 0.7065, 0.7760, 0.7799, 0.5898, 0.7988,
        0.7022, 0.8250, 0.8393, 0.7693, 0.8396, 0.6679, 0.5710],
       device='cuda:0') torch.Size([16])
percent tensor([0.5854, 0.7072, 0.6074, 0.5083, 0.6282, 0.7159, 0.6871, 0.4442, 0.6757,
        0.6970, 0.7517, 0.5271, 0.7274, 0.5960, 0.4196, 0.5145],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9996, 1.0000, 0.9997, 1.0000,
        0.9998, 1.0000, 0.9999, 0.9999, 0.9996, 0.9996, 0.9994],
       device='cuda:0') torch.Size([16])
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(186.1566, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(839.7726, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(836.2161, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1509.6338, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(472.8435, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2326.2798, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4238.5356, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1339.5901, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6344.5137, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11486.2793, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3760.6804, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(15857.9834, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 221 | Batch_idx: 0 |  Loss: (0.0304) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 221 | Batch_idx: 10 |  Loss: (0.0508) |  Loss2: (0.0000) | Acc: (98.00%) (1382/1408)
Epoch: 221 | Batch_idx: 20 |  Loss: (0.0518) |  Loss2: (0.0000) | Acc: (98.00%) (2639/2688)
Epoch: 221 | Batch_idx: 30 |  Loss: (0.0506) |  Loss2: (0.0000) | Acc: (98.00%) (3902/3968)
Epoch: 221 | Batch_idx: 40 |  Loss: (0.0489) |  Loss2: (0.0000) | Acc: (98.00%) (5167/5248)
Epoch: 221 | Batch_idx: 50 |  Loss: (0.0495) |  Loss2: (0.0000) | Acc: (98.00%) (6424/6528)
Epoch: 221 | Batch_idx: 60 |  Loss: (0.0494) |  Loss2: (0.0000) | Acc: (98.00%) (7685/7808)
Epoch: 221 | Batch_idx: 70 |  Loss: (0.0471) |  Loss2: (0.0000) | Acc: (98.00%) (8955/9088)
Epoch: 221 | Batch_idx: 80 |  Loss: (0.0470) |  Loss2: (0.0000) | Acc: (98.00%) (10213/10368)
Epoch: 221 | Batch_idx: 90 |  Loss: (0.0473) |  Loss2: (0.0000) | Acc: (98.00%) (11472/11648)
Epoch: 221 | Batch_idx: 100 |  Loss: (0.0476) |  Loss2: (0.0000) | Acc: (98.00%) (12729/12928)
Epoch: 221 | Batch_idx: 110 |  Loss: (0.0488) |  Loss2: (0.0000) | Acc: (98.00%) (13980/14208)
Epoch: 221 | Batch_idx: 120 |  Loss: (0.0476) |  Loss2: (0.0000) | Acc: (98.00%) (15249/15488)
Epoch: 221 | Batch_idx: 130 |  Loss: (0.0485) |  Loss2: (0.0000) | Acc: (98.00%) (16503/16768)
Epoch: 221 | Batch_idx: 140 |  Loss: (0.0492) |  Loss2: (0.0000) | Acc: (98.00%) (17758/18048)
Epoch: 221 | Batch_idx: 150 |  Loss: (0.0488) |  Loss2: (0.0000) | Acc: (98.00%) (19016/19328)
Epoch: 221 | Batch_idx: 160 |  Loss: (0.0495) |  Loss2: (0.0000) | Acc: (98.00%) (20269/20608)
Epoch: 221 | Batch_idx: 170 |  Loss: (0.0498) |  Loss2: (0.0000) | Acc: (98.00%) (21523/21888)
Epoch: 221 | Batch_idx: 180 |  Loss: (0.0500) |  Loss2: (0.0000) | Acc: (98.00%) (22783/23168)
Epoch: 221 | Batch_idx: 190 |  Loss: (0.0501) |  Loss2: (0.0000) | Acc: (98.00%) (24038/24448)
Epoch: 221 | Batch_idx: 200 |  Loss: (0.0507) |  Loss2: (0.0000) | Acc: (98.00%) (25294/25728)
Epoch: 221 | Batch_idx: 210 |  Loss: (0.0509) |  Loss2: (0.0000) | Acc: (98.00%) (26551/27008)
Epoch: 221 | Batch_idx: 220 |  Loss: (0.0506) |  Loss2: (0.0000) | Acc: (98.00%) (27817/28288)
Epoch: 221 | Batch_idx: 230 |  Loss: (0.0504) |  Loss2: (0.0000) | Acc: (98.00%) (29077/29568)
Epoch: 221 | Batch_idx: 240 |  Loss: (0.0504) |  Loss2: (0.0000) | Acc: (98.00%) (30334/30848)
Epoch: 221 | Batch_idx: 250 |  Loss: (0.0503) |  Loss2: (0.0000) | Acc: (98.00%) (31593/32128)
Epoch: 221 | Batch_idx: 260 |  Loss: (0.0504) |  Loss2: (0.0000) | Acc: (98.00%) (32851/33408)
Epoch: 221 | Batch_idx: 270 |  Loss: (0.0505) |  Loss2: (0.0000) | Acc: (98.00%) (34109/34688)
Epoch: 221 | Batch_idx: 280 |  Loss: (0.0506) |  Loss2: (0.0000) | Acc: (98.00%) (35368/35968)
Epoch: 221 | Batch_idx: 290 |  Loss: (0.0501) |  Loss2: (0.0000) | Acc: (98.00%) (36634/37248)
Epoch: 221 | Batch_idx: 300 |  Loss: (0.0504) |  Loss2: (0.0000) | Acc: (98.00%) (37895/38528)
Epoch: 221 | Batch_idx: 310 |  Loss: (0.0503) |  Loss2: (0.0000) | Acc: (98.00%) (39153/39808)
Epoch: 221 | Batch_idx: 320 |  Loss: (0.0500) |  Loss2: (0.0000) | Acc: (98.00%) (40410/41088)
Epoch: 221 | Batch_idx: 330 |  Loss: (0.0504) |  Loss2: (0.0000) | Acc: (98.00%) (41661/42368)
Epoch: 221 | Batch_idx: 340 |  Loss: (0.0502) |  Loss2: (0.0000) | Acc: (98.00%) (42925/43648)
Epoch: 221 | Batch_idx: 350 |  Loss: (0.0499) |  Loss2: (0.0000) | Acc: (98.00%) (44192/44928)
Epoch: 221 | Batch_idx: 360 |  Loss: (0.0498) |  Loss2: (0.0000) | Acc: (98.00%) (45452/46208)
Epoch: 221 | Batch_idx: 370 |  Loss: (0.0500) |  Loss2: (0.0000) | Acc: (98.00%) (46709/47488)
Epoch: 221 | Batch_idx: 380 |  Loss: (0.0498) |  Loss2: (0.0000) | Acc: (98.00%) (47969/48768)
Epoch: 221 | Batch_idx: 390 |  Loss: (0.0499) |  Loss2: (0.0000) | Acc: (98.00%) (49177/50000)
# TEST : Loss: (0.4458) | Acc: (88.00%) (8874/10000)
percent tensor([0.5745, 0.5928, 0.5835, 0.5520, 0.5933, 0.5941, 0.6014, 0.5707, 0.5765,
        0.5862, 0.5890, 0.5930, 0.5745, 0.5563, 0.5939, 0.5698],
       device='cuda:0') torch.Size([16])
percent tensor([0.5427, 0.5444, 0.5534, 0.5550, 0.5551, 0.5413, 0.5487, 0.5577, 0.5481,
        0.5468, 0.5422, 0.5501, 0.5424, 0.5486, 0.5426, 0.5459],
       device='cuda:0') torch.Size([16])
percent tensor([0.6925, 0.7175, 0.5127, 0.4742, 0.4472, 0.6306, 0.6541, 0.5200, 0.5845,
        0.6875, 0.6891, 0.5947, 0.7265, 0.6664, 0.6766, 0.6757],
       device='cuda:0') torch.Size([16])
percent tensor([0.7337, 0.6299, 0.7151, 0.7101, 0.7182, 0.7942, 0.6856, 0.7235, 0.6856,
        0.6661, 0.6737, 0.6623, 0.6118, 0.7096, 0.7026, 0.7874],
       device='cuda:0') torch.Size([16])
percent tensor([0.6680, 0.6888, 0.6907, 0.6443, 0.6502, 0.6420, 0.6942, 0.6677, 0.6938,
        0.6656, 0.6458, 0.6425, 0.6676, 0.6685, 0.6862, 0.6336],
       device='cuda:0') torch.Size([16])
percent tensor([0.6259, 0.7738, 0.7366, 0.7778, 0.7213, 0.7813, 0.7794, 0.6272, 0.7922,
        0.7083, 0.8239, 0.8415, 0.7699, 0.8416, 0.6714, 0.5705],
       device='cuda:0') torch.Size([16])
percent tensor([0.5806, 0.7139, 0.6198, 0.4683, 0.6360, 0.7340, 0.6800, 0.4559, 0.6571,
        0.6851, 0.7276, 0.5078, 0.7148, 0.6130, 0.3763, 0.4874],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9999, 1.0000, 0.9998, 0.9999, 0.9996, 1.0000, 0.9998, 1.0000,
        0.9998, 1.0000, 0.9999, 0.9999, 0.9996, 0.9996, 0.9991],
       device='cuda:0') torch.Size([16])
Epoch: 222 | Batch_idx: 0 |  Loss: (0.0177) |  Loss2: (0.0000) | Acc: (100.00%) (128/128)
Epoch: 222 | Batch_idx: 10 |  Loss: (0.0540) |  Loss2: (0.0000) | Acc: (97.00%) (1378/1408)
Epoch: 222 | Batch_idx: 20 |  Loss: (0.0517) |  Loss2: (0.0000) | Acc: (98.00%) (2637/2688)
Epoch: 222 | Batch_idx: 30 |  Loss: (0.0462) |  Loss2: (0.0000) | Acc: (98.00%) (3904/3968)
Epoch: 222 | Batch_idx: 40 |  Loss: (0.0467) |  Loss2: (0.0000) | Acc: (98.00%) (5167/5248)
Epoch: 222 | Batch_idx: 50 |  Loss: (0.0453) |  Loss2: (0.0000) | Acc: (98.00%) (6430/6528)
Epoch: 222 | Batch_idx: 60 |  Loss: (0.0429) |  Loss2: (0.0000) | Acc: (98.00%) (7699/7808)
Epoch: 222 | Batch_idx: 70 |  Loss: (0.0435) |  Loss2: (0.0000) | Acc: (98.00%) (8956/9088)
Epoch: 222 | Batch_idx: 80 |  Loss: (0.0432) |  Loss2: (0.0000) | Acc: (98.00%) (10220/10368)
Epoch: 222 | Batch_idx: 90 |  Loss: (0.0436) |  Loss2: (0.0000) | Acc: (98.00%) (11481/11648)
Epoch: 222 | Batch_idx: 100 |  Loss: (0.0435) |  Loss2: (0.0000) | Acc: (98.00%) (12744/12928)
Epoch: 222 | Batch_idx: 110 |  Loss: (0.0435) |  Loss2: (0.0000) | Acc: (98.00%) (14006/14208)
Epoch: 222 | Batch_idx: 120 |  Loss: (0.0448) |  Loss2: (0.0000) | Acc: (98.00%) (15265/15488)
Epoch: 222 | Batch_idx: 130 |  Loss: (0.0455) |  Loss2: (0.0000) | Acc: (98.00%) (16520/16768)
Epoch: 222 | Batch_idx: 140 |  Loss: (0.0460) |  Loss2: (0.0000) | Acc: (98.00%) (17780/18048)
Epoch: 222 | Batch_idx: 150 |  Loss: (0.0463) |  Loss2: (0.0000) | Acc: (98.00%) (19042/19328)
Epoch: 222 | Batch_idx: 160 |  Loss: (0.0467) |  Loss2: (0.0000) | Acc: (98.00%) (20296/20608)
Epoch: 222 | Batch_idx: 170 |  Loss: (0.0473) |  Loss2: (0.0000) | Acc: (98.00%) (21552/21888)
Epoch: 222 | Batch_idx: 180 |  Loss: (0.0468) |  Loss2: (0.0000) | Acc: (98.00%) (22818/23168)
Epoch: 222 | Batch_idx: 190 |  Loss: (0.0464) |  Loss2: (0.0000) | Acc: (98.00%) (24083/24448)
Epoch: 222 | Batch_idx: 200 |  Loss: (0.0457) |  Loss2: (0.0000) | Acc: (98.00%) (25350/25728)
Epoch: 222 | Batch_idx: 210 |  Loss: (0.0454) |  Loss2: (0.0000) | Acc: (98.00%) (26612/27008)
Epoch: 222 | Batch_idx: 220 |  Loss: (0.0451) |  Loss2: (0.0000) | Acc: (98.00%) (27882/28288)
Epoch: 222 | Batch_idx: 230 |  Loss: (0.0455) |  Loss2: (0.0000) | Acc: (98.00%) (29134/29568)
Epoch: 222 | Batch_idx: 240 |  Loss: (0.0451) |  Loss2: (0.0000) | Acc: (98.00%) (30402/30848)
Epoch: 222 | Batch_idx: 250 |  Loss: (0.0456) |  Loss2: (0.0000) | Acc: (98.00%) (31658/32128)
Epoch: 222 | Batch_idx: 260 |  Loss: (0.0453) |  Loss2: (0.0000) | Acc: (98.00%) (32926/33408)
Epoch: 222 | Batch_idx: 270 |  Loss: (0.0454) |  Loss2: (0.0000) | Acc: (98.00%) (34186/34688)
Epoch: 222 | Batch_idx: 280 |  Loss: (0.0455) |  Loss2: (0.0000) | Acc: (98.00%) (35448/35968)
Epoch: 222 | Batch_idx: 290 |  Loss: (0.0455) |  Loss2: (0.0000) | Acc: (98.00%) (36710/37248)
Epoch: 222 | Batch_idx: 300 |  Loss: (0.0455) |  Loss2: (0.0000) | Acc: (98.00%) (37969/38528)
Epoch: 222 | Batch_idx: 310 |  Loss: (0.0453) |  Loss2: (0.0000) | Acc: (98.00%) (39234/39808)
Epoch: 222 | Batch_idx: 320 |  Loss: (0.0455) |  Loss2: (0.0000) | Acc: (98.00%) (40494/41088)
Epoch: 222 | Batch_idx: 330 |  Loss: (0.0456) |  Loss2: (0.0000) | Acc: (98.00%) (41749/42368)
Epoch: 222 | Batch_idx: 340 |  Loss: (0.0454) |  Loss2: (0.0000) | Acc: (98.00%) (43007/43648)
Epoch: 222 | Batch_idx: 350 |  Loss: (0.0458) |  Loss2: (0.0000) | Acc: (98.00%) (44266/44928)
Epoch: 222 | Batch_idx: 360 |  Loss: (0.0456) |  Loss2: (0.0000) | Acc: (98.00%) (45526/46208)
Epoch: 222 | Batch_idx: 370 |  Loss: (0.0457) |  Loss2: (0.0000) | Acc: (98.00%) (46786/47488)
Epoch: 222 | Batch_idx: 380 |  Loss: (0.0459) |  Loss2: (0.0000) | Acc: (98.00%) (48043/48768)
Epoch: 222 | Batch_idx: 390 |  Loss: (0.0464) |  Loss2: (0.0000) | Acc: (98.00%) (49247/50000)
# TEST : Loss: (0.4422) | Acc: (88.00%) (8884/10000)
percent tensor([0.5731, 0.5934, 0.5817, 0.5520, 0.5916, 0.5908, 0.6011, 0.5701, 0.5747,
        0.5863, 0.5872, 0.5916, 0.5736, 0.5574, 0.5929, 0.5690],
       device='cuda:0') torch.Size([16])
percent tensor([0.5427, 0.5446, 0.5520, 0.5551, 0.5539, 0.5423, 0.5476, 0.5569, 0.5479,
        0.5458, 0.5429, 0.5481, 0.5422, 0.5487, 0.5439, 0.5463],
       device='cuda:0') torch.Size([16])
percent tensor([0.6919, 0.7212, 0.4984, 0.4548, 0.4338, 0.6235, 0.6635, 0.5088, 0.5839,
        0.6923, 0.6928, 0.6030, 0.7274, 0.6793, 0.6750, 0.6714],
       device='cuda:0') torch.Size([16])
percent tensor([0.7279, 0.6145, 0.7081, 0.7100, 0.7146, 0.8008, 0.6769, 0.7168, 0.6856,
        0.6509, 0.6736, 0.6434, 0.5990, 0.7019, 0.7065, 0.7837],
       device='cuda:0') torch.Size([16])
percent tensor([0.6748, 0.6843, 0.6883, 0.6342, 0.6507, 0.6305, 0.6942, 0.6730, 0.6914,
        0.6687, 0.6432, 0.6418, 0.6675, 0.6863, 0.6795, 0.6337],
       device='cuda:0') torch.Size([16])
percent tensor([0.6159, 0.7598, 0.6855, 0.7689, 0.7075, 0.7873, 0.7702, 0.5934, 0.7864,
        0.6933, 0.8150, 0.8398, 0.7693, 0.8222, 0.6562, 0.5787],
       device='cuda:0') torch.Size([16])
percent tensor([0.5771, 0.7030, 0.6226, 0.4659, 0.6427, 0.7376, 0.6785, 0.4377, 0.6724,
        0.6743, 0.7246, 0.5008, 0.7179, 0.5822, 0.3796, 0.4924],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9999, 0.9999, 0.9998, 0.9999, 0.9996, 1.0000, 0.9995, 0.9999,
        0.9999, 1.0000, 0.9999, 0.9999, 0.9998, 0.9996, 0.9992],
       device='cuda:0') torch.Size([16])
Epoch: 223 | Batch_idx: 0 |  Loss: (0.0447) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 223 | Batch_idx: 10 |  Loss: (0.0316) |  Loss2: (0.0000) | Acc: (99.00%) (1397/1408)
Epoch: 223 | Batch_idx: 20 |  Loss: (0.0375) |  Loss2: (0.0000) | Acc: (98.00%) (2660/2688)
Epoch: 223 | Batch_idx: 30 |  Loss: (0.0369) |  Loss2: (0.0000) | Acc: (98.00%) (3921/3968)
Epoch: 223 | Batch_idx: 40 |  Loss: (0.0386) |  Loss2: (0.0000) | Acc: (98.00%) (5181/5248)
Epoch: 223 | Batch_idx: 50 |  Loss: (0.0379) |  Loss2: (0.0000) | Acc: (98.00%) (6450/6528)
Epoch: 223 | Batch_idx: 60 |  Loss: (0.0399) |  Loss2: (0.0000) | Acc: (98.00%) (7706/7808)
Epoch: 223 | Batch_idx: 70 |  Loss: (0.0404) |  Loss2: (0.0000) | Acc: (98.00%) (8969/9088)
Epoch: 223 | Batch_idx: 80 |  Loss: (0.0404) |  Loss2: (0.0000) | Acc: (98.00%) (10231/10368)
Epoch: 223 | Batch_idx: 90 |  Loss: (0.0408) |  Loss2: (0.0000) | Acc: (98.00%) (11492/11648)
Epoch: 223 | Batch_idx: 100 |  Loss: (0.0415) |  Loss2: (0.0000) | Acc: (98.00%) (12752/12928)
Epoch: 223 | Batch_idx: 110 |  Loss: (0.0421) |  Loss2: (0.0000) | Acc: (98.00%) (14013/14208)
Epoch: 223 | Batch_idx: 120 |  Loss: (0.0414) |  Loss2: (0.0000) | Acc: (98.00%) (15285/15488)
Epoch: 223 | Batch_idx: 130 |  Loss: (0.0427) |  Loss2: (0.0000) | Acc: (98.00%) (16539/16768)
Epoch: 223 | Batch_idx: 140 |  Loss: (0.0425) |  Loss2: (0.0000) | Acc: (98.00%) (17805/18048)
Epoch: 223 | Batch_idx: 150 |  Loss: (0.0433) |  Loss2: (0.0000) | Acc: (98.00%) (19060/19328)
Epoch: 223 | Batch_idx: 160 |  Loss: (0.0434) |  Loss2: (0.0000) | Acc: (98.00%) (20322/20608)
Epoch: 223 | Batch_idx: 170 |  Loss: (0.0439) |  Loss2: (0.0000) | Acc: (98.00%) (21579/21888)
Epoch: 223 | Batch_idx: 180 |  Loss: (0.0447) |  Loss2: (0.0000) | Acc: (98.00%) (22838/23168)
Epoch: 223 | Batch_idx: 190 |  Loss: (0.0441) |  Loss2: (0.0000) | Acc: (98.00%) (24107/24448)
Epoch: 223 | Batch_idx: 200 |  Loss: (0.0441) |  Loss2: (0.0000) | Acc: (98.00%) (25366/25728)
Epoch: 223 | Batch_idx: 210 |  Loss: (0.0440) |  Loss2: (0.0000) | Acc: (98.00%) (26627/27008)
Epoch: 223 | Batch_idx: 220 |  Loss: (0.0436) |  Loss2: (0.0000) | Acc: (98.00%) (27894/28288)
Epoch: 223 | Batch_idx: 230 |  Loss: (0.0436) |  Loss2: (0.0000) | Acc: (98.00%) (29152/29568)
Epoch: 223 | Batch_idx: 240 |  Loss: (0.0436) |  Loss2: (0.0000) | Acc: (98.00%) (30412/30848)
Epoch: 223 | Batch_idx: 250 |  Loss: (0.0438) |  Loss2: (0.0000) | Acc: (98.00%) (31670/32128)
Epoch: 223 | Batch_idx: 260 |  Loss: (0.0438) |  Loss2: (0.0000) | Acc: (98.00%) (32933/33408)
Epoch: 223 | Batch_idx: 270 |  Loss: (0.0437) |  Loss2: (0.0000) | Acc: (98.00%) (34198/34688)
Epoch: 223 | Batch_idx: 280 |  Loss: (0.0442) |  Loss2: (0.0000) | Acc: (98.00%) (35448/35968)
Epoch: 223 | Batch_idx: 290 |  Loss: (0.0442) |  Loss2: (0.0000) | Acc: (98.00%) (36713/37248)
Epoch: 223 | Batch_idx: 300 |  Loss: (0.0443) |  Loss2: (0.0000) | Acc: (98.00%) (37971/38528)
Epoch: 223 | Batch_idx: 310 |  Loss: (0.0443) |  Loss2: (0.0000) | Acc: (98.00%) (39232/39808)
Epoch: 223 | Batch_idx: 320 |  Loss: (0.0442) |  Loss2: (0.0000) | Acc: (98.00%) (40495/41088)
Epoch: 223 | Batch_idx: 330 |  Loss: (0.0442) |  Loss2: (0.0000) | Acc: (98.00%) (41759/42368)
Epoch: 223 | Batch_idx: 340 |  Loss: (0.0444) |  Loss2: (0.0000) | Acc: (98.00%) (43019/43648)
Epoch: 223 | Batch_idx: 350 |  Loss: (0.0442) |  Loss2: (0.0000) | Acc: (98.00%) (44285/44928)
Epoch: 223 | Batch_idx: 360 |  Loss: (0.0441) |  Loss2: (0.0000) | Acc: (98.00%) (45550/46208)
Epoch: 223 | Batch_idx: 370 |  Loss: (0.0443) |  Loss2: (0.0000) | Acc: (98.00%) (46806/47488)
Epoch: 223 | Batch_idx: 380 |  Loss: (0.0446) |  Loss2: (0.0000) | Acc: (98.00%) (48066/48768)
Epoch: 223 | Batch_idx: 390 |  Loss: (0.0449) |  Loss2: (0.0000) | Acc: (98.00%) (49272/50000)
# TEST : Loss: (0.4799) | Acc: (88.00%) (8858/10000)
percent tensor([0.5711, 0.5918, 0.5808, 0.5511, 0.5917, 0.5898, 0.6004, 0.5690, 0.5751,
        0.5848, 0.5855, 0.5903, 0.5722, 0.5565, 0.5914, 0.5681],
       device='cuda:0') torch.Size([16])
percent tensor([0.5428, 0.5449, 0.5520, 0.5542, 0.5529, 0.5412, 0.5481, 0.5572, 0.5466,
        0.5465, 0.5423, 0.5485, 0.5422, 0.5490, 0.5433, 0.5459],
       device='cuda:0') torch.Size([16])
percent tensor([0.6942, 0.7214, 0.4878, 0.4500, 0.4391, 0.6299, 0.6596, 0.4916, 0.5930,
        0.6834, 0.7015, 0.5902, 0.7297, 0.6749, 0.6774, 0.6764],
       device='cuda:0') torch.Size([16])
percent tensor([0.7465, 0.6527, 0.7217, 0.7218, 0.7288, 0.8042, 0.7072, 0.7301, 0.6916,
        0.6713, 0.6900, 0.6682, 0.6247, 0.7182, 0.7219, 0.8017],
       device='cuda:0') torch.Size([16])
percent tensor([0.6717, 0.6829, 0.6896, 0.6372, 0.6486, 0.6204, 0.6853, 0.6777, 0.6959,
        0.6647, 0.6431, 0.6453, 0.6724, 0.6701, 0.6766, 0.6257],
       device='cuda:0') torch.Size([16])
percent tensor([0.5840, 0.7609, 0.6664, 0.7429, 0.7002, 0.7689, 0.7627, 0.5582, 0.7708,
        0.6912, 0.8159, 0.8220, 0.7481, 0.8495, 0.6685, 0.5578],
       device='cuda:0') torch.Size([16])
percent tensor([0.5949, 0.7526, 0.6115, 0.4666, 0.6500, 0.7198, 0.7079, 0.4568, 0.6644,
        0.7101, 0.7668, 0.5279, 0.7295, 0.6435, 0.4253, 0.5009],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9999, 0.9999, 0.9997, 0.9998, 0.9995, 1.0000, 0.9996, 1.0000,
        0.9999, 1.0000, 1.0000, 0.9999, 0.9997, 0.9996, 0.9994],
       device='cuda:0') torch.Size([16])
Epoch: 224 | Batch_idx: 0 |  Loss: (0.0235) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 224 | Batch_idx: 10 |  Loss: (0.0450) |  Loss2: (0.0000) | Acc: (98.00%) (1391/1408)
Epoch: 224 | Batch_idx: 20 |  Loss: (0.0496) |  Loss2: (0.0000) | Acc: (98.00%) (2648/2688)
Epoch: 224 | Batch_idx: 30 |  Loss: (0.0473) |  Loss2: (0.0000) | Acc: (98.00%) (3914/3968)
Epoch: 224 | Batch_idx: 40 |  Loss: (0.0471) |  Loss2: (0.0000) | Acc: (98.00%) (5175/5248)
Epoch: 224 | Batch_idx: 50 |  Loss: (0.0465) |  Loss2: (0.0000) | Acc: (98.00%) (6436/6528)
Epoch: 224 | Batch_idx: 60 |  Loss: (0.0455) |  Loss2: (0.0000) | Acc: (98.00%) (7706/7808)
Epoch: 224 | Batch_idx: 70 |  Loss: (0.0458) |  Loss2: (0.0000) | Acc: (98.00%) (8970/9088)
Epoch: 224 | Batch_idx: 80 |  Loss: (0.0458) |  Loss2: (0.0000) | Acc: (98.00%) (10231/10368)
Epoch: 224 | Batch_idx: 90 |  Loss: (0.0455) |  Loss2: (0.0000) | Acc: (98.00%) (11496/11648)
Epoch: 224 | Batch_idx: 100 |  Loss: (0.0447) |  Loss2: (0.0000) | Acc: (98.00%) (12764/12928)
Epoch: 224 | Batch_idx: 110 |  Loss: (0.0451) |  Loss2: (0.0000) | Acc: (98.00%) (14021/14208)
Epoch: 224 | Batch_idx: 120 |  Loss: (0.0458) |  Loss2: (0.0000) | Acc: (98.00%) (15279/15488)
Epoch: 224 | Batch_idx: 130 |  Loss: (0.0455) |  Loss2: (0.0000) | Acc: (98.00%) (16541/16768)
Epoch: 224 | Batch_idx: 140 |  Loss: (0.0456) |  Loss2: (0.0000) | Acc: (98.00%) (17800/18048)
Epoch: 224 | Batch_idx: 150 |  Loss: (0.0463) |  Loss2: (0.0000) | Acc: (98.00%) (19056/19328)
Epoch: 224 | Batch_idx: 160 |  Loss: (0.0462) |  Loss2: (0.0000) | Acc: (98.00%) (20313/20608)
Epoch: 224 | Batch_idx: 170 |  Loss: (0.0459) |  Loss2: (0.0000) | Acc: (98.00%) (21573/21888)
Epoch: 224 | Batch_idx: 180 |  Loss: (0.0460) |  Loss2: (0.0000) | Acc: (98.00%) (22830/23168)
Epoch: 224 | Batch_idx: 190 |  Loss: (0.0457) |  Loss2: (0.0000) | Acc: (98.00%) (24090/24448)
Epoch: 224 | Batch_idx: 200 |  Loss: (0.0451) |  Loss2: (0.0000) | Acc: (98.00%) (25357/25728)
Epoch: 224 | Batch_idx: 210 |  Loss: (0.0448) |  Loss2: (0.0000) | Acc: (98.00%) (26623/27008)
Epoch: 224 | Batch_idx: 220 |  Loss: (0.0443) |  Loss2: (0.0000) | Acc: (98.00%) (27891/28288)
Epoch: 224 | Batch_idx: 230 |  Loss: (0.0442) |  Loss2: (0.0000) | Acc: (98.00%) (29155/29568)
Epoch: 224 | Batch_idx: 240 |  Loss: (0.0438) |  Loss2: (0.0000) | Acc: (98.00%) (30424/30848)
Epoch: 224 | Batch_idx: 250 |  Loss: (0.0435) |  Loss2: (0.0000) | Acc: (98.00%) (31693/32128)
Epoch: 224 | Batch_idx: 260 |  Loss: (0.0435) |  Loss2: (0.0000) | Acc: (98.00%) (32952/33408)
Epoch: 224 | Batch_idx: 270 |  Loss: (0.0435) |  Loss2: (0.0000) | Acc: (98.00%) (34215/34688)
Epoch: 224 | Batch_idx: 280 |  Loss: (0.0435) |  Loss2: (0.0000) | Acc: (98.00%) (35476/35968)
Epoch: 224 | Batch_idx: 290 |  Loss: (0.0431) |  Loss2: (0.0000) | Acc: (98.00%) (36746/37248)
Epoch: 224 | Batch_idx: 300 |  Loss: (0.0430) |  Loss2: (0.0000) | Acc: (98.00%) (38011/38528)
Epoch: 224 | Batch_idx: 310 |  Loss: (0.0431) |  Loss2: (0.0000) | Acc: (98.00%) (39272/39808)
Epoch: 224 | Batch_idx: 320 |  Loss: (0.0431) |  Loss2: (0.0000) | Acc: (98.00%) (40529/41088)
Epoch: 224 | Batch_idx: 330 |  Loss: (0.0429) |  Loss2: (0.0000) | Acc: (98.00%) (41796/42368)
Epoch: 224 | Batch_idx: 340 |  Loss: (0.0427) |  Loss2: (0.0000) | Acc: (98.00%) (43060/43648)
Epoch: 224 | Batch_idx: 350 |  Loss: (0.0428) |  Loss2: (0.0000) | Acc: (98.00%) (44320/44928)
Epoch: 224 | Batch_idx: 360 |  Loss: (0.0429) |  Loss2: (0.0000) | Acc: (98.00%) (45586/46208)
Epoch: 224 | Batch_idx: 370 |  Loss: (0.0428) |  Loss2: (0.0000) | Acc: (98.00%) (46849/47488)
Epoch: 224 | Batch_idx: 380 |  Loss: (0.0430) |  Loss2: (0.0000) | Acc: (98.00%) (48109/48768)
Epoch: 224 | Batch_idx: 390 |  Loss: (0.0428) |  Loss2: (0.0000) | Acc: (98.00%) (49324/50000)
# TEST : Loss: (0.4313) | Acc: (89.00%) (8910/10000)
percent tensor([0.5741, 0.5918, 0.5849, 0.5527, 0.5955, 0.5928, 0.6016, 0.5711, 0.5765,
        0.5866, 0.5871, 0.5946, 0.5744, 0.5548, 0.5930, 0.5693],
       device='cuda:0') torch.Size([16])
percent tensor([0.5429, 0.5445, 0.5521, 0.5547, 0.5533, 0.5410, 0.5481, 0.5568, 0.5475,
        0.5458, 0.5425, 0.5486, 0.5422, 0.5490, 0.5428, 0.5455],
       device='cuda:0') torch.Size([16])
percent tensor([0.6995, 0.7110, 0.5196, 0.4743, 0.4596, 0.6426, 0.6534, 0.5108, 0.5919,
        0.6783, 0.6911, 0.6007, 0.7285, 0.6565, 0.6754, 0.6725],
       device='cuda:0') torch.Size([16])
percent tensor([0.7465, 0.6462, 0.7218, 0.7229, 0.7301, 0.8054, 0.7060, 0.7292, 0.6978,
        0.6699, 0.6875, 0.6757, 0.6211, 0.7182, 0.7223, 0.7947],
       device='cuda:0') torch.Size([16])
percent tensor([0.6787, 0.6919, 0.7008, 0.6501, 0.6605, 0.6315, 0.6995, 0.6810, 0.7016,
        0.6763, 0.6559, 0.6486, 0.6812, 0.6803, 0.6788, 0.6419],
       device='cuda:0') torch.Size([16])
percent tensor([0.6175, 0.7726, 0.7515, 0.7697, 0.7272, 0.7788, 0.7867, 0.6339, 0.8016,
        0.7180, 0.8310, 0.8397, 0.7698, 0.8434, 0.6918, 0.5914],
       device='cuda:0') torch.Size([16])
percent tensor([0.5795, 0.7098, 0.6120, 0.4798, 0.6478, 0.7194, 0.6945, 0.4749, 0.6668,
        0.6926, 0.7345, 0.5090, 0.7041, 0.6105, 0.4068, 0.4931],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9998, 0.9999, 0.9999, 0.9999, 0.9995, 1.0000, 0.9997, 1.0000,
        0.9999, 1.0000, 0.9999, 0.9999, 0.9998, 0.9997, 0.9993],
       device='cuda:0') torch.Size([16])
Epoch: 225 | Batch_idx: 0 |  Loss: (0.0421) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 225 | Batch_idx: 10 |  Loss: (0.0496) |  Loss2: (0.0000) | Acc: (98.00%) (1384/1408)
Epoch: 225 | Batch_idx: 20 |  Loss: (0.0409) |  Loss2: (0.0000) | Acc: (98.00%) (2653/2688)
Epoch: 225 | Batch_idx: 30 |  Loss: (0.0395) |  Loss2: (0.0000) | Acc: (98.00%) (3921/3968)
Epoch: 225 | Batch_idx: 40 |  Loss: (0.0387) |  Loss2: (0.0000) | Acc: (98.00%) (5184/5248)
Epoch: 225 | Batch_idx: 50 |  Loss: (0.0409) |  Loss2: (0.0000) | Acc: (98.00%) (6442/6528)
Epoch: 225 | Batch_idx: 60 |  Loss: (0.0399) |  Loss2: (0.0000) | Acc: (98.00%) (7709/7808)
Epoch: 225 | Batch_idx: 70 |  Loss: (0.0404) |  Loss2: (0.0000) | Acc: (98.00%) (8968/9088)
Epoch: 225 | Batch_idx: 80 |  Loss: (0.0416) |  Loss2: (0.0000) | Acc: (98.00%) (10226/10368)
Epoch: 225 | Batch_idx: 90 |  Loss: (0.0422) |  Loss2: (0.0000) | Acc: (98.00%) (11488/11648)
Epoch: 225 | Batch_idx: 100 |  Loss: (0.0426) |  Loss2: (0.0000) | Acc: (98.00%) (12751/12928)
Epoch: 225 | Batch_idx: 110 |  Loss: (0.0430) |  Loss2: (0.0000) | Acc: (98.00%) (14015/14208)
Epoch: 225 | Batch_idx: 120 |  Loss: (0.0434) |  Loss2: (0.0000) | Acc: (98.00%) (15272/15488)
Epoch: 225 | Batch_idx: 130 |  Loss: (0.0441) |  Loss2: (0.0000) | Acc: (98.00%) (16529/16768)
Epoch: 225 | Batch_idx: 140 |  Loss: (0.0451) |  Loss2: (0.0000) | Acc: (98.00%) (17785/18048)
Epoch: 225 | Batch_idx: 150 |  Loss: (0.0454) |  Loss2: (0.0000) | Acc: (98.00%) (19044/19328)
Epoch: 225 | Batch_idx: 160 |  Loss: (0.0452) |  Loss2: (0.0000) | Acc: (98.00%) (20310/20608)
Epoch: 225 | Batch_idx: 170 |  Loss: (0.0449) |  Loss2: (0.0000) | Acc: (98.00%) (21575/21888)
Epoch: 225 | Batch_idx: 180 |  Loss: (0.0443) |  Loss2: (0.0000) | Acc: (98.00%) (22843/23168)
Epoch: 225 | Batch_idx: 190 |  Loss: (0.0442) |  Loss2: (0.0000) | Acc: (98.00%) (24107/24448)
Epoch: 225 | Batch_idx: 200 |  Loss: (0.0446) |  Loss2: (0.0000) | Acc: (98.00%) (25368/25728)
Epoch: 225 | Batch_idx: 210 |  Loss: (0.0445) |  Loss2: (0.0000) | Acc: (98.00%) (26631/27008)
Epoch: 225 | Batch_idx: 220 |  Loss: (0.0448) |  Loss2: (0.0000) | Acc: (98.00%) (27888/28288)
Epoch: 225 | Batch_idx: 230 |  Loss: (0.0444) |  Loss2: (0.0000) | Acc: (98.00%) (29157/29568)
Epoch: 225 | Batch_idx: 240 |  Loss: (0.0440) |  Loss2: (0.0000) | Acc: (98.00%) (30423/30848)
Epoch: 225 | Batch_idx: 250 |  Loss: (0.0440) |  Loss2: (0.0000) | Acc: (98.00%) (31681/32128)
Epoch: 225 | Batch_idx: 260 |  Loss: (0.0442) |  Loss2: (0.0000) | Acc: (98.00%) (32939/33408)
Epoch: 225 | Batch_idx: 270 |  Loss: (0.0441) |  Loss2: (0.0000) | Acc: (98.00%) (34204/34688)
Epoch: 225 | Batch_idx: 280 |  Loss: (0.0443) |  Loss2: (0.0000) | Acc: (98.00%) (35462/35968)
Epoch: 225 | Batch_idx: 290 |  Loss: (0.0443) |  Loss2: (0.0000) | Acc: (98.00%) (36722/37248)
Epoch: 225 | Batch_idx: 300 |  Loss: (0.0445) |  Loss2: (0.0000) | Acc: (98.00%) (37976/38528)
Epoch: 225 | Batch_idx: 310 |  Loss: (0.0445) |  Loss2: (0.0000) | Acc: (98.00%) (39235/39808)
Epoch: 225 | Batch_idx: 320 |  Loss: (0.0445) |  Loss2: (0.0000) | Acc: (98.00%) (40497/41088)
Epoch: 225 | Batch_idx: 330 |  Loss: (0.0442) |  Loss2: (0.0000) | Acc: (98.00%) (41764/42368)
Epoch: 225 | Batch_idx: 340 |  Loss: (0.0444) |  Loss2: (0.0000) | Acc: (98.00%) (43024/43648)
Epoch: 225 | Batch_idx: 350 |  Loss: (0.0446) |  Loss2: (0.0000) | Acc: (98.00%) (44280/44928)
Epoch: 225 | Batch_idx: 360 |  Loss: (0.0446) |  Loss2: (0.0000) | Acc: (98.00%) (45537/46208)
Epoch: 225 | Batch_idx: 370 |  Loss: (0.0445) |  Loss2: (0.0000) | Acc: (98.00%) (46803/47488)
Epoch: 225 | Batch_idx: 380 |  Loss: (0.0447) |  Loss2: (0.0000) | Acc: (98.00%) (48059/48768)
Epoch: 225 | Batch_idx: 390 |  Loss: (0.0447) |  Loss2: (0.0000) | Acc: (98.00%) (49271/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_225.pth.tar'
# TEST : Loss: (0.4243) | Acc: (89.00%) (8934/10000)
percent tensor([0.5762, 0.5938, 0.5869, 0.5541, 0.5979, 0.5968, 0.6039, 0.5727, 0.5787,
        0.5881, 0.5892, 0.5966, 0.5762, 0.5564, 0.5960, 0.5715],
       device='cuda:0') torch.Size([16])
percent tensor([0.5430, 0.5452, 0.5508, 0.5550, 0.5531, 0.5422, 0.5481, 0.5560, 0.5467,
        0.5462, 0.5424, 0.5473, 0.5422, 0.5496, 0.5434, 0.5459],
       device='cuda:0') torch.Size([16])
percent tensor([0.6954, 0.7223, 0.5080, 0.4643, 0.4411, 0.6266, 0.6628, 0.5263, 0.6055,
        0.6959, 0.7094, 0.6084, 0.7315, 0.6812, 0.6788, 0.6794],
       device='cuda:0') torch.Size([16])
percent tensor([0.7410, 0.6437, 0.7158, 0.7156, 0.7237, 0.8005, 0.6956, 0.7220, 0.6906,
        0.6721, 0.6870, 0.6622, 0.6230, 0.7203, 0.7160, 0.7941],
       device='cuda:0') torch.Size([16])
percent tensor([0.6844, 0.6949, 0.7024, 0.6551, 0.6607, 0.6277, 0.7032, 0.6899, 0.7041,
        0.6841, 0.6656, 0.6563, 0.6775, 0.6823, 0.6824, 0.6459],
       device='cuda:0') torch.Size([16])
percent tensor([0.5819, 0.7732, 0.6782, 0.7622, 0.7073, 0.7581, 0.7831, 0.5971, 0.7910,
        0.7010, 0.8143, 0.8157, 0.7628, 0.8447, 0.6520, 0.5561],
       device='cuda:0') torch.Size([16])
percent tensor([0.5536, 0.7277, 0.5831, 0.4637, 0.6477, 0.7025, 0.6833, 0.4363, 0.6465,
        0.6687, 0.7192, 0.4570, 0.7177, 0.6207, 0.3779, 0.4725],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9998, 0.9999, 0.9998, 0.9999, 0.9998, 1.0000, 0.9996, 0.9999,
        0.9998, 1.0000, 0.9999, 1.0000, 0.9997, 0.9996, 0.9994],
       device='cuda:0') torch.Size([16])
Epoch: 226 | Batch_idx: 0 |  Loss: (0.0217) |  Loss2: (0.0000) | Acc: (100.00%) (128/128)
Epoch: 226 | Batch_idx: 10 |  Loss: (0.0434) |  Loss2: (0.0000) | Acc: (98.00%) (1392/1408)
Epoch: 226 | Batch_idx: 20 |  Loss: (0.0440) |  Loss2: (0.0000) | Acc: (98.00%) (2656/2688)
Epoch: 226 | Batch_idx: 30 |  Loss: (0.0440) |  Loss2: (0.0000) | Acc: (98.00%) (3919/3968)
Epoch: 226 | Batch_idx: 40 |  Loss: (0.0425) |  Loss2: (0.0000) | Acc: (98.00%) (5188/5248)
Epoch: 226 | Batch_idx: 50 |  Loss: (0.0429) |  Loss2: (0.0000) | Acc: (98.00%) (6446/6528)
Epoch: 226 | Batch_idx: 60 |  Loss: (0.0434) |  Loss2: (0.0000) | Acc: (98.00%) (7708/7808)
Epoch: 226 | Batch_idx: 70 |  Loss: (0.0432) |  Loss2: (0.0000) | Acc: (98.00%) (8971/9088)
Epoch: 226 | Batch_idx: 80 |  Loss: (0.0427) |  Loss2: (0.0000) | Acc: (98.00%) (10230/10368)
Epoch: 226 | Batch_idx: 90 |  Loss: (0.0423) |  Loss2: (0.0000) | Acc: (98.00%) (11492/11648)
Epoch: 226 | Batch_idx: 100 |  Loss: (0.0423) |  Loss2: (0.0000) | Acc: (98.00%) (12755/12928)
Epoch: 226 | Batch_idx: 110 |  Loss: (0.0418) |  Loss2: (0.0000) | Acc: (98.00%) (14022/14208)
Epoch: 226 | Batch_idx: 120 |  Loss: (0.0413) |  Loss2: (0.0000) | Acc: (98.00%) (15288/15488)
Epoch: 226 | Batch_idx: 130 |  Loss: (0.0416) |  Loss2: (0.0000) | Acc: (98.00%) (16553/16768)
Epoch: 226 | Batch_idx: 140 |  Loss: (0.0412) |  Loss2: (0.0000) | Acc: (98.00%) (17820/18048)
Epoch: 226 | Batch_idx: 150 |  Loss: (0.0410) |  Loss2: (0.0000) | Acc: (98.00%) (19082/19328)
Epoch: 226 | Batch_idx: 160 |  Loss: (0.0412) |  Loss2: (0.0000) | Acc: (98.00%) (20340/20608)
Epoch: 226 | Batch_idx: 170 |  Loss: (0.0406) |  Loss2: (0.0000) | Acc: (98.00%) (21613/21888)
Epoch: 226 | Batch_idx: 180 |  Loss: (0.0405) |  Loss2: (0.0000) | Acc: (98.00%) (22875/23168)
Epoch: 226 | Batch_idx: 190 |  Loss: (0.0405) |  Loss2: (0.0000) | Acc: (98.00%) (24140/24448)
Epoch: 226 | Batch_idx: 200 |  Loss: (0.0412) |  Loss2: (0.0000) | Acc: (98.00%) (25395/25728)
Epoch: 226 | Batch_idx: 210 |  Loss: (0.0412) |  Loss2: (0.0000) | Acc: (98.00%) (26654/27008)
Epoch: 226 | Batch_idx: 220 |  Loss: (0.0415) |  Loss2: (0.0000) | Acc: (98.00%) (27910/28288)
Epoch: 226 | Batch_idx: 230 |  Loss: (0.0418) |  Loss2: (0.0000) | Acc: (98.00%) (29166/29568)
Epoch: 226 | Batch_idx: 240 |  Loss: (0.0418) |  Loss2: (0.0000) | Acc: (98.00%) (30429/30848)
Epoch: 226 | Batch_idx: 250 |  Loss: (0.0420) |  Loss2: (0.0000) | Acc: (98.00%) (31692/32128)
Epoch: 226 | Batch_idx: 260 |  Loss: (0.0423) |  Loss2: (0.0000) | Acc: (98.00%) (32949/33408)
Epoch: 226 | Batch_idx: 270 |  Loss: (0.0427) |  Loss2: (0.0000) | Acc: (98.00%) (34205/34688)
Epoch: 226 | Batch_idx: 280 |  Loss: (0.0428) |  Loss2: (0.0000) | Acc: (98.00%) (35465/35968)
Epoch: 226 | Batch_idx: 290 |  Loss: (0.0429) |  Loss2: (0.0000) | Acc: (98.00%) (36727/37248)
Epoch: 226 | Batch_idx: 300 |  Loss: (0.0428) |  Loss2: (0.0000) | Acc: (98.00%) (37986/38528)
Epoch: 226 | Batch_idx: 310 |  Loss: (0.0427) |  Loss2: (0.0000) | Acc: (98.00%) (39251/39808)
Epoch: 226 | Batch_idx: 320 |  Loss: (0.0429) |  Loss2: (0.0000) | Acc: (98.00%) (40508/41088)
Epoch: 226 | Batch_idx: 330 |  Loss: (0.0429) |  Loss2: (0.0000) | Acc: (98.00%) (41773/42368)
Epoch: 226 | Batch_idx: 340 |  Loss: (0.0428) |  Loss2: (0.0000) | Acc: (98.00%) (43039/43648)
Epoch: 226 | Batch_idx: 350 |  Loss: (0.0428) |  Loss2: (0.0000) | Acc: (98.00%) (44299/44928)
Epoch: 226 | Batch_idx: 360 |  Loss: (0.0429) |  Loss2: (0.0000) | Acc: (98.00%) (45559/46208)
Epoch: 226 | Batch_idx: 370 |  Loss: (0.0430) |  Loss2: (0.0000) | Acc: (98.00%) (46817/47488)
Epoch: 226 | Batch_idx: 380 |  Loss: (0.0431) |  Loss2: (0.0000) | Acc: (98.00%) (48080/48768)
Epoch: 226 | Batch_idx: 390 |  Loss: (0.0432) |  Loss2: (0.0000) | Acc: (98.00%) (49290/50000)
# TEST : Loss: (0.4599) | Acc: (88.00%) (8877/10000)
percent tensor([0.5774, 0.5938, 0.5878, 0.5535, 0.5991, 0.5977, 0.6046, 0.5729, 0.5798,
        0.5883, 0.5904, 0.5973, 0.5771, 0.5558, 0.5962, 0.5720],
       device='cuda:0') torch.Size([16])
percent tensor([0.5439, 0.5454, 0.5529, 0.5561, 0.5538, 0.5416, 0.5488, 0.5582, 0.5484,
        0.5477, 0.5436, 0.5497, 0.5435, 0.5497, 0.5435, 0.5468],
       device='cuda:0') torch.Size([16])
percent tensor([0.7032, 0.7248, 0.5357, 0.4864, 0.4703, 0.6460, 0.6718, 0.5262, 0.6052,
        0.6958, 0.7092, 0.6083, 0.7348, 0.6825, 0.6890, 0.6896],
       device='cuda:0') torch.Size([16])
percent tensor([0.7472, 0.6363, 0.7284, 0.7223, 0.7302, 0.8005, 0.7010, 0.7272, 0.6947,
        0.6763, 0.6940, 0.6738, 0.6341, 0.7166, 0.7126, 0.7977],
       device='cuda:0') torch.Size([16])
percent tensor([0.6756, 0.6939, 0.6906, 0.6435, 0.6509, 0.6371, 0.6995, 0.6697, 0.7004,
        0.6727, 0.6565, 0.6451, 0.6750, 0.6828, 0.6879, 0.6423],
       device='cuda:0') torch.Size([16])
percent tensor([0.5995, 0.7742, 0.7098, 0.7570, 0.7280, 0.7810, 0.7728, 0.6059, 0.7903,
        0.6981, 0.8213, 0.8233, 0.7535, 0.8379, 0.6563, 0.5777],
       device='cuda:0') torch.Size([16])
percent tensor([0.5940, 0.7301, 0.6091, 0.4665, 0.6564, 0.7271, 0.6883, 0.4734, 0.6836,
        0.6951, 0.7315, 0.4860, 0.7244, 0.6108, 0.4043, 0.5091],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9999, 0.9999, 0.9999, 0.9998, 0.9998, 1.0000, 0.9996, 1.0000,
        0.9998, 1.0000, 1.0000, 1.0000, 0.9997, 0.9996, 0.9993],
       device='cuda:0') torch.Size([16])
Epoch: 227 | Batch_idx: 0 |  Loss: (0.0398) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 227 | Batch_idx: 10 |  Loss: (0.0496) |  Loss2: (0.0000) | Acc: (98.00%) (1389/1408)
Epoch: 227 | Batch_idx: 20 |  Loss: (0.0462) |  Loss2: (0.0000) | Acc: (98.00%) (2649/2688)
Epoch: 227 | Batch_idx: 30 |  Loss: (0.0445) |  Loss2: (0.0000) | Acc: (98.00%) (3914/3968)
Epoch: 227 | Batch_idx: 40 |  Loss: (0.0413) |  Loss2: (0.0000) | Acc: (98.00%) (5184/5248)
Epoch: 227 | Batch_idx: 50 |  Loss: (0.0408) |  Loss2: (0.0000) | Acc: (98.00%) (6448/6528)
Epoch: 227 | Batch_idx: 60 |  Loss: (0.0414) |  Loss2: (0.0000) | Acc: (98.00%) (7709/7808)
Epoch: 227 | Batch_idx: 70 |  Loss: (0.0401) |  Loss2: (0.0000) | Acc: (98.00%) (8976/9088)
Epoch: 227 | Batch_idx: 80 |  Loss: (0.0407) |  Loss2: (0.0000) | Acc: (98.00%) (10238/10368)
Epoch: 227 | Batch_idx: 90 |  Loss: (0.0394) |  Loss2: (0.0000) | Acc: (98.00%) (11507/11648)
Epoch: 227 | Batch_idx: 100 |  Loss: (0.0395) |  Loss2: (0.0000) | Acc: (98.00%) (12769/12928)
Epoch: 227 | Batch_idx: 110 |  Loss: (0.0395) |  Loss2: (0.0000) | Acc: (98.00%) (14036/14208)
Epoch: 227 | Batch_idx: 120 |  Loss: (0.0391) |  Loss2: (0.0000) | Acc: (98.00%) (15302/15488)
Epoch: 227 | Batch_idx: 130 |  Loss: (0.0391) |  Loss2: (0.0000) | Acc: (98.00%) (16566/16768)
Epoch: 227 | Batch_idx: 140 |  Loss: (0.0393) |  Loss2: (0.0000) | Acc: (98.00%) (17828/18048)
Epoch: 227 | Batch_idx: 150 |  Loss: (0.0394) |  Loss2: (0.0000) | Acc: (98.00%) (19092/19328)
Epoch: 227 | Batch_idx: 160 |  Loss: (0.0398) |  Loss2: (0.0000) | Acc: (98.00%) (20350/20608)
Epoch: 227 | Batch_idx: 170 |  Loss: (0.0397) |  Loss2: (0.0000) | Acc: (98.00%) (21614/21888)
Epoch: 227 | Batch_idx: 180 |  Loss: (0.0393) |  Loss2: (0.0000) | Acc: (98.00%) (22882/23168)
Epoch: 227 | Batch_idx: 190 |  Loss: (0.0393) |  Loss2: (0.0000) | Acc: (98.00%) (24148/24448)
Epoch: 227 | Batch_idx: 200 |  Loss: (0.0396) |  Loss2: (0.0000) | Acc: (98.00%) (25414/25728)
Epoch: 227 | Batch_idx: 210 |  Loss: (0.0397) |  Loss2: (0.0000) | Acc: (98.00%) (26680/27008)
Epoch: 227 | Batch_idx: 220 |  Loss: (0.0402) |  Loss2: (0.0000) | Acc: (98.00%) (27935/28288)
Epoch: 227 | Batch_idx: 230 |  Loss: (0.0402) |  Loss2: (0.0000) | Acc: (98.00%) (29198/29568)
Epoch: 227 | Batch_idx: 240 |  Loss: (0.0400) |  Loss2: (0.0000) | Acc: (98.00%) (30466/30848)
Epoch: 227 | Batch_idx: 250 |  Loss: (0.0405) |  Loss2: (0.0000) | Acc: (98.00%) (31727/32128)
Epoch: 227 | Batch_idx: 260 |  Loss: (0.0406) |  Loss2: (0.0000) | Acc: (98.00%) (32991/33408)
Epoch: 227 | Batch_idx: 270 |  Loss: (0.0406) |  Loss2: (0.0000) | Acc: (98.00%) (34254/34688)
Epoch: 227 | Batch_idx: 280 |  Loss: (0.0409) |  Loss2: (0.0000) | Acc: (98.00%) (35512/35968)
Epoch: 227 | Batch_idx: 290 |  Loss: (0.0412) |  Loss2: (0.0000) | Acc: (98.00%) (36772/37248)
Epoch: 227 | Batch_idx: 300 |  Loss: (0.0414) |  Loss2: (0.0000) | Acc: (98.00%) (38029/38528)
Epoch: 227 | Batch_idx: 310 |  Loss: (0.0417) |  Loss2: (0.0000) | Acc: (98.00%) (39287/39808)
Epoch: 227 | Batch_idx: 320 |  Loss: (0.0416) |  Loss2: (0.0000) | Acc: (98.00%) (40546/41088)
Epoch: 227 | Batch_idx: 330 |  Loss: (0.0417) |  Loss2: (0.0000) | Acc: (98.00%) (41809/42368)
Epoch: 227 | Batch_idx: 340 |  Loss: (0.0418) |  Loss2: (0.0000) | Acc: (98.00%) (43068/43648)
Epoch: 227 | Batch_idx: 350 |  Loss: (0.0419) |  Loss2: (0.0000) | Acc: (98.00%) (44328/44928)
Epoch: 227 | Batch_idx: 360 |  Loss: (0.0419) |  Loss2: (0.0000) | Acc: (98.00%) (45594/46208)
Epoch: 227 | Batch_idx: 370 |  Loss: (0.0422) |  Loss2: (0.0000) | Acc: (98.00%) (46846/47488)
Epoch: 227 | Batch_idx: 380 |  Loss: (0.0423) |  Loss2: (0.0000) | Acc: (98.00%) (48106/48768)
Epoch: 227 | Batch_idx: 390 |  Loss: (0.0422) |  Loss2: (0.0000) | Acc: (98.00%) (49325/50000)
# TEST : Loss: (0.4392) | Acc: (89.00%) (8911/10000)
percent tensor([0.5781, 0.5966, 0.5902, 0.5557, 0.6007, 0.5970, 0.6064, 0.5766, 0.5809,
        0.5915, 0.5915, 0.5997, 0.5789, 0.5582, 0.5977, 0.5735],
       device='cuda:0') torch.Size([16])
percent tensor([0.5437, 0.5452, 0.5519, 0.5547, 0.5535, 0.5420, 0.5487, 0.5569, 0.5485,
        0.5464, 0.5433, 0.5485, 0.5428, 0.5501, 0.5435, 0.5466],
       device='cuda:0') torch.Size([16])
percent tensor([0.6998, 0.7203, 0.5246, 0.4902, 0.4584, 0.6477, 0.6545, 0.5266, 0.5976,
        0.6909, 0.7025, 0.6135, 0.7361, 0.6733, 0.6823, 0.6876],
       device='cuda:0') torch.Size([16])
percent tensor([0.7449, 0.6316, 0.7252, 0.7175, 0.7292, 0.8140, 0.6947, 0.7183, 0.6989,
        0.6599, 0.6913, 0.6687, 0.6216, 0.7184, 0.7214, 0.7968],
       device='cuda:0') torch.Size([16])
percent tensor([0.6806, 0.6997, 0.6948, 0.6556, 0.6576, 0.6216, 0.7029, 0.6885, 0.6893,
        0.6751, 0.6468, 0.6540, 0.6793, 0.6706, 0.6877, 0.6406],
       device='cuda:0') torch.Size([16])
percent tensor([0.6076, 0.7771, 0.7069, 0.7731, 0.7195, 0.7680, 0.7615, 0.5921, 0.8090,
        0.6967, 0.8286, 0.8205, 0.7673, 0.8401, 0.6490, 0.5561],
       device='cuda:0') torch.Size([16])
percent tensor([0.5809, 0.7273, 0.5753, 0.4807, 0.6251, 0.6953, 0.6716, 0.4307, 0.6808,
        0.6689, 0.7265, 0.4719, 0.7268, 0.6034, 0.3814, 0.4633],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9998, 0.9999, 0.9999, 0.9999, 0.9997, 0.9999, 0.9997, 0.9999,
        0.9998, 0.9999, 0.9999, 1.0000, 0.9998, 0.9997, 0.9993],
       device='cuda:0') torch.Size([16])
Epoch: 228 | Batch_idx: 0 |  Loss: (0.0436) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 228 | Batch_idx: 10 |  Loss: (0.0494) |  Loss2: (0.0000) | Acc: (98.00%) (1385/1408)
Epoch: 228 | Batch_idx: 20 |  Loss: (0.0443) |  Loss2: (0.0000) | Acc: (98.00%) (2650/2688)
Epoch: 228 | Batch_idx: 30 |  Loss: (0.0385) |  Loss2: (0.0000) | Acc: (98.00%) (3921/3968)
Epoch: 228 | Batch_idx: 40 |  Loss: (0.0383) |  Loss2: (0.0000) | Acc: (98.00%) (5187/5248)
Epoch: 228 | Batch_idx: 50 |  Loss: (0.0370) |  Loss2: (0.0000) | Acc: (98.00%) (6456/6528)
Epoch: 228 | Batch_idx: 60 |  Loss: (0.0377) |  Loss2: (0.0000) | Acc: (98.00%) (7720/7808)
Epoch: 228 | Batch_idx: 70 |  Loss: (0.0378) |  Loss2: (0.0000) | Acc: (98.00%) (8984/9088)
Epoch: 228 | Batch_idx: 80 |  Loss: (0.0382) |  Loss2: (0.0000) | Acc: (98.00%) (10244/10368)
Epoch: 228 | Batch_idx: 90 |  Loss: (0.0401) |  Loss2: (0.0000) | Acc: (98.00%) (11501/11648)
Epoch: 228 | Batch_idx: 100 |  Loss: (0.0407) |  Loss2: (0.0000) | Acc: (98.00%) (12765/12928)
Epoch: 228 | Batch_idx: 110 |  Loss: (0.0411) |  Loss2: (0.0000) | Acc: (98.00%) (14024/14208)
Epoch: 228 | Batch_idx: 120 |  Loss: (0.0418) |  Loss2: (0.0000) | Acc: (98.00%) (15283/15488)
Epoch: 228 | Batch_idx: 130 |  Loss: (0.0425) |  Loss2: (0.0000) | Acc: (98.00%) (16544/16768)
Epoch: 228 | Batch_idx: 140 |  Loss: (0.0437) |  Loss2: (0.0000) | Acc: (98.00%) (17803/18048)
Epoch: 228 | Batch_idx: 150 |  Loss: (0.0441) |  Loss2: (0.0000) | Acc: (98.00%) (19059/19328)
Epoch: 228 | Batch_idx: 160 |  Loss: (0.0444) |  Loss2: (0.0000) | Acc: (98.00%) (20319/20608)
Epoch: 228 | Batch_idx: 170 |  Loss: (0.0436) |  Loss2: (0.0000) | Acc: (98.00%) (21593/21888)
Epoch: 228 | Batch_idx: 180 |  Loss: (0.0435) |  Loss2: (0.0000) | Acc: (98.00%) (22856/23168)
Epoch: 228 | Batch_idx: 190 |  Loss: (0.0435) |  Loss2: (0.0000) | Acc: (98.00%) (24120/24448)
Epoch: 228 | Batch_idx: 200 |  Loss: (0.0432) |  Loss2: (0.0000) | Acc: (98.00%) (25386/25728)
Epoch: 228 | Batch_idx: 210 |  Loss: (0.0426) |  Loss2: (0.0000) | Acc: (98.00%) (26655/27008)
Epoch: 228 | Batch_idx: 220 |  Loss: (0.0422) |  Loss2: (0.0000) | Acc: (98.00%) (27921/28288)
Epoch: 228 | Batch_idx: 230 |  Loss: (0.0419) |  Loss2: (0.0000) | Acc: (98.00%) (29185/29568)
Epoch: 228 | Batch_idx: 240 |  Loss: (0.0419) |  Loss2: (0.0000) | Acc: (98.00%) (30449/30848)
Epoch: 228 | Batch_idx: 250 |  Loss: (0.0417) |  Loss2: (0.0000) | Acc: (98.00%) (31714/32128)
Epoch: 228 | Batch_idx: 260 |  Loss: (0.0413) |  Loss2: (0.0000) | Acc: (98.00%) (32984/33408)
Epoch: 228 | Batch_idx: 270 |  Loss: (0.0413) |  Loss2: (0.0000) | Acc: (98.00%) (34244/34688)
Epoch: 228 | Batch_idx: 280 |  Loss: (0.0414) |  Loss2: (0.0000) | Acc: (98.00%) (35509/35968)
Epoch: 228 | Batch_idx: 290 |  Loss: (0.0412) |  Loss2: (0.0000) | Acc: (98.00%) (36780/37248)
Epoch: 228 | Batch_idx: 300 |  Loss: (0.0410) |  Loss2: (0.0000) | Acc: (98.00%) (38047/38528)
Epoch: 228 | Batch_idx: 310 |  Loss: (0.0410) |  Loss2: (0.0000) | Acc: (98.00%) (39305/39808)
Epoch: 228 | Batch_idx: 320 |  Loss: (0.0414) |  Loss2: (0.0000) | Acc: (98.00%) (40565/41088)
Epoch: 228 | Batch_idx: 330 |  Loss: (0.0415) |  Loss2: (0.0000) | Acc: (98.00%) (41828/42368)
Epoch: 228 | Batch_idx: 340 |  Loss: (0.0411) |  Loss2: (0.0000) | Acc: (98.00%) (43095/43648)
Epoch: 228 | Batch_idx: 350 |  Loss: (0.0409) |  Loss2: (0.0000) | Acc: (98.00%) (44366/44928)
Epoch: 228 | Batch_idx: 360 |  Loss: (0.0413) |  Loss2: (0.0000) | Acc: (98.00%) (45622/46208)
Epoch: 228 | Batch_idx: 370 |  Loss: (0.0418) |  Loss2: (0.0000) | Acc: (98.00%) (46876/47488)
Epoch: 228 | Batch_idx: 380 |  Loss: (0.0418) |  Loss2: (0.0000) | Acc: (98.00%) (48139/48768)
Epoch: 228 | Batch_idx: 390 |  Loss: (0.0421) |  Loss2: (0.0000) | Acc: (98.00%) (49346/50000)
# TEST : Loss: (0.4462) | Acc: (89.00%) (8903/10000)
percent tensor([0.5792, 0.5981, 0.5863, 0.5532, 0.5989, 0.5983, 0.6079, 0.5735, 0.5820,
        0.5910, 0.5941, 0.5984, 0.5795, 0.5618, 0.5984, 0.5742],
       device='cuda:0') torch.Size([16])
percent tensor([0.5439, 0.5445, 0.5543, 0.5559, 0.5547, 0.5426, 0.5487, 0.5580, 0.5484,
        0.5472, 0.5431, 0.5502, 0.5431, 0.5482, 0.5435, 0.5466],
       device='cuda:0') torch.Size([16])
percent tensor([0.7062, 0.7312, 0.5286, 0.4804, 0.4652, 0.6445, 0.6741, 0.5265, 0.6032,
        0.6974, 0.7113, 0.6160, 0.7404, 0.6855, 0.6938, 0.6900],
       device='cuda:0') torch.Size([16])
percent tensor([0.7452, 0.6366, 0.7170, 0.7205, 0.7223, 0.8116, 0.6946, 0.7250, 0.6902,
        0.6635, 0.6869, 0.6650, 0.6156, 0.7114, 0.7226, 0.7979],
       device='cuda:0') torch.Size([16])
percent tensor([0.6710, 0.6961, 0.6942, 0.6489, 0.6547, 0.6124, 0.7002, 0.6857, 0.7010,
        0.6744, 0.6451, 0.6569, 0.6827, 0.6831, 0.6763, 0.6245],
       device='cuda:0') torch.Size([16])
percent tensor([0.6197, 0.7719, 0.7297, 0.7480, 0.7317, 0.7946, 0.7763, 0.5867, 0.8092,
        0.7074, 0.8257, 0.8372, 0.7562, 0.8430, 0.6535, 0.5832],
       device='cuda:0') torch.Size([16])
percent tensor([0.5918, 0.7254, 0.5836, 0.4546, 0.6246, 0.7323, 0.6819, 0.4464, 0.6664,
        0.6861, 0.7433, 0.4710, 0.7145, 0.5952, 0.4138, 0.5123],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9999, 0.9999, 0.9999, 0.9998, 0.9997, 1.0000, 0.9997, 0.9999,
        0.9999, 0.9999, 0.9999, 1.0000, 0.9998, 0.9996, 0.9995],
       device='cuda:0') torch.Size([16])
Epoch: 229 | Batch_idx: 0 |  Loss: (0.0308) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 229 | Batch_idx: 10 |  Loss: (0.0412) |  Loss2: (0.0000) | Acc: (98.00%) (1393/1408)
Epoch: 229 | Batch_idx: 20 |  Loss: (0.0366) |  Loss2: (0.0000) | Acc: (99.00%) (2662/2688)
Epoch: 229 | Batch_idx: 30 |  Loss: (0.0360) |  Loss2: (0.0000) | Acc: (99.00%) (3931/3968)
Epoch: 229 | Batch_idx: 40 |  Loss: (0.0328) |  Loss2: (0.0000) | Acc: (99.00%) (5205/5248)
Epoch: 229 | Batch_idx: 50 |  Loss: (0.0336) |  Loss2: (0.0000) | Acc: (99.00%) (6467/6528)
Epoch: 229 | Batch_idx: 60 |  Loss: (0.0348) |  Loss2: (0.0000) | Acc: (99.00%) (7730/7808)
Epoch: 229 | Batch_idx: 70 |  Loss: (0.0337) |  Loss2: (0.0000) | Acc: (99.00%) (9004/9088)
Epoch: 229 | Batch_idx: 80 |  Loss: (0.0337) |  Loss2: (0.0000) | Acc: (99.00%) (10273/10368)
Epoch: 229 | Batch_idx: 90 |  Loss: (0.0341) |  Loss2: (0.0000) | Acc: (99.00%) (11540/11648)
Epoch: 229 | Batch_idx: 100 |  Loss: (0.0338) |  Loss2: (0.0000) | Acc: (99.00%) (12810/12928)
Epoch: 229 | Batch_idx: 110 |  Loss: (0.0336) |  Loss2: (0.0000) | Acc: (99.00%) (14073/14208)
Epoch: 229 | Batch_idx: 120 |  Loss: (0.0340) |  Loss2: (0.0000) | Acc: (99.00%) (15339/15488)
Epoch: 229 | Batch_idx: 130 |  Loss: (0.0341) |  Loss2: (0.0000) | Acc: (99.00%) (16606/16768)
Epoch: 229 | Batch_idx: 140 |  Loss: (0.0349) |  Loss2: (0.0000) | Acc: (99.00%) (17873/18048)
Epoch: 229 | Batch_idx: 150 |  Loss: (0.0353) |  Loss2: (0.0000) | Acc: (98.00%) (19132/19328)
Epoch: 229 | Batch_idx: 160 |  Loss: (0.0358) |  Loss2: (0.0000) | Acc: (98.00%) (20398/20608)
Epoch: 229 | Batch_idx: 170 |  Loss: (0.0364) |  Loss2: (0.0000) | Acc: (98.00%) (21654/21888)
Epoch: 229 | Batch_idx: 180 |  Loss: (0.0373) |  Loss2: (0.0000) | Acc: (98.00%) (22915/23168)
Epoch: 229 | Batch_idx: 190 |  Loss: (0.0379) |  Loss2: (0.0000) | Acc: (98.00%) (24167/24448)
Epoch: 229 | Batch_idx: 200 |  Loss: (0.0381) |  Loss2: (0.0000) | Acc: (98.00%) (25431/25728)
Epoch: 229 | Batch_idx: 210 |  Loss: (0.0384) |  Loss2: (0.0000) | Acc: (98.00%) (26693/27008)
Epoch: 229 | Batch_idx: 220 |  Loss: (0.0384) |  Loss2: (0.0000) | Acc: (98.00%) (27956/28288)
Epoch: 229 | Batch_idx: 230 |  Loss: (0.0382) |  Loss2: (0.0000) | Acc: (98.00%) (29223/29568)
Epoch: 229 | Batch_idx: 240 |  Loss: (0.0382) |  Loss2: (0.0000) | Acc: (98.00%) (30488/30848)
Epoch: 229 | Batch_idx: 250 |  Loss: (0.0381) |  Loss2: (0.0000) | Acc: (98.00%) (31754/32128)
Epoch: 229 | Batch_idx: 260 |  Loss: (0.0382) |  Loss2: (0.0000) | Acc: (98.00%) (33016/33408)
Epoch: 229 | Batch_idx: 270 |  Loss: (0.0383) |  Loss2: (0.0000) | Acc: (98.00%) (34279/34688)
Epoch: 229 | Batch_idx: 280 |  Loss: (0.0385) |  Loss2: (0.0000) | Acc: (98.00%) (35544/35968)
Epoch: 229 | Batch_idx: 290 |  Loss: (0.0383) |  Loss2: (0.0000) | Acc: (98.00%) (36811/37248)
Epoch: 229 | Batch_idx: 300 |  Loss: (0.0383) |  Loss2: (0.0000) | Acc: (98.00%) (38079/38528)
Epoch: 229 | Batch_idx: 310 |  Loss: (0.0382) |  Loss2: (0.0000) | Acc: (98.00%) (39348/39808)
Epoch: 229 | Batch_idx: 320 |  Loss: (0.0381) |  Loss2: (0.0000) | Acc: (98.00%) (40617/41088)
Epoch: 229 | Batch_idx: 330 |  Loss: (0.0383) |  Loss2: (0.0000) | Acc: (98.00%) (41880/42368)
Epoch: 229 | Batch_idx: 340 |  Loss: (0.0384) |  Loss2: (0.0000) | Acc: (98.00%) (43140/43648)
Epoch: 229 | Batch_idx: 350 |  Loss: (0.0383) |  Loss2: (0.0000) | Acc: (98.00%) (44402/44928)
Epoch: 229 | Batch_idx: 360 |  Loss: (0.0388) |  Loss2: (0.0000) | Acc: (98.00%) (45662/46208)
Epoch: 229 | Batch_idx: 370 |  Loss: (0.0391) |  Loss2: (0.0000) | Acc: (98.00%) (46922/47488)
Epoch: 229 | Batch_idx: 380 |  Loss: (0.0395) |  Loss2: (0.0000) | Acc: (98.00%) (48178/48768)
Epoch: 229 | Batch_idx: 390 |  Loss: (0.0398) |  Loss2: (0.0000) | Acc: (98.00%) (49387/50000)
# TEST : Loss: (0.4500) | Acc: (88.00%) (8893/10000)
percent tensor([0.5770, 0.5928, 0.5892, 0.5550, 0.5989, 0.5951, 0.6040, 0.5749, 0.5779,
        0.5891, 0.5887, 0.5980, 0.5768, 0.5540, 0.5952, 0.5717],
       device='cuda:0') torch.Size([16])
percent tensor([0.5426, 0.5453, 0.5508, 0.5538, 0.5523, 0.5410, 0.5486, 0.5561, 0.5474,
        0.5466, 0.5425, 0.5483, 0.5422, 0.5496, 0.5429, 0.5456],
       device='cuda:0') torch.Size([16])
percent tensor([0.7102, 0.7180, 0.5305, 0.4983, 0.4685, 0.6717, 0.6610, 0.5304, 0.6009,
        0.6892, 0.7089, 0.6063, 0.7373, 0.6737, 0.6939, 0.6967],
       device='cuda:0') torch.Size([16])
percent tensor([0.7435, 0.6393, 0.7110, 0.7209, 0.7162, 0.8070, 0.6926, 0.7239, 0.6915,
        0.6637, 0.6849, 0.6595, 0.6240, 0.7168, 0.7161, 0.7998],
       device='cuda:0') torch.Size([16])
percent tensor([0.6686, 0.6854, 0.6948, 0.6453, 0.6556, 0.6326, 0.7000, 0.6850, 0.6963,
        0.6692, 0.6464, 0.6531, 0.6671, 0.6672, 0.6745, 0.6299],
       device='cuda:0') torch.Size([16])
percent tensor([0.6152, 0.7843, 0.6975, 0.7538, 0.7062, 0.7918, 0.7739, 0.5968, 0.8110,
        0.7186, 0.8332, 0.8171, 0.7692, 0.8544, 0.6642, 0.5907],
       device='cuda:0') torch.Size([16])
percent tensor([0.6066, 0.7360, 0.5924, 0.4537, 0.5994, 0.7323, 0.6893, 0.4660, 0.6942,
        0.7204, 0.7725, 0.4987, 0.7405, 0.6318, 0.4129, 0.5145],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9998, 0.9999, 0.9998, 0.9999, 0.9994, 1.0000, 0.9998, 0.9999,
        0.9999, 1.0000, 0.9999, 0.9999, 0.9996, 0.9996, 0.9994],
       device='cuda:0') torch.Size([16])
Epoch: 230 | Batch_idx: 0 |  Loss: (0.0425) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 230 | Batch_idx: 10 |  Loss: (0.0344) |  Loss2: (0.0000) | Acc: (99.00%) (1394/1408)
Epoch: 230 | Batch_idx: 20 |  Loss: (0.0373) |  Loss2: (0.0000) | Acc: (99.00%) (2662/2688)
Epoch: 230 | Batch_idx: 30 |  Loss: (0.0397) |  Loss2: (0.0000) | Acc: (98.00%) (3926/3968)
Epoch: 230 | Batch_idx: 40 |  Loss: (0.0368) |  Loss2: (0.0000) | Acc: (99.00%) (5198/5248)
Epoch: 230 | Batch_idx: 50 |  Loss: (0.0398) |  Loss2: (0.0000) | Acc: (98.00%) (6458/6528)
Epoch: 230 | Batch_idx: 60 |  Loss: (0.0391) |  Loss2: (0.0000) | Acc: (98.00%) (7722/7808)
Epoch: 230 | Batch_idx: 70 |  Loss: (0.0388) |  Loss2: (0.0000) | Acc: (98.00%) (8988/9088)
Epoch: 230 | Batch_idx: 80 |  Loss: (0.0389) |  Loss2: (0.0000) | Acc: (98.00%) (10256/10368)
Epoch: 230 | Batch_idx: 90 |  Loss: (0.0387) |  Loss2: (0.0000) | Acc: (98.00%) (11524/11648)
Epoch: 230 | Batch_idx: 100 |  Loss: (0.0375) |  Loss2: (0.0000) | Acc: (98.00%) (12798/12928)
Epoch: 230 | Batch_idx: 110 |  Loss: (0.0375) |  Loss2: (0.0000) | Acc: (98.00%) (14064/14208)
Epoch: 230 | Batch_idx: 120 |  Loss: (0.0376) |  Loss2: (0.0000) | Acc: (98.00%) (15325/15488)
Epoch: 230 | Batch_idx: 130 |  Loss: (0.0377) |  Loss2: (0.0000) | Acc: (98.00%) (16590/16768)
Epoch: 230 | Batch_idx: 140 |  Loss: (0.0373) |  Loss2: (0.0000) | Acc: (98.00%) (17854/18048)
Epoch: 230 | Batch_idx: 150 |  Loss: (0.0373) |  Loss2: (0.0000) | Acc: (98.00%) (19123/19328)
Epoch: 230 | Batch_idx: 160 |  Loss: (0.0369) |  Loss2: (0.0000) | Acc: (98.00%) (20390/20608)
Epoch: 230 | Batch_idx: 170 |  Loss: (0.0363) |  Loss2: (0.0000) | Acc: (98.00%) (21660/21888)
Epoch: 230 | Batch_idx: 180 |  Loss: (0.0365) |  Loss2: (0.0000) | Acc: (98.00%) (22928/23168)
Epoch: 230 | Batch_idx: 190 |  Loss: (0.0366) |  Loss2: (0.0000) | Acc: (98.00%) (24195/24448)
Epoch: 230 | Batch_idx: 200 |  Loss: (0.0365) |  Loss2: (0.0000) | Acc: (98.00%) (25461/25728)
Epoch: 230 | Batch_idx: 210 |  Loss: (0.0364) |  Loss2: (0.0000) | Acc: (98.00%) (26726/27008)
Epoch: 230 | Batch_idx: 220 |  Loss: (0.0363) |  Loss2: (0.0000) | Acc: (98.00%) (27996/28288)
Epoch: 230 | Batch_idx: 230 |  Loss: (0.0363) |  Loss2: (0.0000) | Acc: (98.00%) (29258/29568)
Epoch: 230 | Batch_idx: 240 |  Loss: (0.0366) |  Loss2: (0.0000) | Acc: (98.00%) (30525/30848)
Epoch: 230 | Batch_idx: 250 |  Loss: (0.0367) |  Loss2: (0.0000) | Acc: (98.00%) (31788/32128)
Epoch: 230 | Batch_idx: 260 |  Loss: (0.0375) |  Loss2: (0.0000) | Acc: (98.00%) (33044/33408)
Epoch: 230 | Batch_idx: 270 |  Loss: (0.0376) |  Loss2: (0.0000) | Acc: (98.00%) (34307/34688)
Epoch: 230 | Batch_idx: 280 |  Loss: (0.0380) |  Loss2: (0.0000) | Acc: (98.00%) (35563/35968)
Epoch: 230 | Batch_idx: 290 |  Loss: (0.0384) |  Loss2: (0.0000) | Acc: (98.00%) (36820/37248)
Epoch: 230 | Batch_idx: 300 |  Loss: (0.0384) |  Loss2: (0.0000) | Acc: (98.00%) (38083/38528)
Epoch: 230 | Batch_idx: 310 |  Loss: (0.0388) |  Loss2: (0.0000) | Acc: (98.00%) (39346/39808)
Epoch: 230 | Batch_idx: 320 |  Loss: (0.0389) |  Loss2: (0.0000) | Acc: (98.00%) (40602/41088)
Epoch: 230 | Batch_idx: 330 |  Loss: (0.0390) |  Loss2: (0.0000) | Acc: (98.00%) (41866/42368)
Epoch: 230 | Batch_idx: 340 |  Loss: (0.0394) |  Loss2: (0.0000) | Acc: (98.00%) (43122/43648)
Epoch: 230 | Batch_idx: 350 |  Loss: (0.0393) |  Loss2: (0.0000) | Acc: (98.00%) (44387/44928)
Epoch: 230 | Batch_idx: 360 |  Loss: (0.0394) |  Loss2: (0.0000) | Acc: (98.00%) (45648/46208)
Epoch: 230 | Batch_idx: 370 |  Loss: (0.0393) |  Loss2: (0.0000) | Acc: (98.00%) (46909/47488)
Epoch: 230 | Batch_idx: 380 |  Loss: (0.0391) |  Loss2: (0.0000) | Acc: (98.00%) (48174/48768)
Epoch: 230 | Batch_idx: 390 |  Loss: (0.0389) |  Loss2: (0.0000) | Acc: (98.00%) (49391/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_230.pth.tar'
# TEST : Loss: (0.4257) | Acc: (89.00%) (8935/10000)
percent tensor([0.5786, 0.5947, 0.5916, 0.5563, 0.6021, 0.5985, 0.6062, 0.5760, 0.5800,
        0.5904, 0.5907, 0.6011, 0.5779, 0.5559, 0.5973, 0.5730],
       device='cuda:0') torch.Size([16])
percent tensor([0.5441, 0.5459, 0.5526, 0.5552, 0.5539, 0.5419, 0.5491, 0.5576, 0.5490,
        0.5476, 0.5442, 0.5496, 0.5440, 0.5490, 0.5438, 0.5467],
       device='cuda:0') torch.Size([16])
percent tensor([0.6987, 0.7181, 0.5317, 0.4830, 0.4716, 0.6547, 0.6641, 0.5189, 0.5966,
        0.6926, 0.7008, 0.6106, 0.7272, 0.6795, 0.6837, 0.6835],
       device='cuda:0') torch.Size([16])
percent tensor([0.7388, 0.6316, 0.7113, 0.7155, 0.7139, 0.8042, 0.6886, 0.7144, 0.6905,
        0.6585, 0.6856, 0.6609, 0.6150, 0.7090, 0.7125, 0.7918],
       device='cuda:0') torch.Size([16])
percent tensor([0.6795, 0.7008, 0.7052, 0.6455, 0.6644, 0.6353, 0.7051, 0.6966, 0.7002,
        0.6752, 0.6548, 0.6590, 0.6787, 0.6847, 0.6834, 0.6388],
       device='cuda:0') torch.Size([16])
percent tensor([0.6030, 0.7622, 0.7132, 0.7431, 0.7067, 0.7848, 0.7842, 0.5830, 0.7907,
        0.6936, 0.8174, 0.8207, 0.7496, 0.8334, 0.6460, 0.5833],
       device='cuda:0') torch.Size([16])
percent tensor([0.6131, 0.7205, 0.5983, 0.4345, 0.6048, 0.7282, 0.6881, 0.4676, 0.6630,
        0.6924, 0.7588, 0.5001, 0.7079, 0.5994, 0.4126, 0.4991],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9999, 0.9999, 0.9998, 0.9999, 0.9995, 1.0000, 0.9997, 0.9999,
        0.9999, 1.0000, 0.9999, 0.9999, 0.9998, 0.9996, 0.9992],
       device='cuda:0') torch.Size([16])
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(186.6916, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(842.0408, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(838.6687, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1509.8131, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(471.2928, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2334.7698, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4239.7837, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1334.7432, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6367.2515, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11458.2363, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3746.1587, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(15793.8926, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 231 | Batch_idx: 0 |  Loss: (0.0317) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 231 | Batch_idx: 10 |  Loss: (0.0340) |  Loss2: (0.0000) | Acc: (98.00%) (1392/1408)
Epoch: 231 | Batch_idx: 20 |  Loss: (0.0392) |  Loss2: (0.0000) | Acc: (98.00%) (2654/2688)
Epoch: 231 | Batch_idx: 30 |  Loss: (0.0403) |  Loss2: (0.0000) | Acc: (98.00%) (3917/3968)
Epoch: 231 | Batch_idx: 40 |  Loss: (0.0398) |  Loss2: (0.0000) | Acc: (98.00%) (5180/5248)
Epoch: 231 | Batch_idx: 50 |  Loss: (0.0410) |  Loss2: (0.0000) | Acc: (98.00%) (6438/6528)
Epoch: 231 | Batch_idx: 60 |  Loss: (0.0389) |  Loss2: (0.0000) | Acc: (98.00%) (7704/7808)
Epoch: 231 | Batch_idx: 70 |  Loss: (0.0401) |  Loss2: (0.0000) | Acc: (98.00%) (8962/9088)
Epoch: 231 | Batch_idx: 80 |  Loss: (0.0393) |  Loss2: (0.0000) | Acc: (98.00%) (10229/10368)
Epoch: 231 | Batch_idx: 90 |  Loss: (0.0396) |  Loss2: (0.0000) | Acc: (98.00%) (11492/11648)
Epoch: 231 | Batch_idx: 100 |  Loss: (0.0398) |  Loss2: (0.0000) | Acc: (98.00%) (12752/12928)
Epoch: 231 | Batch_idx: 110 |  Loss: (0.0402) |  Loss2: (0.0000) | Acc: (98.00%) (14013/14208)
Epoch: 231 | Batch_idx: 120 |  Loss: (0.0408) |  Loss2: (0.0000) | Acc: (98.00%) (15274/15488)
Epoch: 231 | Batch_idx: 130 |  Loss: (0.0416) |  Loss2: (0.0000) | Acc: (98.00%) (16529/16768)
Epoch: 231 | Batch_idx: 140 |  Loss: (0.0419) |  Loss2: (0.0000) | Acc: (98.00%) (17786/18048)
Epoch: 231 | Batch_idx: 150 |  Loss: (0.0423) |  Loss2: (0.0000) | Acc: (98.00%) (19044/19328)
Epoch: 231 | Batch_idx: 160 |  Loss: (0.0423) |  Loss2: (0.0000) | Acc: (98.00%) (20313/20608)
Epoch: 231 | Batch_idx: 170 |  Loss: (0.0421) |  Loss2: (0.0000) | Acc: (98.00%) (21580/21888)
Epoch: 231 | Batch_idx: 180 |  Loss: (0.0418) |  Loss2: (0.0000) | Acc: (98.00%) (22847/23168)
Epoch: 231 | Batch_idx: 190 |  Loss: (0.0415) |  Loss2: (0.0000) | Acc: (98.00%) (24112/24448)
Epoch: 231 | Batch_idx: 200 |  Loss: (0.0418) |  Loss2: (0.0000) | Acc: (98.00%) (25368/25728)
Epoch: 231 | Batch_idx: 210 |  Loss: (0.0417) |  Loss2: (0.0000) | Acc: (98.00%) (26630/27008)
Epoch: 231 | Batch_idx: 220 |  Loss: (0.0419) |  Loss2: (0.0000) | Acc: (98.00%) (27889/28288)
Epoch: 231 | Batch_idx: 230 |  Loss: (0.0418) |  Loss2: (0.0000) | Acc: (98.00%) (29151/29568)
Epoch: 231 | Batch_idx: 240 |  Loss: (0.0422) |  Loss2: (0.0000) | Acc: (98.00%) (30406/30848)
Epoch: 231 | Batch_idx: 250 |  Loss: (0.0422) |  Loss2: (0.0000) | Acc: (98.00%) (31666/32128)
Epoch: 231 | Batch_idx: 260 |  Loss: (0.0422) |  Loss2: (0.0000) | Acc: (98.00%) (32930/33408)
Epoch: 231 | Batch_idx: 270 |  Loss: (0.0420) |  Loss2: (0.0000) | Acc: (98.00%) (34190/34688)
Epoch: 231 | Batch_idx: 280 |  Loss: (0.0419) |  Loss2: (0.0000) | Acc: (98.00%) (35452/35968)
Epoch: 231 | Batch_idx: 290 |  Loss: (0.0420) |  Loss2: (0.0000) | Acc: (98.00%) (36716/37248)
Epoch: 231 | Batch_idx: 300 |  Loss: (0.0418) |  Loss2: (0.0000) | Acc: (98.00%) (37980/38528)
Epoch: 231 | Batch_idx: 310 |  Loss: (0.0421) |  Loss2: (0.0000) | Acc: (98.00%) (39243/39808)
Epoch: 231 | Batch_idx: 320 |  Loss: (0.0420) |  Loss2: (0.0000) | Acc: (98.00%) (40507/41088)
Epoch: 231 | Batch_idx: 330 |  Loss: (0.0420) |  Loss2: (0.0000) | Acc: (98.00%) (41772/42368)
Epoch: 231 | Batch_idx: 340 |  Loss: (0.0422) |  Loss2: (0.0000) | Acc: (98.00%) (43032/43648)
Epoch: 231 | Batch_idx: 350 |  Loss: (0.0420) |  Loss2: (0.0000) | Acc: (98.00%) (44300/44928)
Epoch: 231 | Batch_idx: 360 |  Loss: (0.0419) |  Loss2: (0.0000) | Acc: (98.00%) (45564/46208)
Epoch: 231 | Batch_idx: 370 |  Loss: (0.0423) |  Loss2: (0.0000) | Acc: (98.00%) (46825/47488)
Epoch: 231 | Batch_idx: 380 |  Loss: (0.0421) |  Loss2: (0.0000) | Acc: (98.00%) (48089/48768)
Epoch: 231 | Batch_idx: 390 |  Loss: (0.0425) |  Loss2: (0.0000) | Acc: (98.00%) (49298/50000)
# TEST : Loss: (0.4661) | Acc: (88.00%) (8895/10000)
percent tensor([0.5748, 0.5945, 0.5840, 0.5534, 0.5952, 0.5942, 0.6037, 0.5734, 0.5779,
        0.5877, 0.5893, 0.5942, 0.5757, 0.5595, 0.5946, 0.5712],
       device='cuda:0') torch.Size([16])
percent tensor([0.5434, 0.5450, 0.5533, 0.5558, 0.5547, 0.5425, 0.5484, 0.5572, 0.5477,
        0.5465, 0.5425, 0.5498, 0.5427, 0.5481, 0.5436, 0.5462],
       device='cuda:0') torch.Size([16])
percent tensor([0.7069, 0.7166, 0.5179, 0.4725, 0.4508, 0.6493, 0.6598, 0.5380, 0.6031,
        0.6856, 0.7138, 0.5986, 0.7324, 0.6837, 0.6816, 0.6842],
       device='cuda:0') torch.Size([16])
percent tensor([0.7372, 0.6324, 0.7085, 0.7185, 0.7160, 0.8075, 0.6897, 0.7133, 0.6879,
        0.6562, 0.6879, 0.6642, 0.6131, 0.7167, 0.7130, 0.7965],
       device='cuda:0') torch.Size([16])
percent tensor([0.6849, 0.6967, 0.7011, 0.6406, 0.6508, 0.6213, 0.7003, 0.6951, 0.7025,
        0.6739, 0.6504, 0.6586, 0.6837, 0.6884, 0.6834, 0.6314],
       device='cuda:0') torch.Size([16])
percent tensor([0.6395, 0.7667, 0.7232, 0.7595, 0.7215, 0.7916, 0.7906, 0.6078, 0.7967,
        0.6790, 0.8199, 0.8239, 0.7532, 0.8371, 0.6685, 0.5886],
       device='cuda:0') torch.Size([16])
percent tensor([0.6122, 0.7091, 0.6203, 0.4683, 0.6023, 0.7367, 0.6873, 0.4672, 0.6619,
        0.6713, 0.7602, 0.4759, 0.6961, 0.6113, 0.4205, 0.4970],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9999, 0.9999, 0.9998, 0.9999, 0.9995, 1.0000, 0.9997, 1.0000,
        0.9998, 1.0000, 0.9999, 0.9999, 0.9998, 0.9997, 0.9995],
       device='cuda:0') torch.Size([16])
Epoch: 232 | Batch_idx: 0 |  Loss: (0.0779) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 232 | Batch_idx: 10 |  Loss: (0.0445) |  Loss2: (0.0000) | Acc: (98.00%) (1385/1408)
Epoch: 232 | Batch_idx: 20 |  Loss: (0.0427) |  Loss2: (0.0000) | Acc: (98.00%) (2646/2688)
Epoch: 232 | Batch_idx: 30 |  Loss: (0.0418) |  Loss2: (0.0000) | Acc: (98.00%) (3908/3968)
Epoch: 232 | Batch_idx: 40 |  Loss: (0.0412) |  Loss2: (0.0000) | Acc: (98.00%) (5175/5248)
Epoch: 232 | Batch_idx: 50 |  Loss: (0.0387) |  Loss2: (0.0000) | Acc: (98.00%) (6447/6528)
Epoch: 232 | Batch_idx: 60 |  Loss: (0.0388) |  Loss2: (0.0000) | Acc: (98.00%) (7708/7808)
Epoch: 232 | Batch_idx: 70 |  Loss: (0.0413) |  Loss2: (0.0000) | Acc: (98.00%) (8961/9088)
Epoch: 232 | Batch_idx: 80 |  Loss: (0.0416) |  Loss2: (0.0000) | Acc: (98.00%) (10227/10368)
Epoch: 232 | Batch_idx: 90 |  Loss: (0.0413) |  Loss2: (0.0000) | Acc: (98.00%) (11496/11648)
Epoch: 232 | Batch_idx: 100 |  Loss: (0.0409) |  Loss2: (0.0000) | Acc: (98.00%) (12762/12928)
Epoch: 232 | Batch_idx: 110 |  Loss: (0.0396) |  Loss2: (0.0000) | Acc: (98.00%) (14031/14208)
Epoch: 232 | Batch_idx: 120 |  Loss: (0.0391) |  Loss2: (0.0000) | Acc: (98.00%) (15296/15488)
Epoch: 232 | Batch_idx: 130 |  Loss: (0.0390) |  Loss2: (0.0000) | Acc: (98.00%) (16562/16768)
Epoch: 232 | Batch_idx: 140 |  Loss: (0.0391) |  Loss2: (0.0000) | Acc: (98.00%) (17826/18048)
Epoch: 232 | Batch_idx: 150 |  Loss: (0.0397) |  Loss2: (0.0000) | Acc: (98.00%) (19083/19328)
Epoch: 232 | Batch_idx: 160 |  Loss: (0.0400) |  Loss2: (0.0000) | Acc: (98.00%) (20344/20608)
Epoch: 232 | Batch_idx: 170 |  Loss: (0.0394) |  Loss2: (0.0000) | Acc: (98.00%) (21616/21888)
Epoch: 232 | Batch_idx: 180 |  Loss: (0.0397) |  Loss2: (0.0000) | Acc: (98.00%) (22875/23168)
Epoch: 232 | Batch_idx: 190 |  Loss: (0.0399) |  Loss2: (0.0000) | Acc: (98.00%) (24138/24448)
Epoch: 232 | Batch_idx: 200 |  Loss: (0.0396) |  Loss2: (0.0000) | Acc: (98.00%) (25403/25728)
Epoch: 232 | Batch_idx: 210 |  Loss: (0.0401) |  Loss2: (0.0000) | Acc: (98.00%) (26660/27008)
Epoch: 232 | Batch_idx: 220 |  Loss: (0.0402) |  Loss2: (0.0000) | Acc: (98.00%) (27922/28288)
Epoch: 232 | Batch_idx: 230 |  Loss: (0.0404) |  Loss2: (0.0000) | Acc: (98.00%) (29182/29568)
Epoch: 232 | Batch_idx: 240 |  Loss: (0.0408) |  Loss2: (0.0000) | Acc: (98.00%) (30440/30848)
Epoch: 232 | Batch_idx: 250 |  Loss: (0.0411) |  Loss2: (0.0000) | Acc: (98.00%) (31701/32128)
Epoch: 232 | Batch_idx: 260 |  Loss: (0.0413) |  Loss2: (0.0000) | Acc: (98.00%) (32958/33408)
Epoch: 232 | Batch_idx: 270 |  Loss: (0.0417) |  Loss2: (0.0000) | Acc: (98.00%) (34213/34688)
Epoch: 232 | Batch_idx: 280 |  Loss: (0.0417) |  Loss2: (0.0000) | Acc: (98.00%) (35479/35968)
Epoch: 232 | Batch_idx: 290 |  Loss: (0.0415) |  Loss2: (0.0000) | Acc: (98.00%) (36744/37248)
Epoch: 232 | Batch_idx: 300 |  Loss: (0.0416) |  Loss2: (0.0000) | Acc: (98.00%) (38008/38528)
Epoch: 232 | Batch_idx: 310 |  Loss: (0.0415) |  Loss2: (0.0000) | Acc: (98.00%) (39273/39808)
Epoch: 232 | Batch_idx: 320 |  Loss: (0.0416) |  Loss2: (0.0000) | Acc: (98.00%) (40531/41088)
Epoch: 232 | Batch_idx: 330 |  Loss: (0.0417) |  Loss2: (0.0000) | Acc: (98.00%) (41783/42368)
Epoch: 232 | Batch_idx: 340 |  Loss: (0.0419) |  Loss2: (0.0000) | Acc: (98.00%) (43043/43648)
Epoch: 232 | Batch_idx: 350 |  Loss: (0.0420) |  Loss2: (0.0000) | Acc: (98.00%) (44300/44928)
Epoch: 232 | Batch_idx: 360 |  Loss: (0.0419) |  Loss2: (0.0000) | Acc: (98.00%) (45563/46208)
Epoch: 232 | Batch_idx: 370 |  Loss: (0.0419) |  Loss2: (0.0000) | Acc: (98.00%) (46823/47488)
Epoch: 232 | Batch_idx: 380 |  Loss: (0.0419) |  Loss2: (0.0000) | Acc: (98.00%) (48088/48768)
Epoch: 232 | Batch_idx: 390 |  Loss: (0.0420) |  Loss2: (0.0000) | Acc: (98.00%) (49301/50000)
# TEST : Loss: (0.4632) | Acc: (88.00%) (8889/10000)
percent tensor([0.5800, 0.5994, 0.5887, 0.5577, 0.5998, 0.5987, 0.6081, 0.5776, 0.5824,
        0.5925, 0.5949, 0.5977, 0.5808, 0.5622, 0.5998, 0.5761],
       device='cuda:0') torch.Size([16])
percent tensor([0.5452, 0.5469, 0.5551, 0.5573, 0.5565, 0.5433, 0.5502, 0.5598, 0.5503,
        0.5485, 0.5444, 0.5517, 0.5447, 0.5506, 0.5447, 0.5481],
       device='cuda:0') torch.Size([16])
percent tensor([0.7042, 0.7222, 0.5210, 0.4874, 0.4604, 0.6573, 0.6650, 0.5291, 0.6008,
        0.6973, 0.7106, 0.6130, 0.7333, 0.6793, 0.6935, 0.6850],
       device='cuda:0') torch.Size([16])
percent tensor([0.7332, 0.6320, 0.7207, 0.7180, 0.7206, 0.8022, 0.6878, 0.7230, 0.6974,
        0.6650, 0.6854, 0.6705, 0.6161, 0.7152, 0.7121, 0.7926],
       device='cuda:0') torch.Size([16])
percent tensor([0.6882, 0.7122, 0.6857, 0.6447, 0.6459, 0.6404, 0.7037, 0.6829, 0.6926,
        0.6747, 0.6531, 0.6473, 0.6852, 0.6785, 0.6934, 0.6450],
       device='cuda:0') torch.Size([16])
percent tensor([0.6107, 0.7542, 0.7145, 0.7490, 0.7151, 0.7959, 0.7841, 0.5636, 0.8063,
        0.7056, 0.8208, 0.8374, 0.7578, 0.8448, 0.6556, 0.5838],
       device='cuda:0') torch.Size([16])
percent tensor([0.5970, 0.6980, 0.5820, 0.4181, 0.5810, 0.7223, 0.6936, 0.4162, 0.6781,
        0.6801, 0.7712, 0.4800, 0.7170, 0.6416, 0.3996, 0.5075],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9999, 0.9999, 0.9998, 0.9998, 0.9995, 1.0000, 0.9996, 0.9999,
        0.9998, 1.0000, 0.9999, 0.9999, 0.9997, 0.9996, 0.9992],
       device='cuda:0') torch.Size([16])
Epoch: 233 | Batch_idx: 0 |  Loss: (0.0318) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 233 | Batch_idx: 10 |  Loss: (0.0316) |  Loss2: (0.0000) | Acc: (99.00%) (1394/1408)
Epoch: 233 | Batch_idx: 20 |  Loss: (0.0317) |  Loss2: (0.0000) | Acc: (99.00%) (2663/2688)
Epoch: 233 | Batch_idx: 30 |  Loss: (0.0328) |  Loss2: (0.0000) | Acc: (99.00%) (3930/3968)
Epoch: 233 | Batch_idx: 40 |  Loss: (0.0332) |  Loss2: (0.0000) | Acc: (99.00%) (5198/5248)
Epoch: 233 | Batch_idx: 50 |  Loss: (0.0324) |  Loss2: (0.0000) | Acc: (99.00%) (6468/6528)
Epoch: 233 | Batch_idx: 60 |  Loss: (0.0334) |  Loss2: (0.0000) | Acc: (99.00%) (7734/7808)
Epoch: 233 | Batch_idx: 70 |  Loss: (0.0336) |  Loss2: (0.0000) | Acc: (98.00%) (8997/9088)
Epoch: 233 | Batch_idx: 80 |  Loss: (0.0339) |  Loss2: (0.0000) | Acc: (99.00%) (10267/10368)
Epoch: 233 | Batch_idx: 90 |  Loss: (0.0340) |  Loss2: (0.0000) | Acc: (99.00%) (11535/11648)
Epoch: 233 | Batch_idx: 100 |  Loss: (0.0347) |  Loss2: (0.0000) | Acc: (98.00%) (12792/12928)
Epoch: 233 | Batch_idx: 110 |  Loss: (0.0352) |  Loss2: (0.0000) | Acc: (98.00%) (14060/14208)
Epoch: 233 | Batch_idx: 120 |  Loss: (0.0358) |  Loss2: (0.0000) | Acc: (98.00%) (15322/15488)
Epoch: 233 | Batch_idx: 130 |  Loss: (0.0365) |  Loss2: (0.0000) | Acc: (98.00%) (16583/16768)
Epoch: 233 | Batch_idx: 140 |  Loss: (0.0372) |  Loss2: (0.0000) | Acc: (98.00%) (17840/18048)
Epoch: 233 | Batch_idx: 150 |  Loss: (0.0367) |  Loss2: (0.0000) | Acc: (98.00%) (19112/19328)
Epoch: 233 | Batch_idx: 160 |  Loss: (0.0373) |  Loss2: (0.0000) | Acc: (98.00%) (20373/20608)
Epoch: 233 | Batch_idx: 170 |  Loss: (0.0372) |  Loss2: (0.0000) | Acc: (98.00%) (21640/21888)
Epoch: 233 | Batch_idx: 180 |  Loss: (0.0376) |  Loss2: (0.0000) | Acc: (98.00%) (22898/23168)
Epoch: 233 | Batch_idx: 190 |  Loss: (0.0371) |  Loss2: (0.0000) | Acc: (98.00%) (24168/24448)
Epoch: 233 | Batch_idx: 200 |  Loss: (0.0366) |  Loss2: (0.0000) | Acc: (98.00%) (25437/25728)
Epoch: 233 | Batch_idx: 210 |  Loss: (0.0366) |  Loss2: (0.0000) | Acc: (98.00%) (26704/27008)
Epoch: 233 | Batch_idx: 220 |  Loss: (0.0364) |  Loss2: (0.0000) | Acc: (98.00%) (27971/28288)
Epoch: 233 | Batch_idx: 230 |  Loss: (0.0367) |  Loss2: (0.0000) | Acc: (98.00%) (29234/29568)
Epoch: 233 | Batch_idx: 240 |  Loss: (0.0362) |  Loss2: (0.0000) | Acc: (98.00%) (30505/30848)
Epoch: 233 | Batch_idx: 250 |  Loss: (0.0362) |  Loss2: (0.0000) | Acc: (98.00%) (31769/32128)
Epoch: 233 | Batch_idx: 260 |  Loss: (0.0362) |  Loss2: (0.0000) | Acc: (98.00%) (33036/33408)
Epoch: 233 | Batch_idx: 270 |  Loss: (0.0363) |  Loss2: (0.0000) | Acc: (98.00%) (34298/34688)
Epoch: 233 | Batch_idx: 280 |  Loss: (0.0368) |  Loss2: (0.0000) | Acc: (98.00%) (35555/35968)
Epoch: 233 | Batch_idx: 290 |  Loss: (0.0370) |  Loss2: (0.0000) | Acc: (98.00%) (36822/37248)
Epoch: 233 | Batch_idx: 300 |  Loss: (0.0371) |  Loss2: (0.0000) | Acc: (98.00%) (38082/38528)
Epoch: 233 | Batch_idx: 310 |  Loss: (0.0370) |  Loss2: (0.0000) | Acc: (98.00%) (39352/39808)
Epoch: 233 | Batch_idx: 320 |  Loss: (0.0369) |  Loss2: (0.0000) | Acc: (98.00%) (40619/41088)
Epoch: 233 | Batch_idx: 330 |  Loss: (0.0368) |  Loss2: (0.0000) | Acc: (98.00%) (41886/42368)
Epoch: 233 | Batch_idx: 340 |  Loss: (0.0370) |  Loss2: (0.0000) | Acc: (98.00%) (43149/43648)
Epoch: 233 | Batch_idx: 350 |  Loss: (0.0370) |  Loss2: (0.0000) | Acc: (98.00%) (44412/44928)
Epoch: 233 | Batch_idx: 360 |  Loss: (0.0373) |  Loss2: (0.0000) | Acc: (98.00%) (45670/46208)
Epoch: 233 | Batch_idx: 370 |  Loss: (0.0371) |  Loss2: (0.0000) | Acc: (98.00%) (46942/47488)
Epoch: 233 | Batch_idx: 380 |  Loss: (0.0376) |  Loss2: (0.0000) | Acc: (98.00%) (48202/48768)
Epoch: 233 | Batch_idx: 390 |  Loss: (0.0372) |  Loss2: (0.0000) | Acc: (98.00%) (49429/50000)
# TEST : Loss: (0.4407) | Acc: (89.00%) (8913/10000)
percent tensor([0.5824, 0.6013, 0.5891, 0.5583, 0.6016, 0.6002, 0.6105, 0.5773, 0.5850,
        0.5938, 0.5971, 0.5997, 0.5825, 0.5646, 0.6010, 0.5777],
       device='cuda:0') torch.Size([16])
percent tensor([0.5449, 0.5468, 0.5553, 0.5574, 0.5562, 0.5436, 0.5500, 0.5596, 0.5491,
        0.5484, 0.5437, 0.5512, 0.5446, 0.5499, 0.5451, 0.5477],
       device='cuda:0') torch.Size([16])
percent tensor([0.7011, 0.7206, 0.5119, 0.4813, 0.4496, 0.6378, 0.6644, 0.5165, 0.6091,
        0.6921, 0.7141, 0.5956, 0.7319, 0.6817, 0.6839, 0.6825],
       device='cuda:0') torch.Size([16])
percent tensor([0.7409, 0.6415, 0.7099, 0.7257, 0.7188, 0.8029, 0.6933, 0.7214, 0.6871,
        0.6618, 0.6824, 0.6605, 0.6103, 0.7124, 0.7173, 0.7967],
       device='cuda:0') torch.Size([16])
percent tensor([0.6771, 0.6984, 0.7010, 0.6422, 0.6658, 0.6254, 0.7095, 0.6901, 0.7111,
        0.6713, 0.6571, 0.6590, 0.6838, 0.6854, 0.6856, 0.6309],
       device='cuda:0') torch.Size([16])
percent tensor([0.6283, 0.7788, 0.7368, 0.7698, 0.7414, 0.7979, 0.7867, 0.6247, 0.8066,
        0.7064, 0.8254, 0.8288, 0.7466, 0.8448, 0.6635, 0.5891],
       device='cuda:0') torch.Size([16])
percent tensor([0.6244, 0.7510, 0.6060, 0.4355, 0.6185, 0.7030, 0.6891, 0.4576, 0.6941,
        0.7179, 0.7770, 0.4909, 0.7368, 0.6304, 0.4008, 0.4965],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9999, 1.0000, 0.9998, 0.9999, 0.9997, 1.0000, 0.9996, 0.9999,
        0.9999, 1.0000, 0.9999, 0.9999, 0.9998, 0.9996, 0.9993],
       device='cuda:0') torch.Size([16])
Epoch: 234 | Batch_idx: 0 |  Loss: (0.0345) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 234 | Batch_idx: 10 |  Loss: (0.0374) |  Loss2: (0.0000) | Acc: (98.00%) (1392/1408)
Epoch: 234 | Batch_idx: 20 |  Loss: (0.0327) |  Loss2: (0.0000) | Acc: (98.00%) (2661/2688)
Epoch: 234 | Batch_idx: 30 |  Loss: (0.0349) |  Loss2: (0.0000) | Acc: (98.00%) (3926/3968)
Epoch: 234 | Batch_idx: 40 |  Loss: (0.0334) |  Loss2: (0.0000) | Acc: (99.00%) (5196/5248)
Epoch: 234 | Batch_idx: 50 |  Loss: (0.0323) |  Loss2: (0.0000) | Acc: (99.00%) (6466/6528)
Epoch: 234 | Batch_idx: 60 |  Loss: (0.0340) |  Loss2: (0.0000) | Acc: (98.00%) (7729/7808)
Epoch: 234 | Batch_idx: 70 |  Loss: (0.0338) |  Loss2: (0.0000) | Acc: (99.00%) (8999/9088)
Epoch: 234 | Batch_idx: 80 |  Loss: (0.0328) |  Loss2: (0.0000) | Acc: (99.00%) (10272/10368)
Epoch: 234 | Batch_idx: 90 |  Loss: (0.0331) |  Loss2: (0.0000) | Acc: (99.00%) (11538/11648)
Epoch: 234 | Batch_idx: 100 |  Loss: (0.0335) |  Loss2: (0.0000) | Acc: (99.00%) (12800/12928)
Epoch: 234 | Batch_idx: 110 |  Loss: (0.0332) |  Loss2: (0.0000) | Acc: (99.00%) (14067/14208)
Epoch: 234 | Batch_idx: 120 |  Loss: (0.0333) |  Loss2: (0.0000) | Acc: (98.00%) (15330/15488)
Epoch: 234 | Batch_idx: 130 |  Loss: (0.0337) |  Loss2: (0.0000) | Acc: (98.00%) (16596/16768)
Epoch: 234 | Batch_idx: 140 |  Loss: (0.0340) |  Loss2: (0.0000) | Acc: (98.00%) (17861/18048)
Epoch: 234 | Batch_idx: 150 |  Loss: (0.0336) |  Loss2: (0.0000) | Acc: (98.00%) (19131/19328)
Epoch: 234 | Batch_idx: 160 |  Loss: (0.0339) |  Loss2: (0.0000) | Acc: (98.00%) (20393/20608)
Epoch: 234 | Batch_idx: 170 |  Loss: (0.0338) |  Loss2: (0.0000) | Acc: (98.00%) (21661/21888)
Epoch: 234 | Batch_idx: 180 |  Loss: (0.0343) |  Loss2: (0.0000) | Acc: (98.00%) (22925/23168)
Epoch: 234 | Batch_idx: 190 |  Loss: (0.0341) |  Loss2: (0.0000) | Acc: (98.00%) (24200/24448)
Epoch: 234 | Batch_idx: 200 |  Loss: (0.0338) |  Loss2: (0.0000) | Acc: (98.00%) (25470/25728)
Epoch: 234 | Batch_idx: 210 |  Loss: (0.0338) |  Loss2: (0.0000) | Acc: (98.00%) (26733/27008)
Epoch: 234 | Batch_idx: 220 |  Loss: (0.0344) |  Loss2: (0.0000) | Acc: (98.00%) (27996/28288)
Epoch: 234 | Batch_idx: 230 |  Loss: (0.0342) |  Loss2: (0.0000) | Acc: (98.00%) (29262/29568)
Epoch: 234 | Batch_idx: 240 |  Loss: (0.0343) |  Loss2: (0.0000) | Acc: (98.00%) (30525/30848)
Epoch: 234 | Batch_idx: 250 |  Loss: (0.0353) |  Loss2: (0.0000) | Acc: (98.00%) (31776/32128)
Epoch: 234 | Batch_idx: 260 |  Loss: (0.0355) |  Loss2: (0.0000) | Acc: (98.00%) (33039/33408)
Epoch: 234 | Batch_idx: 270 |  Loss: (0.0356) |  Loss2: (0.0000) | Acc: (98.00%) (34303/34688)
Epoch: 234 | Batch_idx: 280 |  Loss: (0.0361) |  Loss2: (0.0000) | Acc: (98.00%) (35561/35968)
Epoch: 234 | Batch_idx: 290 |  Loss: (0.0360) |  Loss2: (0.0000) | Acc: (98.00%) (36828/37248)
Epoch: 234 | Batch_idx: 300 |  Loss: (0.0361) |  Loss2: (0.0000) | Acc: (98.00%) (38088/38528)
Epoch: 234 | Batch_idx: 310 |  Loss: (0.0362) |  Loss2: (0.0000) | Acc: (98.00%) (39352/39808)
Epoch: 234 | Batch_idx: 320 |  Loss: (0.0364) |  Loss2: (0.0000) | Acc: (98.00%) (40619/41088)
Epoch: 234 | Batch_idx: 330 |  Loss: (0.0365) |  Loss2: (0.0000) | Acc: (98.00%) (41881/42368)
Epoch: 234 | Batch_idx: 340 |  Loss: (0.0371) |  Loss2: (0.0000) | Acc: (98.00%) (43140/43648)
Epoch: 234 | Batch_idx: 350 |  Loss: (0.0373) |  Loss2: (0.0000) | Acc: (98.00%) (44401/44928)
Epoch: 234 | Batch_idx: 360 |  Loss: (0.0373) |  Loss2: (0.0000) | Acc: (98.00%) (45666/46208)
Epoch: 234 | Batch_idx: 370 |  Loss: (0.0370) |  Loss2: (0.0000) | Acc: (98.00%) (46941/47488)
Epoch: 234 | Batch_idx: 380 |  Loss: (0.0372) |  Loss2: (0.0000) | Acc: (98.00%) (48205/48768)
Epoch: 234 | Batch_idx: 390 |  Loss: (0.0372) |  Loss2: (0.0000) | Acc: (98.00%) (49415/50000)
# TEST : Loss: (0.4377) | Acc: (89.00%) (8904/10000)
percent tensor([0.5762, 0.5985, 0.5805, 0.5542, 0.5933, 0.5937, 0.6057, 0.5727, 0.5787,
        0.5894, 0.5911, 0.5923, 0.5775, 0.5652, 0.5964, 0.5731],
       device='cuda:0') torch.Size([16])
percent tensor([0.5462, 0.5479, 0.5560, 0.5583, 0.5570, 0.5448, 0.5507, 0.5600, 0.5504,
        0.5494, 0.5458, 0.5517, 0.5456, 0.5514, 0.5464, 0.5494],
       device='cuda:0') torch.Size([16])
percent tensor([0.6956, 0.7213, 0.5029, 0.4637, 0.4440, 0.6350, 0.6607, 0.5092, 0.5975,
        0.6989, 0.7093, 0.6037, 0.7307, 0.6753, 0.6817, 0.6798],
       device='cuda:0') torch.Size([16])
percent tensor([0.7420, 0.6395, 0.7237, 0.7316, 0.7251, 0.8105, 0.6857, 0.7227, 0.6884,
        0.6634, 0.6818, 0.6630, 0.6104, 0.7177, 0.7187, 0.7954],
       device='cuda:0') torch.Size([16])
percent tensor([0.6842, 0.7054, 0.6883, 0.6268, 0.6531, 0.6089, 0.7158, 0.6892, 0.7098,
        0.6803, 0.6667, 0.6588, 0.6916, 0.6836, 0.6825, 0.6414],
       device='cuda:0') torch.Size([16])
percent tensor([0.6432, 0.7752, 0.7308, 0.7858, 0.7315, 0.8103, 0.7988, 0.6193, 0.8056,
        0.7043, 0.8163, 0.8376, 0.7634, 0.8571, 0.6796, 0.6074],
       device='cuda:0') torch.Size([16])
percent tensor([0.6049, 0.7235, 0.5690, 0.4347, 0.5805, 0.7077, 0.6771, 0.4409, 0.6949,
        0.6787, 0.7560, 0.4763, 0.7277, 0.6104, 0.4065, 0.5045],
       device='cuda:0') torch.Size([16])
percent tensor([0.9998, 0.9999, 0.9999, 0.9997, 0.9999, 0.9995, 1.0000, 0.9996, 0.9999,
        0.9999, 1.0000, 0.9999, 0.9999, 0.9997, 0.9995, 0.9993],
       device='cuda:0') torch.Size([16])
Epoch: 235 | Batch_idx: 0 |  Loss: (0.0356) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 235 | Batch_idx: 10 |  Loss: (0.0410) |  Loss2: (0.0000) | Acc: (98.00%) (1389/1408)
Epoch: 235 | Batch_idx: 20 |  Loss: (0.0368) |  Loss2: (0.0000) | Acc: (98.00%) (2655/2688)
Epoch: 235 | Batch_idx: 30 |  Loss: (0.0350) |  Loss2: (0.0000) | Acc: (98.00%) (3926/3968)
Epoch: 235 | Batch_idx: 40 |  Loss: (0.0355) |  Loss2: (0.0000) | Acc: (98.00%) (5190/5248)
Epoch: 235 | Batch_idx: 50 |  Loss: (0.0345) |  Loss2: (0.0000) | Acc: (98.00%) (6456/6528)
Epoch: 235 | Batch_idx: 60 |  Loss: (0.0338) |  Loss2: (0.0000) | Acc: (98.00%) (7724/7808)
Epoch: 235 | Batch_idx: 70 |  Loss: (0.0368) |  Loss2: (0.0000) | Acc: (98.00%) (8979/9088)
Epoch: 235 | Batch_idx: 80 |  Loss: (0.0360) |  Loss2: (0.0000) | Acc: (98.00%) (10246/10368)
Epoch: 235 | Batch_idx: 90 |  Loss: (0.0358) |  Loss2: (0.0000) | Acc: (98.00%) (11509/11648)
Epoch: 235 | Batch_idx: 100 |  Loss: (0.0365) |  Loss2: (0.0000) | Acc: (98.00%) (12774/12928)
Epoch: 235 | Batch_idx: 110 |  Loss: (0.0358) |  Loss2: (0.0000) | Acc: (98.00%) (14041/14208)
Epoch: 235 | Batch_idx: 120 |  Loss: (0.0356) |  Loss2: (0.0000) | Acc: (98.00%) (15309/15488)
Epoch: 235 | Batch_idx: 130 |  Loss: (0.0361) |  Loss2: (0.0000) | Acc: (98.00%) (16570/16768)
Epoch: 235 | Batch_idx: 140 |  Loss: (0.0362) |  Loss2: (0.0000) | Acc: (98.00%) (17836/18048)
Epoch: 235 | Batch_idx: 150 |  Loss: (0.0363) |  Loss2: (0.0000) | Acc: (98.00%) (19100/19328)
Epoch: 235 | Batch_idx: 160 |  Loss: (0.0370) |  Loss2: (0.0000) | Acc: (98.00%) (20356/20608)
Epoch: 235 | Batch_idx: 170 |  Loss: (0.0369) |  Loss2: (0.0000) | Acc: (98.00%) (21623/21888)
Epoch: 235 | Batch_idx: 180 |  Loss: (0.0364) |  Loss2: (0.0000) | Acc: (98.00%) (22897/23168)
Epoch: 235 | Batch_idx: 190 |  Loss: (0.0363) |  Loss2: (0.0000) | Acc: (98.00%) (24162/24448)
Epoch: 235 | Batch_idx: 200 |  Loss: (0.0365) |  Loss2: (0.0000) | Acc: (98.00%) (25421/25728)
Epoch: 235 | Batch_idx: 210 |  Loss: (0.0361) |  Loss2: (0.0000) | Acc: (98.00%) (26689/27008)
Epoch: 235 | Batch_idx: 220 |  Loss: (0.0365) |  Loss2: (0.0000) | Acc: (98.00%) (27949/28288)
Epoch: 235 | Batch_idx: 230 |  Loss: (0.0367) |  Loss2: (0.0000) | Acc: (98.00%) (29215/29568)
Epoch: 235 | Batch_idx: 240 |  Loss: (0.0368) |  Loss2: (0.0000) | Acc: (98.00%) (30478/30848)
Epoch: 235 | Batch_idx: 250 |  Loss: (0.0371) |  Loss2: (0.0000) | Acc: (98.00%) (31738/32128)
Epoch: 235 | Batch_idx: 260 |  Loss: (0.0371) |  Loss2: (0.0000) | Acc: (98.00%) (32999/33408)
Epoch: 235 | Batch_idx: 270 |  Loss: (0.0370) |  Loss2: (0.0000) | Acc: (98.00%) (34269/34688)
Epoch: 235 | Batch_idx: 280 |  Loss: (0.0370) |  Loss2: (0.0000) | Acc: (98.00%) (35536/35968)
Epoch: 235 | Batch_idx: 290 |  Loss: (0.0372) |  Loss2: (0.0000) | Acc: (98.00%) (36794/37248)
Epoch: 235 | Batch_idx: 300 |  Loss: (0.0371) |  Loss2: (0.0000) | Acc: (98.00%) (38059/38528)
Epoch: 235 | Batch_idx: 310 |  Loss: (0.0369) |  Loss2: (0.0000) | Acc: (98.00%) (39329/39808)
Epoch: 235 | Batch_idx: 320 |  Loss: (0.0368) |  Loss2: (0.0000) | Acc: (98.00%) (40597/41088)
Epoch: 235 | Batch_idx: 330 |  Loss: (0.0369) |  Loss2: (0.0000) | Acc: (98.00%) (41857/42368)
Epoch: 235 | Batch_idx: 340 |  Loss: (0.0369) |  Loss2: (0.0000) | Acc: (98.00%) (43126/43648)
Epoch: 235 | Batch_idx: 350 |  Loss: (0.0367) |  Loss2: (0.0000) | Acc: (98.00%) (44397/44928)
Epoch: 235 | Batch_idx: 360 |  Loss: (0.0368) |  Loss2: (0.0000) | Acc: (98.00%) (45660/46208)
Epoch: 235 | Batch_idx: 370 |  Loss: (0.0368) |  Loss2: (0.0000) | Acc: (98.00%) (46923/47488)
Epoch: 235 | Batch_idx: 380 |  Loss: (0.0368) |  Loss2: (0.0000) | Acc: (98.00%) (48187/48768)
Epoch: 235 | Batch_idx: 390 |  Loss: (0.0370) |  Loss2: (0.0000) | Acc: (98.00%) (49399/50000)
=> saving checkpoint 'drive/app/torch/save_Schedule/checkpoint_235.pth.tar'
# TEST : Loss: (0.4683) | Acc: (88.00%) (8843/10000)
percent tensor([0.5772, 0.5990, 0.5816, 0.5542, 0.5942, 0.5952, 0.6065, 0.5718, 0.5796,
        0.5893, 0.5928, 0.5926, 0.5776, 0.5647, 0.5971, 0.5742],
       device='cuda:0') torch.Size([16])
percent tensor([0.5454, 0.5472, 0.5561, 0.5580, 0.5566, 0.5428, 0.5505, 0.5611, 0.5495,
        0.5495, 0.5442, 0.5522, 0.5454, 0.5502, 0.5453, 0.5482],
       device='cuda:0') torch.Size([16])
percent tensor([0.7085, 0.7252, 0.5308, 0.4873, 0.4734, 0.6571, 0.6705, 0.5267, 0.6246,
        0.7020, 0.7173, 0.6153, 0.7348, 0.6815, 0.6964, 0.6937],
       device='cuda:0') torch.Size([16])
percent tensor([0.7374, 0.6288, 0.7077, 0.7192, 0.7088, 0.7996, 0.6833, 0.7187, 0.6859,
        0.6582, 0.6808, 0.6567, 0.6129, 0.7078, 0.7037, 0.7908],
       device='cuda:0') torch.Size([16])
percent tensor([0.6800, 0.7044, 0.6869, 0.6314, 0.6608, 0.6252, 0.7094, 0.6823, 0.7000,
        0.6715, 0.6551, 0.6452, 0.6827, 0.6907, 0.6905, 0.6391],
       device='cuda:0') torch.Size([16])
percent tensor([0.6443, 0.7875, 0.7353, 0.7728, 0.7318, 0.8095, 0.8033, 0.6160, 0.8133,
        0.7340, 0.8273, 0.8476, 0.7738, 0.8494, 0.6905, 0.6076],
       device='cuda:0') torch.Size([16])
percent tensor([0.5929, 0.7173, 0.5580, 0.4286, 0.6046, 0.6924, 0.6734, 0.4560, 0.6869,
        0.6957, 0.7418, 0.4955, 0.7245, 0.6096, 0.3927, 0.4792],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9996, 1.0000, 0.9996, 0.9999,
        0.9999, 1.0000, 0.9999, 0.9999, 0.9997, 0.9996, 0.9990],
       device='cuda:0') torch.Size([16])
Epoch: 236 | Batch_idx: 0 |  Loss: (0.0313) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 236 | Batch_idx: 10 |  Loss: (0.0520) |  Loss2: (0.0000) | Acc: (98.00%) (1384/1408)
Epoch: 236 | Batch_idx: 20 |  Loss: (0.0436) |  Loss2: (0.0000) | Acc: (98.00%) (2652/2688)
Epoch: 236 | Batch_idx: 30 |  Loss: (0.0393) |  Loss2: (0.0000) | Acc: (98.00%) (3922/3968)
Epoch: 236 | Batch_idx: 40 |  Loss: (0.0376) |  Loss2: (0.0000) | Acc: (98.00%) (5190/5248)
Epoch: 236 | Batch_idx: 50 |  Loss: (0.0383) |  Loss2: (0.0000) | Acc: (98.00%) (6457/6528)
Epoch: 236 | Batch_idx: 60 |  Loss: (0.0378) |  Loss2: (0.0000) | Acc: (98.00%) (7724/7808)
Epoch: 236 | Batch_idx: 70 |  Loss: (0.0389) |  Loss2: (0.0000) | Acc: (98.00%) (8983/9088)
Epoch: 236 | Batch_idx: 80 |  Loss: (0.0392) |  Loss2: (0.0000) | Acc: (98.00%) (10246/10368)
Epoch: 236 | Batch_idx: 90 |  Loss: (0.0388) |  Loss2: (0.0000) | Acc: (98.00%) (11513/11648)
Epoch: 236 | Batch_idx: 100 |  Loss: (0.0384) |  Loss2: (0.0000) | Acc: (98.00%) (12780/12928)
Epoch: 236 | Batch_idx: 110 |  Loss: (0.0388) |  Loss2: (0.0000) | Acc: (98.00%) (14049/14208)
Epoch: 236 | Batch_idx: 120 |  Loss: (0.0393) |  Loss2: (0.0000) | Acc: (98.00%) (15305/15488)
Epoch: 236 | Batch_idx: 130 |  Loss: (0.0389) |  Loss2: (0.0000) | Acc: (98.00%) (16573/16768)
Epoch: 236 | Batch_idx: 140 |  Loss: (0.0391) |  Loss2: (0.0000) | Acc: (98.00%) (17836/18048)
Epoch: 236 | Batch_idx: 150 |  Loss: (0.0393) |  Loss2: (0.0000) | Acc: (98.00%) (19100/19328)
Epoch: 236 | Batch_idx: 160 |  Loss: (0.0391) |  Loss2: (0.0000) | Acc: (98.00%) (20366/20608)
Epoch: 236 | Batch_idx: 170 |  Loss: (0.0389) |  Loss2: (0.0000) | Acc: (98.00%) (21637/21888)
Epoch: 236 | Batch_idx: 180 |  Loss: (0.0391) |  Loss2: (0.0000) | Acc: (98.00%) (22898/23168)
Epoch: 236 | Batch_idx: 190 |  Loss: (0.0394) |  Loss2: (0.0000) | Acc: (98.00%) (24161/24448)
Epoch: 236 | Batch_idx: 200 |  Loss: (0.0394) |  Loss2: (0.0000) | Acc: (98.00%) (25425/25728)
Epoch: 236 | Batch_idx: 210 |  Loss: (0.0396) |  Loss2: (0.0000) | Acc: (98.00%) (26689/27008)
Epoch: 236 | Batch_idx: 220 |  Loss: (0.0393) |  Loss2: (0.0000) | Acc: (98.00%) (27958/28288)
Epoch: 236 | Batch_idx: 230 |  Loss: (0.0393) |  Loss2: (0.0000) | Acc: (98.00%) (29213/29568)
Epoch: 236 | Batch_idx: 240 |  Loss: (0.0390) |  Loss2: (0.0000) | Acc: (98.00%) (30482/30848)
Epoch: 236 | Batch_idx: 250 |  Loss: (0.0388) |  Loss2: (0.0000) | Acc: (98.00%) (31748/32128)
Epoch: 236 | Batch_idx: 260 |  Loss: (0.0389) |  Loss2: (0.0000) | Acc: (98.00%) (33015/33408)
Epoch: 236 | Batch_idx: 270 |  Loss: (0.0389) |  Loss2: (0.0000) | Acc: (98.00%) (34278/34688)
Epoch: 236 | Batch_idx: 280 |  Loss: (0.0386) |  Loss2: (0.0000) | Acc: (98.00%) (35546/35968)
Epoch: 236 | Batch_idx: 290 |  Loss: (0.0385) |  Loss2: (0.0000) | Acc: (98.00%) (36809/37248)
Epoch: 236 | Batch_idx: 300 |  Loss: (0.0384) |  Loss2: (0.0000) | Acc: (98.00%) (38077/38528)
Epoch: 236 | Batch_idx: 310 |  Loss: (0.0381) |  Loss2: (0.0000) | Acc: (98.00%) (39344/39808)
Epoch: 236 | Batch_idx: 320 |  Loss: (0.0378) |  Loss2: (0.0000) | Acc: (98.00%) (40614/41088)
Epoch: 236 | Batch_idx: 330 |  Loss: (0.0375) |  Loss2: (0.0000) | Acc: (98.00%) (41879/42368)
Epoch: 236 | Batch_idx: 340 |  Loss: (0.0373) |  Loss2: (0.0000) | Acc: (98.00%) (43144/43648)
Epoch: 236 | Batch_idx: 350 |  Loss: (0.0371) |  Loss2: (0.0000) | Acc: (98.00%) (44411/44928)
Epoch: 236 | Batch_idx: 360 |  Loss: (0.0370) |  Loss2: (0.0000) | Acc: (98.00%) (45676/46208)
Epoch: 236 | Batch_idx: 370 |  Loss: (0.0371) |  Loss2: (0.0000) | Acc: (98.00%) (46944/47488)
Epoch: 236 | Batch_idx: 380 |  Loss: (0.0370) |  Loss2: (0.0000) | Acc: (98.00%) (48207/48768)
Epoch: 236 | Batch_idx: 390 |  Loss: (0.0371) |  Loss2: (0.0000) | Acc: (98.00%) (49423/50000)
# TEST : Loss: (0.4406) | Acc: (89.00%) (8913/10000)
percent tensor([0.5788, 0.5966, 0.5875, 0.5568, 0.5984, 0.5966, 0.6059, 0.5747, 0.5803,
        0.5898, 0.5919, 0.5960, 0.5790, 0.5586, 0.5977, 0.5742],
       device='cuda:0') torch.Size([16])
percent tensor([0.5449, 0.5470, 0.5546, 0.5576, 0.5562, 0.5437, 0.5507, 0.5598, 0.5497,
        0.5488, 0.5449, 0.5516, 0.5445, 0.5515, 0.5457, 0.5482],
       device='cuda:0') torch.Size([16])
percent tensor([0.6984, 0.7188, 0.5292, 0.4713, 0.4536, 0.6335, 0.6598, 0.5220, 0.6015,
        0.6949, 0.7028, 0.6091, 0.7331, 0.6635, 0.6793, 0.6805],
       device='cuda:0') torch.Size([16])
percent tensor([0.7452, 0.6330, 0.7258, 0.7170, 0.7280, 0.8074, 0.6976, 0.7264, 0.7022,
        0.6651, 0.6894, 0.6699, 0.6210, 0.7172, 0.7159, 0.7955],
       device='cuda:0') torch.Size([16])
percent tensor([0.6843, 0.6954, 0.6869, 0.6323, 0.6519, 0.6269, 0.7023, 0.6844, 0.6905,
        0.6670, 0.6472, 0.6457, 0.6809, 0.6697, 0.6864, 0.6416],
       device='cuda:0') torch.Size([16])
percent tensor([0.6355, 0.7829, 0.7181, 0.7659, 0.6979, 0.7808, 0.7909, 0.5912, 0.8124,
        0.7334, 0.8275, 0.8338, 0.7676, 0.8510, 0.6826, 0.5926],
       device='cuda:0') torch.Size([16])
percent tensor([0.5952, 0.7180, 0.5848, 0.4480, 0.5942, 0.6918, 0.6839, 0.4788, 0.6907,
        0.6907, 0.7528, 0.5036, 0.7084, 0.5943, 0.4144, 0.4726],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9999, 0.9999, 0.9998, 0.9998, 0.9994, 1.0000, 0.9996, 1.0000,
        0.9999, 1.0000, 0.9999, 0.9999, 0.9997, 0.9997, 0.9991],
       device='cuda:0') torch.Size([16])
Epoch: 237 | Batch_idx: 0 |  Loss: (0.0311) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 237 | Batch_idx: 10 |  Loss: (0.0299) |  Loss2: (0.0000) | Acc: (99.00%) (1397/1408)
Epoch: 237 | Batch_idx: 20 |  Loss: (0.0350) |  Loss2: (0.0000) | Acc: (99.00%) (2662/2688)
Epoch: 237 | Batch_idx: 30 |  Loss: (0.0357) |  Loss2: (0.0000) | Acc: (98.00%) (3926/3968)
Epoch: 237 | Batch_idx: 40 |  Loss: (0.0364) |  Loss2: (0.0000) | Acc: (98.00%) (5189/5248)
Epoch: 237 | Batch_idx: 50 |  Loss: (0.0341) |  Loss2: (0.0000) | Acc: (99.00%) (6463/6528)
Epoch: 237 | Batch_idx: 60 |  Loss: (0.0330) |  Loss2: (0.0000) | Acc: (99.00%) (7733/7808)
Epoch: 237 | Batch_idx: 70 |  Loss: (0.0325) |  Loss2: (0.0000) | Acc: (99.00%) (9003/9088)
Epoch: 237 | Batch_idx: 80 |  Loss: (0.0332) |  Loss2: (0.0000) | Acc: (99.00%) (10269/10368)
Epoch: 237 | Batch_idx: 90 |  Loss: (0.0341) |  Loss2: (0.0000) | Acc: (98.00%) (11529/11648)
Epoch: 237 | Batch_idx: 100 |  Loss: (0.0335) |  Loss2: (0.0000) | Acc: (98.00%) (12798/12928)
Epoch: 237 | Batch_idx: 110 |  Loss: (0.0343) |  Loss2: (0.0000) | Acc: (98.00%) (14058/14208)
Epoch: 237 | Batch_idx: 120 |  Loss: (0.0347) |  Loss2: (0.0000) | Acc: (98.00%) (15324/15488)
Epoch: 237 | Batch_idx: 130 |  Loss: (0.0344) |  Loss2: (0.0000) | Acc: (98.00%) (16592/16768)
Epoch: 237 | Batch_idx: 140 |  Loss: (0.0353) |  Loss2: (0.0000) | Acc: (98.00%) (17851/18048)
Epoch: 237 | Batch_idx: 150 |  Loss: (0.0353) |  Loss2: (0.0000) | Acc: (98.00%) (19119/19328)
Epoch: 237 | Batch_idx: 160 |  Loss: (0.0354) |  Loss2: (0.0000) | Acc: (98.00%) (20387/20608)
Epoch: 237 | Batch_idx: 170 |  Loss: (0.0351) |  Loss2: (0.0000) | Acc: (98.00%) (21658/21888)
Epoch: 237 | Batch_idx: 180 |  Loss: (0.0354) |  Loss2: (0.0000) | Acc: (98.00%) (22921/23168)
Epoch: 237 | Batch_idx: 190 |  Loss: (0.0353) |  Loss2: (0.0000) | Acc: (98.00%) (24191/24448)
Epoch: 237 | Batch_idx: 200 |  Loss: (0.0355) |  Loss2: (0.0000) | Acc: (98.00%) (25457/25728)
Epoch: 237 | Batch_idx: 210 |  Loss: (0.0353) |  Loss2: (0.0000) | Acc: (98.00%) (26727/27008)
Epoch: 237 | Batch_idx: 220 |  Loss: (0.0352) |  Loss2: (0.0000) | Acc: (98.00%) (27993/28288)
Epoch: 237 | Batch_idx: 230 |  Loss: (0.0353) |  Loss2: (0.0000) | Acc: (98.00%) (29258/29568)
Epoch: 237 | Batch_idx: 240 |  Loss: (0.0353) |  Loss2: (0.0000) | Acc: (98.00%) (30523/30848)
Epoch: 237 | Batch_idx: 250 |  Loss: (0.0352) |  Loss2: (0.0000) | Acc: (98.00%) (31788/32128)
Epoch: 237 | Batch_idx: 260 |  Loss: (0.0354) |  Loss2: (0.0000) | Acc: (98.00%) (33050/33408)
Epoch: 237 | Batch_idx: 270 |  Loss: (0.0355) |  Loss2: (0.0000) | Acc: (98.00%) (34318/34688)
Epoch: 237 | Batch_idx: 280 |  Loss: (0.0356) |  Loss2: (0.0000) | Acc: (98.00%) (35584/35968)
Epoch: 237 | Batch_idx: 290 |  Loss: (0.0359) |  Loss2: (0.0000) | Acc: (98.00%) (36849/37248)
Epoch: 237 | Batch_idx: 300 |  Loss: (0.0359) |  Loss2: (0.0000) | Acc: (98.00%) (38115/38528)
Epoch: 237 | Batch_idx: 310 |  Loss: (0.0362) |  Loss2: (0.0000) | Acc: (98.00%) (39373/39808)
Epoch: 237 | Batch_idx: 320 |  Loss: (0.0363) |  Loss2: (0.0000) | Acc: (98.00%) (40640/41088)
Epoch: 237 | Batch_idx: 330 |  Loss: (0.0363) |  Loss2: (0.0000) | Acc: (98.00%) (41904/42368)
Epoch: 237 | Batch_idx: 340 |  Loss: (0.0364) |  Loss2: (0.0000) | Acc: (98.00%) (43168/43648)
Epoch: 237 | Batch_idx: 350 |  Loss: (0.0364) |  Loss2: (0.0000) | Acc: (98.00%) (44434/44928)
Epoch: 237 | Batch_idx: 360 |  Loss: (0.0365) |  Loss2: (0.0000) | Acc: (98.00%) (45699/46208)
Epoch: 237 | Batch_idx: 370 |  Loss: (0.0362) |  Loss2: (0.0000) | Acc: (98.00%) (46969/47488)
Epoch: 237 | Batch_idx: 380 |  Loss: (0.0364) |  Loss2: (0.0000) | Acc: (98.00%) (48231/48768)
Epoch: 237 | Batch_idx: 390 |  Loss: (0.0365) |  Loss2: (0.0000) | Acc: (98.00%) (49448/50000)
# TEST : Loss: (0.4517) | Acc: (89.00%) (8928/10000)
percent tensor([0.5797, 0.5979, 0.5870, 0.5561, 0.5982, 0.6000, 0.6069, 0.5752, 0.5813,
        0.5904, 0.5938, 0.5967, 0.5795, 0.5609, 0.5992, 0.5754],
       device='cuda:0') torch.Size([16])
percent tensor([0.5437, 0.5468, 0.5535, 0.5566, 0.5551, 0.5426, 0.5504, 0.5579, 0.5490,
        0.5478, 0.5442, 0.5501, 0.5438, 0.5509, 0.5446, 0.5473],
       device='cuda:0') torch.Size([16])
percent tensor([0.7018, 0.7177, 0.5335, 0.4728, 0.4615, 0.6425, 0.6673, 0.5257, 0.6006,
        0.7006, 0.7089, 0.6179, 0.7323, 0.6741, 0.6814, 0.6814],
       device='cuda:0') torch.Size([16])
percent tensor([0.7274, 0.6296, 0.7168, 0.7174, 0.7224, 0.8050, 0.6906, 0.7215, 0.6890,
        0.6587, 0.6779, 0.6630, 0.6020, 0.7173, 0.7092, 0.7876],
       device='cuda:0') torch.Size([16])
percent tensor([0.6904, 0.7025, 0.6930, 0.6350, 0.6522, 0.6261, 0.7121, 0.6905, 0.7002,
        0.6795, 0.6499, 0.6471, 0.6833, 0.6898, 0.6931, 0.6450],
       device='cuda:0') torch.Size([16])
percent tensor([0.6179, 0.7682, 0.7277, 0.7676, 0.7044, 0.8004, 0.7999, 0.6086, 0.8066,
        0.7178, 0.8245, 0.8302, 0.7692, 0.8441, 0.6753, 0.5795],
       device='cuda:0') torch.Size([16])
percent tensor([0.6125, 0.7448, 0.5687, 0.4552, 0.6013, 0.7194, 0.7024, 0.4572, 0.6821,
        0.7002, 0.7583, 0.4812, 0.7354, 0.6349, 0.4132, 0.5056],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9999, 0.9999, 0.9998, 0.9999, 0.9995, 1.0000, 0.9996, 1.0000,
        0.9999, 1.0000, 0.9999, 0.9999, 0.9998, 0.9996, 0.9995],
       device='cuda:0') torch.Size([16])
Epoch: 238 | Batch_idx: 0 |  Loss: (0.0237) |  Loss2: (0.0000) | Acc: (100.00%) (128/128)
Epoch: 238 | Batch_idx: 10 |  Loss: (0.0314) |  Loss2: (0.0000) | Acc: (99.00%) (1394/1408)
Epoch: 238 | Batch_idx: 20 |  Loss: (0.0306) |  Loss2: (0.0000) | Acc: (99.00%) (2665/2688)
Epoch: 238 | Batch_idx: 30 |  Loss: (0.0324) |  Loss2: (0.0000) | Acc: (99.00%) (3931/3968)
Epoch: 238 | Batch_idx: 40 |  Loss: (0.0319) |  Loss2: (0.0000) | Acc: (99.00%) (5200/5248)
Epoch: 238 | Batch_idx: 50 |  Loss: (0.0314) |  Loss2: (0.0000) | Acc: (99.00%) (6467/6528)
Epoch: 238 | Batch_idx: 60 |  Loss: (0.0322) |  Loss2: (0.0000) | Acc: (99.00%) (7732/7808)
Epoch: 238 | Batch_idx: 70 |  Loss: (0.0324) |  Loss2: (0.0000) | Acc: (98.00%) (8997/9088)
Epoch: 238 | Batch_idx: 80 |  Loss: (0.0329) |  Loss2: (0.0000) | Acc: (98.00%) (10258/10368)
Epoch: 238 | Batch_idx: 90 |  Loss: (0.0335) |  Loss2: (0.0000) | Acc: (98.00%) (11522/11648)
Epoch: 238 | Batch_idx: 100 |  Loss: (0.0344) |  Loss2: (0.0000) | Acc: (98.00%) (12781/12928)
Epoch: 238 | Batch_idx: 110 |  Loss: (0.0351) |  Loss2: (0.0000) | Acc: (98.00%) (14041/14208)
Epoch: 238 | Batch_idx: 120 |  Loss: (0.0353) |  Loss2: (0.0000) | Acc: (98.00%) (15307/15488)
Epoch: 238 | Batch_idx: 130 |  Loss: (0.0354) |  Loss2: (0.0000) | Acc: (98.00%) (16570/16768)
Epoch: 238 | Batch_idx: 140 |  Loss: (0.0356) |  Loss2: (0.0000) | Acc: (98.00%) (17836/18048)
Epoch: 238 | Batch_idx: 150 |  Loss: (0.0357) |  Loss2: (0.0000) | Acc: (98.00%) (19101/19328)
Epoch: 238 | Batch_idx: 160 |  Loss: (0.0353) |  Loss2: (0.0000) | Acc: (98.00%) (20370/20608)
Epoch: 238 | Batch_idx: 170 |  Loss: (0.0350) |  Loss2: (0.0000) | Acc: (98.00%) (21640/21888)
Epoch: 238 | Batch_idx: 180 |  Loss: (0.0346) |  Loss2: (0.0000) | Acc: (98.00%) (22909/23168)
Epoch: 238 | Batch_idx: 190 |  Loss: (0.0344) |  Loss2: (0.0000) | Acc: (98.00%) (24175/24448)
Epoch: 238 | Batch_idx: 200 |  Loss: (0.0338) |  Loss2: (0.0000) | Acc: (98.00%) (25448/25728)
Epoch: 238 | Batch_idx: 210 |  Loss: (0.0337) |  Loss2: (0.0000) | Acc: (98.00%) (26720/27008)
Epoch: 238 | Batch_idx: 220 |  Loss: (0.0337) |  Loss2: (0.0000) | Acc: (98.00%) (27982/28288)
Epoch: 238 | Batch_idx: 230 |  Loss: (0.0336) |  Loss2: (0.0000) | Acc: (98.00%) (29249/29568)
Epoch: 238 | Batch_idx: 240 |  Loss: (0.0338) |  Loss2: (0.0000) | Acc: (98.00%) (30509/30848)
Epoch: 238 | Batch_idx: 250 |  Loss: (0.0334) |  Loss2: (0.0000) | Acc: (98.00%) (31784/32128)
Epoch: 238 | Batch_idx: 260 |  Loss: (0.0334) |  Loss2: (0.0000) | Acc: (98.00%) (33050/33408)
Epoch: 238 | Batch_idx: 270 |  Loss: (0.0336) |  Loss2: (0.0000) | Acc: (98.00%) (34314/34688)
Epoch: 238 | Batch_idx: 280 |  Loss: (0.0336) |  Loss2: (0.0000) | Acc: (98.00%) (35583/35968)
Epoch: 238 | Batch_idx: 290 |  Loss: (0.0335) |  Loss2: (0.0000) | Acc: (98.00%) (36854/37248)
Epoch: 238 | Batch_idx: 300 |  Loss: (0.0340) |  Loss2: (0.0000) | Acc: (98.00%) (38119/38528)
Epoch: 238 | Batch_idx: 310 |  Loss: (0.0336) |  Loss2: (0.0000) | Acc: (98.00%) (39391/39808)
Epoch: 238 | Batch_idx: 320 |  Loss: (0.0335) |  Loss2: (0.0000) | Acc: (98.00%) (40663/41088)
Epoch: 238 | Batch_idx: 330 |  Loss: (0.0337) |  Loss2: (0.0000) | Acc: (98.00%) (41923/42368)
Epoch: 238 | Batch_idx: 340 |  Loss: (0.0337) |  Loss2: (0.0000) | Acc: (98.00%) (43187/43648)
Epoch: 238 | Batch_idx: 350 |  Loss: (0.0335) |  Loss2: (0.0000) | Acc: (98.00%) (44462/44928)
Epoch: 238 | Batch_idx: 360 |  Loss: (0.0334) |  Loss2: (0.0000) | Acc: (98.00%) (45730/46208)
Epoch: 238 | Batch_idx: 370 |  Loss: (0.0331) |  Loss2: (0.0000) | Acc: (98.00%) (47003/47488)
Epoch: 238 | Batch_idx: 380 |  Loss: (0.0330) |  Loss2: (0.0000) | Acc: (98.00%) (48273/48768)
Epoch: 238 | Batch_idx: 390 |  Loss: (0.0332) |  Loss2: (0.0000) | Acc: (98.00%) (49488/50000)
# TEST : Loss: (0.4275) | Acc: (89.00%) (8913/10000)
percent tensor([0.5776, 0.5975, 0.5858, 0.5570, 0.5958, 0.5959, 0.6055, 0.5757, 0.5785,
        0.5899, 0.5914, 0.5953, 0.5779, 0.5614, 0.5975, 0.5742],
       device='cuda:0') torch.Size([16])
percent tensor([0.5444, 0.5470, 0.5537, 0.5563, 0.5558, 0.5431, 0.5502, 0.5585, 0.5493,
        0.5481, 0.5444, 0.5505, 0.5446, 0.5511, 0.5451, 0.5477],
       device='cuda:0') torch.Size([16])
percent tensor([0.7048, 0.7200, 0.5414, 0.4806, 0.4657, 0.6440, 0.6677, 0.5389, 0.6139,
        0.6989, 0.7096, 0.6175, 0.7338, 0.6763, 0.6854, 0.6829],
       device='cuda:0') torch.Size([16])
percent tensor([0.7347, 0.6290, 0.7143, 0.7138, 0.7227, 0.8080, 0.6902, 0.7181, 0.6928,
        0.6632, 0.6888, 0.6618, 0.6123, 0.7154, 0.7132, 0.7912],
       device='cuda:0') torch.Size([16])
percent tensor([0.6857, 0.6983, 0.6997, 0.6522, 0.6631, 0.6333, 0.7116, 0.6972, 0.7065,
        0.6680, 0.6409, 0.6544, 0.6814, 0.6914, 0.6860, 0.6440],
       device='cuda:0') torch.Size([16])
percent tensor([0.6175, 0.7748, 0.7358, 0.7532, 0.7084, 0.7996, 0.8013, 0.6076, 0.8057,
        0.7270, 0.8251, 0.8372, 0.7715, 0.8417, 0.6786, 0.5983],
       device='cuda:0') torch.Size([16])
percent tensor([0.6022, 0.7295, 0.5728, 0.4603, 0.5933, 0.7175, 0.6879, 0.4611, 0.6726,
        0.7021, 0.7595, 0.4951, 0.7186, 0.6054, 0.3917, 0.5033],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9999, 0.9999, 0.9998, 0.9999, 0.9996, 1.0000, 0.9996, 0.9999,
        0.9999, 1.0000, 0.9999, 0.9999, 0.9998, 0.9997, 0.9995],
       device='cuda:0') torch.Size([16])
Epoch: 239 | Batch_idx: 0 |  Loss: (0.0411) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 239 | Batch_idx: 10 |  Loss: (0.0233) |  Loss2: (0.0000) | Acc: (99.00%) (1400/1408)
Epoch: 239 | Batch_idx: 20 |  Loss: (0.0278) |  Loss2: (0.0000) | Acc: (99.00%) (2666/2688)
Epoch: 239 | Batch_idx: 30 |  Loss: (0.0278) |  Loss2: (0.0000) | Acc: (99.00%) (3935/3968)
Epoch: 239 | Batch_idx: 40 |  Loss: (0.0270) |  Loss2: (0.0000) | Acc: (99.00%) (5209/5248)
Epoch: 239 | Batch_idx: 50 |  Loss: (0.0269) |  Loss2: (0.0000) | Acc: (99.00%) (6480/6528)
Epoch: 239 | Batch_idx: 60 |  Loss: (0.0259) |  Loss2: (0.0000) | Acc: (99.00%) (7751/7808)
Epoch: 239 | Batch_idx: 70 |  Loss: (0.0268) |  Loss2: (0.0000) | Acc: (99.00%) (9023/9088)
Epoch: 239 | Batch_idx: 80 |  Loss: (0.0268) |  Loss2: (0.0000) | Acc: (99.00%) (10295/10368)
Epoch: 239 | Batch_idx: 90 |  Loss: (0.0269) |  Loss2: (0.0000) | Acc: (99.00%) (11568/11648)
Epoch: 239 | Batch_idx: 100 |  Loss: (0.0275) |  Loss2: (0.0000) | Acc: (99.00%) (12836/12928)
Epoch: 239 | Batch_idx: 110 |  Loss: (0.0277) |  Loss2: (0.0000) | Acc: (99.00%) (14105/14208)
Epoch: 239 | Batch_idx: 120 |  Loss: (0.0274) |  Loss2: (0.0000) | Acc: (99.00%) (15375/15488)
Epoch: 239 | Batch_idx: 130 |  Loss: (0.0274) |  Loss2: (0.0000) | Acc: (99.00%) (16647/16768)
Epoch: 239 | Batch_idx: 140 |  Loss: (0.0274) |  Loss2: (0.0000) | Acc: (99.00%) (17917/18048)
Epoch: 239 | Batch_idx: 150 |  Loss: (0.0282) |  Loss2: (0.0000) | Acc: (99.00%) (19181/19328)
Epoch: 239 | Batch_idx: 160 |  Loss: (0.0284) |  Loss2: (0.0000) | Acc: (99.00%) (20449/20608)
Epoch: 239 | Batch_idx: 170 |  Loss: (0.0282) |  Loss2: (0.0000) | Acc: (99.00%) (21721/21888)
Epoch: 239 | Batch_idx: 180 |  Loss: (0.0286) |  Loss2: (0.0000) | Acc: (99.00%) (22987/23168)
Epoch: 239 | Batch_idx: 190 |  Loss: (0.0284) |  Loss2: (0.0000) | Acc: (99.00%) (24260/24448)
Epoch: 239 | Batch_idx: 200 |  Loss: (0.0288) |  Loss2: (0.0000) | Acc: (99.00%) (25525/25728)
Epoch: 239 | Batch_idx: 210 |  Loss: (0.0289) |  Loss2: (0.0000) | Acc: (99.00%) (26792/27008)
Epoch: 239 | Batch_idx: 220 |  Loss: (0.0296) |  Loss2: (0.0000) | Acc: (99.00%) (28056/28288)
Epoch: 239 | Batch_idx: 230 |  Loss: (0.0297) |  Loss2: (0.0000) | Acc: (99.00%) (29324/29568)
Epoch: 239 | Batch_idx: 240 |  Loss: (0.0302) |  Loss2: (0.0000) | Acc: (99.00%) (30590/30848)
Epoch: 239 | Batch_idx: 250 |  Loss: (0.0306) |  Loss2: (0.0000) | Acc: (99.00%) (31855/32128)
Epoch: 239 | Batch_idx: 260 |  Loss: (0.0305) |  Loss2: (0.0000) | Acc: (99.00%) (33125/33408)
Epoch: 239 | Batch_idx: 270 |  Loss: (0.0307) |  Loss2: (0.0000) | Acc: (99.00%) (34394/34688)
Epoch: 239 | Batch_idx: 280 |  Loss: (0.0308) |  Loss2: (0.0000) | Acc: (99.00%) (35663/35968)
Epoch: 239 | Batch_idx: 290 |  Loss: (0.0310) |  Loss2: (0.0000) | Acc: (99.00%) (36926/37248)
Epoch: 239 | Batch_idx: 300 |  Loss: (0.0309) |  Loss2: (0.0000) | Acc: (99.00%) (38197/38528)
Epoch: 239 | Batch_idx: 310 |  Loss: (0.0310) |  Loss2: (0.0000) | Acc: (99.00%) (39465/39808)
Epoch: 239 | Batch_idx: 320 |  Loss: (0.0309) |  Loss2: (0.0000) | Acc: (99.00%) (40736/41088)
Epoch: 239 | Batch_idx: 330 |  Loss: (0.0313) |  Loss2: (0.0000) | Acc: (99.00%) (41999/42368)
Epoch: 239 | Batch_idx: 340 |  Loss: (0.0316) |  Loss2: (0.0000) | Acc: (99.00%) (43259/43648)
Epoch: 239 | Batch_idx: 350 |  Loss: (0.0317) |  Loss2: (0.0000) | Acc: (99.00%) (44528/44928)
Epoch: 239 | Batch_idx: 360 |  Loss: (0.0319) |  Loss2: (0.0000) | Acc: (99.00%) (45794/46208)
Epoch: 239 | Batch_idx: 370 |  Loss: (0.0322) |  Loss2: (0.0000) | Acc: (99.00%) (47059/47488)
Epoch: 239 | Batch_idx: 380 |  Loss: (0.0326) |  Loss2: (0.0000) | Acc: (99.00%) (48315/48768)
Epoch: 239 | Batch_idx: 390 |  Loss: (0.0327) |  Loss2: (0.0000) | Acc: (99.00%) (49532/50000)
# TEST : Loss: (0.4754) | Acc: (88.00%) (8870/10000)
percent tensor([0.5797, 0.5987, 0.5886, 0.5573, 0.6008, 0.6006, 0.6084, 0.5762, 0.5821,
        0.5918, 0.5939, 0.5994, 0.5798, 0.5620, 0.5998, 0.5756],
       device='cuda:0') torch.Size([16])
percent tensor([0.5456, 0.5488, 0.5552, 0.5583, 0.5562, 0.5435, 0.5519, 0.5606, 0.5512,
        0.5494, 0.5456, 0.5507, 0.5457, 0.5537, 0.5457, 0.5492],
       device='cuda:0') torch.Size([16])
percent tensor([0.7042, 0.7222, 0.4992, 0.4751, 0.4342, 0.6492, 0.6582, 0.5248, 0.6030,
        0.6863, 0.7071, 0.5844, 0.7332, 0.6791, 0.6899, 0.6834],
       device='cuda:0') torch.Size([16])
percent tensor([0.7417, 0.6264, 0.7123, 0.7176, 0.7203, 0.8049, 0.6904, 0.7190, 0.6932,
        0.6549, 0.6797, 0.6660, 0.6086, 0.7194, 0.7099, 0.7953],
       device='cuda:0') torch.Size([16])
percent tensor([0.6841, 0.7104, 0.7000, 0.6358, 0.6573, 0.6335, 0.7185, 0.6993, 0.7109,
        0.6789, 0.6596, 0.6527, 0.6923, 0.6871, 0.6931, 0.6421],
       device='cuda:0') torch.Size([16])
percent tensor([0.6105, 0.7690, 0.7584, 0.7851, 0.7452, 0.8251, 0.7980, 0.6168, 0.8019,
        0.7130, 0.8148, 0.8514, 0.7621, 0.8413, 0.6932, 0.6292],
       device='cuda:0') torch.Size([16])
percent tensor([0.5829, 0.7069, 0.5954, 0.4700, 0.6186, 0.7116, 0.6588, 0.4589, 0.6494,
        0.6847, 0.7450, 0.5231, 0.7081, 0.5989, 0.4120, 0.4998],
       device='cuda:0') torch.Size([16])
percent tensor([0.9999, 0.9999, 0.9999, 0.9998, 0.9999, 0.9995, 0.9999, 0.9998, 0.9999,
        0.9999, 1.0000, 1.0000, 0.9999, 0.9997, 0.9997, 0.9993],
       device='cuda:0') torch.Size([16])
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(187.0762, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(843.8270, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(840.5513, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1509.2368, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(469.8189, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2340.6919, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4239.9761, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1330.4816, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6384.9912, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11431.7148, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3733.1331, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(15736.7236, device='cuda:0', grad_fn=<NormBackward0>)
6 hours 10 mins 19 secs for training