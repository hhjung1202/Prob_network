Files already downloaded and verified
USE 1 GPUs!
Epoch: 0 | Batch_idx: 0 |  Loss: (2.3256) |  Loss2: (0.0000) | Acc: (6.00%) (8/128)
Epoch: 0 | Batch_idx: 10 |  Loss: (2.2984) |  Loss2: (0.0000) | Acc: (12.00%) (180/1408)
Epoch: 0 | Batch_idx: 20 |  Loss: (2.2937) |  Loss2: (0.0000) | Acc: (13.00%) (372/2688)
Epoch: 0 | Batch_idx: 30 |  Loss: (2.2873) |  Loss2: (0.0000) | Acc: (14.00%) (571/3968)
Epoch: 0 | Batch_idx: 40 |  Loss: (2.2805) |  Loss2: (0.0000) | Acc: (15.00%) (817/5248)
Epoch: 0 | Batch_idx: 50 |  Loss: (2.2703) |  Loss2: (0.0000) | Acc: (16.00%) (1064/6528)
Epoch: 0 | Batch_idx: 60 |  Loss: (2.2618) |  Loss2: (0.0000) | Acc: (17.00%) (1331/7808)
Epoch: 0 | Batch_idx: 70 |  Loss: (2.2523) |  Loss2: (0.0000) | Acc: (17.00%) (1605/9088)
Epoch: 0 | Batch_idx: 80 |  Loss: (2.2414) |  Loss2: (0.0000) | Acc: (18.00%) (1890/10368)
Epoch: 0 | Batch_idx: 90 |  Loss: (2.2325) |  Loss2: (0.0000) | Acc: (18.00%) (2179/11648)
Epoch: 0 | Batch_idx: 100 |  Loss: (2.2231) |  Loss2: (0.0000) | Acc: (19.00%) (2474/12928)
Epoch: 0 | Batch_idx: 110 |  Loss: (2.2132) |  Loss2: (0.0000) | Acc: (19.00%) (2808/14208)
Epoch: 0 | Batch_idx: 120 |  Loss: (2.2044) |  Loss2: (0.0000) | Acc: (20.00%) (3109/15488)
Epoch: 0 | Batch_idx: 130 |  Loss: (2.1955) |  Loss2: (0.0000) | Acc: (20.00%) (3422/16768)
Epoch: 0 | Batch_idx: 140 |  Loss: (2.1870) |  Loss2: (0.0000) | Acc: (20.00%) (3732/18048)
Epoch: 0 | Batch_idx: 150 |  Loss: (2.1783) |  Loss2: (0.0000) | Acc: (21.00%) (4062/19328)
Epoch: 0 | Batch_idx: 160 |  Loss: (2.1702) |  Loss2: (0.0000) | Acc: (21.00%) (4386/20608)
Epoch: 0 | Batch_idx: 170 |  Loss: (2.1607) |  Loss2: (0.0000) | Acc: (21.00%) (4765/21888)
Epoch: 0 | Batch_idx: 180 |  Loss: (2.1530) |  Loss2: (0.0000) | Acc: (21.00%) (5081/23168)
Epoch: 0 | Batch_idx: 190 |  Loss: (2.1459) |  Loss2: (0.0000) | Acc: (22.00%) (5412/24448)
Epoch: 0 | Batch_idx: 200 |  Loss: (2.1385) |  Loss2: (0.0000) | Acc: (22.00%) (5766/25728)
Epoch: 0 | Batch_idx: 210 |  Loss: (2.1317) |  Loss2: (0.0000) | Acc: (22.00%) (6130/27008)
Epoch: 0 | Batch_idx: 220 |  Loss: (2.1241) |  Loss2: (0.0000) | Acc: (22.00%) (6494/28288)
Epoch: 0 | Batch_idx: 230 |  Loss: (2.1167) |  Loss2: (0.0000) | Acc: (23.00%) (6861/29568)
Epoch: 0 | Batch_idx: 240 |  Loss: (2.1103) |  Loss2: (0.0000) | Acc: (23.00%) (7225/30848)
Epoch: 0 | Batch_idx: 250 |  Loss: (2.1022) |  Loss2: (0.0000) | Acc: (23.00%) (7611/32128)
Epoch: 0 | Batch_idx: 260 |  Loss: (2.0961) |  Loss2: (0.0000) | Acc: (23.00%) (8009/33408)
Epoch: 0 | Batch_idx: 270 |  Loss: (2.0892) |  Loss2: (0.0000) | Acc: (24.00%) (8406/34688)
Epoch: 0 | Batch_idx: 280 |  Loss: (2.0826) |  Loss2: (0.0000) | Acc: (24.00%) (8826/35968)
Epoch: 0 | Batch_idx: 290 |  Loss: (2.0762) |  Loss2: (0.0000) | Acc: (24.00%) (9230/37248)
Epoch: 0 | Batch_idx: 300 |  Loss: (2.0707) |  Loss2: (0.0000) | Acc: (24.00%) (9606/38528)
Epoch: 0 | Batch_idx: 310 |  Loss: (2.0648) |  Loss2: (0.0000) | Acc: (25.00%) (9994/39808)
Epoch: 0 | Batch_idx: 320 |  Loss: (2.0592) |  Loss2: (0.0000) | Acc: (25.00%) (10408/41088)
Epoch: 0 | Batch_idx: 330 |  Loss: (2.0536) |  Loss2: (0.0000) | Acc: (25.00%) (10798/42368)
Epoch: 0 | Batch_idx: 340 |  Loss: (2.0487) |  Loss2: (0.0000) | Acc: (25.00%) (11196/43648)
Epoch: 0 | Batch_idx: 350 |  Loss: (2.0437) |  Loss2: (0.0000) | Acc: (25.00%) (11619/44928)
Epoch: 0 | Batch_idx: 360 |  Loss: (2.0387) |  Loss2: (0.0000) | Acc: (25.00%) (12010/46208)
Epoch: 0 | Batch_idx: 370 |  Loss: (2.0335) |  Loss2: (0.0000) | Acc: (26.00%) (12426/47488)
Epoch: 0 | Batch_idx: 380 |  Loss: (2.0289) |  Loss2: (0.0000) | Acc: (26.00%) (12853/48768)
Epoch: 0 | Batch_idx: 390 |  Loss: (2.0237) |  Loss2: (0.0000) | Acc: (26.00%) (13263/50000)
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type ResNet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type BasicBlock. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type _Gate. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_000.pth.tar'
# TEST : Loss: (1.7932) | Acc: (34.00%) (3435/10000)
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(167.9001, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(766.9728, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(764.3212, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1534.8134, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(509.8291, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2169.7302, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4346.4194, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1445.2970, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6137.2163, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12287.4521, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4087.8960, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(17369.8574, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 1 | Batch_idx: 0 |  Loss: (1.8120) |  Loss2: (0.0000) | Acc: (30.00%) (39/128)
Epoch: 1 | Batch_idx: 10 |  Loss: (1.8499) |  Loss2: (0.0000) | Acc: (32.00%) (456/1408)
Epoch: 1 | Batch_idx: 20 |  Loss: (1.8372) |  Loss2: (0.0000) | Acc: (32.00%) (876/2688)
Epoch: 1 | Batch_idx: 30 |  Loss: (1.8329) |  Loss2: (0.0000) | Acc: (33.00%) (1314/3968)
Epoch: 1 | Batch_idx: 40 |  Loss: (1.8244) |  Loss2: (0.0000) | Acc: (33.00%) (1747/5248)
Epoch: 1 | Batch_idx: 50 |  Loss: (1.8217) |  Loss2: (0.0000) | Acc: (33.00%) (2195/6528)
Epoch: 1 | Batch_idx: 60 |  Loss: (1.8143) |  Loss2: (0.0000) | Acc: (33.00%) (2636/7808)
Epoch: 1 | Batch_idx: 70 |  Loss: (1.8129) |  Loss2: (0.0000) | Acc: (33.00%) (3053/9088)
Epoch: 1 | Batch_idx: 80 |  Loss: (1.8077) |  Loss2: (0.0000) | Acc: (33.00%) (3506/10368)
Epoch: 1 | Batch_idx: 90 |  Loss: (1.8021) |  Loss2: (0.0000) | Acc: (34.00%) (3961/11648)
Epoch: 1 | Batch_idx: 100 |  Loss: (1.7963) |  Loss2: (0.0000) | Acc: (34.00%) (4460/12928)
Epoch: 1 | Batch_idx: 110 |  Loss: (1.7933) |  Loss2: (0.0000) | Acc: (34.00%) (4901/14208)
Epoch: 1 | Batch_idx: 120 |  Loss: (1.7913) |  Loss2: (0.0000) | Acc: (34.00%) (5340/15488)
Epoch: 1 | Batch_idx: 130 |  Loss: (1.7885) |  Loss2: (0.0000) | Acc: (34.00%) (5816/16768)
Epoch: 1 | Batch_idx: 140 |  Loss: (1.7850) |  Loss2: (0.0000) | Acc: (34.00%) (6278/18048)
Epoch: 1 | Batch_idx: 150 |  Loss: (1.7800) |  Loss2: (0.0000) | Acc: (34.00%) (6756/19328)
Epoch: 1 | Batch_idx: 160 |  Loss: (1.7758) |  Loss2: (0.0000) | Acc: (35.00%) (7237/20608)
Epoch: 1 | Batch_idx: 170 |  Loss: (1.7725) |  Loss2: (0.0000) | Acc: (35.00%) (7689/21888)
Epoch: 1 | Batch_idx: 180 |  Loss: (1.7705) |  Loss2: (0.0000) | Acc: (35.00%) (8114/23168)
Epoch: 1 | Batch_idx: 190 |  Loss: (1.7677) |  Loss2: (0.0000) | Acc: (35.00%) (8576/24448)
Epoch: 1 | Batch_idx: 200 |  Loss: (1.7638) |  Loss2: (0.0000) | Acc: (35.00%) (9073/25728)
Epoch: 1 | Batch_idx: 210 |  Loss: (1.7602) |  Loss2: (0.0000) | Acc: (35.00%) (9546/27008)
Epoch: 1 | Batch_idx: 220 |  Loss: (1.7572) |  Loss2: (0.0000) | Acc: (35.00%) (10003/28288)
Epoch: 1 | Batch_idx: 230 |  Loss: (1.7538) |  Loss2: (0.0000) | Acc: (35.00%) (10486/29568)
Epoch: 1 | Batch_idx: 240 |  Loss: (1.7499) |  Loss2: (0.0000) | Acc: (35.00%) (10993/30848)
Epoch: 1 | Batch_idx: 250 |  Loss: (1.7466) |  Loss2: (0.0000) | Acc: (35.00%) (11485/32128)
Epoch: 1 | Batch_idx: 260 |  Loss: (1.7432) |  Loss2: (0.0000) | Acc: (35.00%) (11970/33408)
Epoch: 1 | Batch_idx: 270 |  Loss: (1.7403) |  Loss2: (0.0000) | Acc: (35.00%) (12469/34688)
Epoch: 1 | Batch_idx: 280 |  Loss: (1.7379) |  Loss2: (0.0000) | Acc: (35.00%) (12945/35968)
Epoch: 1 | Batch_idx: 290 |  Loss: (1.7348) |  Loss2: (0.0000) | Acc: (36.00%) (13457/37248)
Epoch: 1 | Batch_idx: 300 |  Loss: (1.7315) |  Loss2: (0.0000) | Acc: (36.00%) (13952/38528)
Epoch: 1 | Batch_idx: 310 |  Loss: (1.7277) |  Loss2: (0.0000) | Acc: (36.00%) (14496/39808)
Epoch: 1 | Batch_idx: 320 |  Loss: (1.7255) |  Loss2: (0.0000) | Acc: (36.00%) (15010/41088)
Epoch: 1 | Batch_idx: 330 |  Loss: (1.7218) |  Loss2: (0.0000) | Acc: (36.00%) (15523/42368)
Epoch: 1 | Batch_idx: 340 |  Loss: (1.7194) |  Loss2: (0.0000) | Acc: (36.00%) (16008/43648)
Epoch: 1 | Batch_idx: 350 |  Loss: (1.7168) |  Loss2: (0.0000) | Acc: (36.00%) (16511/44928)
Epoch: 1 | Batch_idx: 360 |  Loss: (1.7138) |  Loss2: (0.0000) | Acc: (36.00%) (17059/46208)
Epoch: 1 | Batch_idx: 370 |  Loss: (1.7109) |  Loss2: (0.0000) | Acc: (37.00%) (17580/47488)
Epoch: 1 | Batch_idx: 380 |  Loss: (1.7076) |  Loss2: (0.0000) | Acc: (37.00%) (18107/48768)
Epoch: 1 | Batch_idx: 390 |  Loss: (1.7054) |  Loss2: (0.0000) | Acc: (37.00%) (18586/50000)
# TEST : Loss: (1.7550) | Acc: (34.00%) (3490/10000)
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
Epoch: 2 | Batch_idx: 0 |  Loss: (1.5348) |  Loss2: (0.0000) | Acc: (42.00%) (54/128)
Epoch: 2 | Batch_idx: 10 |  Loss: (1.5794) |  Loss2: (0.0000) | Acc: (41.00%) (579/1408)
Epoch: 2 | Batch_idx: 20 |  Loss: (1.5737) |  Loss2: (0.0000) | Acc: (42.00%) (1131/2688)
Epoch: 2 | Batch_idx: 30 |  Loss: (1.5767) |  Loss2: (0.0000) | Acc: (42.00%) (1677/3968)
Epoch: 2 | Batch_idx: 40 |  Loss: (1.5837) |  Loss2: (0.0000) | Acc: (41.00%) (2192/5248)
Epoch: 2 | Batch_idx: 50 |  Loss: (1.5896) |  Loss2: (0.0000) | Acc: (41.00%) (2692/6528)
Epoch: 2 | Batch_idx: 60 |  Loss: (1.5876) |  Loss2: (0.0000) | Acc: (41.00%) (3236/7808)
Epoch: 2 | Batch_idx: 70 |  Loss: (1.5830) |  Loss2: (0.0000) | Acc: (41.00%) (3775/9088)
Epoch: 2 | Batch_idx: 80 |  Loss: (1.5836) |  Loss2: (0.0000) | Acc: (41.00%) (4289/10368)
Epoch: 2 | Batch_idx: 90 |  Loss: (1.5838) |  Loss2: (0.0000) | Acc: (41.00%) (4806/11648)
Epoch: 2 | Batch_idx: 100 |  Loss: (1.5829) |  Loss2: (0.0000) | Acc: (41.00%) (5323/12928)
Epoch: 2 | Batch_idx: 110 |  Loss: (1.5821) |  Loss2: (0.0000) | Acc: (41.00%) (5861/14208)
Epoch: 2 | Batch_idx: 120 |  Loss: (1.5793) |  Loss2: (0.0000) | Acc: (41.00%) (6407/15488)
Epoch: 2 | Batch_idx: 130 |  Loss: (1.5788) |  Loss2: (0.0000) | Acc: (41.00%) (6911/16768)
Epoch: 2 | Batch_idx: 140 |  Loss: (1.5771) |  Loss2: (0.0000) | Acc: (41.00%) (7440/18048)
Epoch: 2 | Batch_idx: 150 |  Loss: (1.5758) |  Loss2: (0.0000) | Acc: (41.00%) (7970/19328)
Epoch: 2 | Batch_idx: 160 |  Loss: (1.5764) |  Loss2: (0.0000) | Acc: (41.00%) (8494/20608)
Epoch: 2 | Batch_idx: 170 |  Loss: (1.5746) |  Loss2: (0.0000) | Acc: (41.00%) (9048/21888)
Epoch: 2 | Batch_idx: 180 |  Loss: (1.5720) |  Loss2: (0.0000) | Acc: (41.00%) (9592/23168)
Epoch: 2 | Batch_idx: 190 |  Loss: (1.5700) |  Loss2: (0.0000) | Acc: (41.00%) (10162/24448)
Epoch: 2 | Batch_idx: 200 |  Loss: (1.5670) |  Loss2: (0.0000) | Acc: (41.00%) (10742/25728)
Epoch: 2 | Batch_idx: 210 |  Loss: (1.5650) |  Loss2: (0.0000) | Acc: (41.00%) (11299/27008)
Epoch: 2 | Batch_idx: 220 |  Loss: (1.5636) |  Loss2: (0.0000) | Acc: (41.00%) (11849/28288)
Epoch: 2 | Batch_idx: 230 |  Loss: (1.5614) |  Loss2: (0.0000) | Acc: (41.00%) (12413/29568)
Epoch: 2 | Batch_idx: 240 |  Loss: (1.5584) |  Loss2: (0.0000) | Acc: (42.00%) (13008/30848)
Epoch: 2 | Batch_idx: 250 |  Loss: (1.5568) |  Loss2: (0.0000) | Acc: (42.00%) (13561/32128)
Epoch: 2 | Batch_idx: 260 |  Loss: (1.5555) |  Loss2: (0.0000) | Acc: (42.00%) (14120/33408)
Epoch: 2 | Batch_idx: 270 |  Loss: (1.5525) |  Loss2: (0.0000) | Acc: (42.00%) (14723/34688)
Epoch: 2 | Batch_idx: 280 |  Loss: (1.5509) |  Loss2: (0.0000) | Acc: (42.00%) (15280/35968)
Epoch: 2 | Batch_idx: 290 |  Loss: (1.5476) |  Loss2: (0.0000) | Acc: (42.00%) (15890/37248)
Epoch: 2 | Batch_idx: 300 |  Loss: (1.5469) |  Loss2: (0.0000) | Acc: (42.00%) (16439/38528)
Epoch: 2 | Batch_idx: 310 |  Loss: (1.5464) |  Loss2: (0.0000) | Acc: (42.00%) (17008/39808)
Epoch: 2 | Batch_idx: 320 |  Loss: (1.5447) |  Loss2: (0.0000) | Acc: (42.00%) (17595/41088)
Epoch: 2 | Batch_idx: 330 |  Loss: (1.5428) |  Loss2: (0.0000) | Acc: (42.00%) (18175/42368)
Epoch: 2 | Batch_idx: 340 |  Loss: (1.5406) |  Loss2: (0.0000) | Acc: (43.00%) (18770/43648)
Epoch: 2 | Batch_idx: 350 |  Loss: (1.5395) |  Loss2: (0.0000) | Acc: (43.00%) (19330/44928)
Epoch: 2 | Batch_idx: 360 |  Loss: (1.5375) |  Loss2: (0.0000) | Acc: (43.00%) (19912/46208)
Epoch: 2 | Batch_idx: 370 |  Loss: (1.5359) |  Loss2: (0.0000) | Acc: (43.00%) (20499/47488)
Epoch: 2 | Batch_idx: 380 |  Loss: (1.5345) |  Loss2: (0.0000) | Acc: (43.00%) (21034/48768)
Epoch: 2 | Batch_idx: 390 |  Loss: (1.5327) |  Loss2: (0.0000) | Acc: (43.00%) (21600/50000)
# TEST : Loss: (1.5264) | Acc: (43.00%) (4384/10000)
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
Epoch: 3 | Batch_idx: 0 |  Loss: (1.5118) |  Loss2: (0.0000) | Acc: (47.00%) (61/128)
Epoch: 3 | Batch_idx: 10 |  Loss: (1.4761) |  Loss2: (0.0000) | Acc: (44.00%) (632/1408)
Epoch: 3 | Batch_idx: 20 |  Loss: (1.4587) |  Loss2: (0.0000) | Acc: (46.00%) (1241/2688)
Epoch: 3 | Batch_idx: 30 |  Loss: (1.4593) |  Loss2: (0.0000) | Acc: (46.00%) (1826/3968)
Epoch: 3 | Batch_idx: 40 |  Loss: (1.4553) |  Loss2: (0.0000) | Acc: (46.00%) (2432/5248)
Epoch: 3 | Batch_idx: 50 |  Loss: (1.4515) |  Loss2: (0.0000) | Acc: (46.00%) (3030/6528)
Epoch: 3 | Batch_idx: 60 |  Loss: (1.4471) |  Loss2: (0.0000) | Acc: (46.00%) (3643/7808)
Epoch: 3 | Batch_idx: 70 |  Loss: (1.4488) |  Loss2: (0.0000) | Acc: (46.00%) (4262/9088)
Epoch: 3 | Batch_idx: 80 |  Loss: (1.4455) |  Loss2: (0.0000) | Acc: (47.00%) (4893/10368)
Epoch: 3 | Batch_idx: 90 |  Loss: (1.4465) |  Loss2: (0.0000) | Acc: (47.00%) (5477/11648)
Epoch: 3 | Batch_idx: 100 |  Loss: (1.4457) |  Loss2: (0.0000) | Acc: (46.00%) (6047/12928)
Epoch: 3 | Batch_idx: 110 |  Loss: (1.4446) |  Loss2: (0.0000) | Acc: (46.00%) (6660/14208)
Epoch: 3 | Batch_idx: 120 |  Loss: (1.4416) |  Loss2: (0.0000) | Acc: (47.00%) (7287/15488)
Epoch: 3 | Batch_idx: 130 |  Loss: (1.4396) |  Loss2: (0.0000) | Acc: (47.00%) (7888/16768)
Epoch: 3 | Batch_idx: 140 |  Loss: (1.4347) |  Loss2: (0.0000) | Acc: (47.00%) (8531/18048)
Epoch: 3 | Batch_idx: 150 |  Loss: (1.4358) |  Loss2: (0.0000) | Acc: (47.00%) (9117/19328)
Epoch: 3 | Batch_idx: 160 |  Loss: (1.4322) |  Loss2: (0.0000) | Acc: (47.00%) (9751/20608)
Epoch: 3 | Batch_idx: 170 |  Loss: (1.4307) |  Loss2: (0.0000) | Acc: (47.00%) (10386/21888)
Epoch: 3 | Batch_idx: 180 |  Loss: (1.4271) |  Loss2: (0.0000) | Acc: (47.00%) (11043/23168)
Epoch: 3 | Batch_idx: 190 |  Loss: (1.4254) |  Loss2: (0.0000) | Acc: (47.00%) (11683/24448)
Epoch: 3 | Batch_idx: 200 |  Loss: (1.4214) |  Loss2: (0.0000) | Acc: (47.00%) (12321/25728)
Epoch: 3 | Batch_idx: 210 |  Loss: (1.4184) |  Loss2: (0.0000) | Acc: (48.00%) (12992/27008)
Epoch: 3 | Batch_idx: 220 |  Loss: (1.4169) |  Loss2: (0.0000) | Acc: (48.00%) (13660/28288)
Epoch: 3 | Batch_idx: 230 |  Loss: (1.4152) |  Loss2: (0.0000) | Acc: (48.00%) (14279/29568)
Epoch: 3 | Batch_idx: 240 |  Loss: (1.4135) |  Loss2: (0.0000) | Acc: (48.00%) (14919/30848)
Epoch: 3 | Batch_idx: 250 |  Loss: (1.4128) |  Loss2: (0.0000) | Acc: (48.00%) (15528/32128)
Epoch: 3 | Batch_idx: 260 |  Loss: (1.4108) |  Loss2: (0.0000) | Acc: (48.00%) (16172/33408)
Epoch: 3 | Batch_idx: 270 |  Loss: (1.4100) |  Loss2: (0.0000) | Acc: (48.00%) (16818/34688)
Epoch: 3 | Batch_idx: 280 |  Loss: (1.4104) |  Loss2: (0.0000) | Acc: (48.00%) (17429/35968)
Epoch: 3 | Batch_idx: 290 |  Loss: (1.4085) |  Loss2: (0.0000) | Acc: (48.00%) (18081/37248)
Epoch: 3 | Batch_idx: 300 |  Loss: (1.4067) |  Loss2: (0.0000) | Acc: (48.00%) (18718/38528)
Epoch: 3 | Batch_idx: 310 |  Loss: (1.4059) |  Loss2: (0.0000) | Acc: (48.00%) (19384/39808)
Epoch: 3 | Batch_idx: 320 |  Loss: (1.4034) |  Loss2: (0.0000) | Acc: (48.00%) (20046/41088)
Epoch: 3 | Batch_idx: 330 |  Loss: (1.4021) |  Loss2: (0.0000) | Acc: (48.00%) (20695/42368)
Epoch: 3 | Batch_idx: 340 |  Loss: (1.4004) |  Loss2: (0.0000) | Acc: (48.00%) (21354/43648)
Epoch: 3 | Batch_idx: 350 |  Loss: (1.3989) |  Loss2: (0.0000) | Acc: (48.00%) (22013/44928)
Epoch: 3 | Batch_idx: 360 |  Loss: (1.3972) |  Loss2: (0.0000) | Acc: (49.00%) (22692/46208)
Epoch: 3 | Batch_idx: 370 |  Loss: (1.3957) |  Loss2: (0.0000) | Acc: (49.00%) (23365/47488)
Epoch: 3 | Batch_idx: 380 |  Loss: (1.3939) |  Loss2: (0.0000) | Acc: (49.00%) (24058/48768)
Epoch: 3 | Batch_idx: 390 |  Loss: (1.3926) |  Loss2: (0.0000) | Acc: (49.00%) (24681/50000)
# TEST : Loss: (1.3975) | Acc: (49.00%) (4940/10000)
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
Epoch: 4 | Batch_idx: 0 |  Loss: (1.2493) |  Loss2: (0.0000) | Acc: (53.00%) (69/128)
Epoch: 4 | Batch_idx: 10 |  Loss: (1.3323) |  Loss2: (0.0000) | Acc: (52.00%) (739/1408)
Epoch: 4 | Batch_idx: 20 |  Loss: (1.3292) |  Loss2: (0.0000) | Acc: (52.00%) (1406/2688)
Epoch: 4 | Batch_idx: 30 |  Loss: (1.3304) |  Loss2: (0.0000) | Acc: (52.00%) (2066/3968)
Epoch: 4 | Batch_idx: 40 |  Loss: (1.3209) |  Loss2: (0.0000) | Acc: (52.00%) (2737/5248)
Epoch: 4 | Batch_idx: 50 |  Loss: (1.3164) |  Loss2: (0.0000) | Acc: (52.00%) (3410/6528)
Epoch: 4 | Batch_idx: 60 |  Loss: (1.3158) |  Loss2: (0.0000) | Acc: (52.00%) (4071/7808)
Epoch: 4 | Batch_idx: 70 |  Loss: (1.3145) |  Loss2: (0.0000) | Acc: (52.00%) (4754/9088)
Epoch: 4 | Batch_idx: 80 |  Loss: (1.3118) |  Loss2: (0.0000) | Acc: (52.00%) (5436/10368)
Epoch: 4 | Batch_idx: 90 |  Loss: (1.3074) |  Loss2: (0.0000) | Acc: (52.00%) (6142/11648)
Epoch: 4 | Batch_idx: 100 |  Loss: (1.3076) |  Loss2: (0.0000) | Acc: (52.00%) (6823/12928)
Epoch: 4 | Batch_idx: 110 |  Loss: (1.3045) |  Loss2: (0.0000) | Acc: (52.00%) (7528/14208)
Epoch: 4 | Batch_idx: 120 |  Loss: (1.3035) |  Loss2: (0.0000) | Acc: (53.00%) (8225/15488)
Epoch: 4 | Batch_idx: 130 |  Loss: (1.3011) |  Loss2: (0.0000) | Acc: (53.00%) (8950/16768)
Epoch: 4 | Batch_idx: 140 |  Loss: (1.3019) |  Loss2: (0.0000) | Acc: (53.00%) (9616/18048)
Epoch: 4 | Batch_idx: 150 |  Loss: (1.2970) |  Loss2: (0.0000) | Acc: (53.00%) (10343/19328)
Epoch: 4 | Batch_idx: 160 |  Loss: (1.2952) |  Loss2: (0.0000) | Acc: (53.00%) (11031/20608)
Epoch: 4 | Batch_idx: 170 |  Loss: (1.2943) |  Loss2: (0.0000) | Acc: (53.00%) (11713/21888)
Epoch: 4 | Batch_idx: 180 |  Loss: (1.2931) |  Loss2: (0.0000) | Acc: (53.00%) (12441/23168)
Epoch: 4 | Batch_idx: 190 |  Loss: (1.2943) |  Loss2: (0.0000) | Acc: (53.00%) (13125/24448)
Epoch: 4 | Batch_idx: 200 |  Loss: (1.2948) |  Loss2: (0.0000) | Acc: (53.00%) (13806/25728)
Epoch: 4 | Batch_idx: 210 |  Loss: (1.2960) |  Loss2: (0.0000) | Acc: (53.00%) (14471/27008)
Epoch: 4 | Batch_idx: 220 |  Loss: (1.2935) |  Loss2: (0.0000) | Acc: (53.00%) (15225/28288)
Epoch: 4 | Batch_idx: 230 |  Loss: (1.2921) |  Loss2: (0.0000) | Acc: (53.00%) (15939/29568)
Epoch: 4 | Batch_idx: 240 |  Loss: (1.2886) |  Loss2: (0.0000) | Acc: (54.00%) (16677/30848)
Epoch: 4 | Batch_idx: 250 |  Loss: (1.2861) |  Loss2: (0.0000) | Acc: (54.00%) (17371/32128)
Epoch: 4 | Batch_idx: 260 |  Loss: (1.2840) |  Loss2: (0.0000) | Acc: (54.00%) (18095/33408)
Epoch: 4 | Batch_idx: 270 |  Loss: (1.2827) |  Loss2: (0.0000) | Acc: (54.00%) (18787/34688)
Epoch: 4 | Batch_idx: 280 |  Loss: (1.2823) |  Loss2: (0.0000) | Acc: (54.00%) (19489/35968)
Epoch: 4 | Batch_idx: 290 |  Loss: (1.2809) |  Loss2: (0.0000) | Acc: (54.00%) (20217/37248)
Epoch: 4 | Batch_idx: 300 |  Loss: (1.2780) |  Loss2: (0.0000) | Acc: (54.00%) (20956/38528)
Epoch: 4 | Batch_idx: 310 |  Loss: (1.2767) |  Loss2: (0.0000) | Acc: (54.00%) (21654/39808)
Epoch: 4 | Batch_idx: 320 |  Loss: (1.2759) |  Loss2: (0.0000) | Acc: (54.00%) (22361/41088)
Epoch: 4 | Batch_idx: 330 |  Loss: (1.2746) |  Loss2: (0.0000) | Acc: (54.00%) (23089/42368)
Epoch: 4 | Batch_idx: 340 |  Loss: (1.2725) |  Loss2: (0.0000) | Acc: (54.00%) (23816/43648)
Epoch: 4 | Batch_idx: 350 |  Loss: (1.2697) |  Loss2: (0.0000) | Acc: (54.00%) (24570/44928)
Epoch: 4 | Batch_idx: 360 |  Loss: (1.2690) |  Loss2: (0.0000) | Acc: (54.00%) (25247/46208)
Epoch: 4 | Batch_idx: 370 |  Loss: (1.2668) |  Loss2: (0.0000) | Acc: (54.00%) (25988/47488)
Epoch: 4 | Batch_idx: 380 |  Loss: (1.2650) |  Loss2: (0.0000) | Acc: (54.00%) (26726/48768)
Epoch: 4 | Batch_idx: 390 |  Loss: (1.2637) |  Loss2: (0.0000) | Acc: (54.00%) (27423/50000)
# TEST : Loss: (1.2452) | Acc: (55.00%) (5547/10000)
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 5 | Batch_idx: 0 |  Loss: (1.0700) |  Loss2: (0.0000) | Acc: (69.00%) (89/128)
Epoch: 5 | Batch_idx: 10 |  Loss: (1.2717) |  Loss2: (0.0000) | Acc: (55.00%) (784/1408)
Epoch: 5 | Batch_idx: 20 |  Loss: (1.3472) |  Loss2: (0.0000) | Acc: (52.00%) (1420/2688)
Epoch: 5 | Batch_idx: 30 |  Loss: (1.3787) |  Loss2: (0.0000) | Acc: (51.00%) (2035/3968)
Epoch: 5 | Batch_idx: 40 |  Loss: (1.4048) |  Loss2: (0.0000) | Acc: (50.00%) (2637/5248)
Epoch: 5 | Batch_idx: 50 |  Loss: (1.4161) |  Loss2: (0.0000) | Acc: (50.00%) (3270/6528)
Epoch: 5 | Batch_idx: 60 |  Loss: (1.4312) |  Loss2: (0.0000) | Acc: (49.00%) (3863/7808)
Epoch: 5 | Batch_idx: 70 |  Loss: (1.4332) |  Loss2: (0.0000) | Acc: (49.00%) (4478/9088)
Epoch: 5 | Batch_idx: 80 |  Loss: (1.4370) |  Loss2: (0.0000) | Acc: (48.00%) (5071/10368)
Epoch: 5 | Batch_idx: 90 |  Loss: (1.4352) |  Loss2: (0.0000) | Acc: (49.00%) (5716/11648)
Epoch: 5 | Batch_idx: 100 |  Loss: (1.4343) |  Loss2: (0.0000) | Acc: (49.00%) (6344/12928)
Epoch: 5 | Batch_idx: 110 |  Loss: (1.4331) |  Loss2: (0.0000) | Acc: (49.00%) (6967/14208)
Epoch: 5 | Batch_idx: 120 |  Loss: (1.4287) |  Loss2: (0.0000) | Acc: (49.00%) (7625/15488)
Epoch: 5 | Batch_idx: 130 |  Loss: (1.4239) |  Loss2: (0.0000) | Acc: (49.00%) (8278/16768)
Epoch: 5 | Batch_idx: 140 |  Loss: (1.4233) |  Loss2: (0.0000) | Acc: (49.00%) (8900/18048)
Epoch: 5 | Batch_idx: 150 |  Loss: (1.4246) |  Loss2: (0.0000) | Acc: (49.00%) (9534/19328)
Epoch: 5 | Batch_idx: 160 |  Loss: (1.4226) |  Loss2: (0.0000) | Acc: (49.00%) (10178/20608)
Epoch: 5 | Batch_idx: 170 |  Loss: (1.4215) |  Loss2: (0.0000) | Acc: (49.00%) (10806/21888)
Epoch: 5 | Batch_idx: 180 |  Loss: (1.4195) |  Loss2: (0.0000) | Acc: (49.00%) (11451/23168)
Epoch: 5 | Batch_idx: 190 |  Loss: (1.4168) |  Loss2: (0.0000) | Acc: (49.00%) (12095/24448)
Epoch: 5 | Batch_idx: 200 |  Loss: (1.4145) |  Loss2: (0.0000) | Acc: (49.00%) (12734/25728)
Epoch: 5 | Batch_idx: 210 |  Loss: (1.4139) |  Loss2: (0.0000) | Acc: (49.00%) (13394/27008)
Epoch: 5 | Batch_idx: 220 |  Loss: (1.4109) |  Loss2: (0.0000) | Acc: (49.00%) (14064/28288)
Epoch: 5 | Batch_idx: 230 |  Loss: (1.4107) |  Loss2: (0.0000) | Acc: (49.00%) (14681/29568)
Epoch: 5 | Batch_idx: 240 |  Loss: (1.4086) |  Loss2: (0.0000) | Acc: (49.00%) (15315/30848)
Epoch: 5 | Batch_idx: 250 |  Loss: (1.4072) |  Loss2: (0.0000) | Acc: (49.00%) (15965/32128)
Epoch: 5 | Batch_idx: 260 |  Loss: (1.4029) |  Loss2: (0.0000) | Acc: (49.00%) (16665/33408)
Epoch: 5 | Batch_idx: 270 |  Loss: (1.4000) |  Loss2: (0.0000) | Acc: (50.00%) (17367/34688)
Epoch: 5 | Batch_idx: 280 |  Loss: (1.3972) |  Loss2: (0.0000) | Acc: (50.00%) (18017/35968)
Epoch: 5 | Batch_idx: 290 |  Loss: (1.3948) |  Loss2: (0.0000) | Acc: (50.00%) (18676/37248)
Epoch: 5 | Batch_idx: 300 |  Loss: (1.3928) |  Loss2: (0.0000) | Acc: (50.00%) (19341/38528)
Epoch: 5 | Batch_idx: 310 |  Loss: (1.3907) |  Loss2: (0.0000) | Acc: (50.00%) (19984/39808)
Epoch: 5 | Batch_idx: 320 |  Loss: (1.3882) |  Loss2: (0.0000) | Acc: (50.00%) (20675/41088)
Epoch: 5 | Batch_idx: 330 |  Loss: (1.3857) |  Loss2: (0.0000) | Acc: (50.00%) (21361/42368)
Epoch: 5 | Batch_idx: 340 |  Loss: (1.3833) |  Loss2: (0.0000) | Acc: (50.00%) (22064/43648)
Epoch: 5 | Batch_idx: 350 |  Loss: (1.3817) |  Loss2: (0.0000) | Acc: (50.00%) (22731/44928)
Epoch: 5 | Batch_idx: 360 |  Loss: (1.3803) |  Loss2: (0.0000) | Acc: (50.00%) (23385/46208)
Epoch: 5 | Batch_idx: 370 |  Loss: (1.3776) |  Loss2: (0.0000) | Acc: (50.00%) (24071/47488)
Epoch: 5 | Batch_idx: 380 |  Loss: (1.3762) |  Loss2: (0.0000) | Acc: (50.00%) (24744/48768)
Epoch: 5 | Batch_idx: 390 |  Loss: (1.3732) |  Loss2: (0.0000) | Acc: (50.00%) (25395/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_005.pth.tar'
# TEST : Loss: (1.2769) | Acc: (53.00%) (5350/10000)
percent tensor([0.5069, 0.4931], device='cuda:0')
percent tensor([0.4941, 0.5059], device='cuda:0')
percent tensor([0.4929, 0.5071], device='cuda:0')
percent tensor([0.5026, 0.4974], device='cuda:0')
percent tensor([0.5028, 0.4972], device='cuda:0')
percent tensor([0.5071, 0.4929], device='cuda:0')
percent tensor([0.5090, 0.4910], device='cuda:0')
percent tensor([0.5393, 0.4607], device='cuda:0')
Epoch: 6 | Batch_idx: 0 |  Loss: (1.2735) |  Loss2: (0.0000) | Acc: (53.00%) (68/128)
Epoch: 6 | Batch_idx: 10 |  Loss: (1.2976) |  Loss2: (0.0000) | Acc: (51.00%) (731/1408)
Epoch: 6 | Batch_idx: 20 |  Loss: (1.3048) |  Loss2: (0.0000) | Acc: (51.00%) (1391/2688)
Epoch: 6 | Batch_idx: 30 |  Loss: (1.3097) |  Loss2: (0.0000) | Acc: (52.00%) (2071/3968)
Epoch: 6 | Batch_idx: 40 |  Loss: (1.3056) |  Loss2: (0.0000) | Acc: (52.00%) (2744/5248)
Epoch: 6 | Batch_idx: 50 |  Loss: (1.3057) |  Loss2: (0.0000) | Acc: (52.00%) (3424/6528)
Epoch: 6 | Batch_idx: 60 |  Loss: (1.2959) |  Loss2: (0.0000) | Acc: (53.00%) (4149/7808)
Epoch: 6 | Batch_idx: 70 |  Loss: (1.2956) |  Loss2: (0.0000) | Acc: (53.00%) (4820/9088)
Epoch: 6 | Batch_idx: 80 |  Loss: (1.2930) |  Loss2: (0.0000) | Acc: (53.00%) (5526/10368)
Epoch: 6 | Batch_idx: 90 |  Loss: (1.2918) |  Loss2: (0.0000) | Acc: (53.00%) (6185/11648)
Epoch: 6 | Batch_idx: 100 |  Loss: (1.2869) |  Loss2: (0.0000) | Acc: (53.00%) (6908/12928)
Epoch: 6 | Batch_idx: 110 |  Loss: (1.2873) |  Loss2: (0.0000) | Acc: (53.00%) (7600/14208)
Epoch: 6 | Batch_idx: 120 |  Loss: (1.2884) |  Loss2: (0.0000) | Acc: (53.00%) (8269/15488)
Epoch: 6 | Batch_idx: 130 |  Loss: (1.2850) |  Loss2: (0.0000) | Acc: (53.00%) (8984/16768)
Epoch: 6 | Batch_idx: 140 |  Loss: (1.2845) |  Loss2: (0.0000) | Acc: (53.00%) (9669/18048)
Epoch: 6 | Batch_idx: 150 |  Loss: (1.2831) |  Loss2: (0.0000) | Acc: (53.00%) (10347/19328)
Epoch: 6 | Batch_idx: 160 |  Loss: (1.2823) |  Loss2: (0.0000) | Acc: (53.00%) (11042/20608)
Epoch: 6 | Batch_idx: 170 |  Loss: (1.2829) |  Loss2: (0.0000) | Acc: (53.00%) (11721/21888)
Epoch: 6 | Batch_idx: 180 |  Loss: (1.2786) |  Loss2: (0.0000) | Acc: (53.00%) (12435/23168)
Epoch: 6 | Batch_idx: 190 |  Loss: (1.2768) |  Loss2: (0.0000) | Acc: (53.00%) (13139/24448)
Epoch: 6 | Batch_idx: 200 |  Loss: (1.2783) |  Loss2: (0.0000) | Acc: (53.00%) (13823/25728)
Epoch: 6 | Batch_idx: 210 |  Loss: (1.2768) |  Loss2: (0.0000) | Acc: (53.00%) (14531/27008)
Epoch: 6 | Batch_idx: 220 |  Loss: (1.2762) |  Loss2: (0.0000) | Acc: (53.00%) (15214/28288)
Epoch: 6 | Batch_idx: 230 |  Loss: (1.2732) |  Loss2: (0.0000) | Acc: (53.00%) (15956/29568)
Epoch: 6 | Batch_idx: 240 |  Loss: (1.2700) |  Loss2: (0.0000) | Acc: (54.00%) (16687/30848)
Epoch: 6 | Batch_idx: 250 |  Loss: (1.2692) |  Loss2: (0.0000) | Acc: (54.00%) (17378/32128)
Epoch: 6 | Batch_idx: 260 |  Loss: (1.2691) |  Loss2: (0.0000) | Acc: (54.00%) (18072/33408)
Epoch: 6 | Batch_idx: 270 |  Loss: (1.2674) |  Loss2: (0.0000) | Acc: (54.00%) (18801/34688)
Epoch: 6 | Batch_idx: 280 |  Loss: (1.2658) |  Loss2: (0.0000) | Acc: (54.00%) (19512/35968)
Epoch: 6 | Batch_idx: 290 |  Loss: (1.2661) |  Loss2: (0.0000) | Acc: (54.00%) (20198/37248)
Epoch: 6 | Batch_idx: 300 |  Loss: (1.2670) |  Loss2: (0.0000) | Acc: (54.00%) (20886/38528)
Epoch: 6 | Batch_idx: 310 |  Loss: (1.2651) |  Loss2: (0.0000) | Acc: (54.00%) (21612/39808)
Epoch: 6 | Batch_idx: 320 |  Loss: (1.2646) |  Loss2: (0.0000) | Acc: (54.00%) (22332/41088)
Epoch: 6 | Batch_idx: 330 |  Loss: (1.2651) |  Loss2: (0.0000) | Acc: (54.00%) (23014/42368)
Epoch: 6 | Batch_idx: 340 |  Loss: (1.2655) |  Loss2: (0.0000) | Acc: (54.00%) (23689/43648)
Epoch: 6 | Batch_idx: 350 |  Loss: (1.2648) |  Loss2: (0.0000) | Acc: (54.00%) (24402/44928)
Epoch: 6 | Batch_idx: 360 |  Loss: (1.2643) |  Loss2: (0.0000) | Acc: (54.00%) (25083/46208)
Epoch: 6 | Batch_idx: 370 |  Loss: (1.2637) |  Loss2: (0.0000) | Acc: (54.00%) (25784/47488)
Epoch: 6 | Batch_idx: 380 |  Loss: (1.2629) |  Loss2: (0.0000) | Acc: (54.00%) (26497/48768)
Epoch: 6 | Batch_idx: 390 |  Loss: (1.2616) |  Loss2: (0.0000) | Acc: (54.00%) (27200/50000)
# TEST : Loss: (1.2319) | Acc: (55.00%) (5551/10000)
percent tensor([0.5121, 0.4879], device='cuda:0')
percent tensor([0.4966, 0.5034], device='cuda:0')
percent tensor([0.4869, 0.5131], device='cuda:0')
percent tensor([0.5093, 0.4907], device='cuda:0')
percent tensor([0.5138, 0.4862], device='cuda:0')
percent tensor([0.5142, 0.4858], device='cuda:0')
percent tensor([0.5224, 0.4776], device='cuda:0')
percent tensor([0.6587, 0.3413], device='cuda:0')
Epoch: 7 | Batch_idx: 0 |  Loss: (1.2048) |  Loss2: (0.0000) | Acc: (57.00%) (73/128)
Epoch: 7 | Batch_idx: 10 |  Loss: (1.2344) |  Loss2: (0.0000) | Acc: (56.00%) (790/1408)
Epoch: 7 | Batch_idx: 20 |  Loss: (1.2379) |  Loss2: (0.0000) | Acc: (55.00%) (1502/2688)
Epoch: 7 | Batch_idx: 30 |  Loss: (1.2374) |  Loss2: (0.0000) | Acc: (55.00%) (2205/3968)
Epoch: 7 | Batch_idx: 40 |  Loss: (1.2442) |  Loss2: (0.0000) | Acc: (54.00%) (2880/5248)
Epoch: 7 | Batch_idx: 50 |  Loss: (1.2371) |  Loss2: (0.0000) | Acc: (55.00%) (3599/6528)
Epoch: 7 | Batch_idx: 60 |  Loss: (1.2411) |  Loss2: (0.0000) | Acc: (55.00%) (4296/7808)
Epoch: 7 | Batch_idx: 70 |  Loss: (1.2452) |  Loss2: (0.0000) | Acc: (54.00%) (4977/9088)
Epoch: 7 | Batch_idx: 80 |  Loss: (1.2453) |  Loss2: (0.0000) | Acc: (54.00%) (5677/10368)
Epoch: 7 | Batch_idx: 90 |  Loss: (1.2441) |  Loss2: (0.0000) | Acc: (54.00%) (6377/11648)
Epoch: 7 | Batch_idx: 100 |  Loss: (1.2465) |  Loss2: (0.0000) | Acc: (54.00%) (7072/12928)
Epoch: 7 | Batch_idx: 110 |  Loss: (1.2459) |  Loss2: (0.0000) | Acc: (54.00%) (7759/14208)
Epoch: 7 | Batch_idx: 120 |  Loss: (1.2469) |  Loss2: (0.0000) | Acc: (54.00%) (8450/15488)
Epoch: 7 | Batch_idx: 130 |  Loss: (1.2414) |  Loss2: (0.0000) | Acc: (54.00%) (9164/16768)
Epoch: 7 | Batch_idx: 140 |  Loss: (1.2400) |  Loss2: (0.0000) | Acc: (54.00%) (9873/18048)
Epoch: 7 | Batch_idx: 150 |  Loss: (1.2374) |  Loss2: (0.0000) | Acc: (54.00%) (10593/19328)
Epoch: 7 | Batch_idx: 160 |  Loss: (1.2387) |  Loss2: (0.0000) | Acc: (54.00%) (11270/20608)
Epoch: 7 | Batch_idx: 170 |  Loss: (1.2382) |  Loss2: (0.0000) | Acc: (54.00%) (11980/21888)
Epoch: 7 | Batch_idx: 180 |  Loss: (1.2380) |  Loss2: (0.0000) | Acc: (54.00%) (12671/23168)
Epoch: 7 | Batch_idx: 190 |  Loss: (1.2394) |  Loss2: (0.0000) | Acc: (54.00%) (13347/24448)
Epoch: 7 | Batch_idx: 200 |  Loss: (1.2360) |  Loss2: (0.0000) | Acc: (54.00%) (14086/25728)
Epoch: 7 | Batch_idx: 210 |  Loss: (1.2333) |  Loss2: (0.0000) | Acc: (54.00%) (14794/27008)
Epoch: 7 | Batch_idx: 220 |  Loss: (1.2337) |  Loss2: (0.0000) | Acc: (54.00%) (15479/28288)
Epoch: 7 | Batch_idx: 230 |  Loss: (1.2364) |  Loss2: (0.0000) | Acc: (54.00%) (16138/29568)
Epoch: 7 | Batch_idx: 240 |  Loss: (1.2350) |  Loss2: (0.0000) | Acc: (54.00%) (16859/30848)
Epoch: 7 | Batch_idx: 250 |  Loss: (1.2345) |  Loss2: (0.0000) | Acc: (54.00%) (17581/32128)
Epoch: 7 | Batch_idx: 260 |  Loss: (1.2321) |  Loss2: (0.0000) | Acc: (54.00%) (18322/33408)
Epoch: 7 | Batch_idx: 270 |  Loss: (1.2319) |  Loss2: (0.0000) | Acc: (54.00%) (19050/34688)
Epoch: 7 | Batch_idx: 280 |  Loss: (1.2309) |  Loss2: (0.0000) | Acc: (54.00%) (19782/35968)
Epoch: 7 | Batch_idx: 290 |  Loss: (1.2301) |  Loss2: (0.0000) | Acc: (55.00%) (20512/37248)
Epoch: 7 | Batch_idx: 300 |  Loss: (1.2282) |  Loss2: (0.0000) | Acc: (55.00%) (21257/38528)
Epoch: 7 | Batch_idx: 310 |  Loss: (1.2282) |  Loss2: (0.0000) | Acc: (55.00%) (21961/39808)
Epoch: 7 | Batch_idx: 320 |  Loss: (1.2294) |  Loss2: (0.0000) | Acc: (55.00%) (22666/41088)
Epoch: 7 | Batch_idx: 330 |  Loss: (1.2293) |  Loss2: (0.0000) | Acc: (55.00%) (23367/42368)
Epoch: 7 | Batch_idx: 340 |  Loss: (1.2280) |  Loss2: (0.0000) | Acc: (55.00%) (24067/43648)
Epoch: 7 | Batch_idx: 350 |  Loss: (1.2287) |  Loss2: (0.0000) | Acc: (55.00%) (24764/44928)
Epoch: 7 | Batch_idx: 360 |  Loss: (1.2287) |  Loss2: (0.0000) | Acc: (55.00%) (25470/46208)
Epoch: 7 | Batch_idx: 370 |  Loss: (1.2293) |  Loss2: (0.0000) | Acc: (55.00%) (26155/47488)
Epoch: 7 | Batch_idx: 380 |  Loss: (1.2281) |  Loss2: (0.0000) | Acc: (55.00%) (26900/48768)
Epoch: 7 | Batch_idx: 390 |  Loss: (1.2280) |  Loss2: (0.0000) | Acc: (55.00%) (27566/50000)
# TEST : Loss: (1.2189) | Acc: (55.00%) (5583/10000)
percent tensor([0.5160, 0.4840], device='cuda:0')
percent tensor([0.4996, 0.5004], device='cuda:0')
percent tensor([0.4823, 0.5177], device='cuda:0')
percent tensor([0.5160, 0.4840], device='cuda:0')
percent tensor([0.5260, 0.4740], device='cuda:0')
percent tensor([0.5164, 0.4836], device='cuda:0')
percent tensor([0.5269, 0.4731], device='cuda:0')
percent tensor([0.7348, 0.2652], device='cuda:0')
Epoch: 8 | Batch_idx: 0 |  Loss: (1.1083) |  Loss2: (0.0000) | Acc: (61.00%) (79/128)
Epoch: 8 | Batch_idx: 10 |  Loss: (1.2195) |  Loss2: (0.0000) | Acc: (56.00%) (796/1408)
Epoch: 8 | Batch_idx: 20 |  Loss: (1.2222) |  Loss2: (0.0000) | Acc: (56.00%) (1522/2688)
Epoch: 8 | Batch_idx: 30 |  Loss: (1.2300) |  Loss2: (0.0000) | Acc: (56.00%) (2228/3968)
Epoch: 8 | Batch_idx: 40 |  Loss: (1.2260) |  Loss2: (0.0000) | Acc: (55.00%) (2938/5248)
Epoch: 8 | Batch_idx: 50 |  Loss: (1.2289) |  Loss2: (0.0000) | Acc: (56.00%) (3672/6528)
Epoch: 8 | Batch_idx: 60 |  Loss: (1.2202) |  Loss2: (0.0000) | Acc: (56.00%) (4404/7808)
Epoch: 8 | Batch_idx: 70 |  Loss: (1.2225) |  Loss2: (0.0000) | Acc: (56.00%) (5094/9088)
Epoch: 8 | Batch_idx: 80 |  Loss: (1.2167) |  Loss2: (0.0000) | Acc: (56.00%) (5819/10368)
Epoch: 8 | Batch_idx: 90 |  Loss: (1.2211) |  Loss2: (0.0000) | Acc: (55.00%) (6511/11648)
Epoch: 8 | Batch_idx: 100 |  Loss: (1.2211) |  Loss2: (0.0000) | Acc: (55.00%) (7210/12928)
Epoch: 8 | Batch_idx: 110 |  Loss: (1.2146) |  Loss2: (0.0000) | Acc: (55.00%) (7950/14208)
Epoch: 8 | Batch_idx: 120 |  Loss: (1.2116) |  Loss2: (0.0000) | Acc: (56.00%) (8675/15488)
Epoch: 8 | Batch_idx: 130 |  Loss: (1.2128) |  Loss2: (0.0000) | Acc: (55.00%) (9372/16768)
Epoch: 8 | Batch_idx: 140 |  Loss: (1.2151) |  Loss2: (0.0000) | Acc: (55.00%) (10060/18048)
Epoch: 8 | Batch_idx: 150 |  Loss: (1.2135) |  Loss2: (0.0000) | Acc: (55.00%) (10777/19328)
Epoch: 8 | Batch_idx: 160 |  Loss: (1.2109) |  Loss2: (0.0000) | Acc: (55.00%) (11494/20608)
Epoch: 8 | Batch_idx: 170 |  Loss: (1.2099) |  Loss2: (0.0000) | Acc: (55.00%) (12203/21888)
Epoch: 8 | Batch_idx: 180 |  Loss: (1.2102) |  Loss2: (0.0000) | Acc: (55.00%) (12932/23168)
Epoch: 8 | Batch_idx: 190 |  Loss: (1.2103) |  Loss2: (0.0000) | Acc: (55.00%) (13656/24448)
Epoch: 8 | Batch_idx: 200 |  Loss: (1.2101) |  Loss2: (0.0000) | Acc: (55.00%) (14388/25728)
Epoch: 8 | Batch_idx: 210 |  Loss: (1.2120) |  Loss2: (0.0000) | Acc: (55.00%) (15085/27008)
Epoch: 8 | Batch_idx: 220 |  Loss: (1.2110) |  Loss2: (0.0000) | Acc: (55.00%) (15815/28288)
Epoch: 8 | Batch_idx: 230 |  Loss: (1.2107) |  Loss2: (0.0000) | Acc: (55.00%) (16542/29568)
Epoch: 8 | Batch_idx: 240 |  Loss: (1.2121) |  Loss2: (0.0000) | Acc: (55.00%) (17245/30848)
Epoch: 8 | Batch_idx: 250 |  Loss: (1.2126) |  Loss2: (0.0000) | Acc: (55.00%) (17956/32128)
Epoch: 8 | Batch_idx: 260 |  Loss: (1.2140) |  Loss2: (0.0000) | Acc: (55.00%) (18647/33408)
Epoch: 8 | Batch_idx: 270 |  Loss: (1.2138) |  Loss2: (0.0000) | Acc: (55.00%) (19349/34688)
Epoch: 8 | Batch_idx: 280 |  Loss: (1.2138) |  Loss2: (0.0000) | Acc: (55.00%) (20053/35968)
Epoch: 8 | Batch_idx: 290 |  Loss: (1.2140) |  Loss2: (0.0000) | Acc: (55.00%) (20755/37248)
Epoch: 8 | Batch_idx: 300 |  Loss: (1.2141) |  Loss2: (0.0000) | Acc: (55.00%) (21465/38528)
Epoch: 8 | Batch_idx: 310 |  Loss: (1.2129) |  Loss2: (0.0000) | Acc: (55.00%) (22209/39808)
Epoch: 8 | Batch_idx: 320 |  Loss: (1.2107) |  Loss2: (0.0000) | Acc: (55.00%) (22946/41088)
Epoch: 8 | Batch_idx: 330 |  Loss: (1.2116) |  Loss2: (0.0000) | Acc: (55.00%) (23659/42368)
Epoch: 8 | Batch_idx: 340 |  Loss: (1.2127) |  Loss2: (0.0000) | Acc: (55.00%) (24368/43648)
Epoch: 8 | Batch_idx: 350 |  Loss: (1.2124) |  Loss2: (0.0000) | Acc: (55.00%) (25103/44928)
Epoch: 8 | Batch_idx: 360 |  Loss: (1.2127) |  Loss2: (0.0000) | Acc: (55.00%) (25798/46208)
Epoch: 8 | Batch_idx: 370 |  Loss: (1.2128) |  Loss2: (0.0000) | Acc: (55.00%) (26518/47488)
Epoch: 8 | Batch_idx: 380 |  Loss: (1.2117) |  Loss2: (0.0000) | Acc: (55.00%) (27244/48768)
Epoch: 8 | Batch_idx: 390 |  Loss: (1.2121) |  Loss2: (0.0000) | Acc: (55.00%) (27931/50000)
# TEST : Loss: (1.2099) | Acc: (56.00%) (5639/10000)
percent tensor([0.5169, 0.4831], device='cuda:0')
percent tensor([0.5014, 0.4986], device='cuda:0')
percent tensor([0.4785, 0.5215], device='cuda:0')
percent tensor([0.5229, 0.4771], device='cuda:0')
percent tensor([0.5394, 0.4606], device='cuda:0')
percent tensor([0.5178, 0.4822], device='cuda:0')
percent tensor([0.5268, 0.4732], device='cuda:0')
percent tensor([0.7588, 0.2412], device='cuda:0')
Epoch: 9 | Batch_idx: 0 |  Loss: (1.1936) |  Loss2: (0.0000) | Acc: (53.00%) (69/128)
Epoch: 9 | Batch_idx: 10 |  Loss: (1.2070) |  Loss2: (0.0000) | Acc: (54.00%) (763/1408)
Epoch: 9 | Batch_idx: 20 |  Loss: (1.1663) |  Loss2: (0.0000) | Acc: (56.00%) (1517/2688)
Epoch: 9 | Batch_idx: 30 |  Loss: (1.1784) |  Loss2: (0.0000) | Acc: (56.00%) (2243/3968)
Epoch: 9 | Batch_idx: 40 |  Loss: (1.1912) |  Loss2: (0.0000) | Acc: (56.00%) (2945/5248)
Epoch: 9 | Batch_idx: 50 |  Loss: (1.1863) |  Loss2: (0.0000) | Acc: (55.00%) (3655/6528)
Epoch: 9 | Batch_idx: 60 |  Loss: (1.1884) |  Loss2: (0.0000) | Acc: (56.00%) (4375/7808)
Epoch: 9 | Batch_idx: 70 |  Loss: (1.1869) |  Loss2: (0.0000) | Acc: (56.00%) (5095/9088)
Epoch: 9 | Batch_idx: 80 |  Loss: (1.1894) |  Loss2: (0.0000) | Acc: (55.00%) (5788/10368)
Epoch: 9 | Batch_idx: 90 |  Loss: (1.1975) |  Loss2: (0.0000) | Acc: (55.00%) (6483/11648)
Epoch: 9 | Batch_idx: 100 |  Loss: (1.1960) |  Loss2: (0.0000) | Acc: (55.00%) (7221/12928)
Epoch: 9 | Batch_idx: 110 |  Loss: (1.1968) |  Loss2: (0.0000) | Acc: (55.00%) (7939/14208)
Epoch: 9 | Batch_idx: 120 |  Loss: (1.1929) |  Loss2: (0.0000) | Acc: (55.00%) (8666/15488)
Epoch: 9 | Batch_idx: 130 |  Loss: (1.1903) |  Loss2: (0.0000) | Acc: (56.00%) (9424/16768)
Epoch: 9 | Batch_idx: 140 |  Loss: (1.1888) |  Loss2: (0.0000) | Acc: (56.00%) (10166/18048)
Epoch: 9 | Batch_idx: 150 |  Loss: (1.1936) |  Loss2: (0.0000) | Acc: (56.00%) (10869/19328)
Epoch: 9 | Batch_idx: 160 |  Loss: (1.1966) |  Loss2: (0.0000) | Acc: (56.00%) (11574/20608)
Epoch: 9 | Batch_idx: 170 |  Loss: (1.1965) |  Loss2: (0.0000) | Acc: (56.00%) (12301/21888)
Epoch: 9 | Batch_idx: 180 |  Loss: (1.1976) |  Loss2: (0.0000) | Acc: (56.00%) (13034/23168)
Epoch: 9 | Batch_idx: 190 |  Loss: (1.1965) |  Loss2: (0.0000) | Acc: (56.00%) (13783/24448)
Epoch: 9 | Batch_idx: 200 |  Loss: (1.1979) |  Loss2: (0.0000) | Acc: (56.00%) (14492/25728)
Epoch: 9 | Batch_idx: 210 |  Loss: (1.1976) |  Loss2: (0.0000) | Acc: (56.00%) (15222/27008)
Epoch: 9 | Batch_idx: 220 |  Loss: (1.1965) |  Loss2: (0.0000) | Acc: (56.00%) (15974/28288)
Epoch: 9 | Batch_idx: 230 |  Loss: (1.1978) |  Loss2: (0.0000) | Acc: (56.00%) (16674/29568)
Epoch: 9 | Batch_idx: 240 |  Loss: (1.1962) |  Loss2: (0.0000) | Acc: (56.00%) (17433/30848)
Epoch: 9 | Batch_idx: 250 |  Loss: (1.1961) |  Loss2: (0.0000) | Acc: (56.00%) (18171/32128)
Epoch: 9 | Batch_idx: 260 |  Loss: (1.1972) |  Loss2: (0.0000) | Acc: (56.00%) (18871/33408)
Epoch: 9 | Batch_idx: 270 |  Loss: (1.1990) |  Loss2: (0.0000) | Acc: (56.00%) (19546/34688)
Epoch: 9 | Batch_idx: 280 |  Loss: (1.1993) |  Loss2: (0.0000) | Acc: (56.00%) (20279/35968)
Epoch: 9 | Batch_idx: 290 |  Loss: (1.1987) |  Loss2: (0.0000) | Acc: (56.00%) (21016/37248)
Epoch: 9 | Batch_idx: 300 |  Loss: (1.1989) |  Loss2: (0.0000) | Acc: (56.00%) (21723/38528)
Epoch: 9 | Batch_idx: 310 |  Loss: (1.1977) |  Loss2: (0.0000) | Acc: (56.00%) (22474/39808)
Epoch: 9 | Batch_idx: 320 |  Loss: (1.1993) |  Loss2: (0.0000) | Acc: (56.00%) (23169/41088)
Epoch: 9 | Batch_idx: 330 |  Loss: (1.1994) |  Loss2: (0.0000) | Acc: (56.00%) (23893/42368)
Epoch: 9 | Batch_idx: 340 |  Loss: (1.2006) |  Loss2: (0.0000) | Acc: (56.00%) (24594/43648)
Epoch: 9 | Batch_idx: 350 |  Loss: (1.2016) |  Loss2: (0.0000) | Acc: (56.00%) (25307/44928)
Epoch: 9 | Batch_idx: 360 |  Loss: (1.2021) |  Loss2: (0.0000) | Acc: (56.00%) (26025/46208)
Epoch: 9 | Batch_idx: 370 |  Loss: (1.2027) |  Loss2: (0.0000) | Acc: (56.00%) (26729/47488)
Epoch: 9 | Batch_idx: 380 |  Loss: (1.2026) |  Loss2: (0.0000) | Acc: (56.00%) (27445/48768)
Epoch: 9 | Batch_idx: 390 |  Loss: (1.2018) |  Loss2: (0.0000) | Acc: (56.00%) (28147/50000)
# TEST : Loss: (1.1964) | Acc: (56.00%) (5665/10000)
percent tensor([0.5184, 0.4816], device='cuda:0')
percent tensor([0.5036, 0.4964], device='cuda:0')
percent tensor([0.4764, 0.5236], device='cuda:0')
percent tensor([0.5310, 0.4690], device='cuda:0')
percent tensor([0.5494, 0.4506], device='cuda:0')
percent tensor([0.5183, 0.4817], device='cuda:0')
percent tensor([0.5258, 0.4742], device='cuda:0')
percent tensor([0.7744, 0.2256], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 10 | Batch_idx: 0 |  Loss: (1.1443) |  Loss2: (0.0000) | Acc: (57.00%) (74/128)
Epoch: 10 | Batch_idx: 10 |  Loss: (1.1957) |  Loss2: (0.0000) | Acc: (56.00%) (789/1408)
Epoch: 10 | Batch_idx: 20 |  Loss: (1.2033) |  Loss2: (0.0000) | Acc: (55.00%) (1497/2688)
Epoch: 10 | Batch_idx: 30 |  Loss: (1.1859) |  Loss2: (0.0000) | Acc: (56.00%) (2225/3968)
Epoch: 10 | Batch_idx: 40 |  Loss: (1.1816) |  Loss2: (0.0000) | Acc: (56.00%) (2968/5248)
Epoch: 10 | Batch_idx: 50 |  Loss: (1.1747) |  Loss2: (0.0000) | Acc: (56.00%) (3719/6528)
Epoch: 10 | Batch_idx: 60 |  Loss: (1.1745) |  Loss2: (0.0000) | Acc: (57.00%) (4469/7808)
Epoch: 10 | Batch_idx: 70 |  Loss: (1.1760) |  Loss2: (0.0000) | Acc: (57.00%) (5202/9088)
Epoch: 10 | Batch_idx: 80 |  Loss: (1.1691) |  Loss2: (0.0000) | Acc: (57.00%) (5967/10368)
Epoch: 10 | Batch_idx: 90 |  Loss: (1.1658) |  Loss2: (0.0000) | Acc: (57.00%) (6718/11648)
Epoch: 10 | Batch_idx: 100 |  Loss: (1.1645) |  Loss2: (0.0000) | Acc: (57.00%) (7443/12928)
Epoch: 10 | Batch_idx: 110 |  Loss: (1.1653) |  Loss2: (0.0000) | Acc: (57.00%) (8173/14208)
Epoch: 10 | Batch_idx: 120 |  Loss: (1.1634) |  Loss2: (0.0000) | Acc: (57.00%) (8932/15488)
Epoch: 10 | Batch_idx: 130 |  Loss: (1.1625) |  Loss2: (0.0000) | Acc: (57.00%) (9685/16768)
Epoch: 10 | Batch_idx: 140 |  Loss: (1.1616) |  Loss2: (0.0000) | Acc: (57.00%) (10438/18048)
Epoch: 10 | Batch_idx: 150 |  Loss: (1.1627) |  Loss2: (0.0000) | Acc: (57.00%) (11165/19328)
Epoch: 10 | Batch_idx: 160 |  Loss: (1.1591) |  Loss2: (0.0000) | Acc: (57.00%) (11931/20608)
Epoch: 10 | Batch_idx: 170 |  Loss: (1.1566) |  Loss2: (0.0000) | Acc: (57.00%) (12690/21888)
Epoch: 10 | Batch_idx: 180 |  Loss: (1.1540) |  Loss2: (0.0000) | Acc: (58.00%) (13455/23168)
Epoch: 10 | Batch_idx: 190 |  Loss: (1.1533) |  Loss2: (0.0000) | Acc: (58.00%) (14214/24448)
Epoch: 10 | Batch_idx: 200 |  Loss: (1.1520) |  Loss2: (0.0000) | Acc: (58.00%) (14993/25728)
Epoch: 10 | Batch_idx: 210 |  Loss: (1.1500) |  Loss2: (0.0000) | Acc: (58.00%) (15770/27008)
Epoch: 10 | Batch_idx: 220 |  Loss: (1.1496) |  Loss2: (0.0000) | Acc: (58.00%) (16517/28288)
Epoch: 10 | Batch_idx: 230 |  Loss: (1.1476) |  Loss2: (0.0000) | Acc: (58.00%) (17278/29568)
Epoch: 10 | Batch_idx: 240 |  Loss: (1.1471) |  Loss2: (0.0000) | Acc: (58.00%) (18029/30848)
Epoch: 10 | Batch_idx: 250 |  Loss: (1.1460) |  Loss2: (0.0000) | Acc: (58.00%) (18802/32128)
Epoch: 10 | Batch_idx: 260 |  Loss: (1.1456) |  Loss2: (0.0000) | Acc: (58.00%) (19550/33408)
Epoch: 10 | Batch_idx: 270 |  Loss: (1.1444) |  Loss2: (0.0000) | Acc: (58.00%) (20314/34688)
Epoch: 10 | Batch_idx: 280 |  Loss: (1.1438) |  Loss2: (0.0000) | Acc: (58.00%) (21084/35968)
Epoch: 10 | Batch_idx: 290 |  Loss: (1.1432) |  Loss2: (0.0000) | Acc: (58.00%) (21850/37248)
Epoch: 10 | Batch_idx: 300 |  Loss: (1.1422) |  Loss2: (0.0000) | Acc: (58.00%) (22624/38528)
Epoch: 10 | Batch_idx: 310 |  Loss: (1.1416) |  Loss2: (0.0000) | Acc: (58.00%) (23397/39808)
Epoch: 10 | Batch_idx: 320 |  Loss: (1.1388) |  Loss2: (0.0000) | Acc: (58.00%) (24197/41088)
Epoch: 10 | Batch_idx: 330 |  Loss: (1.1372) |  Loss2: (0.0000) | Acc: (58.00%) (24988/42368)
Epoch: 10 | Batch_idx: 340 |  Loss: (1.1349) |  Loss2: (0.0000) | Acc: (59.00%) (25800/43648)
Epoch: 10 | Batch_idx: 350 |  Loss: (1.1336) |  Loss2: (0.0000) | Acc: (59.00%) (26573/44928)
Epoch: 10 | Batch_idx: 360 |  Loss: (1.1332) |  Loss2: (0.0000) | Acc: (59.00%) (27339/46208)
Epoch: 10 | Batch_idx: 370 |  Loss: (1.1314) |  Loss2: (0.0000) | Acc: (59.00%) (28140/47488)
Epoch: 10 | Batch_idx: 380 |  Loss: (1.1303) |  Loss2: (0.0000) | Acc: (59.00%) (28917/48768)
Epoch: 10 | Batch_idx: 390 |  Loss: (1.1290) |  Loss2: (0.0000) | Acc: (59.00%) (29681/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_010.pth.tar'
# TEST : Loss: (1.1054) | Acc: (59.00%) (5997/10000)
percent tensor([0.5175, 0.4825], device='cuda:0')
percent tensor([0.5037, 0.4963], device='cuda:0')
percent tensor([0.4768, 0.5232], device='cuda:0')
percent tensor([0.5319, 0.4681], device='cuda:0')
percent tensor([0.5495, 0.4505], device='cuda:0')
percent tensor([0.5184, 0.4816], device='cuda:0')
percent tensor([0.5258, 0.4742], device='cuda:0')
percent tensor([0.7771, 0.2229], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(168.4986, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(772.9026, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(768.8468, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1530.8716, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(507.8289, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2165.1082, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4329.1650, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1439.6720, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6107.7583, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12232.9238, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4071.2241, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(17281.3105, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 11 | Batch_idx: 0 |  Loss: (1.0827) |  Loss2: (0.0000) | Acc: (60.00%) (78/128)
Epoch: 11 | Batch_idx: 10 |  Loss: (1.0592) |  Loss2: (0.0000) | Acc: (61.00%) (859/1408)
Epoch: 11 | Batch_idx: 20 |  Loss: (1.0678) |  Loss2: (0.0000) | Acc: (61.00%) (1646/2688)
Epoch: 11 | Batch_idx: 30 |  Loss: (1.0621) |  Loss2: (0.0000) | Acc: (62.00%) (2463/3968)
Epoch: 11 | Batch_idx: 40 |  Loss: (1.0666) |  Loss2: (0.0000) | Acc: (61.00%) (3248/5248)
Epoch: 11 | Batch_idx: 50 |  Loss: (1.0681) |  Loss2: (0.0000) | Acc: (61.00%) (4019/6528)
Epoch: 11 | Batch_idx: 60 |  Loss: (1.0619) |  Loss2: (0.0000) | Acc: (61.00%) (4816/7808)
Epoch: 11 | Batch_idx: 70 |  Loss: (1.0633) |  Loss2: (0.0000) | Acc: (61.00%) (5601/9088)
Epoch: 11 | Batch_idx: 80 |  Loss: (1.0595) |  Loss2: (0.0000) | Acc: (61.00%) (6410/10368)
Epoch: 11 | Batch_idx: 90 |  Loss: (1.0623) |  Loss2: (0.0000) | Acc: (61.00%) (7193/11648)
Epoch: 11 | Batch_idx: 100 |  Loss: (1.0619) |  Loss2: (0.0000) | Acc: (61.00%) (8006/12928)
Epoch: 11 | Batch_idx: 110 |  Loss: (1.0589) |  Loss2: (0.0000) | Acc: (61.00%) (8796/14208)
Epoch: 11 | Batch_idx: 120 |  Loss: (1.0605) |  Loss2: (0.0000) | Acc: (61.00%) (9575/15488)
Epoch: 11 | Batch_idx: 130 |  Loss: (1.0572) |  Loss2: (0.0000) | Acc: (61.00%) (10393/16768)
Epoch: 11 | Batch_idx: 140 |  Loss: (1.0553) |  Loss2: (0.0000) | Acc: (62.00%) (11229/18048)
Epoch: 11 | Batch_idx: 150 |  Loss: (1.0536) |  Loss2: (0.0000) | Acc: (62.00%) (12038/19328)
Epoch: 11 | Batch_idx: 160 |  Loss: (1.0516) |  Loss2: (0.0000) | Acc: (62.00%) (12847/20608)
Epoch: 11 | Batch_idx: 170 |  Loss: (1.0470) |  Loss2: (0.0000) | Acc: (62.00%) (13692/21888)
Epoch: 11 | Batch_idx: 180 |  Loss: (1.0456) |  Loss2: (0.0000) | Acc: (62.00%) (14503/23168)
Epoch: 11 | Batch_idx: 190 |  Loss: (1.0424) |  Loss2: (0.0000) | Acc: (62.00%) (15331/24448)
Epoch: 11 | Batch_idx: 200 |  Loss: (1.0402) |  Loss2: (0.0000) | Acc: (62.00%) (16145/25728)
Epoch: 11 | Batch_idx: 210 |  Loss: (1.0378) |  Loss2: (0.0000) | Acc: (62.00%) (16966/27008)
Epoch: 11 | Batch_idx: 220 |  Loss: (1.0371) |  Loss2: (0.0000) | Acc: (62.00%) (17777/28288)
Epoch: 11 | Batch_idx: 230 |  Loss: (1.0337) |  Loss2: (0.0000) | Acc: (63.00%) (18643/29568)
Epoch: 11 | Batch_idx: 240 |  Loss: (1.0328) |  Loss2: (0.0000) | Acc: (63.00%) (19454/30848)
Epoch: 11 | Batch_idx: 250 |  Loss: (1.0310) |  Loss2: (0.0000) | Acc: (63.00%) (20303/32128)
Epoch: 11 | Batch_idx: 260 |  Loss: (1.0298) |  Loss2: (0.0000) | Acc: (63.00%) (21119/33408)
Epoch: 11 | Batch_idx: 270 |  Loss: (1.0292) |  Loss2: (0.0000) | Acc: (63.00%) (21939/34688)
Epoch: 11 | Batch_idx: 280 |  Loss: (1.0294) |  Loss2: (0.0000) | Acc: (63.00%) (22749/35968)
Epoch: 11 | Batch_idx: 290 |  Loss: (1.0285) |  Loss2: (0.0000) | Acc: (63.00%) (23558/37248)
Epoch: 11 | Batch_idx: 300 |  Loss: (1.0267) |  Loss2: (0.0000) | Acc: (63.00%) (24380/38528)
Epoch: 11 | Batch_idx: 310 |  Loss: (1.0258) |  Loss2: (0.0000) | Acc: (63.00%) (25197/39808)
Epoch: 11 | Batch_idx: 320 |  Loss: (1.0252) |  Loss2: (0.0000) | Acc: (63.00%) (26021/41088)
Epoch: 11 | Batch_idx: 330 |  Loss: (1.0232) |  Loss2: (0.0000) | Acc: (63.00%) (26853/42368)
Epoch: 11 | Batch_idx: 340 |  Loss: (1.0210) |  Loss2: (0.0000) | Acc: (63.00%) (27720/43648)
Epoch: 11 | Batch_idx: 350 |  Loss: (1.0199) |  Loss2: (0.0000) | Acc: (63.00%) (28545/44928)
Epoch: 11 | Batch_idx: 360 |  Loss: (1.0184) |  Loss2: (0.0000) | Acc: (63.00%) (29397/46208)
Epoch: 11 | Batch_idx: 370 |  Loss: (1.0180) |  Loss2: (0.0000) | Acc: (63.00%) (30221/47488)
Epoch: 11 | Batch_idx: 380 |  Loss: (1.0168) |  Loss2: (0.0000) | Acc: (63.00%) (31058/48768)
Epoch: 11 | Batch_idx: 390 |  Loss: (1.0153) |  Loss2: (0.0000) | Acc: (63.00%) (31880/50000)
# TEST : Loss: (1.0851) | Acc: (61.00%) (6132/10000)
percent tensor([0.5172, 0.4828], device='cuda:0')
percent tensor([0.5038, 0.4962], device='cuda:0')
percent tensor([0.4766, 0.5234], device='cuda:0')
percent tensor([0.5317, 0.4683], device='cuda:0')
percent tensor([0.5525, 0.4475], device='cuda:0')
percent tensor([0.5197, 0.4803], device='cuda:0')
percent tensor([0.5279, 0.4721], device='cuda:0')
percent tensor([0.7967, 0.2033], device='cuda:0')
Epoch: 12 | Batch_idx: 0 |  Loss: (0.9216) |  Loss2: (0.0000) | Acc: (61.00%) (79/128)
Epoch: 12 | Batch_idx: 10 |  Loss: (0.9952) |  Loss2: (0.0000) | Acc: (64.00%) (907/1408)
Epoch: 12 | Batch_idx: 20 |  Loss: (0.9885) |  Loss2: (0.0000) | Acc: (64.00%) (1737/2688)
Epoch: 12 | Batch_idx: 30 |  Loss: (0.9840) |  Loss2: (0.0000) | Acc: (64.00%) (2557/3968)
Epoch: 12 | Batch_idx: 40 |  Loss: (0.9856) |  Loss2: (0.0000) | Acc: (64.00%) (3399/5248)
Epoch: 12 | Batch_idx: 50 |  Loss: (0.9745) |  Loss2: (0.0000) | Acc: (65.00%) (4252/6528)
Epoch: 12 | Batch_idx: 60 |  Loss: (0.9699) |  Loss2: (0.0000) | Acc: (65.00%) (5123/7808)
Epoch: 12 | Batch_idx: 70 |  Loss: (0.9726) |  Loss2: (0.0000) | Acc: (65.00%) (5951/9088)
Epoch: 12 | Batch_idx: 80 |  Loss: (0.9709) |  Loss2: (0.0000) | Acc: (65.00%) (6782/10368)
Epoch: 12 | Batch_idx: 90 |  Loss: (0.9675) |  Loss2: (0.0000) | Acc: (65.00%) (7625/11648)
Epoch: 12 | Batch_idx: 100 |  Loss: (0.9634) |  Loss2: (0.0000) | Acc: (65.00%) (8477/12928)
Epoch: 12 | Batch_idx: 110 |  Loss: (0.9562) |  Loss2: (0.0000) | Acc: (65.00%) (9360/14208)
Epoch: 12 | Batch_idx: 120 |  Loss: (0.9545) |  Loss2: (0.0000) | Acc: (65.00%) (10215/15488)
Epoch: 12 | Batch_idx: 130 |  Loss: (0.9589) |  Loss2: (0.0000) | Acc: (65.00%) (11045/16768)
Epoch: 12 | Batch_idx: 140 |  Loss: (0.9559) |  Loss2: (0.0000) | Acc: (65.00%) (11898/18048)
Epoch: 12 | Batch_idx: 150 |  Loss: (0.9562) |  Loss2: (0.0000) | Acc: (65.00%) (12743/19328)
Epoch: 12 | Batch_idx: 160 |  Loss: (0.9591) |  Loss2: (0.0000) | Acc: (65.00%) (13557/20608)
Epoch: 12 | Batch_idx: 170 |  Loss: (0.9613) |  Loss2: (0.0000) | Acc: (65.00%) (14378/21888)
Epoch: 12 | Batch_idx: 180 |  Loss: (0.9613) |  Loss2: (0.0000) | Acc: (65.00%) (15220/23168)
Epoch: 12 | Batch_idx: 190 |  Loss: (0.9612) |  Loss2: (0.0000) | Acc: (65.00%) (16065/24448)
Epoch: 12 | Batch_idx: 200 |  Loss: (0.9602) |  Loss2: (0.0000) | Acc: (65.00%) (16903/25728)
Epoch: 12 | Batch_idx: 210 |  Loss: (0.9591) |  Loss2: (0.0000) | Acc: (65.00%) (17767/27008)
Epoch: 12 | Batch_idx: 220 |  Loss: (0.9574) |  Loss2: (0.0000) | Acc: (65.00%) (18617/28288)
Epoch: 12 | Batch_idx: 230 |  Loss: (0.9567) |  Loss2: (0.0000) | Acc: (65.00%) (19480/29568)
Epoch: 12 | Batch_idx: 240 |  Loss: (0.9565) |  Loss2: (0.0000) | Acc: (65.00%) (20328/30848)
Epoch: 12 | Batch_idx: 250 |  Loss: (0.9550) |  Loss2: (0.0000) | Acc: (65.00%) (21183/32128)
Epoch: 12 | Batch_idx: 260 |  Loss: (0.9542) |  Loss2: (0.0000) | Acc: (65.00%) (22037/33408)
Epoch: 12 | Batch_idx: 270 |  Loss: (0.9542) |  Loss2: (0.0000) | Acc: (65.00%) (22881/34688)
Epoch: 12 | Batch_idx: 280 |  Loss: (0.9536) |  Loss2: (0.0000) | Acc: (65.00%) (23732/35968)
Epoch: 12 | Batch_idx: 290 |  Loss: (0.9549) |  Loss2: (0.0000) | Acc: (65.00%) (24568/37248)
Epoch: 12 | Batch_idx: 300 |  Loss: (0.9541) |  Loss2: (0.0000) | Acc: (66.00%) (25436/38528)
Epoch: 12 | Batch_idx: 310 |  Loss: (0.9523) |  Loss2: (0.0000) | Acc: (66.00%) (26303/39808)
Epoch: 12 | Batch_idx: 320 |  Loss: (0.9510) |  Loss2: (0.0000) | Acc: (66.00%) (27171/41088)
Epoch: 12 | Batch_idx: 330 |  Loss: (0.9489) |  Loss2: (0.0000) | Acc: (66.00%) (28063/42368)
Epoch: 12 | Batch_idx: 340 |  Loss: (0.9480) |  Loss2: (0.0000) | Acc: (66.00%) (28929/43648)
Epoch: 12 | Batch_idx: 350 |  Loss: (0.9471) |  Loss2: (0.0000) | Acc: (66.00%) (29787/44928)
Epoch: 12 | Batch_idx: 360 |  Loss: (0.9461) |  Loss2: (0.0000) | Acc: (66.00%) (30645/46208)
Epoch: 12 | Batch_idx: 370 |  Loss: (0.9456) |  Loss2: (0.0000) | Acc: (66.00%) (31518/47488)
Epoch: 12 | Batch_idx: 380 |  Loss: (0.9447) |  Loss2: (0.0000) | Acc: (66.00%) (32392/48768)
Epoch: 12 | Batch_idx: 390 |  Loss: (0.9437) |  Loss2: (0.0000) | Acc: (66.00%) (33226/50000)
# TEST : Loss: (0.9457) | Acc: (66.00%) (6659/10000)
percent tensor([0.5167, 0.4833], device='cuda:0')
percent tensor([0.5037, 0.4963], device='cuda:0')
percent tensor([0.4769, 0.5231], device='cuda:0')
percent tensor([0.5312, 0.4688], device='cuda:0')
percent tensor([0.5501, 0.4499], device='cuda:0')
percent tensor([0.5189, 0.4811], device='cuda:0')
percent tensor([0.5258, 0.4742], device='cuda:0')
percent tensor([0.7881, 0.2119], device='cuda:0')
Epoch: 13 | Batch_idx: 0 |  Loss: (0.8239) |  Loss2: (0.0000) | Acc: (70.00%) (90/128)
Epoch: 13 | Batch_idx: 10 |  Loss: (0.9286) |  Loss2: (0.0000) | Acc: (66.00%) (938/1408)
Epoch: 13 | Batch_idx: 20 |  Loss: (0.9015) |  Loss2: (0.0000) | Acc: (68.00%) (1832/2688)
Epoch: 13 | Batch_idx: 30 |  Loss: (0.8938) |  Loss2: (0.0000) | Acc: (68.00%) (2700/3968)
Epoch: 13 | Batch_idx: 40 |  Loss: (0.8980) |  Loss2: (0.0000) | Acc: (67.00%) (3566/5248)
Epoch: 13 | Batch_idx: 50 |  Loss: (0.8904) |  Loss2: (0.0000) | Acc: (68.00%) (4451/6528)
Epoch: 13 | Batch_idx: 60 |  Loss: (0.8900) |  Loss2: (0.0000) | Acc: (68.00%) (5326/7808)
Epoch: 13 | Batch_idx: 70 |  Loss: (0.8910) |  Loss2: (0.0000) | Acc: (68.00%) (6198/9088)
Epoch: 13 | Batch_idx: 80 |  Loss: (0.8894) |  Loss2: (0.0000) | Acc: (68.00%) (7080/10368)
Epoch: 13 | Batch_idx: 90 |  Loss: (0.8815) |  Loss2: (0.0000) | Acc: (68.00%) (7985/11648)
Epoch: 13 | Batch_idx: 100 |  Loss: (0.8879) |  Loss2: (0.0000) | Acc: (68.00%) (8830/12928)
Epoch: 13 | Batch_idx: 110 |  Loss: (0.8903) |  Loss2: (0.0000) | Acc: (68.00%) (9685/14208)
Epoch: 13 | Batch_idx: 120 |  Loss: (0.8944) |  Loss2: (0.0000) | Acc: (68.00%) (10540/15488)
Epoch: 13 | Batch_idx: 130 |  Loss: (0.8912) |  Loss2: (0.0000) | Acc: (68.00%) (11440/16768)
Epoch: 13 | Batch_idx: 140 |  Loss: (0.8957) |  Loss2: (0.0000) | Acc: (68.00%) (12287/18048)
Epoch: 13 | Batch_idx: 150 |  Loss: (0.8937) |  Loss2: (0.0000) | Acc: (68.00%) (13178/19328)
Epoch: 13 | Batch_idx: 160 |  Loss: (0.8939) |  Loss2: (0.0000) | Acc: (68.00%) (14051/20608)
Epoch: 13 | Batch_idx: 170 |  Loss: (0.8950) |  Loss2: (0.0000) | Acc: (68.00%) (14928/21888)
Epoch: 13 | Batch_idx: 180 |  Loss: (0.8960) |  Loss2: (0.0000) | Acc: (68.00%) (15787/23168)
Epoch: 13 | Batch_idx: 190 |  Loss: (0.8970) |  Loss2: (0.0000) | Acc: (68.00%) (16652/24448)
Epoch: 13 | Batch_idx: 200 |  Loss: (0.8965) |  Loss2: (0.0000) | Acc: (68.00%) (17532/25728)
Epoch: 13 | Batch_idx: 210 |  Loss: (0.8970) |  Loss2: (0.0000) | Acc: (68.00%) (18395/27008)
Epoch: 13 | Batch_idx: 220 |  Loss: (0.8954) |  Loss2: (0.0000) | Acc: (68.00%) (19281/28288)
Epoch: 13 | Batch_idx: 230 |  Loss: (0.8949) |  Loss2: (0.0000) | Acc: (68.00%) (20155/29568)
Epoch: 13 | Batch_idx: 240 |  Loss: (0.8939) |  Loss2: (0.0000) | Acc: (68.00%) (21035/30848)
Epoch: 13 | Batch_idx: 250 |  Loss: (0.8941) |  Loss2: (0.0000) | Acc: (68.00%) (21916/32128)
Epoch: 13 | Batch_idx: 260 |  Loss: (0.8953) |  Loss2: (0.0000) | Acc: (68.00%) (22793/33408)
Epoch: 13 | Batch_idx: 270 |  Loss: (0.8944) |  Loss2: (0.0000) | Acc: (68.00%) (23685/34688)
Epoch: 13 | Batch_idx: 280 |  Loss: (0.8935) |  Loss2: (0.0000) | Acc: (68.00%) (24583/35968)
Epoch: 13 | Batch_idx: 290 |  Loss: (0.8916) |  Loss2: (0.0000) | Acc: (68.00%) (25491/37248)
Epoch: 13 | Batch_idx: 300 |  Loss: (0.8914) |  Loss2: (0.0000) | Acc: (68.00%) (26375/38528)
Epoch: 13 | Batch_idx: 310 |  Loss: (0.8921) |  Loss2: (0.0000) | Acc: (68.00%) (27237/39808)
Epoch: 13 | Batch_idx: 320 |  Loss: (0.8924) |  Loss2: (0.0000) | Acc: (68.00%) (28109/41088)
Epoch: 13 | Batch_idx: 330 |  Loss: (0.8927) |  Loss2: (0.0000) | Acc: (68.00%) (28987/42368)
Epoch: 13 | Batch_idx: 340 |  Loss: (0.8921) |  Loss2: (0.0000) | Acc: (68.00%) (29882/43648)
Epoch: 13 | Batch_idx: 350 |  Loss: (0.8915) |  Loss2: (0.0000) | Acc: (68.00%) (30776/44928)
Epoch: 13 | Batch_idx: 360 |  Loss: (0.8902) |  Loss2: (0.0000) | Acc: (68.00%) (31681/46208)
Epoch: 13 | Batch_idx: 370 |  Loss: (0.8893) |  Loss2: (0.0000) | Acc: (68.00%) (32576/47488)
Epoch: 13 | Batch_idx: 380 |  Loss: (0.8883) |  Loss2: (0.0000) | Acc: (68.00%) (33485/48768)
Epoch: 13 | Batch_idx: 390 |  Loss: (0.8873) |  Loss2: (0.0000) | Acc: (68.00%) (34352/50000)
# TEST : Loss: (0.9341) | Acc: (67.00%) (6725/10000)
percent tensor([0.5168, 0.4832], device='cuda:0')
percent tensor([0.5038, 0.4962], device='cuda:0')
percent tensor([0.4765, 0.5235], device='cuda:0')
percent tensor([0.5310, 0.4690], device='cuda:0')
percent tensor([0.5519, 0.4481], device='cuda:0')
percent tensor([0.5194, 0.4806], device='cuda:0')
percent tensor([0.5271, 0.4729], device='cuda:0')
percent tensor([0.7998, 0.2002], device='cuda:0')
Epoch: 14 | Batch_idx: 0 |  Loss: (0.7329) |  Loss2: (0.0000) | Acc: (73.00%) (94/128)
Epoch: 14 | Batch_idx: 10 |  Loss: (0.8267) |  Loss2: (0.0000) | Acc: (71.00%) (1002/1408)
Epoch: 14 | Batch_idx: 20 |  Loss: (0.8389) |  Loss2: (0.0000) | Acc: (71.00%) (1914/2688)
Epoch: 14 | Batch_idx: 30 |  Loss: (0.8332) |  Loss2: (0.0000) | Acc: (71.00%) (2819/3968)
Epoch: 14 | Batch_idx: 40 |  Loss: (0.8438) |  Loss2: (0.0000) | Acc: (70.00%) (3694/5248)
Epoch: 14 | Batch_idx: 50 |  Loss: (0.8470) |  Loss2: (0.0000) | Acc: (70.00%) (4587/6528)
Epoch: 14 | Batch_idx: 60 |  Loss: (0.8465) |  Loss2: (0.0000) | Acc: (70.00%) (5498/7808)
Epoch: 14 | Batch_idx: 70 |  Loss: (0.8425) |  Loss2: (0.0000) | Acc: (70.00%) (6396/9088)
Epoch: 14 | Batch_idx: 80 |  Loss: (0.8492) |  Loss2: (0.0000) | Acc: (70.00%) (7284/10368)
Epoch: 14 | Batch_idx: 90 |  Loss: (0.8432) |  Loss2: (0.0000) | Acc: (70.00%) (8196/11648)
Epoch: 14 | Batch_idx: 100 |  Loss: (0.8395) |  Loss2: (0.0000) | Acc: (70.00%) (9112/12928)
Epoch: 14 | Batch_idx: 110 |  Loss: (0.8426) |  Loss2: (0.0000) | Acc: (70.00%) (9991/14208)
Epoch: 14 | Batch_idx: 120 |  Loss: (0.8426) |  Loss2: (0.0000) | Acc: (70.00%) (10896/15488)
Epoch: 14 | Batch_idx: 130 |  Loss: (0.8412) |  Loss2: (0.0000) | Acc: (70.00%) (11797/16768)
Epoch: 14 | Batch_idx: 140 |  Loss: (0.8431) |  Loss2: (0.0000) | Acc: (70.00%) (12678/18048)
Epoch: 14 | Batch_idx: 150 |  Loss: (0.8423) |  Loss2: (0.0000) | Acc: (70.00%) (13579/19328)
Epoch: 14 | Batch_idx: 160 |  Loss: (0.8429) |  Loss2: (0.0000) | Acc: (70.00%) (14470/20608)
Epoch: 14 | Batch_idx: 170 |  Loss: (0.8432) |  Loss2: (0.0000) | Acc: (70.00%) (15359/21888)
Epoch: 14 | Batch_idx: 180 |  Loss: (0.8424) |  Loss2: (0.0000) | Acc: (70.00%) (16267/23168)
Epoch: 14 | Batch_idx: 190 |  Loss: (0.8430) |  Loss2: (0.0000) | Acc: (70.00%) (17152/24448)
Epoch: 14 | Batch_idx: 200 |  Loss: (0.8414) |  Loss2: (0.0000) | Acc: (70.00%) (18066/25728)
Epoch: 14 | Batch_idx: 210 |  Loss: (0.8417) |  Loss2: (0.0000) | Acc: (70.00%) (18968/27008)
Epoch: 14 | Batch_idx: 220 |  Loss: (0.8399) |  Loss2: (0.0000) | Acc: (70.00%) (19881/28288)
Epoch: 14 | Batch_idx: 230 |  Loss: (0.8398) |  Loss2: (0.0000) | Acc: (70.00%) (20764/29568)
Epoch: 14 | Batch_idx: 240 |  Loss: (0.8398) |  Loss2: (0.0000) | Acc: (70.00%) (21653/30848)
Epoch: 14 | Batch_idx: 250 |  Loss: (0.8394) |  Loss2: (0.0000) | Acc: (70.00%) (22557/32128)
Epoch: 14 | Batch_idx: 260 |  Loss: (0.8385) |  Loss2: (0.0000) | Acc: (70.00%) (23467/33408)
Epoch: 14 | Batch_idx: 270 |  Loss: (0.8370) |  Loss2: (0.0000) | Acc: (70.00%) (24391/34688)
Epoch: 14 | Batch_idx: 280 |  Loss: (0.8373) |  Loss2: (0.0000) | Acc: (70.00%) (25294/35968)
Epoch: 14 | Batch_idx: 290 |  Loss: (0.8372) |  Loss2: (0.0000) | Acc: (70.00%) (26197/37248)
Epoch: 14 | Batch_idx: 300 |  Loss: (0.8355) |  Loss2: (0.0000) | Acc: (70.00%) (27136/38528)
Epoch: 14 | Batch_idx: 310 |  Loss: (0.8367) |  Loss2: (0.0000) | Acc: (70.00%) (28029/39808)
Epoch: 14 | Batch_idx: 320 |  Loss: (0.8369) |  Loss2: (0.0000) | Acc: (70.00%) (28921/41088)
Epoch: 14 | Batch_idx: 330 |  Loss: (0.8373) |  Loss2: (0.0000) | Acc: (70.00%) (29827/42368)
Epoch: 14 | Batch_idx: 340 |  Loss: (0.8357) |  Loss2: (0.0000) | Acc: (70.00%) (30747/43648)
Epoch: 14 | Batch_idx: 350 |  Loss: (0.8341) |  Loss2: (0.0000) | Acc: (70.00%) (31692/44928)
Epoch: 14 | Batch_idx: 360 |  Loss: (0.8348) |  Loss2: (0.0000) | Acc: (70.00%) (32577/46208)
Epoch: 14 | Batch_idx: 370 |  Loss: (0.8347) |  Loss2: (0.0000) | Acc: (70.00%) (33482/47488)
Epoch: 14 | Batch_idx: 380 |  Loss: (0.8338) |  Loss2: (0.0000) | Acc: (70.00%) (34401/48768)
Epoch: 14 | Batch_idx: 390 |  Loss: (0.8324) |  Loss2: (0.0000) | Acc: (70.00%) (35291/50000)
# TEST : Loss: (0.8621) | Acc: (69.00%) (6972/10000)
percent tensor([0.5168, 0.4832], device='cuda:0')
percent tensor([0.5038, 0.4962], device='cuda:0')
percent tensor([0.4773, 0.5227], device='cuda:0')
percent tensor([0.5305, 0.4695], device='cuda:0')
percent tensor([0.5497, 0.4503], device='cuda:0')
percent tensor([0.5186, 0.4814], device='cuda:0')
percent tensor([0.5261, 0.4739], device='cuda:0')
percent tensor([0.7888, 0.2112], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 15 | Batch_idx: 0 |  Loss: (0.6573) |  Loss2: (0.0000) | Acc: (80.00%) (103/128)
Epoch: 15 | Batch_idx: 10 |  Loss: (0.8436) |  Loss2: (0.0000) | Acc: (70.00%) (994/1408)
Epoch: 15 | Batch_idx: 20 |  Loss: (0.9084) |  Loss2: (0.0000) | Acc: (69.00%) (1856/2688)
Epoch: 15 | Batch_idx: 30 |  Loss: (0.9269) |  Loss2: (0.0000) | Acc: (68.00%) (2700/3968)
Epoch: 15 | Batch_idx: 40 |  Loss: (0.9299) |  Loss2: (0.0000) | Acc: (67.00%) (3552/5248)
Epoch: 15 | Batch_idx: 50 |  Loss: (0.9394) |  Loss2: (0.0000) | Acc: (67.00%) (4392/6528)
Epoch: 15 | Batch_idx: 60 |  Loss: (0.9404) |  Loss2: (0.0000) | Acc: (66.00%) (5231/7808)
Epoch: 15 | Batch_idx: 70 |  Loss: (0.9427) |  Loss2: (0.0000) | Acc: (67.00%) (6100/9088)
Epoch: 15 | Batch_idx: 80 |  Loss: (0.9419) |  Loss2: (0.0000) | Acc: (67.00%) (6964/10368)
Epoch: 15 | Batch_idx: 90 |  Loss: (0.9444) |  Loss2: (0.0000) | Acc: (66.00%) (7796/11648)
Epoch: 15 | Batch_idx: 100 |  Loss: (0.9444) |  Loss2: (0.0000) | Acc: (66.00%) (8657/12928)
Epoch: 15 | Batch_idx: 110 |  Loss: (0.9416) |  Loss2: (0.0000) | Acc: (66.00%) (9516/14208)
Epoch: 15 | Batch_idx: 120 |  Loss: (0.9349) |  Loss2: (0.0000) | Acc: (67.00%) (10399/15488)
Epoch: 15 | Batch_idx: 130 |  Loss: (0.9307) |  Loss2: (0.0000) | Acc: (67.00%) (11286/16768)
Epoch: 15 | Batch_idx: 140 |  Loss: (0.9261) |  Loss2: (0.0000) | Acc: (67.00%) (12184/18048)
Epoch: 15 | Batch_idx: 150 |  Loss: (0.9245) |  Loss2: (0.0000) | Acc: (67.00%) (13065/19328)
Epoch: 15 | Batch_idx: 160 |  Loss: (0.9248) |  Loss2: (0.0000) | Acc: (67.00%) (13927/20608)
Epoch: 15 | Batch_idx: 170 |  Loss: (0.9225) |  Loss2: (0.0000) | Acc: (67.00%) (14812/21888)
Epoch: 15 | Batch_idx: 180 |  Loss: (0.9200) |  Loss2: (0.0000) | Acc: (67.00%) (15697/23168)
Epoch: 15 | Batch_idx: 190 |  Loss: (0.9187) |  Loss2: (0.0000) | Acc: (67.00%) (16551/24448)
Epoch: 15 | Batch_idx: 200 |  Loss: (0.9170) |  Loss2: (0.0000) | Acc: (67.00%) (17417/25728)
Epoch: 15 | Batch_idx: 210 |  Loss: (0.9151) |  Loss2: (0.0000) | Acc: (67.00%) (18292/27008)
Epoch: 15 | Batch_idx: 220 |  Loss: (0.9117) |  Loss2: (0.0000) | Acc: (67.00%) (19197/28288)
Epoch: 15 | Batch_idx: 230 |  Loss: (0.9091) |  Loss2: (0.0000) | Acc: (67.00%) (20094/29568)
Epoch: 15 | Batch_idx: 240 |  Loss: (0.9067) |  Loss2: (0.0000) | Acc: (68.00%) (20996/30848)
Epoch: 15 | Batch_idx: 250 |  Loss: (0.9068) |  Loss2: (0.0000) | Acc: (68.00%) (21855/32128)
Epoch: 15 | Batch_idx: 260 |  Loss: (0.9043) |  Loss2: (0.0000) | Acc: (68.00%) (22746/33408)
Epoch: 15 | Batch_idx: 270 |  Loss: (0.9052) |  Loss2: (0.0000) | Acc: (68.00%) (23611/34688)
Epoch: 15 | Batch_idx: 280 |  Loss: (0.9027) |  Loss2: (0.0000) | Acc: (68.00%) (24510/35968)
Epoch: 15 | Batch_idx: 290 |  Loss: (0.9013) |  Loss2: (0.0000) | Acc: (68.00%) (25384/37248)
Epoch: 15 | Batch_idx: 300 |  Loss: (0.9002) |  Loss2: (0.0000) | Acc: (68.00%) (26278/38528)
Epoch: 15 | Batch_idx: 310 |  Loss: (0.8973) |  Loss2: (0.0000) | Acc: (68.00%) (27168/39808)
Epoch: 15 | Batch_idx: 320 |  Loss: (0.8948) |  Loss2: (0.0000) | Acc: (68.00%) (28078/41088)
Epoch: 15 | Batch_idx: 330 |  Loss: (0.8941) |  Loss2: (0.0000) | Acc: (68.00%) (28971/42368)
Epoch: 15 | Batch_idx: 340 |  Loss: (0.8920) |  Loss2: (0.0000) | Acc: (68.00%) (29890/43648)
Epoch: 15 | Batch_idx: 350 |  Loss: (0.8904) |  Loss2: (0.0000) | Acc: (68.00%) (30792/44928)
Epoch: 15 | Batch_idx: 360 |  Loss: (0.8895) |  Loss2: (0.0000) | Acc: (68.00%) (31697/46208)
Epoch: 15 | Batch_idx: 370 |  Loss: (0.8892) |  Loss2: (0.0000) | Acc: (68.00%) (32578/47488)
Epoch: 15 | Batch_idx: 380 |  Loss: (0.8894) |  Loss2: (0.0000) | Acc: (68.00%) (33457/48768)
Epoch: 15 | Batch_idx: 390 |  Loss: (0.8885) |  Loss2: (0.0000) | Acc: (68.00%) (34314/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_015.pth.tar'
# TEST : Loss: (0.8692) | Acc: (69.00%) (6913/10000)
percent tensor([0.5132, 0.4868], device='cuda:0')
percent tensor([0.5084, 0.4916], device='cuda:0')
percent tensor([0.4777, 0.5223], device='cuda:0')
percent tensor([0.5271, 0.4729], device='cuda:0')
percent tensor([0.5599, 0.4401], device='cuda:0')
percent tensor([0.5145, 0.4855], device='cuda:0')
percent tensor([0.5217, 0.4783], device='cuda:0')
percent tensor([0.8609, 0.1391], device='cuda:0')
Epoch: 16 | Batch_idx: 0 |  Loss: (0.7284) |  Loss2: (0.0000) | Acc: (71.00%) (91/128)
Epoch: 16 | Batch_idx: 10 |  Loss: (0.8272) |  Loss2: (0.0000) | Acc: (71.00%) (1000/1408)
Epoch: 16 | Batch_idx: 20 |  Loss: (0.8459) |  Loss2: (0.0000) | Acc: (70.00%) (1888/2688)
Epoch: 16 | Batch_idx: 30 |  Loss: (0.8611) |  Loss2: (0.0000) | Acc: (69.00%) (2763/3968)
Epoch: 16 | Batch_idx: 40 |  Loss: (0.8558) |  Loss2: (0.0000) | Acc: (70.00%) (3689/5248)
Epoch: 16 | Batch_idx: 50 |  Loss: (0.8499) |  Loss2: (0.0000) | Acc: (70.00%) (4592/6528)
Epoch: 16 | Batch_idx: 60 |  Loss: (0.8497) |  Loss2: (0.0000) | Acc: (69.00%) (5464/7808)
Epoch: 16 | Batch_idx: 70 |  Loss: (0.8439) |  Loss2: (0.0000) | Acc: (70.00%) (6378/9088)
Epoch: 16 | Batch_idx: 80 |  Loss: (0.8409) |  Loss2: (0.0000) | Acc: (70.00%) (7287/10368)
Epoch: 16 | Batch_idx: 90 |  Loss: (0.8401) |  Loss2: (0.0000) | Acc: (70.00%) (8194/11648)
Epoch: 16 | Batch_idx: 100 |  Loss: (0.8421) |  Loss2: (0.0000) | Acc: (70.00%) (9074/12928)
Epoch: 16 | Batch_idx: 110 |  Loss: (0.8423) |  Loss2: (0.0000) | Acc: (70.00%) (9981/14208)
Epoch: 16 | Batch_idx: 120 |  Loss: (0.8387) |  Loss2: (0.0000) | Acc: (70.00%) (10903/15488)
Epoch: 16 | Batch_idx: 130 |  Loss: (0.8370) |  Loss2: (0.0000) | Acc: (70.00%) (11811/16768)
Epoch: 16 | Batch_idx: 140 |  Loss: (0.8394) |  Loss2: (0.0000) | Acc: (70.00%) (12694/18048)
Epoch: 16 | Batch_idx: 150 |  Loss: (0.8393) |  Loss2: (0.0000) | Acc: (70.00%) (13607/19328)
Epoch: 16 | Batch_idx: 160 |  Loss: (0.8381) |  Loss2: (0.0000) | Acc: (70.00%) (14529/20608)
Epoch: 16 | Batch_idx: 170 |  Loss: (0.8381) |  Loss2: (0.0000) | Acc: (70.00%) (15447/21888)
Epoch: 16 | Batch_idx: 180 |  Loss: (0.8353) |  Loss2: (0.0000) | Acc: (70.00%) (16351/23168)
Epoch: 16 | Batch_idx: 190 |  Loss: (0.8360) |  Loss2: (0.0000) | Acc: (70.00%) (17244/24448)
Epoch: 16 | Batch_idx: 200 |  Loss: (0.8374) |  Loss2: (0.0000) | Acc: (70.00%) (18120/25728)
Epoch: 16 | Batch_idx: 210 |  Loss: (0.8374) |  Loss2: (0.0000) | Acc: (70.00%) (19026/27008)
Epoch: 16 | Batch_idx: 220 |  Loss: (0.8373) |  Loss2: (0.0000) | Acc: (70.00%) (19918/28288)
Epoch: 16 | Batch_idx: 230 |  Loss: (0.8373) |  Loss2: (0.0000) | Acc: (70.00%) (20809/29568)
Epoch: 16 | Batch_idx: 240 |  Loss: (0.8362) |  Loss2: (0.0000) | Acc: (70.00%) (21707/30848)
Epoch: 16 | Batch_idx: 250 |  Loss: (0.8364) |  Loss2: (0.0000) | Acc: (70.00%) (22604/32128)
Epoch: 16 | Batch_idx: 260 |  Loss: (0.8379) |  Loss2: (0.0000) | Acc: (70.00%) (23474/33408)
Epoch: 16 | Batch_idx: 270 |  Loss: (0.8371) |  Loss2: (0.0000) | Acc: (70.00%) (24388/34688)
Epoch: 16 | Batch_idx: 280 |  Loss: (0.8355) |  Loss2: (0.0000) | Acc: (70.00%) (25307/35968)
Epoch: 16 | Batch_idx: 290 |  Loss: (0.8353) |  Loss2: (0.0000) | Acc: (70.00%) (26206/37248)
Epoch: 16 | Batch_idx: 300 |  Loss: (0.8344) |  Loss2: (0.0000) | Acc: (70.00%) (27124/38528)
Epoch: 16 | Batch_idx: 310 |  Loss: (0.8335) |  Loss2: (0.0000) | Acc: (70.00%) (28058/39808)
Epoch: 16 | Batch_idx: 320 |  Loss: (0.8345) |  Loss2: (0.0000) | Acc: (70.00%) (28940/41088)
Epoch: 16 | Batch_idx: 330 |  Loss: (0.8328) |  Loss2: (0.0000) | Acc: (70.00%) (29856/42368)
Epoch: 16 | Batch_idx: 340 |  Loss: (0.8332) |  Loss2: (0.0000) | Acc: (70.00%) (30749/43648)
Epoch: 16 | Batch_idx: 350 |  Loss: (0.8345) |  Loss2: (0.0000) | Acc: (70.00%) (31628/44928)
Epoch: 16 | Batch_idx: 360 |  Loss: (0.8332) |  Loss2: (0.0000) | Acc: (70.00%) (32544/46208)
Epoch: 16 | Batch_idx: 370 |  Loss: (0.8325) |  Loss2: (0.0000) | Acc: (70.00%) (33477/47488)
Epoch: 16 | Batch_idx: 380 |  Loss: (0.8323) |  Loss2: (0.0000) | Acc: (70.00%) (34377/48768)
Epoch: 16 | Batch_idx: 390 |  Loss: (0.8316) |  Loss2: (0.0000) | Acc: (70.00%) (35282/50000)
# TEST : Loss: (0.8483) | Acc: (70.00%) (7027/10000)
percent tensor([0.5137, 0.4863], device='cuda:0')
percent tensor([0.5110, 0.4890], device='cuda:0')
percent tensor([0.4787, 0.5213], device='cuda:0')
percent tensor([0.5312, 0.4688], device='cuda:0')
percent tensor([0.5651, 0.4349], device='cuda:0')
percent tensor([0.5128, 0.4872], device='cuda:0')
percent tensor([0.5238, 0.4762], device='cuda:0')
percent tensor([0.9042, 0.0958], device='cuda:0')
Epoch: 17 | Batch_idx: 0 |  Loss: (0.8239) |  Loss2: (0.0000) | Acc: (74.00%) (95/128)
Epoch: 17 | Batch_idx: 10 |  Loss: (0.8024) |  Loss2: (0.0000) | Acc: (72.00%) (1018/1408)
Epoch: 17 | Batch_idx: 20 |  Loss: (0.8038) |  Loss2: (0.0000) | Acc: (71.00%) (1932/2688)
Epoch: 17 | Batch_idx: 30 |  Loss: (0.8101) |  Loss2: (0.0000) | Acc: (71.00%) (2839/3968)
Epoch: 17 | Batch_idx: 40 |  Loss: (0.8093) |  Loss2: (0.0000) | Acc: (71.00%) (3758/5248)
Epoch: 17 | Batch_idx: 50 |  Loss: (0.8193) |  Loss2: (0.0000) | Acc: (71.00%) (4650/6528)
Epoch: 17 | Batch_idx: 60 |  Loss: (0.8246) |  Loss2: (0.0000) | Acc: (71.00%) (5553/7808)
Epoch: 17 | Batch_idx: 70 |  Loss: (0.8285) |  Loss2: (0.0000) | Acc: (71.00%) (6457/9088)
Epoch: 17 | Batch_idx: 80 |  Loss: (0.8279) |  Loss2: (0.0000) | Acc: (71.00%) (7367/10368)
Epoch: 17 | Batch_idx: 90 |  Loss: (0.8305) |  Loss2: (0.0000) | Acc: (70.00%) (8262/11648)
Epoch: 17 | Batch_idx: 100 |  Loss: (0.8236) |  Loss2: (0.0000) | Acc: (71.00%) (9199/12928)
Epoch: 17 | Batch_idx: 110 |  Loss: (0.8228) |  Loss2: (0.0000) | Acc: (71.00%) (10105/14208)
Epoch: 17 | Batch_idx: 120 |  Loss: (0.8194) |  Loss2: (0.0000) | Acc: (71.00%) (11021/15488)
Epoch: 17 | Batch_idx: 130 |  Loss: (0.8201) |  Loss2: (0.0000) | Acc: (71.00%) (11934/16768)
Epoch: 17 | Batch_idx: 140 |  Loss: (0.8182) |  Loss2: (0.0000) | Acc: (71.00%) (12855/18048)
Epoch: 17 | Batch_idx: 150 |  Loss: (0.8175) |  Loss2: (0.0000) | Acc: (71.00%) (13764/19328)
Epoch: 17 | Batch_idx: 160 |  Loss: (0.8156) |  Loss2: (0.0000) | Acc: (71.00%) (14669/20608)
Epoch: 17 | Batch_idx: 170 |  Loss: (0.8160) |  Loss2: (0.0000) | Acc: (71.00%) (15591/21888)
Epoch: 17 | Batch_idx: 180 |  Loss: (0.8164) |  Loss2: (0.0000) | Acc: (71.00%) (16482/23168)
Epoch: 17 | Batch_idx: 190 |  Loss: (0.8158) |  Loss2: (0.0000) | Acc: (71.00%) (17398/24448)
Epoch: 17 | Batch_idx: 200 |  Loss: (0.8191) |  Loss2: (0.0000) | Acc: (71.00%) (18280/25728)
Epoch: 17 | Batch_idx: 210 |  Loss: (0.8167) |  Loss2: (0.0000) | Acc: (71.00%) (19213/27008)
Epoch: 17 | Batch_idx: 220 |  Loss: (0.8170) |  Loss2: (0.0000) | Acc: (71.00%) (20134/28288)
Epoch: 17 | Batch_idx: 230 |  Loss: (0.8170) |  Loss2: (0.0000) | Acc: (71.00%) (21043/29568)
Epoch: 17 | Batch_idx: 240 |  Loss: (0.8152) |  Loss2: (0.0000) | Acc: (71.00%) (21978/30848)
Epoch: 17 | Batch_idx: 250 |  Loss: (0.8145) |  Loss2: (0.0000) | Acc: (71.00%) (22893/32128)
Epoch: 17 | Batch_idx: 260 |  Loss: (0.8146) |  Loss2: (0.0000) | Acc: (71.00%) (23802/33408)
Epoch: 17 | Batch_idx: 270 |  Loss: (0.8127) |  Loss2: (0.0000) | Acc: (71.00%) (24750/34688)
Epoch: 17 | Batch_idx: 280 |  Loss: (0.8103) |  Loss2: (0.0000) | Acc: (71.00%) (25687/35968)
Epoch: 17 | Batch_idx: 290 |  Loss: (0.8096) |  Loss2: (0.0000) | Acc: (71.00%) (26606/37248)
Epoch: 17 | Batch_idx: 300 |  Loss: (0.8105) |  Loss2: (0.0000) | Acc: (71.00%) (27504/38528)
Epoch: 17 | Batch_idx: 310 |  Loss: (0.8104) |  Loss2: (0.0000) | Acc: (71.00%) (28430/39808)
Epoch: 17 | Batch_idx: 320 |  Loss: (0.8105) |  Loss2: (0.0000) | Acc: (71.00%) (29346/41088)
Epoch: 17 | Batch_idx: 330 |  Loss: (0.8114) |  Loss2: (0.0000) | Acc: (71.00%) (30237/42368)
Epoch: 17 | Batch_idx: 340 |  Loss: (0.8104) |  Loss2: (0.0000) | Acc: (71.00%) (31168/43648)
Epoch: 17 | Batch_idx: 350 |  Loss: (0.8107) |  Loss2: (0.0000) | Acc: (71.00%) (32074/44928)
Epoch: 17 | Batch_idx: 360 |  Loss: (0.8098) |  Loss2: (0.0000) | Acc: (71.00%) (32997/46208)
Epoch: 17 | Batch_idx: 370 |  Loss: (0.8102) |  Loss2: (0.0000) | Acc: (71.00%) (33897/47488)
Epoch: 17 | Batch_idx: 380 |  Loss: (0.8100) |  Loss2: (0.0000) | Acc: (71.00%) (34829/48768)
Epoch: 17 | Batch_idx: 390 |  Loss: (0.8109) |  Loss2: (0.0000) | Acc: (71.00%) (35674/50000)
# TEST : Loss: (0.8386) | Acc: (70.00%) (7048/10000)
percent tensor([0.5151, 0.4849], device='cuda:0')
percent tensor([0.5113, 0.4887], device='cuda:0')
percent tensor([0.4786, 0.5214], device='cuda:0')
percent tensor([0.5373, 0.4627], device='cuda:0')
percent tensor([0.5678, 0.4322], device='cuda:0')
percent tensor([0.5127, 0.4873], device='cuda:0')
percent tensor([0.5249, 0.4751], device='cuda:0')
percent tensor([0.9282, 0.0718], device='cuda:0')
Epoch: 18 | Batch_idx: 0 |  Loss: (0.9898) |  Loss2: (0.0000) | Acc: (64.00%) (82/128)
Epoch: 18 | Batch_idx: 10 |  Loss: (0.7518) |  Loss2: (0.0000) | Acc: (73.00%) (1040/1408)
Epoch: 18 | Batch_idx: 20 |  Loss: (0.7782) |  Loss2: (0.0000) | Acc: (72.00%) (1943/2688)
Epoch: 18 | Batch_idx: 30 |  Loss: (0.7898) |  Loss2: (0.0000) | Acc: (71.00%) (2850/3968)
Epoch: 18 | Batch_idx: 40 |  Loss: (0.7914) |  Loss2: (0.0000) | Acc: (71.00%) (3766/5248)
Epoch: 18 | Batch_idx: 50 |  Loss: (0.7983) |  Loss2: (0.0000) | Acc: (71.00%) (4682/6528)
Epoch: 18 | Batch_idx: 60 |  Loss: (0.8027) |  Loss2: (0.0000) | Acc: (71.00%) (5580/7808)
Epoch: 18 | Batch_idx: 70 |  Loss: (0.8046) |  Loss2: (0.0000) | Acc: (71.00%) (6490/9088)
Epoch: 18 | Batch_idx: 80 |  Loss: (0.8126) |  Loss2: (0.0000) | Acc: (71.00%) (7366/10368)
Epoch: 18 | Batch_idx: 90 |  Loss: (0.8089) |  Loss2: (0.0000) | Acc: (71.00%) (8300/11648)
Epoch: 18 | Batch_idx: 100 |  Loss: (0.8069) |  Loss2: (0.0000) | Acc: (71.00%) (9226/12928)
Epoch: 18 | Batch_idx: 110 |  Loss: (0.8081) |  Loss2: (0.0000) | Acc: (71.00%) (10125/14208)
Epoch: 18 | Batch_idx: 120 |  Loss: (0.8083) |  Loss2: (0.0000) | Acc: (71.00%) (11023/15488)
Epoch: 18 | Batch_idx: 130 |  Loss: (0.8078) |  Loss2: (0.0000) | Acc: (71.00%) (11923/16768)
Epoch: 18 | Batch_idx: 140 |  Loss: (0.8078) |  Loss2: (0.0000) | Acc: (71.00%) (12837/18048)
Epoch: 18 | Batch_idx: 150 |  Loss: (0.8049) |  Loss2: (0.0000) | Acc: (71.00%) (13766/19328)
Epoch: 18 | Batch_idx: 160 |  Loss: (0.8041) |  Loss2: (0.0000) | Acc: (71.00%) (14706/20608)
Epoch: 18 | Batch_idx: 170 |  Loss: (0.8033) |  Loss2: (0.0000) | Acc: (71.00%) (15631/21888)
Epoch: 18 | Batch_idx: 180 |  Loss: (0.8046) |  Loss2: (0.0000) | Acc: (71.00%) (16532/23168)
Epoch: 18 | Batch_idx: 190 |  Loss: (0.8040) |  Loss2: (0.0000) | Acc: (71.00%) (17446/24448)
Epoch: 18 | Batch_idx: 200 |  Loss: (0.8034) |  Loss2: (0.0000) | Acc: (71.00%) (18373/25728)
Epoch: 18 | Batch_idx: 210 |  Loss: (0.8043) |  Loss2: (0.0000) | Acc: (71.00%) (19281/27008)
Epoch: 18 | Batch_idx: 220 |  Loss: (0.8040) |  Loss2: (0.0000) | Acc: (71.00%) (20216/28288)
Epoch: 18 | Batch_idx: 230 |  Loss: (0.8044) |  Loss2: (0.0000) | Acc: (71.00%) (21137/29568)
Epoch: 18 | Batch_idx: 240 |  Loss: (0.8027) |  Loss2: (0.0000) | Acc: (71.00%) (22068/30848)
Epoch: 18 | Batch_idx: 250 |  Loss: (0.8016) |  Loss2: (0.0000) | Acc: (71.00%) (22998/32128)
Epoch: 18 | Batch_idx: 260 |  Loss: (0.8004) |  Loss2: (0.0000) | Acc: (71.00%) (23910/33408)
Epoch: 18 | Batch_idx: 270 |  Loss: (0.7992) |  Loss2: (0.0000) | Acc: (71.00%) (24827/34688)
Epoch: 18 | Batch_idx: 280 |  Loss: (0.8000) |  Loss2: (0.0000) | Acc: (71.00%) (25747/35968)
Epoch: 18 | Batch_idx: 290 |  Loss: (0.7985) |  Loss2: (0.0000) | Acc: (71.00%) (26682/37248)
Epoch: 18 | Batch_idx: 300 |  Loss: (0.7980) |  Loss2: (0.0000) | Acc: (71.00%) (27606/38528)
Epoch: 18 | Batch_idx: 310 |  Loss: (0.7968) |  Loss2: (0.0000) | Acc: (71.00%) (28539/39808)
Epoch: 18 | Batch_idx: 320 |  Loss: (0.7968) |  Loss2: (0.0000) | Acc: (71.00%) (29435/41088)
Epoch: 18 | Batch_idx: 330 |  Loss: (0.7968) |  Loss2: (0.0000) | Acc: (71.00%) (30331/42368)
Epoch: 18 | Batch_idx: 340 |  Loss: (0.7972) |  Loss2: (0.0000) | Acc: (71.00%) (31236/43648)
Epoch: 18 | Batch_idx: 350 |  Loss: (0.7972) |  Loss2: (0.0000) | Acc: (71.00%) (32144/44928)
Epoch: 18 | Batch_idx: 360 |  Loss: (0.7966) |  Loss2: (0.0000) | Acc: (71.00%) (33075/46208)
Epoch: 18 | Batch_idx: 370 |  Loss: (0.7980) |  Loss2: (0.0000) | Acc: (71.00%) (33950/47488)
Epoch: 18 | Batch_idx: 380 |  Loss: (0.7980) |  Loss2: (0.0000) | Acc: (71.00%) (34873/48768)
Epoch: 18 | Batch_idx: 390 |  Loss: (0.7990) |  Loss2: (0.0000) | Acc: (71.00%) (35739/50000)
# TEST : Loss: (0.8336) | Acc: (70.00%) (7073/10000)
percent tensor([0.5173, 0.4827], device='cuda:0')
percent tensor([0.5114, 0.4886], device='cuda:0')
percent tensor([0.4800, 0.5200], device='cuda:0')
percent tensor([0.5412, 0.4588], device='cuda:0')
percent tensor([0.5746, 0.4254], device='cuda:0')
percent tensor([0.5131, 0.4869], device='cuda:0')
percent tensor([0.5290, 0.4710], device='cuda:0')
percent tensor([0.9427, 0.0573], device='cuda:0')
Epoch: 19 | Batch_idx: 0 |  Loss: (0.9866) |  Loss2: (0.0000) | Acc: (63.00%) (81/128)
Epoch: 19 | Batch_idx: 10 |  Loss: (0.7946) |  Loss2: (0.0000) | Acc: (72.00%) (1014/1408)
Epoch: 19 | Batch_idx: 20 |  Loss: (0.7809) |  Loss2: (0.0000) | Acc: (71.00%) (1928/2688)
Epoch: 19 | Batch_idx: 30 |  Loss: (0.7802) |  Loss2: (0.0000) | Acc: (71.00%) (2849/3968)
Epoch: 19 | Batch_idx: 40 |  Loss: (0.7768) |  Loss2: (0.0000) | Acc: (71.00%) (3772/5248)
Epoch: 19 | Batch_idx: 50 |  Loss: (0.7677) |  Loss2: (0.0000) | Acc: (72.00%) (4712/6528)
Epoch: 19 | Batch_idx: 60 |  Loss: (0.7738) |  Loss2: (0.0000) | Acc: (71.00%) (5621/7808)
Epoch: 19 | Batch_idx: 70 |  Loss: (0.7762) |  Loss2: (0.0000) | Acc: (71.00%) (6536/9088)
Epoch: 19 | Batch_idx: 80 |  Loss: (0.7826) |  Loss2: (0.0000) | Acc: (71.00%) (7433/10368)
Epoch: 19 | Batch_idx: 90 |  Loss: (0.7811) |  Loss2: (0.0000) | Acc: (71.00%) (8361/11648)
Epoch: 19 | Batch_idx: 100 |  Loss: (0.7837) |  Loss2: (0.0000) | Acc: (71.00%) (9279/12928)
Epoch: 19 | Batch_idx: 110 |  Loss: (0.7828) |  Loss2: (0.0000) | Acc: (71.00%) (10191/14208)
Epoch: 19 | Batch_idx: 120 |  Loss: (0.7900) |  Loss2: (0.0000) | Acc: (71.00%) (11080/15488)
Epoch: 19 | Batch_idx: 130 |  Loss: (0.7916) |  Loss2: (0.0000) | Acc: (71.00%) (11991/16768)
Epoch: 19 | Batch_idx: 140 |  Loss: (0.7935) |  Loss2: (0.0000) | Acc: (71.00%) (12912/18048)
Epoch: 19 | Batch_idx: 150 |  Loss: (0.7965) |  Loss2: (0.0000) | Acc: (71.00%) (13814/19328)
Epoch: 19 | Batch_idx: 160 |  Loss: (0.7986) |  Loss2: (0.0000) | Acc: (71.00%) (14708/20608)
Epoch: 19 | Batch_idx: 170 |  Loss: (0.7962) |  Loss2: (0.0000) | Acc: (71.00%) (15645/21888)
Epoch: 19 | Batch_idx: 180 |  Loss: (0.7944) |  Loss2: (0.0000) | Acc: (71.00%) (16584/23168)
Epoch: 19 | Batch_idx: 190 |  Loss: (0.7948) |  Loss2: (0.0000) | Acc: (71.00%) (17506/24448)
Epoch: 19 | Batch_idx: 200 |  Loss: (0.7958) |  Loss2: (0.0000) | Acc: (71.00%) (18413/25728)
Epoch: 19 | Batch_idx: 210 |  Loss: (0.7969) |  Loss2: (0.0000) | Acc: (71.00%) (19291/27008)
Epoch: 19 | Batch_idx: 220 |  Loss: (0.7955) |  Loss2: (0.0000) | Acc: (71.00%) (20217/28288)
Epoch: 19 | Batch_idx: 230 |  Loss: (0.7956) |  Loss2: (0.0000) | Acc: (71.00%) (21124/29568)
Epoch: 19 | Batch_idx: 240 |  Loss: (0.7975) |  Loss2: (0.0000) | Acc: (71.00%) (22015/30848)
Epoch: 19 | Batch_idx: 250 |  Loss: (0.7965) |  Loss2: (0.0000) | Acc: (71.00%) (22946/32128)
Epoch: 19 | Batch_idx: 260 |  Loss: (0.7978) |  Loss2: (0.0000) | Acc: (71.00%) (23855/33408)
Epoch: 19 | Batch_idx: 270 |  Loss: (0.7968) |  Loss2: (0.0000) | Acc: (71.00%) (24785/34688)
Epoch: 19 | Batch_idx: 280 |  Loss: (0.7975) |  Loss2: (0.0000) | Acc: (71.00%) (25699/35968)
Epoch: 19 | Batch_idx: 290 |  Loss: (0.7968) |  Loss2: (0.0000) | Acc: (71.00%) (26637/37248)
Epoch: 19 | Batch_idx: 300 |  Loss: (0.7966) |  Loss2: (0.0000) | Acc: (71.00%) (27570/38528)
Epoch: 19 | Batch_idx: 310 |  Loss: (0.7981) |  Loss2: (0.0000) | Acc: (71.00%) (28463/39808)
Epoch: 19 | Batch_idx: 320 |  Loss: (0.8002) |  Loss2: (0.0000) | Acc: (71.00%) (29320/41088)
Epoch: 19 | Batch_idx: 330 |  Loss: (0.7995) |  Loss2: (0.0000) | Acc: (71.00%) (30227/42368)
Epoch: 19 | Batch_idx: 340 |  Loss: (0.7992) |  Loss2: (0.0000) | Acc: (71.00%) (31152/43648)
Epoch: 19 | Batch_idx: 350 |  Loss: (0.7996) |  Loss2: (0.0000) | Acc: (71.00%) (32064/44928)
Epoch: 19 | Batch_idx: 360 |  Loss: (0.8001) |  Loss2: (0.0000) | Acc: (71.00%) (32983/46208)
Epoch: 19 | Batch_idx: 370 |  Loss: (0.7987) |  Loss2: (0.0000) | Acc: (71.00%) (33908/47488)
Epoch: 19 | Batch_idx: 380 |  Loss: (0.7988) |  Loss2: (0.0000) | Acc: (71.00%) (34828/48768)
Epoch: 19 | Batch_idx: 390 |  Loss: (0.7979) |  Loss2: (0.0000) | Acc: (71.00%) (35721/50000)
# TEST : Loss: (0.8276) | Acc: (70.00%) (7097/10000)
percent tensor([0.5202, 0.4798], device='cuda:0')
percent tensor([0.5111, 0.4889], device='cuda:0')
percent tensor([0.4823, 0.5177], device='cuda:0')
percent tensor([0.5418, 0.4582], device='cuda:0')
percent tensor([0.5748, 0.4252], device='cuda:0')
percent tensor([0.5141, 0.4859], device='cuda:0')
percent tensor([0.5348, 0.4652], device='cuda:0')
percent tensor([0.9501, 0.0499], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 20 | Batch_idx: 0 |  Loss: (0.9664) |  Loss2: (0.0000) | Acc: (67.00%) (87/128)
Epoch: 20 | Batch_idx: 10 |  Loss: (0.8039) |  Loss2: (0.0000) | Acc: (70.00%) (997/1408)
Epoch: 20 | Batch_idx: 20 |  Loss: (0.8090) |  Loss2: (0.0000) | Acc: (71.00%) (1919/2688)
Epoch: 20 | Batch_idx: 30 |  Loss: (0.8052) |  Loss2: (0.0000) | Acc: (71.00%) (2830/3968)
Epoch: 20 | Batch_idx: 40 |  Loss: (0.8032) |  Loss2: (0.0000) | Acc: (71.00%) (3766/5248)
Epoch: 20 | Batch_idx: 50 |  Loss: (0.7930) |  Loss2: (0.0000) | Acc: (72.00%) (4728/6528)
Epoch: 20 | Batch_idx: 60 |  Loss: (0.7911) |  Loss2: (0.0000) | Acc: (72.00%) (5653/7808)
Epoch: 20 | Batch_idx: 70 |  Loss: (0.7945) |  Loss2: (0.0000) | Acc: (72.00%) (6556/9088)
Epoch: 20 | Batch_idx: 80 |  Loss: (0.7906) |  Loss2: (0.0000) | Acc: (72.00%) (7488/10368)
Epoch: 20 | Batch_idx: 90 |  Loss: (0.7911) |  Loss2: (0.0000) | Acc: (72.00%) (8410/11648)
Epoch: 20 | Batch_idx: 100 |  Loss: (0.7919) |  Loss2: (0.0000) | Acc: (72.00%) (9330/12928)
Epoch: 20 | Batch_idx: 110 |  Loss: (0.7928) |  Loss2: (0.0000) | Acc: (72.00%) (10251/14208)
Epoch: 20 | Batch_idx: 120 |  Loss: (0.7932) |  Loss2: (0.0000) | Acc: (71.00%) (11146/15488)
Epoch: 20 | Batch_idx: 130 |  Loss: (0.7916) |  Loss2: (0.0000) | Acc: (71.00%) (12072/16768)
Epoch: 20 | Batch_idx: 140 |  Loss: (0.7850) |  Loss2: (0.0000) | Acc: (72.00%) (13033/18048)
Epoch: 20 | Batch_idx: 150 |  Loss: (0.7860) |  Loss2: (0.0000) | Acc: (72.00%) (13943/19328)
Epoch: 20 | Batch_idx: 160 |  Loss: (0.7873) |  Loss2: (0.0000) | Acc: (72.00%) (14876/20608)
Epoch: 20 | Batch_idx: 170 |  Loss: (0.7887) |  Loss2: (0.0000) | Acc: (72.00%) (15787/21888)
Epoch: 20 | Batch_idx: 180 |  Loss: (0.7903) |  Loss2: (0.0000) | Acc: (72.00%) (16705/23168)
Epoch: 20 | Batch_idx: 190 |  Loss: (0.7905) |  Loss2: (0.0000) | Acc: (72.00%) (17624/24448)
Epoch: 20 | Batch_idx: 200 |  Loss: (0.7912) |  Loss2: (0.0000) | Acc: (72.00%) (18554/25728)
Epoch: 20 | Batch_idx: 210 |  Loss: (0.7917) |  Loss2: (0.0000) | Acc: (72.00%) (19482/27008)
Epoch: 20 | Batch_idx: 220 |  Loss: (0.7907) |  Loss2: (0.0000) | Acc: (72.00%) (20419/28288)
Epoch: 20 | Batch_idx: 230 |  Loss: (0.7886) |  Loss2: (0.0000) | Acc: (72.00%) (21375/29568)
Epoch: 20 | Batch_idx: 240 |  Loss: (0.7882) |  Loss2: (0.0000) | Acc: (72.00%) (22302/30848)
Epoch: 20 | Batch_idx: 250 |  Loss: (0.7901) |  Loss2: (0.0000) | Acc: (72.00%) (23209/32128)
Epoch: 20 | Batch_idx: 260 |  Loss: (0.7915) |  Loss2: (0.0000) | Acc: (72.00%) (24104/33408)
Epoch: 20 | Batch_idx: 270 |  Loss: (0.7922) |  Loss2: (0.0000) | Acc: (72.00%) (24997/34688)
Epoch: 20 | Batch_idx: 280 |  Loss: (0.7924) |  Loss2: (0.0000) | Acc: (72.00%) (25911/35968)
Epoch: 20 | Batch_idx: 290 |  Loss: (0.7914) |  Loss2: (0.0000) | Acc: (72.00%) (26833/37248)
Epoch: 20 | Batch_idx: 300 |  Loss: (0.7917) |  Loss2: (0.0000) | Acc: (72.00%) (27747/38528)
Epoch: 20 | Batch_idx: 310 |  Loss: (0.7908) |  Loss2: (0.0000) | Acc: (72.00%) (28692/39808)
Epoch: 20 | Batch_idx: 320 |  Loss: (0.7896) |  Loss2: (0.0000) | Acc: (72.00%) (29630/41088)
Epoch: 20 | Batch_idx: 330 |  Loss: (0.7884) |  Loss2: (0.0000) | Acc: (72.00%) (30579/42368)
Epoch: 20 | Batch_idx: 340 |  Loss: (0.7867) |  Loss2: (0.0000) | Acc: (72.00%) (31529/43648)
Epoch: 20 | Batch_idx: 350 |  Loss: (0.7872) |  Loss2: (0.0000) | Acc: (72.00%) (32456/44928)
Epoch: 20 | Batch_idx: 360 |  Loss: (0.7877) |  Loss2: (0.0000) | Acc: (72.00%) (33370/46208)
Epoch: 20 | Batch_idx: 370 |  Loss: (0.7874) |  Loss2: (0.0000) | Acc: (72.00%) (34296/47488)
Epoch: 20 | Batch_idx: 380 |  Loss: (0.7875) |  Loss2: (0.0000) | Acc: (72.00%) (35225/48768)
Epoch: 20 | Batch_idx: 390 |  Loss: (0.7863) |  Loss2: (0.0000) | Acc: (72.00%) (36136/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_020.pth.tar'
# TEST : Loss: (0.8718) | Acc: (70.00%) (7041/10000)
percent tensor([0.5202, 0.4798], device='cuda:0')
percent tensor([0.5107, 0.4893], device='cuda:0')
percent tensor([0.4818, 0.5182], device='cuda:0')
percent tensor([0.5424, 0.4576], device='cuda:0')
percent tensor([0.5763, 0.4237], device='cuda:0')
percent tensor([0.5143, 0.4857], device='cuda:0')
percent tensor([0.5346, 0.4654], device='cuda:0')
percent tensor([0.9492, 0.0508], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(170.0464, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(781.2100, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(776.6609, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1529.9296, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(506.3695, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2172.0833, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4317.7002, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1434.2269, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6090.0811, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12182.5391, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4055.2380, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(17200.2910, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 21 | Batch_idx: 0 |  Loss: (0.7584) |  Loss2: (0.0000) | Acc: (73.00%) (94/128)
Epoch: 21 | Batch_idx: 10 |  Loss: (0.7366) |  Loss2: (0.0000) | Acc: (73.00%) (1028/1408)
Epoch: 21 | Batch_idx: 20 |  Loss: (0.7222) |  Loss2: (0.0000) | Acc: (74.00%) (1991/2688)
Epoch: 21 | Batch_idx: 30 |  Loss: (0.7383) |  Loss2: (0.0000) | Acc: (73.00%) (2921/3968)
Epoch: 21 | Batch_idx: 40 |  Loss: (0.7545) |  Loss2: (0.0000) | Acc: (72.00%) (3830/5248)
Epoch: 21 | Batch_idx: 50 |  Loss: (0.7461) |  Loss2: (0.0000) | Acc: (73.00%) (4788/6528)
Epoch: 21 | Batch_idx: 60 |  Loss: (0.7403) |  Loss2: (0.0000) | Acc: (73.00%) (5755/7808)
Epoch: 21 | Batch_idx: 70 |  Loss: (0.7429) |  Loss2: (0.0000) | Acc: (73.00%) (6698/9088)
Epoch: 21 | Batch_idx: 80 |  Loss: (0.7450) |  Loss2: (0.0000) | Acc: (73.00%) (7630/10368)
Epoch: 21 | Batch_idx: 90 |  Loss: (0.7424) |  Loss2: (0.0000) | Acc: (73.00%) (8587/11648)
Epoch: 21 | Batch_idx: 100 |  Loss: (0.7423) |  Loss2: (0.0000) | Acc: (73.00%) (9538/12928)
Epoch: 21 | Batch_idx: 110 |  Loss: (0.7432) |  Loss2: (0.0000) | Acc: (73.00%) (10491/14208)
Epoch: 21 | Batch_idx: 120 |  Loss: (0.7454) |  Loss2: (0.0000) | Acc: (73.00%) (11422/15488)
Epoch: 21 | Batch_idx: 130 |  Loss: (0.7495) |  Loss2: (0.0000) | Acc: (73.00%) (12344/16768)
Epoch: 21 | Batch_idx: 140 |  Loss: (0.7486) |  Loss2: (0.0000) | Acc: (73.00%) (13300/18048)
Epoch: 21 | Batch_idx: 150 |  Loss: (0.7485) |  Loss2: (0.0000) | Acc: (73.00%) (14243/19328)
Epoch: 21 | Batch_idx: 160 |  Loss: (0.7459) |  Loss2: (0.0000) | Acc: (73.00%) (15209/20608)
Epoch: 21 | Batch_idx: 170 |  Loss: (0.7453) |  Loss2: (0.0000) | Acc: (73.00%) (16132/21888)
Epoch: 21 | Batch_idx: 180 |  Loss: (0.7441) |  Loss2: (0.0000) | Acc: (73.00%) (17101/23168)
Epoch: 21 | Batch_idx: 190 |  Loss: (0.7457) |  Loss2: (0.0000) | Acc: (73.00%) (18020/24448)
Epoch: 21 | Batch_idx: 200 |  Loss: (0.7464) |  Loss2: (0.0000) | Acc: (73.00%) (18954/25728)
Epoch: 21 | Batch_idx: 210 |  Loss: (0.7478) |  Loss2: (0.0000) | Acc: (73.00%) (19891/27008)
Epoch: 21 | Batch_idx: 220 |  Loss: (0.7496) |  Loss2: (0.0000) | Acc: (73.00%) (20820/28288)
Epoch: 21 | Batch_idx: 230 |  Loss: (0.7489) |  Loss2: (0.0000) | Acc: (73.00%) (21780/29568)
Epoch: 21 | Batch_idx: 240 |  Loss: (0.7485) |  Loss2: (0.0000) | Acc: (73.00%) (22717/30848)
Epoch: 21 | Batch_idx: 250 |  Loss: (0.7466) |  Loss2: (0.0000) | Acc: (73.00%) (23651/32128)
Epoch: 21 | Batch_idx: 260 |  Loss: (0.7467) |  Loss2: (0.0000) | Acc: (73.00%) (24590/33408)
Epoch: 21 | Batch_idx: 270 |  Loss: (0.7485) |  Loss2: (0.0000) | Acc: (73.00%) (25521/34688)
Epoch: 21 | Batch_idx: 280 |  Loss: (0.7487) |  Loss2: (0.0000) | Acc: (73.00%) (26455/35968)
Epoch: 21 | Batch_idx: 290 |  Loss: (0.7481) |  Loss2: (0.0000) | Acc: (73.00%) (27398/37248)
Epoch: 21 | Batch_idx: 300 |  Loss: (0.7473) |  Loss2: (0.0000) | Acc: (73.00%) (28351/38528)
Epoch: 21 | Batch_idx: 310 |  Loss: (0.7471) |  Loss2: (0.0000) | Acc: (73.00%) (29296/39808)
Epoch: 21 | Batch_idx: 320 |  Loss: (0.7456) |  Loss2: (0.0000) | Acc: (73.00%) (30275/41088)
Epoch: 21 | Batch_idx: 330 |  Loss: (0.7453) |  Loss2: (0.0000) | Acc: (73.00%) (31226/42368)
Epoch: 21 | Batch_idx: 340 |  Loss: (0.7451) |  Loss2: (0.0000) | Acc: (73.00%) (32178/43648)
Epoch: 21 | Batch_idx: 350 |  Loss: (0.7445) |  Loss2: (0.0000) | Acc: (73.00%) (33141/44928)
Epoch: 21 | Batch_idx: 360 |  Loss: (0.7438) |  Loss2: (0.0000) | Acc: (73.00%) (34103/46208)
Epoch: 21 | Batch_idx: 370 |  Loss: (0.7430) |  Loss2: (0.0000) | Acc: (73.00%) (35056/47488)
Epoch: 21 | Batch_idx: 380 |  Loss: (0.7418) |  Loss2: (0.0000) | Acc: (73.00%) (36032/48768)
Epoch: 21 | Batch_idx: 390 |  Loss: (0.7402) |  Loss2: (0.0000) | Acc: (73.00%) (36969/50000)
# TEST : Loss: (0.8035) | Acc: (72.00%) (7216/10000)
percent tensor([0.5201, 0.4799], device='cuda:0')
percent tensor([0.5107, 0.4893], device='cuda:0')
percent tensor([0.4824, 0.5176], device='cuda:0')
percent tensor([0.5420, 0.4580], device='cuda:0')
percent tensor([0.5761, 0.4239], device='cuda:0')
percent tensor([0.5139, 0.4861], device='cuda:0')
percent tensor([0.5350, 0.4650], device='cuda:0')
percent tensor([0.9519, 0.0481], device='cuda:0')
Epoch: 22 | Batch_idx: 0 |  Loss: (0.6565) |  Loss2: (0.0000) | Acc: (76.00%) (98/128)
Epoch: 22 | Batch_idx: 10 |  Loss: (0.6683) |  Loss2: (0.0000) | Acc: (76.00%) (1083/1408)
Epoch: 22 | Batch_idx: 20 |  Loss: (0.7014) |  Loss2: (0.0000) | Acc: (75.00%) (2031/2688)
Epoch: 22 | Batch_idx: 30 |  Loss: (0.6920) |  Loss2: (0.0000) | Acc: (75.00%) (3010/3968)
Epoch: 22 | Batch_idx: 40 |  Loss: (0.6935) |  Loss2: (0.0000) | Acc: (75.00%) (3964/5248)
Epoch: 22 | Batch_idx: 50 |  Loss: (0.6908) |  Loss2: (0.0000) | Acc: (75.00%) (4952/6528)
Epoch: 22 | Batch_idx: 60 |  Loss: (0.6928) |  Loss2: (0.0000) | Acc: (75.00%) (5925/7808)
Epoch: 22 | Batch_idx: 70 |  Loss: (0.6881) |  Loss2: (0.0000) | Acc: (76.00%) (6920/9088)
Epoch: 22 | Batch_idx: 80 |  Loss: (0.6918) |  Loss2: (0.0000) | Acc: (76.00%) (7887/10368)
Epoch: 22 | Batch_idx: 90 |  Loss: (0.6992) |  Loss2: (0.0000) | Acc: (75.00%) (8814/11648)
Epoch: 22 | Batch_idx: 100 |  Loss: (0.6967) |  Loss2: (0.0000) | Acc: (75.00%) (9802/12928)
Epoch: 22 | Batch_idx: 110 |  Loss: (0.6975) |  Loss2: (0.0000) | Acc: (75.00%) (10761/14208)
Epoch: 22 | Batch_idx: 120 |  Loss: (0.6968) |  Loss2: (0.0000) | Acc: (75.00%) (11737/15488)
Epoch: 22 | Batch_idx: 130 |  Loss: (0.6985) |  Loss2: (0.0000) | Acc: (75.00%) (12686/16768)
Epoch: 22 | Batch_idx: 140 |  Loss: (0.7016) |  Loss2: (0.0000) | Acc: (75.00%) (13642/18048)
Epoch: 22 | Batch_idx: 150 |  Loss: (0.7014) |  Loss2: (0.0000) | Acc: (75.00%) (14632/19328)
Epoch: 22 | Batch_idx: 160 |  Loss: (0.7017) |  Loss2: (0.0000) | Acc: (75.00%) (15596/20608)
Epoch: 22 | Batch_idx: 170 |  Loss: (0.7060) |  Loss2: (0.0000) | Acc: (75.00%) (16529/21888)
Epoch: 22 | Batch_idx: 180 |  Loss: (0.7044) |  Loss2: (0.0000) | Acc: (75.00%) (17500/23168)
Epoch: 22 | Batch_idx: 190 |  Loss: (0.7030) |  Loss2: (0.0000) | Acc: (75.00%) (18481/24448)
Epoch: 22 | Batch_idx: 200 |  Loss: (0.7028) |  Loss2: (0.0000) | Acc: (75.00%) (19433/25728)
Epoch: 22 | Batch_idx: 210 |  Loss: (0.7027) |  Loss2: (0.0000) | Acc: (75.00%) (20384/27008)
Epoch: 22 | Batch_idx: 220 |  Loss: (0.7028) |  Loss2: (0.0000) | Acc: (75.00%) (21339/28288)
Epoch: 22 | Batch_idx: 230 |  Loss: (0.7014) |  Loss2: (0.0000) | Acc: (75.00%) (22317/29568)
Epoch: 22 | Batch_idx: 240 |  Loss: (0.7006) |  Loss2: (0.0000) | Acc: (75.00%) (23283/30848)
Epoch: 22 | Batch_idx: 250 |  Loss: (0.6994) |  Loss2: (0.0000) | Acc: (75.00%) (24254/32128)
Epoch: 22 | Batch_idx: 260 |  Loss: (0.6982) |  Loss2: (0.0000) | Acc: (75.00%) (25243/33408)
Epoch: 22 | Batch_idx: 270 |  Loss: (0.6984) |  Loss2: (0.0000) | Acc: (75.00%) (26219/34688)
Epoch: 22 | Batch_idx: 280 |  Loss: (0.6986) |  Loss2: (0.0000) | Acc: (75.00%) (27191/35968)
Epoch: 22 | Batch_idx: 290 |  Loss: (0.7000) |  Loss2: (0.0000) | Acc: (75.00%) (28148/37248)
Epoch: 22 | Batch_idx: 300 |  Loss: (0.6994) |  Loss2: (0.0000) | Acc: (75.00%) (29121/38528)
Epoch: 22 | Batch_idx: 310 |  Loss: (0.6992) |  Loss2: (0.0000) | Acc: (75.00%) (30103/39808)
Epoch: 22 | Batch_idx: 320 |  Loss: (0.6995) |  Loss2: (0.0000) | Acc: (75.00%) (31061/41088)
Epoch: 22 | Batch_idx: 330 |  Loss: (0.6995) |  Loss2: (0.0000) | Acc: (75.00%) (32024/42368)
Epoch: 22 | Batch_idx: 340 |  Loss: (0.6997) |  Loss2: (0.0000) | Acc: (75.00%) (32988/43648)
Epoch: 22 | Batch_idx: 350 |  Loss: (0.6988) |  Loss2: (0.0000) | Acc: (75.00%) (33946/44928)
Epoch: 22 | Batch_idx: 360 |  Loss: (0.6975) |  Loss2: (0.0000) | Acc: (75.00%) (34932/46208)
Epoch: 22 | Batch_idx: 370 |  Loss: (0.6964) |  Loss2: (0.0000) | Acc: (75.00%) (35923/47488)
Epoch: 22 | Batch_idx: 380 |  Loss: (0.6954) |  Loss2: (0.0000) | Acc: (75.00%) (36908/48768)
Epoch: 22 | Batch_idx: 390 |  Loss: (0.6943) |  Loss2: (0.0000) | Acc: (75.00%) (37840/50000)
# TEST : Loss: (0.7787) | Acc: (73.00%) (7314/10000)
percent tensor([0.5199, 0.4801], device='cuda:0')
percent tensor([0.5109, 0.4891], device='cuda:0')
percent tensor([0.4814, 0.5186], device='cuda:0')
percent tensor([0.5425, 0.4575], device='cuda:0')
percent tensor([0.5778, 0.4222], device='cuda:0')
percent tensor([0.5151, 0.4849], device='cuda:0')
percent tensor([0.5371, 0.4629], device='cuda:0')
percent tensor([0.9601, 0.0399], device='cuda:0')
Epoch: 23 | Batch_idx: 0 |  Loss: (0.4998) |  Loss2: (0.0000) | Acc: (82.00%) (105/128)
Epoch: 23 | Batch_idx: 10 |  Loss: (0.6052) |  Loss2: (0.0000) | Acc: (78.00%) (1101/1408)
Epoch: 23 | Batch_idx: 20 |  Loss: (0.6387) |  Loss2: (0.0000) | Acc: (77.00%) (2089/2688)
Epoch: 23 | Batch_idx: 30 |  Loss: (0.6345) |  Loss2: (0.0000) | Acc: (77.00%) (3080/3968)
Epoch: 23 | Batch_idx: 40 |  Loss: (0.6425) |  Loss2: (0.0000) | Acc: (77.00%) (4063/5248)
Epoch: 23 | Batch_idx: 50 |  Loss: (0.6421) |  Loss2: (0.0000) | Acc: (77.00%) (5056/6528)
Epoch: 23 | Batch_idx: 60 |  Loss: (0.6399) |  Loss2: (0.0000) | Acc: (77.00%) (6055/7808)
Epoch: 23 | Batch_idx: 70 |  Loss: (0.6446) |  Loss2: (0.0000) | Acc: (77.00%) (7034/9088)
Epoch: 23 | Batch_idx: 80 |  Loss: (0.6427) |  Loss2: (0.0000) | Acc: (77.00%) (8039/10368)
Epoch: 23 | Batch_idx: 90 |  Loss: (0.6460) |  Loss2: (0.0000) | Acc: (77.00%) (9006/11648)
Epoch: 23 | Batch_idx: 100 |  Loss: (0.6459) |  Loss2: (0.0000) | Acc: (77.00%) (10009/12928)
Epoch: 23 | Batch_idx: 110 |  Loss: (0.6472) |  Loss2: (0.0000) | Acc: (77.00%) (10995/14208)
Epoch: 23 | Batch_idx: 120 |  Loss: (0.6476) |  Loss2: (0.0000) | Acc: (77.00%) (11986/15488)
Epoch: 23 | Batch_idx: 130 |  Loss: (0.6498) |  Loss2: (0.0000) | Acc: (77.00%) (12953/16768)
Epoch: 23 | Batch_idx: 140 |  Loss: (0.6550) |  Loss2: (0.0000) | Acc: (77.00%) (13919/18048)
Epoch: 23 | Batch_idx: 150 |  Loss: (0.6549) |  Loss2: (0.0000) | Acc: (77.00%) (14906/19328)
Epoch: 23 | Batch_idx: 160 |  Loss: (0.6550) |  Loss2: (0.0000) | Acc: (77.00%) (15891/20608)
Epoch: 23 | Batch_idx: 170 |  Loss: (0.6540) |  Loss2: (0.0000) | Acc: (77.00%) (16881/21888)
Epoch: 23 | Batch_idx: 180 |  Loss: (0.6542) |  Loss2: (0.0000) | Acc: (77.00%) (17858/23168)
Epoch: 23 | Batch_idx: 190 |  Loss: (0.6550) |  Loss2: (0.0000) | Acc: (77.00%) (18850/24448)
Epoch: 23 | Batch_idx: 200 |  Loss: (0.6562) |  Loss2: (0.0000) | Acc: (77.00%) (19832/25728)
Epoch: 23 | Batch_idx: 210 |  Loss: (0.6562) |  Loss2: (0.0000) | Acc: (77.00%) (20821/27008)
Epoch: 23 | Batch_idx: 220 |  Loss: (0.6545) |  Loss2: (0.0000) | Acc: (77.00%) (21826/28288)
Epoch: 23 | Batch_idx: 230 |  Loss: (0.6530) |  Loss2: (0.0000) | Acc: (77.00%) (22842/29568)
Epoch: 23 | Batch_idx: 240 |  Loss: (0.6542) |  Loss2: (0.0000) | Acc: (77.00%) (23807/30848)
Epoch: 23 | Batch_idx: 250 |  Loss: (0.6549) |  Loss2: (0.0000) | Acc: (77.00%) (24786/32128)
Epoch: 23 | Batch_idx: 260 |  Loss: (0.6541) |  Loss2: (0.0000) | Acc: (77.00%) (25784/33408)
Epoch: 23 | Batch_idx: 270 |  Loss: (0.6530) |  Loss2: (0.0000) | Acc: (77.00%) (26787/34688)
Epoch: 23 | Batch_idx: 280 |  Loss: (0.6528) |  Loss2: (0.0000) | Acc: (77.00%) (27774/35968)
Epoch: 23 | Batch_idx: 290 |  Loss: (0.6516) |  Loss2: (0.0000) | Acc: (77.00%) (28783/37248)
Epoch: 23 | Batch_idx: 300 |  Loss: (0.6532) |  Loss2: (0.0000) | Acc: (77.00%) (29762/38528)
Epoch: 23 | Batch_idx: 310 |  Loss: (0.6534) |  Loss2: (0.0000) | Acc: (77.00%) (30753/39808)
Epoch: 23 | Batch_idx: 320 |  Loss: (0.6527) |  Loss2: (0.0000) | Acc: (77.00%) (31754/41088)
Epoch: 23 | Batch_idx: 330 |  Loss: (0.6527) |  Loss2: (0.0000) | Acc: (77.00%) (32739/42368)
Epoch: 23 | Batch_idx: 340 |  Loss: (0.6534) |  Loss2: (0.0000) | Acc: (77.00%) (33713/43648)
Epoch: 23 | Batch_idx: 350 |  Loss: (0.6546) |  Loss2: (0.0000) | Acc: (77.00%) (34695/44928)
Epoch: 23 | Batch_idx: 360 |  Loss: (0.6553) |  Loss2: (0.0000) | Acc: (77.00%) (35666/46208)
Epoch: 23 | Batch_idx: 370 |  Loss: (0.6546) |  Loss2: (0.0000) | Acc: (77.00%) (36668/47488)
Epoch: 23 | Batch_idx: 380 |  Loss: (0.6536) |  Loss2: (0.0000) | Acc: (77.00%) (37690/48768)
Epoch: 23 | Batch_idx: 390 |  Loss: (0.6539) |  Loss2: (0.0000) | Acc: (77.00%) (38645/50000)
# TEST : Loss: (0.7714) | Acc: (74.00%) (7431/10000)
percent tensor([0.5196, 0.4804], device='cuda:0')
percent tensor([0.5110, 0.4890], device='cuda:0')
percent tensor([0.4823, 0.5177], device='cuda:0')
percent tensor([0.5425, 0.4575], device='cuda:0')
percent tensor([0.5749, 0.4251], device='cuda:0')
percent tensor([0.5146, 0.4854], device='cuda:0')
percent tensor([0.5372, 0.4628], device='cuda:0')
percent tensor([0.9588, 0.0412], device='cuda:0')
Epoch: 24 | Batch_idx: 0 |  Loss: (0.6985) |  Loss2: (0.0000) | Acc: (77.00%) (99/128)
Epoch: 24 | Batch_idx: 10 |  Loss: (0.6215) |  Loss2: (0.0000) | Acc: (78.00%) (1110/1408)
Epoch: 24 | Batch_idx: 20 |  Loss: (0.6223) |  Loss2: (0.0000) | Acc: (78.00%) (2119/2688)
Epoch: 24 | Batch_idx: 30 |  Loss: (0.6277) |  Loss2: (0.0000) | Acc: (78.00%) (3120/3968)
Epoch: 24 | Batch_idx: 40 |  Loss: (0.6243) |  Loss2: (0.0000) | Acc: (78.00%) (4136/5248)
Epoch: 24 | Batch_idx: 50 |  Loss: (0.6173) |  Loss2: (0.0000) | Acc: (79.00%) (5159/6528)
Epoch: 24 | Batch_idx: 60 |  Loss: (0.6233) |  Loss2: (0.0000) | Acc: (78.00%) (6149/7808)
Epoch: 24 | Batch_idx: 70 |  Loss: (0.6278) |  Loss2: (0.0000) | Acc: (78.00%) (7140/9088)
Epoch: 24 | Batch_idx: 80 |  Loss: (0.6327) |  Loss2: (0.0000) | Acc: (78.00%) (8126/10368)
Epoch: 24 | Batch_idx: 90 |  Loss: (0.6291) |  Loss2: (0.0000) | Acc: (78.00%) (9145/11648)
Epoch: 24 | Batch_idx: 100 |  Loss: (0.6293) |  Loss2: (0.0000) | Acc: (78.00%) (10151/12928)
Epoch: 24 | Batch_idx: 110 |  Loss: (0.6248) |  Loss2: (0.0000) | Acc: (78.00%) (11170/14208)
Epoch: 24 | Batch_idx: 120 |  Loss: (0.6271) |  Loss2: (0.0000) | Acc: (78.00%) (12152/15488)
Epoch: 24 | Batch_idx: 130 |  Loss: (0.6255) |  Loss2: (0.0000) | Acc: (78.00%) (13149/16768)
Epoch: 24 | Batch_idx: 140 |  Loss: (0.6258) |  Loss2: (0.0000) | Acc: (78.00%) (14157/18048)
Epoch: 24 | Batch_idx: 150 |  Loss: (0.6271) |  Loss2: (0.0000) | Acc: (78.00%) (15151/19328)
Epoch: 24 | Batch_idx: 160 |  Loss: (0.6262) |  Loss2: (0.0000) | Acc: (78.00%) (16153/20608)
Epoch: 24 | Batch_idx: 170 |  Loss: (0.6262) |  Loss2: (0.0000) | Acc: (78.00%) (17149/21888)
Epoch: 24 | Batch_idx: 180 |  Loss: (0.6245) |  Loss2: (0.0000) | Acc: (78.00%) (18175/23168)
Epoch: 24 | Batch_idx: 190 |  Loss: (0.6242) |  Loss2: (0.0000) | Acc: (78.00%) (19179/24448)
Epoch: 24 | Batch_idx: 200 |  Loss: (0.6235) |  Loss2: (0.0000) | Acc: (78.00%) (20189/25728)
Epoch: 24 | Batch_idx: 210 |  Loss: (0.6200) |  Loss2: (0.0000) | Acc: (78.00%) (21234/27008)
Epoch: 24 | Batch_idx: 220 |  Loss: (0.6199) |  Loss2: (0.0000) | Acc: (78.00%) (22227/28288)
Epoch: 24 | Batch_idx: 230 |  Loss: (0.6199) |  Loss2: (0.0000) | Acc: (78.00%) (23232/29568)
Epoch: 24 | Batch_idx: 240 |  Loss: (0.6215) |  Loss2: (0.0000) | Acc: (78.00%) (24222/30848)
Epoch: 24 | Batch_idx: 250 |  Loss: (0.6214) |  Loss2: (0.0000) | Acc: (78.00%) (25221/32128)
Epoch: 24 | Batch_idx: 260 |  Loss: (0.6217) |  Loss2: (0.0000) | Acc: (78.00%) (26225/33408)
Epoch: 24 | Batch_idx: 270 |  Loss: (0.6211) |  Loss2: (0.0000) | Acc: (78.00%) (27244/34688)
Epoch: 24 | Batch_idx: 280 |  Loss: (0.6210) |  Loss2: (0.0000) | Acc: (78.00%) (28237/35968)
Epoch: 24 | Batch_idx: 290 |  Loss: (0.6213) |  Loss2: (0.0000) | Acc: (78.00%) (29235/37248)
Epoch: 24 | Batch_idx: 300 |  Loss: (0.6201) |  Loss2: (0.0000) | Acc: (78.00%) (30254/38528)
Epoch: 24 | Batch_idx: 310 |  Loss: (0.6195) |  Loss2: (0.0000) | Acc: (78.00%) (31262/39808)
Epoch: 24 | Batch_idx: 320 |  Loss: (0.6201) |  Loss2: (0.0000) | Acc: (78.00%) (32264/41088)
Epoch: 24 | Batch_idx: 330 |  Loss: (0.6192) |  Loss2: (0.0000) | Acc: (78.00%) (33291/42368)
Epoch: 24 | Batch_idx: 340 |  Loss: (0.6205) |  Loss2: (0.0000) | Acc: (78.00%) (34265/43648)
Epoch: 24 | Batch_idx: 350 |  Loss: (0.6208) |  Loss2: (0.0000) | Acc: (78.00%) (35268/44928)
Epoch: 24 | Batch_idx: 360 |  Loss: (0.6204) |  Loss2: (0.0000) | Acc: (78.00%) (36291/46208)
Epoch: 24 | Batch_idx: 370 |  Loss: (0.6210) |  Loss2: (0.0000) | Acc: (78.00%) (37288/47488)
Epoch: 24 | Batch_idx: 380 |  Loss: (0.6205) |  Loss2: (0.0000) | Acc: (78.00%) (38301/48768)
Epoch: 24 | Batch_idx: 390 |  Loss: (0.6209) |  Loss2: (0.0000) | Acc: (78.00%) (39260/50000)
# TEST : Loss: (0.7882) | Acc: (73.00%) (7355/10000)
percent tensor([0.5193, 0.4807], device='cuda:0')
percent tensor([0.5106, 0.4894], device='cuda:0')
percent tensor([0.4820, 0.5180], device='cuda:0')
percent tensor([0.5418, 0.4582], device='cuda:0')
percent tensor([0.5724, 0.4276], device='cuda:0')
percent tensor([0.5140, 0.4860], device='cuda:0')
percent tensor([0.5351, 0.4649], device='cuda:0')
percent tensor([0.9552, 0.0448], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 25 | Batch_idx: 0 |  Loss: (0.5369) |  Loss2: (0.0000) | Acc: (80.00%) (103/128)
Epoch: 25 | Batch_idx: 10 |  Loss: (0.6326) |  Loss2: (0.0000) | Acc: (77.00%) (1098/1408)
Epoch: 25 | Batch_idx: 20 |  Loss: (0.6712) |  Loss2: (0.0000) | Acc: (76.00%) (2057/2688)
Epoch: 25 | Batch_idx: 30 |  Loss: (0.6960) |  Loss2: (0.0000) | Acc: (75.00%) (2999/3968)
Epoch: 25 | Batch_idx: 40 |  Loss: (0.7193) |  Loss2: (0.0000) | Acc: (74.00%) (3927/5248)
Epoch: 25 | Batch_idx: 50 |  Loss: (0.7286) |  Loss2: (0.0000) | Acc: (74.00%) (4875/6528)
Epoch: 25 | Batch_idx: 60 |  Loss: (0.7269) |  Loss2: (0.0000) | Acc: (74.00%) (5841/7808)
Epoch: 25 | Batch_idx: 70 |  Loss: (0.7367) |  Loss2: (0.0000) | Acc: (74.00%) (6778/9088)
Epoch: 25 | Batch_idx: 80 |  Loss: (0.7374) |  Loss2: (0.0000) | Acc: (74.00%) (7745/10368)
Epoch: 25 | Batch_idx: 90 |  Loss: (0.7317) |  Loss2: (0.0000) | Acc: (74.00%) (8726/11648)
Epoch: 25 | Batch_idx: 100 |  Loss: (0.7318) |  Loss2: (0.0000) | Acc: (74.00%) (9672/12928)
Epoch: 25 | Batch_idx: 110 |  Loss: (0.7305) |  Loss2: (0.0000) | Acc: (74.00%) (10627/14208)
Epoch: 25 | Batch_idx: 120 |  Loss: (0.7275) |  Loss2: (0.0000) | Acc: (74.00%) (11602/15488)
Epoch: 25 | Batch_idx: 130 |  Loss: (0.7225) |  Loss2: (0.0000) | Acc: (75.00%) (12588/16768)
Epoch: 25 | Batch_idx: 140 |  Loss: (0.7200) |  Loss2: (0.0000) | Acc: (75.00%) (13560/18048)
Epoch: 25 | Batch_idx: 150 |  Loss: (0.7183) |  Loss2: (0.0000) | Acc: (75.00%) (14511/19328)
Epoch: 25 | Batch_idx: 160 |  Loss: (0.7169) |  Loss2: (0.0000) | Acc: (75.00%) (15473/20608)
Epoch: 25 | Batch_idx: 170 |  Loss: (0.7138) |  Loss2: (0.0000) | Acc: (75.00%) (16458/21888)
Epoch: 25 | Batch_idx: 180 |  Loss: (0.7105) |  Loss2: (0.0000) | Acc: (75.00%) (17447/23168)
Epoch: 25 | Batch_idx: 190 |  Loss: (0.7085) |  Loss2: (0.0000) | Acc: (75.00%) (18429/24448)
Epoch: 25 | Batch_idx: 200 |  Loss: (0.7038) |  Loss2: (0.0000) | Acc: (75.00%) (19448/25728)
Epoch: 25 | Batch_idx: 210 |  Loss: (0.7020) |  Loss2: (0.0000) | Acc: (75.00%) (20431/27008)
Epoch: 25 | Batch_idx: 220 |  Loss: (0.7006) |  Loss2: (0.0000) | Acc: (75.00%) (21402/28288)
Epoch: 25 | Batch_idx: 230 |  Loss: (0.6997) |  Loss2: (0.0000) | Acc: (75.00%) (22354/29568)
Epoch: 25 | Batch_idx: 240 |  Loss: (0.6996) |  Loss2: (0.0000) | Acc: (75.00%) (23311/30848)
Epoch: 25 | Batch_idx: 250 |  Loss: (0.6986) |  Loss2: (0.0000) | Acc: (75.00%) (24289/32128)
Epoch: 25 | Batch_idx: 260 |  Loss: (0.6972) |  Loss2: (0.0000) | Acc: (75.00%) (25266/33408)
Epoch: 25 | Batch_idx: 270 |  Loss: (0.6968) |  Loss2: (0.0000) | Acc: (75.00%) (26248/34688)
Epoch: 25 | Batch_idx: 280 |  Loss: (0.6969) |  Loss2: (0.0000) | Acc: (75.00%) (27221/35968)
Epoch: 25 | Batch_idx: 290 |  Loss: (0.6956) |  Loss2: (0.0000) | Acc: (75.00%) (28216/37248)
Epoch: 25 | Batch_idx: 300 |  Loss: (0.6924) |  Loss2: (0.0000) | Acc: (75.00%) (29228/38528)
Epoch: 25 | Batch_idx: 310 |  Loss: (0.6923) |  Loss2: (0.0000) | Acc: (75.00%) (30179/39808)
Epoch: 25 | Batch_idx: 320 |  Loss: (0.6914) |  Loss2: (0.0000) | Acc: (75.00%) (31177/41088)
Epoch: 25 | Batch_idx: 330 |  Loss: (0.6895) |  Loss2: (0.0000) | Acc: (75.00%) (32171/42368)
Epoch: 25 | Batch_idx: 340 |  Loss: (0.6898) |  Loss2: (0.0000) | Acc: (75.00%) (33135/43648)
Epoch: 25 | Batch_idx: 350 |  Loss: (0.6892) |  Loss2: (0.0000) | Acc: (75.00%) (34098/44928)
Epoch: 25 | Batch_idx: 360 |  Loss: (0.6889) |  Loss2: (0.0000) | Acc: (75.00%) (35065/46208)
Epoch: 25 | Batch_idx: 370 |  Loss: (0.6887) |  Loss2: (0.0000) | Acc: (75.00%) (36049/47488)
Epoch: 25 | Batch_idx: 380 |  Loss: (0.6870) |  Loss2: (0.0000) | Acc: (75.00%) (37047/48768)
Epoch: 25 | Batch_idx: 390 |  Loss: (0.6873) |  Loss2: (0.0000) | Acc: (75.00%) (37977/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_025.pth.tar'
# TEST : Loss: (0.6894) | Acc: (76.00%) (7623/10000)
percent tensor([0.5191, 0.4809], device='cuda:0')
percent tensor([0.5137, 0.4863], device='cuda:0')
percent tensor([0.4877, 0.5123], device='cuda:0')
percent tensor([0.5885, 0.4115], device='cuda:0')
percent tensor([0.5448, 0.4552], device='cuda:0')
percent tensor([0.5279, 0.4721], device='cuda:0')
percent tensor([0.5389, 0.4611], device='cuda:0')
percent tensor([0.9710, 0.0290], device='cuda:0')
Epoch: 26 | Batch_idx: 0 |  Loss: (0.5585) |  Loss2: (0.0000) | Acc: (82.00%) (105/128)
Epoch: 26 | Batch_idx: 10 |  Loss: (0.5655) |  Loss2: (0.0000) | Acc: (80.00%) (1130/1408)
Epoch: 26 | Batch_idx: 20 |  Loss: (0.5861) |  Loss2: (0.0000) | Acc: (80.00%) (2153/2688)
Epoch: 26 | Batch_idx: 30 |  Loss: (0.5963) |  Loss2: (0.0000) | Acc: (79.00%) (3147/3968)
Epoch: 26 | Batch_idx: 40 |  Loss: (0.6093) |  Loss2: (0.0000) | Acc: (78.00%) (4137/5248)
Epoch: 26 | Batch_idx: 50 |  Loss: (0.6221) |  Loss2: (0.0000) | Acc: (78.00%) (5109/6528)
Epoch: 26 | Batch_idx: 60 |  Loss: (0.6228) |  Loss2: (0.0000) | Acc: (78.00%) (6110/7808)
Epoch: 26 | Batch_idx: 70 |  Loss: (0.6249) |  Loss2: (0.0000) | Acc: (78.00%) (7106/9088)
Epoch: 26 | Batch_idx: 80 |  Loss: (0.6254) |  Loss2: (0.0000) | Acc: (78.00%) (8089/10368)
Epoch: 26 | Batch_idx: 90 |  Loss: (0.6302) |  Loss2: (0.0000) | Acc: (77.00%) (9078/11648)
Epoch: 26 | Batch_idx: 100 |  Loss: (0.6308) |  Loss2: (0.0000) | Acc: (77.00%) (10072/12928)
Epoch: 26 | Batch_idx: 110 |  Loss: (0.6309) |  Loss2: (0.0000) | Acc: (77.00%) (11066/14208)
Epoch: 26 | Batch_idx: 120 |  Loss: (0.6337) |  Loss2: (0.0000) | Acc: (77.00%) (12060/15488)
Epoch: 26 | Batch_idx: 130 |  Loss: (0.6321) |  Loss2: (0.0000) | Acc: (77.00%) (13071/16768)
Epoch: 26 | Batch_idx: 140 |  Loss: (0.6315) |  Loss2: (0.0000) | Acc: (77.00%) (14060/18048)
Epoch: 26 | Batch_idx: 150 |  Loss: (0.6334) |  Loss2: (0.0000) | Acc: (77.00%) (15030/19328)
Epoch: 26 | Batch_idx: 160 |  Loss: (0.6335) |  Loss2: (0.0000) | Acc: (77.00%) (16027/20608)
Epoch: 26 | Batch_idx: 170 |  Loss: (0.6338) |  Loss2: (0.0000) | Acc: (77.00%) (17012/21888)
Epoch: 26 | Batch_idx: 180 |  Loss: (0.6321) |  Loss2: (0.0000) | Acc: (77.00%) (18031/23168)
Epoch: 26 | Batch_idx: 190 |  Loss: (0.6310) |  Loss2: (0.0000) | Acc: (77.00%) (19032/24448)
Epoch: 26 | Batch_idx: 200 |  Loss: (0.6307) |  Loss2: (0.0000) | Acc: (77.00%) (20022/25728)
Epoch: 26 | Batch_idx: 210 |  Loss: (0.6296) |  Loss2: (0.0000) | Acc: (77.00%) (21020/27008)
Epoch: 26 | Batch_idx: 220 |  Loss: (0.6294) |  Loss2: (0.0000) | Acc: (77.00%) (22030/28288)
Epoch: 26 | Batch_idx: 230 |  Loss: (0.6296) |  Loss2: (0.0000) | Acc: (77.00%) (23019/29568)
Epoch: 26 | Batch_idx: 240 |  Loss: (0.6291) |  Loss2: (0.0000) | Acc: (77.00%) (24037/30848)
Epoch: 26 | Batch_idx: 250 |  Loss: (0.6292) |  Loss2: (0.0000) | Acc: (77.00%) (25032/32128)
Epoch: 26 | Batch_idx: 260 |  Loss: (0.6300) |  Loss2: (0.0000) | Acc: (77.00%) (26015/33408)
Epoch: 26 | Batch_idx: 270 |  Loss: (0.6304) |  Loss2: (0.0000) | Acc: (77.00%) (27016/34688)
Epoch: 26 | Batch_idx: 280 |  Loss: (0.6311) |  Loss2: (0.0000) | Acc: (77.00%) (28008/35968)
Epoch: 26 | Batch_idx: 290 |  Loss: (0.6316) |  Loss2: (0.0000) | Acc: (77.00%) (28993/37248)
Epoch: 26 | Batch_idx: 300 |  Loss: (0.6315) |  Loss2: (0.0000) | Acc: (77.00%) (29992/38528)
Epoch: 26 | Batch_idx: 310 |  Loss: (0.6308) |  Loss2: (0.0000) | Acc: (77.00%) (31010/39808)
Epoch: 26 | Batch_idx: 320 |  Loss: (0.6308) |  Loss2: (0.0000) | Acc: (77.00%) (32009/41088)
Epoch: 26 | Batch_idx: 330 |  Loss: (0.6304) |  Loss2: (0.0000) | Acc: (77.00%) (33028/42368)
Epoch: 26 | Batch_idx: 340 |  Loss: (0.6299) |  Loss2: (0.0000) | Acc: (77.00%) (34030/43648)
Epoch: 26 | Batch_idx: 350 |  Loss: (0.6300) |  Loss2: (0.0000) | Acc: (77.00%) (35032/44928)
Epoch: 26 | Batch_idx: 360 |  Loss: (0.6292) |  Loss2: (0.0000) | Acc: (78.00%) (36045/46208)
Epoch: 26 | Batch_idx: 370 |  Loss: (0.6289) |  Loss2: (0.0000) | Acc: (78.00%) (37048/47488)
Epoch: 26 | Batch_idx: 380 |  Loss: (0.6296) |  Loss2: (0.0000) | Acc: (77.00%) (38025/48768)
Epoch: 26 | Batch_idx: 390 |  Loss: (0.6288) |  Loss2: (0.0000) | Acc: (77.00%) (38993/50000)
# TEST : Loss: (0.6618) | Acc: (77.00%) (7714/10000)
percent tensor([0.5207, 0.4793], device='cuda:0')
percent tensor([0.5182, 0.4818], device='cuda:0')
percent tensor([0.4906, 0.5094], device='cuda:0')
percent tensor([0.5889, 0.4111], device='cuda:0')
percent tensor([0.5401, 0.4599], device='cuda:0')
percent tensor([0.5390, 0.4610], device='cuda:0')
percent tensor([0.5471, 0.4529], device='cuda:0')
percent tensor([0.9834, 0.0166], device='cuda:0')
Epoch: 27 | Batch_idx: 0 |  Loss: (0.5435) |  Loss2: (0.0000) | Acc: (81.00%) (104/128)
Epoch: 27 | Batch_idx: 10 |  Loss: (0.6320) |  Loss2: (0.0000) | Acc: (77.00%) (1092/1408)
Epoch: 27 | Batch_idx: 20 |  Loss: (0.6379) |  Loss2: (0.0000) | Acc: (77.00%) (2078/2688)
Epoch: 27 | Batch_idx: 30 |  Loss: (0.6239) |  Loss2: (0.0000) | Acc: (78.00%) (3097/3968)
Epoch: 27 | Batch_idx: 40 |  Loss: (0.6211) |  Loss2: (0.0000) | Acc: (77.00%) (4080/5248)
Epoch: 27 | Batch_idx: 50 |  Loss: (0.6170) |  Loss2: (0.0000) | Acc: (78.00%) (5092/6528)
Epoch: 27 | Batch_idx: 60 |  Loss: (0.6220) |  Loss2: (0.0000) | Acc: (78.00%) (6097/7808)
Epoch: 27 | Batch_idx: 70 |  Loss: (0.6193) |  Loss2: (0.0000) | Acc: (78.00%) (7108/9088)
Epoch: 27 | Batch_idx: 80 |  Loss: (0.6174) |  Loss2: (0.0000) | Acc: (78.00%) (8125/10368)
Epoch: 27 | Batch_idx: 90 |  Loss: (0.6177) |  Loss2: (0.0000) | Acc: (78.00%) (9122/11648)
Epoch: 27 | Batch_idx: 100 |  Loss: (0.6187) |  Loss2: (0.0000) | Acc: (78.00%) (10106/12928)
Epoch: 27 | Batch_idx: 110 |  Loss: (0.6181) |  Loss2: (0.0000) | Acc: (78.00%) (11119/14208)
Epoch: 27 | Batch_idx: 120 |  Loss: (0.6157) |  Loss2: (0.0000) | Acc: (78.00%) (12142/15488)
Epoch: 27 | Batch_idx: 130 |  Loss: (0.6152) |  Loss2: (0.0000) | Acc: (78.00%) (13155/16768)
Epoch: 27 | Batch_idx: 140 |  Loss: (0.6150) |  Loss2: (0.0000) | Acc: (78.00%) (14168/18048)
Epoch: 27 | Batch_idx: 150 |  Loss: (0.6134) |  Loss2: (0.0000) | Acc: (78.00%) (15173/19328)
Epoch: 27 | Batch_idx: 160 |  Loss: (0.6150) |  Loss2: (0.0000) | Acc: (78.00%) (16171/20608)
Epoch: 27 | Batch_idx: 170 |  Loss: (0.6147) |  Loss2: (0.0000) | Acc: (78.00%) (17183/21888)
Epoch: 27 | Batch_idx: 180 |  Loss: (0.6172) |  Loss2: (0.0000) | Acc: (78.00%) (18156/23168)
Epoch: 27 | Batch_idx: 190 |  Loss: (0.6168) |  Loss2: (0.0000) | Acc: (78.00%) (19165/24448)
Epoch: 27 | Batch_idx: 200 |  Loss: (0.6166) |  Loss2: (0.0000) | Acc: (78.00%) (20165/25728)
Epoch: 27 | Batch_idx: 210 |  Loss: (0.6159) |  Loss2: (0.0000) | Acc: (78.00%) (21173/27008)
Epoch: 27 | Batch_idx: 220 |  Loss: (0.6162) |  Loss2: (0.0000) | Acc: (78.00%) (22165/28288)
Epoch: 27 | Batch_idx: 230 |  Loss: (0.6160) |  Loss2: (0.0000) | Acc: (78.00%) (23162/29568)
Epoch: 27 | Batch_idx: 240 |  Loss: (0.6159) |  Loss2: (0.0000) | Acc: (78.00%) (24174/30848)
Epoch: 27 | Batch_idx: 250 |  Loss: (0.6151) |  Loss2: (0.0000) | Acc: (78.00%) (25178/32128)
Epoch: 27 | Batch_idx: 260 |  Loss: (0.6154) |  Loss2: (0.0000) | Acc: (78.00%) (26190/33408)
Epoch: 27 | Batch_idx: 270 |  Loss: (0.6156) |  Loss2: (0.0000) | Acc: (78.00%) (27191/34688)
Epoch: 27 | Batch_idx: 280 |  Loss: (0.6147) |  Loss2: (0.0000) | Acc: (78.00%) (28204/35968)
Epoch: 27 | Batch_idx: 290 |  Loss: (0.6149) |  Loss2: (0.0000) | Acc: (78.00%) (29205/37248)
Epoch: 27 | Batch_idx: 300 |  Loss: (0.6149) |  Loss2: (0.0000) | Acc: (78.00%) (30201/38528)
Epoch: 27 | Batch_idx: 310 |  Loss: (0.6141) |  Loss2: (0.0000) | Acc: (78.00%) (31222/39808)
Epoch: 27 | Batch_idx: 320 |  Loss: (0.6138) |  Loss2: (0.0000) | Acc: (78.00%) (32235/41088)
Epoch: 27 | Batch_idx: 330 |  Loss: (0.6137) |  Loss2: (0.0000) | Acc: (78.00%) (33241/42368)
Epoch: 27 | Batch_idx: 340 |  Loss: (0.6138) |  Loss2: (0.0000) | Acc: (78.00%) (34242/43648)
Epoch: 27 | Batch_idx: 350 |  Loss: (0.6138) |  Loss2: (0.0000) | Acc: (78.00%) (35246/44928)
Epoch: 27 | Batch_idx: 360 |  Loss: (0.6139) |  Loss2: (0.0000) | Acc: (78.00%) (36259/46208)
Epoch: 27 | Batch_idx: 370 |  Loss: (0.6137) |  Loss2: (0.0000) | Acc: (78.00%) (37273/47488)
Epoch: 27 | Batch_idx: 380 |  Loss: (0.6133) |  Loss2: (0.0000) | Acc: (78.00%) (38280/48768)
Epoch: 27 | Batch_idx: 390 |  Loss: (0.6140) |  Loss2: (0.0000) | Acc: (78.00%) (39248/50000)
# TEST : Loss: (0.6438) | Acc: (77.00%) (7779/10000)
percent tensor([0.5232, 0.4768], device='cuda:0')
percent tensor([0.5223, 0.4777], device='cuda:0')
percent tensor([0.4923, 0.5077], device='cuda:0')
percent tensor([0.5895, 0.4105], device='cuda:0')
percent tensor([0.5416, 0.4584], device='cuda:0')
percent tensor([0.5470, 0.4530], device='cuda:0')
percent tensor([0.5553, 0.4447], device='cuda:0')
percent tensor([0.9888, 0.0112], device='cuda:0')
Epoch: 28 | Batch_idx: 0 |  Loss: (0.5725) |  Loss2: (0.0000) | Acc: (85.00%) (109/128)
Epoch: 28 | Batch_idx: 10 |  Loss: (0.6189) |  Loss2: (0.0000) | Acc: (79.00%) (1114/1408)
Epoch: 28 | Batch_idx: 20 |  Loss: (0.6194) |  Loss2: (0.0000) | Acc: (78.00%) (2097/2688)
Epoch: 28 | Batch_idx: 30 |  Loss: (0.6084) |  Loss2: (0.0000) | Acc: (78.00%) (3097/3968)
Epoch: 28 | Batch_idx: 40 |  Loss: (0.6158) |  Loss2: (0.0000) | Acc: (77.00%) (4087/5248)
Epoch: 28 | Batch_idx: 50 |  Loss: (0.6227) |  Loss2: (0.0000) | Acc: (77.00%) (5045/6528)
Epoch: 28 | Batch_idx: 60 |  Loss: (0.6176) |  Loss2: (0.0000) | Acc: (77.00%) (6062/7808)
Epoch: 28 | Batch_idx: 70 |  Loss: (0.6161) |  Loss2: (0.0000) | Acc: (77.00%) (7061/9088)
Epoch: 28 | Batch_idx: 80 |  Loss: (0.6163) |  Loss2: (0.0000) | Acc: (77.00%) (8069/10368)
Epoch: 28 | Batch_idx: 90 |  Loss: (0.6171) |  Loss2: (0.0000) | Acc: (77.00%) (9073/11648)
Epoch: 28 | Batch_idx: 100 |  Loss: (0.6163) |  Loss2: (0.0000) | Acc: (77.00%) (10070/12928)
Epoch: 28 | Batch_idx: 110 |  Loss: (0.6168) |  Loss2: (0.0000) | Acc: (77.00%) (11071/14208)
Epoch: 28 | Batch_idx: 120 |  Loss: (0.6208) |  Loss2: (0.0000) | Acc: (77.00%) (12057/15488)
Epoch: 28 | Batch_idx: 130 |  Loss: (0.6216) |  Loss2: (0.0000) | Acc: (77.00%) (13048/16768)
Epoch: 28 | Batch_idx: 140 |  Loss: (0.6211) |  Loss2: (0.0000) | Acc: (77.00%) (14055/18048)
Epoch: 28 | Batch_idx: 150 |  Loss: (0.6212) |  Loss2: (0.0000) | Acc: (77.00%) (15057/19328)
Epoch: 28 | Batch_idx: 160 |  Loss: (0.6206) |  Loss2: (0.0000) | Acc: (78.00%) (16081/20608)
Epoch: 28 | Batch_idx: 170 |  Loss: (0.6204) |  Loss2: (0.0000) | Acc: (78.00%) (17108/21888)
Epoch: 28 | Batch_idx: 180 |  Loss: (0.6208) |  Loss2: (0.0000) | Acc: (78.00%) (18101/23168)
Epoch: 28 | Batch_idx: 190 |  Loss: (0.6177) |  Loss2: (0.0000) | Acc: (78.00%) (19139/24448)
Epoch: 28 | Batch_idx: 200 |  Loss: (0.6170) |  Loss2: (0.0000) | Acc: (78.00%) (20151/25728)
Epoch: 28 | Batch_idx: 210 |  Loss: (0.6176) |  Loss2: (0.0000) | Acc: (78.00%) (21141/27008)
Epoch: 28 | Batch_idx: 220 |  Loss: (0.6153) |  Loss2: (0.0000) | Acc: (78.00%) (22186/28288)
Epoch: 28 | Batch_idx: 230 |  Loss: (0.6145) |  Loss2: (0.0000) | Acc: (78.00%) (23192/29568)
Epoch: 28 | Batch_idx: 240 |  Loss: (0.6155) |  Loss2: (0.0000) | Acc: (78.00%) (24177/30848)
Epoch: 28 | Batch_idx: 250 |  Loss: (0.6141) |  Loss2: (0.0000) | Acc: (78.00%) (25215/32128)
Epoch: 28 | Batch_idx: 260 |  Loss: (0.6118) |  Loss2: (0.0000) | Acc: (78.00%) (26251/33408)
Epoch: 28 | Batch_idx: 270 |  Loss: (0.6111) |  Loss2: (0.0000) | Acc: (78.00%) (27252/34688)
Epoch: 28 | Batch_idx: 280 |  Loss: (0.6115) |  Loss2: (0.0000) | Acc: (78.00%) (28248/35968)
Epoch: 28 | Batch_idx: 290 |  Loss: (0.6099) |  Loss2: (0.0000) | Acc: (78.00%) (29275/37248)
Epoch: 28 | Batch_idx: 300 |  Loss: (0.6083) |  Loss2: (0.0000) | Acc: (78.00%) (30325/38528)
Epoch: 28 | Batch_idx: 310 |  Loss: (0.6073) |  Loss2: (0.0000) | Acc: (78.00%) (31352/39808)
Epoch: 28 | Batch_idx: 320 |  Loss: (0.6060) |  Loss2: (0.0000) | Acc: (78.00%) (32374/41088)
Epoch: 28 | Batch_idx: 330 |  Loss: (0.6061) |  Loss2: (0.0000) | Acc: (78.00%) (33392/42368)
Epoch: 28 | Batch_idx: 340 |  Loss: (0.6054) |  Loss2: (0.0000) | Acc: (78.00%) (34407/43648)
Epoch: 28 | Batch_idx: 350 |  Loss: (0.6050) |  Loss2: (0.0000) | Acc: (78.00%) (35433/44928)
Epoch: 28 | Batch_idx: 360 |  Loss: (0.6044) |  Loss2: (0.0000) | Acc: (78.00%) (36455/46208)
Epoch: 28 | Batch_idx: 370 |  Loss: (0.6047) |  Loss2: (0.0000) | Acc: (78.00%) (37454/47488)
Epoch: 28 | Batch_idx: 380 |  Loss: (0.6056) |  Loss2: (0.0000) | Acc: (78.00%) (38442/48768)
Epoch: 28 | Batch_idx: 390 |  Loss: (0.6059) |  Loss2: (0.0000) | Acc: (78.00%) (39413/50000)
# TEST : Loss: (0.6374) | Acc: (78.00%) (7806/10000)
percent tensor([0.5246, 0.4754], device='cuda:0')
percent tensor([0.5228, 0.4772], device='cuda:0')
percent tensor([0.4934, 0.5066], device='cuda:0')
percent tensor([0.5857, 0.4143], device='cuda:0')
percent tensor([0.5460, 0.4540], device='cuda:0')
percent tensor([0.5544, 0.4456], device='cuda:0')
percent tensor([0.5586, 0.4414], device='cuda:0')
percent tensor([0.9918, 0.0082], device='cuda:0')
Epoch: 29 | Batch_idx: 0 |  Loss: (0.6946) |  Loss2: (0.0000) | Acc: (72.00%) (93/128)
Epoch: 29 | Batch_idx: 10 |  Loss: (0.6085) |  Loss2: (0.0000) | Acc: (78.00%) (1105/1408)
Epoch: 29 | Batch_idx: 20 |  Loss: (0.6012) |  Loss2: (0.0000) | Acc: (79.00%) (2135/2688)
Epoch: 29 | Batch_idx: 30 |  Loss: (0.5936) |  Loss2: (0.0000) | Acc: (79.00%) (3159/3968)
Epoch: 29 | Batch_idx: 40 |  Loss: (0.5898) |  Loss2: (0.0000) | Acc: (79.00%) (4177/5248)
Epoch: 29 | Batch_idx: 50 |  Loss: (0.5971) |  Loss2: (0.0000) | Acc: (79.00%) (5180/6528)
Epoch: 29 | Batch_idx: 60 |  Loss: (0.5986) |  Loss2: (0.0000) | Acc: (79.00%) (6181/7808)
Epoch: 29 | Batch_idx: 70 |  Loss: (0.5960) |  Loss2: (0.0000) | Acc: (79.00%) (7208/9088)
Epoch: 29 | Batch_idx: 80 |  Loss: (0.6008) |  Loss2: (0.0000) | Acc: (79.00%) (8210/10368)
Epoch: 29 | Batch_idx: 90 |  Loss: (0.5992) |  Loss2: (0.0000) | Acc: (79.00%) (9221/11648)
Epoch: 29 | Batch_idx: 100 |  Loss: (0.5998) |  Loss2: (0.0000) | Acc: (79.00%) (10248/12928)
Epoch: 29 | Batch_idx: 110 |  Loss: (0.6046) |  Loss2: (0.0000) | Acc: (79.00%) (11255/14208)
Epoch: 29 | Batch_idx: 120 |  Loss: (0.6051) |  Loss2: (0.0000) | Acc: (79.00%) (12260/15488)
Epoch: 29 | Batch_idx: 130 |  Loss: (0.6017) |  Loss2: (0.0000) | Acc: (79.00%) (13293/16768)
Epoch: 29 | Batch_idx: 140 |  Loss: (0.6058) |  Loss2: (0.0000) | Acc: (79.00%) (14278/18048)
Epoch: 29 | Batch_idx: 150 |  Loss: (0.6053) |  Loss2: (0.0000) | Acc: (79.00%) (15289/19328)
Epoch: 29 | Batch_idx: 160 |  Loss: (0.6043) |  Loss2: (0.0000) | Acc: (79.00%) (16311/20608)
Epoch: 29 | Batch_idx: 170 |  Loss: (0.6049) |  Loss2: (0.0000) | Acc: (79.00%) (17316/21888)
Epoch: 29 | Batch_idx: 180 |  Loss: (0.6057) |  Loss2: (0.0000) | Acc: (78.00%) (18298/23168)
Epoch: 29 | Batch_idx: 190 |  Loss: (0.6048) |  Loss2: (0.0000) | Acc: (79.00%) (19330/24448)
Epoch: 29 | Batch_idx: 200 |  Loss: (0.6043) |  Loss2: (0.0000) | Acc: (79.00%) (20352/25728)
Epoch: 29 | Batch_idx: 210 |  Loss: (0.6056) |  Loss2: (0.0000) | Acc: (79.00%) (21348/27008)
Epoch: 29 | Batch_idx: 220 |  Loss: (0.6049) |  Loss2: (0.0000) | Acc: (79.00%) (22363/28288)
Epoch: 29 | Batch_idx: 230 |  Loss: (0.6032) |  Loss2: (0.0000) | Acc: (79.00%) (23405/29568)
Epoch: 29 | Batch_idx: 240 |  Loss: (0.6024) |  Loss2: (0.0000) | Acc: (79.00%) (24432/30848)
Epoch: 29 | Batch_idx: 250 |  Loss: (0.6018) |  Loss2: (0.0000) | Acc: (79.00%) (25456/32128)
Epoch: 29 | Batch_idx: 260 |  Loss: (0.6007) |  Loss2: (0.0000) | Acc: (79.00%) (26486/33408)
Epoch: 29 | Batch_idx: 270 |  Loss: (0.5996) |  Loss2: (0.0000) | Acc: (79.00%) (27503/34688)
Epoch: 29 | Batch_idx: 280 |  Loss: (0.5987) |  Loss2: (0.0000) | Acc: (79.00%) (28520/35968)
Epoch: 29 | Batch_idx: 290 |  Loss: (0.5988) |  Loss2: (0.0000) | Acc: (79.00%) (29525/37248)
Epoch: 29 | Batch_idx: 300 |  Loss: (0.5985) |  Loss2: (0.0000) | Acc: (79.00%) (30522/38528)
Epoch: 29 | Batch_idx: 310 |  Loss: (0.5982) |  Loss2: (0.0000) | Acc: (79.00%) (31544/39808)
Epoch: 29 | Batch_idx: 320 |  Loss: (0.5970) |  Loss2: (0.0000) | Acc: (79.00%) (32572/41088)
Epoch: 29 | Batch_idx: 330 |  Loss: (0.5958) |  Loss2: (0.0000) | Acc: (79.00%) (33608/42368)
Epoch: 29 | Batch_idx: 340 |  Loss: (0.5970) |  Loss2: (0.0000) | Acc: (79.00%) (34598/43648)
Epoch: 29 | Batch_idx: 350 |  Loss: (0.5956) |  Loss2: (0.0000) | Acc: (79.00%) (35625/44928)
Epoch: 29 | Batch_idx: 360 |  Loss: (0.5963) |  Loss2: (0.0000) | Acc: (79.00%) (36621/46208)
Epoch: 29 | Batch_idx: 370 |  Loss: (0.5969) |  Loss2: (0.0000) | Acc: (79.00%) (37618/47488)
Epoch: 29 | Batch_idx: 380 |  Loss: (0.5985) |  Loss2: (0.0000) | Acc: (79.00%) (38596/48768)
Epoch: 29 | Batch_idx: 390 |  Loss: (0.5984) |  Loss2: (0.0000) | Acc: (79.00%) (39574/50000)
# TEST : Loss: (0.6301) | Acc: (78.00%) (7830/10000)
percent tensor([0.5253, 0.4747], device='cuda:0')
percent tensor([0.5239, 0.4761], device='cuda:0')
percent tensor([0.4936, 0.5064], device='cuda:0')
percent tensor([0.5836, 0.4164], device='cuda:0')
percent tensor([0.5472, 0.4528], device='cuda:0')
percent tensor([0.5591, 0.4409], device='cuda:0')
percent tensor([0.5638, 0.4362], device='cuda:0')
percent tensor([0.9937, 0.0063], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 30 | Batch_idx: 0 |  Loss: (0.4722) |  Loss2: (0.0000) | Acc: (83.00%) (107/128)
Epoch: 30 | Batch_idx: 10 |  Loss: (0.5620) |  Loss2: (0.0000) | Acc: (80.00%) (1128/1408)
Epoch: 30 | Batch_idx: 20 |  Loss: (0.5793) |  Loss2: (0.0000) | Acc: (79.00%) (2148/2688)
Epoch: 30 | Batch_idx: 30 |  Loss: (0.5952) |  Loss2: (0.0000) | Acc: (79.00%) (3136/3968)
Epoch: 30 | Batch_idx: 40 |  Loss: (0.5887) |  Loss2: (0.0000) | Acc: (79.00%) (4154/5248)
Epoch: 30 | Batch_idx: 50 |  Loss: (0.5986) |  Loss2: (0.0000) | Acc: (79.00%) (5170/6528)
Epoch: 30 | Batch_idx: 60 |  Loss: (0.5981) |  Loss2: (0.0000) | Acc: (79.00%) (6185/7808)
Epoch: 30 | Batch_idx: 70 |  Loss: (0.6005) |  Loss2: (0.0000) | Acc: (79.00%) (7195/9088)
Epoch: 30 | Batch_idx: 80 |  Loss: (0.5976) |  Loss2: (0.0000) | Acc: (79.00%) (8210/10368)
Epoch: 30 | Batch_idx: 90 |  Loss: (0.6001) |  Loss2: (0.0000) | Acc: (79.00%) (9224/11648)
Epoch: 30 | Batch_idx: 100 |  Loss: (0.6012) |  Loss2: (0.0000) | Acc: (79.00%) (10242/12928)
Epoch: 30 | Batch_idx: 110 |  Loss: (0.6019) |  Loss2: (0.0000) | Acc: (79.00%) (11244/14208)
Epoch: 30 | Batch_idx: 120 |  Loss: (0.6007) |  Loss2: (0.0000) | Acc: (79.00%) (12253/15488)
Epoch: 30 | Batch_idx: 130 |  Loss: (0.6012) |  Loss2: (0.0000) | Acc: (79.00%) (13264/16768)
Epoch: 30 | Batch_idx: 140 |  Loss: (0.6010) |  Loss2: (0.0000) | Acc: (79.00%) (14274/18048)
Epoch: 30 | Batch_idx: 150 |  Loss: (0.6019) |  Loss2: (0.0000) | Acc: (79.00%) (15285/19328)
Epoch: 30 | Batch_idx: 160 |  Loss: (0.6009) |  Loss2: (0.0000) | Acc: (79.00%) (16309/20608)
Epoch: 30 | Batch_idx: 170 |  Loss: (0.6029) |  Loss2: (0.0000) | Acc: (79.00%) (17296/21888)
Epoch: 30 | Batch_idx: 180 |  Loss: (0.6027) |  Loss2: (0.0000) | Acc: (78.00%) (18302/23168)
Epoch: 30 | Batch_idx: 190 |  Loss: (0.6028) |  Loss2: (0.0000) | Acc: (78.00%) (19309/24448)
Epoch: 30 | Batch_idx: 200 |  Loss: (0.6018) |  Loss2: (0.0000) | Acc: (79.00%) (20328/25728)
Epoch: 30 | Batch_idx: 210 |  Loss: (0.6015) |  Loss2: (0.0000) | Acc: (79.00%) (21341/27008)
Epoch: 30 | Batch_idx: 220 |  Loss: (0.6006) |  Loss2: (0.0000) | Acc: (79.00%) (22378/28288)
Epoch: 30 | Batch_idx: 230 |  Loss: (0.5999) |  Loss2: (0.0000) | Acc: (79.00%) (23391/29568)
Epoch: 30 | Batch_idx: 240 |  Loss: (0.5989) |  Loss2: (0.0000) | Acc: (79.00%) (24410/30848)
Epoch: 30 | Batch_idx: 250 |  Loss: (0.5994) |  Loss2: (0.0000) | Acc: (79.00%) (25429/32128)
Epoch: 30 | Batch_idx: 260 |  Loss: (0.5983) |  Loss2: (0.0000) | Acc: (79.00%) (26463/33408)
Epoch: 30 | Batch_idx: 270 |  Loss: (0.5968) |  Loss2: (0.0000) | Acc: (79.00%) (27483/34688)
Epoch: 30 | Batch_idx: 280 |  Loss: (0.5991) |  Loss2: (0.0000) | Acc: (79.00%) (28460/35968)
Epoch: 30 | Batch_idx: 290 |  Loss: (0.5989) |  Loss2: (0.0000) | Acc: (79.00%) (29469/37248)
Epoch: 30 | Batch_idx: 300 |  Loss: (0.5986) |  Loss2: (0.0000) | Acc: (79.00%) (30492/38528)
Epoch: 30 | Batch_idx: 310 |  Loss: (0.5974) |  Loss2: (0.0000) | Acc: (79.00%) (31537/39808)
Epoch: 30 | Batch_idx: 320 |  Loss: (0.5968) |  Loss2: (0.0000) | Acc: (79.00%) (32567/41088)
Epoch: 30 | Batch_idx: 330 |  Loss: (0.5964) |  Loss2: (0.0000) | Acc: (79.00%) (33591/42368)
Epoch: 30 | Batch_idx: 340 |  Loss: (0.5961) |  Loss2: (0.0000) | Acc: (79.00%) (34606/43648)
Epoch: 30 | Batch_idx: 350 |  Loss: (0.5953) |  Loss2: (0.0000) | Acc: (79.00%) (35647/44928)
Epoch: 30 | Batch_idx: 360 |  Loss: (0.5936) |  Loss2: (0.0000) | Acc: (79.00%) (36682/46208)
Epoch: 30 | Batch_idx: 370 |  Loss: (0.5941) |  Loss2: (0.0000) | Acc: (79.00%) (37702/47488)
Epoch: 30 | Batch_idx: 380 |  Loss: (0.5959) |  Loss2: (0.0000) | Acc: (79.00%) (38688/48768)
Epoch: 30 | Batch_idx: 390 |  Loss: (0.5951) |  Loss2: (0.0000) | Acc: (79.00%) (39680/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_030.pth.tar'
# TEST : Loss: (0.7693) | Acc: (73.00%) (7390/10000)
percent tensor([0.5254, 0.4746], device='cuda:0')
percent tensor([0.5244, 0.4756], device='cuda:0')
percent tensor([0.4935, 0.5065], device='cuda:0')
percent tensor([0.5820, 0.4180], device='cuda:0')
percent tensor([0.5490, 0.4510], device='cuda:0')
percent tensor([0.5606, 0.4394], device='cuda:0')
percent tensor([0.5680, 0.4320], device='cuda:0')
percent tensor([0.9953, 0.0047], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(171.8327, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(787.8500, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(783.4050, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1529.3767, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(504.9400, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2181.9280, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4310.4673, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1428.8828, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6086.5161, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12136.1875, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4039.6501, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(17124.3613, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 31 | Batch_idx: 0 |  Loss: (0.5433) |  Loss2: (0.0000) | Acc: (80.00%) (103/128)
Epoch: 31 | Batch_idx: 10 |  Loss: (0.5909) |  Loss2: (0.0000) | Acc: (79.00%) (1121/1408)
Epoch: 31 | Batch_idx: 20 |  Loss: (0.5755) |  Loss2: (0.0000) | Acc: (80.00%) (2154/2688)
Epoch: 31 | Batch_idx: 30 |  Loss: (0.5695) |  Loss2: (0.0000) | Acc: (80.00%) (3180/3968)
Epoch: 31 | Batch_idx: 40 |  Loss: (0.5747) |  Loss2: (0.0000) | Acc: (79.00%) (4188/5248)
Epoch: 31 | Batch_idx: 50 |  Loss: (0.5654) |  Loss2: (0.0000) | Acc: (80.00%) (5248/6528)
Epoch: 31 | Batch_idx: 60 |  Loss: (0.5629) |  Loss2: (0.0000) | Acc: (80.00%) (6272/7808)
Epoch: 31 | Batch_idx: 70 |  Loss: (0.5622) |  Loss2: (0.0000) | Acc: (80.00%) (7312/9088)
Epoch: 31 | Batch_idx: 80 |  Loss: (0.5598) |  Loss2: (0.0000) | Acc: (80.00%) (8341/10368)
Epoch: 31 | Batch_idx: 90 |  Loss: (0.5579) |  Loss2: (0.0000) | Acc: (80.00%) (9369/11648)
Epoch: 31 | Batch_idx: 100 |  Loss: (0.5581) |  Loss2: (0.0000) | Acc: (80.00%) (10408/12928)
Epoch: 31 | Batch_idx: 110 |  Loss: (0.5587) |  Loss2: (0.0000) | Acc: (80.00%) (11429/14208)
Epoch: 31 | Batch_idx: 120 |  Loss: (0.5583) |  Loss2: (0.0000) | Acc: (80.00%) (12459/15488)
Epoch: 31 | Batch_idx: 130 |  Loss: (0.5581) |  Loss2: (0.0000) | Acc: (80.00%) (13498/16768)
Epoch: 31 | Batch_idx: 140 |  Loss: (0.5587) |  Loss2: (0.0000) | Acc: (80.00%) (14522/18048)
Epoch: 31 | Batch_idx: 150 |  Loss: (0.5602) |  Loss2: (0.0000) | Acc: (80.00%) (15559/19328)
Epoch: 31 | Batch_idx: 160 |  Loss: (0.5618) |  Loss2: (0.0000) | Acc: (80.00%) (16573/20608)
Epoch: 31 | Batch_idx: 170 |  Loss: (0.5614) |  Loss2: (0.0000) | Acc: (80.00%) (17592/21888)
Epoch: 31 | Batch_idx: 180 |  Loss: (0.5628) |  Loss2: (0.0000) | Acc: (80.00%) (18618/23168)
Epoch: 31 | Batch_idx: 190 |  Loss: (0.5628) |  Loss2: (0.0000) | Acc: (80.00%) (19653/24448)
Epoch: 31 | Batch_idx: 200 |  Loss: (0.5657) |  Loss2: (0.0000) | Acc: (80.00%) (20659/25728)
Epoch: 31 | Batch_idx: 210 |  Loss: (0.5647) |  Loss2: (0.0000) | Acc: (80.00%) (21701/27008)
Epoch: 31 | Batch_idx: 220 |  Loss: (0.5662) |  Loss2: (0.0000) | Acc: (80.00%) (22731/28288)
Epoch: 31 | Batch_idx: 230 |  Loss: (0.5662) |  Loss2: (0.0000) | Acc: (80.00%) (23757/29568)
Epoch: 31 | Batch_idx: 240 |  Loss: (0.5655) |  Loss2: (0.0000) | Acc: (80.00%) (24791/30848)
Epoch: 31 | Batch_idx: 250 |  Loss: (0.5653) |  Loss2: (0.0000) | Acc: (80.00%) (25820/32128)
Epoch: 31 | Batch_idx: 260 |  Loss: (0.5646) |  Loss2: (0.0000) | Acc: (80.00%) (26872/33408)
Epoch: 31 | Batch_idx: 270 |  Loss: (0.5641) |  Loss2: (0.0000) | Acc: (80.00%) (27898/34688)
Epoch: 31 | Batch_idx: 280 |  Loss: (0.5648) |  Loss2: (0.0000) | Acc: (80.00%) (28922/35968)
Epoch: 31 | Batch_idx: 290 |  Loss: (0.5670) |  Loss2: (0.0000) | Acc: (80.00%) (29929/37248)
Epoch: 31 | Batch_idx: 300 |  Loss: (0.5667) |  Loss2: (0.0000) | Acc: (80.00%) (30969/38528)
Epoch: 31 | Batch_idx: 310 |  Loss: (0.5684) |  Loss2: (0.0000) | Acc: (80.00%) (31983/39808)
Epoch: 31 | Batch_idx: 320 |  Loss: (0.5686) |  Loss2: (0.0000) | Acc: (80.00%) (33017/41088)
Epoch: 31 | Batch_idx: 330 |  Loss: (0.5694) |  Loss2: (0.0000) | Acc: (80.00%) (34019/42368)
Epoch: 31 | Batch_idx: 340 |  Loss: (0.5700) |  Loss2: (0.0000) | Acc: (80.00%) (35027/43648)
Epoch: 31 | Batch_idx: 350 |  Loss: (0.5696) |  Loss2: (0.0000) | Acc: (80.00%) (36058/44928)
Epoch: 31 | Batch_idx: 360 |  Loss: (0.5688) |  Loss2: (0.0000) | Acc: (80.00%) (37078/46208)
Epoch: 31 | Batch_idx: 370 |  Loss: (0.5696) |  Loss2: (0.0000) | Acc: (80.00%) (38101/47488)
Epoch: 31 | Batch_idx: 380 |  Loss: (0.5687) |  Loss2: (0.0000) | Acc: (80.00%) (39126/48768)
Epoch: 31 | Batch_idx: 390 |  Loss: (0.5692) |  Loss2: (0.0000) | Acc: (80.00%) (40112/50000)
# TEST : Loss: (0.7399) | Acc: (75.00%) (7519/10000)
percent tensor([0.5250, 0.4750], device='cuda:0')
percent tensor([0.5243, 0.4757], device='cuda:0')
percent tensor([0.4937, 0.5063], device='cuda:0')
percent tensor([0.5823, 0.4177], device='cuda:0')
percent tensor([0.5490, 0.4510], device='cuda:0')
percent tensor([0.5594, 0.4406], device='cuda:0')
percent tensor([0.5662, 0.4338], device='cuda:0')
percent tensor([0.9954, 0.0046], device='cuda:0')
Epoch: 32 | Batch_idx: 0 |  Loss: (0.7115) |  Loss2: (0.0000) | Acc: (78.00%) (101/128)
Epoch: 32 | Batch_idx: 10 |  Loss: (0.5667) |  Loss2: (0.0000) | Acc: (79.00%) (1118/1408)
Epoch: 32 | Batch_idx: 20 |  Loss: (0.5532) |  Loss2: (0.0000) | Acc: (80.00%) (2174/2688)
Epoch: 32 | Batch_idx: 30 |  Loss: (0.5396) |  Loss2: (0.0000) | Acc: (81.00%) (3233/3968)
Epoch: 32 | Batch_idx: 40 |  Loss: (0.5333) |  Loss2: (0.0000) | Acc: (81.00%) (4292/5248)
Epoch: 32 | Batch_idx: 50 |  Loss: (0.5315) |  Loss2: (0.0000) | Acc: (81.00%) (5345/6528)
Epoch: 32 | Batch_idx: 60 |  Loss: (0.5321) |  Loss2: (0.0000) | Acc: (81.00%) (6389/7808)
Epoch: 32 | Batch_idx: 70 |  Loss: (0.5366) |  Loss2: (0.0000) | Acc: (81.00%) (7430/9088)
Epoch: 32 | Batch_idx: 80 |  Loss: (0.5368) |  Loss2: (0.0000) | Acc: (81.00%) (8477/10368)
Epoch: 32 | Batch_idx: 90 |  Loss: (0.5416) |  Loss2: (0.0000) | Acc: (81.00%) (9491/11648)
Epoch: 32 | Batch_idx: 100 |  Loss: (0.5423) |  Loss2: (0.0000) | Acc: (81.00%) (10527/12928)
Epoch: 32 | Batch_idx: 110 |  Loss: (0.5438) |  Loss2: (0.0000) | Acc: (81.00%) (11566/14208)
Epoch: 32 | Batch_idx: 120 |  Loss: (0.5443) |  Loss2: (0.0000) | Acc: (81.00%) (12605/15488)
Epoch: 32 | Batch_idx: 130 |  Loss: (0.5438) |  Loss2: (0.0000) | Acc: (81.00%) (13650/16768)
Epoch: 32 | Batch_idx: 140 |  Loss: (0.5439) |  Loss2: (0.0000) | Acc: (81.00%) (14689/18048)
Epoch: 32 | Batch_idx: 150 |  Loss: (0.5418) |  Loss2: (0.0000) | Acc: (81.00%) (15731/19328)
Epoch: 32 | Batch_idx: 160 |  Loss: (0.5431) |  Loss2: (0.0000) | Acc: (81.00%) (16769/20608)
Epoch: 32 | Batch_idx: 170 |  Loss: (0.5451) |  Loss2: (0.0000) | Acc: (81.00%) (17790/21888)
Epoch: 32 | Batch_idx: 180 |  Loss: (0.5442) |  Loss2: (0.0000) | Acc: (81.00%) (18844/23168)
Epoch: 32 | Batch_idx: 190 |  Loss: (0.5434) |  Loss2: (0.0000) | Acc: (81.00%) (19891/24448)
Epoch: 32 | Batch_idx: 200 |  Loss: (0.5428) |  Loss2: (0.0000) | Acc: (81.00%) (20939/25728)
Epoch: 32 | Batch_idx: 210 |  Loss: (0.5424) |  Loss2: (0.0000) | Acc: (81.00%) (21991/27008)
Epoch: 32 | Batch_idx: 220 |  Loss: (0.5440) |  Loss2: (0.0000) | Acc: (81.00%) (23002/28288)
Epoch: 32 | Batch_idx: 230 |  Loss: (0.5467) |  Loss2: (0.0000) | Acc: (81.00%) (24010/29568)
Epoch: 32 | Batch_idx: 240 |  Loss: (0.5456) |  Loss2: (0.0000) | Acc: (81.00%) (25075/30848)
Epoch: 32 | Batch_idx: 250 |  Loss: (0.5473) |  Loss2: (0.0000) | Acc: (81.00%) (26091/32128)
Epoch: 32 | Batch_idx: 260 |  Loss: (0.5462) |  Loss2: (0.0000) | Acc: (81.00%) (27146/33408)
Epoch: 32 | Batch_idx: 270 |  Loss: (0.5461) |  Loss2: (0.0000) | Acc: (81.00%) (28187/34688)
Epoch: 32 | Batch_idx: 280 |  Loss: (0.5460) |  Loss2: (0.0000) | Acc: (81.00%) (29228/35968)
Epoch: 32 | Batch_idx: 290 |  Loss: (0.5468) |  Loss2: (0.0000) | Acc: (81.00%) (30254/37248)
Epoch: 32 | Batch_idx: 300 |  Loss: (0.5469) |  Loss2: (0.0000) | Acc: (81.00%) (31286/38528)
Epoch: 32 | Batch_idx: 310 |  Loss: (0.5461) |  Loss2: (0.0000) | Acc: (81.00%) (32334/39808)
Epoch: 32 | Batch_idx: 320 |  Loss: (0.5456) |  Loss2: (0.0000) | Acc: (81.00%) (33373/41088)
Epoch: 32 | Batch_idx: 330 |  Loss: (0.5452) |  Loss2: (0.0000) | Acc: (81.00%) (34396/42368)
Epoch: 32 | Batch_idx: 340 |  Loss: (0.5452) |  Loss2: (0.0000) | Acc: (81.00%) (35428/43648)
Epoch: 32 | Batch_idx: 350 |  Loss: (0.5455) |  Loss2: (0.0000) | Acc: (81.00%) (36462/44928)
Epoch: 32 | Batch_idx: 360 |  Loss: (0.5451) |  Loss2: (0.0000) | Acc: (81.00%) (37518/46208)
Epoch: 32 | Batch_idx: 370 |  Loss: (0.5456) |  Loss2: (0.0000) | Acc: (81.00%) (38543/47488)
Epoch: 32 | Batch_idx: 380 |  Loss: (0.5451) |  Loss2: (0.0000) | Acc: (81.00%) (39597/48768)
Epoch: 32 | Batch_idx: 390 |  Loss: (0.5461) |  Loss2: (0.0000) | Acc: (81.00%) (40574/50000)
# TEST : Loss: (0.6330) | Acc: (78.00%) (7827/10000)
percent tensor([0.5250, 0.4750], device='cuda:0')
percent tensor([0.5238, 0.4762], device='cuda:0')
percent tensor([0.4937, 0.5063], device='cuda:0')
percent tensor([0.5837, 0.4163], device='cuda:0')
percent tensor([0.5471, 0.4529], device='cuda:0')
percent tensor([0.5589, 0.4411], device='cuda:0')
percent tensor([0.5649, 0.4351], device='cuda:0')
percent tensor([0.9947, 0.0053], device='cuda:0')
Epoch: 33 | Batch_idx: 0 |  Loss: (0.6192) |  Loss2: (0.0000) | Acc: (80.00%) (103/128)
Epoch: 33 | Batch_idx: 10 |  Loss: (0.5259) |  Loss2: (0.0000) | Acc: (82.00%) (1155/1408)
Epoch: 33 | Batch_idx: 20 |  Loss: (0.5447) |  Loss2: (0.0000) | Acc: (81.00%) (2181/2688)
Epoch: 33 | Batch_idx: 30 |  Loss: (0.5218) |  Loss2: (0.0000) | Acc: (82.00%) (3257/3968)
Epoch: 33 | Batch_idx: 40 |  Loss: (0.5264) |  Loss2: (0.0000) | Acc: (81.00%) (4286/5248)
Epoch: 33 | Batch_idx: 50 |  Loss: (0.5344) |  Loss2: (0.0000) | Acc: (81.00%) (5314/6528)
Epoch: 33 | Batch_idx: 60 |  Loss: (0.5322) |  Loss2: (0.0000) | Acc: (81.00%) (6364/7808)
Epoch: 33 | Batch_idx: 70 |  Loss: (0.5301) |  Loss2: (0.0000) | Acc: (81.00%) (7405/9088)
Epoch: 33 | Batch_idx: 80 |  Loss: (0.5287) |  Loss2: (0.0000) | Acc: (81.00%) (8453/10368)
Epoch: 33 | Batch_idx: 90 |  Loss: (0.5259) |  Loss2: (0.0000) | Acc: (81.00%) (9511/11648)
Epoch: 33 | Batch_idx: 100 |  Loss: (0.5243) |  Loss2: (0.0000) | Acc: (81.00%) (10571/12928)
Epoch: 33 | Batch_idx: 110 |  Loss: (0.5275) |  Loss2: (0.0000) | Acc: (81.00%) (11608/14208)
Epoch: 33 | Batch_idx: 120 |  Loss: (0.5269) |  Loss2: (0.0000) | Acc: (81.00%) (12652/15488)
Epoch: 33 | Batch_idx: 130 |  Loss: (0.5292) |  Loss2: (0.0000) | Acc: (81.00%) (13685/16768)
Epoch: 33 | Batch_idx: 140 |  Loss: (0.5314) |  Loss2: (0.0000) | Acc: (81.00%) (14714/18048)
Epoch: 33 | Batch_idx: 150 |  Loss: (0.5312) |  Loss2: (0.0000) | Acc: (81.00%) (15760/19328)
Epoch: 33 | Batch_idx: 160 |  Loss: (0.5290) |  Loss2: (0.0000) | Acc: (81.00%) (16816/20608)
Epoch: 33 | Batch_idx: 170 |  Loss: (0.5279) |  Loss2: (0.0000) | Acc: (81.00%) (17882/21888)
Epoch: 33 | Batch_idx: 180 |  Loss: (0.5294) |  Loss2: (0.0000) | Acc: (81.00%) (18918/23168)
Epoch: 33 | Batch_idx: 190 |  Loss: (0.5291) |  Loss2: (0.0000) | Acc: (81.00%) (19964/24448)
Epoch: 33 | Batch_idx: 200 |  Loss: (0.5277) |  Loss2: (0.0000) | Acc: (81.00%) (21017/25728)
Epoch: 33 | Batch_idx: 210 |  Loss: (0.5258) |  Loss2: (0.0000) | Acc: (81.00%) (22082/27008)
Epoch: 33 | Batch_idx: 220 |  Loss: (0.5254) |  Loss2: (0.0000) | Acc: (81.00%) (23142/28288)
Epoch: 33 | Batch_idx: 230 |  Loss: (0.5259) |  Loss2: (0.0000) | Acc: (81.00%) (24185/29568)
Epoch: 33 | Batch_idx: 240 |  Loss: (0.5258) |  Loss2: (0.0000) | Acc: (81.00%) (25236/30848)
Epoch: 33 | Batch_idx: 250 |  Loss: (0.5252) |  Loss2: (0.0000) | Acc: (81.00%) (26286/32128)
Epoch: 33 | Batch_idx: 260 |  Loss: (0.5240) |  Loss2: (0.0000) | Acc: (81.00%) (27336/33408)
Epoch: 33 | Batch_idx: 270 |  Loss: (0.5233) |  Loss2: (0.0000) | Acc: (81.00%) (28381/34688)
Epoch: 33 | Batch_idx: 280 |  Loss: (0.5224) |  Loss2: (0.0000) | Acc: (81.00%) (29448/35968)
Epoch: 33 | Batch_idx: 290 |  Loss: (0.5216) |  Loss2: (0.0000) | Acc: (81.00%) (30512/37248)
Epoch: 33 | Batch_idx: 300 |  Loss: (0.5230) |  Loss2: (0.0000) | Acc: (81.00%) (31531/38528)
Epoch: 33 | Batch_idx: 310 |  Loss: (0.5234) |  Loss2: (0.0000) | Acc: (81.00%) (32581/39808)
Epoch: 33 | Batch_idx: 320 |  Loss: (0.5229) |  Loss2: (0.0000) | Acc: (81.00%) (33632/41088)
Epoch: 33 | Batch_idx: 330 |  Loss: (0.5227) |  Loss2: (0.0000) | Acc: (81.00%) (34687/42368)
Epoch: 33 | Batch_idx: 340 |  Loss: (0.5223) |  Loss2: (0.0000) | Acc: (81.00%) (35745/43648)
Epoch: 33 | Batch_idx: 350 |  Loss: (0.5226) |  Loss2: (0.0000) | Acc: (81.00%) (36778/44928)
Epoch: 33 | Batch_idx: 360 |  Loss: (0.5232) |  Loss2: (0.0000) | Acc: (81.00%) (37812/46208)
Epoch: 33 | Batch_idx: 370 |  Loss: (0.5223) |  Loss2: (0.0000) | Acc: (81.00%) (38877/47488)
Epoch: 33 | Batch_idx: 380 |  Loss: (0.5232) |  Loss2: (0.0000) | Acc: (81.00%) (39912/48768)
Epoch: 33 | Batch_idx: 390 |  Loss: (0.5234) |  Loss2: (0.0000) | Acc: (81.00%) (40913/50000)
# TEST : Loss: (0.6824) | Acc: (77.00%) (7719/10000)
percent tensor([0.5246, 0.4754], device='cuda:0')
percent tensor([0.5239, 0.4761], device='cuda:0')
percent tensor([0.4936, 0.5064], device='cuda:0')
percent tensor([0.5827, 0.4173], device='cuda:0')
percent tensor([0.5454, 0.4546], device='cuda:0')
percent tensor([0.5586, 0.4414], device='cuda:0')
percent tensor([0.5625, 0.4375], device='cuda:0')
percent tensor([0.9944, 0.0056], device='cuda:0')
Epoch: 34 | Batch_idx: 0 |  Loss: (0.5642) |  Loss2: (0.0000) | Acc: (80.00%) (103/128)
Epoch: 34 | Batch_idx: 10 |  Loss: (0.5229) |  Loss2: (0.0000) | Acc: (82.00%) (1167/1408)
Epoch: 34 | Batch_idx: 20 |  Loss: (0.5102) |  Loss2: (0.0000) | Acc: (83.00%) (2238/2688)
Epoch: 34 | Batch_idx: 30 |  Loss: (0.4929) |  Loss2: (0.0000) | Acc: (83.00%) (3320/3968)
Epoch: 34 | Batch_idx: 40 |  Loss: (0.5038) |  Loss2: (0.0000) | Acc: (82.00%) (4350/5248)
Epoch: 34 | Batch_idx: 50 |  Loss: (0.5001) |  Loss2: (0.0000) | Acc: (82.00%) (5409/6528)
Epoch: 34 | Batch_idx: 60 |  Loss: (0.5011) |  Loss2: (0.0000) | Acc: (82.00%) (6452/7808)
Epoch: 34 | Batch_idx: 70 |  Loss: (0.5014) |  Loss2: (0.0000) | Acc: (82.00%) (7514/9088)
Epoch: 34 | Batch_idx: 80 |  Loss: (0.4997) |  Loss2: (0.0000) | Acc: (82.00%) (8586/10368)
Epoch: 34 | Batch_idx: 90 |  Loss: (0.5014) |  Loss2: (0.0000) | Acc: (82.00%) (9644/11648)
Epoch: 34 | Batch_idx: 100 |  Loss: (0.5013) |  Loss2: (0.0000) | Acc: (82.00%) (10714/12928)
Epoch: 34 | Batch_idx: 110 |  Loss: (0.5000) |  Loss2: (0.0000) | Acc: (82.00%) (11775/14208)
Epoch: 34 | Batch_idx: 120 |  Loss: (0.5016) |  Loss2: (0.0000) | Acc: (82.00%) (12814/15488)
Epoch: 34 | Batch_idx: 130 |  Loss: (0.5004) |  Loss2: (0.0000) | Acc: (82.00%) (13883/16768)
Epoch: 34 | Batch_idx: 140 |  Loss: (0.5017) |  Loss2: (0.0000) | Acc: (82.00%) (14937/18048)
Epoch: 34 | Batch_idx: 150 |  Loss: (0.5051) |  Loss2: (0.0000) | Acc: (82.00%) (15977/19328)
Epoch: 34 | Batch_idx: 160 |  Loss: (0.5068) |  Loss2: (0.0000) | Acc: (82.00%) (17004/20608)
Epoch: 34 | Batch_idx: 170 |  Loss: (0.5045) |  Loss2: (0.0000) | Acc: (82.00%) (18083/21888)
Epoch: 34 | Batch_idx: 180 |  Loss: (0.5043) |  Loss2: (0.0000) | Acc: (82.00%) (19153/23168)
Epoch: 34 | Batch_idx: 190 |  Loss: (0.5034) |  Loss2: (0.0000) | Acc: (82.00%) (20201/24448)
Epoch: 34 | Batch_idx: 200 |  Loss: (0.5023) |  Loss2: (0.0000) | Acc: (82.00%) (21251/25728)
Epoch: 34 | Batch_idx: 210 |  Loss: (0.5018) |  Loss2: (0.0000) | Acc: (82.00%) (22313/27008)
Epoch: 34 | Batch_idx: 220 |  Loss: (0.5025) |  Loss2: (0.0000) | Acc: (82.00%) (23366/28288)
Epoch: 34 | Batch_idx: 230 |  Loss: (0.5016) |  Loss2: (0.0000) | Acc: (82.00%) (24426/29568)
Epoch: 34 | Batch_idx: 240 |  Loss: (0.5012) |  Loss2: (0.0000) | Acc: (82.00%) (25479/30848)
Epoch: 34 | Batch_idx: 250 |  Loss: (0.5020) |  Loss2: (0.0000) | Acc: (82.00%) (26532/32128)
Epoch: 34 | Batch_idx: 260 |  Loss: (0.5007) |  Loss2: (0.0000) | Acc: (82.00%) (27612/33408)
Epoch: 34 | Batch_idx: 270 |  Loss: (0.5016) |  Loss2: (0.0000) | Acc: (82.00%) (28664/34688)
Epoch: 34 | Batch_idx: 280 |  Loss: (0.5022) |  Loss2: (0.0000) | Acc: (82.00%) (29698/35968)
Epoch: 34 | Batch_idx: 290 |  Loss: (0.5027) |  Loss2: (0.0000) | Acc: (82.00%) (30737/37248)
Epoch: 34 | Batch_idx: 300 |  Loss: (0.5031) |  Loss2: (0.0000) | Acc: (82.00%) (31797/38528)
Epoch: 34 | Batch_idx: 310 |  Loss: (0.5046) |  Loss2: (0.0000) | Acc: (82.00%) (32836/39808)
Epoch: 34 | Batch_idx: 320 |  Loss: (0.5047) |  Loss2: (0.0000) | Acc: (82.00%) (33893/41088)
Epoch: 34 | Batch_idx: 330 |  Loss: (0.5041) |  Loss2: (0.0000) | Acc: (82.00%) (34970/42368)
Epoch: 34 | Batch_idx: 340 |  Loss: (0.5047) |  Loss2: (0.0000) | Acc: (82.00%) (36017/43648)
Epoch: 34 | Batch_idx: 350 |  Loss: (0.5038) |  Loss2: (0.0000) | Acc: (82.00%) (37083/44928)
Epoch: 34 | Batch_idx: 360 |  Loss: (0.5017) |  Loss2: (0.0000) | Acc: (82.00%) (38173/46208)
Epoch: 34 | Batch_idx: 370 |  Loss: (0.5007) |  Loss2: (0.0000) | Acc: (82.00%) (39264/47488)
Epoch: 34 | Batch_idx: 380 |  Loss: (0.5002) |  Loss2: (0.0000) | Acc: (82.00%) (40322/48768)
Epoch: 34 | Batch_idx: 390 |  Loss: (0.5008) |  Loss2: (0.0000) | Acc: (82.00%) (41327/50000)
# TEST : Loss: (0.6236) | Acc: (79.00%) (7939/10000)
percent tensor([0.5248, 0.4752], device='cuda:0')
percent tensor([0.5244, 0.4756], device='cuda:0')
percent tensor([0.4935, 0.5065], device='cuda:0')
percent tensor([0.5830, 0.4170], device='cuda:0')
percent tensor([0.5463, 0.4537], device='cuda:0')
percent tensor([0.5591, 0.4409], device='cuda:0')
percent tensor([0.5649, 0.4351], device='cuda:0')
percent tensor([0.9946, 0.0054], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 35 | Batch_idx: 0 |  Loss: (0.3281) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 35 | Batch_idx: 10 |  Loss: (0.4981) |  Loss2: (0.0000) | Acc: (82.00%) (1167/1408)
Epoch: 35 | Batch_idx: 20 |  Loss: (0.5668) |  Loss2: (0.0000) | Acc: (80.00%) (2168/2688)
Epoch: 35 | Batch_idx: 30 |  Loss: (0.5962) |  Loss2: (0.0000) | Acc: (79.00%) (3156/3968)
Epoch: 35 | Batch_idx: 40 |  Loss: (0.6079) |  Loss2: (0.0000) | Acc: (78.00%) (4142/5248)
Epoch: 35 | Batch_idx: 50 |  Loss: (0.6194) |  Loss2: (0.0000) | Acc: (78.00%) (5130/6528)
Epoch: 35 | Batch_idx: 60 |  Loss: (0.6265) |  Loss2: (0.0000) | Acc: (78.00%) (6114/7808)
Epoch: 35 | Batch_idx: 70 |  Loss: (0.6389) |  Loss2: (0.0000) | Acc: (77.00%) (7080/9088)
Epoch: 35 | Batch_idx: 80 |  Loss: (0.6450) |  Loss2: (0.0000) | Acc: (77.00%) (8066/10368)
Epoch: 35 | Batch_idx: 90 |  Loss: (0.6518) |  Loss2: (0.0000) | Acc: (77.00%) (9023/11648)
Epoch: 35 | Batch_idx: 100 |  Loss: (0.6544) |  Loss2: (0.0000) | Acc: (77.00%) (10001/12928)
Epoch: 35 | Batch_idx: 110 |  Loss: (0.6593) |  Loss2: (0.0000) | Acc: (77.00%) (10964/14208)
Epoch: 35 | Batch_idx: 120 |  Loss: (0.6609) |  Loss2: (0.0000) | Acc: (77.00%) (11949/15488)
Epoch: 35 | Batch_idx: 130 |  Loss: (0.6592) |  Loss2: (0.0000) | Acc: (77.00%) (12956/16768)
Epoch: 35 | Batch_idx: 140 |  Loss: (0.6551) |  Loss2: (0.0000) | Acc: (77.00%) (13958/18048)
Epoch: 35 | Batch_idx: 150 |  Loss: (0.6546) |  Loss2: (0.0000) | Acc: (77.00%) (14938/19328)
Epoch: 35 | Batch_idx: 160 |  Loss: (0.6519) |  Loss2: (0.0000) | Acc: (77.00%) (15945/20608)
Epoch: 35 | Batch_idx: 170 |  Loss: (0.6496) |  Loss2: (0.0000) | Acc: (77.00%) (16952/21888)
Epoch: 35 | Batch_idx: 180 |  Loss: (0.6452) |  Loss2: (0.0000) | Acc: (77.00%) (17967/23168)
Epoch: 35 | Batch_idx: 190 |  Loss: (0.6438) |  Loss2: (0.0000) | Acc: (77.00%) (18976/24448)
Epoch: 35 | Batch_idx: 200 |  Loss: (0.6393) |  Loss2: (0.0000) | Acc: (77.00%) (20015/25728)
Epoch: 35 | Batch_idx: 210 |  Loss: (0.6379) |  Loss2: (0.0000) | Acc: (77.00%) (21025/27008)
Epoch: 35 | Batch_idx: 220 |  Loss: (0.6358) |  Loss2: (0.0000) | Acc: (77.00%) (22048/28288)
Epoch: 35 | Batch_idx: 230 |  Loss: (0.6347) |  Loss2: (0.0000) | Acc: (77.00%) (23060/29568)
Epoch: 35 | Batch_idx: 240 |  Loss: (0.6336) |  Loss2: (0.0000) | Acc: (78.00%) (24065/30848)
Epoch: 35 | Batch_idx: 250 |  Loss: (0.6330) |  Loss2: (0.0000) | Acc: (78.00%) (25072/32128)
Epoch: 35 | Batch_idx: 260 |  Loss: (0.6312) |  Loss2: (0.0000) | Acc: (78.00%) (26086/33408)
Epoch: 35 | Batch_idx: 270 |  Loss: (0.6292) |  Loss2: (0.0000) | Acc: (78.00%) (27116/34688)
Epoch: 35 | Batch_idx: 280 |  Loss: (0.6272) |  Loss2: (0.0000) | Acc: (78.00%) (28136/35968)
Epoch: 35 | Batch_idx: 290 |  Loss: (0.6265) |  Loss2: (0.0000) | Acc: (78.00%) (29143/37248)
Epoch: 35 | Batch_idx: 300 |  Loss: (0.6237) |  Loss2: (0.0000) | Acc: (78.00%) (30180/38528)
Epoch: 35 | Batch_idx: 310 |  Loss: (0.6224) |  Loss2: (0.0000) | Acc: (78.00%) (31200/39808)
Epoch: 35 | Batch_idx: 320 |  Loss: (0.6223) |  Loss2: (0.0000) | Acc: (78.00%) (32216/41088)
Epoch: 35 | Batch_idx: 330 |  Loss: (0.6203) |  Loss2: (0.0000) | Acc: (78.00%) (33243/42368)
Epoch: 35 | Batch_idx: 340 |  Loss: (0.6196) |  Loss2: (0.0000) | Acc: (78.00%) (34260/43648)
Epoch: 35 | Batch_idx: 350 |  Loss: (0.6180) |  Loss2: (0.0000) | Acc: (78.00%) (35289/44928)
Epoch: 35 | Batch_idx: 360 |  Loss: (0.6158) |  Loss2: (0.0000) | Acc: (78.00%) (36302/46208)
Epoch: 35 | Batch_idx: 370 |  Loss: (0.6149) |  Loss2: (0.0000) | Acc: (78.00%) (37325/47488)
Epoch: 35 | Batch_idx: 380 |  Loss: (0.6138) |  Loss2: (0.0000) | Acc: (78.00%) (38356/48768)
Epoch: 35 | Batch_idx: 390 |  Loss: (0.6124) |  Loss2: (0.0000) | Acc: (78.00%) (39335/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_035.pth.tar'
# TEST : Loss: (0.6350) | Acc: (78.00%) (7832/10000)
percent tensor([0.5497, 0.4503], device='cuda:0')
percent tensor([0.5262, 0.4738], device='cuda:0')
percent tensor([0.4891, 0.5109], device='cuda:0')
percent tensor([0.5738, 0.4262], device='cuda:0')
percent tensor([0.5351, 0.4649], device='cuda:0')
percent tensor([0.5611, 0.4389], device='cuda:0')
percent tensor([0.5335, 0.4665], device='cuda:0')
percent tensor([0.9943, 0.0057], device='cuda:0')
Epoch: 36 | Batch_idx: 0 |  Loss: (0.6135) |  Loss2: (0.0000) | Acc: (78.00%) (100/128)
Epoch: 36 | Batch_idx: 10 |  Loss: (0.5732) |  Loss2: (0.0000) | Acc: (79.00%) (1126/1408)
Epoch: 36 | Batch_idx: 20 |  Loss: (0.5720) |  Loss2: (0.0000) | Acc: (79.00%) (2144/2688)
Epoch: 36 | Batch_idx: 30 |  Loss: (0.5772) |  Loss2: (0.0000) | Acc: (79.00%) (3155/3968)
Epoch: 36 | Batch_idx: 40 |  Loss: (0.5679) |  Loss2: (0.0000) | Acc: (80.00%) (4199/5248)
Epoch: 36 | Batch_idx: 50 |  Loss: (0.5624) |  Loss2: (0.0000) | Acc: (80.00%) (5237/6528)
Epoch: 36 | Batch_idx: 60 |  Loss: (0.5611) |  Loss2: (0.0000) | Acc: (80.00%) (6267/7808)
Epoch: 36 | Batch_idx: 70 |  Loss: (0.5583) |  Loss2: (0.0000) | Acc: (80.00%) (7312/9088)
Epoch: 36 | Batch_idx: 80 |  Loss: (0.5630) |  Loss2: (0.0000) | Acc: (80.00%) (8321/10368)
Epoch: 36 | Batch_idx: 90 |  Loss: (0.5627) |  Loss2: (0.0000) | Acc: (80.00%) (9355/11648)
Epoch: 36 | Batch_idx: 100 |  Loss: (0.5673) |  Loss2: (0.0000) | Acc: (80.00%) (10375/12928)
Epoch: 36 | Batch_idx: 110 |  Loss: (0.5684) |  Loss2: (0.0000) | Acc: (80.00%) (11402/14208)
Epoch: 36 | Batch_idx: 120 |  Loss: (0.5635) |  Loss2: (0.0000) | Acc: (80.00%) (12456/15488)
Epoch: 36 | Batch_idx: 130 |  Loss: (0.5648) |  Loss2: (0.0000) | Acc: (80.00%) (13465/16768)
Epoch: 36 | Batch_idx: 140 |  Loss: (0.5625) |  Loss2: (0.0000) | Acc: (80.00%) (14501/18048)
Epoch: 36 | Batch_idx: 150 |  Loss: (0.5594) |  Loss2: (0.0000) | Acc: (80.00%) (15551/19328)
Epoch: 36 | Batch_idx: 160 |  Loss: (0.5584) |  Loss2: (0.0000) | Acc: (80.00%) (16599/20608)
Epoch: 36 | Batch_idx: 170 |  Loss: (0.5571) |  Loss2: (0.0000) | Acc: (80.00%) (17649/21888)
Epoch: 36 | Batch_idx: 180 |  Loss: (0.5568) |  Loss2: (0.0000) | Acc: (80.00%) (18677/23168)
Epoch: 36 | Batch_idx: 190 |  Loss: (0.5582) |  Loss2: (0.0000) | Acc: (80.00%) (19711/24448)
Epoch: 36 | Batch_idx: 200 |  Loss: (0.5591) |  Loss2: (0.0000) | Acc: (80.00%) (20747/25728)
Epoch: 36 | Batch_idx: 210 |  Loss: (0.5569) |  Loss2: (0.0000) | Acc: (80.00%) (21796/27008)
Epoch: 36 | Batch_idx: 220 |  Loss: (0.5541) |  Loss2: (0.0000) | Acc: (80.00%) (22854/28288)
Epoch: 36 | Batch_idx: 230 |  Loss: (0.5516) |  Loss2: (0.0000) | Acc: (80.00%) (23919/29568)
Epoch: 36 | Batch_idx: 240 |  Loss: (0.5510) |  Loss2: (0.0000) | Acc: (80.00%) (24962/30848)
Epoch: 36 | Batch_idx: 250 |  Loss: (0.5494) |  Loss2: (0.0000) | Acc: (80.00%) (26017/32128)
Epoch: 36 | Batch_idx: 260 |  Loss: (0.5501) |  Loss2: (0.0000) | Acc: (80.00%) (27043/33408)
Epoch: 36 | Batch_idx: 270 |  Loss: (0.5499) |  Loss2: (0.0000) | Acc: (80.00%) (28067/34688)
Epoch: 36 | Batch_idx: 280 |  Loss: (0.5479) |  Loss2: (0.0000) | Acc: (80.00%) (29125/35968)
Epoch: 36 | Batch_idx: 290 |  Loss: (0.5471) |  Loss2: (0.0000) | Acc: (80.00%) (30168/37248)
Epoch: 36 | Batch_idx: 300 |  Loss: (0.5466) |  Loss2: (0.0000) | Acc: (81.00%) (31215/38528)
Epoch: 36 | Batch_idx: 310 |  Loss: (0.5471) |  Loss2: (0.0000) | Acc: (81.00%) (32255/39808)
Epoch: 36 | Batch_idx: 320 |  Loss: (0.5457) |  Loss2: (0.0000) | Acc: (81.00%) (33311/41088)
Epoch: 36 | Batch_idx: 330 |  Loss: (0.5448) |  Loss2: (0.0000) | Acc: (81.00%) (34360/42368)
Epoch: 36 | Batch_idx: 340 |  Loss: (0.5453) |  Loss2: (0.0000) | Acc: (81.00%) (35392/43648)
Epoch: 36 | Batch_idx: 350 |  Loss: (0.5455) |  Loss2: (0.0000) | Acc: (81.00%) (36425/44928)
Epoch: 36 | Batch_idx: 360 |  Loss: (0.5459) |  Loss2: (0.0000) | Acc: (81.00%) (37450/46208)
Epoch: 36 | Batch_idx: 370 |  Loss: (0.5451) |  Loss2: (0.0000) | Acc: (81.00%) (38509/47488)
Epoch: 36 | Batch_idx: 380 |  Loss: (0.5446) |  Loss2: (0.0000) | Acc: (81.00%) (39550/48768)
Epoch: 36 | Batch_idx: 390 |  Loss: (0.5432) |  Loss2: (0.0000) | Acc: (81.00%) (40582/50000)
# TEST : Loss: (0.5855) | Acc: (79.00%) (7989/10000)
percent tensor([0.5512, 0.4488], device='cuda:0')
percent tensor([0.5272, 0.4728], device='cuda:0')
percent tensor([0.4906, 0.5094], device='cuda:0')
percent tensor([0.5707, 0.4293], device='cuda:0')
percent tensor([0.5322, 0.4678], device='cuda:0')
percent tensor([0.5686, 0.4314], device='cuda:0')
percent tensor([0.5386, 0.4614], device='cuda:0')
percent tensor([0.9954, 0.0046], device='cuda:0')
Epoch: 37 | Batch_idx: 0 |  Loss: (0.4772) |  Loss2: (0.0000) | Acc: (84.00%) (108/128)
Epoch: 37 | Batch_idx: 10 |  Loss: (0.5307) |  Loss2: (0.0000) | Acc: (81.00%) (1144/1408)
Epoch: 37 | Batch_idx: 20 |  Loss: (0.5274) |  Loss2: (0.0000) | Acc: (81.00%) (2179/2688)
Epoch: 37 | Batch_idx: 30 |  Loss: (0.5175) |  Loss2: (0.0000) | Acc: (81.00%) (3249/3968)
Epoch: 37 | Batch_idx: 40 |  Loss: (0.5199) |  Loss2: (0.0000) | Acc: (81.00%) (4299/5248)
Epoch: 37 | Batch_idx: 50 |  Loss: (0.5200) |  Loss2: (0.0000) | Acc: (82.00%) (5354/6528)
Epoch: 37 | Batch_idx: 60 |  Loss: (0.5220) |  Loss2: (0.0000) | Acc: (82.00%) (6407/7808)
Epoch: 37 | Batch_idx: 70 |  Loss: (0.5231) |  Loss2: (0.0000) | Acc: (81.00%) (7439/9088)
Epoch: 37 | Batch_idx: 80 |  Loss: (0.5260) |  Loss2: (0.0000) | Acc: (81.00%) (8481/10368)
Epoch: 37 | Batch_idx: 90 |  Loss: (0.5237) |  Loss2: (0.0000) | Acc: (81.00%) (9538/11648)
Epoch: 37 | Batch_idx: 100 |  Loss: (0.5191) |  Loss2: (0.0000) | Acc: (82.00%) (10602/12928)
Epoch: 37 | Batch_idx: 110 |  Loss: (0.5190) |  Loss2: (0.0000) | Acc: (81.00%) (11643/14208)
Epoch: 37 | Batch_idx: 120 |  Loss: (0.5189) |  Loss2: (0.0000) | Acc: (82.00%) (12707/15488)
Epoch: 37 | Batch_idx: 130 |  Loss: (0.5198) |  Loss2: (0.0000) | Acc: (82.00%) (13759/16768)
Epoch: 37 | Batch_idx: 140 |  Loss: (0.5209) |  Loss2: (0.0000) | Acc: (82.00%) (14814/18048)
Epoch: 37 | Batch_idx: 150 |  Loss: (0.5219) |  Loss2: (0.0000) | Acc: (81.00%) (15838/19328)
Epoch: 37 | Batch_idx: 160 |  Loss: (0.5216) |  Loss2: (0.0000) | Acc: (81.00%) (16876/20608)
Epoch: 37 | Batch_idx: 170 |  Loss: (0.5204) |  Loss2: (0.0000) | Acc: (81.00%) (17937/21888)
Epoch: 37 | Batch_idx: 180 |  Loss: (0.5215) |  Loss2: (0.0000) | Acc: (81.00%) (18982/23168)
Epoch: 37 | Batch_idx: 190 |  Loss: (0.5200) |  Loss2: (0.0000) | Acc: (81.00%) (20040/24448)
Epoch: 37 | Batch_idx: 200 |  Loss: (0.5178) |  Loss2: (0.0000) | Acc: (82.00%) (21106/25728)
Epoch: 37 | Batch_idx: 210 |  Loss: (0.5176) |  Loss2: (0.0000) | Acc: (82.00%) (22152/27008)
Epoch: 37 | Batch_idx: 220 |  Loss: (0.5186) |  Loss2: (0.0000) | Acc: (82.00%) (23202/28288)
Epoch: 37 | Batch_idx: 230 |  Loss: (0.5187) |  Loss2: (0.0000) | Acc: (82.00%) (24248/29568)
Epoch: 37 | Batch_idx: 240 |  Loss: (0.5190) |  Loss2: (0.0000) | Acc: (81.00%) (25288/30848)
Epoch: 37 | Batch_idx: 250 |  Loss: (0.5190) |  Loss2: (0.0000) | Acc: (81.00%) (26333/32128)
Epoch: 37 | Batch_idx: 260 |  Loss: (0.5196) |  Loss2: (0.0000) | Acc: (81.00%) (27369/33408)
Epoch: 37 | Batch_idx: 270 |  Loss: (0.5184) |  Loss2: (0.0000) | Acc: (81.00%) (28427/34688)
Epoch: 37 | Batch_idx: 280 |  Loss: (0.5187) |  Loss2: (0.0000) | Acc: (81.00%) (29489/35968)
Epoch: 37 | Batch_idx: 290 |  Loss: (0.5184) |  Loss2: (0.0000) | Acc: (82.00%) (30547/37248)
Epoch: 37 | Batch_idx: 300 |  Loss: (0.5190) |  Loss2: (0.0000) | Acc: (82.00%) (31598/38528)
Epoch: 37 | Batch_idx: 310 |  Loss: (0.5183) |  Loss2: (0.0000) | Acc: (82.00%) (32656/39808)
Epoch: 37 | Batch_idx: 320 |  Loss: (0.5177) |  Loss2: (0.0000) | Acc: (82.00%) (33714/41088)
Epoch: 37 | Batch_idx: 330 |  Loss: (0.5175) |  Loss2: (0.0000) | Acc: (82.00%) (34783/42368)
Epoch: 37 | Batch_idx: 340 |  Loss: (0.5174) |  Loss2: (0.0000) | Acc: (82.00%) (35822/43648)
Epoch: 37 | Batch_idx: 350 |  Loss: (0.5171) |  Loss2: (0.0000) | Acc: (82.00%) (36885/44928)
Epoch: 37 | Batch_idx: 360 |  Loss: (0.5176) |  Loss2: (0.0000) | Acc: (82.00%) (37928/46208)
Epoch: 37 | Batch_idx: 370 |  Loss: (0.5173) |  Loss2: (0.0000) | Acc: (82.00%) (38982/47488)
Epoch: 37 | Batch_idx: 380 |  Loss: (0.5177) |  Loss2: (0.0000) | Acc: (82.00%) (40022/48768)
Epoch: 37 | Batch_idx: 390 |  Loss: (0.5181) |  Loss2: (0.0000) | Acc: (82.00%) (41019/50000)
# TEST : Loss: (0.5674) | Acc: (80.00%) (8063/10000)
percent tensor([0.5491, 0.4509], device='cuda:0')
percent tensor([0.5265, 0.4735], device='cuda:0')
percent tensor([0.4924, 0.5076], device='cuda:0')
percent tensor([0.5691, 0.4309], device='cuda:0')
percent tensor([0.5335, 0.4665], device='cuda:0')
percent tensor([0.5764, 0.4236], device='cuda:0')
percent tensor([0.5532, 0.4468], device='cuda:0')
percent tensor([0.9964, 0.0036], device='cuda:0')
Epoch: 38 | Batch_idx: 0 |  Loss: (0.3551) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 38 | Batch_idx: 10 |  Loss: (0.4639) |  Loss2: (0.0000) | Acc: (84.00%) (1189/1408)
Epoch: 38 | Batch_idx: 20 |  Loss: (0.4941) |  Loss2: (0.0000) | Acc: (83.00%) (2232/2688)
Epoch: 38 | Batch_idx: 30 |  Loss: (0.5083) |  Loss2: (0.0000) | Acc: (82.00%) (3277/3968)
Epoch: 38 | Batch_idx: 40 |  Loss: (0.4972) |  Loss2: (0.0000) | Acc: (83.00%) (4357/5248)
Epoch: 38 | Batch_idx: 50 |  Loss: (0.4990) |  Loss2: (0.0000) | Acc: (82.00%) (5407/6528)
Epoch: 38 | Batch_idx: 60 |  Loss: (0.5002) |  Loss2: (0.0000) | Acc: (82.00%) (6450/7808)
Epoch: 38 | Batch_idx: 70 |  Loss: (0.5012) |  Loss2: (0.0000) | Acc: (82.00%) (7499/9088)
Epoch: 38 | Batch_idx: 80 |  Loss: (0.5018) |  Loss2: (0.0000) | Acc: (82.00%) (8567/10368)
Epoch: 38 | Batch_idx: 90 |  Loss: (0.5030) |  Loss2: (0.0000) | Acc: (82.00%) (9619/11648)
Epoch: 38 | Batch_idx: 100 |  Loss: (0.5033) |  Loss2: (0.0000) | Acc: (82.00%) (10669/12928)
Epoch: 38 | Batch_idx: 110 |  Loss: (0.5049) |  Loss2: (0.0000) | Acc: (82.00%) (11711/14208)
Epoch: 38 | Batch_idx: 120 |  Loss: (0.5095) |  Loss2: (0.0000) | Acc: (82.00%) (12736/15488)
Epoch: 38 | Batch_idx: 130 |  Loss: (0.5096) |  Loss2: (0.0000) | Acc: (82.00%) (13791/16768)
Epoch: 38 | Batch_idx: 140 |  Loss: (0.5098) |  Loss2: (0.0000) | Acc: (82.00%) (14846/18048)
Epoch: 38 | Batch_idx: 150 |  Loss: (0.5071) |  Loss2: (0.0000) | Acc: (82.00%) (15931/19328)
Epoch: 38 | Batch_idx: 160 |  Loss: (0.5066) |  Loss2: (0.0000) | Acc: (82.00%) (16981/20608)
Epoch: 38 | Batch_idx: 170 |  Loss: (0.5050) |  Loss2: (0.0000) | Acc: (82.00%) (18040/21888)
Epoch: 38 | Batch_idx: 180 |  Loss: (0.5041) |  Loss2: (0.0000) | Acc: (82.00%) (19108/23168)
Epoch: 38 | Batch_idx: 190 |  Loss: (0.5029) |  Loss2: (0.0000) | Acc: (82.00%) (20186/24448)
Epoch: 38 | Batch_idx: 200 |  Loss: (0.5042) |  Loss2: (0.0000) | Acc: (82.00%) (21241/25728)
Epoch: 38 | Batch_idx: 210 |  Loss: (0.5021) |  Loss2: (0.0000) | Acc: (82.00%) (22333/27008)
Epoch: 38 | Batch_idx: 220 |  Loss: (0.5024) |  Loss2: (0.0000) | Acc: (82.00%) (23380/28288)
Epoch: 38 | Batch_idx: 230 |  Loss: (0.5015) |  Loss2: (0.0000) | Acc: (82.00%) (24437/29568)
Epoch: 38 | Batch_idx: 240 |  Loss: (0.5012) |  Loss2: (0.0000) | Acc: (82.00%) (25505/30848)
Epoch: 38 | Batch_idx: 250 |  Loss: (0.5010) |  Loss2: (0.0000) | Acc: (82.00%) (26569/32128)
Epoch: 38 | Batch_idx: 260 |  Loss: (0.5022) |  Loss2: (0.0000) | Acc: (82.00%) (27608/33408)
Epoch: 38 | Batch_idx: 270 |  Loss: (0.5028) |  Loss2: (0.0000) | Acc: (82.00%) (28664/34688)
Epoch: 38 | Batch_idx: 280 |  Loss: (0.5025) |  Loss2: (0.0000) | Acc: (82.00%) (29725/35968)
Epoch: 38 | Batch_idx: 290 |  Loss: (0.5032) |  Loss2: (0.0000) | Acc: (82.00%) (30777/37248)
Epoch: 38 | Batch_idx: 300 |  Loss: (0.5029) |  Loss2: (0.0000) | Acc: (82.00%) (31829/38528)
Epoch: 38 | Batch_idx: 310 |  Loss: (0.5037) |  Loss2: (0.0000) | Acc: (82.00%) (32874/39808)
Epoch: 38 | Batch_idx: 320 |  Loss: (0.5058) |  Loss2: (0.0000) | Acc: (82.00%) (33902/41088)
Epoch: 38 | Batch_idx: 330 |  Loss: (0.5053) |  Loss2: (0.0000) | Acc: (82.00%) (34970/42368)
Epoch: 38 | Batch_idx: 340 |  Loss: (0.5040) |  Loss2: (0.0000) | Acc: (82.00%) (36057/43648)
Epoch: 38 | Batch_idx: 350 |  Loss: (0.5028) |  Loss2: (0.0000) | Acc: (82.00%) (37138/44928)
Epoch: 38 | Batch_idx: 360 |  Loss: (0.5021) |  Loss2: (0.0000) | Acc: (82.00%) (38212/46208)
Epoch: 38 | Batch_idx: 370 |  Loss: (0.5023) |  Loss2: (0.0000) | Acc: (82.00%) (39255/47488)
Epoch: 38 | Batch_idx: 380 |  Loss: (0.5021) |  Loss2: (0.0000) | Acc: (82.00%) (40320/48768)
Epoch: 38 | Batch_idx: 390 |  Loss: (0.5031) |  Loss2: (0.0000) | Acc: (82.00%) (41317/50000)
# TEST : Loss: (0.5557) | Acc: (81.00%) (8103/10000)
percent tensor([0.5498, 0.4502], device='cuda:0')
percent tensor([0.5288, 0.4712], device='cuda:0')
percent tensor([0.4949, 0.5051], device='cuda:0')
percent tensor([0.5684, 0.4316], device='cuda:0')
percent tensor([0.5357, 0.4643], device='cuda:0')
percent tensor([0.5780, 0.4220], device='cuda:0')
percent tensor([0.5668, 0.4332], device='cuda:0')
percent tensor([0.9969, 0.0031], device='cuda:0')
Epoch: 39 | Batch_idx: 0 |  Loss: (0.5606) |  Loss2: (0.0000) | Acc: (83.00%) (107/128)
Epoch: 39 | Batch_idx: 10 |  Loss: (0.5062) |  Loss2: (0.0000) | Acc: (83.00%) (1172/1408)
Epoch: 39 | Batch_idx: 20 |  Loss: (0.5016) |  Loss2: (0.0000) | Acc: (83.00%) (2235/2688)
Epoch: 39 | Batch_idx: 30 |  Loss: (0.4939) |  Loss2: (0.0000) | Acc: (83.00%) (3296/3968)
Epoch: 39 | Batch_idx: 40 |  Loss: (0.5011) |  Loss2: (0.0000) | Acc: (82.00%) (4331/5248)
Epoch: 39 | Batch_idx: 50 |  Loss: (0.4940) |  Loss2: (0.0000) | Acc: (83.00%) (5424/6528)
Epoch: 39 | Batch_idx: 60 |  Loss: (0.4892) |  Loss2: (0.0000) | Acc: (83.00%) (6510/7808)
Epoch: 39 | Batch_idx: 70 |  Loss: (0.4947) |  Loss2: (0.0000) | Acc: (83.00%) (7556/9088)
Epoch: 39 | Batch_idx: 80 |  Loss: (0.4924) |  Loss2: (0.0000) | Acc: (83.00%) (8626/10368)
Epoch: 39 | Batch_idx: 90 |  Loss: (0.4926) |  Loss2: (0.0000) | Acc: (83.00%) (9695/11648)
Epoch: 39 | Batch_idx: 100 |  Loss: (0.4932) |  Loss2: (0.0000) | Acc: (83.00%) (10754/12928)
Epoch: 39 | Batch_idx: 110 |  Loss: (0.4966) |  Loss2: (0.0000) | Acc: (83.00%) (11801/14208)
Epoch: 39 | Batch_idx: 120 |  Loss: (0.4965) |  Loss2: (0.0000) | Acc: (83.00%) (12870/15488)
Epoch: 39 | Batch_idx: 130 |  Loss: (0.4984) |  Loss2: (0.0000) | Acc: (82.00%) (13908/16768)
Epoch: 39 | Batch_idx: 140 |  Loss: (0.4995) |  Loss2: (0.0000) | Acc: (82.00%) (14968/18048)
Epoch: 39 | Batch_idx: 150 |  Loss: (0.4986) |  Loss2: (0.0000) | Acc: (82.00%) (16040/19328)
Epoch: 39 | Batch_idx: 160 |  Loss: (0.4989) |  Loss2: (0.0000) | Acc: (82.00%) (17091/20608)
Epoch: 39 | Batch_idx: 170 |  Loss: (0.4993) |  Loss2: (0.0000) | Acc: (82.00%) (18156/21888)
Epoch: 39 | Batch_idx: 180 |  Loss: (0.4979) |  Loss2: (0.0000) | Acc: (82.00%) (19221/23168)
Epoch: 39 | Batch_idx: 190 |  Loss: (0.4984) |  Loss2: (0.0000) | Acc: (82.00%) (20264/24448)
Epoch: 39 | Batch_idx: 200 |  Loss: (0.4984) |  Loss2: (0.0000) | Acc: (82.00%) (21324/25728)
Epoch: 39 | Batch_idx: 210 |  Loss: (0.4975) |  Loss2: (0.0000) | Acc: (82.00%) (22387/27008)
Epoch: 39 | Batch_idx: 220 |  Loss: (0.4977) |  Loss2: (0.0000) | Acc: (82.00%) (23451/28288)
Epoch: 39 | Batch_idx: 230 |  Loss: (0.4980) |  Loss2: (0.0000) | Acc: (82.00%) (24515/29568)
Epoch: 39 | Batch_idx: 240 |  Loss: (0.4976) |  Loss2: (0.0000) | Acc: (82.00%) (25576/30848)
Epoch: 39 | Batch_idx: 250 |  Loss: (0.4954) |  Loss2: (0.0000) | Acc: (82.00%) (26657/32128)
Epoch: 39 | Batch_idx: 260 |  Loss: (0.4945) |  Loss2: (0.0000) | Acc: (83.00%) (27729/33408)
Epoch: 39 | Batch_idx: 270 |  Loss: (0.4961) |  Loss2: (0.0000) | Acc: (82.00%) (28758/34688)
Epoch: 39 | Batch_idx: 280 |  Loss: (0.4972) |  Loss2: (0.0000) | Acc: (82.00%) (29811/35968)
Epoch: 39 | Batch_idx: 290 |  Loss: (0.4961) |  Loss2: (0.0000) | Acc: (82.00%) (30880/37248)
Epoch: 39 | Batch_idx: 300 |  Loss: (0.4963) |  Loss2: (0.0000) | Acc: (82.00%) (31934/38528)
Epoch: 39 | Batch_idx: 310 |  Loss: (0.4979) |  Loss2: (0.0000) | Acc: (82.00%) (32980/39808)
Epoch: 39 | Batch_idx: 320 |  Loss: (0.4978) |  Loss2: (0.0000) | Acc: (82.00%) (34036/41088)
Epoch: 39 | Batch_idx: 330 |  Loss: (0.4963) |  Loss2: (0.0000) | Acc: (82.00%) (35115/42368)
Epoch: 39 | Batch_idx: 340 |  Loss: (0.4961) |  Loss2: (0.0000) | Acc: (82.00%) (36187/43648)
Epoch: 39 | Batch_idx: 350 |  Loss: (0.4956) |  Loss2: (0.0000) | Acc: (82.00%) (37257/44928)
Epoch: 39 | Batch_idx: 360 |  Loss: (0.4966) |  Loss2: (0.0000) | Acc: (82.00%) (38294/46208)
Epoch: 39 | Batch_idx: 370 |  Loss: (0.4961) |  Loss2: (0.0000) | Acc: (82.00%) (39365/47488)
Epoch: 39 | Batch_idx: 380 |  Loss: (0.4968) |  Loss2: (0.0000) | Acc: (82.00%) (40416/48768)
Epoch: 39 | Batch_idx: 390 |  Loss: (0.4972) |  Loss2: (0.0000) | Acc: (82.00%) (41427/50000)
# TEST : Loss: (0.5510) | Acc: (81.00%) (8119/10000)
percent tensor([0.5499, 0.4501], device='cuda:0')
percent tensor([0.5288, 0.4712], device='cuda:0')
percent tensor([0.4967, 0.5033], device='cuda:0')
percent tensor([0.5674, 0.4326], device='cuda:0')
percent tensor([0.5362, 0.4638], device='cuda:0')
percent tensor([0.5789, 0.4211], device='cuda:0')
percent tensor([0.5810, 0.4190], device='cuda:0')
percent tensor([0.9975, 0.0025], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 40 | Batch_idx: 0 |  Loss: (0.4998) |  Loss2: (0.0000) | Acc: (81.00%) (104/128)
Epoch: 40 | Batch_idx: 10 |  Loss: (0.5040) |  Loss2: (0.0000) | Acc: (81.00%) (1154/1408)
Epoch: 40 | Batch_idx: 20 |  Loss: (0.5040) |  Loss2: (0.0000) | Acc: (82.00%) (2217/2688)
Epoch: 40 | Batch_idx: 30 |  Loss: (0.5030) |  Loss2: (0.0000) | Acc: (82.00%) (3278/3968)
Epoch: 40 | Batch_idx: 40 |  Loss: (0.5050) |  Loss2: (0.0000) | Acc: (82.00%) (4335/5248)
Epoch: 40 | Batch_idx: 50 |  Loss: (0.5034) |  Loss2: (0.0000) | Acc: (82.00%) (5395/6528)
Epoch: 40 | Batch_idx: 60 |  Loss: (0.5015) |  Loss2: (0.0000) | Acc: (82.00%) (6456/7808)
Epoch: 40 | Batch_idx: 70 |  Loss: (0.4977) |  Loss2: (0.0000) | Acc: (82.00%) (7535/9088)
Epoch: 40 | Batch_idx: 80 |  Loss: (0.4979) |  Loss2: (0.0000) | Acc: (82.00%) (8594/10368)
Epoch: 40 | Batch_idx: 90 |  Loss: (0.4926) |  Loss2: (0.0000) | Acc: (83.00%) (9677/11648)
Epoch: 40 | Batch_idx: 100 |  Loss: (0.4900) |  Loss2: (0.0000) | Acc: (83.00%) (10761/12928)
Epoch: 40 | Batch_idx: 110 |  Loss: (0.4928) |  Loss2: (0.0000) | Acc: (83.00%) (11818/14208)
Epoch: 40 | Batch_idx: 120 |  Loss: (0.4923) |  Loss2: (0.0000) | Acc: (83.00%) (12879/15488)
Epoch: 40 | Batch_idx: 130 |  Loss: (0.4926) |  Loss2: (0.0000) | Acc: (83.00%) (13944/16768)
Epoch: 40 | Batch_idx: 140 |  Loss: (0.4927) |  Loss2: (0.0000) | Acc: (83.00%) (14996/18048)
Epoch: 40 | Batch_idx: 150 |  Loss: (0.4897) |  Loss2: (0.0000) | Acc: (83.00%) (16089/19328)
Epoch: 40 | Batch_idx: 160 |  Loss: (0.4928) |  Loss2: (0.0000) | Acc: (83.00%) (17139/20608)
Epoch: 40 | Batch_idx: 170 |  Loss: (0.4899) |  Loss2: (0.0000) | Acc: (83.00%) (18231/21888)
Epoch: 40 | Batch_idx: 180 |  Loss: (0.4911) |  Loss2: (0.0000) | Acc: (83.00%) (19281/23168)
Epoch: 40 | Batch_idx: 190 |  Loss: (0.4907) |  Loss2: (0.0000) | Acc: (83.00%) (20354/24448)
Epoch: 40 | Batch_idx: 200 |  Loss: (0.4916) |  Loss2: (0.0000) | Acc: (83.00%) (21406/25728)
Epoch: 40 | Batch_idx: 210 |  Loss: (0.4910) |  Loss2: (0.0000) | Acc: (83.00%) (22475/27008)
Epoch: 40 | Batch_idx: 220 |  Loss: (0.4927) |  Loss2: (0.0000) | Acc: (83.00%) (23505/28288)
Epoch: 40 | Batch_idx: 230 |  Loss: (0.4934) |  Loss2: (0.0000) | Acc: (83.00%) (24556/29568)
Epoch: 40 | Batch_idx: 240 |  Loss: (0.4942) |  Loss2: (0.0000) | Acc: (83.00%) (25606/30848)
Epoch: 40 | Batch_idx: 250 |  Loss: (0.4944) |  Loss2: (0.0000) | Acc: (82.00%) (26661/32128)
Epoch: 40 | Batch_idx: 260 |  Loss: (0.4942) |  Loss2: (0.0000) | Acc: (83.00%) (27734/33408)
Epoch: 40 | Batch_idx: 270 |  Loss: (0.4935) |  Loss2: (0.0000) | Acc: (83.00%) (28809/34688)
Epoch: 40 | Batch_idx: 280 |  Loss: (0.4938) |  Loss2: (0.0000) | Acc: (83.00%) (29865/35968)
Epoch: 40 | Batch_idx: 290 |  Loss: (0.4940) |  Loss2: (0.0000) | Acc: (83.00%) (30928/37248)
Epoch: 40 | Batch_idx: 300 |  Loss: (0.4942) |  Loss2: (0.0000) | Acc: (83.00%) (31989/38528)
Epoch: 40 | Batch_idx: 310 |  Loss: (0.4953) |  Loss2: (0.0000) | Acc: (83.00%) (33044/39808)
Epoch: 40 | Batch_idx: 320 |  Loss: (0.4952) |  Loss2: (0.0000) | Acc: (83.00%) (34106/41088)
Epoch: 40 | Batch_idx: 330 |  Loss: (0.4951) |  Loss2: (0.0000) | Acc: (83.00%) (35171/42368)
Epoch: 40 | Batch_idx: 340 |  Loss: (0.4944) |  Loss2: (0.0000) | Acc: (83.00%) (36242/43648)
Epoch: 40 | Batch_idx: 350 |  Loss: (0.4941) |  Loss2: (0.0000) | Acc: (83.00%) (37306/44928)
Epoch: 40 | Batch_idx: 360 |  Loss: (0.4943) |  Loss2: (0.0000) | Acc: (83.00%) (38363/46208)
Epoch: 40 | Batch_idx: 370 |  Loss: (0.4929) |  Loss2: (0.0000) | Acc: (83.00%) (39442/47488)
Epoch: 40 | Batch_idx: 380 |  Loss: (0.4924) |  Loss2: (0.0000) | Acc: (83.00%) (40521/48768)
Epoch: 40 | Batch_idx: 390 |  Loss: (0.4919) |  Loss2: (0.0000) | Acc: (83.00%) (41555/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_040.pth.tar'
# TEST : Loss: (0.6020) | Acc: (80.00%) (8005/10000)
percent tensor([0.5508, 0.4492], device='cuda:0')
percent tensor([0.5293, 0.4707], device='cuda:0')
percent tensor([0.4967, 0.5033], device='cuda:0')
percent tensor([0.5673, 0.4327], device='cuda:0')
percent tensor([0.5363, 0.4637], device='cuda:0')
percent tensor([0.5776, 0.4224], device='cuda:0')
percent tensor([0.5795, 0.4205], device='cuda:0')
percent tensor([0.9973, 0.0027], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(173.4489, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(793.3569, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(788.5699, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1528.4121, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(503.2297, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2192.6248, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4304.7378, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1423.8434, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6090.1562, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12094.0361, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4024.0537, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(17052.0371, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 41 | Batch_idx: 0 |  Loss: (0.4702) |  Loss2: (0.0000) | Acc: (82.00%) (106/128)
Epoch: 41 | Batch_idx: 10 |  Loss: (0.4559) |  Loss2: (0.0000) | Acc: (84.00%) (1189/1408)
Epoch: 41 | Batch_idx: 20 |  Loss: (0.4662) |  Loss2: (0.0000) | Acc: (83.00%) (2253/2688)
Epoch: 41 | Batch_idx: 30 |  Loss: (0.4700) |  Loss2: (0.0000) | Acc: (83.00%) (3322/3968)
Epoch: 41 | Batch_idx: 40 |  Loss: (0.4749) |  Loss2: (0.0000) | Acc: (83.00%) (4375/5248)
Epoch: 41 | Batch_idx: 50 |  Loss: (0.4741) |  Loss2: (0.0000) | Acc: (83.00%) (5440/6528)
Epoch: 41 | Batch_idx: 60 |  Loss: (0.4770) |  Loss2: (0.0000) | Acc: (83.00%) (6507/7808)
Epoch: 41 | Batch_idx: 70 |  Loss: (0.4734) |  Loss2: (0.0000) | Acc: (83.00%) (7590/9088)
Epoch: 41 | Batch_idx: 80 |  Loss: (0.4750) |  Loss2: (0.0000) | Acc: (83.00%) (8653/10368)
Epoch: 41 | Batch_idx: 90 |  Loss: (0.4779) |  Loss2: (0.0000) | Acc: (83.00%) (9717/11648)
Epoch: 41 | Batch_idx: 100 |  Loss: (0.4746) |  Loss2: (0.0000) | Acc: (83.00%) (10802/12928)
Epoch: 41 | Batch_idx: 110 |  Loss: (0.4758) |  Loss2: (0.0000) | Acc: (83.00%) (11867/14208)
Epoch: 41 | Batch_idx: 120 |  Loss: (0.4779) |  Loss2: (0.0000) | Acc: (83.00%) (12929/15488)
Epoch: 41 | Batch_idx: 130 |  Loss: (0.4765) |  Loss2: (0.0000) | Acc: (83.00%) (14004/16768)
Epoch: 41 | Batch_idx: 140 |  Loss: (0.4757) |  Loss2: (0.0000) | Acc: (83.00%) (15087/18048)
Epoch: 41 | Batch_idx: 150 |  Loss: (0.4751) |  Loss2: (0.0000) | Acc: (83.00%) (16174/19328)
Epoch: 41 | Batch_idx: 160 |  Loss: (0.4735) |  Loss2: (0.0000) | Acc: (83.00%) (17258/20608)
Epoch: 41 | Batch_idx: 170 |  Loss: (0.4742) |  Loss2: (0.0000) | Acc: (83.00%) (18325/21888)
Epoch: 41 | Batch_idx: 180 |  Loss: (0.4751) |  Loss2: (0.0000) | Acc: (83.00%) (19390/23168)
Epoch: 41 | Batch_idx: 190 |  Loss: (0.4767) |  Loss2: (0.0000) | Acc: (83.00%) (20446/24448)
Epoch: 41 | Batch_idx: 200 |  Loss: (0.4767) |  Loss2: (0.0000) | Acc: (83.00%) (21519/25728)
Epoch: 41 | Batch_idx: 210 |  Loss: (0.4773) |  Loss2: (0.0000) | Acc: (83.00%) (22589/27008)
Epoch: 41 | Batch_idx: 220 |  Loss: (0.4770) |  Loss2: (0.0000) | Acc: (83.00%) (23648/28288)
Epoch: 41 | Batch_idx: 230 |  Loss: (0.4745) |  Loss2: (0.0000) | Acc: (83.00%) (24731/29568)
Epoch: 41 | Batch_idx: 240 |  Loss: (0.4760) |  Loss2: (0.0000) | Acc: (83.00%) (25795/30848)
Epoch: 41 | Batch_idx: 250 |  Loss: (0.4757) |  Loss2: (0.0000) | Acc: (83.00%) (26862/32128)
Epoch: 41 | Batch_idx: 260 |  Loss: (0.4745) |  Loss2: (0.0000) | Acc: (83.00%) (27949/33408)
Epoch: 41 | Batch_idx: 270 |  Loss: (0.4740) |  Loss2: (0.0000) | Acc: (83.00%) (29039/34688)
Epoch: 41 | Batch_idx: 280 |  Loss: (0.4740) |  Loss2: (0.0000) | Acc: (83.00%) (30113/35968)
Epoch: 41 | Batch_idx: 290 |  Loss: (0.4741) |  Loss2: (0.0000) | Acc: (83.00%) (31175/37248)
Epoch: 41 | Batch_idx: 300 |  Loss: (0.4734) |  Loss2: (0.0000) | Acc: (83.00%) (32252/38528)
Epoch: 41 | Batch_idx: 310 |  Loss: (0.4760) |  Loss2: (0.0000) | Acc: (83.00%) (33281/39808)
Epoch: 41 | Batch_idx: 320 |  Loss: (0.4762) |  Loss2: (0.0000) | Acc: (83.00%) (34340/41088)
Epoch: 41 | Batch_idx: 330 |  Loss: (0.4761) |  Loss2: (0.0000) | Acc: (83.00%) (35404/42368)
Epoch: 41 | Batch_idx: 340 |  Loss: (0.4758) |  Loss2: (0.0000) | Acc: (83.00%) (36481/43648)
Epoch: 41 | Batch_idx: 350 |  Loss: (0.4754) |  Loss2: (0.0000) | Acc: (83.00%) (37563/44928)
Epoch: 41 | Batch_idx: 360 |  Loss: (0.4762) |  Loss2: (0.0000) | Acc: (83.00%) (38611/46208)
Epoch: 41 | Batch_idx: 370 |  Loss: (0.4756) |  Loss2: (0.0000) | Acc: (83.00%) (39687/47488)
Epoch: 41 | Batch_idx: 380 |  Loss: (0.4748) |  Loss2: (0.0000) | Acc: (83.00%) (40767/48768)
Epoch: 41 | Batch_idx: 390 |  Loss: (0.4744) |  Loss2: (0.0000) | Acc: (83.00%) (41811/50000)
# TEST : Loss: (0.5882) | Acc: (80.00%) (8053/10000)
percent tensor([0.5499, 0.4501], device='cuda:0')
percent tensor([0.5291, 0.4709], device='cuda:0')
percent tensor([0.4965, 0.5035], device='cuda:0')
percent tensor([0.5667, 0.4333], device='cuda:0')
percent tensor([0.5365, 0.4635], device='cuda:0')
percent tensor([0.5785, 0.4215], device='cuda:0')
percent tensor([0.5815, 0.4185], device='cuda:0')
percent tensor([0.9980, 0.0020], device='cuda:0')
Epoch: 42 | Batch_idx: 0 |  Loss: (0.5118) |  Loss2: (0.0000) | Acc: (84.00%) (108/128)
Epoch: 42 | Batch_idx: 10 |  Loss: (0.4789) |  Loss2: (0.0000) | Acc: (83.00%) (1180/1408)
Epoch: 42 | Batch_idx: 20 |  Loss: (0.4641) |  Loss2: (0.0000) | Acc: (84.00%) (2262/2688)
Epoch: 42 | Batch_idx: 30 |  Loss: (0.4632) |  Loss2: (0.0000) | Acc: (84.00%) (3342/3968)
Epoch: 42 | Batch_idx: 40 |  Loss: (0.4596) |  Loss2: (0.0000) | Acc: (84.00%) (4428/5248)
Epoch: 42 | Batch_idx: 50 |  Loss: (0.4552) |  Loss2: (0.0000) | Acc: (84.00%) (5509/6528)
Epoch: 42 | Batch_idx: 60 |  Loss: (0.4590) |  Loss2: (0.0000) | Acc: (84.00%) (6572/7808)
Epoch: 42 | Batch_idx: 70 |  Loss: (0.4594) |  Loss2: (0.0000) | Acc: (84.00%) (7649/9088)
Epoch: 42 | Batch_idx: 80 |  Loss: (0.4610) |  Loss2: (0.0000) | Acc: (84.00%) (8723/10368)
Epoch: 42 | Batch_idx: 90 |  Loss: (0.4607) |  Loss2: (0.0000) | Acc: (84.00%) (9798/11648)
Epoch: 42 | Batch_idx: 100 |  Loss: (0.4604) |  Loss2: (0.0000) | Acc: (84.00%) (10887/12928)
Epoch: 42 | Batch_idx: 110 |  Loss: (0.4584) |  Loss2: (0.0000) | Acc: (84.00%) (11980/14208)
Epoch: 42 | Batch_idx: 120 |  Loss: (0.4577) |  Loss2: (0.0000) | Acc: (84.00%) (13055/15488)
Epoch: 42 | Batch_idx: 130 |  Loss: (0.4547) |  Loss2: (0.0000) | Acc: (84.00%) (14159/16768)
Epoch: 42 | Batch_idx: 140 |  Loss: (0.4549) |  Loss2: (0.0000) | Acc: (84.00%) (15242/18048)
Epoch: 42 | Batch_idx: 150 |  Loss: (0.4553) |  Loss2: (0.0000) | Acc: (84.00%) (16319/19328)
Epoch: 42 | Batch_idx: 160 |  Loss: (0.4579) |  Loss2: (0.0000) | Acc: (84.00%) (17381/20608)
Epoch: 42 | Batch_idx: 170 |  Loss: (0.4570) |  Loss2: (0.0000) | Acc: (84.00%) (18458/21888)
Epoch: 42 | Batch_idx: 180 |  Loss: (0.4566) |  Loss2: (0.0000) | Acc: (84.00%) (19552/23168)
Epoch: 42 | Batch_idx: 190 |  Loss: (0.4556) |  Loss2: (0.0000) | Acc: (84.00%) (20630/24448)
Epoch: 42 | Batch_idx: 200 |  Loss: (0.4571) |  Loss2: (0.0000) | Acc: (84.00%) (21702/25728)
Epoch: 42 | Batch_idx: 210 |  Loss: (0.4589) |  Loss2: (0.0000) | Acc: (84.00%) (22775/27008)
Epoch: 42 | Batch_idx: 220 |  Loss: (0.4581) |  Loss2: (0.0000) | Acc: (84.00%) (23865/28288)
Epoch: 42 | Batch_idx: 230 |  Loss: (0.4583) |  Loss2: (0.0000) | Acc: (84.00%) (24939/29568)
Epoch: 42 | Batch_idx: 240 |  Loss: (0.4566) |  Loss2: (0.0000) | Acc: (84.00%) (26032/30848)
Epoch: 42 | Batch_idx: 250 |  Loss: (0.4576) |  Loss2: (0.0000) | Acc: (84.00%) (27102/32128)
Epoch: 42 | Batch_idx: 260 |  Loss: (0.4583) |  Loss2: (0.0000) | Acc: (84.00%) (28160/33408)
Epoch: 42 | Batch_idx: 270 |  Loss: (0.4582) |  Loss2: (0.0000) | Acc: (84.00%) (29220/34688)
Epoch: 42 | Batch_idx: 280 |  Loss: (0.4588) |  Loss2: (0.0000) | Acc: (84.00%) (30293/35968)
Epoch: 42 | Batch_idx: 290 |  Loss: (0.4585) |  Loss2: (0.0000) | Acc: (84.00%) (31374/37248)
Epoch: 42 | Batch_idx: 300 |  Loss: (0.4576) |  Loss2: (0.0000) | Acc: (84.00%) (32470/38528)
Epoch: 42 | Batch_idx: 310 |  Loss: (0.4558) |  Loss2: (0.0000) | Acc: (84.00%) (33562/39808)
Epoch: 42 | Batch_idx: 320 |  Loss: (0.4565) |  Loss2: (0.0000) | Acc: (84.00%) (34628/41088)
Epoch: 42 | Batch_idx: 330 |  Loss: (0.4574) |  Loss2: (0.0000) | Acc: (84.00%) (35696/42368)
Epoch: 42 | Batch_idx: 340 |  Loss: (0.4579) |  Loss2: (0.0000) | Acc: (84.00%) (36756/43648)
Epoch: 42 | Batch_idx: 350 |  Loss: (0.4584) |  Loss2: (0.0000) | Acc: (84.00%) (37824/44928)
Epoch: 42 | Batch_idx: 360 |  Loss: (0.4589) |  Loss2: (0.0000) | Acc: (84.00%) (38898/46208)
Epoch: 42 | Batch_idx: 370 |  Loss: (0.4571) |  Loss2: (0.0000) | Acc: (84.00%) (40004/47488)
Epoch: 42 | Batch_idx: 380 |  Loss: (0.4573) |  Loss2: (0.0000) | Acc: (84.00%) (41077/48768)
Epoch: 42 | Batch_idx: 390 |  Loss: (0.4574) |  Loss2: (0.0000) | Acc: (84.00%) (42121/50000)
# TEST : Loss: (0.5733) | Acc: (80.00%) (8080/10000)
percent tensor([0.5497, 0.4503], device='cuda:0')
percent tensor([0.5292, 0.4708], device='cuda:0')
percent tensor([0.4967, 0.5033], device='cuda:0')
percent tensor([0.5666, 0.4334], device='cuda:0')
percent tensor([0.5368, 0.4632], device='cuda:0')
percent tensor([0.5775, 0.4225], device='cuda:0')
percent tensor([0.5825, 0.4175], device='cuda:0')
percent tensor([0.9974, 0.0026], device='cuda:0')
Epoch: 43 | Batch_idx: 0 |  Loss: (0.4114) |  Loss2: (0.0000) | Acc: (83.00%) (107/128)
Epoch: 43 | Batch_idx: 10 |  Loss: (0.4375) |  Loss2: (0.0000) | Acc: (84.00%) (1185/1408)
Epoch: 43 | Batch_idx: 20 |  Loss: (0.4330) |  Loss2: (0.0000) | Acc: (84.00%) (2282/2688)
Epoch: 43 | Batch_idx: 30 |  Loss: (0.4414) |  Loss2: (0.0000) | Acc: (84.00%) (3356/3968)
Epoch: 43 | Batch_idx: 40 |  Loss: (0.4386) |  Loss2: (0.0000) | Acc: (84.00%) (4450/5248)
Epoch: 43 | Batch_idx: 50 |  Loss: (0.4386) |  Loss2: (0.0000) | Acc: (84.00%) (5529/6528)
Epoch: 43 | Batch_idx: 60 |  Loss: (0.4348) |  Loss2: (0.0000) | Acc: (84.00%) (6624/7808)
Epoch: 43 | Batch_idx: 70 |  Loss: (0.4317) |  Loss2: (0.0000) | Acc: (85.00%) (7725/9088)
Epoch: 43 | Batch_idx: 80 |  Loss: (0.4303) |  Loss2: (0.0000) | Acc: (85.00%) (8829/10368)
Epoch: 43 | Batch_idx: 90 |  Loss: (0.4366) |  Loss2: (0.0000) | Acc: (84.00%) (9894/11648)
Epoch: 43 | Batch_idx: 100 |  Loss: (0.4401) |  Loss2: (0.0000) | Acc: (84.00%) (10955/12928)
Epoch: 43 | Batch_idx: 110 |  Loss: (0.4428) |  Loss2: (0.0000) | Acc: (84.00%) (12026/14208)
Epoch: 43 | Batch_idx: 120 |  Loss: (0.4440) |  Loss2: (0.0000) | Acc: (84.00%) (13106/15488)
Epoch: 43 | Batch_idx: 130 |  Loss: (0.4403) |  Loss2: (0.0000) | Acc: (84.00%) (14206/16768)
Epoch: 43 | Batch_idx: 140 |  Loss: (0.4414) |  Loss2: (0.0000) | Acc: (84.00%) (15272/18048)
Epoch: 43 | Batch_idx: 150 |  Loss: (0.4407) |  Loss2: (0.0000) | Acc: (84.00%) (16365/19328)
Epoch: 43 | Batch_idx: 160 |  Loss: (0.4406) |  Loss2: (0.0000) | Acc: (84.00%) (17453/20608)
Epoch: 43 | Batch_idx: 170 |  Loss: (0.4389) |  Loss2: (0.0000) | Acc: (84.00%) (18549/21888)
Epoch: 43 | Batch_idx: 180 |  Loss: (0.4373) |  Loss2: (0.0000) | Acc: (84.00%) (19643/23168)
Epoch: 43 | Batch_idx: 190 |  Loss: (0.4359) |  Loss2: (0.0000) | Acc: (84.00%) (20749/24448)
Epoch: 43 | Batch_idx: 200 |  Loss: (0.4337) |  Loss2: (0.0000) | Acc: (84.00%) (21859/25728)
Epoch: 43 | Batch_idx: 210 |  Loss: (0.4351) |  Loss2: (0.0000) | Acc: (84.00%) (22932/27008)
Epoch: 43 | Batch_idx: 220 |  Loss: (0.4377) |  Loss2: (0.0000) | Acc: (84.00%) (24003/28288)
Epoch: 43 | Batch_idx: 230 |  Loss: (0.4394) |  Loss2: (0.0000) | Acc: (84.00%) (25083/29568)
Epoch: 43 | Batch_idx: 240 |  Loss: (0.4411) |  Loss2: (0.0000) | Acc: (84.00%) (26146/30848)
Epoch: 43 | Batch_idx: 250 |  Loss: (0.4402) |  Loss2: (0.0000) | Acc: (84.00%) (27241/32128)
Epoch: 43 | Batch_idx: 260 |  Loss: (0.4416) |  Loss2: (0.0000) | Acc: (84.00%) (28323/33408)
Epoch: 43 | Batch_idx: 270 |  Loss: (0.4411) |  Loss2: (0.0000) | Acc: (84.00%) (29422/34688)
Epoch: 43 | Batch_idx: 280 |  Loss: (0.4422) |  Loss2: (0.0000) | Acc: (84.00%) (30497/35968)
Epoch: 43 | Batch_idx: 290 |  Loss: (0.4429) |  Loss2: (0.0000) | Acc: (84.00%) (31573/37248)
Epoch: 43 | Batch_idx: 300 |  Loss: (0.4426) |  Loss2: (0.0000) | Acc: (84.00%) (32651/38528)
Epoch: 43 | Batch_idx: 310 |  Loss: (0.4420) |  Loss2: (0.0000) | Acc: (84.00%) (33728/39808)
Epoch: 43 | Batch_idx: 320 |  Loss: (0.4413) |  Loss2: (0.0000) | Acc: (84.00%) (34823/41088)
Epoch: 43 | Batch_idx: 330 |  Loss: (0.4409) |  Loss2: (0.0000) | Acc: (84.00%) (35904/42368)
Epoch: 43 | Batch_idx: 340 |  Loss: (0.4413) |  Loss2: (0.0000) | Acc: (84.00%) (36996/43648)
Epoch: 43 | Batch_idx: 350 |  Loss: (0.4409) |  Loss2: (0.0000) | Acc: (84.00%) (38097/44928)
Epoch: 43 | Batch_idx: 360 |  Loss: (0.4410) |  Loss2: (0.0000) | Acc: (84.00%) (39184/46208)
Epoch: 43 | Batch_idx: 370 |  Loss: (0.4411) |  Loss2: (0.0000) | Acc: (84.00%) (40265/47488)
Epoch: 43 | Batch_idx: 380 |  Loss: (0.4408) |  Loss2: (0.0000) | Acc: (84.00%) (41355/48768)
Epoch: 43 | Batch_idx: 390 |  Loss: (0.4416) |  Loss2: (0.0000) | Acc: (84.00%) (42384/50000)
# TEST : Loss: (0.6232) | Acc: (78.00%) (7898/10000)
percent tensor([0.5500, 0.4500], device='cuda:0')
percent tensor([0.5297, 0.4703], device='cuda:0')
percent tensor([0.4968, 0.5032], device='cuda:0')
percent tensor([0.5668, 0.4332], device='cuda:0')
percent tensor([0.5380, 0.4620], device='cuda:0')
percent tensor([0.5801, 0.4199], device='cuda:0')
percent tensor([0.5876, 0.4124], device='cuda:0')
percent tensor([0.9984, 0.0016], device='cuda:0')
Epoch: 44 | Batch_idx: 0 |  Loss: (0.5431) |  Loss2: (0.0000) | Acc: (78.00%) (101/128)
Epoch: 44 | Batch_idx: 10 |  Loss: (0.4590) |  Loss2: (0.0000) | Acc: (84.00%) (1187/1408)
Epoch: 44 | Batch_idx: 20 |  Loss: (0.4506) |  Loss2: (0.0000) | Acc: (84.00%) (2269/2688)
Epoch: 44 | Batch_idx: 30 |  Loss: (0.4401) |  Loss2: (0.0000) | Acc: (84.00%) (3354/3968)
Epoch: 44 | Batch_idx: 40 |  Loss: (0.4449) |  Loss2: (0.0000) | Acc: (84.00%) (4432/5248)
Epoch: 44 | Batch_idx: 50 |  Loss: (0.4378) |  Loss2: (0.0000) | Acc: (85.00%) (5551/6528)
Epoch: 44 | Batch_idx: 60 |  Loss: (0.4398) |  Loss2: (0.0000) | Acc: (85.00%) (6640/7808)
Epoch: 44 | Batch_idx: 70 |  Loss: (0.4472) |  Loss2: (0.0000) | Acc: (84.00%) (7703/9088)
Epoch: 44 | Batch_idx: 80 |  Loss: (0.4416) |  Loss2: (0.0000) | Acc: (85.00%) (8814/10368)
Epoch: 44 | Batch_idx: 90 |  Loss: (0.4363) |  Loss2: (0.0000) | Acc: (85.00%) (9914/11648)
Epoch: 44 | Batch_idx: 100 |  Loss: (0.4388) |  Loss2: (0.0000) | Acc: (85.00%) (10990/12928)
Epoch: 44 | Batch_idx: 110 |  Loss: (0.4367) |  Loss2: (0.0000) | Acc: (85.00%) (12091/14208)
Epoch: 44 | Batch_idx: 120 |  Loss: (0.4370) |  Loss2: (0.0000) | Acc: (85.00%) (13182/15488)
Epoch: 44 | Batch_idx: 130 |  Loss: (0.4363) |  Loss2: (0.0000) | Acc: (85.00%) (14271/16768)
Epoch: 44 | Batch_idx: 140 |  Loss: (0.4350) |  Loss2: (0.0000) | Acc: (85.00%) (15373/18048)
Epoch: 44 | Batch_idx: 150 |  Loss: (0.4344) |  Loss2: (0.0000) | Acc: (85.00%) (16459/19328)
Epoch: 44 | Batch_idx: 160 |  Loss: (0.4356) |  Loss2: (0.0000) | Acc: (85.00%) (17541/20608)
Epoch: 44 | Batch_idx: 170 |  Loss: (0.4353) |  Loss2: (0.0000) | Acc: (85.00%) (18631/21888)
Epoch: 44 | Batch_idx: 180 |  Loss: (0.4334) |  Loss2: (0.0000) | Acc: (85.00%) (19727/23168)
Epoch: 44 | Batch_idx: 190 |  Loss: (0.4331) |  Loss2: (0.0000) | Acc: (85.00%) (20821/24448)
Epoch: 44 | Batch_idx: 200 |  Loss: (0.4318) |  Loss2: (0.0000) | Acc: (85.00%) (21930/25728)
Epoch: 44 | Batch_idx: 210 |  Loss: (0.4334) |  Loss2: (0.0000) | Acc: (85.00%) (23001/27008)
Epoch: 44 | Batch_idx: 220 |  Loss: (0.4324) |  Loss2: (0.0000) | Acc: (85.00%) (24088/28288)
Epoch: 44 | Batch_idx: 230 |  Loss: (0.4315) |  Loss2: (0.0000) | Acc: (85.00%) (25191/29568)
Epoch: 44 | Batch_idx: 240 |  Loss: (0.4308) |  Loss2: (0.0000) | Acc: (85.00%) (26298/30848)
Epoch: 44 | Batch_idx: 250 |  Loss: (0.4300) |  Loss2: (0.0000) | Acc: (85.00%) (27404/32128)
Epoch: 44 | Batch_idx: 260 |  Loss: (0.4301) |  Loss2: (0.0000) | Acc: (85.00%) (28500/33408)
Epoch: 44 | Batch_idx: 270 |  Loss: (0.4307) |  Loss2: (0.0000) | Acc: (85.00%) (29576/34688)
Epoch: 44 | Batch_idx: 280 |  Loss: (0.4301) |  Loss2: (0.0000) | Acc: (85.00%) (30674/35968)
Epoch: 44 | Batch_idx: 290 |  Loss: (0.4300) |  Loss2: (0.0000) | Acc: (85.00%) (31765/37248)
Epoch: 44 | Batch_idx: 300 |  Loss: (0.4297) |  Loss2: (0.0000) | Acc: (85.00%) (32859/38528)
Epoch: 44 | Batch_idx: 310 |  Loss: (0.4289) |  Loss2: (0.0000) | Acc: (85.00%) (33958/39808)
Epoch: 44 | Batch_idx: 320 |  Loss: (0.4294) |  Loss2: (0.0000) | Acc: (85.00%) (35044/41088)
Epoch: 44 | Batch_idx: 330 |  Loss: (0.4285) |  Loss2: (0.0000) | Acc: (85.00%) (36153/42368)
Epoch: 44 | Batch_idx: 340 |  Loss: (0.4286) |  Loss2: (0.0000) | Acc: (85.00%) (37238/43648)
Epoch: 44 | Batch_idx: 350 |  Loss: (0.4295) |  Loss2: (0.0000) | Acc: (85.00%) (38309/44928)
Epoch: 44 | Batch_idx: 360 |  Loss: (0.4289) |  Loss2: (0.0000) | Acc: (85.00%) (39410/46208)
Epoch: 44 | Batch_idx: 370 |  Loss: (0.4282) |  Loss2: (0.0000) | Acc: (85.00%) (40513/47488)
Epoch: 44 | Batch_idx: 380 |  Loss: (0.4280) |  Loss2: (0.0000) | Acc: (85.00%) (41615/48768)
Epoch: 44 | Batch_idx: 390 |  Loss: (0.4269) |  Loss2: (0.0000) | Acc: (85.00%) (42679/50000)
# TEST : Loss: (0.5610) | Acc: (81.00%) (8144/10000)
percent tensor([0.5500, 0.4500], device='cuda:0')
percent tensor([0.5298, 0.4702], device='cuda:0')
percent tensor([0.4966, 0.5034], device='cuda:0')
percent tensor([0.5660, 0.4340], device='cuda:0')
percent tensor([0.5370, 0.4630], device='cuda:0')
percent tensor([0.5773, 0.4227], device='cuda:0')
percent tensor([0.5820, 0.4180], device='cuda:0')
percent tensor([0.9977, 0.0023], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 45 | Batch_idx: 0 |  Loss: (0.4219) |  Loss2: (0.0000) | Acc: (81.00%) (104/128)
Epoch: 45 | Batch_idx: 10 |  Loss: (0.4201) |  Loss2: (0.0000) | Acc: (85.00%) (1210/1408)
Epoch: 45 | Batch_idx: 20 |  Loss: (0.4317) |  Loss2: (0.0000) | Acc: (85.00%) (2293/2688)
Epoch: 45 | Batch_idx: 30 |  Loss: (0.4359) |  Loss2: (0.0000) | Acc: (84.00%) (3363/3968)
Epoch: 45 | Batch_idx: 40 |  Loss: (0.4466) |  Loss2: (0.0000) | Acc: (84.00%) (4420/5248)
Epoch: 45 | Batch_idx: 50 |  Loss: (0.4414) |  Loss2: (0.0000) | Acc: (84.00%) (5505/6528)
Epoch: 45 | Batch_idx: 60 |  Loss: (0.4419) |  Loss2: (0.0000) | Acc: (84.00%) (6576/7808)
Epoch: 45 | Batch_idx: 70 |  Loss: (0.4452) |  Loss2: (0.0000) | Acc: (84.00%) (7652/9088)
Epoch: 45 | Batch_idx: 80 |  Loss: (0.4495) |  Loss2: (0.0000) | Acc: (84.00%) (8726/10368)
Epoch: 45 | Batch_idx: 90 |  Loss: (0.4470) |  Loss2: (0.0000) | Acc: (84.00%) (9810/11648)
Epoch: 45 | Batch_idx: 100 |  Loss: (0.4482) |  Loss2: (0.0000) | Acc: (84.00%) (10893/12928)
Epoch: 45 | Batch_idx: 110 |  Loss: (0.4485) |  Loss2: (0.0000) | Acc: (84.00%) (11969/14208)
Epoch: 45 | Batch_idx: 120 |  Loss: (0.4489) |  Loss2: (0.0000) | Acc: (84.00%) (13053/15488)
Epoch: 45 | Batch_idx: 130 |  Loss: (0.4509) |  Loss2: (0.0000) | Acc: (84.00%) (14129/16768)
Epoch: 45 | Batch_idx: 140 |  Loss: (0.4521) |  Loss2: (0.0000) | Acc: (84.00%) (15203/18048)
Epoch: 45 | Batch_idx: 150 |  Loss: (0.4523) |  Loss2: (0.0000) | Acc: (84.00%) (16271/19328)
Epoch: 45 | Batch_idx: 160 |  Loss: (0.4514) |  Loss2: (0.0000) | Acc: (84.00%) (17358/20608)
Epoch: 45 | Batch_idx: 170 |  Loss: (0.4517) |  Loss2: (0.0000) | Acc: (84.00%) (18433/21888)
Epoch: 45 | Batch_idx: 180 |  Loss: (0.4502) |  Loss2: (0.0000) | Acc: (84.00%) (19525/23168)
Epoch: 45 | Batch_idx: 190 |  Loss: (0.4516) |  Loss2: (0.0000) | Acc: (84.00%) (20596/24448)
Epoch: 45 | Batch_idx: 200 |  Loss: (0.4493) |  Loss2: (0.0000) | Acc: (84.00%) (21704/25728)
Epoch: 45 | Batch_idx: 210 |  Loss: (0.4491) |  Loss2: (0.0000) | Acc: (84.00%) (22780/27008)
Epoch: 45 | Batch_idx: 220 |  Loss: (0.4501) |  Loss2: (0.0000) | Acc: (84.00%) (23866/28288)
Epoch: 45 | Batch_idx: 230 |  Loss: (0.4512) |  Loss2: (0.0000) | Acc: (84.00%) (24931/29568)
Epoch: 45 | Batch_idx: 240 |  Loss: (0.4521) |  Loss2: (0.0000) | Acc: (84.00%) (26005/30848)
Epoch: 45 | Batch_idx: 250 |  Loss: (0.4514) |  Loss2: (0.0000) | Acc: (84.00%) (27099/32128)
Epoch: 45 | Batch_idx: 260 |  Loss: (0.4516) |  Loss2: (0.0000) | Acc: (84.00%) (28176/33408)
Epoch: 45 | Batch_idx: 270 |  Loss: (0.4504) |  Loss2: (0.0000) | Acc: (84.00%) (29283/34688)
Epoch: 45 | Batch_idx: 280 |  Loss: (0.4495) |  Loss2: (0.0000) | Acc: (84.00%) (30362/35968)
Epoch: 45 | Batch_idx: 290 |  Loss: (0.4494) |  Loss2: (0.0000) | Acc: (84.00%) (31455/37248)
Epoch: 45 | Batch_idx: 300 |  Loss: (0.4474) |  Loss2: (0.0000) | Acc: (84.00%) (32564/38528)
Epoch: 45 | Batch_idx: 310 |  Loss: (0.4465) |  Loss2: (0.0000) | Acc: (84.00%) (33649/39808)
Epoch: 45 | Batch_idx: 320 |  Loss: (0.4454) |  Loss2: (0.0000) | Acc: (84.00%) (34742/41088)
Epoch: 45 | Batch_idx: 330 |  Loss: (0.4447) |  Loss2: (0.0000) | Acc: (84.00%) (35836/42368)
Epoch: 45 | Batch_idx: 340 |  Loss: (0.4442) |  Loss2: (0.0000) | Acc: (84.00%) (36916/43648)
Epoch: 45 | Batch_idx: 350 |  Loss: (0.4451) |  Loss2: (0.0000) | Acc: (84.00%) (38009/44928)
Epoch: 45 | Batch_idx: 360 |  Loss: (0.4455) |  Loss2: (0.0000) | Acc: (84.00%) (39087/46208)
Epoch: 45 | Batch_idx: 370 |  Loss: (0.4446) |  Loss2: (0.0000) | Acc: (84.00%) (40188/47488)
Epoch: 45 | Batch_idx: 380 |  Loss: (0.4443) |  Loss2: (0.0000) | Acc: (84.00%) (41276/48768)
Epoch: 45 | Batch_idx: 390 |  Loss: (0.4443) |  Loss2: (0.0000) | Acc: (84.00%) (42319/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_045.pth.tar'
# TEST : Loss: (0.5086) | Acc: (83.00%) (8301/10000)
percent tensor([0.5559, 0.4441], device='cuda:0')
percent tensor([0.5387, 0.4613], device='cuda:0')
percent tensor([0.5005, 0.4995], device='cuda:0')
percent tensor([0.5568, 0.4432], device='cuda:0')
percent tensor([0.5459, 0.4541], device='cuda:0')
percent tensor([0.6176, 0.3824], device='cuda:0')
percent tensor([0.6007, 0.3993], device='cuda:0')
percent tensor([0.9979, 0.0021], device='cuda:0')
Epoch: 46 | Batch_idx: 0 |  Loss: (0.4533) |  Loss2: (0.0000) | Acc: (82.00%) (106/128)
Epoch: 46 | Batch_idx: 10 |  Loss: (0.4784) |  Loss2: (0.0000) | Acc: (83.00%) (1177/1408)
Epoch: 46 | Batch_idx: 20 |  Loss: (0.4675) |  Loss2: (0.0000) | Acc: (84.00%) (2262/2688)
Epoch: 46 | Batch_idx: 30 |  Loss: (0.4626) |  Loss2: (0.0000) | Acc: (83.00%) (3333/3968)
Epoch: 46 | Batch_idx: 40 |  Loss: (0.4532) |  Loss2: (0.0000) | Acc: (84.00%) (4415/5248)
Epoch: 46 | Batch_idx: 50 |  Loss: (0.4486) |  Loss2: (0.0000) | Acc: (84.00%) (5498/6528)
Epoch: 46 | Batch_idx: 60 |  Loss: (0.4427) |  Loss2: (0.0000) | Acc: (84.00%) (6595/7808)
Epoch: 46 | Batch_idx: 70 |  Loss: (0.4464) |  Loss2: (0.0000) | Acc: (84.00%) (7662/9088)
Epoch: 46 | Batch_idx: 80 |  Loss: (0.4458) |  Loss2: (0.0000) | Acc: (84.00%) (8741/10368)
Epoch: 46 | Batch_idx: 90 |  Loss: (0.4404) |  Loss2: (0.0000) | Acc: (84.00%) (9840/11648)
Epoch: 46 | Batch_idx: 100 |  Loss: (0.4354) |  Loss2: (0.0000) | Acc: (84.00%) (10939/12928)
Epoch: 46 | Batch_idx: 110 |  Loss: (0.4379) |  Loss2: (0.0000) | Acc: (84.00%) (12014/14208)
Epoch: 46 | Batch_idx: 120 |  Loss: (0.4371) |  Loss2: (0.0000) | Acc: (84.00%) (13104/15488)
Epoch: 46 | Batch_idx: 130 |  Loss: (0.4378) |  Loss2: (0.0000) | Acc: (84.00%) (14190/16768)
Epoch: 46 | Batch_idx: 140 |  Loss: (0.4348) |  Loss2: (0.0000) | Acc: (84.00%) (15305/18048)
Epoch: 46 | Batch_idx: 150 |  Loss: (0.4345) |  Loss2: (0.0000) | Acc: (84.00%) (16395/19328)
Epoch: 46 | Batch_idx: 160 |  Loss: (0.4314) |  Loss2: (0.0000) | Acc: (84.00%) (17507/20608)
Epoch: 46 | Batch_idx: 170 |  Loss: (0.4329) |  Loss2: (0.0000) | Acc: (84.00%) (18580/21888)
Epoch: 46 | Batch_idx: 180 |  Loss: (0.4321) |  Loss2: (0.0000) | Acc: (84.00%) (19682/23168)
Epoch: 46 | Batch_idx: 190 |  Loss: (0.4305) |  Loss2: (0.0000) | Acc: (85.00%) (20791/24448)
Epoch: 46 | Batch_idx: 200 |  Loss: (0.4301) |  Loss2: (0.0000) | Acc: (85.00%) (21878/25728)
Epoch: 46 | Batch_idx: 210 |  Loss: (0.4301) |  Loss2: (0.0000) | Acc: (85.00%) (22961/27008)
Epoch: 46 | Batch_idx: 220 |  Loss: (0.4293) |  Loss2: (0.0000) | Acc: (85.00%) (24067/28288)
Epoch: 46 | Batch_idx: 230 |  Loss: (0.4281) |  Loss2: (0.0000) | Acc: (85.00%) (25170/29568)
Epoch: 46 | Batch_idx: 240 |  Loss: (0.4268) |  Loss2: (0.0000) | Acc: (85.00%) (26273/30848)
Epoch: 46 | Batch_idx: 250 |  Loss: (0.4273) |  Loss2: (0.0000) | Acc: (85.00%) (27361/32128)
Epoch: 46 | Batch_idx: 260 |  Loss: (0.4262) |  Loss2: (0.0000) | Acc: (85.00%) (28470/33408)
Epoch: 46 | Batch_idx: 270 |  Loss: (0.4261) |  Loss2: (0.0000) | Acc: (85.00%) (29558/34688)
Epoch: 46 | Batch_idx: 280 |  Loss: (0.4263) |  Loss2: (0.0000) | Acc: (85.00%) (30651/35968)
Epoch: 46 | Batch_idx: 290 |  Loss: (0.4254) |  Loss2: (0.0000) | Acc: (85.00%) (31748/37248)
Epoch: 46 | Batch_idx: 300 |  Loss: (0.4247) |  Loss2: (0.0000) | Acc: (85.00%) (32850/38528)
Epoch: 46 | Batch_idx: 310 |  Loss: (0.4239) |  Loss2: (0.0000) | Acc: (85.00%) (33945/39808)
Epoch: 46 | Batch_idx: 320 |  Loss: (0.4232) |  Loss2: (0.0000) | Acc: (85.00%) (35035/41088)
Epoch: 46 | Batch_idx: 330 |  Loss: (0.4225) |  Loss2: (0.0000) | Acc: (85.00%) (36140/42368)
Epoch: 46 | Batch_idx: 340 |  Loss: (0.4206) |  Loss2: (0.0000) | Acc: (85.00%) (37254/43648)
Epoch: 46 | Batch_idx: 350 |  Loss: (0.4205) |  Loss2: (0.0000) | Acc: (85.00%) (38350/44928)
Epoch: 46 | Batch_idx: 360 |  Loss: (0.4215) |  Loss2: (0.0000) | Acc: (85.00%) (39429/46208)
Epoch: 46 | Batch_idx: 370 |  Loss: (0.4212) |  Loss2: (0.0000) | Acc: (85.00%) (40522/47488)
Epoch: 46 | Batch_idx: 380 |  Loss: (0.4208) |  Loss2: (0.0000) | Acc: (85.00%) (41615/48768)
Epoch: 46 | Batch_idx: 390 |  Loss: (0.4212) |  Loss2: (0.0000) | Acc: (85.00%) (42659/50000)
# TEST : Loss: (0.4865) | Acc: (83.00%) (8365/10000)
percent tensor([0.5554, 0.4446], device='cuda:0')
percent tensor([0.5442, 0.4558], device='cuda:0')
percent tensor([0.5032, 0.4968], device='cuda:0')
percent tensor([0.5546, 0.4454], device='cuda:0')
percent tensor([0.5538, 0.4462], device='cuda:0')
percent tensor([0.6394, 0.3606], device='cuda:0')
percent tensor([0.6172, 0.3828], device='cuda:0')
percent tensor([0.9980, 0.0020], device='cuda:0')
Epoch: 47 | Batch_idx: 0 |  Loss: (0.4421) |  Loss2: (0.0000) | Acc: (82.00%) (106/128)
Epoch: 47 | Batch_idx: 10 |  Loss: (0.4127) |  Loss2: (0.0000) | Acc: (84.00%) (1195/1408)
Epoch: 47 | Batch_idx: 20 |  Loss: (0.4077) |  Loss2: (0.0000) | Acc: (85.00%) (2301/2688)
Epoch: 47 | Batch_idx: 30 |  Loss: (0.4162) |  Loss2: (0.0000) | Acc: (85.00%) (3385/3968)
Epoch: 47 | Batch_idx: 40 |  Loss: (0.4136) |  Loss2: (0.0000) | Acc: (85.00%) (4486/5248)
Epoch: 47 | Batch_idx: 50 |  Loss: (0.4084) |  Loss2: (0.0000) | Acc: (85.00%) (5583/6528)
Epoch: 47 | Batch_idx: 60 |  Loss: (0.4057) |  Loss2: (0.0000) | Acc: (85.00%) (6690/7808)
Epoch: 47 | Batch_idx: 70 |  Loss: (0.4070) |  Loss2: (0.0000) | Acc: (85.00%) (7792/9088)
Epoch: 47 | Batch_idx: 80 |  Loss: (0.4079) |  Loss2: (0.0000) | Acc: (85.00%) (8889/10368)
Epoch: 47 | Batch_idx: 90 |  Loss: (0.4085) |  Loss2: (0.0000) | Acc: (85.00%) (9986/11648)
Epoch: 47 | Batch_idx: 100 |  Loss: (0.4056) |  Loss2: (0.0000) | Acc: (85.00%) (11104/12928)
Epoch: 47 | Batch_idx: 110 |  Loss: (0.4070) |  Loss2: (0.0000) | Acc: (85.00%) (12207/14208)
Epoch: 47 | Batch_idx: 120 |  Loss: (0.4041) |  Loss2: (0.0000) | Acc: (85.00%) (13318/15488)
Epoch: 47 | Batch_idx: 130 |  Loss: (0.4036) |  Loss2: (0.0000) | Acc: (86.00%) (14429/16768)
Epoch: 47 | Batch_idx: 140 |  Loss: (0.4051) |  Loss2: (0.0000) | Acc: (86.00%) (15524/18048)
Epoch: 47 | Batch_idx: 150 |  Loss: (0.4027) |  Loss2: (0.0000) | Acc: (86.00%) (16646/19328)
Epoch: 47 | Batch_idx: 160 |  Loss: (0.4064) |  Loss2: (0.0000) | Acc: (86.00%) (17723/20608)
Epoch: 47 | Batch_idx: 170 |  Loss: (0.4072) |  Loss2: (0.0000) | Acc: (85.00%) (18817/21888)
Epoch: 47 | Batch_idx: 180 |  Loss: (0.4080) |  Loss2: (0.0000) | Acc: (85.00%) (19910/23168)
Epoch: 47 | Batch_idx: 190 |  Loss: (0.4093) |  Loss2: (0.0000) | Acc: (85.00%) (20996/24448)
Epoch: 47 | Batch_idx: 200 |  Loss: (0.4078) |  Loss2: (0.0000) | Acc: (85.00%) (22102/25728)
Epoch: 47 | Batch_idx: 210 |  Loss: (0.4085) |  Loss2: (0.0000) | Acc: (85.00%) (23197/27008)
Epoch: 47 | Batch_idx: 220 |  Loss: (0.4074) |  Loss2: (0.0000) | Acc: (85.00%) (24314/28288)
Epoch: 47 | Batch_idx: 230 |  Loss: (0.4076) |  Loss2: (0.0000) | Acc: (85.00%) (25415/29568)
Epoch: 47 | Batch_idx: 240 |  Loss: (0.4080) |  Loss2: (0.0000) | Acc: (85.00%) (26514/30848)
Epoch: 47 | Batch_idx: 250 |  Loss: (0.4078) |  Loss2: (0.0000) | Acc: (85.00%) (27605/32128)
Epoch: 47 | Batch_idx: 260 |  Loss: (0.4087) |  Loss2: (0.0000) | Acc: (85.00%) (28689/33408)
Epoch: 47 | Batch_idx: 270 |  Loss: (0.4093) |  Loss2: (0.0000) | Acc: (85.00%) (29776/34688)
Epoch: 47 | Batch_idx: 280 |  Loss: (0.4090) |  Loss2: (0.0000) | Acc: (85.00%) (30879/35968)
Epoch: 47 | Batch_idx: 290 |  Loss: (0.4100) |  Loss2: (0.0000) | Acc: (85.00%) (31962/37248)
Epoch: 47 | Batch_idx: 300 |  Loss: (0.4100) |  Loss2: (0.0000) | Acc: (85.00%) (33061/38528)
Epoch: 47 | Batch_idx: 310 |  Loss: (0.4095) |  Loss2: (0.0000) | Acc: (85.00%) (34176/39808)
Epoch: 47 | Batch_idx: 320 |  Loss: (0.4110) |  Loss2: (0.0000) | Acc: (85.00%) (35246/41088)
Epoch: 47 | Batch_idx: 330 |  Loss: (0.4096) |  Loss2: (0.0000) | Acc: (85.00%) (36359/42368)
Epoch: 47 | Batch_idx: 340 |  Loss: (0.4102) |  Loss2: (0.0000) | Acc: (85.00%) (37456/43648)
Epoch: 47 | Batch_idx: 350 |  Loss: (0.4097) |  Loss2: (0.0000) | Acc: (85.00%) (38565/44928)
Epoch: 47 | Batch_idx: 360 |  Loss: (0.4097) |  Loss2: (0.0000) | Acc: (85.00%) (39662/46208)
Epoch: 47 | Batch_idx: 370 |  Loss: (0.4111) |  Loss2: (0.0000) | Acc: (85.00%) (40747/47488)
Epoch: 47 | Batch_idx: 380 |  Loss: (0.4108) |  Loss2: (0.0000) | Acc: (85.00%) (41853/48768)
Epoch: 47 | Batch_idx: 390 |  Loss: (0.4103) |  Loss2: (0.0000) | Acc: (85.00%) (42931/50000)
# TEST : Loss: (0.4791) | Acc: (83.00%) (8383/10000)
percent tensor([0.5537, 0.4463], device='cuda:0')
percent tensor([0.5464, 0.4536], device='cuda:0')
percent tensor([0.5045, 0.4955], device='cuda:0')
percent tensor([0.5545, 0.4455], device='cuda:0')
percent tensor([0.5533, 0.4467], device='cuda:0')
percent tensor([0.6503, 0.3497], device='cuda:0')
percent tensor([0.6292, 0.3708], device='cuda:0')
percent tensor([0.9982, 0.0018], device='cuda:0')
Epoch: 48 | Batch_idx: 0 |  Loss: (0.4919) |  Loss2: (0.0000) | Acc: (82.00%) (105/128)
Epoch: 48 | Batch_idx: 10 |  Loss: (0.4115) |  Loss2: (0.0000) | Acc: (86.00%) (1211/1408)
Epoch: 48 | Batch_idx: 20 |  Loss: (0.4053) |  Loss2: (0.0000) | Acc: (86.00%) (2317/2688)
Epoch: 48 | Batch_idx: 30 |  Loss: (0.4133) |  Loss2: (0.0000) | Acc: (85.00%) (3406/3968)
Epoch: 48 | Batch_idx: 40 |  Loss: (0.4138) |  Loss2: (0.0000) | Acc: (85.00%) (4498/5248)
Epoch: 48 | Batch_idx: 50 |  Loss: (0.4135) |  Loss2: (0.0000) | Acc: (85.00%) (5597/6528)
Epoch: 48 | Batch_idx: 60 |  Loss: (0.4106) |  Loss2: (0.0000) | Acc: (85.00%) (6708/7808)
Epoch: 48 | Batch_idx: 70 |  Loss: (0.4118) |  Loss2: (0.0000) | Acc: (85.00%) (7801/9088)
Epoch: 48 | Batch_idx: 80 |  Loss: (0.4192) |  Loss2: (0.0000) | Acc: (85.00%) (8884/10368)
Epoch: 48 | Batch_idx: 90 |  Loss: (0.4140) |  Loss2: (0.0000) | Acc: (85.00%) (10005/11648)
Epoch: 48 | Batch_idx: 100 |  Loss: (0.4104) |  Loss2: (0.0000) | Acc: (86.00%) (11123/12928)
Epoch: 48 | Batch_idx: 110 |  Loss: (0.4080) |  Loss2: (0.0000) | Acc: (86.00%) (12236/14208)
Epoch: 48 | Batch_idx: 120 |  Loss: (0.4116) |  Loss2: (0.0000) | Acc: (85.00%) (13312/15488)
Epoch: 48 | Batch_idx: 130 |  Loss: (0.4074) |  Loss2: (0.0000) | Acc: (86.00%) (14441/16768)
Epoch: 48 | Batch_idx: 140 |  Loss: (0.4054) |  Loss2: (0.0000) | Acc: (86.00%) (15551/18048)
Epoch: 48 | Batch_idx: 150 |  Loss: (0.4053) |  Loss2: (0.0000) | Acc: (86.00%) (16652/19328)
Epoch: 48 | Batch_idx: 160 |  Loss: (0.4010) |  Loss2: (0.0000) | Acc: (86.00%) (17787/20608)
Epoch: 48 | Batch_idx: 170 |  Loss: (0.3995) |  Loss2: (0.0000) | Acc: (86.00%) (18907/21888)
Epoch: 48 | Batch_idx: 180 |  Loss: (0.3989) |  Loss2: (0.0000) | Acc: (86.00%) (20017/23168)
Epoch: 48 | Batch_idx: 190 |  Loss: (0.4021) |  Loss2: (0.0000) | Acc: (86.00%) (21102/24448)
Epoch: 48 | Batch_idx: 200 |  Loss: (0.4016) |  Loss2: (0.0000) | Acc: (86.00%) (22211/25728)
Epoch: 48 | Batch_idx: 210 |  Loss: (0.4024) |  Loss2: (0.0000) | Acc: (86.00%) (23295/27008)
Epoch: 48 | Batch_idx: 220 |  Loss: (0.4017) |  Loss2: (0.0000) | Acc: (86.00%) (24405/28288)
Epoch: 48 | Batch_idx: 230 |  Loss: (0.4015) |  Loss2: (0.0000) | Acc: (86.00%) (25500/29568)
Epoch: 48 | Batch_idx: 240 |  Loss: (0.4013) |  Loss2: (0.0000) | Acc: (86.00%) (26604/30848)
Epoch: 48 | Batch_idx: 250 |  Loss: (0.4020) |  Loss2: (0.0000) | Acc: (86.00%) (27699/32128)
Epoch: 48 | Batch_idx: 260 |  Loss: (0.4020) |  Loss2: (0.0000) | Acc: (86.00%) (28815/33408)
Epoch: 48 | Batch_idx: 270 |  Loss: (0.4026) |  Loss2: (0.0000) | Acc: (86.00%) (29909/34688)
Epoch: 48 | Batch_idx: 280 |  Loss: (0.4028) |  Loss2: (0.0000) | Acc: (86.00%) (31020/35968)
Epoch: 48 | Batch_idx: 290 |  Loss: (0.4016) |  Loss2: (0.0000) | Acc: (86.00%) (32141/37248)
Epoch: 48 | Batch_idx: 300 |  Loss: (0.4017) |  Loss2: (0.0000) | Acc: (86.00%) (33246/38528)
Epoch: 48 | Batch_idx: 310 |  Loss: (0.4024) |  Loss2: (0.0000) | Acc: (86.00%) (34338/39808)
Epoch: 48 | Batch_idx: 320 |  Loss: (0.4036) |  Loss2: (0.0000) | Acc: (86.00%) (35431/41088)
Epoch: 48 | Batch_idx: 330 |  Loss: (0.4033) |  Loss2: (0.0000) | Acc: (86.00%) (36531/42368)
Epoch: 48 | Batch_idx: 340 |  Loss: (0.4032) |  Loss2: (0.0000) | Acc: (86.00%) (37637/43648)
Epoch: 48 | Batch_idx: 350 |  Loss: (0.4035) |  Loss2: (0.0000) | Acc: (86.00%) (38733/44928)
Epoch: 48 | Batch_idx: 360 |  Loss: (0.4030) |  Loss2: (0.0000) | Acc: (86.00%) (39844/46208)
Epoch: 48 | Batch_idx: 370 |  Loss: (0.4021) |  Loss2: (0.0000) | Acc: (86.00%) (40958/47488)
Epoch: 48 | Batch_idx: 380 |  Loss: (0.4022) |  Loss2: (0.0000) | Acc: (86.00%) (42069/48768)
Epoch: 48 | Batch_idx: 390 |  Loss: (0.4024) |  Loss2: (0.0000) | Acc: (86.00%) (43135/50000)
# TEST : Loss: (0.4732) | Acc: (83.00%) (8397/10000)
percent tensor([0.5575, 0.4425], device='cuda:0')
percent tensor([0.5448, 0.4552], device='cuda:0')
percent tensor([0.5061, 0.4939], device='cuda:0')
percent tensor([0.5569, 0.4431], device='cuda:0')
percent tensor([0.5521, 0.4479], device='cuda:0')
percent tensor([0.6528, 0.3472], device='cuda:0')
percent tensor([0.6395, 0.3605], device='cuda:0')
percent tensor([0.9984, 0.0016], device='cuda:0')
Epoch: 49 | Batch_idx: 0 |  Loss: (0.4175) |  Loss2: (0.0000) | Acc: (85.00%) (110/128)
Epoch: 49 | Batch_idx: 10 |  Loss: (0.3799) |  Loss2: (0.0000) | Acc: (87.00%) (1225/1408)
Epoch: 49 | Batch_idx: 20 |  Loss: (0.3870) |  Loss2: (0.0000) | Acc: (86.00%) (2328/2688)
Epoch: 49 | Batch_idx: 30 |  Loss: (0.3842) |  Loss2: (0.0000) | Acc: (86.00%) (3436/3968)
Epoch: 49 | Batch_idx: 40 |  Loss: (0.3933) |  Loss2: (0.0000) | Acc: (86.00%) (4522/5248)
Epoch: 49 | Batch_idx: 50 |  Loss: (0.3886) |  Loss2: (0.0000) | Acc: (86.00%) (5645/6528)
Epoch: 49 | Batch_idx: 60 |  Loss: (0.3915) |  Loss2: (0.0000) | Acc: (86.00%) (6751/7808)
Epoch: 49 | Batch_idx: 70 |  Loss: (0.3952) |  Loss2: (0.0000) | Acc: (86.00%) (7860/9088)
Epoch: 49 | Batch_idx: 80 |  Loss: (0.3960) |  Loss2: (0.0000) | Acc: (86.00%) (8973/10368)
Epoch: 49 | Batch_idx: 90 |  Loss: (0.3943) |  Loss2: (0.0000) | Acc: (86.00%) (10094/11648)
Epoch: 49 | Batch_idx: 100 |  Loss: (0.3955) |  Loss2: (0.0000) | Acc: (86.00%) (11188/12928)
Epoch: 49 | Batch_idx: 110 |  Loss: (0.3968) |  Loss2: (0.0000) | Acc: (86.00%) (12280/14208)
Epoch: 49 | Batch_idx: 120 |  Loss: (0.3956) |  Loss2: (0.0000) | Acc: (86.00%) (13386/15488)
Epoch: 49 | Batch_idx: 130 |  Loss: (0.3961) |  Loss2: (0.0000) | Acc: (86.00%) (14492/16768)
Epoch: 49 | Batch_idx: 140 |  Loss: (0.3960) |  Loss2: (0.0000) | Acc: (86.00%) (15594/18048)
Epoch: 49 | Batch_idx: 150 |  Loss: (0.3954) |  Loss2: (0.0000) | Acc: (86.00%) (16704/19328)
Epoch: 49 | Batch_idx: 160 |  Loss: (0.3936) |  Loss2: (0.0000) | Acc: (86.00%) (17818/20608)
Epoch: 49 | Batch_idx: 170 |  Loss: (0.3933) |  Loss2: (0.0000) | Acc: (86.00%) (18922/21888)
Epoch: 49 | Batch_idx: 180 |  Loss: (0.3944) |  Loss2: (0.0000) | Acc: (86.00%) (20009/23168)
Epoch: 49 | Batch_idx: 190 |  Loss: (0.3961) |  Loss2: (0.0000) | Acc: (86.00%) (21100/24448)
Epoch: 49 | Batch_idx: 200 |  Loss: (0.3955) |  Loss2: (0.0000) | Acc: (86.00%) (22192/25728)
Epoch: 49 | Batch_idx: 210 |  Loss: (0.3961) |  Loss2: (0.0000) | Acc: (86.00%) (23302/27008)
Epoch: 49 | Batch_idx: 220 |  Loss: (0.3946) |  Loss2: (0.0000) | Acc: (86.00%) (24423/28288)
Epoch: 49 | Batch_idx: 230 |  Loss: (0.3947) |  Loss2: (0.0000) | Acc: (86.00%) (25533/29568)
Epoch: 49 | Batch_idx: 240 |  Loss: (0.3959) |  Loss2: (0.0000) | Acc: (86.00%) (26626/30848)
Epoch: 49 | Batch_idx: 250 |  Loss: (0.3951) |  Loss2: (0.0000) | Acc: (86.00%) (27748/32128)
Epoch: 49 | Batch_idx: 260 |  Loss: (0.3958) |  Loss2: (0.0000) | Acc: (86.00%) (28846/33408)
Epoch: 49 | Batch_idx: 270 |  Loss: (0.3944) |  Loss2: (0.0000) | Acc: (86.00%) (29972/34688)
Epoch: 49 | Batch_idx: 280 |  Loss: (0.3944) |  Loss2: (0.0000) | Acc: (86.00%) (31084/35968)
Epoch: 49 | Batch_idx: 290 |  Loss: (0.3944) |  Loss2: (0.0000) | Acc: (86.00%) (32188/37248)
Epoch: 49 | Batch_idx: 300 |  Loss: (0.3937) |  Loss2: (0.0000) | Acc: (86.00%) (33312/38528)
Epoch: 49 | Batch_idx: 310 |  Loss: (0.3945) |  Loss2: (0.0000) | Acc: (86.00%) (34404/39808)
Epoch: 49 | Batch_idx: 320 |  Loss: (0.3944) |  Loss2: (0.0000) | Acc: (86.00%) (35521/41088)
Epoch: 49 | Batch_idx: 330 |  Loss: (0.3945) |  Loss2: (0.0000) | Acc: (86.00%) (36625/42368)
Epoch: 49 | Batch_idx: 340 |  Loss: (0.3958) |  Loss2: (0.0000) | Acc: (86.00%) (37723/43648)
Epoch: 49 | Batch_idx: 350 |  Loss: (0.3951) |  Loss2: (0.0000) | Acc: (86.00%) (38833/44928)
Epoch: 49 | Batch_idx: 360 |  Loss: (0.3950) |  Loss2: (0.0000) | Acc: (86.00%) (39939/46208)
Epoch: 49 | Batch_idx: 370 |  Loss: (0.3953) |  Loss2: (0.0000) | Acc: (86.00%) (41043/47488)
Epoch: 49 | Batch_idx: 380 |  Loss: (0.3959) |  Loss2: (0.0000) | Acc: (86.00%) (42140/48768)
Epoch: 49 | Batch_idx: 390 |  Loss: (0.3966) |  Loss2: (0.0000) | Acc: (86.00%) (43202/50000)
# TEST : Loss: (0.4676) | Acc: (84.00%) (8418/10000)
percent tensor([0.5567, 0.4433], device='cuda:0')
percent tensor([0.5426, 0.4574], device='cuda:0')
percent tensor([0.5072, 0.4928], device='cuda:0')
percent tensor([0.5555, 0.4445], device='cuda:0')
percent tensor([0.5552, 0.4448], device='cuda:0')
percent tensor([0.6568, 0.3432], device='cuda:0')
percent tensor([0.6465, 0.3535], device='cuda:0')
percent tensor([0.9985, 0.0015], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 50 | Batch_idx: 0 |  Loss: (0.4518) |  Loss2: (0.0000) | Acc: (81.00%) (104/128)
Epoch: 50 | Batch_idx: 10 |  Loss: (0.4067) |  Loss2: (0.0000) | Acc: (85.00%) (1202/1408)
Epoch: 50 | Batch_idx: 20 |  Loss: (0.3981) |  Loss2: (0.0000) | Acc: (86.00%) (2318/2688)
Epoch: 50 | Batch_idx: 30 |  Loss: (0.4009) |  Loss2: (0.0000) | Acc: (86.00%) (3418/3968)
Epoch: 50 | Batch_idx: 40 |  Loss: (0.4077) |  Loss2: (0.0000) | Acc: (85.00%) (4501/5248)
Epoch: 50 | Batch_idx: 50 |  Loss: (0.4119) |  Loss2: (0.0000) | Acc: (85.00%) (5590/6528)
Epoch: 50 | Batch_idx: 60 |  Loss: (0.4074) |  Loss2: (0.0000) | Acc: (85.00%) (6697/7808)
Epoch: 50 | Batch_idx: 70 |  Loss: (0.4035) |  Loss2: (0.0000) | Acc: (86.00%) (7821/9088)
Epoch: 50 | Batch_idx: 80 |  Loss: (0.4064) |  Loss2: (0.0000) | Acc: (85.00%) (8913/10368)
Epoch: 50 | Batch_idx: 90 |  Loss: (0.4086) |  Loss2: (0.0000) | Acc: (85.00%) (10001/11648)
Epoch: 50 | Batch_idx: 100 |  Loss: (0.4109) |  Loss2: (0.0000) | Acc: (85.00%) (11098/12928)
Epoch: 50 | Batch_idx: 110 |  Loss: (0.4145) |  Loss2: (0.0000) | Acc: (85.00%) (12174/14208)
Epoch: 50 | Batch_idx: 120 |  Loss: (0.4131) |  Loss2: (0.0000) | Acc: (85.00%) (13269/15488)
Epoch: 50 | Batch_idx: 130 |  Loss: (0.4095) |  Loss2: (0.0000) | Acc: (85.00%) (14381/16768)
Epoch: 50 | Batch_idx: 140 |  Loss: (0.4082) |  Loss2: (0.0000) | Acc: (85.00%) (15496/18048)
Epoch: 50 | Batch_idx: 150 |  Loss: (0.4068) |  Loss2: (0.0000) | Acc: (85.00%) (16600/19328)
Epoch: 50 | Batch_idx: 160 |  Loss: (0.4091) |  Loss2: (0.0000) | Acc: (85.00%) (17691/20608)
Epoch: 50 | Batch_idx: 170 |  Loss: (0.4103) |  Loss2: (0.0000) | Acc: (85.00%) (18773/21888)
Epoch: 50 | Batch_idx: 180 |  Loss: (0.4105) |  Loss2: (0.0000) | Acc: (85.00%) (19872/23168)
Epoch: 50 | Batch_idx: 190 |  Loss: (0.4109) |  Loss2: (0.0000) | Acc: (85.00%) (20965/24448)
Epoch: 50 | Batch_idx: 200 |  Loss: (0.4110) |  Loss2: (0.0000) | Acc: (85.00%) (22057/25728)
Epoch: 50 | Batch_idx: 210 |  Loss: (0.4100) |  Loss2: (0.0000) | Acc: (85.00%) (23169/27008)
Epoch: 50 | Batch_idx: 220 |  Loss: (0.4104) |  Loss2: (0.0000) | Acc: (85.00%) (24275/28288)
Epoch: 50 | Batch_idx: 230 |  Loss: (0.4114) |  Loss2: (0.0000) | Acc: (85.00%) (25351/29568)
Epoch: 50 | Batch_idx: 240 |  Loss: (0.4126) |  Loss2: (0.0000) | Acc: (85.00%) (26433/30848)
Epoch: 50 | Batch_idx: 250 |  Loss: (0.4130) |  Loss2: (0.0000) | Acc: (85.00%) (27525/32128)
Epoch: 50 | Batch_idx: 260 |  Loss: (0.4146) |  Loss2: (0.0000) | Acc: (85.00%) (28601/33408)
Epoch: 50 | Batch_idx: 270 |  Loss: (0.4138) |  Loss2: (0.0000) | Acc: (85.00%) (29712/34688)
Epoch: 50 | Batch_idx: 280 |  Loss: (0.4153) |  Loss2: (0.0000) | Acc: (85.00%) (30796/35968)
Epoch: 50 | Batch_idx: 290 |  Loss: (0.4159) |  Loss2: (0.0000) | Acc: (85.00%) (31876/37248)
Epoch: 50 | Batch_idx: 300 |  Loss: (0.4164) |  Loss2: (0.0000) | Acc: (85.00%) (32948/38528)
Epoch: 50 | Batch_idx: 310 |  Loss: (0.4166) |  Loss2: (0.0000) | Acc: (85.00%) (34040/39808)
Epoch: 50 | Batch_idx: 320 |  Loss: (0.4155) |  Loss2: (0.0000) | Acc: (85.00%) (35151/41088)
Epoch: 50 | Batch_idx: 330 |  Loss: (0.4159) |  Loss2: (0.0000) | Acc: (85.00%) (36248/42368)
Epoch: 50 | Batch_idx: 340 |  Loss: (0.4166) |  Loss2: (0.0000) | Acc: (85.00%) (37337/43648)
Epoch: 50 | Batch_idx: 350 |  Loss: (0.4159) |  Loss2: (0.0000) | Acc: (85.00%) (38431/44928)
Epoch: 50 | Batch_idx: 360 |  Loss: (0.4148) |  Loss2: (0.0000) | Acc: (85.00%) (39543/46208)
Epoch: 50 | Batch_idx: 370 |  Loss: (0.4145) |  Loss2: (0.0000) | Acc: (85.00%) (40634/47488)
Epoch: 50 | Batch_idx: 380 |  Loss: (0.4153) |  Loss2: (0.0000) | Acc: (85.00%) (41722/48768)
Epoch: 50 | Batch_idx: 390 |  Loss: (0.4158) |  Loss2: (0.0000) | Acc: (85.00%) (42762/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_050.pth.tar'
# TEST : Loss: (0.5325) | Acc: (82.00%) (8248/10000)
percent tensor([0.5568, 0.4432], device='cuda:0')
percent tensor([0.5430, 0.4570], device='cuda:0')
percent tensor([0.5077, 0.4923], device='cuda:0')
percent tensor([0.5555, 0.4445], device='cuda:0')
percent tensor([0.5570, 0.4430], device='cuda:0')
percent tensor([0.6584, 0.3416], device='cuda:0')
percent tensor([0.6468, 0.3532], device='cuda:0')
percent tensor([0.9985, 0.0015], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(174.7440, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(798.0910, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(793.1094, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1527.7993, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(501.6890, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2202.1140, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4300.1274, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1418.9355, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6097.2949, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12053.9443, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4008.5034, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16981.6504, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 51 | Batch_idx: 0 |  Loss: (0.3550) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 51 | Batch_idx: 10 |  Loss: (0.4036) |  Loss2: (0.0000) | Acc: (86.00%) (1212/1408)
Epoch: 51 | Batch_idx: 20 |  Loss: (0.3981) |  Loss2: (0.0000) | Acc: (86.00%) (2318/2688)
Epoch: 51 | Batch_idx: 30 |  Loss: (0.3974) |  Loss2: (0.0000) | Acc: (86.00%) (3424/3968)
Epoch: 51 | Batch_idx: 40 |  Loss: (0.3973) |  Loss2: (0.0000) | Acc: (86.00%) (4533/5248)
Epoch: 51 | Batch_idx: 50 |  Loss: (0.3991) |  Loss2: (0.0000) | Acc: (86.00%) (5635/6528)
Epoch: 51 | Batch_idx: 60 |  Loss: (0.3990) |  Loss2: (0.0000) | Acc: (86.00%) (6739/7808)
Epoch: 51 | Batch_idx: 70 |  Loss: (0.4008) |  Loss2: (0.0000) | Acc: (86.00%) (7848/9088)
Epoch: 51 | Batch_idx: 80 |  Loss: (0.3993) |  Loss2: (0.0000) | Acc: (86.00%) (8950/10368)
Epoch: 51 | Batch_idx: 90 |  Loss: (0.4039) |  Loss2: (0.0000) | Acc: (86.00%) (10030/11648)
Epoch: 51 | Batch_idx: 100 |  Loss: (0.4048) |  Loss2: (0.0000) | Acc: (86.00%) (11129/12928)
Epoch: 51 | Batch_idx: 110 |  Loss: (0.4061) |  Loss2: (0.0000) | Acc: (86.00%) (12219/14208)
Epoch: 51 | Batch_idx: 120 |  Loss: (0.4069) |  Loss2: (0.0000) | Acc: (86.00%) (13320/15488)
Epoch: 51 | Batch_idx: 130 |  Loss: (0.4059) |  Loss2: (0.0000) | Acc: (86.00%) (14427/16768)
Epoch: 51 | Batch_idx: 140 |  Loss: (0.4067) |  Loss2: (0.0000) | Acc: (85.00%) (15519/18048)
Epoch: 51 | Batch_idx: 150 |  Loss: (0.4065) |  Loss2: (0.0000) | Acc: (85.00%) (16622/19328)
Epoch: 51 | Batch_idx: 160 |  Loss: (0.4067) |  Loss2: (0.0000) | Acc: (86.00%) (17724/20608)
Epoch: 51 | Batch_idx: 170 |  Loss: (0.4066) |  Loss2: (0.0000) | Acc: (85.00%) (18821/21888)
Epoch: 51 | Batch_idx: 180 |  Loss: (0.4073) |  Loss2: (0.0000) | Acc: (85.00%) (19922/23168)
Epoch: 51 | Batch_idx: 190 |  Loss: (0.4067) |  Loss2: (0.0000) | Acc: (86.00%) (21027/24448)
Epoch: 51 | Batch_idx: 200 |  Loss: (0.4055) |  Loss2: (0.0000) | Acc: (86.00%) (22133/25728)
Epoch: 51 | Batch_idx: 210 |  Loss: (0.4061) |  Loss2: (0.0000) | Acc: (86.00%) (23227/27008)
Epoch: 51 | Batch_idx: 220 |  Loss: (0.4055) |  Loss2: (0.0000) | Acc: (86.00%) (24342/28288)
Epoch: 51 | Batch_idx: 230 |  Loss: (0.4046) |  Loss2: (0.0000) | Acc: (86.00%) (25445/29568)
Epoch: 51 | Batch_idx: 240 |  Loss: (0.4046) |  Loss2: (0.0000) | Acc: (86.00%) (26534/30848)
Epoch: 51 | Batch_idx: 250 |  Loss: (0.4036) |  Loss2: (0.0000) | Acc: (86.00%) (27658/32128)
Epoch: 51 | Batch_idx: 260 |  Loss: (0.4028) |  Loss2: (0.0000) | Acc: (86.00%) (28768/33408)
Epoch: 51 | Batch_idx: 270 |  Loss: (0.4023) |  Loss2: (0.0000) | Acc: (86.00%) (29892/34688)
Epoch: 51 | Batch_idx: 280 |  Loss: (0.4015) |  Loss2: (0.0000) | Acc: (86.00%) (31006/35968)
Epoch: 51 | Batch_idx: 290 |  Loss: (0.4021) |  Loss2: (0.0000) | Acc: (86.00%) (32097/37248)
Epoch: 51 | Batch_idx: 300 |  Loss: (0.4020) |  Loss2: (0.0000) | Acc: (86.00%) (33191/38528)
Epoch: 51 | Batch_idx: 310 |  Loss: (0.4025) |  Loss2: (0.0000) | Acc: (86.00%) (34280/39808)
Epoch: 51 | Batch_idx: 320 |  Loss: (0.4021) |  Loss2: (0.0000) | Acc: (86.00%) (35389/41088)
Epoch: 51 | Batch_idx: 330 |  Loss: (0.4035) |  Loss2: (0.0000) | Acc: (86.00%) (36484/42368)
Epoch: 51 | Batch_idx: 340 |  Loss: (0.4044) |  Loss2: (0.0000) | Acc: (86.00%) (37579/43648)
Epoch: 51 | Batch_idx: 350 |  Loss: (0.4042) |  Loss2: (0.0000) | Acc: (86.00%) (38676/44928)
Epoch: 51 | Batch_idx: 360 |  Loss: (0.4039) |  Loss2: (0.0000) | Acc: (86.00%) (39785/46208)
Epoch: 51 | Batch_idx: 370 |  Loss: (0.4048) |  Loss2: (0.0000) | Acc: (86.00%) (40871/47488)
Epoch: 51 | Batch_idx: 380 |  Loss: (0.4043) |  Loss2: (0.0000) | Acc: (86.00%) (41983/48768)
Epoch: 51 | Batch_idx: 390 |  Loss: (0.4048) |  Loss2: (0.0000) | Acc: (86.00%) (43041/50000)
# TEST : Loss: (0.5306) | Acc: (82.00%) (8273/10000)
percent tensor([0.5569, 0.4431], device='cuda:0')
percent tensor([0.5422, 0.4578], device='cuda:0')
percent tensor([0.5074, 0.4926], device='cuda:0')
percent tensor([0.5552, 0.4448], device='cuda:0')
percent tensor([0.5544, 0.4456], device='cuda:0')
percent tensor([0.6554, 0.3446], device='cuda:0')
percent tensor([0.6466, 0.3534], device='cuda:0')
percent tensor([0.9987, 0.0013], device='cuda:0')
Epoch: 52 | Batch_idx: 0 |  Loss: (0.3159) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 52 | Batch_idx: 10 |  Loss: (0.3944) |  Loss2: (0.0000) | Acc: (86.00%) (1213/1408)
Epoch: 52 | Batch_idx: 20 |  Loss: (0.3917) |  Loss2: (0.0000) | Acc: (86.00%) (2316/2688)
Epoch: 52 | Batch_idx: 30 |  Loss: (0.4024) |  Loss2: (0.0000) | Acc: (85.00%) (3403/3968)
Epoch: 52 | Batch_idx: 40 |  Loss: (0.3971) |  Loss2: (0.0000) | Acc: (86.00%) (4516/5248)
Epoch: 52 | Batch_idx: 50 |  Loss: (0.3883) |  Loss2: (0.0000) | Acc: (86.00%) (5634/6528)
Epoch: 52 | Batch_idx: 60 |  Loss: (0.3909) |  Loss2: (0.0000) | Acc: (86.00%) (6746/7808)
Epoch: 52 | Batch_idx: 70 |  Loss: (0.3914) |  Loss2: (0.0000) | Acc: (86.00%) (7838/9088)
Epoch: 52 | Batch_idx: 80 |  Loss: (0.3922) |  Loss2: (0.0000) | Acc: (86.00%) (8948/10368)
Epoch: 52 | Batch_idx: 90 |  Loss: (0.3924) |  Loss2: (0.0000) | Acc: (86.00%) (10064/11648)
Epoch: 52 | Batch_idx: 100 |  Loss: (0.3926) |  Loss2: (0.0000) | Acc: (86.00%) (11177/12928)
Epoch: 52 | Batch_idx: 110 |  Loss: (0.3908) |  Loss2: (0.0000) | Acc: (86.00%) (12301/14208)
Epoch: 52 | Batch_idx: 120 |  Loss: (0.3898) |  Loss2: (0.0000) | Acc: (86.00%) (13409/15488)
Epoch: 52 | Batch_idx: 130 |  Loss: (0.3914) |  Loss2: (0.0000) | Acc: (86.00%) (14511/16768)
Epoch: 52 | Batch_idx: 140 |  Loss: (0.3898) |  Loss2: (0.0000) | Acc: (86.00%) (15626/18048)
Epoch: 52 | Batch_idx: 150 |  Loss: (0.3878) |  Loss2: (0.0000) | Acc: (86.00%) (16761/19328)
Epoch: 52 | Batch_idx: 160 |  Loss: (0.3856) |  Loss2: (0.0000) | Acc: (86.00%) (17870/20608)
Epoch: 52 | Batch_idx: 170 |  Loss: (0.3864) |  Loss2: (0.0000) | Acc: (86.00%) (18970/21888)
Epoch: 52 | Batch_idx: 180 |  Loss: (0.3877) |  Loss2: (0.0000) | Acc: (86.00%) (20066/23168)
Epoch: 52 | Batch_idx: 190 |  Loss: (0.3863) |  Loss2: (0.0000) | Acc: (86.00%) (21193/24448)
Epoch: 52 | Batch_idx: 200 |  Loss: (0.3873) |  Loss2: (0.0000) | Acc: (86.00%) (22291/25728)
Epoch: 52 | Batch_idx: 210 |  Loss: (0.3865) |  Loss2: (0.0000) | Acc: (86.00%) (23409/27008)
Epoch: 52 | Batch_idx: 220 |  Loss: (0.3862) |  Loss2: (0.0000) | Acc: (86.00%) (24520/28288)
Epoch: 52 | Batch_idx: 230 |  Loss: (0.3860) |  Loss2: (0.0000) | Acc: (86.00%) (25636/29568)
Epoch: 52 | Batch_idx: 240 |  Loss: (0.3872) |  Loss2: (0.0000) | Acc: (86.00%) (26729/30848)
Epoch: 52 | Batch_idx: 250 |  Loss: (0.3861) |  Loss2: (0.0000) | Acc: (86.00%) (27841/32128)
Epoch: 52 | Batch_idx: 260 |  Loss: (0.3853) |  Loss2: (0.0000) | Acc: (86.00%) (28970/33408)
Epoch: 52 | Batch_idx: 270 |  Loss: (0.3856) |  Loss2: (0.0000) | Acc: (86.00%) (30081/34688)
Epoch: 52 | Batch_idx: 280 |  Loss: (0.3848) |  Loss2: (0.0000) | Acc: (86.00%) (31205/35968)
Epoch: 52 | Batch_idx: 290 |  Loss: (0.3862) |  Loss2: (0.0000) | Acc: (86.00%) (32303/37248)
Epoch: 52 | Batch_idx: 300 |  Loss: (0.3861) |  Loss2: (0.0000) | Acc: (86.00%) (33428/38528)
Epoch: 52 | Batch_idx: 310 |  Loss: (0.3858) |  Loss2: (0.0000) | Acc: (86.00%) (34531/39808)
Epoch: 52 | Batch_idx: 320 |  Loss: (0.3857) |  Loss2: (0.0000) | Acc: (86.00%) (35639/41088)
Epoch: 52 | Batch_idx: 330 |  Loss: (0.3848) |  Loss2: (0.0000) | Acc: (86.00%) (36768/42368)
Epoch: 52 | Batch_idx: 340 |  Loss: (0.3849) |  Loss2: (0.0000) | Acc: (86.00%) (37883/43648)
Epoch: 52 | Batch_idx: 350 |  Loss: (0.3854) |  Loss2: (0.0000) | Acc: (86.00%) (38993/44928)
Epoch: 52 | Batch_idx: 360 |  Loss: (0.3859) |  Loss2: (0.0000) | Acc: (86.00%) (40106/46208)
Epoch: 52 | Batch_idx: 370 |  Loss: (0.3864) |  Loss2: (0.0000) | Acc: (86.00%) (41205/47488)
Epoch: 52 | Batch_idx: 380 |  Loss: (0.3862) |  Loss2: (0.0000) | Acc: (86.00%) (42316/48768)
Epoch: 52 | Batch_idx: 390 |  Loss: (0.3863) |  Loss2: (0.0000) | Acc: (86.00%) (43376/50000)
# TEST : Loss: (0.4952) | Acc: (83.00%) (8349/10000)
percent tensor([0.5567, 0.4433], device='cuda:0')
percent tensor([0.5423, 0.4577], device='cuda:0')
percent tensor([0.5074, 0.4926], device='cuda:0')
percent tensor([0.5556, 0.4444], device='cuda:0')
percent tensor([0.5561, 0.4439], device='cuda:0')
percent tensor([0.6559, 0.3441], device='cuda:0')
percent tensor([0.6439, 0.3561], device='cuda:0')
percent tensor([0.9987, 0.0013], device='cuda:0')
Epoch: 53 | Batch_idx: 0 |  Loss: (0.4423) |  Loss2: (0.0000) | Acc: (85.00%) (110/128)
Epoch: 53 | Batch_idx: 10 |  Loss: (0.4176) |  Loss2: (0.0000) | Acc: (86.00%) (1213/1408)
Epoch: 53 | Batch_idx: 20 |  Loss: (0.3822) |  Loss2: (0.0000) | Acc: (87.00%) (2341/2688)
Epoch: 53 | Batch_idx: 30 |  Loss: (0.3808) |  Loss2: (0.0000) | Acc: (87.00%) (3462/3968)
Epoch: 53 | Batch_idx: 40 |  Loss: (0.3824) |  Loss2: (0.0000) | Acc: (87.00%) (4566/5248)
Epoch: 53 | Batch_idx: 50 |  Loss: (0.3835) |  Loss2: (0.0000) | Acc: (86.00%) (5665/6528)
Epoch: 53 | Batch_idx: 60 |  Loss: (0.3852) |  Loss2: (0.0000) | Acc: (86.00%) (6766/7808)
Epoch: 53 | Batch_idx: 70 |  Loss: (0.3860) |  Loss2: (0.0000) | Acc: (86.00%) (7871/9088)
Epoch: 53 | Batch_idx: 80 |  Loss: (0.3860) |  Loss2: (0.0000) | Acc: (86.00%) (8978/10368)
Epoch: 53 | Batch_idx: 90 |  Loss: (0.3816) |  Loss2: (0.0000) | Acc: (86.00%) (10107/11648)
Epoch: 53 | Batch_idx: 100 |  Loss: (0.3812) |  Loss2: (0.0000) | Acc: (86.00%) (11232/12928)
Epoch: 53 | Batch_idx: 110 |  Loss: (0.3816) |  Loss2: (0.0000) | Acc: (86.00%) (12344/14208)
Epoch: 53 | Batch_idx: 120 |  Loss: (0.3788) |  Loss2: (0.0000) | Acc: (87.00%) (13486/15488)
Epoch: 53 | Batch_idx: 130 |  Loss: (0.3763) |  Loss2: (0.0000) | Acc: (87.00%) (14612/16768)
Epoch: 53 | Batch_idx: 140 |  Loss: (0.3776) |  Loss2: (0.0000) | Acc: (87.00%) (15730/18048)
Epoch: 53 | Batch_idx: 150 |  Loss: (0.3786) |  Loss2: (0.0000) | Acc: (87.00%) (16833/19328)
Epoch: 53 | Batch_idx: 160 |  Loss: (0.3799) |  Loss2: (0.0000) | Acc: (86.00%) (17924/20608)
Epoch: 53 | Batch_idx: 170 |  Loss: (0.3803) |  Loss2: (0.0000) | Acc: (86.00%) (19039/21888)
Epoch: 53 | Batch_idx: 180 |  Loss: (0.3810) |  Loss2: (0.0000) | Acc: (86.00%) (20133/23168)
Epoch: 53 | Batch_idx: 190 |  Loss: (0.3794) |  Loss2: (0.0000) | Acc: (86.00%) (21262/24448)
Epoch: 53 | Batch_idx: 200 |  Loss: (0.3780) |  Loss2: (0.0000) | Acc: (87.00%) (22388/25728)
Epoch: 53 | Batch_idx: 210 |  Loss: (0.3770) |  Loss2: (0.0000) | Acc: (87.00%) (23512/27008)
Epoch: 53 | Batch_idx: 220 |  Loss: (0.3778) |  Loss2: (0.0000) | Acc: (87.00%) (24624/28288)
Epoch: 53 | Batch_idx: 230 |  Loss: (0.3770) |  Loss2: (0.0000) | Acc: (87.00%) (25740/29568)
Epoch: 53 | Batch_idx: 240 |  Loss: (0.3782) |  Loss2: (0.0000) | Acc: (87.00%) (26843/30848)
Epoch: 53 | Batch_idx: 250 |  Loss: (0.3785) |  Loss2: (0.0000) | Acc: (86.00%) (27946/32128)
Epoch: 53 | Batch_idx: 260 |  Loss: (0.3796) |  Loss2: (0.0000) | Acc: (86.00%) (29044/33408)
Epoch: 53 | Batch_idx: 270 |  Loss: (0.3789) |  Loss2: (0.0000) | Acc: (86.00%) (30160/34688)
Epoch: 53 | Batch_idx: 280 |  Loss: (0.3795) |  Loss2: (0.0000) | Acc: (86.00%) (31264/35968)
Epoch: 53 | Batch_idx: 290 |  Loss: (0.3801) |  Loss2: (0.0000) | Acc: (86.00%) (32374/37248)
Epoch: 53 | Batch_idx: 300 |  Loss: (0.3811) |  Loss2: (0.0000) | Acc: (86.00%) (33470/38528)
Epoch: 53 | Batch_idx: 310 |  Loss: (0.3811) |  Loss2: (0.0000) | Acc: (86.00%) (34585/39808)
Epoch: 53 | Batch_idx: 320 |  Loss: (0.3804) |  Loss2: (0.0000) | Acc: (86.00%) (35719/41088)
Epoch: 53 | Batch_idx: 330 |  Loss: (0.3807) |  Loss2: (0.0000) | Acc: (86.00%) (36836/42368)
Epoch: 53 | Batch_idx: 340 |  Loss: (0.3801) |  Loss2: (0.0000) | Acc: (86.00%) (37972/43648)
Epoch: 53 | Batch_idx: 350 |  Loss: (0.3799) |  Loss2: (0.0000) | Acc: (87.00%) (39089/44928)
Epoch: 53 | Batch_idx: 360 |  Loss: (0.3815) |  Loss2: (0.0000) | Acc: (86.00%) (40177/46208)
Epoch: 53 | Batch_idx: 370 |  Loss: (0.3823) |  Loss2: (0.0000) | Acc: (86.00%) (41264/47488)
Epoch: 53 | Batch_idx: 380 |  Loss: (0.3827) |  Loss2: (0.0000) | Acc: (86.00%) (42376/48768)
Epoch: 53 | Batch_idx: 390 |  Loss: (0.3824) |  Loss2: (0.0000) | Acc: (86.00%) (43454/50000)
# TEST : Loss: (0.5668) | Acc: (81.00%) (8164/10000)
percent tensor([0.5566, 0.4434], device='cuda:0')
percent tensor([0.5431, 0.4569], device='cuda:0')
percent tensor([0.5074, 0.4926], device='cuda:0')
percent tensor([0.5557, 0.4443], device='cuda:0')
percent tensor([0.5549, 0.4451], device='cuda:0')
percent tensor([0.6541, 0.3459], device='cuda:0')
percent tensor([0.6395, 0.3605], device='cuda:0')
percent tensor([0.9985, 0.0015], device='cuda:0')
Epoch: 54 | Batch_idx: 0 |  Loss: (0.3131) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 54 | Batch_idx: 10 |  Loss: (0.3505) |  Loss2: (0.0000) | Acc: (89.00%) (1259/1408)
Epoch: 54 | Batch_idx: 20 |  Loss: (0.3450) |  Loss2: (0.0000) | Acc: (88.00%) (2388/2688)
Epoch: 54 | Batch_idx: 30 |  Loss: (0.3583) |  Loss2: (0.0000) | Acc: (88.00%) (3520/3968)
Epoch: 54 | Batch_idx: 40 |  Loss: (0.3550) |  Loss2: (0.0000) | Acc: (88.00%) (4651/5248)
Epoch: 54 | Batch_idx: 50 |  Loss: (0.3474) |  Loss2: (0.0000) | Acc: (88.00%) (5793/6528)
Epoch: 54 | Batch_idx: 60 |  Loss: (0.3469) |  Loss2: (0.0000) | Acc: (88.00%) (6928/7808)
Epoch: 54 | Batch_idx: 70 |  Loss: (0.3491) |  Loss2: (0.0000) | Acc: (88.00%) (8062/9088)
Epoch: 54 | Batch_idx: 80 |  Loss: (0.3490) |  Loss2: (0.0000) | Acc: (88.00%) (9178/10368)
Epoch: 54 | Batch_idx: 90 |  Loss: (0.3508) |  Loss2: (0.0000) | Acc: (88.00%) (10291/11648)
Epoch: 54 | Batch_idx: 100 |  Loss: (0.3511) |  Loss2: (0.0000) | Acc: (88.00%) (11410/12928)
Epoch: 54 | Batch_idx: 110 |  Loss: (0.3538) |  Loss2: (0.0000) | Acc: (88.00%) (12516/14208)
Epoch: 54 | Batch_idx: 120 |  Loss: (0.3575) |  Loss2: (0.0000) | Acc: (87.00%) (13611/15488)
Epoch: 54 | Batch_idx: 130 |  Loss: (0.3552) |  Loss2: (0.0000) | Acc: (87.00%) (14741/16768)
Epoch: 54 | Batch_idx: 140 |  Loss: (0.3575) |  Loss2: (0.0000) | Acc: (87.00%) (15849/18048)
Epoch: 54 | Batch_idx: 150 |  Loss: (0.3600) |  Loss2: (0.0000) | Acc: (87.00%) (16956/19328)
Epoch: 54 | Batch_idx: 160 |  Loss: (0.3614) |  Loss2: (0.0000) | Acc: (87.00%) (18069/20608)
Epoch: 54 | Batch_idx: 170 |  Loss: (0.3602) |  Loss2: (0.0000) | Acc: (87.00%) (19201/21888)
Epoch: 54 | Batch_idx: 180 |  Loss: (0.3607) |  Loss2: (0.0000) | Acc: (87.00%) (20317/23168)
Epoch: 54 | Batch_idx: 190 |  Loss: (0.3610) |  Loss2: (0.0000) | Acc: (87.00%) (21424/24448)
Epoch: 54 | Batch_idx: 200 |  Loss: (0.3613) |  Loss2: (0.0000) | Acc: (87.00%) (22553/25728)
Epoch: 54 | Batch_idx: 210 |  Loss: (0.3618) |  Loss2: (0.0000) | Acc: (87.00%) (23661/27008)
Epoch: 54 | Batch_idx: 220 |  Loss: (0.3630) |  Loss2: (0.0000) | Acc: (87.00%) (24774/28288)
Epoch: 54 | Batch_idx: 230 |  Loss: (0.3643) |  Loss2: (0.0000) | Acc: (87.00%) (25884/29568)
Epoch: 54 | Batch_idx: 240 |  Loss: (0.3644) |  Loss2: (0.0000) | Acc: (87.00%) (26994/30848)
Epoch: 54 | Batch_idx: 250 |  Loss: (0.3635) |  Loss2: (0.0000) | Acc: (87.00%) (28130/32128)
Epoch: 54 | Batch_idx: 260 |  Loss: (0.3639) |  Loss2: (0.0000) | Acc: (87.00%) (29265/33408)
Epoch: 54 | Batch_idx: 270 |  Loss: (0.3640) |  Loss2: (0.0000) | Acc: (87.00%) (30376/34688)
Epoch: 54 | Batch_idx: 280 |  Loss: (0.3636) |  Loss2: (0.0000) | Acc: (87.00%) (31509/35968)
Epoch: 54 | Batch_idx: 290 |  Loss: (0.3637) |  Loss2: (0.0000) | Acc: (87.00%) (32624/37248)
Epoch: 54 | Batch_idx: 300 |  Loss: (0.3634) |  Loss2: (0.0000) | Acc: (87.00%) (33741/38528)
Epoch: 54 | Batch_idx: 310 |  Loss: (0.3644) |  Loss2: (0.0000) | Acc: (87.00%) (34841/39808)
Epoch: 54 | Batch_idx: 320 |  Loss: (0.3654) |  Loss2: (0.0000) | Acc: (87.00%) (35938/41088)
Epoch: 54 | Batch_idx: 330 |  Loss: (0.3652) |  Loss2: (0.0000) | Acc: (87.00%) (37057/42368)
Epoch: 54 | Batch_idx: 340 |  Loss: (0.3648) |  Loss2: (0.0000) | Acc: (87.00%) (38191/43648)
Epoch: 54 | Batch_idx: 350 |  Loss: (0.3646) |  Loss2: (0.0000) | Acc: (87.00%) (39302/44928)
Epoch: 54 | Batch_idx: 360 |  Loss: (0.3647) |  Loss2: (0.0000) | Acc: (87.00%) (40422/46208)
Epoch: 54 | Batch_idx: 370 |  Loss: (0.3662) |  Loss2: (0.0000) | Acc: (87.00%) (41524/47488)
Epoch: 54 | Batch_idx: 380 |  Loss: (0.3678) |  Loss2: (0.0000) | Acc: (87.00%) (42614/48768)
Epoch: 54 | Batch_idx: 390 |  Loss: (0.3677) |  Loss2: (0.0000) | Acc: (87.00%) (43695/50000)
# TEST : Loss: (0.4775) | Acc: (84.00%) (8412/10000)
percent tensor([0.5563, 0.4437], device='cuda:0')
percent tensor([0.5430, 0.4570], device='cuda:0')
percent tensor([0.5074, 0.4926], device='cuda:0')
percent tensor([0.5556, 0.4444], device='cuda:0')
percent tensor([0.5552, 0.4448], device='cuda:0')
percent tensor([0.6573, 0.3427], device='cuda:0')
percent tensor([0.6444, 0.3556], device='cuda:0')
percent tensor([0.9988, 0.0012], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 55 | Batch_idx: 0 |  Loss: (0.3784) |  Loss2: (0.0000) | Acc: (83.00%) (107/128)
Epoch: 55 | Batch_idx: 10 |  Loss: (0.3865) |  Loss2: (0.0000) | Acc: (86.00%) (1213/1408)
Epoch: 55 | Batch_idx: 20 |  Loss: (0.3995) |  Loss2: (0.0000) | Acc: (85.00%) (2306/2688)
Epoch: 55 | Batch_idx: 30 |  Loss: (0.4170) |  Loss2: (0.0000) | Acc: (85.00%) (3387/3968)
Epoch: 55 | Batch_idx: 40 |  Loss: (0.4239) |  Loss2: (0.0000) | Acc: (85.00%) (4475/5248)
Epoch: 55 | Batch_idx: 50 |  Loss: (0.4343) |  Loss2: (0.0000) | Acc: (85.00%) (5552/6528)
Epoch: 55 | Batch_idx: 60 |  Loss: (0.4486) |  Loss2: (0.0000) | Acc: (84.00%) (6599/7808)
Epoch: 55 | Batch_idx: 70 |  Loss: (0.4498) |  Loss2: (0.0000) | Acc: (84.00%) (7666/9088)
Epoch: 55 | Batch_idx: 80 |  Loss: (0.4511) |  Loss2: (0.0000) | Acc: (84.00%) (8735/10368)
Epoch: 55 | Batch_idx: 90 |  Loss: (0.4436) |  Loss2: (0.0000) | Acc: (84.00%) (9851/11648)
Epoch: 55 | Batch_idx: 100 |  Loss: (0.4441) |  Loss2: (0.0000) | Acc: (84.00%) (10916/12928)
Epoch: 55 | Batch_idx: 110 |  Loss: (0.4431) |  Loss2: (0.0000) | Acc: (84.00%) (12007/14208)
Epoch: 55 | Batch_idx: 120 |  Loss: (0.4448) |  Loss2: (0.0000) | Acc: (84.00%) (13077/15488)
Epoch: 55 | Batch_idx: 130 |  Loss: (0.4433) |  Loss2: (0.0000) | Acc: (84.00%) (14176/16768)
Epoch: 55 | Batch_idx: 140 |  Loss: (0.4423) |  Loss2: (0.0000) | Acc: (84.00%) (15258/18048)
Epoch: 55 | Batch_idx: 150 |  Loss: (0.4411) |  Loss2: (0.0000) | Acc: (84.00%) (16351/19328)
Epoch: 55 | Batch_idx: 160 |  Loss: (0.4394) |  Loss2: (0.0000) | Acc: (84.00%) (17451/20608)
Epoch: 55 | Batch_idx: 170 |  Loss: (0.4373) |  Loss2: (0.0000) | Acc: (84.00%) (18547/21888)
Epoch: 55 | Batch_idx: 180 |  Loss: (0.4379) |  Loss2: (0.0000) | Acc: (84.00%) (19627/23168)
Epoch: 55 | Batch_idx: 190 |  Loss: (0.4385) |  Loss2: (0.0000) | Acc: (84.00%) (20711/24448)
Epoch: 55 | Batch_idx: 200 |  Loss: (0.4367) |  Loss2: (0.0000) | Acc: (84.00%) (21808/25728)
Epoch: 55 | Batch_idx: 210 |  Loss: (0.4359) |  Loss2: (0.0000) | Acc: (84.00%) (22899/27008)
Epoch: 55 | Batch_idx: 220 |  Loss: (0.4362) |  Loss2: (0.0000) | Acc: (84.00%) (23979/28288)
Epoch: 55 | Batch_idx: 230 |  Loss: (0.4342) |  Loss2: (0.0000) | Acc: (84.00%) (25084/29568)
Epoch: 55 | Batch_idx: 240 |  Loss: (0.4316) |  Loss2: (0.0000) | Acc: (84.00%) (26199/30848)
Epoch: 55 | Batch_idx: 250 |  Loss: (0.4296) |  Loss2: (0.0000) | Acc: (85.00%) (27310/32128)
Epoch: 55 | Batch_idx: 260 |  Loss: (0.4288) |  Loss2: (0.0000) | Acc: (85.00%) (28411/33408)
Epoch: 55 | Batch_idx: 270 |  Loss: (0.4275) |  Loss2: (0.0000) | Acc: (85.00%) (29512/34688)
Epoch: 55 | Batch_idx: 280 |  Loss: (0.4279) |  Loss2: (0.0000) | Acc: (85.00%) (30591/35968)
Epoch: 55 | Batch_idx: 290 |  Loss: (0.4268) |  Loss2: (0.0000) | Acc: (85.00%) (31696/37248)
Epoch: 55 | Batch_idx: 300 |  Loss: (0.4252) |  Loss2: (0.0000) | Acc: (85.00%) (32808/38528)
Epoch: 55 | Batch_idx: 310 |  Loss: (0.4255) |  Loss2: (0.0000) | Acc: (85.00%) (33901/39808)
Epoch: 55 | Batch_idx: 320 |  Loss: (0.4241) |  Loss2: (0.0000) | Acc: (85.00%) (35013/41088)
Epoch: 55 | Batch_idx: 330 |  Loss: (0.4221) |  Loss2: (0.0000) | Acc: (85.00%) (36138/42368)
Epoch: 55 | Batch_idx: 340 |  Loss: (0.4213) |  Loss2: (0.0000) | Acc: (85.00%) (37249/43648)
Epoch: 55 | Batch_idx: 350 |  Loss: (0.4194) |  Loss2: (0.0000) | Acc: (85.00%) (38369/44928)
Epoch: 55 | Batch_idx: 360 |  Loss: (0.4188) |  Loss2: (0.0000) | Acc: (85.00%) (39465/46208)
Epoch: 55 | Batch_idx: 370 |  Loss: (0.4182) |  Loss2: (0.0000) | Acc: (85.00%) (40560/47488)
Epoch: 55 | Batch_idx: 380 |  Loss: (0.4167) |  Loss2: (0.0000) | Acc: (85.00%) (41678/48768)
Epoch: 55 | Batch_idx: 390 |  Loss: (0.4166) |  Loss2: (0.0000) | Acc: (85.00%) (42737/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_055.pth.tar'
# TEST : Loss: (0.4897) | Acc: (83.00%) (8347/10000)
percent tensor([0.5872, 0.4128], device='cuda:0')
percent tensor([0.5354, 0.4646], device='cuda:0')
percent tensor([0.5091, 0.4909], device='cuda:0')
percent tensor([0.5726, 0.4274], device='cuda:0')
percent tensor([0.5799, 0.4201], device='cuda:0')
percent tensor([0.7193, 0.2807], device='cuda:0')
percent tensor([0.6735, 0.3265], device='cuda:0')
percent tensor([0.9987, 0.0013], device='cuda:0')
Epoch: 56 | Batch_idx: 0 |  Loss: (0.3547) |  Loss2: (0.0000) | Acc: (84.00%) (108/128)
Epoch: 56 | Batch_idx: 10 |  Loss: (0.3745) |  Loss2: (0.0000) | Acc: (86.00%) (1220/1408)
Epoch: 56 | Batch_idx: 20 |  Loss: (0.3986) |  Loss2: (0.0000) | Acc: (85.00%) (2311/2688)
Epoch: 56 | Batch_idx: 30 |  Loss: (0.3894) |  Loss2: (0.0000) | Acc: (86.00%) (3424/3968)
Epoch: 56 | Batch_idx: 40 |  Loss: (0.3951) |  Loss2: (0.0000) | Acc: (86.00%) (4532/5248)
Epoch: 56 | Batch_idx: 50 |  Loss: (0.3959) |  Loss2: (0.0000) | Acc: (86.00%) (5638/6528)
Epoch: 56 | Batch_idx: 60 |  Loss: (0.3911) |  Loss2: (0.0000) | Acc: (86.00%) (6759/7808)
Epoch: 56 | Batch_idx: 70 |  Loss: (0.3919) |  Loss2: (0.0000) | Acc: (86.00%) (7863/9088)
Epoch: 56 | Batch_idx: 80 |  Loss: (0.3898) |  Loss2: (0.0000) | Acc: (86.00%) (8976/10368)
Epoch: 56 | Batch_idx: 90 |  Loss: (0.3883) |  Loss2: (0.0000) | Acc: (86.00%) (10092/11648)
Epoch: 56 | Batch_idx: 100 |  Loss: (0.3925) |  Loss2: (0.0000) | Acc: (86.00%) (11192/12928)
Epoch: 56 | Batch_idx: 110 |  Loss: (0.3920) |  Loss2: (0.0000) | Acc: (86.00%) (12307/14208)
Epoch: 56 | Batch_idx: 120 |  Loss: (0.3913) |  Loss2: (0.0000) | Acc: (86.00%) (13419/15488)
Epoch: 56 | Batch_idx: 130 |  Loss: (0.3907) |  Loss2: (0.0000) | Acc: (86.00%) (14521/16768)
Epoch: 56 | Batch_idx: 140 |  Loss: (0.3924) |  Loss2: (0.0000) | Acc: (86.00%) (15624/18048)
Epoch: 56 | Batch_idx: 150 |  Loss: (0.3929) |  Loss2: (0.0000) | Acc: (86.00%) (16721/19328)
Epoch: 56 | Batch_idx: 160 |  Loss: (0.3899) |  Loss2: (0.0000) | Acc: (86.00%) (17862/20608)
Epoch: 56 | Batch_idx: 170 |  Loss: (0.3911) |  Loss2: (0.0000) | Acc: (86.00%) (18959/21888)
Epoch: 56 | Batch_idx: 180 |  Loss: (0.3899) |  Loss2: (0.0000) | Acc: (86.00%) (20069/23168)
Epoch: 56 | Batch_idx: 190 |  Loss: (0.3886) |  Loss2: (0.0000) | Acc: (86.00%) (21199/24448)
Epoch: 56 | Batch_idx: 200 |  Loss: (0.3897) |  Loss2: (0.0000) | Acc: (86.00%) (22294/25728)
Epoch: 56 | Batch_idx: 210 |  Loss: (0.3893) |  Loss2: (0.0000) | Acc: (86.00%) (23400/27008)
Epoch: 56 | Batch_idx: 220 |  Loss: (0.3893) |  Loss2: (0.0000) | Acc: (86.00%) (24490/28288)
Epoch: 56 | Batch_idx: 230 |  Loss: (0.3894) |  Loss2: (0.0000) | Acc: (86.00%) (25603/29568)
Epoch: 56 | Batch_idx: 240 |  Loss: (0.3889) |  Loss2: (0.0000) | Acc: (86.00%) (26718/30848)
Epoch: 56 | Batch_idx: 250 |  Loss: (0.3883) |  Loss2: (0.0000) | Acc: (86.00%) (27836/32128)
Epoch: 56 | Batch_idx: 260 |  Loss: (0.3885) |  Loss2: (0.0000) | Acc: (86.00%) (28945/33408)
Epoch: 56 | Batch_idx: 270 |  Loss: (0.3880) |  Loss2: (0.0000) | Acc: (86.00%) (30056/34688)
Epoch: 56 | Batch_idx: 280 |  Loss: (0.3865) |  Loss2: (0.0000) | Acc: (86.00%) (31190/35968)
Epoch: 56 | Batch_idx: 290 |  Loss: (0.3855) |  Loss2: (0.0000) | Acc: (86.00%) (32319/37248)
Epoch: 56 | Batch_idx: 300 |  Loss: (0.3841) |  Loss2: (0.0000) | Acc: (86.00%) (33445/38528)
Epoch: 56 | Batch_idx: 310 |  Loss: (0.3837) |  Loss2: (0.0000) | Acc: (86.00%) (34569/39808)
Epoch: 56 | Batch_idx: 320 |  Loss: (0.3847) |  Loss2: (0.0000) | Acc: (86.00%) (35675/41088)
Epoch: 56 | Batch_idx: 330 |  Loss: (0.3835) |  Loss2: (0.0000) | Acc: (86.00%) (36811/42368)
Epoch: 56 | Batch_idx: 340 |  Loss: (0.3842) |  Loss2: (0.0000) | Acc: (86.00%) (37917/43648)
Epoch: 56 | Batch_idx: 350 |  Loss: (0.3840) |  Loss2: (0.0000) | Acc: (86.00%) (39038/44928)
Epoch: 56 | Batch_idx: 360 |  Loss: (0.3841) |  Loss2: (0.0000) | Acc: (86.00%) (40151/46208)
Epoch: 56 | Batch_idx: 370 |  Loss: (0.3845) |  Loss2: (0.0000) | Acc: (86.00%) (41252/47488)
Epoch: 56 | Batch_idx: 380 |  Loss: (0.3846) |  Loss2: (0.0000) | Acc: (86.00%) (42356/48768)
Epoch: 56 | Batch_idx: 390 |  Loss: (0.3840) |  Loss2: (0.0000) | Acc: (86.00%) (43438/50000)
# TEST : Loss: (0.4676) | Acc: (84.00%) (8440/10000)
percent tensor([0.5855, 0.4145], device='cuda:0')
percent tensor([0.5348, 0.4652], device='cuda:0')
percent tensor([0.5097, 0.4903], device='cuda:0')
percent tensor([0.5749, 0.4251], device='cuda:0')
percent tensor([0.5886, 0.4114], device='cuda:0')
percent tensor([0.7309, 0.2691], device='cuda:0')
percent tensor([0.6802, 0.3198], device='cuda:0')
percent tensor([0.9987, 0.0013], device='cuda:0')
Epoch: 57 | Batch_idx: 0 |  Loss: (0.5568) |  Loss2: (0.0000) | Acc: (81.00%) (104/128)
Epoch: 57 | Batch_idx: 10 |  Loss: (0.4245) |  Loss2: (0.0000) | Acc: (85.00%) (1207/1408)
Epoch: 57 | Batch_idx: 20 |  Loss: (0.3988) |  Loss2: (0.0000) | Acc: (86.00%) (2324/2688)
Epoch: 57 | Batch_idx: 30 |  Loss: (0.3915) |  Loss2: (0.0000) | Acc: (86.00%) (3439/3968)
Epoch: 57 | Batch_idx: 40 |  Loss: (0.3835) |  Loss2: (0.0000) | Acc: (86.00%) (4565/5248)
Epoch: 57 | Batch_idx: 50 |  Loss: (0.3797) |  Loss2: (0.0000) | Acc: (86.00%) (5679/6528)
Epoch: 57 | Batch_idx: 60 |  Loss: (0.3852) |  Loss2: (0.0000) | Acc: (86.00%) (6776/7808)
Epoch: 57 | Batch_idx: 70 |  Loss: (0.3866) |  Loss2: (0.0000) | Acc: (86.00%) (7886/9088)
Epoch: 57 | Batch_idx: 80 |  Loss: (0.3842) |  Loss2: (0.0000) | Acc: (86.00%) (9017/10368)
Epoch: 57 | Batch_idx: 90 |  Loss: (0.3814) |  Loss2: (0.0000) | Acc: (87.00%) (10151/11648)
Epoch: 57 | Batch_idx: 100 |  Loss: (0.3805) |  Loss2: (0.0000) | Acc: (87.00%) (11265/12928)
Epoch: 57 | Batch_idx: 110 |  Loss: (0.3811) |  Loss2: (0.0000) | Acc: (87.00%) (12383/14208)
Epoch: 57 | Batch_idx: 120 |  Loss: (0.3797) |  Loss2: (0.0000) | Acc: (87.00%) (13511/15488)
Epoch: 57 | Batch_idx: 130 |  Loss: (0.3761) |  Loss2: (0.0000) | Acc: (87.00%) (14649/16768)
Epoch: 57 | Batch_idx: 140 |  Loss: (0.3794) |  Loss2: (0.0000) | Acc: (87.00%) (15740/18048)
Epoch: 57 | Batch_idx: 150 |  Loss: (0.3772) |  Loss2: (0.0000) | Acc: (87.00%) (16866/19328)
Epoch: 57 | Batch_idx: 160 |  Loss: (0.3771) |  Loss2: (0.0000) | Acc: (87.00%) (17980/20608)
Epoch: 57 | Batch_idx: 170 |  Loss: (0.3749) |  Loss2: (0.0000) | Acc: (87.00%) (19123/21888)
Epoch: 57 | Batch_idx: 180 |  Loss: (0.3757) |  Loss2: (0.0000) | Acc: (87.00%) (20229/23168)
Epoch: 57 | Batch_idx: 190 |  Loss: (0.3746) |  Loss2: (0.0000) | Acc: (87.00%) (21346/24448)
Epoch: 57 | Batch_idx: 200 |  Loss: (0.3742) |  Loss2: (0.0000) | Acc: (87.00%) (22461/25728)
Epoch: 57 | Batch_idx: 210 |  Loss: (0.3726) |  Loss2: (0.0000) | Acc: (87.00%) (23604/27008)
Epoch: 57 | Batch_idx: 220 |  Loss: (0.3728) |  Loss2: (0.0000) | Acc: (87.00%) (24725/28288)
Epoch: 57 | Batch_idx: 230 |  Loss: (0.3725) |  Loss2: (0.0000) | Acc: (87.00%) (25840/29568)
Epoch: 57 | Batch_idx: 240 |  Loss: (0.3712) |  Loss2: (0.0000) | Acc: (87.00%) (26965/30848)
Epoch: 57 | Batch_idx: 250 |  Loss: (0.3708) |  Loss2: (0.0000) | Acc: (87.00%) (28077/32128)
Epoch: 57 | Batch_idx: 260 |  Loss: (0.3705) |  Loss2: (0.0000) | Acc: (87.00%) (29190/33408)
Epoch: 57 | Batch_idx: 270 |  Loss: (0.3701) |  Loss2: (0.0000) | Acc: (87.00%) (30300/34688)
Epoch: 57 | Batch_idx: 280 |  Loss: (0.3688) |  Loss2: (0.0000) | Acc: (87.00%) (31422/35968)
Epoch: 57 | Batch_idx: 290 |  Loss: (0.3702) |  Loss2: (0.0000) | Acc: (87.00%) (32523/37248)
Epoch: 57 | Batch_idx: 300 |  Loss: (0.3703) |  Loss2: (0.0000) | Acc: (87.00%) (33633/38528)
Epoch: 57 | Batch_idx: 310 |  Loss: (0.3699) |  Loss2: (0.0000) | Acc: (87.00%) (34771/39808)
Epoch: 57 | Batch_idx: 320 |  Loss: (0.3689) |  Loss2: (0.0000) | Acc: (87.00%) (35899/41088)
Epoch: 57 | Batch_idx: 330 |  Loss: (0.3690) |  Loss2: (0.0000) | Acc: (87.00%) (37010/42368)
Epoch: 57 | Batch_idx: 340 |  Loss: (0.3694) |  Loss2: (0.0000) | Acc: (87.00%) (38125/43648)
Epoch: 57 | Batch_idx: 350 |  Loss: (0.3682) |  Loss2: (0.0000) | Acc: (87.00%) (39256/44928)
Epoch: 57 | Batch_idx: 360 |  Loss: (0.3684) |  Loss2: (0.0000) | Acc: (87.00%) (40377/46208)
Epoch: 57 | Batch_idx: 370 |  Loss: (0.3683) |  Loss2: (0.0000) | Acc: (87.00%) (41504/47488)
Epoch: 57 | Batch_idx: 380 |  Loss: (0.3697) |  Loss2: (0.0000) | Acc: (87.00%) (42601/48768)
Epoch: 57 | Batch_idx: 390 |  Loss: (0.3699) |  Loss2: (0.0000) | Acc: (87.00%) (43673/50000)
# TEST : Loss: (0.4597) | Acc: (84.00%) (8447/10000)
percent tensor([0.5747, 0.4253], device='cuda:0')
percent tensor([0.5347, 0.4653], device='cuda:0')
percent tensor([0.5096, 0.4904], device='cuda:0')
percent tensor([0.5723, 0.4277], device='cuda:0')
percent tensor([0.5926, 0.4074], device='cuda:0')
percent tensor([0.7289, 0.2711], device='cuda:0')
percent tensor([0.6843, 0.3157], device='cuda:0')
percent tensor([0.9988, 0.0012], device='cuda:0')
Epoch: 58 | Batch_idx: 0 |  Loss: (0.3157) |  Loss2: (0.0000) | Acc: (87.00%) (112/128)
Epoch: 58 | Batch_idx: 10 |  Loss: (0.3643) |  Loss2: (0.0000) | Acc: (86.00%) (1221/1408)
Epoch: 58 | Batch_idx: 20 |  Loss: (0.3706) |  Loss2: (0.0000) | Acc: (86.00%) (2330/2688)
Epoch: 58 | Batch_idx: 30 |  Loss: (0.3737) |  Loss2: (0.0000) | Acc: (86.00%) (3440/3968)
Epoch: 58 | Batch_idx: 40 |  Loss: (0.3699) |  Loss2: (0.0000) | Acc: (86.00%) (4561/5248)
Epoch: 58 | Batch_idx: 50 |  Loss: (0.3663) |  Loss2: (0.0000) | Acc: (87.00%) (5693/6528)
Epoch: 58 | Batch_idx: 60 |  Loss: (0.3682) |  Loss2: (0.0000) | Acc: (87.00%) (6805/7808)
Epoch: 58 | Batch_idx: 70 |  Loss: (0.3648) |  Loss2: (0.0000) | Acc: (87.00%) (7927/9088)
Epoch: 58 | Batch_idx: 80 |  Loss: (0.3675) |  Loss2: (0.0000) | Acc: (87.00%) (9036/10368)
Epoch: 58 | Batch_idx: 90 |  Loss: (0.3680) |  Loss2: (0.0000) | Acc: (87.00%) (10148/11648)
Epoch: 58 | Batch_idx: 100 |  Loss: (0.3683) |  Loss2: (0.0000) | Acc: (87.00%) (11259/12928)
Epoch: 58 | Batch_idx: 110 |  Loss: (0.3648) |  Loss2: (0.0000) | Acc: (87.00%) (12409/14208)
Epoch: 58 | Batch_idx: 120 |  Loss: (0.3674) |  Loss2: (0.0000) | Acc: (87.00%) (13499/15488)
Epoch: 58 | Batch_idx: 130 |  Loss: (0.3689) |  Loss2: (0.0000) | Acc: (87.00%) (14611/16768)
Epoch: 58 | Batch_idx: 140 |  Loss: (0.3650) |  Loss2: (0.0000) | Acc: (87.00%) (15769/18048)
Epoch: 58 | Batch_idx: 150 |  Loss: (0.3658) |  Loss2: (0.0000) | Acc: (87.00%) (16888/19328)
Epoch: 58 | Batch_idx: 160 |  Loss: (0.3640) |  Loss2: (0.0000) | Acc: (87.00%) (18017/20608)
Epoch: 58 | Batch_idx: 170 |  Loss: (0.3640) |  Loss2: (0.0000) | Acc: (87.00%) (19123/21888)
Epoch: 58 | Batch_idx: 180 |  Loss: (0.3639) |  Loss2: (0.0000) | Acc: (87.00%) (20247/23168)
Epoch: 58 | Batch_idx: 190 |  Loss: (0.3625) |  Loss2: (0.0000) | Acc: (87.00%) (21384/24448)
Epoch: 58 | Batch_idx: 200 |  Loss: (0.3623) |  Loss2: (0.0000) | Acc: (87.00%) (22519/25728)
Epoch: 58 | Batch_idx: 210 |  Loss: (0.3634) |  Loss2: (0.0000) | Acc: (87.00%) (23640/27008)
Epoch: 58 | Batch_idx: 220 |  Loss: (0.3631) |  Loss2: (0.0000) | Acc: (87.00%) (24739/28288)
Epoch: 58 | Batch_idx: 230 |  Loss: (0.3623) |  Loss2: (0.0000) | Acc: (87.00%) (25863/29568)
Epoch: 58 | Batch_idx: 240 |  Loss: (0.3621) |  Loss2: (0.0000) | Acc: (87.00%) (26984/30848)
Epoch: 58 | Batch_idx: 250 |  Loss: (0.3621) |  Loss2: (0.0000) | Acc: (87.00%) (28109/32128)
Epoch: 58 | Batch_idx: 260 |  Loss: (0.3620) |  Loss2: (0.0000) | Acc: (87.00%) (29227/33408)
Epoch: 58 | Batch_idx: 270 |  Loss: (0.3623) |  Loss2: (0.0000) | Acc: (87.00%) (30352/34688)
Epoch: 58 | Batch_idx: 280 |  Loss: (0.3617) |  Loss2: (0.0000) | Acc: (87.00%) (31473/35968)
Epoch: 58 | Batch_idx: 290 |  Loss: (0.3619) |  Loss2: (0.0000) | Acc: (87.00%) (32603/37248)
Epoch: 58 | Batch_idx: 300 |  Loss: (0.3616) |  Loss2: (0.0000) | Acc: (87.00%) (33718/38528)
Epoch: 58 | Batch_idx: 310 |  Loss: (0.3612) |  Loss2: (0.0000) | Acc: (87.00%) (34838/39808)
Epoch: 58 | Batch_idx: 320 |  Loss: (0.3617) |  Loss2: (0.0000) | Acc: (87.00%) (35946/41088)
Epoch: 58 | Batch_idx: 330 |  Loss: (0.3628) |  Loss2: (0.0000) | Acc: (87.00%) (37044/42368)
Epoch: 58 | Batch_idx: 340 |  Loss: (0.3624) |  Loss2: (0.0000) | Acc: (87.00%) (38185/43648)
Epoch: 58 | Batch_idx: 350 |  Loss: (0.3628) |  Loss2: (0.0000) | Acc: (87.00%) (39299/44928)
Epoch: 58 | Batch_idx: 360 |  Loss: (0.3629) |  Loss2: (0.0000) | Acc: (87.00%) (40420/46208)
Epoch: 58 | Batch_idx: 370 |  Loss: (0.3628) |  Loss2: (0.0000) | Acc: (87.00%) (41552/47488)
Epoch: 58 | Batch_idx: 380 |  Loss: (0.3627) |  Loss2: (0.0000) | Acc: (87.00%) (42683/48768)
Epoch: 58 | Batch_idx: 390 |  Loss: (0.3627) |  Loss2: (0.0000) | Acc: (87.00%) (43754/50000)
# TEST : Loss: (0.4525) | Acc: (84.00%) (8477/10000)
percent tensor([0.5725, 0.4275], device='cuda:0')
percent tensor([0.5343, 0.4657], device='cuda:0')
percent tensor([0.5094, 0.4906], device='cuda:0')
percent tensor([0.5724, 0.4276], device='cuda:0')
percent tensor([0.5905, 0.4095], device='cuda:0')
percent tensor([0.7272, 0.2728], device='cuda:0')
percent tensor([0.6918, 0.3082], device='cuda:0')
percent tensor([0.9988, 0.0012], device='cuda:0')
Epoch: 59 | Batch_idx: 0 |  Loss: (0.2561) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 59 | Batch_idx: 10 |  Loss: (0.3963) |  Loss2: (0.0000) | Acc: (86.00%) (1220/1408)
Epoch: 59 | Batch_idx: 20 |  Loss: (0.3845) |  Loss2: (0.0000) | Acc: (86.00%) (2337/2688)
Epoch: 59 | Batch_idx: 30 |  Loss: (0.3749) |  Loss2: (0.0000) | Acc: (86.00%) (3451/3968)
Epoch: 59 | Batch_idx: 40 |  Loss: (0.3764) |  Loss2: (0.0000) | Acc: (87.00%) (4568/5248)
Epoch: 59 | Batch_idx: 50 |  Loss: (0.3744) |  Loss2: (0.0000) | Acc: (87.00%) (5684/6528)
Epoch: 59 | Batch_idx: 60 |  Loss: (0.3665) |  Loss2: (0.0000) | Acc: (87.00%) (6827/7808)
Epoch: 59 | Batch_idx: 70 |  Loss: (0.3629) |  Loss2: (0.0000) | Acc: (87.00%) (7961/9088)
Epoch: 59 | Batch_idx: 80 |  Loss: (0.3655) |  Loss2: (0.0000) | Acc: (87.00%) (9066/10368)
Epoch: 59 | Batch_idx: 90 |  Loss: (0.3662) |  Loss2: (0.0000) | Acc: (87.00%) (10172/11648)
Epoch: 59 | Batch_idx: 100 |  Loss: (0.3665) |  Loss2: (0.0000) | Acc: (87.00%) (11293/12928)
Epoch: 59 | Batch_idx: 110 |  Loss: (0.3648) |  Loss2: (0.0000) | Acc: (87.00%) (12411/14208)
Epoch: 59 | Batch_idx: 120 |  Loss: (0.3650) |  Loss2: (0.0000) | Acc: (87.00%) (13527/15488)
Epoch: 59 | Batch_idx: 130 |  Loss: (0.3625) |  Loss2: (0.0000) | Acc: (87.00%) (14666/16768)
Epoch: 59 | Batch_idx: 140 |  Loss: (0.3623) |  Loss2: (0.0000) | Acc: (87.00%) (15787/18048)
Epoch: 59 | Batch_idx: 150 |  Loss: (0.3651) |  Loss2: (0.0000) | Acc: (87.00%) (16901/19328)
Epoch: 59 | Batch_idx: 160 |  Loss: (0.3624) |  Loss2: (0.0000) | Acc: (87.00%) (18046/20608)
Epoch: 59 | Batch_idx: 170 |  Loss: (0.3592) |  Loss2: (0.0000) | Acc: (87.00%) (19197/21888)
Epoch: 59 | Batch_idx: 180 |  Loss: (0.3600) |  Loss2: (0.0000) | Acc: (87.00%) (20316/23168)
Epoch: 59 | Batch_idx: 190 |  Loss: (0.3611) |  Loss2: (0.0000) | Acc: (87.00%) (21426/24448)
Epoch: 59 | Batch_idx: 200 |  Loss: (0.3607) |  Loss2: (0.0000) | Acc: (87.00%) (22554/25728)
Epoch: 59 | Batch_idx: 210 |  Loss: (0.3614) |  Loss2: (0.0000) | Acc: (87.00%) (23665/27008)
Epoch: 59 | Batch_idx: 220 |  Loss: (0.3595) |  Loss2: (0.0000) | Acc: (87.00%) (24796/28288)
Epoch: 59 | Batch_idx: 230 |  Loss: (0.3592) |  Loss2: (0.0000) | Acc: (87.00%) (25924/29568)
Epoch: 59 | Batch_idx: 240 |  Loss: (0.3594) |  Loss2: (0.0000) | Acc: (87.00%) (27035/30848)
Epoch: 59 | Batch_idx: 250 |  Loss: (0.3580) |  Loss2: (0.0000) | Acc: (87.00%) (28173/32128)
Epoch: 59 | Batch_idx: 260 |  Loss: (0.3589) |  Loss2: (0.0000) | Acc: (87.00%) (29297/33408)
Epoch: 59 | Batch_idx: 270 |  Loss: (0.3597) |  Loss2: (0.0000) | Acc: (87.00%) (30400/34688)
Epoch: 59 | Batch_idx: 280 |  Loss: (0.3602) |  Loss2: (0.0000) | Acc: (87.00%) (31524/35968)
Epoch: 59 | Batch_idx: 290 |  Loss: (0.3617) |  Loss2: (0.0000) | Acc: (87.00%) (32634/37248)
Epoch: 59 | Batch_idx: 300 |  Loss: (0.3620) |  Loss2: (0.0000) | Acc: (87.00%) (33748/38528)
Epoch: 59 | Batch_idx: 310 |  Loss: (0.3623) |  Loss2: (0.0000) | Acc: (87.00%) (34860/39808)
Epoch: 59 | Batch_idx: 320 |  Loss: (0.3625) |  Loss2: (0.0000) | Acc: (87.00%) (35978/41088)
Epoch: 59 | Batch_idx: 330 |  Loss: (0.3614) |  Loss2: (0.0000) | Acc: (87.00%) (37110/42368)
Epoch: 59 | Batch_idx: 340 |  Loss: (0.3610) |  Loss2: (0.0000) | Acc: (87.00%) (38237/43648)
Epoch: 59 | Batch_idx: 350 |  Loss: (0.3602) |  Loss2: (0.0000) | Acc: (87.00%) (39380/44928)
Epoch: 59 | Batch_idx: 360 |  Loss: (0.3607) |  Loss2: (0.0000) | Acc: (87.00%) (40506/46208)
Epoch: 59 | Batch_idx: 370 |  Loss: (0.3606) |  Loss2: (0.0000) | Acc: (87.00%) (41635/47488)
Epoch: 59 | Batch_idx: 380 |  Loss: (0.3604) |  Loss2: (0.0000) | Acc: (87.00%) (42752/48768)
Epoch: 59 | Batch_idx: 390 |  Loss: (0.3604) |  Loss2: (0.0000) | Acc: (87.00%) (43826/50000)
# TEST : Loss: (0.4531) | Acc: (84.00%) (8476/10000)
percent tensor([0.5732, 0.4268], device='cuda:0')
percent tensor([0.5353, 0.4647], device='cuda:0')
percent tensor([0.5103, 0.4897], device='cuda:0')
percent tensor([0.5737, 0.4263], device='cuda:0')
percent tensor([0.5919, 0.4081], device='cuda:0')
percent tensor([0.7256, 0.2744], device='cuda:0')
percent tensor([0.6952, 0.3048], device='cuda:0')
percent tensor([0.9990, 0.0010], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 60 | Batch_idx: 0 |  Loss: (0.3520) |  Loss2: (0.0000) | Acc: (87.00%) (112/128)
Epoch: 60 | Batch_idx: 10 |  Loss: (0.3694) |  Loss2: (0.0000) | Acc: (87.00%) (1226/1408)
Epoch: 60 | Batch_idx: 20 |  Loss: (0.3614) |  Loss2: (0.0000) | Acc: (87.00%) (2353/2688)
Epoch: 60 | Batch_idx: 30 |  Loss: (0.3575) |  Loss2: (0.0000) | Acc: (87.00%) (3473/3968)
Epoch: 60 | Batch_idx: 40 |  Loss: (0.3564) |  Loss2: (0.0000) | Acc: (87.00%) (4591/5248)
Epoch: 60 | Batch_idx: 50 |  Loss: (0.3552) |  Loss2: (0.0000) | Acc: (87.00%) (5721/6528)
Epoch: 60 | Batch_idx: 60 |  Loss: (0.3556) |  Loss2: (0.0000) | Acc: (87.00%) (6844/7808)
Epoch: 60 | Batch_idx: 70 |  Loss: (0.3615) |  Loss2: (0.0000) | Acc: (87.00%) (7937/9088)
Epoch: 60 | Batch_idx: 80 |  Loss: (0.3602) |  Loss2: (0.0000) | Acc: (87.00%) (9058/10368)
Epoch: 60 | Batch_idx: 90 |  Loss: (0.3642) |  Loss2: (0.0000) | Acc: (87.00%) (10182/11648)
Epoch: 60 | Batch_idx: 100 |  Loss: (0.3617) |  Loss2: (0.0000) | Acc: (87.00%) (11312/12928)
Epoch: 60 | Batch_idx: 110 |  Loss: (0.3617) |  Loss2: (0.0000) | Acc: (87.00%) (12424/14208)
Epoch: 60 | Batch_idx: 120 |  Loss: (0.3615) |  Loss2: (0.0000) | Acc: (87.00%) (13539/15488)
Epoch: 60 | Batch_idx: 130 |  Loss: (0.3629) |  Loss2: (0.0000) | Acc: (87.00%) (14660/16768)
Epoch: 60 | Batch_idx: 140 |  Loss: (0.3631) |  Loss2: (0.0000) | Acc: (87.00%) (15778/18048)
Epoch: 60 | Batch_idx: 150 |  Loss: (0.3615) |  Loss2: (0.0000) | Acc: (87.00%) (16906/19328)
Epoch: 60 | Batch_idx: 160 |  Loss: (0.3610) |  Loss2: (0.0000) | Acc: (87.00%) (18042/20608)
Epoch: 60 | Batch_idx: 170 |  Loss: (0.3600) |  Loss2: (0.0000) | Acc: (87.00%) (19173/21888)
Epoch: 60 | Batch_idx: 180 |  Loss: (0.3590) |  Loss2: (0.0000) | Acc: (87.00%) (20292/23168)
Epoch: 60 | Batch_idx: 190 |  Loss: (0.3592) |  Loss2: (0.0000) | Acc: (87.00%) (21420/24448)
Epoch: 60 | Batch_idx: 200 |  Loss: (0.3587) |  Loss2: (0.0000) | Acc: (87.00%) (22534/25728)
Epoch: 60 | Batch_idx: 210 |  Loss: (0.3585) |  Loss2: (0.0000) | Acc: (87.00%) (23657/27008)
Epoch: 60 | Batch_idx: 220 |  Loss: (0.3579) |  Loss2: (0.0000) | Acc: (87.00%) (24780/28288)
Epoch: 60 | Batch_idx: 230 |  Loss: (0.3578) |  Loss2: (0.0000) | Acc: (87.00%) (25904/29568)
Epoch: 60 | Batch_idx: 240 |  Loss: (0.3602) |  Loss2: (0.0000) | Acc: (87.00%) (27008/30848)
Epoch: 60 | Batch_idx: 250 |  Loss: (0.3603) |  Loss2: (0.0000) | Acc: (87.00%) (28119/32128)
Epoch: 60 | Batch_idx: 260 |  Loss: (0.3607) |  Loss2: (0.0000) | Acc: (87.00%) (29238/33408)
Epoch: 60 | Batch_idx: 270 |  Loss: (0.3609) |  Loss2: (0.0000) | Acc: (87.00%) (30359/34688)
Epoch: 60 | Batch_idx: 280 |  Loss: (0.3604) |  Loss2: (0.0000) | Acc: (87.00%) (31488/35968)
Epoch: 60 | Batch_idx: 290 |  Loss: (0.3603) |  Loss2: (0.0000) | Acc: (87.00%) (32613/37248)
Epoch: 60 | Batch_idx: 300 |  Loss: (0.3615) |  Loss2: (0.0000) | Acc: (87.00%) (33726/38528)
Epoch: 60 | Batch_idx: 310 |  Loss: (0.3619) |  Loss2: (0.0000) | Acc: (87.00%) (34845/39808)
Epoch: 60 | Batch_idx: 320 |  Loss: (0.3611) |  Loss2: (0.0000) | Acc: (87.00%) (35973/41088)
Epoch: 60 | Batch_idx: 330 |  Loss: (0.3608) |  Loss2: (0.0000) | Acc: (87.00%) (37111/42368)
Epoch: 60 | Batch_idx: 340 |  Loss: (0.3607) |  Loss2: (0.0000) | Acc: (87.00%) (38234/43648)
Epoch: 60 | Batch_idx: 350 |  Loss: (0.3620) |  Loss2: (0.0000) | Acc: (87.00%) (39338/44928)
Epoch: 60 | Batch_idx: 360 |  Loss: (0.3623) |  Loss2: (0.0000) | Acc: (87.00%) (40465/46208)
Epoch: 60 | Batch_idx: 370 |  Loss: (0.3629) |  Loss2: (0.0000) | Acc: (87.00%) (41570/47488)
Epoch: 60 | Batch_idx: 380 |  Loss: (0.3635) |  Loss2: (0.0000) | Acc: (87.00%) (42671/48768)
Epoch: 60 | Batch_idx: 390 |  Loss: (0.3636) |  Loss2: (0.0000) | Acc: (87.00%) (43755/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_060.pth.tar'
# TEST : Loss: (0.4928) | Acc: (83.00%) (8392/10000)
percent tensor([0.5730, 0.4270], device='cuda:0')
percent tensor([0.5354, 0.4646], device='cuda:0')
percent tensor([0.5099, 0.4901], device='cuda:0')
percent tensor([0.5742, 0.4258], device='cuda:0')
percent tensor([0.5903, 0.4097], device='cuda:0')
percent tensor([0.7235, 0.2765], device='cuda:0')
percent tensor([0.6940, 0.3060], device='cuda:0')
percent tensor([0.9990, 0.0010], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(176.0275, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(802.3510, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(797.3834, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1527.4336, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(500.0065, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2211.0747, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4295.6577, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1413.7385, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6106.8257, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12015.6143, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3992.9863, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16912.4277, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 61 | Batch_idx: 0 |  Loss: (0.2408) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 61 | Batch_idx: 10 |  Loss: (0.3446) |  Loss2: (0.0000) | Acc: (89.00%) (1259/1408)
Epoch: 61 | Batch_idx: 20 |  Loss: (0.3534) |  Loss2: (0.0000) | Acc: (88.00%) (2380/2688)
Epoch: 61 | Batch_idx: 30 |  Loss: (0.3491) |  Loss2: (0.0000) | Acc: (88.00%) (3512/3968)
Epoch: 61 | Batch_idx: 40 |  Loss: (0.3561) |  Loss2: (0.0000) | Acc: (88.00%) (4630/5248)
Epoch: 61 | Batch_idx: 50 |  Loss: (0.3534) |  Loss2: (0.0000) | Acc: (88.00%) (5758/6528)
Epoch: 61 | Batch_idx: 60 |  Loss: (0.3609) |  Loss2: (0.0000) | Acc: (87.00%) (6857/7808)
Epoch: 61 | Batch_idx: 70 |  Loss: (0.3619) |  Loss2: (0.0000) | Acc: (87.00%) (7970/9088)
Epoch: 61 | Batch_idx: 80 |  Loss: (0.3609) |  Loss2: (0.0000) | Acc: (87.00%) (9088/10368)
Epoch: 61 | Batch_idx: 90 |  Loss: (0.3570) |  Loss2: (0.0000) | Acc: (87.00%) (10221/11648)
Epoch: 61 | Batch_idx: 100 |  Loss: (0.3560) |  Loss2: (0.0000) | Acc: (87.00%) (11356/12928)
Epoch: 61 | Batch_idx: 110 |  Loss: (0.3542) |  Loss2: (0.0000) | Acc: (87.00%) (12481/14208)
Epoch: 61 | Batch_idx: 120 |  Loss: (0.3552) |  Loss2: (0.0000) | Acc: (87.00%) (13599/15488)
Epoch: 61 | Batch_idx: 130 |  Loss: (0.3541) |  Loss2: (0.0000) | Acc: (87.00%) (14739/16768)
Epoch: 61 | Batch_idx: 140 |  Loss: (0.3519) |  Loss2: (0.0000) | Acc: (87.00%) (15878/18048)
Epoch: 61 | Batch_idx: 150 |  Loss: (0.3531) |  Loss2: (0.0000) | Acc: (87.00%) (16993/19328)
Epoch: 61 | Batch_idx: 160 |  Loss: (0.3512) |  Loss2: (0.0000) | Acc: (88.00%) (18144/20608)
Epoch: 61 | Batch_idx: 170 |  Loss: (0.3517) |  Loss2: (0.0000) | Acc: (88.00%) (19268/21888)
Epoch: 61 | Batch_idx: 180 |  Loss: (0.3526) |  Loss2: (0.0000) | Acc: (88.00%) (20396/23168)
Epoch: 61 | Batch_idx: 190 |  Loss: (0.3529) |  Loss2: (0.0000) | Acc: (88.00%) (21515/24448)
Epoch: 61 | Batch_idx: 200 |  Loss: (0.3544) |  Loss2: (0.0000) | Acc: (87.00%) (22613/25728)
Epoch: 61 | Batch_idx: 210 |  Loss: (0.3546) |  Loss2: (0.0000) | Acc: (87.00%) (23734/27008)
Epoch: 61 | Batch_idx: 220 |  Loss: (0.3547) |  Loss2: (0.0000) | Acc: (87.00%) (24856/28288)
Epoch: 61 | Batch_idx: 230 |  Loss: (0.3533) |  Loss2: (0.0000) | Acc: (87.00%) (26001/29568)
Epoch: 61 | Batch_idx: 240 |  Loss: (0.3541) |  Loss2: (0.0000) | Acc: (87.00%) (27115/30848)
Epoch: 61 | Batch_idx: 250 |  Loss: (0.3537) |  Loss2: (0.0000) | Acc: (87.00%) (28236/32128)
Epoch: 61 | Batch_idx: 260 |  Loss: (0.3534) |  Loss2: (0.0000) | Acc: (87.00%) (29366/33408)
Epoch: 61 | Batch_idx: 270 |  Loss: (0.3533) |  Loss2: (0.0000) | Acc: (87.00%) (30494/34688)
Epoch: 61 | Batch_idx: 280 |  Loss: (0.3532) |  Loss2: (0.0000) | Acc: (87.00%) (31610/35968)
Epoch: 61 | Batch_idx: 290 |  Loss: (0.3530) |  Loss2: (0.0000) | Acc: (87.00%) (32740/37248)
Epoch: 61 | Batch_idx: 300 |  Loss: (0.3524) |  Loss2: (0.0000) | Acc: (87.00%) (33880/38528)
Epoch: 61 | Batch_idx: 310 |  Loss: (0.3524) |  Loss2: (0.0000) | Acc: (87.00%) (35008/39808)
Epoch: 61 | Batch_idx: 320 |  Loss: (0.3506) |  Loss2: (0.0000) | Acc: (88.00%) (36168/41088)
Epoch: 61 | Batch_idx: 330 |  Loss: (0.3505) |  Loss2: (0.0000) | Acc: (88.00%) (37303/42368)
Epoch: 61 | Batch_idx: 340 |  Loss: (0.3512) |  Loss2: (0.0000) | Acc: (88.00%) (38418/43648)
Epoch: 61 | Batch_idx: 350 |  Loss: (0.3506) |  Loss2: (0.0000) | Acc: (88.00%) (39543/44928)
Epoch: 61 | Batch_idx: 360 |  Loss: (0.3507) |  Loss2: (0.0000) | Acc: (88.00%) (40667/46208)
Epoch: 61 | Batch_idx: 370 |  Loss: (0.3500) |  Loss2: (0.0000) | Acc: (88.00%) (41805/47488)
Epoch: 61 | Batch_idx: 380 |  Loss: (0.3497) |  Loss2: (0.0000) | Acc: (88.00%) (42927/48768)
Epoch: 61 | Batch_idx: 390 |  Loss: (0.3498) |  Loss2: (0.0000) | Acc: (88.00%) (44006/50000)
# TEST : Loss: (0.5118) | Acc: (83.00%) (8335/10000)
percent tensor([0.5723, 0.4277], device='cuda:0')
percent tensor([0.5353, 0.4647], device='cuda:0')
percent tensor([0.5104, 0.4896], device='cuda:0')
percent tensor([0.5745, 0.4255], device='cuda:0')
percent tensor([0.5924, 0.4076], device='cuda:0')
percent tensor([0.7274, 0.2726], device='cuda:0')
percent tensor([0.6959, 0.3041], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 62 | Batch_idx: 0 |  Loss: (0.3701) |  Loss2: (0.0000) | Acc: (85.00%) (109/128)
Epoch: 62 | Batch_idx: 10 |  Loss: (0.3542) |  Loss2: (0.0000) | Acc: (87.00%) (1226/1408)
Epoch: 62 | Batch_idx: 20 |  Loss: (0.3469) |  Loss2: (0.0000) | Acc: (87.00%) (2365/2688)
Epoch: 62 | Batch_idx: 30 |  Loss: (0.3387) |  Loss2: (0.0000) | Acc: (88.00%) (3505/3968)
Epoch: 62 | Batch_idx: 40 |  Loss: (0.3268) |  Loss2: (0.0000) | Acc: (88.00%) (4661/5248)
Epoch: 62 | Batch_idx: 50 |  Loss: (0.3269) |  Loss2: (0.0000) | Acc: (88.00%) (5799/6528)
Epoch: 62 | Batch_idx: 60 |  Loss: (0.3276) |  Loss2: (0.0000) | Acc: (88.00%) (6943/7808)
Epoch: 62 | Batch_idx: 70 |  Loss: (0.3358) |  Loss2: (0.0000) | Acc: (88.00%) (8050/9088)
Epoch: 62 | Batch_idx: 80 |  Loss: (0.3391) |  Loss2: (0.0000) | Acc: (88.00%) (9171/10368)
Epoch: 62 | Batch_idx: 90 |  Loss: (0.3384) |  Loss2: (0.0000) | Acc: (88.00%) (10318/11648)
Epoch: 62 | Batch_idx: 100 |  Loss: (0.3353) |  Loss2: (0.0000) | Acc: (88.00%) (11460/12928)
Epoch: 62 | Batch_idx: 110 |  Loss: (0.3360) |  Loss2: (0.0000) | Acc: (88.00%) (12589/14208)
Epoch: 62 | Batch_idx: 120 |  Loss: (0.3360) |  Loss2: (0.0000) | Acc: (88.00%) (13726/15488)
Epoch: 62 | Batch_idx: 130 |  Loss: (0.3354) |  Loss2: (0.0000) | Acc: (88.00%) (14865/16768)
Epoch: 62 | Batch_idx: 140 |  Loss: (0.3359) |  Loss2: (0.0000) | Acc: (88.00%) (15993/18048)
Epoch: 62 | Batch_idx: 150 |  Loss: (0.3377) |  Loss2: (0.0000) | Acc: (88.00%) (17110/19328)
Epoch: 62 | Batch_idx: 160 |  Loss: (0.3364) |  Loss2: (0.0000) | Acc: (88.00%) (18248/20608)
Epoch: 62 | Batch_idx: 170 |  Loss: (0.3364) |  Loss2: (0.0000) | Acc: (88.00%) (19377/21888)
Epoch: 62 | Batch_idx: 180 |  Loss: (0.3355) |  Loss2: (0.0000) | Acc: (88.00%) (20528/23168)
Epoch: 62 | Batch_idx: 190 |  Loss: (0.3364) |  Loss2: (0.0000) | Acc: (88.00%) (21654/24448)
Epoch: 62 | Batch_idx: 200 |  Loss: (0.3373) |  Loss2: (0.0000) | Acc: (88.00%) (22778/25728)
Epoch: 62 | Batch_idx: 210 |  Loss: (0.3367) |  Loss2: (0.0000) | Acc: (88.00%) (23908/27008)
Epoch: 62 | Batch_idx: 220 |  Loss: (0.3365) |  Loss2: (0.0000) | Acc: (88.00%) (25049/28288)
Epoch: 62 | Batch_idx: 230 |  Loss: (0.3380) |  Loss2: (0.0000) | Acc: (88.00%) (26177/29568)
Epoch: 62 | Batch_idx: 240 |  Loss: (0.3385) |  Loss2: (0.0000) | Acc: (88.00%) (27292/30848)
Epoch: 62 | Batch_idx: 250 |  Loss: (0.3384) |  Loss2: (0.0000) | Acc: (88.00%) (28423/32128)
Epoch: 62 | Batch_idx: 260 |  Loss: (0.3384) |  Loss2: (0.0000) | Acc: (88.00%) (29563/33408)
Epoch: 62 | Batch_idx: 270 |  Loss: (0.3391) |  Loss2: (0.0000) | Acc: (88.00%) (30680/34688)
Epoch: 62 | Batch_idx: 280 |  Loss: (0.3393) |  Loss2: (0.0000) | Acc: (88.00%) (31803/35968)
Epoch: 62 | Batch_idx: 290 |  Loss: (0.3407) |  Loss2: (0.0000) | Acc: (88.00%) (32915/37248)
Epoch: 62 | Batch_idx: 300 |  Loss: (0.3406) |  Loss2: (0.0000) | Acc: (88.00%) (34045/38528)
Epoch: 62 | Batch_idx: 310 |  Loss: (0.3414) |  Loss2: (0.0000) | Acc: (88.00%) (35168/39808)
Epoch: 62 | Batch_idx: 320 |  Loss: (0.3411) |  Loss2: (0.0000) | Acc: (88.00%) (36289/41088)
Epoch: 62 | Batch_idx: 330 |  Loss: (0.3408) |  Loss2: (0.0000) | Acc: (88.00%) (37422/42368)
Epoch: 62 | Batch_idx: 340 |  Loss: (0.3413) |  Loss2: (0.0000) | Acc: (88.00%) (38550/43648)
Epoch: 62 | Batch_idx: 350 |  Loss: (0.3416) |  Loss2: (0.0000) | Acc: (88.00%) (39672/44928)
Epoch: 62 | Batch_idx: 360 |  Loss: (0.3425) |  Loss2: (0.0000) | Acc: (88.00%) (40784/46208)
Epoch: 62 | Batch_idx: 370 |  Loss: (0.3425) |  Loss2: (0.0000) | Acc: (88.00%) (41909/47488)
Epoch: 62 | Batch_idx: 380 |  Loss: (0.3423) |  Loss2: (0.0000) | Acc: (88.00%) (43045/48768)
Epoch: 62 | Batch_idx: 390 |  Loss: (0.3432) |  Loss2: (0.0000) | Acc: (88.00%) (44103/50000)
# TEST : Loss: (0.5365) | Acc: (83.00%) (8356/10000)
percent tensor([0.5719, 0.4281], device='cuda:0')
percent tensor([0.5353, 0.4647], device='cuda:0')
percent tensor([0.5100, 0.4900], device='cuda:0')
percent tensor([0.5741, 0.4259], device='cuda:0')
percent tensor([0.5895, 0.4105], device='cuda:0')
percent tensor([0.7240, 0.2760], device='cuda:0')
percent tensor([0.6968, 0.3032], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 63 | Batch_idx: 0 |  Loss: (0.2463) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 63 | Batch_idx: 10 |  Loss: (0.3295) |  Loss2: (0.0000) | Acc: (88.00%) (1242/1408)
Epoch: 63 | Batch_idx: 20 |  Loss: (0.3336) |  Loss2: (0.0000) | Acc: (88.00%) (2369/2688)
Epoch: 63 | Batch_idx: 30 |  Loss: (0.3363) |  Loss2: (0.0000) | Acc: (88.00%) (3497/3968)
Epoch: 63 | Batch_idx: 40 |  Loss: (0.3352) |  Loss2: (0.0000) | Acc: (88.00%) (4627/5248)
Epoch: 63 | Batch_idx: 50 |  Loss: (0.3358) |  Loss2: (0.0000) | Acc: (88.00%) (5762/6528)
Epoch: 63 | Batch_idx: 60 |  Loss: (0.3326) |  Loss2: (0.0000) | Acc: (88.00%) (6896/7808)
Epoch: 63 | Batch_idx: 70 |  Loss: (0.3295) |  Loss2: (0.0000) | Acc: (88.00%) (8038/9088)
Epoch: 63 | Batch_idx: 80 |  Loss: (0.3275) |  Loss2: (0.0000) | Acc: (88.00%) (9192/10368)
Epoch: 63 | Batch_idx: 90 |  Loss: (0.3290) |  Loss2: (0.0000) | Acc: (88.00%) (10316/11648)
Epoch: 63 | Batch_idx: 100 |  Loss: (0.3273) |  Loss2: (0.0000) | Acc: (88.00%) (11450/12928)
Epoch: 63 | Batch_idx: 110 |  Loss: (0.3272) |  Loss2: (0.0000) | Acc: (88.00%) (12590/14208)
Epoch: 63 | Batch_idx: 120 |  Loss: (0.3268) |  Loss2: (0.0000) | Acc: (88.00%) (13724/15488)
Epoch: 63 | Batch_idx: 130 |  Loss: (0.3251) |  Loss2: (0.0000) | Acc: (88.00%) (14864/16768)
Epoch: 63 | Batch_idx: 140 |  Loss: (0.3250) |  Loss2: (0.0000) | Acc: (88.00%) (16000/18048)
Epoch: 63 | Batch_idx: 150 |  Loss: (0.3234) |  Loss2: (0.0000) | Acc: (88.00%) (17150/19328)
Epoch: 63 | Batch_idx: 160 |  Loss: (0.3229) |  Loss2: (0.0000) | Acc: (88.00%) (18301/20608)
Epoch: 63 | Batch_idx: 170 |  Loss: (0.3229) |  Loss2: (0.0000) | Acc: (88.00%) (19435/21888)
Epoch: 63 | Batch_idx: 180 |  Loss: (0.3227) |  Loss2: (0.0000) | Acc: (88.00%) (20571/23168)
Epoch: 63 | Batch_idx: 190 |  Loss: (0.3238) |  Loss2: (0.0000) | Acc: (88.00%) (21703/24448)
Epoch: 63 | Batch_idx: 200 |  Loss: (0.3220) |  Loss2: (0.0000) | Acc: (88.00%) (22846/25728)
Epoch: 63 | Batch_idx: 210 |  Loss: (0.3244) |  Loss2: (0.0000) | Acc: (88.00%) (23965/27008)
Epoch: 63 | Batch_idx: 220 |  Loss: (0.3261) |  Loss2: (0.0000) | Acc: (88.00%) (25077/28288)
Epoch: 63 | Batch_idx: 230 |  Loss: (0.3270) |  Loss2: (0.0000) | Acc: (88.00%) (26205/29568)
Epoch: 63 | Batch_idx: 240 |  Loss: (0.3264) |  Loss2: (0.0000) | Acc: (88.00%) (27343/30848)
Epoch: 63 | Batch_idx: 250 |  Loss: (0.3268) |  Loss2: (0.0000) | Acc: (88.00%) (28464/32128)
Epoch: 63 | Batch_idx: 260 |  Loss: (0.3269) |  Loss2: (0.0000) | Acc: (88.00%) (29603/33408)
Epoch: 63 | Batch_idx: 270 |  Loss: (0.3270) |  Loss2: (0.0000) | Acc: (88.00%) (30734/34688)
Epoch: 63 | Batch_idx: 280 |  Loss: (0.3271) |  Loss2: (0.0000) | Acc: (88.00%) (31868/35968)
Epoch: 63 | Batch_idx: 290 |  Loss: (0.3280) |  Loss2: (0.0000) | Acc: (88.00%) (32990/37248)
Epoch: 63 | Batch_idx: 300 |  Loss: (0.3282) |  Loss2: (0.0000) | Acc: (88.00%) (34117/38528)
Epoch: 63 | Batch_idx: 310 |  Loss: (0.3287) |  Loss2: (0.0000) | Acc: (88.00%) (35249/39808)
Epoch: 63 | Batch_idx: 320 |  Loss: (0.3299) |  Loss2: (0.0000) | Acc: (88.00%) (36364/41088)
Epoch: 63 | Batch_idx: 330 |  Loss: (0.3292) |  Loss2: (0.0000) | Acc: (88.00%) (37507/42368)
Epoch: 63 | Batch_idx: 340 |  Loss: (0.3283) |  Loss2: (0.0000) | Acc: (88.00%) (38656/43648)
Epoch: 63 | Batch_idx: 350 |  Loss: (0.3287) |  Loss2: (0.0000) | Acc: (88.00%) (39789/44928)
Epoch: 63 | Batch_idx: 360 |  Loss: (0.3299) |  Loss2: (0.0000) | Acc: (88.00%) (40911/46208)
Epoch: 63 | Batch_idx: 370 |  Loss: (0.3296) |  Loss2: (0.0000) | Acc: (88.00%) (42054/47488)
Epoch: 63 | Batch_idx: 380 |  Loss: (0.3298) |  Loss2: (0.0000) | Acc: (88.00%) (43199/48768)
Epoch: 63 | Batch_idx: 390 |  Loss: (0.3308) |  Loss2: (0.0000) | Acc: (88.00%) (44288/50000)
# TEST : Loss: (0.5133) | Acc: (83.00%) (8325/10000)
percent tensor([0.5723, 0.4277], device='cuda:0')
percent tensor([0.5355, 0.4645], device='cuda:0')
percent tensor([0.5104, 0.4896], device='cuda:0')
percent tensor([0.5739, 0.4261], device='cuda:0')
percent tensor([0.5900, 0.4100], device='cuda:0')
percent tensor([0.7216, 0.2784], device='cuda:0')
percent tensor([0.6899, 0.3101], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
Epoch: 64 | Batch_idx: 0 |  Loss: (0.3146) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 64 | Batch_idx: 10 |  Loss: (0.3512) |  Loss2: (0.0000) | Acc: (88.00%) (1245/1408)
Epoch: 64 | Batch_idx: 20 |  Loss: (0.3292) |  Loss2: (0.0000) | Acc: (89.00%) (2398/2688)
Epoch: 64 | Batch_idx: 30 |  Loss: (0.3280) |  Loss2: (0.0000) | Acc: (89.00%) (3541/3968)
Epoch: 64 | Batch_idx: 40 |  Loss: (0.3282) |  Loss2: (0.0000) | Acc: (89.00%) (4679/5248)
Epoch: 64 | Batch_idx: 50 |  Loss: (0.3249) |  Loss2: (0.0000) | Acc: (89.00%) (5836/6528)
Epoch: 64 | Batch_idx: 60 |  Loss: (0.3258) |  Loss2: (0.0000) | Acc: (89.00%) (6966/7808)
Epoch: 64 | Batch_idx: 70 |  Loss: (0.3271) |  Loss2: (0.0000) | Acc: (89.00%) (8102/9088)
Epoch: 64 | Batch_idx: 80 |  Loss: (0.3226) |  Loss2: (0.0000) | Acc: (89.00%) (9253/10368)
Epoch: 64 | Batch_idx: 90 |  Loss: (0.3187) |  Loss2: (0.0000) | Acc: (89.00%) (10405/11648)
Epoch: 64 | Batch_idx: 100 |  Loss: (0.3176) |  Loss2: (0.0000) | Acc: (89.00%) (11545/12928)
Epoch: 64 | Batch_idx: 110 |  Loss: (0.3210) |  Loss2: (0.0000) | Acc: (89.00%) (12651/14208)
Epoch: 64 | Batch_idx: 120 |  Loss: (0.3189) |  Loss2: (0.0000) | Acc: (89.00%) (13790/15488)
Epoch: 64 | Batch_idx: 130 |  Loss: (0.3207) |  Loss2: (0.0000) | Acc: (88.00%) (14910/16768)
Epoch: 64 | Batch_idx: 140 |  Loss: (0.3208) |  Loss2: (0.0000) | Acc: (88.00%) (16048/18048)
Epoch: 64 | Batch_idx: 150 |  Loss: (0.3202) |  Loss2: (0.0000) | Acc: (88.00%) (17189/19328)
Epoch: 64 | Batch_idx: 160 |  Loss: (0.3218) |  Loss2: (0.0000) | Acc: (88.00%) (18312/20608)
Epoch: 64 | Batch_idx: 170 |  Loss: (0.3226) |  Loss2: (0.0000) | Acc: (88.00%) (19437/21888)
Epoch: 64 | Batch_idx: 180 |  Loss: (0.3223) |  Loss2: (0.0000) | Acc: (88.00%) (20583/23168)
Epoch: 64 | Batch_idx: 190 |  Loss: (0.3206) |  Loss2: (0.0000) | Acc: (88.00%) (21735/24448)
Epoch: 64 | Batch_idx: 200 |  Loss: (0.3203) |  Loss2: (0.0000) | Acc: (88.00%) (22884/25728)
Epoch: 64 | Batch_idx: 210 |  Loss: (0.3203) |  Loss2: (0.0000) | Acc: (88.00%) (24024/27008)
Epoch: 64 | Batch_idx: 220 |  Loss: (0.3206) |  Loss2: (0.0000) | Acc: (88.00%) (25169/28288)
Epoch: 64 | Batch_idx: 230 |  Loss: (0.3192) |  Loss2: (0.0000) | Acc: (89.00%) (26318/29568)
Epoch: 64 | Batch_idx: 240 |  Loss: (0.3185) |  Loss2: (0.0000) | Acc: (89.00%) (27468/30848)
Epoch: 64 | Batch_idx: 250 |  Loss: (0.3185) |  Loss2: (0.0000) | Acc: (89.00%) (28604/32128)
Epoch: 64 | Batch_idx: 260 |  Loss: (0.3181) |  Loss2: (0.0000) | Acc: (89.00%) (29751/33408)
Epoch: 64 | Batch_idx: 270 |  Loss: (0.3184) |  Loss2: (0.0000) | Acc: (89.00%) (30888/34688)
Epoch: 64 | Batch_idx: 280 |  Loss: (0.3190) |  Loss2: (0.0000) | Acc: (89.00%) (32020/35968)
Epoch: 64 | Batch_idx: 290 |  Loss: (0.3190) |  Loss2: (0.0000) | Acc: (89.00%) (33164/37248)
Epoch: 64 | Batch_idx: 300 |  Loss: (0.3185) |  Loss2: (0.0000) | Acc: (89.00%) (34302/38528)
Epoch: 64 | Batch_idx: 310 |  Loss: (0.3182) |  Loss2: (0.0000) | Acc: (89.00%) (35446/39808)
Epoch: 64 | Batch_idx: 320 |  Loss: (0.3178) |  Loss2: (0.0000) | Acc: (89.00%) (36597/41088)
Epoch: 64 | Batch_idx: 330 |  Loss: (0.3179) |  Loss2: (0.0000) | Acc: (89.00%) (37725/42368)
Epoch: 64 | Batch_idx: 340 |  Loss: (0.3188) |  Loss2: (0.0000) | Acc: (89.00%) (38847/43648)
Epoch: 64 | Batch_idx: 350 |  Loss: (0.3187) |  Loss2: (0.0000) | Acc: (89.00%) (40001/44928)
Epoch: 64 | Batch_idx: 360 |  Loss: (0.3190) |  Loss2: (0.0000) | Acc: (89.00%) (41147/46208)
Epoch: 64 | Batch_idx: 370 |  Loss: (0.3199) |  Loss2: (0.0000) | Acc: (89.00%) (42269/47488)
Epoch: 64 | Batch_idx: 380 |  Loss: (0.3212) |  Loss2: (0.0000) | Acc: (88.00%) (43400/48768)
Epoch: 64 | Batch_idx: 390 |  Loss: (0.3215) |  Loss2: (0.0000) | Acc: (88.00%) (44489/50000)
# TEST : Loss: (0.4979) | Acc: (84.00%) (8444/10000)
percent tensor([0.5712, 0.4288], device='cuda:0')
percent tensor([0.5353, 0.4647], device='cuda:0')
percent tensor([0.5099, 0.4901], device='cuda:0')
percent tensor([0.5742, 0.4258], device='cuda:0')
percent tensor([0.5918, 0.4082], device='cuda:0')
percent tensor([0.7213, 0.2787], device='cuda:0')
percent tensor([0.6950, 0.3050], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 65 | Batch_idx: 0 |  Loss: (0.2625) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 65 | Batch_idx: 10 |  Loss: (0.3379) |  Loss2: (0.0000) | Acc: (87.00%) (1233/1408)
Epoch: 65 | Batch_idx: 20 |  Loss: (0.3724) |  Loss2: (0.0000) | Acc: (87.00%) (2339/2688)
Epoch: 65 | Batch_idx: 30 |  Loss: (0.4093) |  Loss2: (0.0000) | Acc: (85.00%) (3403/3968)
Epoch: 65 | Batch_idx: 40 |  Loss: (0.4254) |  Loss2: (0.0000) | Acc: (85.00%) (4479/5248)
Epoch: 65 | Batch_idx: 50 |  Loss: (0.4322) |  Loss2: (0.0000) | Acc: (84.00%) (5535/6528)
Epoch: 65 | Batch_idx: 60 |  Loss: (0.4350) |  Loss2: (0.0000) | Acc: (84.00%) (6618/7808)
Epoch: 65 | Batch_idx: 70 |  Loss: (0.4356) |  Loss2: (0.0000) | Acc: (84.00%) (7707/9088)
Epoch: 65 | Batch_idx: 80 |  Loss: (0.4361) |  Loss2: (0.0000) | Acc: (84.00%) (8770/10368)
Epoch: 65 | Batch_idx: 90 |  Loss: (0.4340) |  Loss2: (0.0000) | Acc: (84.00%) (9859/11648)
Epoch: 65 | Batch_idx: 100 |  Loss: (0.4353) |  Loss2: (0.0000) | Acc: (84.00%) (10937/12928)
Epoch: 65 | Batch_idx: 110 |  Loss: (0.4334) |  Loss2: (0.0000) | Acc: (84.00%) (12018/14208)
Epoch: 65 | Batch_idx: 120 |  Loss: (0.4334) |  Loss2: (0.0000) | Acc: (84.00%) (13100/15488)
Epoch: 65 | Batch_idx: 130 |  Loss: (0.4296) |  Loss2: (0.0000) | Acc: (84.00%) (14211/16768)
Epoch: 65 | Batch_idx: 140 |  Loss: (0.4311) |  Loss2: (0.0000) | Acc: (84.00%) (15273/18048)
Epoch: 65 | Batch_idx: 150 |  Loss: (0.4318) |  Loss2: (0.0000) | Acc: (84.00%) (16354/19328)
Epoch: 65 | Batch_idx: 160 |  Loss: (0.4306) |  Loss2: (0.0000) | Acc: (84.00%) (17446/20608)
Epoch: 65 | Batch_idx: 170 |  Loss: (0.4286) |  Loss2: (0.0000) | Acc: (84.00%) (18544/21888)
Epoch: 65 | Batch_idx: 180 |  Loss: (0.4264) |  Loss2: (0.0000) | Acc: (84.00%) (19643/23168)
Epoch: 65 | Batch_idx: 190 |  Loss: (0.4265) |  Loss2: (0.0000) | Acc: (84.00%) (20728/24448)
Epoch: 65 | Batch_idx: 200 |  Loss: (0.4285) |  Loss2: (0.0000) | Acc: (84.00%) (21803/25728)
Epoch: 65 | Batch_idx: 210 |  Loss: (0.4312) |  Loss2: (0.0000) | Acc: (84.00%) (22868/27008)
Epoch: 65 | Batch_idx: 220 |  Loss: (0.4291) |  Loss2: (0.0000) | Acc: (84.00%) (23968/28288)
Epoch: 65 | Batch_idx: 230 |  Loss: (0.4292) |  Loss2: (0.0000) | Acc: (84.00%) (25056/29568)
Epoch: 65 | Batch_idx: 240 |  Loss: (0.4276) |  Loss2: (0.0000) | Acc: (84.00%) (26156/30848)
Epoch: 65 | Batch_idx: 250 |  Loss: (0.4261) |  Loss2: (0.0000) | Acc: (84.00%) (27273/32128)
Epoch: 65 | Batch_idx: 260 |  Loss: (0.4241) |  Loss2: (0.0000) | Acc: (84.00%) (28383/33408)
Epoch: 65 | Batch_idx: 270 |  Loss: (0.4217) |  Loss2: (0.0000) | Acc: (85.00%) (29490/34688)
Epoch: 65 | Batch_idx: 280 |  Loss: (0.4205) |  Loss2: (0.0000) | Acc: (85.00%) (30598/35968)
Epoch: 65 | Batch_idx: 290 |  Loss: (0.4200) |  Loss2: (0.0000) | Acc: (85.00%) (31700/37248)
Epoch: 65 | Batch_idx: 300 |  Loss: (0.4182) |  Loss2: (0.0000) | Acc: (85.00%) (32817/38528)
Epoch: 65 | Batch_idx: 310 |  Loss: (0.4175) |  Loss2: (0.0000) | Acc: (85.00%) (33919/39808)
Epoch: 65 | Batch_idx: 320 |  Loss: (0.4163) |  Loss2: (0.0000) | Acc: (85.00%) (35034/41088)
Epoch: 65 | Batch_idx: 330 |  Loss: (0.4168) |  Loss2: (0.0000) | Acc: (85.00%) (36111/42368)
Epoch: 65 | Batch_idx: 340 |  Loss: (0.4149) |  Loss2: (0.0000) | Acc: (85.00%) (37227/43648)
Epoch: 65 | Batch_idx: 350 |  Loss: (0.4142) |  Loss2: (0.0000) | Acc: (85.00%) (38341/44928)
Epoch: 65 | Batch_idx: 360 |  Loss: (0.4137) |  Loss2: (0.0000) | Acc: (85.00%) (39447/46208)
Epoch: 65 | Batch_idx: 370 |  Loss: (0.4138) |  Loss2: (0.0000) | Acc: (85.00%) (40540/47488)
Epoch: 65 | Batch_idx: 380 |  Loss: (0.4134) |  Loss2: (0.0000) | Acc: (85.00%) (41650/48768)
Epoch: 65 | Batch_idx: 390 |  Loss: (0.4116) |  Loss2: (0.0000) | Acc: (85.00%) (42728/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_065.pth.tar'
# TEST : Loss: (0.5212) | Acc: (83.00%) (8326/10000)
percent tensor([0.5596, 0.4404], device='cuda:0')
percent tensor([0.5344, 0.4656], device='cuda:0')
percent tensor([0.5156, 0.4844], device='cuda:0')
percent tensor([0.6194, 0.3806], device='cuda:0')
percent tensor([0.5954, 0.4046], device='cuda:0')
percent tensor([0.6958, 0.3042], device='cuda:0')
percent tensor([0.6514, 0.3486], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
Epoch: 66 | Batch_idx: 0 |  Loss: (0.4931) |  Loss2: (0.0000) | Acc: (87.00%) (112/128)
Epoch: 66 | Batch_idx: 10 |  Loss: (0.3710) |  Loss2: (0.0000) | Acc: (86.00%) (1222/1408)
Epoch: 66 | Batch_idx: 20 |  Loss: (0.3873) |  Loss2: (0.0000) | Acc: (86.00%) (2324/2688)
Epoch: 66 | Batch_idx: 30 |  Loss: (0.3787) |  Loss2: (0.0000) | Acc: (86.00%) (3443/3968)
Epoch: 66 | Batch_idx: 40 |  Loss: (0.3821) |  Loss2: (0.0000) | Acc: (86.00%) (4550/5248)
Epoch: 66 | Batch_idx: 50 |  Loss: (0.3797) |  Loss2: (0.0000) | Acc: (86.00%) (5663/6528)
Epoch: 66 | Batch_idx: 60 |  Loss: (0.3791) |  Loss2: (0.0000) | Acc: (86.00%) (6778/7808)
Epoch: 66 | Batch_idx: 70 |  Loss: (0.3784) |  Loss2: (0.0000) | Acc: (86.00%) (7904/9088)
Epoch: 66 | Batch_idx: 80 |  Loss: (0.3763) |  Loss2: (0.0000) | Acc: (87.00%) (9021/10368)
Epoch: 66 | Batch_idx: 90 |  Loss: (0.3793) |  Loss2: (0.0000) | Acc: (86.00%) (10125/11648)
Epoch: 66 | Batch_idx: 100 |  Loss: (0.3802) |  Loss2: (0.0000) | Acc: (86.00%) (11240/12928)
Epoch: 66 | Batch_idx: 110 |  Loss: (0.3819) |  Loss2: (0.0000) | Acc: (86.00%) (12336/14208)
Epoch: 66 | Batch_idx: 120 |  Loss: (0.3793) |  Loss2: (0.0000) | Acc: (86.00%) (13454/15488)
Epoch: 66 | Batch_idx: 130 |  Loss: (0.3763) |  Loss2: (0.0000) | Acc: (87.00%) (14595/16768)
Epoch: 66 | Batch_idx: 140 |  Loss: (0.3774) |  Loss2: (0.0000) | Acc: (86.00%) (15690/18048)
Epoch: 66 | Batch_idx: 150 |  Loss: (0.3761) |  Loss2: (0.0000) | Acc: (87.00%) (16817/19328)
Epoch: 66 | Batch_idx: 160 |  Loss: (0.3727) |  Loss2: (0.0000) | Acc: (87.00%) (17962/20608)
Epoch: 66 | Batch_idx: 170 |  Loss: (0.3740) |  Loss2: (0.0000) | Acc: (87.00%) (19066/21888)
Epoch: 66 | Batch_idx: 180 |  Loss: (0.3756) |  Loss2: (0.0000) | Acc: (87.00%) (20163/23168)
Epoch: 66 | Batch_idx: 190 |  Loss: (0.3759) |  Loss2: (0.0000) | Acc: (87.00%) (21272/24448)
Epoch: 66 | Batch_idx: 200 |  Loss: (0.3772) |  Loss2: (0.0000) | Acc: (86.00%) (22380/25728)
Epoch: 66 | Batch_idx: 210 |  Loss: (0.3759) |  Loss2: (0.0000) | Acc: (87.00%) (23499/27008)
Epoch: 66 | Batch_idx: 220 |  Loss: (0.3737) |  Loss2: (0.0000) | Acc: (87.00%) (24637/28288)
Epoch: 66 | Batch_idx: 230 |  Loss: (0.3716) |  Loss2: (0.0000) | Acc: (87.00%) (25778/29568)
Epoch: 66 | Batch_idx: 240 |  Loss: (0.3717) |  Loss2: (0.0000) | Acc: (87.00%) (26891/30848)
Epoch: 66 | Batch_idx: 250 |  Loss: (0.3719) |  Loss2: (0.0000) | Acc: (87.00%) (28008/32128)
Epoch: 66 | Batch_idx: 260 |  Loss: (0.3713) |  Loss2: (0.0000) | Acc: (87.00%) (29134/33408)
Epoch: 66 | Batch_idx: 270 |  Loss: (0.3706) |  Loss2: (0.0000) | Acc: (87.00%) (30241/34688)
Epoch: 66 | Batch_idx: 280 |  Loss: (0.3714) |  Loss2: (0.0000) | Acc: (87.00%) (31348/35968)
Epoch: 66 | Batch_idx: 290 |  Loss: (0.3702) |  Loss2: (0.0000) | Acc: (87.00%) (32465/37248)
Epoch: 66 | Batch_idx: 300 |  Loss: (0.3696) |  Loss2: (0.0000) | Acc: (87.00%) (33604/38528)
Epoch: 66 | Batch_idx: 310 |  Loss: (0.3690) |  Loss2: (0.0000) | Acc: (87.00%) (34735/39808)
Epoch: 66 | Batch_idx: 320 |  Loss: (0.3682) |  Loss2: (0.0000) | Acc: (87.00%) (35859/41088)
Epoch: 66 | Batch_idx: 330 |  Loss: (0.3672) |  Loss2: (0.0000) | Acc: (87.00%) (36994/42368)
Epoch: 66 | Batch_idx: 340 |  Loss: (0.3675) |  Loss2: (0.0000) | Acc: (87.00%) (38089/43648)
Epoch: 66 | Batch_idx: 350 |  Loss: (0.3658) |  Loss2: (0.0000) | Acc: (87.00%) (39235/44928)
Epoch: 66 | Batch_idx: 360 |  Loss: (0.3660) |  Loss2: (0.0000) | Acc: (87.00%) (40337/46208)
Epoch: 66 | Batch_idx: 370 |  Loss: (0.3654) |  Loss2: (0.0000) | Acc: (87.00%) (41463/47488)
Epoch: 66 | Batch_idx: 380 |  Loss: (0.3644) |  Loss2: (0.0000) | Acc: (87.00%) (42598/48768)
Epoch: 66 | Batch_idx: 390 |  Loss: (0.3651) |  Loss2: (0.0000) | Acc: (87.00%) (43659/50000)
# TEST : Loss: (0.4829) | Acc: (84.00%) (8415/10000)
percent tensor([0.5668, 0.4332], device='cuda:0')
percent tensor([0.5331, 0.4669], device='cuda:0')
percent tensor([0.5184, 0.4816], device='cuda:0')
percent tensor([0.6313, 0.3687], device='cuda:0')
percent tensor([0.5844, 0.4156], device='cuda:0')
percent tensor([0.6956, 0.3044], device='cuda:0')
percent tensor([0.6518, 0.3482], device='cuda:0')
percent tensor([0.9988, 0.0012], device='cuda:0')
Epoch: 67 | Batch_idx: 0 |  Loss: (0.3193) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 67 | Batch_idx: 10 |  Loss: (0.3223) |  Loss2: (0.0000) | Acc: (89.00%) (1255/1408)
Epoch: 67 | Batch_idx: 20 |  Loss: (0.3284) |  Loss2: (0.0000) | Acc: (88.00%) (2386/2688)
Epoch: 67 | Batch_idx: 30 |  Loss: (0.3464) |  Loss2: (0.0000) | Acc: (87.00%) (3489/3968)
Epoch: 67 | Batch_idx: 40 |  Loss: (0.3459) |  Loss2: (0.0000) | Acc: (88.00%) (4621/5248)
Epoch: 67 | Batch_idx: 50 |  Loss: (0.3447) |  Loss2: (0.0000) | Acc: (88.00%) (5745/6528)
Epoch: 67 | Batch_idx: 60 |  Loss: (0.3470) |  Loss2: (0.0000) | Acc: (87.00%) (6864/7808)
Epoch: 67 | Batch_idx: 70 |  Loss: (0.3445) |  Loss2: (0.0000) | Acc: (87.00%) (7991/9088)
Epoch: 67 | Batch_idx: 80 |  Loss: (0.3495) |  Loss2: (0.0000) | Acc: (87.00%) (9106/10368)
Epoch: 67 | Batch_idx: 90 |  Loss: (0.3463) |  Loss2: (0.0000) | Acc: (87.00%) (10247/11648)
Epoch: 67 | Batch_idx: 100 |  Loss: (0.3464) |  Loss2: (0.0000) | Acc: (87.00%) (11368/12928)
Epoch: 67 | Batch_idx: 110 |  Loss: (0.3510) |  Loss2: (0.0000) | Acc: (87.00%) (12477/14208)
Epoch: 67 | Batch_idx: 120 |  Loss: (0.3510) |  Loss2: (0.0000) | Acc: (87.00%) (13600/15488)
Epoch: 67 | Batch_idx: 130 |  Loss: (0.3503) |  Loss2: (0.0000) | Acc: (87.00%) (14731/16768)
Epoch: 67 | Batch_idx: 140 |  Loss: (0.3507) |  Loss2: (0.0000) | Acc: (87.00%) (15852/18048)
Epoch: 67 | Batch_idx: 150 |  Loss: (0.3506) |  Loss2: (0.0000) | Acc: (87.00%) (16980/19328)
Epoch: 67 | Batch_idx: 160 |  Loss: (0.3497) |  Loss2: (0.0000) | Acc: (87.00%) (18102/20608)
Epoch: 67 | Batch_idx: 170 |  Loss: (0.3477) |  Loss2: (0.0000) | Acc: (87.00%) (19237/21888)
Epoch: 67 | Batch_idx: 180 |  Loss: (0.3484) |  Loss2: (0.0000) | Acc: (87.00%) (20359/23168)
Epoch: 67 | Batch_idx: 190 |  Loss: (0.3473) |  Loss2: (0.0000) | Acc: (87.00%) (21488/24448)
Epoch: 67 | Batch_idx: 200 |  Loss: (0.3478) |  Loss2: (0.0000) | Acc: (87.00%) (22610/25728)
Epoch: 67 | Batch_idx: 210 |  Loss: (0.3472) |  Loss2: (0.0000) | Acc: (87.00%) (23737/27008)
Epoch: 67 | Batch_idx: 220 |  Loss: (0.3471) |  Loss2: (0.0000) | Acc: (87.00%) (24863/28288)
Epoch: 67 | Batch_idx: 230 |  Loss: (0.3477) |  Loss2: (0.0000) | Acc: (87.00%) (25983/29568)
Epoch: 67 | Batch_idx: 240 |  Loss: (0.3467) |  Loss2: (0.0000) | Acc: (87.00%) (27120/30848)
Epoch: 67 | Batch_idx: 250 |  Loss: (0.3453) |  Loss2: (0.0000) | Acc: (87.00%) (28263/32128)
Epoch: 67 | Batch_idx: 260 |  Loss: (0.3446) |  Loss2: (0.0000) | Acc: (88.00%) (29401/33408)
Epoch: 67 | Batch_idx: 270 |  Loss: (0.3446) |  Loss2: (0.0000) | Acc: (88.00%) (30537/34688)
Epoch: 67 | Batch_idx: 280 |  Loss: (0.3457) |  Loss2: (0.0000) | Acc: (88.00%) (31662/35968)
Epoch: 67 | Batch_idx: 290 |  Loss: (0.3465) |  Loss2: (0.0000) | Acc: (88.00%) (32779/37248)
Epoch: 67 | Batch_idx: 300 |  Loss: (0.3466) |  Loss2: (0.0000) | Acc: (88.00%) (33909/38528)
Epoch: 67 | Batch_idx: 310 |  Loss: (0.3468) |  Loss2: (0.0000) | Acc: (87.00%) (35030/39808)
Epoch: 67 | Batch_idx: 320 |  Loss: (0.3465) |  Loss2: (0.0000) | Acc: (88.00%) (36163/41088)
Epoch: 67 | Batch_idx: 330 |  Loss: (0.3461) |  Loss2: (0.0000) | Acc: (88.00%) (37303/42368)
Epoch: 67 | Batch_idx: 340 |  Loss: (0.3459) |  Loss2: (0.0000) | Acc: (88.00%) (38441/43648)
Epoch: 67 | Batch_idx: 350 |  Loss: (0.3461) |  Loss2: (0.0000) | Acc: (88.00%) (39553/44928)
Epoch: 67 | Batch_idx: 360 |  Loss: (0.3454) |  Loss2: (0.0000) | Acc: (88.00%) (40695/46208)
Epoch: 67 | Batch_idx: 370 |  Loss: (0.3449) |  Loss2: (0.0000) | Acc: (88.00%) (41826/47488)
Epoch: 67 | Batch_idx: 380 |  Loss: (0.3451) |  Loss2: (0.0000) | Acc: (88.00%) (42949/48768)
Epoch: 67 | Batch_idx: 390 |  Loss: (0.3453) |  Loss2: (0.0000) | Acc: (88.00%) (44015/50000)
# TEST : Loss: (0.4669) | Acc: (84.00%) (8497/10000)
percent tensor([0.5721, 0.4279], device='cuda:0')
percent tensor([0.5332, 0.4668], device='cuda:0')
percent tensor([0.5183, 0.4817], device='cuda:0')
percent tensor([0.6276, 0.3724], device='cuda:0')
percent tensor([0.5817, 0.4183], device='cuda:0')
percent tensor([0.6983, 0.3017], device='cuda:0')
percent tensor([0.6625, 0.3375], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
Epoch: 68 | Batch_idx: 0 |  Loss: (0.2998) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 68 | Batch_idx: 10 |  Loss: (0.3331) |  Loss2: (0.0000) | Acc: (89.00%) (1263/1408)
Epoch: 68 | Batch_idx: 20 |  Loss: (0.3529) |  Loss2: (0.0000) | Acc: (88.00%) (2372/2688)
Epoch: 68 | Batch_idx: 30 |  Loss: (0.3524) |  Loss2: (0.0000) | Acc: (88.00%) (3507/3968)
Epoch: 68 | Batch_idx: 40 |  Loss: (0.3445) |  Loss2: (0.0000) | Acc: (88.00%) (4646/5248)
Epoch: 68 | Batch_idx: 50 |  Loss: (0.3435) |  Loss2: (0.0000) | Acc: (88.00%) (5768/6528)
Epoch: 68 | Batch_idx: 60 |  Loss: (0.3468) |  Loss2: (0.0000) | Acc: (88.00%) (6888/7808)
Epoch: 68 | Batch_idx: 70 |  Loss: (0.3445) |  Loss2: (0.0000) | Acc: (88.00%) (8022/9088)
Epoch: 68 | Batch_idx: 80 |  Loss: (0.3413) |  Loss2: (0.0000) | Acc: (88.00%) (9164/10368)
Epoch: 68 | Batch_idx: 90 |  Loss: (0.3444) |  Loss2: (0.0000) | Acc: (88.00%) (10273/11648)
Epoch: 68 | Batch_idx: 100 |  Loss: (0.3427) |  Loss2: (0.0000) | Acc: (88.00%) (11409/12928)
Epoch: 68 | Batch_idx: 110 |  Loss: (0.3419) |  Loss2: (0.0000) | Acc: (88.00%) (12547/14208)
Epoch: 68 | Batch_idx: 120 |  Loss: (0.3407) |  Loss2: (0.0000) | Acc: (88.00%) (13675/15488)
Epoch: 68 | Batch_idx: 130 |  Loss: (0.3409) |  Loss2: (0.0000) | Acc: (88.00%) (14805/16768)
Epoch: 68 | Batch_idx: 140 |  Loss: (0.3381) |  Loss2: (0.0000) | Acc: (88.00%) (15946/18048)
Epoch: 68 | Batch_idx: 150 |  Loss: (0.3370) |  Loss2: (0.0000) | Acc: (88.00%) (17081/19328)
Epoch: 68 | Batch_idx: 160 |  Loss: (0.3362) |  Loss2: (0.0000) | Acc: (88.00%) (18227/20608)
Epoch: 68 | Batch_idx: 170 |  Loss: (0.3358) |  Loss2: (0.0000) | Acc: (88.00%) (19365/21888)
Epoch: 68 | Batch_idx: 180 |  Loss: (0.3366) |  Loss2: (0.0000) | Acc: (88.00%) (20488/23168)
Epoch: 68 | Batch_idx: 190 |  Loss: (0.3359) |  Loss2: (0.0000) | Acc: (88.00%) (21632/24448)
Epoch: 68 | Batch_idx: 200 |  Loss: (0.3361) |  Loss2: (0.0000) | Acc: (88.00%) (22751/25728)
Epoch: 68 | Batch_idx: 210 |  Loss: (0.3349) |  Loss2: (0.0000) | Acc: (88.00%) (23889/27008)
Epoch: 68 | Batch_idx: 220 |  Loss: (0.3356) |  Loss2: (0.0000) | Acc: (88.00%) (25007/28288)
Epoch: 68 | Batch_idx: 230 |  Loss: (0.3355) |  Loss2: (0.0000) | Acc: (88.00%) (26136/29568)
Epoch: 68 | Batch_idx: 240 |  Loss: (0.3354) |  Loss2: (0.0000) | Acc: (88.00%) (27271/30848)
Epoch: 68 | Batch_idx: 250 |  Loss: (0.3349) |  Loss2: (0.0000) | Acc: (88.00%) (28414/32128)
Epoch: 68 | Batch_idx: 260 |  Loss: (0.3357) |  Loss2: (0.0000) | Acc: (88.00%) (29537/33408)
Epoch: 68 | Batch_idx: 270 |  Loss: (0.3360) |  Loss2: (0.0000) | Acc: (88.00%) (30671/34688)
Epoch: 68 | Batch_idx: 280 |  Loss: (0.3354) |  Loss2: (0.0000) | Acc: (88.00%) (31807/35968)
Epoch: 68 | Batch_idx: 290 |  Loss: (0.3351) |  Loss2: (0.0000) | Acc: (88.00%) (32942/37248)
Epoch: 68 | Batch_idx: 300 |  Loss: (0.3348) |  Loss2: (0.0000) | Acc: (88.00%) (34082/38528)
Epoch: 68 | Batch_idx: 310 |  Loss: (0.3345) |  Loss2: (0.0000) | Acc: (88.00%) (35225/39808)
Epoch: 68 | Batch_idx: 320 |  Loss: (0.3341) |  Loss2: (0.0000) | Acc: (88.00%) (36355/41088)
Epoch: 68 | Batch_idx: 330 |  Loss: (0.3344) |  Loss2: (0.0000) | Acc: (88.00%) (37489/42368)
Epoch: 68 | Batch_idx: 340 |  Loss: (0.3340) |  Loss2: (0.0000) | Acc: (88.00%) (38629/43648)
Epoch: 68 | Batch_idx: 350 |  Loss: (0.3329) |  Loss2: (0.0000) | Acc: (88.00%) (39784/44928)
Epoch: 68 | Batch_idx: 360 |  Loss: (0.3327) |  Loss2: (0.0000) | Acc: (88.00%) (40928/46208)
Epoch: 68 | Batch_idx: 370 |  Loss: (0.3332) |  Loss2: (0.0000) | Acc: (88.00%) (42048/47488)
Epoch: 68 | Batch_idx: 380 |  Loss: (0.3332) |  Loss2: (0.0000) | Acc: (88.00%) (43185/48768)
Epoch: 68 | Batch_idx: 390 |  Loss: (0.3327) |  Loss2: (0.0000) | Acc: (88.00%) (44286/50000)
# TEST : Loss: (0.4553) | Acc: (85.00%) (8513/10000)
percent tensor([0.5716, 0.4284], device='cuda:0')
percent tensor([0.5336, 0.4664], device='cuda:0')
percent tensor([0.5183, 0.4817], device='cuda:0')
percent tensor([0.6324, 0.3676], device='cuda:0')
percent tensor([0.5818, 0.4182], device='cuda:0')
percent tensor([0.7020, 0.2980], device='cuda:0')
percent tensor([0.6651, 0.3349], device='cuda:0')
percent tensor([0.9990, 0.0010], device='cuda:0')
Epoch: 69 | Batch_idx: 0 |  Loss: (0.2387) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 69 | Batch_idx: 10 |  Loss: (0.3188) |  Loss2: (0.0000) | Acc: (89.00%) (1256/1408)
Epoch: 69 | Batch_idx: 20 |  Loss: (0.3262) |  Loss2: (0.0000) | Acc: (88.00%) (2382/2688)
Epoch: 69 | Batch_idx: 30 |  Loss: (0.3222) |  Loss2: (0.0000) | Acc: (88.00%) (3524/3968)
Epoch: 69 | Batch_idx: 40 |  Loss: (0.3302) |  Loss2: (0.0000) | Acc: (88.00%) (4648/5248)
Epoch: 69 | Batch_idx: 50 |  Loss: (0.3243) |  Loss2: (0.0000) | Acc: (88.00%) (5794/6528)
Epoch: 69 | Batch_idx: 60 |  Loss: (0.3245) |  Loss2: (0.0000) | Acc: (88.00%) (6935/7808)
Epoch: 69 | Batch_idx: 70 |  Loss: (0.3269) |  Loss2: (0.0000) | Acc: (88.00%) (8071/9088)
Epoch: 69 | Batch_idx: 80 |  Loss: (0.3269) |  Loss2: (0.0000) | Acc: (88.00%) (9201/10368)
Epoch: 69 | Batch_idx: 90 |  Loss: (0.3302) |  Loss2: (0.0000) | Acc: (88.00%) (10320/11648)
Epoch: 69 | Batch_idx: 100 |  Loss: (0.3326) |  Loss2: (0.0000) | Acc: (88.00%) (11450/12928)
Epoch: 69 | Batch_idx: 110 |  Loss: (0.3296) |  Loss2: (0.0000) | Acc: (88.00%) (12592/14208)
Epoch: 69 | Batch_idx: 120 |  Loss: (0.3294) |  Loss2: (0.0000) | Acc: (88.00%) (13721/15488)
Epoch: 69 | Batch_idx: 130 |  Loss: (0.3299) |  Loss2: (0.0000) | Acc: (88.00%) (14868/16768)
Epoch: 69 | Batch_idx: 140 |  Loss: (0.3302) |  Loss2: (0.0000) | Acc: (88.00%) (15995/18048)
Epoch: 69 | Batch_idx: 150 |  Loss: (0.3300) |  Loss2: (0.0000) | Acc: (88.00%) (17130/19328)
Epoch: 69 | Batch_idx: 160 |  Loss: (0.3318) |  Loss2: (0.0000) | Acc: (88.00%) (18247/20608)
Epoch: 69 | Batch_idx: 170 |  Loss: (0.3305) |  Loss2: (0.0000) | Acc: (88.00%) (19384/21888)
Epoch: 69 | Batch_idx: 180 |  Loss: (0.3302) |  Loss2: (0.0000) | Acc: (88.00%) (20526/23168)
Epoch: 69 | Batch_idx: 190 |  Loss: (0.3317) |  Loss2: (0.0000) | Acc: (88.00%) (21641/24448)
Epoch: 69 | Batch_idx: 200 |  Loss: (0.3307) |  Loss2: (0.0000) | Acc: (88.00%) (22794/25728)
Epoch: 69 | Batch_idx: 210 |  Loss: (0.3314) |  Loss2: (0.0000) | Acc: (88.00%) (23922/27008)
Epoch: 69 | Batch_idx: 220 |  Loss: (0.3310) |  Loss2: (0.0000) | Acc: (88.00%) (25065/28288)
Epoch: 69 | Batch_idx: 230 |  Loss: (0.3303) |  Loss2: (0.0000) | Acc: (88.00%) (26201/29568)
Epoch: 69 | Batch_idx: 240 |  Loss: (0.3300) |  Loss2: (0.0000) | Acc: (88.00%) (27338/30848)
Epoch: 69 | Batch_idx: 250 |  Loss: (0.3280) |  Loss2: (0.0000) | Acc: (88.00%) (28498/32128)
Epoch: 69 | Batch_idx: 260 |  Loss: (0.3288) |  Loss2: (0.0000) | Acc: (88.00%) (29631/33408)
Epoch: 69 | Batch_idx: 270 |  Loss: (0.3282) |  Loss2: (0.0000) | Acc: (88.00%) (30772/34688)
Epoch: 69 | Batch_idx: 280 |  Loss: (0.3282) |  Loss2: (0.0000) | Acc: (88.00%) (31903/35968)
Epoch: 69 | Batch_idx: 290 |  Loss: (0.3283) |  Loss2: (0.0000) | Acc: (88.00%) (33032/37248)
Epoch: 69 | Batch_idx: 300 |  Loss: (0.3285) |  Loss2: (0.0000) | Acc: (88.00%) (34157/38528)
Epoch: 69 | Batch_idx: 310 |  Loss: (0.3282) |  Loss2: (0.0000) | Acc: (88.00%) (35301/39808)
Epoch: 69 | Batch_idx: 320 |  Loss: (0.3283) |  Loss2: (0.0000) | Acc: (88.00%) (36446/41088)
Epoch: 69 | Batch_idx: 330 |  Loss: (0.3289) |  Loss2: (0.0000) | Acc: (88.00%) (37575/42368)
Epoch: 69 | Batch_idx: 340 |  Loss: (0.3293) |  Loss2: (0.0000) | Acc: (88.00%) (38701/43648)
Epoch: 69 | Batch_idx: 350 |  Loss: (0.3301) |  Loss2: (0.0000) | Acc: (88.00%) (39828/44928)
Epoch: 69 | Batch_idx: 360 |  Loss: (0.3301) |  Loss2: (0.0000) | Acc: (88.00%) (40968/46208)
Epoch: 69 | Batch_idx: 370 |  Loss: (0.3297) |  Loss2: (0.0000) | Acc: (88.00%) (42108/47488)
Epoch: 69 | Batch_idx: 380 |  Loss: (0.3292) |  Loss2: (0.0000) | Acc: (88.00%) (43241/48768)
Epoch: 69 | Batch_idx: 390 |  Loss: (0.3284) |  Loss2: (0.0000) | Acc: (88.00%) (44354/50000)
# TEST : Loss: (0.4462) | Acc: (85.00%) (8536/10000)
percent tensor([0.5684, 0.4316], device='cuda:0')
percent tensor([0.5336, 0.4664], device='cuda:0')
percent tensor([0.5163, 0.4837], device='cuda:0')
percent tensor([0.6293, 0.3707], device='cuda:0')
percent tensor([0.5779, 0.4221], device='cuda:0')
percent tensor([0.6979, 0.3021], device='cuda:0')
percent tensor([0.6721, 0.3279], device='cuda:0')
percent tensor([0.9990, 0.0010], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 70 | Batch_idx: 0 |  Loss: (0.3047) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 70 | Batch_idx: 10 |  Loss: (0.3641) |  Loss2: (0.0000) | Acc: (88.00%) (1247/1408)
Epoch: 70 | Batch_idx: 20 |  Loss: (0.3377) |  Loss2: (0.0000) | Acc: (89.00%) (2394/2688)
Epoch: 70 | Batch_idx: 30 |  Loss: (0.3323) |  Loss2: (0.0000) | Acc: (88.00%) (3528/3968)
Epoch: 70 | Batch_idx: 40 |  Loss: (0.3336) |  Loss2: (0.0000) | Acc: (88.00%) (4661/5248)
Epoch: 70 | Batch_idx: 50 |  Loss: (0.3324) |  Loss2: (0.0000) | Acc: (88.00%) (5791/6528)
Epoch: 70 | Batch_idx: 60 |  Loss: (0.3300) |  Loss2: (0.0000) | Acc: (88.00%) (6930/7808)
Epoch: 70 | Batch_idx: 70 |  Loss: (0.3285) |  Loss2: (0.0000) | Acc: (88.00%) (8068/9088)
Epoch: 70 | Batch_idx: 80 |  Loss: (0.3265) |  Loss2: (0.0000) | Acc: (88.00%) (9220/10368)
Epoch: 70 | Batch_idx: 90 |  Loss: (0.3238) |  Loss2: (0.0000) | Acc: (89.00%) (10367/11648)
Epoch: 70 | Batch_idx: 100 |  Loss: (0.3222) |  Loss2: (0.0000) | Acc: (89.00%) (11510/12928)
Epoch: 70 | Batch_idx: 110 |  Loss: (0.3221) |  Loss2: (0.0000) | Acc: (89.00%) (12656/14208)
Epoch: 70 | Batch_idx: 120 |  Loss: (0.3235) |  Loss2: (0.0000) | Acc: (89.00%) (13786/15488)
Epoch: 70 | Batch_idx: 130 |  Loss: (0.3224) |  Loss2: (0.0000) | Acc: (89.00%) (14935/16768)
Epoch: 70 | Batch_idx: 140 |  Loss: (0.3199) |  Loss2: (0.0000) | Acc: (89.00%) (16088/18048)
Epoch: 70 | Batch_idx: 150 |  Loss: (0.3194) |  Loss2: (0.0000) | Acc: (89.00%) (17238/19328)
Epoch: 70 | Batch_idx: 160 |  Loss: (0.3184) |  Loss2: (0.0000) | Acc: (89.00%) (18380/20608)
Epoch: 70 | Batch_idx: 170 |  Loss: (0.3180) |  Loss2: (0.0000) | Acc: (89.00%) (19526/21888)
Epoch: 70 | Batch_idx: 180 |  Loss: (0.3188) |  Loss2: (0.0000) | Acc: (89.00%) (20651/23168)
Epoch: 70 | Batch_idx: 190 |  Loss: (0.3190) |  Loss2: (0.0000) | Acc: (89.00%) (21783/24448)
Epoch: 70 | Batch_idx: 200 |  Loss: (0.3182) |  Loss2: (0.0000) | Acc: (89.00%) (22924/25728)
Epoch: 70 | Batch_idx: 210 |  Loss: (0.3192) |  Loss2: (0.0000) | Acc: (89.00%) (24048/27008)
Epoch: 70 | Batch_idx: 220 |  Loss: (0.3191) |  Loss2: (0.0000) | Acc: (89.00%) (25183/28288)
Epoch: 70 | Batch_idx: 230 |  Loss: (0.3173) |  Loss2: (0.0000) | Acc: (89.00%) (26340/29568)
Epoch: 70 | Batch_idx: 240 |  Loss: (0.3185) |  Loss2: (0.0000) | Acc: (89.00%) (27469/30848)
Epoch: 70 | Batch_idx: 250 |  Loss: (0.3188) |  Loss2: (0.0000) | Acc: (89.00%) (28605/32128)
Epoch: 70 | Batch_idx: 260 |  Loss: (0.3176) |  Loss2: (0.0000) | Acc: (89.00%) (29760/33408)
Epoch: 70 | Batch_idx: 270 |  Loss: (0.3182) |  Loss2: (0.0000) | Acc: (89.00%) (30879/34688)
Epoch: 70 | Batch_idx: 280 |  Loss: (0.3179) |  Loss2: (0.0000) | Acc: (89.00%) (32019/35968)
Epoch: 70 | Batch_idx: 290 |  Loss: (0.3173) |  Loss2: (0.0000) | Acc: (89.00%) (33177/37248)
Epoch: 70 | Batch_idx: 300 |  Loss: (0.3178) |  Loss2: (0.0000) | Acc: (89.00%) (34318/38528)
Epoch: 70 | Batch_idx: 310 |  Loss: (0.3188) |  Loss2: (0.0000) | Acc: (89.00%) (35440/39808)
Epoch: 70 | Batch_idx: 320 |  Loss: (0.3193) |  Loss2: (0.0000) | Acc: (89.00%) (36579/41088)
Epoch: 70 | Batch_idx: 330 |  Loss: (0.3185) |  Loss2: (0.0000) | Acc: (89.00%) (37734/42368)
Epoch: 70 | Batch_idx: 340 |  Loss: (0.3176) |  Loss2: (0.0000) | Acc: (89.00%) (38887/43648)
Epoch: 70 | Batch_idx: 350 |  Loss: (0.3190) |  Loss2: (0.0000) | Acc: (89.00%) (40004/44928)
Epoch: 70 | Batch_idx: 360 |  Loss: (0.3196) |  Loss2: (0.0000) | Acc: (89.00%) (41133/46208)
Epoch: 70 | Batch_idx: 370 |  Loss: (0.3194) |  Loss2: (0.0000) | Acc: (89.00%) (42286/47488)
Epoch: 70 | Batch_idx: 380 |  Loss: (0.3190) |  Loss2: (0.0000) | Acc: (89.00%) (43427/48768)
Epoch: 70 | Batch_idx: 390 |  Loss: (0.3189) |  Loss2: (0.0000) | Acc: (89.00%) (44515/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_070.pth.tar'
# TEST : Loss: (0.4793) | Acc: (84.00%) (8449/10000)
percent tensor([0.5691, 0.4309], device='cuda:0')
percent tensor([0.5335, 0.4665], device='cuda:0')
percent tensor([0.5159, 0.4841], device='cuda:0')
percent tensor([0.6299, 0.3701], device='cuda:0')
percent tensor([0.5791, 0.4209], device='cuda:0')
percent tensor([0.6972, 0.3028], device='cuda:0')
percent tensor([0.6693, 0.3307], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(177.1248, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(806.0308, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(801.2673, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1526.8949, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(498.5006, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2219.5400, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4292.2446, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1408.5706, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6117.7153, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11978.5146, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3977.5662, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16843.4844, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 71 | Batch_idx: 0 |  Loss: (0.3559) |  Loss2: (0.0000) | Acc: (86.00%) (111/128)
Epoch: 71 | Batch_idx: 10 |  Loss: (0.3419) |  Loss2: (0.0000) | Acc: (88.00%) (1247/1408)
Epoch: 71 | Batch_idx: 20 |  Loss: (0.3254) |  Loss2: (0.0000) | Acc: (89.00%) (2398/2688)
Epoch: 71 | Batch_idx: 30 |  Loss: (0.3232) |  Loss2: (0.0000) | Acc: (89.00%) (3539/3968)
Epoch: 71 | Batch_idx: 40 |  Loss: (0.3167) |  Loss2: (0.0000) | Acc: (89.00%) (4689/5248)
Epoch: 71 | Batch_idx: 50 |  Loss: (0.3109) |  Loss2: (0.0000) | Acc: (89.00%) (5852/6528)
Epoch: 71 | Batch_idx: 60 |  Loss: (0.3108) |  Loss2: (0.0000) | Acc: (89.00%) (7002/7808)
Epoch: 71 | Batch_idx: 70 |  Loss: (0.3111) |  Loss2: (0.0000) | Acc: (89.00%) (8123/9088)
Epoch: 71 | Batch_idx: 80 |  Loss: (0.3082) |  Loss2: (0.0000) | Acc: (89.00%) (9264/10368)
Epoch: 71 | Batch_idx: 90 |  Loss: (0.3084) |  Loss2: (0.0000) | Acc: (89.00%) (10409/11648)
Epoch: 71 | Batch_idx: 100 |  Loss: (0.3092) |  Loss2: (0.0000) | Acc: (89.00%) (11550/12928)
Epoch: 71 | Batch_idx: 110 |  Loss: (0.3098) |  Loss2: (0.0000) | Acc: (89.00%) (12698/14208)
Epoch: 71 | Batch_idx: 120 |  Loss: (0.3118) |  Loss2: (0.0000) | Acc: (89.00%) (13822/15488)
Epoch: 71 | Batch_idx: 130 |  Loss: (0.3119) |  Loss2: (0.0000) | Acc: (89.00%) (14963/16768)
Epoch: 71 | Batch_idx: 140 |  Loss: (0.3109) |  Loss2: (0.0000) | Acc: (89.00%) (16110/18048)
Epoch: 71 | Batch_idx: 150 |  Loss: (0.3115) |  Loss2: (0.0000) | Acc: (89.00%) (17255/19328)
Epoch: 71 | Batch_idx: 160 |  Loss: (0.3117) |  Loss2: (0.0000) | Acc: (89.00%) (18395/20608)
Epoch: 71 | Batch_idx: 170 |  Loss: (0.3106) |  Loss2: (0.0000) | Acc: (89.00%) (19551/21888)
Epoch: 71 | Batch_idx: 180 |  Loss: (0.3102) |  Loss2: (0.0000) | Acc: (89.00%) (20690/23168)
Epoch: 71 | Batch_idx: 190 |  Loss: (0.3106) |  Loss2: (0.0000) | Acc: (89.00%) (21831/24448)
Epoch: 71 | Batch_idx: 200 |  Loss: (0.3111) |  Loss2: (0.0000) | Acc: (89.00%) (22964/25728)
Epoch: 71 | Batch_idx: 210 |  Loss: (0.3109) |  Loss2: (0.0000) | Acc: (89.00%) (24106/27008)
Epoch: 71 | Batch_idx: 220 |  Loss: (0.3104) |  Loss2: (0.0000) | Acc: (89.00%) (25258/28288)
Epoch: 71 | Batch_idx: 230 |  Loss: (0.3102) |  Loss2: (0.0000) | Acc: (89.00%) (26400/29568)
Epoch: 71 | Batch_idx: 240 |  Loss: (0.3106) |  Loss2: (0.0000) | Acc: (89.00%) (27529/30848)
Epoch: 71 | Batch_idx: 250 |  Loss: (0.3117) |  Loss2: (0.0000) | Acc: (89.00%) (28655/32128)
Epoch: 71 | Batch_idx: 260 |  Loss: (0.3112) |  Loss2: (0.0000) | Acc: (89.00%) (29798/33408)
Epoch: 71 | Batch_idx: 270 |  Loss: (0.3114) |  Loss2: (0.0000) | Acc: (89.00%) (30946/34688)
Epoch: 71 | Batch_idx: 280 |  Loss: (0.3115) |  Loss2: (0.0000) | Acc: (89.00%) (32095/35968)
Epoch: 71 | Batch_idx: 290 |  Loss: (0.3113) |  Loss2: (0.0000) | Acc: (89.00%) (33237/37248)
Epoch: 71 | Batch_idx: 300 |  Loss: (0.3119) |  Loss2: (0.0000) | Acc: (89.00%) (34368/38528)
Epoch: 71 | Batch_idx: 310 |  Loss: (0.3120) |  Loss2: (0.0000) | Acc: (89.00%) (35510/39808)
Epoch: 71 | Batch_idx: 320 |  Loss: (0.3117) |  Loss2: (0.0000) | Acc: (89.00%) (36651/41088)
Epoch: 71 | Batch_idx: 330 |  Loss: (0.3124) |  Loss2: (0.0000) | Acc: (89.00%) (37803/42368)
Epoch: 71 | Batch_idx: 340 |  Loss: (0.3124) |  Loss2: (0.0000) | Acc: (89.00%) (38958/43648)
Epoch: 71 | Batch_idx: 350 |  Loss: (0.3115) |  Loss2: (0.0000) | Acc: (89.00%) (40108/44928)
Epoch: 71 | Batch_idx: 360 |  Loss: (0.3127) |  Loss2: (0.0000) | Acc: (89.00%) (41228/46208)
Epoch: 71 | Batch_idx: 370 |  Loss: (0.3130) |  Loss2: (0.0000) | Acc: (89.00%) (42362/47488)
Epoch: 71 | Batch_idx: 380 |  Loss: (0.3132) |  Loss2: (0.0000) | Acc: (89.00%) (43499/48768)
Epoch: 71 | Batch_idx: 390 |  Loss: (0.3131) |  Loss2: (0.0000) | Acc: (89.00%) (44607/50000)
# TEST : Loss: (0.4846) | Acc: (84.00%) (8403/10000)
percent tensor([0.5689, 0.4311], device='cuda:0')
percent tensor([0.5333, 0.4667], device='cuda:0')
percent tensor([0.5162, 0.4838], device='cuda:0')
percent tensor([0.6273, 0.3727], device='cuda:0')
percent tensor([0.5800, 0.4200], device='cuda:0')
percent tensor([0.6961, 0.3039], device='cuda:0')
percent tensor([0.6775, 0.3225], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 72 | Batch_idx: 0 |  Loss: (0.2374) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 72 | Batch_idx: 10 |  Loss: (0.2789) |  Loss2: (0.0000) | Acc: (90.00%) (1276/1408)
Epoch: 72 | Batch_idx: 20 |  Loss: (0.2942) |  Loss2: (0.0000) | Acc: (90.00%) (2424/2688)
Epoch: 72 | Batch_idx: 30 |  Loss: (0.2929) |  Loss2: (0.0000) | Acc: (90.00%) (3581/3968)
Epoch: 72 | Batch_idx: 40 |  Loss: (0.3019) |  Loss2: (0.0000) | Acc: (89.00%) (4718/5248)
Epoch: 72 | Batch_idx: 50 |  Loss: (0.2987) |  Loss2: (0.0000) | Acc: (90.00%) (5881/6528)
Epoch: 72 | Batch_idx: 60 |  Loss: (0.2961) |  Loss2: (0.0000) | Acc: (90.00%) (7033/7808)
Epoch: 72 | Batch_idx: 70 |  Loss: (0.2980) |  Loss2: (0.0000) | Acc: (89.00%) (8167/9088)
Epoch: 72 | Batch_idx: 80 |  Loss: (0.3012) |  Loss2: (0.0000) | Acc: (89.00%) (9289/10368)
Epoch: 72 | Batch_idx: 90 |  Loss: (0.2972) |  Loss2: (0.0000) | Acc: (89.00%) (10456/11648)
Epoch: 72 | Batch_idx: 100 |  Loss: (0.2963) |  Loss2: (0.0000) | Acc: (89.00%) (11614/12928)
Epoch: 72 | Batch_idx: 110 |  Loss: (0.2945) |  Loss2: (0.0000) | Acc: (89.00%) (12766/14208)
Epoch: 72 | Batch_idx: 120 |  Loss: (0.2971) |  Loss2: (0.0000) | Acc: (89.00%) (13895/15488)
Epoch: 72 | Batch_idx: 130 |  Loss: (0.2980) |  Loss2: (0.0000) | Acc: (89.00%) (15033/16768)
Epoch: 72 | Batch_idx: 140 |  Loss: (0.2982) |  Loss2: (0.0000) | Acc: (89.00%) (16180/18048)
Epoch: 72 | Batch_idx: 150 |  Loss: (0.2987) |  Loss2: (0.0000) | Acc: (89.00%) (17323/19328)
Epoch: 72 | Batch_idx: 160 |  Loss: (0.2980) |  Loss2: (0.0000) | Acc: (89.00%) (18481/20608)
Epoch: 72 | Batch_idx: 170 |  Loss: (0.2997) |  Loss2: (0.0000) | Acc: (89.00%) (19613/21888)
Epoch: 72 | Batch_idx: 180 |  Loss: (0.3013) |  Loss2: (0.0000) | Acc: (89.00%) (20745/23168)
Epoch: 72 | Batch_idx: 190 |  Loss: (0.3014) |  Loss2: (0.0000) | Acc: (89.00%) (21891/24448)
Epoch: 72 | Batch_idx: 200 |  Loss: (0.2999) |  Loss2: (0.0000) | Acc: (89.00%) (23056/25728)
Epoch: 72 | Batch_idx: 210 |  Loss: (0.3002) |  Loss2: (0.0000) | Acc: (89.00%) (24201/27008)
Epoch: 72 | Batch_idx: 220 |  Loss: (0.3007) |  Loss2: (0.0000) | Acc: (89.00%) (25349/28288)
Epoch: 72 | Batch_idx: 230 |  Loss: (0.3006) |  Loss2: (0.0000) | Acc: (89.00%) (26484/29568)
Epoch: 72 | Batch_idx: 240 |  Loss: (0.3010) |  Loss2: (0.0000) | Acc: (89.00%) (27632/30848)
Epoch: 72 | Batch_idx: 250 |  Loss: (0.3016) |  Loss2: (0.0000) | Acc: (89.00%) (28768/32128)
Epoch: 72 | Batch_idx: 260 |  Loss: (0.3022) |  Loss2: (0.0000) | Acc: (89.00%) (29896/33408)
Epoch: 72 | Batch_idx: 270 |  Loss: (0.3012) |  Loss2: (0.0000) | Acc: (89.00%) (31054/34688)
Epoch: 72 | Batch_idx: 280 |  Loss: (0.3020) |  Loss2: (0.0000) | Acc: (89.00%) (32181/35968)
Epoch: 72 | Batch_idx: 290 |  Loss: (0.3002) |  Loss2: (0.0000) | Acc: (89.00%) (33358/37248)
Epoch: 72 | Batch_idx: 300 |  Loss: (0.3001) |  Loss2: (0.0000) | Acc: (89.00%) (34502/38528)
Epoch: 72 | Batch_idx: 310 |  Loss: (0.2994) |  Loss2: (0.0000) | Acc: (89.00%) (35655/39808)
Epoch: 72 | Batch_idx: 320 |  Loss: (0.2993) |  Loss2: (0.0000) | Acc: (89.00%) (36799/41088)
Epoch: 72 | Batch_idx: 330 |  Loss: (0.2987) |  Loss2: (0.0000) | Acc: (89.00%) (37957/42368)
Epoch: 72 | Batch_idx: 340 |  Loss: (0.2986) |  Loss2: (0.0000) | Acc: (89.00%) (39103/43648)
Epoch: 72 | Batch_idx: 350 |  Loss: (0.2986) |  Loss2: (0.0000) | Acc: (89.00%) (40254/44928)
Epoch: 72 | Batch_idx: 360 |  Loss: (0.2984) |  Loss2: (0.0000) | Acc: (89.00%) (41418/46208)
Epoch: 72 | Batch_idx: 370 |  Loss: (0.2980) |  Loss2: (0.0000) | Acc: (89.00%) (42563/47488)
Epoch: 72 | Batch_idx: 380 |  Loss: (0.2980) |  Loss2: (0.0000) | Acc: (89.00%) (43709/48768)
Epoch: 72 | Batch_idx: 390 |  Loss: (0.2988) |  Loss2: (0.0000) | Acc: (89.00%) (44801/50000)
# TEST : Loss: (0.4757) | Acc: (84.00%) (8481/10000)
percent tensor([0.5693, 0.4307], device='cuda:0')
percent tensor([0.5338, 0.4662], device='cuda:0')
percent tensor([0.5167, 0.4833], device='cuda:0')
percent tensor([0.6295, 0.3705], device='cuda:0')
percent tensor([0.5783, 0.4217], device='cuda:0')
percent tensor([0.6979, 0.3021], device='cuda:0')
percent tensor([0.6715, 0.3285], device='cuda:0')
percent tensor([0.9990, 0.0010], device='cuda:0')
Epoch: 73 | Batch_idx: 0 |  Loss: (0.3867) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 73 | Batch_idx: 10 |  Loss: (0.2650) |  Loss2: (0.0000) | Acc: (90.00%) (1278/1408)
Epoch: 73 | Batch_idx: 20 |  Loss: (0.2659) |  Loss2: (0.0000) | Acc: (90.00%) (2439/2688)
Epoch: 73 | Batch_idx: 30 |  Loss: (0.2702) |  Loss2: (0.0000) | Acc: (90.00%) (3595/3968)
Epoch: 73 | Batch_idx: 40 |  Loss: (0.2822) |  Loss2: (0.0000) | Acc: (90.00%) (4732/5248)
Epoch: 73 | Batch_idx: 50 |  Loss: (0.2805) |  Loss2: (0.0000) | Acc: (90.00%) (5894/6528)
Epoch: 73 | Batch_idx: 60 |  Loss: (0.2817) |  Loss2: (0.0000) | Acc: (90.00%) (7044/7808)
Epoch: 73 | Batch_idx: 70 |  Loss: (0.2846) |  Loss2: (0.0000) | Acc: (90.00%) (8189/9088)
Epoch: 73 | Batch_idx: 80 |  Loss: (0.2857) |  Loss2: (0.0000) | Acc: (90.00%) (9336/10368)
Epoch: 73 | Batch_idx: 90 |  Loss: (0.2852) |  Loss2: (0.0000) | Acc: (90.00%) (10489/11648)
Epoch: 73 | Batch_idx: 100 |  Loss: (0.2852) |  Loss2: (0.0000) | Acc: (90.00%) (11641/12928)
Epoch: 73 | Batch_idx: 110 |  Loss: (0.2871) |  Loss2: (0.0000) | Acc: (90.00%) (12791/14208)
Epoch: 73 | Batch_idx: 120 |  Loss: (0.2855) |  Loss2: (0.0000) | Acc: (90.00%) (13966/15488)
Epoch: 73 | Batch_idx: 130 |  Loss: (0.2873) |  Loss2: (0.0000) | Acc: (90.00%) (15105/16768)
Epoch: 73 | Batch_idx: 140 |  Loss: (0.2880) |  Loss2: (0.0000) | Acc: (90.00%) (16252/18048)
Epoch: 73 | Batch_idx: 150 |  Loss: (0.2881) |  Loss2: (0.0000) | Acc: (90.00%) (17403/19328)
Epoch: 73 | Batch_idx: 160 |  Loss: (0.2868) |  Loss2: (0.0000) | Acc: (90.00%) (18568/20608)
Epoch: 73 | Batch_idx: 170 |  Loss: (0.2862) |  Loss2: (0.0000) | Acc: (90.00%) (19721/21888)
Epoch: 73 | Batch_idx: 180 |  Loss: (0.2870) |  Loss2: (0.0000) | Acc: (90.00%) (20862/23168)
Epoch: 73 | Batch_idx: 190 |  Loss: (0.2879) |  Loss2: (0.0000) | Acc: (90.00%) (22005/24448)
Epoch: 73 | Batch_idx: 200 |  Loss: (0.2878) |  Loss2: (0.0000) | Acc: (90.00%) (23163/25728)
Epoch: 73 | Batch_idx: 210 |  Loss: (0.2889) |  Loss2: (0.0000) | Acc: (89.00%) (24304/27008)
Epoch: 73 | Batch_idx: 220 |  Loss: (0.2893) |  Loss2: (0.0000) | Acc: (89.00%) (25455/28288)
Epoch: 73 | Batch_idx: 230 |  Loss: (0.2911) |  Loss2: (0.0000) | Acc: (89.00%) (26584/29568)
Epoch: 73 | Batch_idx: 240 |  Loss: (0.2918) |  Loss2: (0.0000) | Acc: (89.00%) (27716/30848)
Epoch: 73 | Batch_idx: 250 |  Loss: (0.2911) |  Loss2: (0.0000) | Acc: (89.00%) (28871/32128)
Epoch: 73 | Batch_idx: 260 |  Loss: (0.2922) |  Loss2: (0.0000) | Acc: (89.00%) (30001/33408)
Epoch: 73 | Batch_idx: 270 |  Loss: (0.2925) |  Loss2: (0.0000) | Acc: (89.00%) (31149/34688)
Epoch: 73 | Batch_idx: 280 |  Loss: (0.2923) |  Loss2: (0.0000) | Acc: (89.00%) (32291/35968)
Epoch: 73 | Batch_idx: 290 |  Loss: (0.2916) |  Loss2: (0.0000) | Acc: (89.00%) (33452/37248)
Epoch: 73 | Batch_idx: 300 |  Loss: (0.2920) |  Loss2: (0.0000) | Acc: (89.00%) (34593/38528)
Epoch: 73 | Batch_idx: 310 |  Loss: (0.2920) |  Loss2: (0.0000) | Acc: (89.00%) (35746/39808)
Epoch: 73 | Batch_idx: 320 |  Loss: (0.2918) |  Loss2: (0.0000) | Acc: (89.00%) (36886/41088)
Epoch: 73 | Batch_idx: 330 |  Loss: (0.2917) |  Loss2: (0.0000) | Acc: (89.00%) (38038/42368)
Epoch: 73 | Batch_idx: 340 |  Loss: (0.2923) |  Loss2: (0.0000) | Acc: (89.00%) (39177/43648)
Epoch: 73 | Batch_idx: 350 |  Loss: (0.2927) |  Loss2: (0.0000) | Acc: (89.00%) (40322/44928)
Epoch: 73 | Batch_idx: 360 |  Loss: (0.2933) |  Loss2: (0.0000) | Acc: (89.00%) (41458/46208)
Epoch: 73 | Batch_idx: 370 |  Loss: (0.2934) |  Loss2: (0.0000) | Acc: (89.00%) (42597/47488)
Epoch: 73 | Batch_idx: 380 |  Loss: (0.2940) |  Loss2: (0.0000) | Acc: (89.00%) (43728/48768)
Epoch: 73 | Batch_idx: 390 |  Loss: (0.2938) |  Loss2: (0.0000) | Acc: (89.00%) (44840/50000)
# TEST : Loss: (0.4636) | Acc: (85.00%) (8537/10000)
percent tensor([0.5694, 0.4306], device='cuda:0')
percent tensor([0.5343, 0.4657], device='cuda:0')
percent tensor([0.5169, 0.4831], device='cuda:0')
percent tensor([0.6291, 0.3709], device='cuda:0')
percent tensor([0.5795, 0.4205], device='cuda:0')
percent tensor([0.6978, 0.3022], device='cuda:0')
percent tensor([0.6735, 0.3265], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 74 | Batch_idx: 0 |  Loss: (0.3097) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 74 | Batch_idx: 10 |  Loss: (0.2764) |  Loss2: (0.0000) | Acc: (91.00%) (1284/1408)
Epoch: 74 | Batch_idx: 20 |  Loss: (0.2803) |  Loss2: (0.0000) | Acc: (90.00%) (2428/2688)
Epoch: 74 | Batch_idx: 30 |  Loss: (0.2788) |  Loss2: (0.0000) | Acc: (90.00%) (3580/3968)
Epoch: 74 | Batch_idx: 40 |  Loss: (0.2830) |  Loss2: (0.0000) | Acc: (90.00%) (4725/5248)
Epoch: 74 | Batch_idx: 50 |  Loss: (0.2826) |  Loss2: (0.0000) | Acc: (89.00%) (5873/6528)
Epoch: 74 | Batch_idx: 60 |  Loss: (0.2759) |  Loss2: (0.0000) | Acc: (90.00%) (7050/7808)
Epoch: 74 | Batch_idx: 70 |  Loss: (0.2746) |  Loss2: (0.0000) | Acc: (90.00%) (8207/9088)
Epoch: 74 | Batch_idx: 80 |  Loss: (0.2761) |  Loss2: (0.0000) | Acc: (90.00%) (9366/10368)
Epoch: 74 | Batch_idx: 90 |  Loss: (0.2749) |  Loss2: (0.0000) | Acc: (90.00%) (10526/11648)
Epoch: 74 | Batch_idx: 100 |  Loss: (0.2763) |  Loss2: (0.0000) | Acc: (90.00%) (11669/12928)
Epoch: 74 | Batch_idx: 110 |  Loss: (0.2791) |  Loss2: (0.0000) | Acc: (90.00%) (12812/14208)
Epoch: 74 | Batch_idx: 120 |  Loss: (0.2794) |  Loss2: (0.0000) | Acc: (90.00%) (13958/15488)
Epoch: 74 | Batch_idx: 130 |  Loss: (0.2765) |  Loss2: (0.0000) | Acc: (90.00%) (15130/16768)
Epoch: 74 | Batch_idx: 140 |  Loss: (0.2765) |  Loss2: (0.0000) | Acc: (90.00%) (16279/18048)
Epoch: 74 | Batch_idx: 150 |  Loss: (0.2770) |  Loss2: (0.0000) | Acc: (90.00%) (17431/19328)
Epoch: 74 | Batch_idx: 160 |  Loss: (0.2779) |  Loss2: (0.0000) | Acc: (90.00%) (18593/20608)
Epoch: 74 | Batch_idx: 170 |  Loss: (0.2795) |  Loss2: (0.0000) | Acc: (90.00%) (19737/21888)
Epoch: 74 | Batch_idx: 180 |  Loss: (0.2825) |  Loss2: (0.0000) | Acc: (90.00%) (20864/23168)
Epoch: 74 | Batch_idx: 190 |  Loss: (0.2815) |  Loss2: (0.0000) | Acc: (90.00%) (22027/24448)
Epoch: 74 | Batch_idx: 200 |  Loss: (0.2835) |  Loss2: (0.0000) | Acc: (90.00%) (23171/25728)
Epoch: 74 | Batch_idx: 210 |  Loss: (0.2840) |  Loss2: (0.0000) | Acc: (90.00%) (24322/27008)
Epoch: 74 | Batch_idx: 220 |  Loss: (0.2833) |  Loss2: (0.0000) | Acc: (90.00%) (25485/28288)
Epoch: 74 | Batch_idx: 230 |  Loss: (0.2836) |  Loss2: (0.0000) | Acc: (90.00%) (26637/29568)
Epoch: 74 | Batch_idx: 240 |  Loss: (0.2854) |  Loss2: (0.0000) | Acc: (90.00%) (27772/30848)
Epoch: 74 | Batch_idx: 250 |  Loss: (0.2842) |  Loss2: (0.0000) | Acc: (90.00%) (28948/32128)
Epoch: 74 | Batch_idx: 260 |  Loss: (0.2847) |  Loss2: (0.0000) | Acc: (90.00%) (30098/33408)
Epoch: 74 | Batch_idx: 270 |  Loss: (0.2868) |  Loss2: (0.0000) | Acc: (90.00%) (31225/34688)
Epoch: 74 | Batch_idx: 280 |  Loss: (0.2866) |  Loss2: (0.0000) | Acc: (90.00%) (32386/35968)
Epoch: 74 | Batch_idx: 290 |  Loss: (0.2854) |  Loss2: (0.0000) | Acc: (90.00%) (33555/37248)
Epoch: 74 | Batch_idx: 300 |  Loss: (0.2848) |  Loss2: (0.0000) | Acc: (90.00%) (34717/38528)
Epoch: 74 | Batch_idx: 310 |  Loss: (0.2848) |  Loss2: (0.0000) | Acc: (90.00%) (35866/39808)
Epoch: 74 | Batch_idx: 320 |  Loss: (0.2844) |  Loss2: (0.0000) | Acc: (90.00%) (37021/41088)
Epoch: 74 | Batch_idx: 330 |  Loss: (0.2852) |  Loss2: (0.0000) | Acc: (90.00%) (38167/42368)
Epoch: 74 | Batch_idx: 340 |  Loss: (0.2847) |  Loss2: (0.0000) | Acc: (90.00%) (39331/43648)
Epoch: 74 | Batch_idx: 350 |  Loss: (0.2847) |  Loss2: (0.0000) | Acc: (90.00%) (40492/44928)
Epoch: 74 | Batch_idx: 360 |  Loss: (0.2856) |  Loss2: (0.0000) | Acc: (90.00%) (41633/46208)
Epoch: 74 | Batch_idx: 370 |  Loss: (0.2862) |  Loss2: (0.0000) | Acc: (90.00%) (42771/47488)
Epoch: 74 | Batch_idx: 380 |  Loss: (0.2866) |  Loss2: (0.0000) | Acc: (90.00%) (43924/48768)
Epoch: 74 | Batch_idx: 390 |  Loss: (0.2874) |  Loss2: (0.0000) | Acc: (90.00%) (45021/50000)
# TEST : Loss: (0.5420) | Acc: (83.00%) (8302/10000)
percent tensor([0.5695, 0.4305], device='cuda:0')
percent tensor([0.5345, 0.4655], device='cuda:0')
percent tensor([0.5164, 0.4836], device='cuda:0')
percent tensor([0.6294, 0.3706], device='cuda:0')
percent tensor([0.5811, 0.4189], device='cuda:0')
percent tensor([0.6960, 0.3040], device='cuda:0')
percent tensor([0.6700, 0.3300], device='cuda:0')
percent tensor([0.9990, 0.0010], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 75 | Batch_idx: 0 |  Loss: (0.3029) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 75 | Batch_idx: 10 |  Loss: (0.2723) |  Loss2: (0.0000) | Acc: (89.00%) (1259/1408)
Epoch: 75 | Batch_idx: 20 |  Loss: (0.3044) |  Loss2: (0.0000) | Acc: (88.00%) (2382/2688)
Epoch: 75 | Batch_idx: 30 |  Loss: (0.3225) |  Loss2: (0.0000) | Acc: (88.00%) (3494/3968)
Epoch: 75 | Batch_idx: 40 |  Loss: (0.3340) |  Loss2: (0.0000) | Acc: (87.00%) (4603/5248)
Epoch: 75 | Batch_idx: 50 |  Loss: (0.3315) |  Loss2: (0.0000) | Acc: (87.00%) (5738/6528)
Epoch: 75 | Batch_idx: 60 |  Loss: (0.3366) |  Loss2: (0.0000) | Acc: (87.00%) (6862/7808)
Epoch: 75 | Batch_idx: 70 |  Loss: (0.3394) |  Loss2: (0.0000) | Acc: (87.00%) (7988/9088)
Epoch: 75 | Batch_idx: 80 |  Loss: (0.3431) |  Loss2: (0.0000) | Acc: (87.00%) (9100/10368)
Epoch: 75 | Batch_idx: 90 |  Loss: (0.3437) |  Loss2: (0.0000) | Acc: (87.00%) (10220/11648)
Epoch: 75 | Batch_idx: 100 |  Loss: (0.3404) |  Loss2: (0.0000) | Acc: (87.00%) (11362/12928)
Epoch: 75 | Batch_idx: 110 |  Loss: (0.3409) |  Loss2: (0.0000) | Acc: (87.00%) (12485/14208)
Epoch: 75 | Batch_idx: 120 |  Loss: (0.3430) |  Loss2: (0.0000) | Acc: (87.00%) (13605/15488)
Epoch: 75 | Batch_idx: 130 |  Loss: (0.3433) |  Loss2: (0.0000) | Acc: (87.00%) (14722/16768)
Epoch: 75 | Batch_idx: 140 |  Loss: (0.3442) |  Loss2: (0.0000) | Acc: (87.00%) (15850/18048)
Epoch: 75 | Batch_idx: 150 |  Loss: (0.3450) |  Loss2: (0.0000) | Acc: (87.00%) (16977/19328)
Epoch: 75 | Batch_idx: 160 |  Loss: (0.3467) |  Loss2: (0.0000) | Acc: (87.00%) (18089/20608)
Epoch: 75 | Batch_idx: 170 |  Loss: (0.3479) |  Loss2: (0.0000) | Acc: (87.00%) (19192/21888)
Epoch: 75 | Batch_idx: 180 |  Loss: (0.3478) |  Loss2: (0.0000) | Acc: (87.00%) (20312/23168)
Epoch: 75 | Batch_idx: 190 |  Loss: (0.3467) |  Loss2: (0.0000) | Acc: (87.00%) (21449/24448)
Epoch: 75 | Batch_idx: 200 |  Loss: (0.3472) |  Loss2: (0.0000) | Acc: (87.00%) (22585/25728)
Epoch: 75 | Batch_idx: 210 |  Loss: (0.3473) |  Loss2: (0.0000) | Acc: (87.00%) (23706/27008)
Epoch: 75 | Batch_idx: 220 |  Loss: (0.3474) |  Loss2: (0.0000) | Acc: (87.00%) (24825/28288)
Epoch: 75 | Batch_idx: 230 |  Loss: (0.3472) |  Loss2: (0.0000) | Acc: (87.00%) (25955/29568)
Epoch: 75 | Batch_idx: 240 |  Loss: (0.3444) |  Loss2: (0.0000) | Acc: (87.00%) (27111/30848)
Epoch: 75 | Batch_idx: 250 |  Loss: (0.3435) |  Loss2: (0.0000) | Acc: (87.00%) (28261/32128)
Epoch: 75 | Batch_idx: 260 |  Loss: (0.3425) |  Loss2: (0.0000) | Acc: (88.00%) (29406/33408)
Epoch: 75 | Batch_idx: 270 |  Loss: (0.3415) |  Loss2: (0.0000) | Acc: (88.00%) (30535/34688)
Epoch: 75 | Batch_idx: 280 |  Loss: (0.3406) |  Loss2: (0.0000) | Acc: (88.00%) (31677/35968)
Epoch: 75 | Batch_idx: 290 |  Loss: (0.3402) |  Loss2: (0.0000) | Acc: (88.00%) (32820/37248)
Epoch: 75 | Batch_idx: 300 |  Loss: (0.3406) |  Loss2: (0.0000) | Acc: (88.00%) (33945/38528)
Epoch: 75 | Batch_idx: 310 |  Loss: (0.3403) |  Loss2: (0.0000) | Acc: (88.00%) (35081/39808)
Epoch: 75 | Batch_idx: 320 |  Loss: (0.3400) |  Loss2: (0.0000) | Acc: (88.00%) (36209/41088)
Epoch: 75 | Batch_idx: 330 |  Loss: (0.3391) |  Loss2: (0.0000) | Acc: (88.00%) (37345/42368)
Epoch: 75 | Batch_idx: 340 |  Loss: (0.3375) |  Loss2: (0.0000) | Acc: (88.00%) (38501/43648)
Epoch: 75 | Batch_idx: 350 |  Loss: (0.3362) |  Loss2: (0.0000) | Acc: (88.00%) (39649/44928)
Epoch: 75 | Batch_idx: 360 |  Loss: (0.3350) |  Loss2: (0.0000) | Acc: (88.00%) (40813/46208)
Epoch: 75 | Batch_idx: 370 |  Loss: (0.3344) |  Loss2: (0.0000) | Acc: (88.00%) (41958/47488)
Epoch: 75 | Batch_idx: 380 |  Loss: (0.3331) |  Loss2: (0.0000) | Acc: (88.00%) (43113/48768)
Epoch: 75 | Batch_idx: 390 |  Loss: (0.3328) |  Loss2: (0.0000) | Acc: (88.00%) (44193/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_075.pth.tar'
# TEST : Loss: (0.4658) | Acc: (84.00%) (8475/10000)
percent tensor([0.5621, 0.4379], device='cuda:0')
percent tensor([0.5465, 0.4535], device='cuda:0')
percent tensor([0.5261, 0.4739], device='cuda:0')
percent tensor([0.5985, 0.4015], device='cuda:0')
percent tensor([0.5682, 0.4318], device='cuda:0')
percent tensor([0.6669, 0.3331], device='cuda:0')
percent tensor([0.6800, 0.3200], device='cuda:0')
percent tensor([0.9990, 0.0010], device='cuda:0')
Epoch: 76 | Batch_idx: 0 |  Loss: (0.3148) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 76 | Batch_idx: 10 |  Loss: (0.3238) |  Loss2: (0.0000) | Acc: (88.00%) (1252/1408)
Epoch: 76 | Batch_idx: 20 |  Loss: (0.3174) |  Loss2: (0.0000) | Acc: (89.00%) (2396/2688)
Epoch: 76 | Batch_idx: 30 |  Loss: (0.3248) |  Loss2: (0.0000) | Acc: (88.00%) (3523/3968)
Epoch: 76 | Batch_idx: 40 |  Loss: (0.3222) |  Loss2: (0.0000) | Acc: (88.00%) (4664/5248)
Epoch: 76 | Batch_idx: 50 |  Loss: (0.3180) |  Loss2: (0.0000) | Acc: (89.00%) (5821/6528)
Epoch: 76 | Batch_idx: 60 |  Loss: (0.3189) |  Loss2: (0.0000) | Acc: (89.00%) (6967/7808)
Epoch: 76 | Batch_idx: 70 |  Loss: (0.3193) |  Loss2: (0.0000) | Acc: (89.00%) (8114/9088)
Epoch: 76 | Batch_idx: 80 |  Loss: (0.3182) |  Loss2: (0.0000) | Acc: (89.00%) (9257/10368)
Epoch: 76 | Batch_idx: 90 |  Loss: (0.3177) |  Loss2: (0.0000) | Acc: (89.00%) (10393/11648)
Epoch: 76 | Batch_idx: 100 |  Loss: (0.3164) |  Loss2: (0.0000) | Acc: (89.00%) (11527/12928)
Epoch: 76 | Batch_idx: 110 |  Loss: (0.3169) |  Loss2: (0.0000) | Acc: (89.00%) (12669/14208)
Epoch: 76 | Batch_idx: 120 |  Loss: (0.3184) |  Loss2: (0.0000) | Acc: (89.00%) (13800/15488)
Epoch: 76 | Batch_idx: 130 |  Loss: (0.3192) |  Loss2: (0.0000) | Acc: (89.00%) (14936/16768)
Epoch: 76 | Batch_idx: 140 |  Loss: (0.3182) |  Loss2: (0.0000) | Acc: (89.00%) (16090/18048)
Epoch: 76 | Batch_idx: 150 |  Loss: (0.3167) |  Loss2: (0.0000) | Acc: (89.00%) (17243/19328)
Epoch: 76 | Batch_idx: 160 |  Loss: (0.3173) |  Loss2: (0.0000) | Acc: (89.00%) (18373/20608)
Epoch: 76 | Batch_idx: 170 |  Loss: (0.3169) |  Loss2: (0.0000) | Acc: (89.00%) (19516/21888)
Epoch: 76 | Batch_idx: 180 |  Loss: (0.3152) |  Loss2: (0.0000) | Acc: (89.00%) (20660/23168)
Epoch: 76 | Batch_idx: 190 |  Loss: (0.3148) |  Loss2: (0.0000) | Acc: (89.00%) (21794/24448)
Epoch: 76 | Batch_idx: 200 |  Loss: (0.3156) |  Loss2: (0.0000) | Acc: (89.00%) (22938/25728)
Epoch: 76 | Batch_idx: 210 |  Loss: (0.3151) |  Loss2: (0.0000) | Acc: (89.00%) (24094/27008)
Epoch: 76 | Batch_idx: 220 |  Loss: (0.3141) |  Loss2: (0.0000) | Acc: (89.00%) (25241/28288)
Epoch: 76 | Batch_idx: 230 |  Loss: (0.3137) |  Loss2: (0.0000) | Acc: (89.00%) (26380/29568)
Epoch: 76 | Batch_idx: 240 |  Loss: (0.3133) |  Loss2: (0.0000) | Acc: (89.00%) (27522/30848)
Epoch: 76 | Batch_idx: 250 |  Loss: (0.3112) |  Loss2: (0.0000) | Acc: (89.00%) (28686/32128)
Epoch: 76 | Batch_idx: 260 |  Loss: (0.3109) |  Loss2: (0.0000) | Acc: (89.00%) (29831/33408)
Epoch: 76 | Batch_idx: 270 |  Loss: (0.3102) |  Loss2: (0.0000) | Acc: (89.00%) (30983/34688)
Epoch: 76 | Batch_idx: 280 |  Loss: (0.3095) |  Loss2: (0.0000) | Acc: (89.00%) (32143/35968)
Epoch: 76 | Batch_idx: 290 |  Loss: (0.3100) |  Loss2: (0.0000) | Acc: (89.00%) (33282/37248)
Epoch: 76 | Batch_idx: 300 |  Loss: (0.3096) |  Loss2: (0.0000) | Acc: (89.00%) (34434/38528)
Epoch: 76 | Batch_idx: 310 |  Loss: (0.3095) |  Loss2: (0.0000) | Acc: (89.00%) (35568/39808)
Epoch: 76 | Batch_idx: 320 |  Loss: (0.3091) |  Loss2: (0.0000) | Acc: (89.00%) (36720/41088)
Epoch: 76 | Batch_idx: 330 |  Loss: (0.3089) |  Loss2: (0.0000) | Acc: (89.00%) (37874/42368)
Epoch: 76 | Batch_idx: 340 |  Loss: (0.3080) |  Loss2: (0.0000) | Acc: (89.00%) (39023/43648)
Epoch: 76 | Batch_idx: 350 |  Loss: (0.3068) |  Loss2: (0.0000) | Acc: (89.00%) (40174/44928)
Epoch: 76 | Batch_idx: 360 |  Loss: (0.3062) |  Loss2: (0.0000) | Acc: (89.00%) (41325/46208)
Epoch: 76 | Batch_idx: 370 |  Loss: (0.3056) |  Loss2: (0.0000) | Acc: (89.00%) (42475/47488)
Epoch: 76 | Batch_idx: 380 |  Loss: (0.3051) |  Loss2: (0.0000) | Acc: (89.00%) (43629/48768)
Epoch: 76 | Batch_idx: 390 |  Loss: (0.3045) |  Loss2: (0.0000) | Acc: (89.00%) (44746/50000)
# TEST : Loss: (0.4439) | Acc: (85.00%) (8539/10000)
percent tensor([0.5602, 0.4398], device='cuda:0')
percent tensor([0.5511, 0.4489], device='cuda:0')
percent tensor([0.5294, 0.4706], device='cuda:0')
percent tensor([0.6036, 0.3964], device='cuda:0')
percent tensor([0.5726, 0.4274], device='cuda:0')
percent tensor([0.6693, 0.3307], device='cuda:0')
percent tensor([0.6864, 0.3136], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 77 | Batch_idx: 0 |  Loss: (0.2396) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 77 | Batch_idx: 10 |  Loss: (0.3153) |  Loss2: (0.0000) | Acc: (89.00%) (1256/1408)
Epoch: 77 | Batch_idx: 20 |  Loss: (0.3063) |  Loss2: (0.0000) | Acc: (89.00%) (2399/2688)
Epoch: 77 | Batch_idx: 30 |  Loss: (0.3033) |  Loss2: (0.0000) | Acc: (89.00%) (3542/3968)
Epoch: 77 | Batch_idx: 40 |  Loss: (0.3007) |  Loss2: (0.0000) | Acc: (89.00%) (4683/5248)
Epoch: 77 | Batch_idx: 50 |  Loss: (0.2991) |  Loss2: (0.0000) | Acc: (89.00%) (5833/6528)
Epoch: 77 | Batch_idx: 60 |  Loss: (0.2986) |  Loss2: (0.0000) | Acc: (89.00%) (6996/7808)
Epoch: 77 | Batch_idx: 70 |  Loss: (0.2916) |  Loss2: (0.0000) | Acc: (89.00%) (8162/9088)
Epoch: 77 | Batch_idx: 80 |  Loss: (0.2939) |  Loss2: (0.0000) | Acc: (89.00%) (9306/10368)
Epoch: 77 | Batch_idx: 90 |  Loss: (0.2932) |  Loss2: (0.0000) | Acc: (89.00%) (10461/11648)
Epoch: 77 | Batch_idx: 100 |  Loss: (0.2944) |  Loss2: (0.0000) | Acc: (89.00%) (11618/12928)
Epoch: 77 | Batch_idx: 110 |  Loss: (0.2940) |  Loss2: (0.0000) | Acc: (89.00%) (12765/14208)
Epoch: 77 | Batch_idx: 120 |  Loss: (0.2930) |  Loss2: (0.0000) | Acc: (89.00%) (13933/15488)
Epoch: 77 | Batch_idx: 130 |  Loss: (0.2927) |  Loss2: (0.0000) | Acc: (89.00%) (15085/16768)
Epoch: 77 | Batch_idx: 140 |  Loss: (0.2924) |  Loss2: (0.0000) | Acc: (89.00%) (16241/18048)
Epoch: 77 | Batch_idx: 150 |  Loss: (0.2913) |  Loss2: (0.0000) | Acc: (89.00%) (17395/19328)
Epoch: 77 | Batch_idx: 160 |  Loss: (0.2914) |  Loss2: (0.0000) | Acc: (90.00%) (18551/20608)
Epoch: 77 | Batch_idx: 170 |  Loss: (0.2921) |  Loss2: (0.0000) | Acc: (90.00%) (19706/21888)
Epoch: 77 | Batch_idx: 180 |  Loss: (0.2934) |  Loss2: (0.0000) | Acc: (89.00%) (20848/23168)
Epoch: 77 | Batch_idx: 190 |  Loss: (0.2927) |  Loss2: (0.0000) | Acc: (90.00%) (22009/24448)
Epoch: 77 | Batch_idx: 200 |  Loss: (0.2909) |  Loss2: (0.0000) | Acc: (90.00%) (23183/25728)
Epoch: 77 | Batch_idx: 210 |  Loss: (0.2910) |  Loss2: (0.0000) | Acc: (90.00%) (24343/27008)
Epoch: 77 | Batch_idx: 220 |  Loss: (0.2923) |  Loss2: (0.0000) | Acc: (90.00%) (25483/28288)
Epoch: 77 | Batch_idx: 230 |  Loss: (0.2927) |  Loss2: (0.0000) | Acc: (90.00%) (26624/29568)
Epoch: 77 | Batch_idx: 240 |  Loss: (0.2923) |  Loss2: (0.0000) | Acc: (90.00%) (27781/30848)
Epoch: 77 | Batch_idx: 250 |  Loss: (0.2927) |  Loss2: (0.0000) | Acc: (90.00%) (28924/32128)
Epoch: 77 | Batch_idx: 260 |  Loss: (0.2929) |  Loss2: (0.0000) | Acc: (90.00%) (30071/33408)
Epoch: 77 | Batch_idx: 270 |  Loss: (0.2921) |  Loss2: (0.0000) | Acc: (90.00%) (31226/34688)
Epoch: 77 | Batch_idx: 280 |  Loss: (0.2912) |  Loss2: (0.0000) | Acc: (90.00%) (32385/35968)
Epoch: 77 | Batch_idx: 290 |  Loss: (0.2900) |  Loss2: (0.0000) | Acc: (90.00%) (33556/37248)
Epoch: 77 | Batch_idx: 300 |  Loss: (0.2901) |  Loss2: (0.0000) | Acc: (90.00%) (34710/38528)
Epoch: 77 | Batch_idx: 310 |  Loss: (0.2895) |  Loss2: (0.0000) | Acc: (90.00%) (35863/39808)
Epoch: 77 | Batch_idx: 320 |  Loss: (0.2892) |  Loss2: (0.0000) | Acc: (90.00%) (37019/41088)
Epoch: 77 | Batch_idx: 330 |  Loss: (0.2891) |  Loss2: (0.0000) | Acc: (90.00%) (38178/42368)
Epoch: 77 | Batch_idx: 340 |  Loss: (0.2895) |  Loss2: (0.0000) | Acc: (90.00%) (39314/43648)
Epoch: 77 | Batch_idx: 350 |  Loss: (0.2902) |  Loss2: (0.0000) | Acc: (90.00%) (40458/44928)
Epoch: 77 | Batch_idx: 360 |  Loss: (0.2896) |  Loss2: (0.0000) | Acc: (90.00%) (41623/46208)
Epoch: 77 | Batch_idx: 370 |  Loss: (0.2899) |  Loss2: (0.0000) | Acc: (90.00%) (42776/47488)
Epoch: 77 | Batch_idx: 380 |  Loss: (0.2899) |  Loss2: (0.0000) | Acc: (90.00%) (43928/48768)
Epoch: 77 | Batch_idx: 390 |  Loss: (0.2891) |  Loss2: (0.0000) | Acc: (90.00%) (45047/50000)
# TEST : Loss: (0.4302) | Acc: (85.00%) (8561/10000)
percent tensor([0.5622, 0.4378], device='cuda:0')
percent tensor([0.5517, 0.4483], device='cuda:0')
percent tensor([0.5309, 0.4691], device='cuda:0')
percent tensor([0.6040, 0.3960], device='cuda:0')
percent tensor([0.5744, 0.4256], device='cuda:0')
percent tensor([0.6762, 0.3238], device='cuda:0')
percent tensor([0.6936, 0.3064], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 78 | Batch_idx: 0 |  Loss: (0.4019) |  Loss2: (0.0000) | Acc: (82.00%) (105/128)
Epoch: 78 | Batch_idx: 10 |  Loss: (0.3082) |  Loss2: (0.0000) | Acc: (89.00%) (1266/1408)
Epoch: 78 | Batch_idx: 20 |  Loss: (0.2921) |  Loss2: (0.0000) | Acc: (90.00%) (2435/2688)
Epoch: 78 | Batch_idx: 30 |  Loss: (0.2830) |  Loss2: (0.0000) | Acc: (90.00%) (3604/3968)
Epoch: 78 | Batch_idx: 40 |  Loss: (0.2868) |  Loss2: (0.0000) | Acc: (90.00%) (4755/5248)
Epoch: 78 | Batch_idx: 50 |  Loss: (0.2862) |  Loss2: (0.0000) | Acc: (90.00%) (5924/6528)
Epoch: 78 | Batch_idx: 60 |  Loss: (0.2816) |  Loss2: (0.0000) | Acc: (90.00%) (7083/7808)
Epoch: 78 | Batch_idx: 70 |  Loss: (0.2820) |  Loss2: (0.0000) | Acc: (90.00%) (8246/9088)
Epoch: 78 | Batch_idx: 80 |  Loss: (0.2786) |  Loss2: (0.0000) | Acc: (90.00%) (9419/10368)
Epoch: 78 | Batch_idx: 90 |  Loss: (0.2793) |  Loss2: (0.0000) | Acc: (90.00%) (10579/11648)
Epoch: 78 | Batch_idx: 100 |  Loss: (0.2826) |  Loss2: (0.0000) | Acc: (90.00%) (11720/12928)
Epoch: 78 | Batch_idx: 110 |  Loss: (0.2828) |  Loss2: (0.0000) | Acc: (90.00%) (12877/14208)
Epoch: 78 | Batch_idx: 120 |  Loss: (0.2815) |  Loss2: (0.0000) | Acc: (90.00%) (14048/15488)
Epoch: 78 | Batch_idx: 130 |  Loss: (0.2809) |  Loss2: (0.0000) | Acc: (90.00%) (15201/16768)
Epoch: 78 | Batch_idx: 140 |  Loss: (0.2794) |  Loss2: (0.0000) | Acc: (90.00%) (16375/18048)
Epoch: 78 | Batch_idx: 150 |  Loss: (0.2787) |  Loss2: (0.0000) | Acc: (90.00%) (17539/19328)
Epoch: 78 | Batch_idx: 160 |  Loss: (0.2779) |  Loss2: (0.0000) | Acc: (90.00%) (18702/20608)
Epoch: 78 | Batch_idx: 170 |  Loss: (0.2770) |  Loss2: (0.0000) | Acc: (90.00%) (19876/21888)
Epoch: 78 | Batch_idx: 180 |  Loss: (0.2773) |  Loss2: (0.0000) | Acc: (90.00%) (21029/23168)
Epoch: 78 | Batch_idx: 190 |  Loss: (0.2758) |  Loss2: (0.0000) | Acc: (90.00%) (22204/24448)
Epoch: 78 | Batch_idx: 200 |  Loss: (0.2764) |  Loss2: (0.0000) | Acc: (90.00%) (23367/25728)
Epoch: 78 | Batch_idx: 210 |  Loss: (0.2771) |  Loss2: (0.0000) | Acc: (90.00%) (24519/27008)
Epoch: 78 | Batch_idx: 220 |  Loss: (0.2772) |  Loss2: (0.0000) | Acc: (90.00%) (25675/28288)
Epoch: 78 | Batch_idx: 230 |  Loss: (0.2778) |  Loss2: (0.0000) | Acc: (90.00%) (26828/29568)
Epoch: 78 | Batch_idx: 240 |  Loss: (0.2779) |  Loss2: (0.0000) | Acc: (90.00%) (27987/30848)
Epoch: 78 | Batch_idx: 250 |  Loss: (0.2784) |  Loss2: (0.0000) | Acc: (90.00%) (29146/32128)
Epoch: 78 | Batch_idx: 260 |  Loss: (0.2783) |  Loss2: (0.0000) | Acc: (90.00%) (30306/33408)
Epoch: 78 | Batch_idx: 270 |  Loss: (0.2787) |  Loss2: (0.0000) | Acc: (90.00%) (31460/34688)
Epoch: 78 | Batch_idx: 280 |  Loss: (0.2782) |  Loss2: (0.0000) | Acc: (90.00%) (32615/35968)
Epoch: 78 | Batch_idx: 290 |  Loss: (0.2782) |  Loss2: (0.0000) | Acc: (90.00%) (33770/37248)
Epoch: 78 | Batch_idx: 300 |  Loss: (0.2787) |  Loss2: (0.0000) | Acc: (90.00%) (34920/38528)
Epoch: 78 | Batch_idx: 310 |  Loss: (0.2795) |  Loss2: (0.0000) | Acc: (90.00%) (36064/39808)
Epoch: 78 | Batch_idx: 320 |  Loss: (0.2800) |  Loss2: (0.0000) | Acc: (90.00%) (37210/41088)
Epoch: 78 | Batch_idx: 330 |  Loss: (0.2799) |  Loss2: (0.0000) | Acc: (90.00%) (38376/42368)
Epoch: 78 | Batch_idx: 340 |  Loss: (0.2798) |  Loss2: (0.0000) | Acc: (90.00%) (39527/43648)
Epoch: 78 | Batch_idx: 350 |  Loss: (0.2795) |  Loss2: (0.0000) | Acc: (90.00%) (40696/44928)
Epoch: 78 | Batch_idx: 360 |  Loss: (0.2799) |  Loss2: (0.0000) | Acc: (90.00%) (41854/46208)
Epoch: 78 | Batch_idx: 370 |  Loss: (0.2809) |  Loss2: (0.0000) | Acc: (90.00%) (42991/47488)
Epoch: 78 | Batch_idx: 380 |  Loss: (0.2807) |  Loss2: (0.0000) | Acc: (90.00%) (44146/48768)
Epoch: 78 | Batch_idx: 390 |  Loss: (0.2807) |  Loss2: (0.0000) | Acc: (90.00%) (45263/50000)
# TEST : Loss: (0.4183) | Acc: (85.00%) (8597/10000)
percent tensor([0.5663, 0.4337], device='cuda:0')
percent tensor([0.5503, 0.4497], device='cuda:0')
percent tensor([0.5311, 0.4689], device='cuda:0')
percent tensor([0.6051, 0.3949], device='cuda:0')
percent tensor([0.5808, 0.4192], device='cuda:0')
percent tensor([0.6826, 0.3174], device='cuda:0')
percent tensor([0.6960, 0.3040], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 79 | Batch_idx: 0 |  Loss: (0.3578) |  Loss2: (0.0000) | Acc: (87.00%) (112/128)
Epoch: 79 | Batch_idx: 10 |  Loss: (0.2468) |  Loss2: (0.0000) | Acc: (91.00%) (1292/1408)
Epoch: 79 | Batch_idx: 20 |  Loss: (0.2642) |  Loss2: (0.0000) | Acc: (91.00%) (2452/2688)
Epoch: 79 | Batch_idx: 30 |  Loss: (0.2774) |  Loss2: (0.0000) | Acc: (90.00%) (3603/3968)
Epoch: 79 | Batch_idx: 40 |  Loss: (0.2761) |  Loss2: (0.0000) | Acc: (90.00%) (4760/5248)
Epoch: 79 | Batch_idx: 50 |  Loss: (0.2763) |  Loss2: (0.0000) | Acc: (90.00%) (5909/6528)
Epoch: 79 | Batch_idx: 60 |  Loss: (0.2776) |  Loss2: (0.0000) | Acc: (90.00%) (7067/7808)
Epoch: 79 | Batch_idx: 70 |  Loss: (0.2787) |  Loss2: (0.0000) | Acc: (90.00%) (8213/9088)
Epoch: 79 | Batch_idx: 80 |  Loss: (0.2769) |  Loss2: (0.0000) | Acc: (90.00%) (9378/10368)
Epoch: 79 | Batch_idx: 90 |  Loss: (0.2776) |  Loss2: (0.0000) | Acc: (90.00%) (10534/11648)
Epoch: 79 | Batch_idx: 100 |  Loss: (0.2779) |  Loss2: (0.0000) | Acc: (90.00%) (11688/12928)
Epoch: 79 | Batch_idx: 110 |  Loss: (0.2785) |  Loss2: (0.0000) | Acc: (90.00%) (12838/14208)
Epoch: 79 | Batch_idx: 120 |  Loss: (0.2782) |  Loss2: (0.0000) | Acc: (90.00%) (13995/15488)
Epoch: 79 | Batch_idx: 130 |  Loss: (0.2791) |  Loss2: (0.0000) | Acc: (90.00%) (15163/16768)
Epoch: 79 | Batch_idx: 140 |  Loss: (0.2814) |  Loss2: (0.0000) | Acc: (90.00%) (16309/18048)
Epoch: 79 | Batch_idx: 150 |  Loss: (0.2820) |  Loss2: (0.0000) | Acc: (90.00%) (17461/19328)
Epoch: 79 | Batch_idx: 160 |  Loss: (0.2812) |  Loss2: (0.0000) | Acc: (90.00%) (18620/20608)
Epoch: 79 | Batch_idx: 170 |  Loss: (0.2817) |  Loss2: (0.0000) | Acc: (90.00%) (19762/21888)
Epoch: 79 | Batch_idx: 180 |  Loss: (0.2799) |  Loss2: (0.0000) | Acc: (90.00%) (20929/23168)
Epoch: 79 | Batch_idx: 190 |  Loss: (0.2793) |  Loss2: (0.0000) | Acc: (90.00%) (22095/24448)
Epoch: 79 | Batch_idx: 200 |  Loss: (0.2779) |  Loss2: (0.0000) | Acc: (90.00%) (23263/25728)
Epoch: 79 | Batch_idx: 210 |  Loss: (0.2783) |  Loss2: (0.0000) | Acc: (90.00%) (24432/27008)
Epoch: 79 | Batch_idx: 220 |  Loss: (0.2781) |  Loss2: (0.0000) | Acc: (90.00%) (25587/28288)
Epoch: 79 | Batch_idx: 230 |  Loss: (0.2778) |  Loss2: (0.0000) | Acc: (90.00%) (26745/29568)
Epoch: 79 | Batch_idx: 240 |  Loss: (0.2786) |  Loss2: (0.0000) | Acc: (90.00%) (27896/30848)
Epoch: 79 | Batch_idx: 250 |  Loss: (0.2777) |  Loss2: (0.0000) | Acc: (90.00%) (29063/32128)
Epoch: 79 | Batch_idx: 260 |  Loss: (0.2770) |  Loss2: (0.0000) | Acc: (90.00%) (30229/33408)
Epoch: 79 | Batch_idx: 270 |  Loss: (0.2756) |  Loss2: (0.0000) | Acc: (90.00%) (31408/34688)
Epoch: 79 | Batch_idx: 280 |  Loss: (0.2746) |  Loss2: (0.0000) | Acc: (90.00%) (32579/35968)
Epoch: 79 | Batch_idx: 290 |  Loss: (0.2736) |  Loss2: (0.0000) | Acc: (90.00%) (33746/37248)
Epoch: 79 | Batch_idx: 300 |  Loss: (0.2736) |  Loss2: (0.0000) | Acc: (90.00%) (34907/38528)
Epoch: 79 | Batch_idx: 310 |  Loss: (0.2741) |  Loss2: (0.0000) | Acc: (90.00%) (36062/39808)
Epoch: 79 | Batch_idx: 320 |  Loss: (0.2733) |  Loss2: (0.0000) | Acc: (90.00%) (37238/41088)
Epoch: 79 | Batch_idx: 330 |  Loss: (0.2736) |  Loss2: (0.0000) | Acc: (90.00%) (38403/42368)
Epoch: 79 | Batch_idx: 340 |  Loss: (0.2730) |  Loss2: (0.0000) | Acc: (90.00%) (39579/43648)
Epoch: 79 | Batch_idx: 350 |  Loss: (0.2734) |  Loss2: (0.0000) | Acc: (90.00%) (40724/44928)
Epoch: 79 | Batch_idx: 360 |  Loss: (0.2733) |  Loss2: (0.0000) | Acc: (90.00%) (41890/46208)
Epoch: 79 | Batch_idx: 370 |  Loss: (0.2722) |  Loss2: (0.0000) | Acc: (90.00%) (43065/47488)
Epoch: 79 | Batch_idx: 380 |  Loss: (0.2728) |  Loss2: (0.0000) | Acc: (90.00%) (44213/48768)
Epoch: 79 | Batch_idx: 390 |  Loss: (0.2727) |  Loss2: (0.0000) | Acc: (90.00%) (45335/50000)
# TEST : Loss: (0.4191) | Acc: (85.00%) (8580/10000)
percent tensor([0.5642, 0.4358], device='cuda:0')
percent tensor([0.5501, 0.4499], device='cuda:0')
percent tensor([0.5309, 0.4691], device='cuda:0')
percent tensor([0.6068, 0.3932], device='cuda:0')
percent tensor([0.5842, 0.4158], device='cuda:0')
percent tensor([0.6925, 0.3075], device='cuda:0')
percent tensor([0.7018, 0.2982], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 80 | Batch_idx: 0 |  Loss: (0.2400) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 80 | Batch_idx: 10 |  Loss: (0.3064) |  Loss2: (0.0000) | Acc: (89.00%) (1263/1408)
Epoch: 80 | Batch_idx: 20 |  Loss: (0.2871) |  Loss2: (0.0000) | Acc: (90.00%) (2432/2688)
Epoch: 80 | Batch_idx: 30 |  Loss: (0.2888) |  Loss2: (0.0000) | Acc: (90.00%) (3582/3968)
Epoch: 80 | Batch_idx: 40 |  Loss: (0.2888) |  Loss2: (0.0000) | Acc: (90.00%) (4739/5248)
Epoch: 80 | Batch_idx: 50 |  Loss: (0.2821) |  Loss2: (0.0000) | Acc: (90.00%) (5906/6528)
Epoch: 80 | Batch_idx: 60 |  Loss: (0.2801) |  Loss2: (0.0000) | Acc: (90.00%) (7077/7808)
Epoch: 80 | Batch_idx: 70 |  Loss: (0.2799) |  Loss2: (0.0000) | Acc: (90.00%) (8234/9088)
Epoch: 80 | Batch_idx: 80 |  Loss: (0.2768) |  Loss2: (0.0000) | Acc: (90.00%) (9403/10368)
Epoch: 80 | Batch_idx: 90 |  Loss: (0.2762) |  Loss2: (0.0000) | Acc: (90.00%) (10574/11648)
Epoch: 80 | Batch_idx: 100 |  Loss: (0.2734) |  Loss2: (0.0000) | Acc: (90.00%) (11742/12928)
Epoch: 80 | Batch_idx: 110 |  Loss: (0.2744) |  Loss2: (0.0000) | Acc: (90.00%) (12902/14208)
Epoch: 80 | Batch_idx: 120 |  Loss: (0.2748) |  Loss2: (0.0000) | Acc: (90.00%) (14061/15488)
Epoch: 80 | Batch_idx: 130 |  Loss: (0.2777) |  Loss2: (0.0000) | Acc: (90.00%) (15205/16768)
Epoch: 80 | Batch_idx: 140 |  Loss: (0.2756) |  Loss2: (0.0000) | Acc: (90.00%) (16373/18048)
Epoch: 80 | Batch_idx: 150 |  Loss: (0.2745) |  Loss2: (0.0000) | Acc: (90.00%) (17528/19328)
Epoch: 80 | Batch_idx: 160 |  Loss: (0.2756) |  Loss2: (0.0000) | Acc: (90.00%) (18684/20608)
Epoch: 80 | Batch_idx: 170 |  Loss: (0.2778) |  Loss2: (0.0000) | Acc: (90.00%) (19821/21888)
Epoch: 80 | Batch_idx: 180 |  Loss: (0.2782) |  Loss2: (0.0000) | Acc: (90.00%) (20978/23168)
Epoch: 80 | Batch_idx: 190 |  Loss: (0.2774) |  Loss2: (0.0000) | Acc: (90.00%) (22150/24448)
Epoch: 80 | Batch_idx: 200 |  Loss: (0.2776) |  Loss2: (0.0000) | Acc: (90.00%) (23306/25728)
Epoch: 80 | Batch_idx: 210 |  Loss: (0.2783) |  Loss2: (0.0000) | Acc: (90.00%) (24448/27008)
Epoch: 80 | Batch_idx: 220 |  Loss: (0.2787) |  Loss2: (0.0000) | Acc: (90.00%) (25607/28288)
Epoch: 80 | Batch_idx: 230 |  Loss: (0.2780) |  Loss2: (0.0000) | Acc: (90.00%) (26765/29568)
Epoch: 80 | Batch_idx: 240 |  Loss: (0.2767) |  Loss2: (0.0000) | Acc: (90.00%) (27946/30848)
Epoch: 80 | Batch_idx: 250 |  Loss: (0.2765) |  Loss2: (0.0000) | Acc: (90.00%) (29101/32128)
Epoch: 80 | Batch_idx: 260 |  Loss: (0.2777) |  Loss2: (0.0000) | Acc: (90.00%) (30242/33408)
Epoch: 80 | Batch_idx: 270 |  Loss: (0.2795) |  Loss2: (0.0000) | Acc: (90.00%) (31383/34688)
Epoch: 80 | Batch_idx: 280 |  Loss: (0.2793) |  Loss2: (0.0000) | Acc: (90.00%) (32561/35968)
Epoch: 80 | Batch_idx: 290 |  Loss: (0.2792) |  Loss2: (0.0000) | Acc: (90.00%) (33718/37248)
Epoch: 80 | Batch_idx: 300 |  Loss: (0.2790) |  Loss2: (0.0000) | Acc: (90.00%) (34886/38528)
Epoch: 80 | Batch_idx: 310 |  Loss: (0.2795) |  Loss2: (0.0000) | Acc: (90.00%) (36028/39808)
Epoch: 80 | Batch_idx: 320 |  Loss: (0.2807) |  Loss2: (0.0000) | Acc: (90.00%) (37160/41088)
Epoch: 80 | Batch_idx: 330 |  Loss: (0.2821) |  Loss2: (0.0000) | Acc: (90.00%) (38300/42368)
Epoch: 80 | Batch_idx: 340 |  Loss: (0.2825) |  Loss2: (0.0000) | Acc: (90.00%) (39455/43648)
Epoch: 80 | Batch_idx: 350 |  Loss: (0.2818) |  Loss2: (0.0000) | Acc: (90.00%) (40627/44928)
Epoch: 80 | Batch_idx: 360 |  Loss: (0.2825) |  Loss2: (0.0000) | Acc: (90.00%) (41766/46208)
Epoch: 80 | Batch_idx: 370 |  Loss: (0.2829) |  Loss2: (0.0000) | Acc: (90.00%) (42906/47488)
Epoch: 80 | Batch_idx: 380 |  Loss: (0.2827) |  Loss2: (0.0000) | Acc: (90.00%) (44067/48768)
Epoch: 80 | Batch_idx: 390 |  Loss: (0.2828) |  Loss2: (0.0000) | Acc: (90.00%) (45172/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_080.pth.tar'
# TEST : Loss: (0.4911) | Acc: (84.00%) (8489/10000)
percent tensor([0.5647, 0.4353], device='cuda:0')
percent tensor([0.5496, 0.4504], device='cuda:0')
percent tensor([0.5306, 0.4694], device='cuda:0')
percent tensor([0.6052, 0.3948], device='cuda:0')
percent tensor([0.5827, 0.4173], device='cuda:0')
percent tensor([0.6911, 0.3089], device='cuda:0')
percent tensor([0.7061, 0.2939], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(177.8474, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(809.2537, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(804.6404, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1525.8732, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(496.9120, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2228.3899, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4288.6484, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1403.5630, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6129.6523, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11942.1406, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3962.1802, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16775.2070, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 81 | Batch_idx: 0 |  Loss: (0.2745) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 81 | Batch_idx: 10 |  Loss: (0.2688) |  Loss2: (0.0000) | Acc: (91.00%) (1283/1408)
Epoch: 81 | Batch_idx: 20 |  Loss: (0.2622) |  Loss2: (0.0000) | Acc: (91.00%) (2452/2688)
Epoch: 81 | Batch_idx: 30 |  Loss: (0.2633) |  Loss2: (0.0000) | Acc: (91.00%) (3614/3968)
Epoch: 81 | Batch_idx: 40 |  Loss: (0.2635) |  Loss2: (0.0000) | Acc: (91.00%) (4781/5248)
Epoch: 81 | Batch_idx: 50 |  Loss: (0.2663) |  Loss2: (0.0000) | Acc: (90.00%) (5938/6528)
Epoch: 81 | Batch_idx: 60 |  Loss: (0.2652) |  Loss2: (0.0000) | Acc: (90.00%) (7096/7808)
Epoch: 81 | Batch_idx: 70 |  Loss: (0.2668) |  Loss2: (0.0000) | Acc: (90.00%) (8252/9088)
Epoch: 81 | Batch_idx: 80 |  Loss: (0.2674) |  Loss2: (0.0000) | Acc: (90.00%) (9422/10368)
Epoch: 81 | Batch_idx: 90 |  Loss: (0.2718) |  Loss2: (0.0000) | Acc: (90.00%) (10571/11648)
Epoch: 81 | Batch_idx: 100 |  Loss: (0.2738) |  Loss2: (0.0000) | Acc: (90.00%) (11733/12928)
Epoch: 81 | Batch_idx: 110 |  Loss: (0.2740) |  Loss2: (0.0000) | Acc: (90.00%) (12897/14208)
Epoch: 81 | Batch_idx: 120 |  Loss: (0.2741) |  Loss2: (0.0000) | Acc: (90.00%) (14059/15488)
Epoch: 81 | Batch_idx: 130 |  Loss: (0.2720) |  Loss2: (0.0000) | Acc: (90.00%) (15234/16768)
Epoch: 81 | Batch_idx: 140 |  Loss: (0.2724) |  Loss2: (0.0000) | Acc: (90.00%) (16402/18048)
Epoch: 81 | Batch_idx: 150 |  Loss: (0.2729) |  Loss2: (0.0000) | Acc: (90.00%) (17573/19328)
Epoch: 81 | Batch_idx: 160 |  Loss: (0.2721) |  Loss2: (0.0000) | Acc: (90.00%) (18729/20608)
Epoch: 81 | Batch_idx: 170 |  Loss: (0.2718) |  Loss2: (0.0000) | Acc: (90.00%) (19881/21888)
Epoch: 81 | Batch_idx: 180 |  Loss: (0.2716) |  Loss2: (0.0000) | Acc: (90.00%) (21054/23168)
Epoch: 81 | Batch_idx: 190 |  Loss: (0.2718) |  Loss2: (0.0000) | Acc: (90.00%) (22210/24448)
Epoch: 81 | Batch_idx: 200 |  Loss: (0.2715) |  Loss2: (0.0000) | Acc: (90.00%) (23368/25728)
Epoch: 81 | Batch_idx: 210 |  Loss: (0.2717) |  Loss2: (0.0000) | Acc: (90.00%) (24533/27008)
Epoch: 81 | Batch_idx: 220 |  Loss: (0.2719) |  Loss2: (0.0000) | Acc: (90.00%) (25688/28288)
Epoch: 81 | Batch_idx: 230 |  Loss: (0.2711) |  Loss2: (0.0000) | Acc: (90.00%) (26861/29568)
Epoch: 81 | Batch_idx: 240 |  Loss: (0.2724) |  Loss2: (0.0000) | Acc: (90.00%) (28006/30848)
Epoch: 81 | Batch_idx: 250 |  Loss: (0.2718) |  Loss2: (0.0000) | Acc: (90.00%) (29175/32128)
Epoch: 81 | Batch_idx: 260 |  Loss: (0.2719) |  Loss2: (0.0000) | Acc: (90.00%) (30342/33408)
Epoch: 81 | Batch_idx: 270 |  Loss: (0.2718) |  Loss2: (0.0000) | Acc: (90.00%) (31503/34688)
Epoch: 81 | Batch_idx: 280 |  Loss: (0.2715) |  Loss2: (0.0000) | Acc: (90.00%) (32656/35968)
Epoch: 81 | Batch_idx: 290 |  Loss: (0.2721) |  Loss2: (0.0000) | Acc: (90.00%) (33799/37248)
Epoch: 81 | Batch_idx: 300 |  Loss: (0.2724) |  Loss2: (0.0000) | Acc: (90.00%) (34949/38528)
Epoch: 81 | Batch_idx: 310 |  Loss: (0.2717) |  Loss2: (0.0000) | Acc: (90.00%) (36115/39808)
Epoch: 81 | Batch_idx: 320 |  Loss: (0.2722) |  Loss2: (0.0000) | Acc: (90.00%) (37270/41088)
Epoch: 81 | Batch_idx: 330 |  Loss: (0.2731) |  Loss2: (0.0000) | Acc: (90.00%) (38413/42368)
Epoch: 81 | Batch_idx: 340 |  Loss: (0.2744) |  Loss2: (0.0000) | Acc: (90.00%) (39551/43648)
Epoch: 81 | Batch_idx: 350 |  Loss: (0.2742) |  Loss2: (0.0000) | Acc: (90.00%) (40719/44928)
Epoch: 81 | Batch_idx: 360 |  Loss: (0.2738) |  Loss2: (0.0000) | Acc: (90.00%) (41894/46208)
Epoch: 81 | Batch_idx: 370 |  Loss: (0.2741) |  Loss2: (0.0000) | Acc: (90.00%) (43052/47488)
Epoch: 81 | Batch_idx: 380 |  Loss: (0.2746) |  Loss2: (0.0000) | Acc: (90.00%) (44202/48768)
Epoch: 81 | Batch_idx: 390 |  Loss: (0.2748) |  Loss2: (0.0000) | Acc: (90.00%) (45329/50000)
# TEST : Loss: (0.5028) | Acc: (84.00%) (8462/10000)
percent tensor([0.5644, 0.4356], device='cuda:0')
percent tensor([0.5506, 0.4494], device='cuda:0')
percent tensor([0.5312, 0.4688], device='cuda:0')
percent tensor([0.6067, 0.3933], device='cuda:0')
percent tensor([0.5834, 0.4166], device='cuda:0')
percent tensor([0.6923, 0.3077], device='cuda:0')
percent tensor([0.7055, 0.2945], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 82 | Batch_idx: 0 |  Loss: (0.2901) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 82 | Batch_idx: 10 |  Loss: (0.2674) |  Loss2: (0.0000) | Acc: (90.00%) (1276/1408)
Epoch: 82 | Batch_idx: 20 |  Loss: (0.2578) |  Loss2: (0.0000) | Acc: (91.00%) (2452/2688)
Epoch: 82 | Batch_idx: 30 |  Loss: (0.2600) |  Loss2: (0.0000) | Acc: (90.00%) (3610/3968)
Epoch: 82 | Batch_idx: 40 |  Loss: (0.2591) |  Loss2: (0.0000) | Acc: (91.00%) (4779/5248)
Epoch: 82 | Batch_idx: 50 |  Loss: (0.2623) |  Loss2: (0.0000) | Acc: (91.00%) (5941/6528)
Epoch: 82 | Batch_idx: 60 |  Loss: (0.2625) |  Loss2: (0.0000) | Acc: (90.00%) (7099/7808)
Epoch: 82 | Batch_idx: 70 |  Loss: (0.2601) |  Loss2: (0.0000) | Acc: (91.00%) (8274/9088)
Epoch: 82 | Batch_idx: 80 |  Loss: (0.2589) |  Loss2: (0.0000) | Acc: (91.00%) (9441/10368)
Epoch: 82 | Batch_idx: 90 |  Loss: (0.2613) |  Loss2: (0.0000) | Acc: (91.00%) (10602/11648)
Epoch: 82 | Batch_idx: 100 |  Loss: (0.2615) |  Loss2: (0.0000) | Acc: (91.00%) (11770/12928)
Epoch: 82 | Batch_idx: 110 |  Loss: (0.2597) |  Loss2: (0.0000) | Acc: (91.00%) (12950/14208)
Epoch: 82 | Batch_idx: 120 |  Loss: (0.2602) |  Loss2: (0.0000) | Acc: (91.00%) (14119/15488)
Epoch: 82 | Batch_idx: 130 |  Loss: (0.2593) |  Loss2: (0.0000) | Acc: (91.00%) (15297/16768)
Epoch: 82 | Batch_idx: 140 |  Loss: (0.2599) |  Loss2: (0.0000) | Acc: (91.00%) (16458/18048)
Epoch: 82 | Batch_idx: 150 |  Loss: (0.2603) |  Loss2: (0.0000) | Acc: (91.00%) (17621/19328)
Epoch: 82 | Batch_idx: 160 |  Loss: (0.2625) |  Loss2: (0.0000) | Acc: (91.00%) (18780/20608)
Epoch: 82 | Batch_idx: 170 |  Loss: (0.2631) |  Loss2: (0.0000) | Acc: (91.00%) (19936/21888)
Epoch: 82 | Batch_idx: 180 |  Loss: (0.2626) |  Loss2: (0.0000) | Acc: (91.00%) (21109/23168)
Epoch: 82 | Batch_idx: 190 |  Loss: (0.2629) |  Loss2: (0.0000) | Acc: (91.00%) (22267/24448)
Epoch: 82 | Batch_idx: 200 |  Loss: (0.2644) |  Loss2: (0.0000) | Acc: (91.00%) (23425/25728)
Epoch: 82 | Batch_idx: 210 |  Loss: (0.2651) |  Loss2: (0.0000) | Acc: (91.00%) (24584/27008)
Epoch: 82 | Batch_idx: 220 |  Loss: (0.2650) |  Loss2: (0.0000) | Acc: (91.00%) (25760/28288)
Epoch: 82 | Batch_idx: 230 |  Loss: (0.2645) |  Loss2: (0.0000) | Acc: (91.00%) (26933/29568)
Epoch: 82 | Batch_idx: 240 |  Loss: (0.2636) |  Loss2: (0.0000) | Acc: (91.00%) (28103/30848)
Epoch: 82 | Batch_idx: 250 |  Loss: (0.2644) |  Loss2: (0.0000) | Acc: (91.00%) (29250/32128)
Epoch: 82 | Batch_idx: 260 |  Loss: (0.2644) |  Loss2: (0.0000) | Acc: (91.00%) (30416/33408)
Epoch: 82 | Batch_idx: 270 |  Loss: (0.2648) |  Loss2: (0.0000) | Acc: (91.00%) (31580/34688)
Epoch: 82 | Batch_idx: 280 |  Loss: (0.2642) |  Loss2: (0.0000) | Acc: (91.00%) (32754/35968)
Epoch: 82 | Batch_idx: 290 |  Loss: (0.2655) |  Loss2: (0.0000) | Acc: (91.00%) (33908/37248)
Epoch: 82 | Batch_idx: 300 |  Loss: (0.2658) |  Loss2: (0.0000) | Acc: (91.00%) (35068/38528)
Epoch: 82 | Batch_idx: 310 |  Loss: (0.2659) |  Loss2: (0.0000) | Acc: (91.00%) (36234/39808)
Epoch: 82 | Batch_idx: 320 |  Loss: (0.2658) |  Loss2: (0.0000) | Acc: (91.00%) (37408/41088)
Epoch: 82 | Batch_idx: 330 |  Loss: (0.2656) |  Loss2: (0.0000) | Acc: (91.00%) (38578/42368)
Epoch: 82 | Batch_idx: 340 |  Loss: (0.2660) |  Loss2: (0.0000) | Acc: (91.00%) (39737/43648)
Epoch: 82 | Batch_idx: 350 |  Loss: (0.2668) |  Loss2: (0.0000) | Acc: (91.00%) (40891/44928)
Epoch: 82 | Batch_idx: 360 |  Loss: (0.2673) |  Loss2: (0.0000) | Acc: (90.00%) (42049/46208)
Epoch: 82 | Batch_idx: 370 |  Loss: (0.2671) |  Loss2: (0.0000) | Acc: (91.00%) (43219/47488)
Epoch: 82 | Batch_idx: 380 |  Loss: (0.2676) |  Loss2: (0.0000) | Acc: (90.00%) (44374/48768)
Epoch: 82 | Batch_idx: 390 |  Loss: (0.2672) |  Loss2: (0.0000) | Acc: (91.00%) (45502/50000)
# TEST : Loss: (0.4351) | Acc: (86.00%) (8608/10000)
percent tensor([0.5651, 0.4349], device='cuda:0')
percent tensor([0.5504, 0.4496], device='cuda:0')
percent tensor([0.5313, 0.4687], device='cuda:0')
percent tensor([0.6059, 0.3941], device='cuda:0')
percent tensor([0.5834, 0.4166], device='cuda:0')
percent tensor([0.6923, 0.3077], device='cuda:0')
percent tensor([0.7063, 0.2937], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 83 | Batch_idx: 0 |  Loss: (0.2393) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 83 | Batch_idx: 10 |  Loss: (0.2298) |  Loss2: (0.0000) | Acc: (92.00%) (1301/1408)
Epoch: 83 | Batch_idx: 20 |  Loss: (0.2331) |  Loss2: (0.0000) | Acc: (92.00%) (2476/2688)
Epoch: 83 | Batch_idx: 30 |  Loss: (0.2340) |  Loss2: (0.0000) | Acc: (92.00%) (3651/3968)
Epoch: 83 | Batch_idx: 40 |  Loss: (0.2389) |  Loss2: (0.0000) | Acc: (92.00%) (4829/5248)
Epoch: 83 | Batch_idx: 50 |  Loss: (0.2356) |  Loss2: (0.0000) | Acc: (92.00%) (6015/6528)
Epoch: 83 | Batch_idx: 60 |  Loss: (0.2367) |  Loss2: (0.0000) | Acc: (92.00%) (7189/7808)
Epoch: 83 | Batch_idx: 70 |  Loss: (0.2393) |  Loss2: (0.0000) | Acc: (91.00%) (8350/9088)
Epoch: 83 | Batch_idx: 80 |  Loss: (0.2449) |  Loss2: (0.0000) | Acc: (91.00%) (9498/10368)
Epoch: 83 | Batch_idx: 90 |  Loss: (0.2479) |  Loss2: (0.0000) | Acc: (91.00%) (10645/11648)
Epoch: 83 | Batch_idx: 100 |  Loss: (0.2503) |  Loss2: (0.0000) | Acc: (91.00%) (11793/12928)
Epoch: 83 | Batch_idx: 110 |  Loss: (0.2501) |  Loss2: (0.0000) | Acc: (91.00%) (12959/14208)
Epoch: 83 | Batch_idx: 120 |  Loss: (0.2493) |  Loss2: (0.0000) | Acc: (91.00%) (14136/15488)
Epoch: 83 | Batch_idx: 130 |  Loss: (0.2486) |  Loss2: (0.0000) | Acc: (91.00%) (15308/16768)
Epoch: 83 | Batch_idx: 140 |  Loss: (0.2482) |  Loss2: (0.0000) | Acc: (91.00%) (16484/18048)
Epoch: 83 | Batch_idx: 150 |  Loss: (0.2493) |  Loss2: (0.0000) | Acc: (91.00%) (17656/19328)
Epoch: 83 | Batch_idx: 160 |  Loss: (0.2478) |  Loss2: (0.0000) | Acc: (91.00%) (18828/20608)
Epoch: 83 | Batch_idx: 170 |  Loss: (0.2493) |  Loss2: (0.0000) | Acc: (91.00%) (19999/21888)
Epoch: 83 | Batch_idx: 180 |  Loss: (0.2505) |  Loss2: (0.0000) | Acc: (91.00%) (21155/23168)
Epoch: 83 | Batch_idx: 190 |  Loss: (0.2505) |  Loss2: (0.0000) | Acc: (91.00%) (22324/24448)
Epoch: 83 | Batch_idx: 200 |  Loss: (0.2530) |  Loss2: (0.0000) | Acc: (91.00%) (23481/25728)
Epoch: 83 | Batch_idx: 210 |  Loss: (0.2525) |  Loss2: (0.0000) | Acc: (91.00%) (24646/27008)
Epoch: 83 | Batch_idx: 220 |  Loss: (0.2520) |  Loss2: (0.0000) | Acc: (91.00%) (25819/28288)
Epoch: 83 | Batch_idx: 230 |  Loss: (0.2537) |  Loss2: (0.0000) | Acc: (91.00%) (26970/29568)
Epoch: 83 | Batch_idx: 240 |  Loss: (0.2555) |  Loss2: (0.0000) | Acc: (91.00%) (28122/30848)
Epoch: 83 | Batch_idx: 250 |  Loss: (0.2559) |  Loss2: (0.0000) | Acc: (91.00%) (29290/32128)
Epoch: 83 | Batch_idx: 260 |  Loss: (0.2561) |  Loss2: (0.0000) | Acc: (91.00%) (30457/33408)
Epoch: 83 | Batch_idx: 270 |  Loss: (0.2567) |  Loss2: (0.0000) | Acc: (91.00%) (31620/34688)
Epoch: 83 | Batch_idx: 280 |  Loss: (0.2566) |  Loss2: (0.0000) | Acc: (91.00%) (32784/35968)
Epoch: 83 | Batch_idx: 290 |  Loss: (0.2573) |  Loss2: (0.0000) | Acc: (91.00%) (33948/37248)
Epoch: 83 | Batch_idx: 300 |  Loss: (0.2572) |  Loss2: (0.0000) | Acc: (91.00%) (35118/38528)
Epoch: 83 | Batch_idx: 310 |  Loss: (0.2584) |  Loss2: (0.0000) | Acc: (91.00%) (36266/39808)
Epoch: 83 | Batch_idx: 320 |  Loss: (0.2589) |  Loss2: (0.0000) | Acc: (91.00%) (37421/41088)
Epoch: 83 | Batch_idx: 330 |  Loss: (0.2600) |  Loss2: (0.0000) | Acc: (91.00%) (38567/42368)
Epoch: 83 | Batch_idx: 340 |  Loss: (0.2607) |  Loss2: (0.0000) | Acc: (91.00%) (39732/43648)
Epoch: 83 | Batch_idx: 350 |  Loss: (0.2603) |  Loss2: (0.0000) | Acc: (91.00%) (40906/44928)
Epoch: 83 | Batch_idx: 360 |  Loss: (0.2604) |  Loss2: (0.0000) | Acc: (91.00%) (42060/46208)
Epoch: 83 | Batch_idx: 370 |  Loss: (0.2609) |  Loss2: (0.0000) | Acc: (91.00%) (43220/47488)
Epoch: 83 | Batch_idx: 380 |  Loss: (0.2605) |  Loss2: (0.0000) | Acc: (91.00%) (44394/48768)
Epoch: 83 | Batch_idx: 390 |  Loss: (0.2611) |  Loss2: (0.0000) | Acc: (91.00%) (45514/50000)
# TEST : Loss: (0.4384) | Acc: (86.00%) (8614/10000)
percent tensor([0.5653, 0.4347], device='cuda:0')
percent tensor([0.5507, 0.4493], device='cuda:0')
percent tensor([0.5313, 0.4687], device='cuda:0')
percent tensor([0.6051, 0.3949], device='cuda:0')
percent tensor([0.5844, 0.4156], device='cuda:0')
percent tensor([0.6904, 0.3096], device='cuda:0')
percent tensor([0.7013, 0.2987], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 84 | Batch_idx: 0 |  Loss: (0.2653) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 84 | Batch_idx: 10 |  Loss: (0.2326) |  Loss2: (0.0000) | Acc: (91.00%) (1294/1408)
Epoch: 84 | Batch_idx: 20 |  Loss: (0.2322) |  Loss2: (0.0000) | Acc: (92.00%) (2474/2688)
Epoch: 84 | Batch_idx: 30 |  Loss: (0.2287) |  Loss2: (0.0000) | Acc: (92.00%) (3655/3968)
Epoch: 84 | Batch_idx: 40 |  Loss: (0.2343) |  Loss2: (0.0000) | Acc: (91.00%) (4822/5248)
Epoch: 84 | Batch_idx: 50 |  Loss: (0.2345) |  Loss2: (0.0000) | Acc: (92.00%) (6011/6528)
Epoch: 84 | Batch_idx: 60 |  Loss: (0.2331) |  Loss2: (0.0000) | Acc: (92.00%) (7199/7808)
Epoch: 84 | Batch_idx: 70 |  Loss: (0.2337) |  Loss2: (0.0000) | Acc: (92.00%) (8369/9088)
Epoch: 84 | Batch_idx: 80 |  Loss: (0.2359) |  Loss2: (0.0000) | Acc: (91.00%) (9538/10368)
Epoch: 84 | Batch_idx: 90 |  Loss: (0.2381) |  Loss2: (0.0000) | Acc: (91.00%) (10707/11648)
Epoch: 84 | Batch_idx: 100 |  Loss: (0.2405) |  Loss2: (0.0000) | Acc: (91.00%) (11862/12928)
Epoch: 84 | Batch_idx: 110 |  Loss: (0.2410) |  Loss2: (0.0000) | Acc: (91.00%) (13031/14208)
Epoch: 84 | Batch_idx: 120 |  Loss: (0.2459) |  Loss2: (0.0000) | Acc: (91.00%) (14173/15488)
Epoch: 84 | Batch_idx: 130 |  Loss: (0.2440) |  Loss2: (0.0000) | Acc: (91.00%) (15354/16768)
Epoch: 84 | Batch_idx: 140 |  Loss: (0.2478) |  Loss2: (0.0000) | Acc: (91.00%) (16500/18048)
Epoch: 84 | Batch_idx: 150 |  Loss: (0.2476) |  Loss2: (0.0000) | Acc: (91.00%) (17667/19328)
Epoch: 84 | Batch_idx: 160 |  Loss: (0.2501) |  Loss2: (0.0000) | Acc: (91.00%) (18810/20608)
Epoch: 84 | Batch_idx: 170 |  Loss: (0.2499) |  Loss2: (0.0000) | Acc: (91.00%) (19981/21888)
Epoch: 84 | Batch_idx: 180 |  Loss: (0.2489) |  Loss2: (0.0000) | Acc: (91.00%) (21159/23168)
Epoch: 84 | Batch_idx: 190 |  Loss: (0.2473) |  Loss2: (0.0000) | Acc: (91.00%) (22343/24448)
Epoch: 84 | Batch_idx: 200 |  Loss: (0.2489) |  Loss2: (0.0000) | Acc: (91.00%) (23495/25728)
Epoch: 84 | Batch_idx: 210 |  Loss: (0.2481) |  Loss2: (0.0000) | Acc: (91.00%) (24675/27008)
Epoch: 84 | Batch_idx: 220 |  Loss: (0.2473) |  Loss2: (0.0000) | Acc: (91.00%) (25852/28288)
Epoch: 84 | Batch_idx: 230 |  Loss: (0.2470) |  Loss2: (0.0000) | Acc: (91.00%) (27020/29568)
Epoch: 84 | Batch_idx: 240 |  Loss: (0.2481) |  Loss2: (0.0000) | Acc: (91.00%) (28168/30848)
Epoch: 84 | Batch_idx: 250 |  Loss: (0.2483) |  Loss2: (0.0000) | Acc: (91.00%) (29337/32128)
Epoch: 84 | Batch_idx: 260 |  Loss: (0.2483) |  Loss2: (0.0000) | Acc: (91.00%) (30503/33408)
Epoch: 84 | Batch_idx: 270 |  Loss: (0.2484) |  Loss2: (0.0000) | Acc: (91.00%) (31679/34688)
Epoch: 84 | Batch_idx: 280 |  Loss: (0.2481) |  Loss2: (0.0000) | Acc: (91.00%) (32863/35968)
Epoch: 84 | Batch_idx: 290 |  Loss: (0.2484) |  Loss2: (0.0000) | Acc: (91.00%) (34029/37248)
Epoch: 84 | Batch_idx: 300 |  Loss: (0.2485) |  Loss2: (0.0000) | Acc: (91.00%) (35206/38528)
Epoch: 84 | Batch_idx: 310 |  Loss: (0.2482) |  Loss2: (0.0000) | Acc: (91.00%) (36374/39808)
Epoch: 84 | Batch_idx: 320 |  Loss: (0.2492) |  Loss2: (0.0000) | Acc: (91.00%) (37533/41088)
Epoch: 84 | Batch_idx: 330 |  Loss: (0.2491) |  Loss2: (0.0000) | Acc: (91.00%) (38701/42368)
Epoch: 84 | Batch_idx: 340 |  Loss: (0.2498) |  Loss2: (0.0000) | Acc: (91.00%) (39866/43648)
Epoch: 84 | Batch_idx: 350 |  Loss: (0.2504) |  Loss2: (0.0000) | Acc: (91.00%) (41020/44928)
Epoch: 84 | Batch_idx: 360 |  Loss: (0.2503) |  Loss2: (0.0000) | Acc: (91.00%) (42192/46208)
Epoch: 84 | Batch_idx: 370 |  Loss: (0.2499) |  Loss2: (0.0000) | Acc: (91.00%) (43364/47488)
Epoch: 84 | Batch_idx: 380 |  Loss: (0.2500) |  Loss2: (0.0000) | Acc: (91.00%) (44531/48768)
Epoch: 84 | Batch_idx: 390 |  Loss: (0.2505) |  Loss2: (0.0000) | Acc: (91.00%) (45640/50000)
# TEST : Loss: (0.4252) | Acc: (86.00%) (8652/10000)
percent tensor([0.5647, 0.4353], device='cuda:0')
percent tensor([0.5507, 0.4493], device='cuda:0')
percent tensor([0.5308, 0.4692], device='cuda:0')
percent tensor([0.6059, 0.3941], device='cuda:0')
percent tensor([0.5841, 0.4159], device='cuda:0')
percent tensor([0.6910, 0.3090], device='cuda:0')
percent tensor([0.7026, 0.2974], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 85 | Batch_idx: 0 |  Loss: (0.2068) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 85 | Batch_idx: 10 |  Loss: (0.2428) |  Loss2: (0.0000) | Acc: (91.00%) (1293/1408)
Epoch: 85 | Batch_idx: 20 |  Loss: (0.2626) |  Loss2: (0.0000) | Acc: (90.00%) (2445/2688)
Epoch: 85 | Batch_idx: 30 |  Loss: (0.2714) |  Loss2: (0.0000) | Acc: (90.00%) (3594/3968)
Epoch: 85 | Batch_idx: 40 |  Loss: (0.2739) |  Loss2: (0.0000) | Acc: (90.00%) (4744/5248)
Epoch: 85 | Batch_idx: 50 |  Loss: (0.2826) |  Loss2: (0.0000) | Acc: (90.00%) (5892/6528)
Epoch: 85 | Batch_idx: 60 |  Loss: (0.2890) |  Loss2: (0.0000) | Acc: (90.00%) (7032/7808)
Epoch: 85 | Batch_idx: 70 |  Loss: (0.2910) |  Loss2: (0.0000) | Acc: (89.00%) (8174/9088)
Epoch: 85 | Batch_idx: 80 |  Loss: (0.2910) |  Loss2: (0.0000) | Acc: (89.00%) (9317/10368)
Epoch: 85 | Batch_idx: 90 |  Loss: (0.2888) |  Loss2: (0.0000) | Acc: (89.00%) (10467/11648)
Epoch: 85 | Batch_idx: 100 |  Loss: (0.2897) |  Loss2: (0.0000) | Acc: (89.00%) (11602/12928)
Epoch: 85 | Batch_idx: 110 |  Loss: (0.2919) |  Loss2: (0.0000) | Acc: (89.00%) (12742/14208)
Epoch: 85 | Batch_idx: 120 |  Loss: (0.2910) |  Loss2: (0.0000) | Acc: (89.00%) (13892/15488)
Epoch: 85 | Batch_idx: 130 |  Loss: (0.2929) |  Loss2: (0.0000) | Acc: (89.00%) (15033/16768)
Epoch: 85 | Batch_idx: 140 |  Loss: (0.2915) |  Loss2: (0.0000) | Acc: (89.00%) (16187/18048)
Epoch: 85 | Batch_idx: 150 |  Loss: (0.2907) |  Loss2: (0.0000) | Acc: (89.00%) (17346/19328)
Epoch: 85 | Batch_idx: 160 |  Loss: (0.2893) |  Loss2: (0.0000) | Acc: (89.00%) (18505/20608)
Epoch: 85 | Batch_idx: 170 |  Loss: (0.2897) |  Loss2: (0.0000) | Acc: (89.00%) (19657/21888)
Epoch: 85 | Batch_idx: 180 |  Loss: (0.2879) |  Loss2: (0.0000) | Acc: (89.00%) (20825/23168)
Epoch: 85 | Batch_idx: 190 |  Loss: (0.2885) |  Loss2: (0.0000) | Acc: (89.00%) (21971/24448)
Epoch: 85 | Batch_idx: 200 |  Loss: (0.2878) |  Loss2: (0.0000) | Acc: (89.00%) (23124/25728)
Epoch: 85 | Batch_idx: 210 |  Loss: (0.2861) |  Loss2: (0.0000) | Acc: (89.00%) (24289/27008)
Epoch: 85 | Batch_idx: 220 |  Loss: (0.2868) |  Loss2: (0.0000) | Acc: (89.00%) (25436/28288)
Epoch: 85 | Batch_idx: 230 |  Loss: (0.2859) |  Loss2: (0.0000) | Acc: (89.00%) (26589/29568)
Epoch: 85 | Batch_idx: 240 |  Loss: (0.2857) |  Loss2: (0.0000) | Acc: (89.00%) (27751/30848)
Epoch: 85 | Batch_idx: 250 |  Loss: (0.2863) |  Loss2: (0.0000) | Acc: (89.00%) (28906/32128)
Epoch: 85 | Batch_idx: 260 |  Loss: (0.2851) |  Loss2: (0.0000) | Acc: (90.00%) (30080/33408)
Epoch: 85 | Batch_idx: 270 |  Loss: (0.2846) |  Loss2: (0.0000) | Acc: (90.00%) (31237/34688)
Epoch: 85 | Batch_idx: 280 |  Loss: (0.2842) |  Loss2: (0.0000) | Acc: (90.00%) (32392/35968)
Epoch: 85 | Batch_idx: 290 |  Loss: (0.2841) |  Loss2: (0.0000) | Acc: (90.00%) (33550/37248)
Epoch: 85 | Batch_idx: 300 |  Loss: (0.2834) |  Loss2: (0.0000) | Acc: (90.00%) (34715/38528)
Epoch: 85 | Batch_idx: 310 |  Loss: (0.2834) |  Loss2: (0.0000) | Acc: (90.00%) (35872/39808)
Epoch: 85 | Batch_idx: 320 |  Loss: (0.2832) |  Loss2: (0.0000) | Acc: (90.00%) (37036/41088)
Epoch: 85 | Batch_idx: 330 |  Loss: (0.2837) |  Loss2: (0.0000) | Acc: (90.00%) (38177/42368)
Epoch: 85 | Batch_idx: 340 |  Loss: (0.2839) |  Loss2: (0.0000) | Acc: (90.00%) (39331/43648)
Epoch: 85 | Batch_idx: 350 |  Loss: (0.2841) |  Loss2: (0.0000) | Acc: (90.00%) (40479/44928)
Epoch: 85 | Batch_idx: 360 |  Loss: (0.2841) |  Loss2: (0.0000) | Acc: (90.00%) (41633/46208)
Epoch: 85 | Batch_idx: 370 |  Loss: (0.2836) |  Loss2: (0.0000) | Acc: (90.00%) (42792/47488)
Epoch: 85 | Batch_idx: 380 |  Loss: (0.2837) |  Loss2: (0.0000) | Acc: (90.00%) (43944/48768)
Epoch: 85 | Batch_idx: 390 |  Loss: (0.2833) |  Loss2: (0.0000) | Acc: (90.00%) (45066/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_085.pth.tar'
# TEST : Loss: (0.4409) | Acc: (85.00%) (8576/10000)
percent tensor([0.5680, 0.4320], device='cuda:0')
percent tensor([0.5487, 0.4513], device='cuda:0')
percent tensor([0.5330, 0.4670], device='cuda:0')
percent tensor([0.6044, 0.3956], device='cuda:0')
percent tensor([0.5705, 0.4295], device='cuda:0')
percent tensor([0.6940, 0.3060], device='cuda:0')
percent tensor([0.6618, 0.3382], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 86 | Batch_idx: 0 |  Loss: (0.2262) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 86 | Batch_idx: 10 |  Loss: (0.2648) |  Loss2: (0.0000) | Acc: (91.00%) (1282/1408)
Epoch: 86 | Batch_idx: 20 |  Loss: (0.2635) |  Loss2: (0.0000) | Acc: (90.00%) (2445/2688)
Epoch: 86 | Batch_idx: 30 |  Loss: (0.2727) |  Loss2: (0.0000) | Acc: (90.00%) (3592/3968)
Epoch: 86 | Batch_idx: 40 |  Loss: (0.2713) |  Loss2: (0.0000) | Acc: (90.00%) (4768/5248)
Epoch: 86 | Batch_idx: 50 |  Loss: (0.2734) |  Loss2: (0.0000) | Acc: (90.00%) (5920/6528)
Epoch: 86 | Batch_idx: 60 |  Loss: (0.2696) |  Loss2: (0.0000) | Acc: (90.00%) (7096/7808)
Epoch: 86 | Batch_idx: 70 |  Loss: (0.2687) |  Loss2: (0.0000) | Acc: (90.00%) (8260/9088)
Epoch: 86 | Batch_idx: 80 |  Loss: (0.2706) |  Loss2: (0.0000) | Acc: (90.00%) (9412/10368)
Epoch: 86 | Batch_idx: 90 |  Loss: (0.2680) |  Loss2: (0.0000) | Acc: (90.00%) (10584/11648)
Epoch: 86 | Batch_idx: 100 |  Loss: (0.2680) |  Loss2: (0.0000) | Acc: (90.00%) (11745/12928)
Epoch: 86 | Batch_idx: 110 |  Loss: (0.2663) |  Loss2: (0.0000) | Acc: (90.00%) (12908/14208)
Epoch: 86 | Batch_idx: 120 |  Loss: (0.2660) |  Loss2: (0.0000) | Acc: (90.00%) (14072/15488)
Epoch: 86 | Batch_idx: 130 |  Loss: (0.2656) |  Loss2: (0.0000) | Acc: (90.00%) (15236/16768)
Epoch: 86 | Batch_idx: 140 |  Loss: (0.2640) |  Loss2: (0.0000) | Acc: (90.00%) (16407/18048)
Epoch: 86 | Batch_idx: 150 |  Loss: (0.2628) |  Loss2: (0.0000) | Acc: (90.00%) (17577/19328)
Epoch: 86 | Batch_idx: 160 |  Loss: (0.2645) |  Loss2: (0.0000) | Acc: (90.00%) (18720/20608)
Epoch: 86 | Batch_idx: 170 |  Loss: (0.2665) |  Loss2: (0.0000) | Acc: (90.00%) (19873/21888)
Epoch: 86 | Batch_idx: 180 |  Loss: (0.2666) |  Loss2: (0.0000) | Acc: (90.00%) (21032/23168)
Epoch: 86 | Batch_idx: 190 |  Loss: (0.2654) |  Loss2: (0.0000) | Acc: (90.00%) (22209/24448)
Epoch: 86 | Batch_idx: 200 |  Loss: (0.2664) |  Loss2: (0.0000) | Acc: (90.00%) (23364/25728)
Epoch: 86 | Batch_idx: 210 |  Loss: (0.2667) |  Loss2: (0.0000) | Acc: (90.00%) (24522/27008)
Epoch: 86 | Batch_idx: 220 |  Loss: (0.2664) |  Loss2: (0.0000) | Acc: (90.00%) (25687/28288)
Epoch: 86 | Batch_idx: 230 |  Loss: (0.2651) |  Loss2: (0.0000) | Acc: (90.00%) (26864/29568)
Epoch: 86 | Batch_idx: 240 |  Loss: (0.2654) |  Loss2: (0.0000) | Acc: (90.00%) (28027/30848)
Epoch: 86 | Batch_idx: 250 |  Loss: (0.2653) |  Loss2: (0.0000) | Acc: (90.00%) (29192/32128)
Epoch: 86 | Batch_idx: 260 |  Loss: (0.2646) |  Loss2: (0.0000) | Acc: (90.00%) (30364/33408)
Epoch: 86 | Batch_idx: 270 |  Loss: (0.2645) |  Loss2: (0.0000) | Acc: (90.00%) (31536/34688)
Epoch: 86 | Batch_idx: 280 |  Loss: (0.2651) |  Loss2: (0.0000) | Acc: (90.00%) (32702/35968)
Epoch: 86 | Batch_idx: 290 |  Loss: (0.2640) |  Loss2: (0.0000) | Acc: (90.00%) (33879/37248)
Epoch: 86 | Batch_idx: 300 |  Loss: (0.2638) |  Loss2: (0.0000) | Acc: (90.00%) (35043/38528)
Epoch: 86 | Batch_idx: 310 |  Loss: (0.2639) |  Loss2: (0.0000) | Acc: (90.00%) (36214/39808)
Epoch: 86 | Batch_idx: 320 |  Loss: (0.2645) |  Loss2: (0.0000) | Acc: (90.00%) (37368/41088)
Epoch: 86 | Batch_idx: 330 |  Loss: (0.2632) |  Loss2: (0.0000) | Acc: (91.00%) (38560/42368)
Epoch: 86 | Batch_idx: 340 |  Loss: (0.2625) |  Loss2: (0.0000) | Acc: (91.00%) (39738/43648)
Epoch: 86 | Batch_idx: 350 |  Loss: (0.2617) |  Loss2: (0.0000) | Acc: (91.00%) (40916/44928)
Epoch: 86 | Batch_idx: 360 |  Loss: (0.2612) |  Loss2: (0.0000) | Acc: (91.00%) (42084/46208)
Epoch: 86 | Batch_idx: 370 |  Loss: (0.2613) |  Loss2: (0.0000) | Acc: (91.00%) (43252/47488)
Epoch: 86 | Batch_idx: 380 |  Loss: (0.2605) |  Loss2: (0.0000) | Acc: (91.00%) (44430/48768)
Epoch: 86 | Batch_idx: 390 |  Loss: (0.2601) |  Loss2: (0.0000) | Acc: (91.00%) (45557/50000)
# TEST : Loss: (0.4189) | Acc: (86.00%) (8642/10000)
percent tensor([0.5698, 0.4302], device='cuda:0')
percent tensor([0.5499, 0.4501], device='cuda:0')
percent tensor([0.5353, 0.4647], device='cuda:0')
percent tensor([0.6111, 0.3889], device='cuda:0')
percent tensor([0.5777, 0.4223], device='cuda:0')
percent tensor([0.6973, 0.3027], device='cuda:0')
percent tensor([0.6631, 0.3369], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 87 | Batch_idx: 0 |  Loss: (0.2589) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 87 | Batch_idx: 10 |  Loss: (0.2216) |  Loss2: (0.0000) | Acc: (92.00%) (1301/1408)
Epoch: 87 | Batch_idx: 20 |  Loss: (0.2312) |  Loss2: (0.0000) | Acc: (92.00%) (2482/2688)
Epoch: 87 | Batch_idx: 30 |  Loss: (0.2499) |  Loss2: (0.0000) | Acc: (91.00%) (3635/3968)
Epoch: 87 | Batch_idx: 40 |  Loss: (0.2476) |  Loss2: (0.0000) | Acc: (91.00%) (4806/5248)
Epoch: 87 | Batch_idx: 50 |  Loss: (0.2471) |  Loss2: (0.0000) | Acc: (91.00%) (5980/6528)
Epoch: 87 | Batch_idx: 60 |  Loss: (0.2499) |  Loss2: (0.0000) | Acc: (91.00%) (7142/7808)
Epoch: 87 | Batch_idx: 70 |  Loss: (0.2504) |  Loss2: (0.0000) | Acc: (91.00%) (8311/9088)
Epoch: 87 | Batch_idx: 80 |  Loss: (0.2497) |  Loss2: (0.0000) | Acc: (91.00%) (9491/10368)
Epoch: 87 | Batch_idx: 90 |  Loss: (0.2489) |  Loss2: (0.0000) | Acc: (91.00%) (10658/11648)
Epoch: 87 | Batch_idx: 100 |  Loss: (0.2494) |  Loss2: (0.0000) | Acc: (91.00%) (11824/12928)
Epoch: 87 | Batch_idx: 110 |  Loss: (0.2508) |  Loss2: (0.0000) | Acc: (91.00%) (13003/14208)
Epoch: 87 | Batch_idx: 120 |  Loss: (0.2509) |  Loss2: (0.0000) | Acc: (91.00%) (14170/15488)
Epoch: 87 | Batch_idx: 130 |  Loss: (0.2505) |  Loss2: (0.0000) | Acc: (91.00%) (15340/16768)
Epoch: 87 | Batch_idx: 140 |  Loss: (0.2490) |  Loss2: (0.0000) | Acc: (91.00%) (16523/18048)
Epoch: 87 | Batch_idx: 150 |  Loss: (0.2508) |  Loss2: (0.0000) | Acc: (91.00%) (17679/19328)
Epoch: 87 | Batch_idx: 160 |  Loss: (0.2530) |  Loss2: (0.0000) | Acc: (91.00%) (18831/20608)
Epoch: 87 | Batch_idx: 170 |  Loss: (0.2543) |  Loss2: (0.0000) | Acc: (91.00%) (20001/21888)
Epoch: 87 | Batch_idx: 180 |  Loss: (0.2536) |  Loss2: (0.0000) | Acc: (91.00%) (21172/23168)
Epoch: 87 | Batch_idx: 190 |  Loss: (0.2552) |  Loss2: (0.0000) | Acc: (91.00%) (22337/24448)
Epoch: 87 | Batch_idx: 200 |  Loss: (0.2551) |  Loss2: (0.0000) | Acc: (91.00%) (23498/25728)
Epoch: 87 | Batch_idx: 210 |  Loss: (0.2546) |  Loss2: (0.0000) | Acc: (91.00%) (24664/27008)
Epoch: 87 | Batch_idx: 220 |  Loss: (0.2561) |  Loss2: (0.0000) | Acc: (91.00%) (25804/28288)
Epoch: 87 | Batch_idx: 230 |  Loss: (0.2555) |  Loss2: (0.0000) | Acc: (91.00%) (26978/29568)
Epoch: 87 | Batch_idx: 240 |  Loss: (0.2566) |  Loss2: (0.0000) | Acc: (91.00%) (28134/30848)
Epoch: 87 | Batch_idx: 250 |  Loss: (0.2566) |  Loss2: (0.0000) | Acc: (91.00%) (29298/32128)
Epoch: 87 | Batch_idx: 260 |  Loss: (0.2563) |  Loss2: (0.0000) | Acc: (91.00%) (30466/33408)
Epoch: 87 | Batch_idx: 270 |  Loss: (0.2561) |  Loss2: (0.0000) | Acc: (91.00%) (31643/34688)
Epoch: 87 | Batch_idx: 280 |  Loss: (0.2572) |  Loss2: (0.0000) | Acc: (91.00%) (32804/35968)
Epoch: 87 | Batch_idx: 290 |  Loss: (0.2568) |  Loss2: (0.0000) | Acc: (91.00%) (33982/37248)
Epoch: 87 | Batch_idx: 300 |  Loss: (0.2563) |  Loss2: (0.0000) | Acc: (91.00%) (35154/38528)
Epoch: 87 | Batch_idx: 310 |  Loss: (0.2562) |  Loss2: (0.0000) | Acc: (91.00%) (36325/39808)
Epoch: 87 | Batch_idx: 320 |  Loss: (0.2560) |  Loss2: (0.0000) | Acc: (91.00%) (37491/41088)
Epoch: 87 | Batch_idx: 330 |  Loss: (0.2550) |  Loss2: (0.0000) | Acc: (91.00%) (38676/42368)
Epoch: 87 | Batch_idx: 340 |  Loss: (0.2543) |  Loss2: (0.0000) | Acc: (91.00%) (39865/43648)
Epoch: 87 | Batch_idx: 350 |  Loss: (0.2534) |  Loss2: (0.0000) | Acc: (91.00%) (41040/44928)
Epoch: 87 | Batch_idx: 360 |  Loss: (0.2528) |  Loss2: (0.0000) | Acc: (91.00%) (42223/46208)
Epoch: 87 | Batch_idx: 370 |  Loss: (0.2527) |  Loss2: (0.0000) | Acc: (91.00%) (43395/47488)
Epoch: 87 | Batch_idx: 380 |  Loss: (0.2526) |  Loss2: (0.0000) | Acc: (91.00%) (44571/48768)
Epoch: 87 | Batch_idx: 390 |  Loss: (0.2522) |  Loss2: (0.0000) | Acc: (91.00%) (45707/50000)
# TEST : Loss: (0.4114) | Acc: (86.00%) (8677/10000)
percent tensor([0.5681, 0.4319], device='cuda:0')
percent tensor([0.5481, 0.4519], device='cuda:0')
percent tensor([0.5361, 0.4639], device='cuda:0')
percent tensor([0.6103, 0.3897], device='cuda:0')
percent tensor([0.5788, 0.4212], device='cuda:0')
percent tensor([0.6988, 0.3012], device='cuda:0')
percent tensor([0.6743, 0.3257], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 88 | Batch_idx: 0 |  Loss: (0.2216) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 88 | Batch_idx: 10 |  Loss: (0.2444) |  Loss2: (0.0000) | Acc: (91.00%) (1292/1408)
Epoch: 88 | Batch_idx: 20 |  Loss: (0.2547) |  Loss2: (0.0000) | Acc: (91.00%) (2447/2688)
Epoch: 88 | Batch_idx: 30 |  Loss: (0.2685) |  Loss2: (0.0000) | Acc: (90.00%) (3593/3968)
Epoch: 88 | Batch_idx: 40 |  Loss: (0.2621) |  Loss2: (0.0000) | Acc: (90.00%) (4765/5248)
Epoch: 88 | Batch_idx: 50 |  Loss: (0.2560) |  Loss2: (0.0000) | Acc: (91.00%) (5945/6528)
Epoch: 88 | Batch_idx: 60 |  Loss: (0.2504) |  Loss2: (0.0000) | Acc: (91.00%) (7130/7808)
Epoch: 88 | Batch_idx: 70 |  Loss: (0.2479) |  Loss2: (0.0000) | Acc: (91.00%) (8310/9088)
Epoch: 88 | Batch_idx: 80 |  Loss: (0.2452) |  Loss2: (0.0000) | Acc: (91.00%) (9486/10368)
Epoch: 88 | Batch_idx: 90 |  Loss: (0.2454) |  Loss2: (0.0000) | Acc: (91.00%) (10658/11648)
Epoch: 88 | Batch_idx: 100 |  Loss: (0.2497) |  Loss2: (0.0000) | Acc: (91.00%) (11812/12928)
Epoch: 88 | Batch_idx: 110 |  Loss: (0.2506) |  Loss2: (0.0000) | Acc: (91.00%) (12983/14208)
Epoch: 88 | Batch_idx: 120 |  Loss: (0.2497) |  Loss2: (0.0000) | Acc: (91.00%) (14154/15488)
Epoch: 88 | Batch_idx: 130 |  Loss: (0.2496) |  Loss2: (0.0000) | Acc: (91.00%) (15326/16768)
Epoch: 88 | Batch_idx: 140 |  Loss: (0.2479) |  Loss2: (0.0000) | Acc: (91.00%) (16500/18048)
Epoch: 88 | Batch_idx: 150 |  Loss: (0.2463) |  Loss2: (0.0000) | Acc: (91.00%) (17687/19328)
Epoch: 88 | Batch_idx: 160 |  Loss: (0.2458) |  Loss2: (0.0000) | Acc: (91.00%) (18854/20608)
Epoch: 88 | Batch_idx: 170 |  Loss: (0.2461) |  Loss2: (0.0000) | Acc: (91.00%) (20023/21888)
Epoch: 88 | Batch_idx: 180 |  Loss: (0.2467) |  Loss2: (0.0000) | Acc: (91.00%) (21195/23168)
Epoch: 88 | Batch_idx: 190 |  Loss: (0.2478) |  Loss2: (0.0000) | Acc: (91.00%) (22345/24448)
Epoch: 88 | Batch_idx: 200 |  Loss: (0.2467) |  Loss2: (0.0000) | Acc: (91.00%) (23521/25728)
Epoch: 88 | Batch_idx: 210 |  Loss: (0.2477) |  Loss2: (0.0000) | Acc: (91.00%) (24683/27008)
Epoch: 88 | Batch_idx: 220 |  Loss: (0.2486) |  Loss2: (0.0000) | Acc: (91.00%) (25850/28288)
Epoch: 88 | Batch_idx: 230 |  Loss: (0.2470) |  Loss2: (0.0000) | Acc: (91.00%) (27032/29568)
Epoch: 88 | Batch_idx: 240 |  Loss: (0.2471) |  Loss2: (0.0000) | Acc: (91.00%) (28198/30848)
Epoch: 88 | Batch_idx: 250 |  Loss: (0.2467) |  Loss2: (0.0000) | Acc: (91.00%) (29372/32128)
Epoch: 88 | Batch_idx: 260 |  Loss: (0.2460) |  Loss2: (0.0000) | Acc: (91.00%) (30566/33408)
Epoch: 88 | Batch_idx: 270 |  Loss: (0.2460) |  Loss2: (0.0000) | Acc: (91.00%) (31740/34688)
Epoch: 88 | Batch_idx: 280 |  Loss: (0.2463) |  Loss2: (0.0000) | Acc: (91.00%) (32910/35968)
Epoch: 88 | Batch_idx: 290 |  Loss: (0.2449) |  Loss2: (0.0000) | Acc: (91.00%) (34103/37248)
Epoch: 88 | Batch_idx: 300 |  Loss: (0.2448) |  Loss2: (0.0000) | Acc: (91.00%) (35283/38528)
Epoch: 88 | Batch_idx: 310 |  Loss: (0.2444) |  Loss2: (0.0000) | Acc: (91.00%) (36461/39808)
Epoch: 88 | Batch_idx: 320 |  Loss: (0.2439) |  Loss2: (0.0000) | Acc: (91.00%) (37642/41088)
Epoch: 88 | Batch_idx: 330 |  Loss: (0.2443) |  Loss2: (0.0000) | Acc: (91.00%) (38807/42368)
Epoch: 88 | Batch_idx: 340 |  Loss: (0.2448) |  Loss2: (0.0000) | Acc: (91.00%) (39975/43648)
Epoch: 88 | Batch_idx: 350 |  Loss: (0.2438) |  Loss2: (0.0000) | Acc: (91.00%) (41171/44928)
Epoch: 88 | Batch_idx: 360 |  Loss: (0.2435) |  Loss2: (0.0000) | Acc: (91.00%) (42343/46208)
Epoch: 88 | Batch_idx: 370 |  Loss: (0.2425) |  Loss2: (0.0000) | Acc: (91.00%) (43538/47488)
Epoch: 88 | Batch_idx: 380 |  Loss: (0.2421) |  Loss2: (0.0000) | Acc: (91.00%) (44721/48768)
Epoch: 88 | Batch_idx: 390 |  Loss: (0.2417) |  Loss2: (0.0000) | Acc: (91.00%) (45857/50000)
# TEST : Loss: (0.4067) | Acc: (86.00%) (8683/10000)
percent tensor([0.5694, 0.4306], device='cuda:0')
percent tensor([0.5486, 0.4514], device='cuda:0')
percent tensor([0.5349, 0.4651], device='cuda:0')
percent tensor([0.6099, 0.3901], device='cuda:0')
percent tensor([0.5791, 0.4209], device='cuda:0')
percent tensor([0.7042, 0.2958], device='cuda:0')
percent tensor([0.6795, 0.3205], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Epoch: 89 | Batch_idx: 0 |  Loss: (0.1711) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 89 | Batch_idx: 10 |  Loss: (0.2309) |  Loss2: (0.0000) | Acc: (91.00%) (1294/1408)
Epoch: 89 | Batch_idx: 20 |  Loss: (0.2346) |  Loss2: (0.0000) | Acc: (91.00%) (2461/2688)
Epoch: 89 | Batch_idx: 30 |  Loss: (0.2328) |  Loss2: (0.0000) | Acc: (91.00%) (3646/3968)
Epoch: 89 | Batch_idx: 40 |  Loss: (0.2415) |  Loss2: (0.0000) | Acc: (91.00%) (4803/5248)
Epoch: 89 | Batch_idx: 50 |  Loss: (0.2432) |  Loss2: (0.0000) | Acc: (91.00%) (5963/6528)
Epoch: 89 | Batch_idx: 60 |  Loss: (0.2470) |  Loss2: (0.0000) | Acc: (91.00%) (7118/7808)
Epoch: 89 | Batch_idx: 70 |  Loss: (0.2453) |  Loss2: (0.0000) | Acc: (91.00%) (8286/9088)
Epoch: 89 | Batch_idx: 80 |  Loss: (0.2455) |  Loss2: (0.0000) | Acc: (91.00%) (9463/10368)
Epoch: 89 | Batch_idx: 90 |  Loss: (0.2457) |  Loss2: (0.0000) | Acc: (91.00%) (10633/11648)
Epoch: 89 | Batch_idx: 100 |  Loss: (0.2417) |  Loss2: (0.0000) | Acc: (91.00%) (11823/12928)
Epoch: 89 | Batch_idx: 110 |  Loss: (0.2451) |  Loss2: (0.0000) | Acc: (91.00%) (12976/14208)
Epoch: 89 | Batch_idx: 120 |  Loss: (0.2433) |  Loss2: (0.0000) | Acc: (91.00%) (14159/15488)
Epoch: 89 | Batch_idx: 130 |  Loss: (0.2435) |  Loss2: (0.0000) | Acc: (91.00%) (15332/16768)
Epoch: 89 | Batch_idx: 140 |  Loss: (0.2418) |  Loss2: (0.0000) | Acc: (91.00%) (16515/18048)
Epoch: 89 | Batch_idx: 150 |  Loss: (0.2436) |  Loss2: (0.0000) | Acc: (91.00%) (17674/19328)
Epoch: 89 | Batch_idx: 160 |  Loss: (0.2426) |  Loss2: (0.0000) | Acc: (91.00%) (18860/20608)
Epoch: 89 | Batch_idx: 170 |  Loss: (0.2424) |  Loss2: (0.0000) | Acc: (91.00%) (20032/21888)
Epoch: 89 | Batch_idx: 180 |  Loss: (0.2425) |  Loss2: (0.0000) | Acc: (91.00%) (21206/23168)
Epoch: 89 | Batch_idx: 190 |  Loss: (0.2431) |  Loss2: (0.0000) | Acc: (91.00%) (22375/24448)
Epoch: 89 | Batch_idx: 200 |  Loss: (0.2426) |  Loss2: (0.0000) | Acc: (91.00%) (23547/25728)
Epoch: 89 | Batch_idx: 210 |  Loss: (0.2434) |  Loss2: (0.0000) | Acc: (91.00%) (24719/27008)
Epoch: 89 | Batch_idx: 220 |  Loss: (0.2424) |  Loss2: (0.0000) | Acc: (91.00%) (25894/28288)
Epoch: 89 | Batch_idx: 230 |  Loss: (0.2438) |  Loss2: (0.0000) | Acc: (91.00%) (27041/29568)
Epoch: 89 | Batch_idx: 240 |  Loss: (0.2438) |  Loss2: (0.0000) | Acc: (91.00%) (28219/30848)
Epoch: 89 | Batch_idx: 250 |  Loss: (0.2438) |  Loss2: (0.0000) | Acc: (91.00%) (29391/32128)
Epoch: 89 | Batch_idx: 260 |  Loss: (0.2443) |  Loss2: (0.0000) | Acc: (91.00%) (30557/33408)
Epoch: 89 | Batch_idx: 270 |  Loss: (0.2440) |  Loss2: (0.0000) | Acc: (91.00%) (31739/34688)
Epoch: 89 | Batch_idx: 280 |  Loss: (0.2446) |  Loss2: (0.0000) | Acc: (91.00%) (32913/35968)
Epoch: 89 | Batch_idx: 290 |  Loss: (0.2439) |  Loss2: (0.0000) | Acc: (91.00%) (34100/37248)
Epoch: 89 | Batch_idx: 300 |  Loss: (0.2439) |  Loss2: (0.0000) | Acc: (91.00%) (35270/38528)
Epoch: 89 | Batch_idx: 310 |  Loss: (0.2443) |  Loss2: (0.0000) | Acc: (91.00%) (36441/39808)
Epoch: 89 | Batch_idx: 320 |  Loss: (0.2433) |  Loss2: (0.0000) | Acc: (91.00%) (37632/41088)
Epoch: 89 | Batch_idx: 330 |  Loss: (0.2419) |  Loss2: (0.0000) | Acc: (91.00%) (38832/42368)
Epoch: 89 | Batch_idx: 340 |  Loss: (0.2416) |  Loss2: (0.0000) | Acc: (91.00%) (40017/43648)
Epoch: 89 | Batch_idx: 350 |  Loss: (0.2420) |  Loss2: (0.0000) | Acc: (91.00%) (41177/44928)
Epoch: 89 | Batch_idx: 360 |  Loss: (0.2416) |  Loss2: (0.0000) | Acc: (91.00%) (42361/46208)
Epoch: 89 | Batch_idx: 370 |  Loss: (0.2414) |  Loss2: (0.0000) | Acc: (91.00%) (43538/47488)
Epoch: 89 | Batch_idx: 380 |  Loss: (0.2418) |  Loss2: (0.0000) | Acc: (91.00%) (44713/48768)
Epoch: 89 | Batch_idx: 390 |  Loss: (0.2415) |  Loss2: (0.0000) | Acc: (91.00%) (45847/50000)
# TEST : Loss: (0.4027) | Acc: (86.00%) (8687/10000)
percent tensor([0.5744, 0.4256], device='cuda:0')
percent tensor([0.5488, 0.4512], device='cuda:0')
percent tensor([0.5345, 0.4655], device='cuda:0')
percent tensor([0.6118, 0.3882], device='cuda:0')
percent tensor([0.5826, 0.4174], device='cuda:0')
percent tensor([0.7072, 0.2928], device='cuda:0')
percent tensor([0.6846, 0.3154], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 90 | Batch_idx: 0 |  Loss: (0.1886) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 90 | Batch_idx: 10 |  Loss: (0.2622) |  Loss2: (0.0000) | Acc: (91.00%) (1287/1408)
Epoch: 90 | Batch_idx: 20 |  Loss: (0.2498) |  Loss2: (0.0000) | Acc: (91.00%) (2468/2688)
Epoch: 90 | Batch_idx: 30 |  Loss: (0.2497) |  Loss2: (0.0000) | Acc: (91.00%) (3642/3968)
Epoch: 90 | Batch_idx: 40 |  Loss: (0.2465) |  Loss2: (0.0000) | Acc: (91.00%) (4823/5248)
Epoch: 90 | Batch_idx: 50 |  Loss: (0.2464) |  Loss2: (0.0000) | Acc: (91.00%) (5993/6528)
Epoch: 90 | Batch_idx: 60 |  Loss: (0.2463) |  Loss2: (0.0000) | Acc: (91.00%) (7157/7808)
Epoch: 90 | Batch_idx: 70 |  Loss: (0.2450) |  Loss2: (0.0000) | Acc: (91.00%) (8329/9088)
Epoch: 90 | Batch_idx: 80 |  Loss: (0.2462) |  Loss2: (0.0000) | Acc: (91.00%) (9487/10368)
Epoch: 90 | Batch_idx: 90 |  Loss: (0.2465) |  Loss2: (0.0000) | Acc: (91.00%) (10658/11648)
Epoch: 90 | Batch_idx: 100 |  Loss: (0.2473) |  Loss2: (0.0000) | Acc: (91.00%) (11836/12928)
Epoch: 90 | Batch_idx: 110 |  Loss: (0.2472) |  Loss2: (0.0000) | Acc: (91.00%) (13009/14208)
Epoch: 90 | Batch_idx: 120 |  Loss: (0.2461) |  Loss2: (0.0000) | Acc: (91.00%) (14182/15488)
Epoch: 90 | Batch_idx: 130 |  Loss: (0.2449) |  Loss2: (0.0000) | Acc: (91.00%) (15366/16768)
Epoch: 90 | Batch_idx: 140 |  Loss: (0.2443) |  Loss2: (0.0000) | Acc: (91.00%) (16542/18048)
Epoch: 90 | Batch_idx: 150 |  Loss: (0.2439) |  Loss2: (0.0000) | Acc: (91.00%) (17711/19328)
Epoch: 90 | Batch_idx: 160 |  Loss: (0.2462) |  Loss2: (0.0000) | Acc: (91.00%) (18859/20608)
Epoch: 90 | Batch_idx: 170 |  Loss: (0.2464) |  Loss2: (0.0000) | Acc: (91.00%) (20039/21888)
Epoch: 90 | Batch_idx: 180 |  Loss: (0.2462) |  Loss2: (0.0000) | Acc: (91.00%) (21212/23168)
Epoch: 90 | Batch_idx: 190 |  Loss: (0.2454) |  Loss2: (0.0000) | Acc: (91.00%) (22395/24448)
Epoch: 90 | Batch_idx: 200 |  Loss: (0.2467) |  Loss2: (0.0000) | Acc: (91.00%) (23552/25728)
Epoch: 90 | Batch_idx: 210 |  Loss: (0.2454) |  Loss2: (0.0000) | Acc: (91.00%) (24741/27008)
Epoch: 90 | Batch_idx: 220 |  Loss: (0.2458) |  Loss2: (0.0000) | Acc: (91.00%) (25896/28288)
Epoch: 90 | Batch_idx: 230 |  Loss: (0.2461) |  Loss2: (0.0000) | Acc: (91.00%) (27066/29568)
Epoch: 90 | Batch_idx: 240 |  Loss: (0.2448) |  Loss2: (0.0000) | Acc: (91.00%) (28257/30848)
Epoch: 90 | Batch_idx: 250 |  Loss: (0.2457) |  Loss2: (0.0000) | Acc: (91.00%) (29412/32128)
Epoch: 90 | Batch_idx: 260 |  Loss: (0.2450) |  Loss2: (0.0000) | Acc: (91.00%) (30587/33408)
Epoch: 90 | Batch_idx: 270 |  Loss: (0.2447) |  Loss2: (0.0000) | Acc: (91.00%) (31767/34688)
Epoch: 90 | Batch_idx: 280 |  Loss: (0.2451) |  Loss2: (0.0000) | Acc: (91.00%) (32940/35968)
Epoch: 90 | Batch_idx: 290 |  Loss: (0.2458) |  Loss2: (0.0000) | Acc: (91.00%) (34105/37248)
Epoch: 90 | Batch_idx: 300 |  Loss: (0.2452) |  Loss2: (0.0000) | Acc: (91.00%) (35285/38528)
Epoch: 90 | Batch_idx: 310 |  Loss: (0.2455) |  Loss2: (0.0000) | Acc: (91.00%) (36446/39808)
Epoch: 90 | Batch_idx: 320 |  Loss: (0.2451) |  Loss2: (0.0000) | Acc: (91.00%) (37625/41088)
Epoch: 90 | Batch_idx: 330 |  Loss: (0.2452) |  Loss2: (0.0000) | Acc: (91.00%) (38798/42368)
Epoch: 90 | Batch_idx: 340 |  Loss: (0.2459) |  Loss2: (0.0000) | Acc: (91.00%) (39963/43648)
Epoch: 90 | Batch_idx: 350 |  Loss: (0.2474) |  Loss2: (0.0000) | Acc: (91.00%) (41096/44928)
Epoch: 90 | Batch_idx: 360 |  Loss: (0.2474) |  Loss2: (0.0000) | Acc: (91.00%) (42268/46208)
Epoch: 90 | Batch_idx: 370 |  Loss: (0.2481) |  Loss2: (0.0000) | Acc: (91.00%) (43430/47488)
Epoch: 90 | Batch_idx: 380 |  Loss: (0.2487) |  Loss2: (0.0000) | Acc: (91.00%) (44583/48768)
Epoch: 90 | Batch_idx: 390 |  Loss: (0.2485) |  Loss2: (0.0000) | Acc: (91.00%) (45707/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_090.pth.tar'
# TEST : Loss: (0.4854) | Acc: (84.00%) (8496/10000)
percent tensor([0.5744, 0.4256], device='cuda:0')
percent tensor([0.5490, 0.4510], device='cuda:0')
percent tensor([0.5347, 0.4653], device='cuda:0')
percent tensor([0.6115, 0.3885], device='cuda:0')
percent tensor([0.5838, 0.4162], device='cuda:0')
percent tensor([0.7088, 0.2912], device='cuda:0')
percent tensor([0.6899, 0.3101], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(178.5706, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(811.8528, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(808.0466, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1524.9700, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(495.3773, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2236.8115, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4284.9639, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1398.5808, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6142.2002, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11906.8779, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3946.8782, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16707.3066, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 91 | Batch_idx: 0 |  Loss: (0.2908) |  Loss2: (0.0000) | Acc: (85.00%) (110/128)
Epoch: 91 | Batch_idx: 10 |  Loss: (0.2391) |  Loss2: (0.0000) | Acc: (91.00%) (1293/1408)
Epoch: 91 | Batch_idx: 20 |  Loss: (0.2449) |  Loss2: (0.0000) | Acc: (91.00%) (2459/2688)
Epoch: 91 | Batch_idx: 30 |  Loss: (0.2397) |  Loss2: (0.0000) | Acc: (91.00%) (3631/3968)
Epoch: 91 | Batch_idx: 40 |  Loss: (0.2353) |  Loss2: (0.0000) | Acc: (91.00%) (4817/5248)
Epoch: 91 | Batch_idx: 50 |  Loss: (0.2342) |  Loss2: (0.0000) | Acc: (91.00%) (6000/6528)
Epoch: 91 | Batch_idx: 60 |  Loss: (0.2318) |  Loss2: (0.0000) | Acc: (92.00%) (7188/7808)
Epoch: 91 | Batch_idx: 70 |  Loss: (0.2390) |  Loss2: (0.0000) | Acc: (91.00%) (8355/9088)
Epoch: 91 | Batch_idx: 80 |  Loss: (0.2396) |  Loss2: (0.0000) | Acc: (91.00%) (9530/10368)
Epoch: 91 | Batch_idx: 90 |  Loss: (0.2410) |  Loss2: (0.0000) | Acc: (91.00%) (10691/11648)
Epoch: 91 | Batch_idx: 100 |  Loss: (0.2423) |  Loss2: (0.0000) | Acc: (91.00%) (11854/12928)
Epoch: 91 | Batch_idx: 110 |  Loss: (0.2412) |  Loss2: (0.0000) | Acc: (91.00%) (13038/14208)
Epoch: 91 | Batch_idx: 120 |  Loss: (0.2396) |  Loss2: (0.0000) | Acc: (91.00%) (14218/15488)
Epoch: 91 | Batch_idx: 130 |  Loss: (0.2404) |  Loss2: (0.0000) | Acc: (91.00%) (15386/16768)
Epoch: 91 | Batch_idx: 140 |  Loss: (0.2428) |  Loss2: (0.0000) | Acc: (91.00%) (16535/18048)
Epoch: 91 | Batch_idx: 150 |  Loss: (0.2420) |  Loss2: (0.0000) | Acc: (91.00%) (17716/19328)
Epoch: 91 | Batch_idx: 160 |  Loss: (0.2411) |  Loss2: (0.0000) | Acc: (91.00%) (18897/20608)
Epoch: 91 | Batch_idx: 170 |  Loss: (0.2413) |  Loss2: (0.0000) | Acc: (91.00%) (20066/21888)
Epoch: 91 | Batch_idx: 180 |  Loss: (0.2424) |  Loss2: (0.0000) | Acc: (91.00%) (21227/23168)
Epoch: 91 | Batch_idx: 190 |  Loss: (0.2427) |  Loss2: (0.0000) | Acc: (91.00%) (22396/24448)
Epoch: 91 | Batch_idx: 200 |  Loss: (0.2424) |  Loss2: (0.0000) | Acc: (91.00%) (23574/25728)
Epoch: 91 | Batch_idx: 210 |  Loss: (0.2427) |  Loss2: (0.0000) | Acc: (91.00%) (24736/27008)
Epoch: 91 | Batch_idx: 220 |  Loss: (0.2417) |  Loss2: (0.0000) | Acc: (91.00%) (25916/28288)
Epoch: 91 | Batch_idx: 230 |  Loss: (0.2424) |  Loss2: (0.0000) | Acc: (91.00%) (27087/29568)
Epoch: 91 | Batch_idx: 240 |  Loss: (0.2423) |  Loss2: (0.0000) | Acc: (91.00%) (28258/30848)
Epoch: 91 | Batch_idx: 250 |  Loss: (0.2428) |  Loss2: (0.0000) | Acc: (91.00%) (29429/32128)
Epoch: 91 | Batch_idx: 260 |  Loss: (0.2422) |  Loss2: (0.0000) | Acc: (91.00%) (30603/33408)
Epoch: 91 | Batch_idx: 270 |  Loss: (0.2420) |  Loss2: (0.0000) | Acc: (91.00%) (31781/34688)
Epoch: 91 | Batch_idx: 280 |  Loss: (0.2419) |  Loss2: (0.0000) | Acc: (91.00%) (32964/35968)
Epoch: 91 | Batch_idx: 290 |  Loss: (0.2422) |  Loss2: (0.0000) | Acc: (91.00%) (34140/37248)
Epoch: 91 | Batch_idx: 300 |  Loss: (0.2436) |  Loss2: (0.0000) | Acc: (91.00%) (35299/38528)
Epoch: 91 | Batch_idx: 310 |  Loss: (0.2435) |  Loss2: (0.0000) | Acc: (91.00%) (36471/39808)
Epoch: 91 | Batch_idx: 320 |  Loss: (0.2435) |  Loss2: (0.0000) | Acc: (91.00%) (37644/41088)
Epoch: 91 | Batch_idx: 330 |  Loss: (0.2434) |  Loss2: (0.0000) | Acc: (91.00%) (38810/42368)
Epoch: 91 | Batch_idx: 340 |  Loss: (0.2433) |  Loss2: (0.0000) | Acc: (91.00%) (39984/43648)
Epoch: 91 | Batch_idx: 350 |  Loss: (0.2442) |  Loss2: (0.0000) | Acc: (91.00%) (41152/44928)
Epoch: 91 | Batch_idx: 360 |  Loss: (0.2440) |  Loss2: (0.0000) | Acc: (91.00%) (42320/46208)
Epoch: 91 | Batch_idx: 370 |  Loss: (0.2449) |  Loss2: (0.0000) | Acc: (91.00%) (43476/47488)
Epoch: 91 | Batch_idx: 380 |  Loss: (0.2444) |  Loss2: (0.0000) | Acc: (91.00%) (44645/48768)
Epoch: 91 | Batch_idx: 390 |  Loss: (0.2444) |  Loss2: (0.0000) | Acc: (91.00%) (45774/50000)
# TEST : Loss: (0.5171) | Acc: (84.00%) (8447/10000)
percent tensor([0.5746, 0.4254], device='cuda:0')
percent tensor([0.5496, 0.4504], device='cuda:0')
percent tensor([0.5350, 0.4650], device='cuda:0')
percent tensor([0.6107, 0.3893], device='cuda:0')
percent tensor([0.5825, 0.4175], device='cuda:0')
percent tensor([0.7065, 0.2935], device='cuda:0')
percent tensor([0.6865, 0.3135], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Epoch: 92 | Batch_idx: 0 |  Loss: (0.1369) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 92 | Batch_idx: 10 |  Loss: (0.2070) |  Loss2: (0.0000) | Acc: (92.00%) (1304/1408)
Epoch: 92 | Batch_idx: 20 |  Loss: (0.2125) |  Loss2: (0.0000) | Acc: (92.00%) (2488/2688)
Epoch: 92 | Batch_idx: 30 |  Loss: (0.2203) |  Loss2: (0.0000) | Acc: (92.00%) (3660/3968)
Epoch: 92 | Batch_idx: 40 |  Loss: (0.2260) |  Loss2: (0.0000) | Acc: (92.00%) (4835/5248)
Epoch: 92 | Batch_idx: 50 |  Loss: (0.2321) |  Loss2: (0.0000) | Acc: (91.00%) (6000/6528)
Epoch: 92 | Batch_idx: 60 |  Loss: (0.2339) |  Loss2: (0.0000) | Acc: (91.00%) (7176/7808)
Epoch: 92 | Batch_idx: 70 |  Loss: (0.2328) |  Loss2: (0.0000) | Acc: (91.00%) (8351/9088)
Epoch: 92 | Batch_idx: 80 |  Loss: (0.2337) |  Loss2: (0.0000) | Acc: (91.00%) (9532/10368)
Epoch: 92 | Batch_idx: 90 |  Loss: (0.2364) |  Loss2: (0.0000) | Acc: (91.00%) (10700/11648)
Epoch: 92 | Batch_idx: 100 |  Loss: (0.2362) |  Loss2: (0.0000) | Acc: (91.00%) (11871/12928)
Epoch: 92 | Batch_idx: 110 |  Loss: (0.2388) |  Loss2: (0.0000) | Acc: (91.00%) (13031/14208)
Epoch: 92 | Batch_idx: 120 |  Loss: (0.2370) |  Loss2: (0.0000) | Acc: (91.00%) (14223/15488)
Epoch: 92 | Batch_idx: 130 |  Loss: (0.2342) |  Loss2: (0.0000) | Acc: (92.00%) (15428/16768)
Epoch: 92 | Batch_idx: 140 |  Loss: (0.2350) |  Loss2: (0.0000) | Acc: (91.00%) (16601/18048)
Epoch: 92 | Batch_idx: 150 |  Loss: (0.2372) |  Loss2: (0.0000) | Acc: (91.00%) (17768/19328)
Epoch: 92 | Batch_idx: 160 |  Loss: (0.2355) |  Loss2: (0.0000) | Acc: (91.00%) (18951/20608)
Epoch: 92 | Batch_idx: 170 |  Loss: (0.2359) |  Loss2: (0.0000) | Acc: (91.00%) (20131/21888)
Epoch: 92 | Batch_idx: 180 |  Loss: (0.2376) |  Loss2: (0.0000) | Acc: (91.00%) (21293/23168)
Epoch: 92 | Batch_idx: 190 |  Loss: (0.2365) |  Loss2: (0.0000) | Acc: (91.00%) (22479/24448)
Epoch: 92 | Batch_idx: 200 |  Loss: (0.2356) |  Loss2: (0.0000) | Acc: (92.00%) (23673/25728)
Epoch: 92 | Batch_idx: 210 |  Loss: (0.2357) |  Loss2: (0.0000) | Acc: (92.00%) (24858/27008)
Epoch: 92 | Batch_idx: 220 |  Loss: (0.2370) |  Loss2: (0.0000) | Acc: (92.00%) (26028/28288)
Epoch: 92 | Batch_idx: 230 |  Loss: (0.2373) |  Loss2: (0.0000) | Acc: (92.00%) (27207/29568)
Epoch: 92 | Batch_idx: 240 |  Loss: (0.2372) |  Loss2: (0.0000) | Acc: (91.00%) (28374/30848)
Epoch: 92 | Batch_idx: 250 |  Loss: (0.2368) |  Loss2: (0.0000) | Acc: (91.00%) (29552/32128)
Epoch: 92 | Batch_idx: 260 |  Loss: (0.2363) |  Loss2: (0.0000) | Acc: (91.00%) (30733/33408)
Epoch: 92 | Batch_idx: 270 |  Loss: (0.2371) |  Loss2: (0.0000) | Acc: (91.00%) (31897/34688)
Epoch: 92 | Batch_idx: 280 |  Loss: (0.2370) |  Loss2: (0.0000) | Acc: (91.00%) (33057/35968)
Epoch: 92 | Batch_idx: 290 |  Loss: (0.2366) |  Loss2: (0.0000) | Acc: (91.00%) (34245/37248)
Epoch: 92 | Batch_idx: 300 |  Loss: (0.2366) |  Loss2: (0.0000) | Acc: (91.00%) (35418/38528)
Epoch: 92 | Batch_idx: 310 |  Loss: (0.2369) |  Loss2: (0.0000) | Acc: (91.00%) (36592/39808)
Epoch: 92 | Batch_idx: 320 |  Loss: (0.2364) |  Loss2: (0.0000) | Acc: (91.00%) (37768/41088)
Epoch: 92 | Batch_idx: 330 |  Loss: (0.2364) |  Loss2: (0.0000) | Acc: (91.00%) (38946/42368)
Epoch: 92 | Batch_idx: 340 |  Loss: (0.2375) |  Loss2: (0.0000) | Acc: (91.00%) (40105/43648)
Epoch: 92 | Batch_idx: 350 |  Loss: (0.2376) |  Loss2: (0.0000) | Acc: (91.00%) (41284/44928)
Epoch: 92 | Batch_idx: 360 |  Loss: (0.2373) |  Loss2: (0.0000) | Acc: (91.00%) (42466/46208)
Epoch: 92 | Batch_idx: 370 |  Loss: (0.2376) |  Loss2: (0.0000) | Acc: (91.00%) (43633/47488)
Epoch: 92 | Batch_idx: 380 |  Loss: (0.2379) |  Loss2: (0.0000) | Acc: (91.00%) (44808/48768)
Epoch: 92 | Batch_idx: 390 |  Loss: (0.2379) |  Loss2: (0.0000) | Acc: (91.00%) (45938/50000)
# TEST : Loss: (0.4318) | Acc: (86.00%) (8639/10000)
percent tensor([0.5749, 0.4251], device='cuda:0')
percent tensor([0.5497, 0.4503], device='cuda:0')
percent tensor([0.5355, 0.4645], device='cuda:0')
percent tensor([0.6112, 0.3888], device='cuda:0')
percent tensor([0.5816, 0.4184], device='cuda:0')
percent tensor([0.7055, 0.2945], device='cuda:0')
percent tensor([0.6846, 0.3154], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 93 | Batch_idx: 0 |  Loss: (0.2848) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 93 | Batch_idx: 10 |  Loss: (0.2102) |  Loss2: (0.0000) | Acc: (93.00%) (1312/1408)
Epoch: 93 | Batch_idx: 20 |  Loss: (0.2261) |  Loss2: (0.0000) | Acc: (92.00%) (2483/2688)
Epoch: 93 | Batch_idx: 30 |  Loss: (0.2328) |  Loss2: (0.0000) | Acc: (92.00%) (3652/3968)
Epoch: 93 | Batch_idx: 40 |  Loss: (0.2365) |  Loss2: (0.0000) | Acc: (91.00%) (4817/5248)
Epoch: 93 | Batch_idx: 50 |  Loss: (0.2298) |  Loss2: (0.0000) | Acc: (92.00%) (6009/6528)
Epoch: 93 | Batch_idx: 60 |  Loss: (0.2294) |  Loss2: (0.0000) | Acc: (92.00%) (7195/7808)
Epoch: 93 | Batch_idx: 70 |  Loss: (0.2310) |  Loss2: (0.0000) | Acc: (92.00%) (8369/9088)
Epoch: 93 | Batch_idx: 80 |  Loss: (0.2284) |  Loss2: (0.0000) | Acc: (92.00%) (9562/10368)
Epoch: 93 | Batch_idx: 90 |  Loss: (0.2305) |  Loss2: (0.0000) | Acc: (92.00%) (10733/11648)
Epoch: 93 | Batch_idx: 100 |  Loss: (0.2293) |  Loss2: (0.0000) | Acc: (92.00%) (11921/12928)
Epoch: 93 | Batch_idx: 110 |  Loss: (0.2305) |  Loss2: (0.0000) | Acc: (92.00%) (13098/14208)
Epoch: 93 | Batch_idx: 120 |  Loss: (0.2294) |  Loss2: (0.0000) | Acc: (92.00%) (14283/15488)
Epoch: 93 | Batch_idx: 130 |  Loss: (0.2289) |  Loss2: (0.0000) | Acc: (92.00%) (15468/16768)
Epoch: 93 | Batch_idx: 140 |  Loss: (0.2287) |  Loss2: (0.0000) | Acc: (92.00%) (16646/18048)
Epoch: 93 | Batch_idx: 150 |  Loss: (0.2291) |  Loss2: (0.0000) | Acc: (92.00%) (17819/19328)
Epoch: 93 | Batch_idx: 160 |  Loss: (0.2299) |  Loss2: (0.0000) | Acc: (92.00%) (18986/20608)
Epoch: 93 | Batch_idx: 170 |  Loss: (0.2314) |  Loss2: (0.0000) | Acc: (92.00%) (20153/21888)
Epoch: 93 | Batch_idx: 180 |  Loss: (0.2321) |  Loss2: (0.0000) | Acc: (92.00%) (21333/23168)
Epoch: 93 | Batch_idx: 190 |  Loss: (0.2302) |  Loss2: (0.0000) | Acc: (92.00%) (22528/24448)
Epoch: 93 | Batch_idx: 200 |  Loss: (0.2290) |  Loss2: (0.0000) | Acc: (92.00%) (23711/25728)
Epoch: 93 | Batch_idx: 210 |  Loss: (0.2279) |  Loss2: (0.0000) | Acc: (92.00%) (24906/27008)
Epoch: 93 | Batch_idx: 220 |  Loss: (0.2267) |  Loss2: (0.0000) | Acc: (92.00%) (26107/28288)
Epoch: 93 | Batch_idx: 230 |  Loss: (0.2284) |  Loss2: (0.0000) | Acc: (92.00%) (27268/29568)
Epoch: 93 | Batch_idx: 240 |  Loss: (0.2276) |  Loss2: (0.0000) | Acc: (92.00%) (28449/30848)
Epoch: 93 | Batch_idx: 250 |  Loss: (0.2291) |  Loss2: (0.0000) | Acc: (92.00%) (29616/32128)
Epoch: 93 | Batch_idx: 260 |  Loss: (0.2287) |  Loss2: (0.0000) | Acc: (92.00%) (30802/33408)
Epoch: 93 | Batch_idx: 270 |  Loss: (0.2287) |  Loss2: (0.0000) | Acc: (92.00%) (31980/34688)
Epoch: 93 | Batch_idx: 280 |  Loss: (0.2289) |  Loss2: (0.0000) | Acc: (92.00%) (33152/35968)
Epoch: 93 | Batch_idx: 290 |  Loss: (0.2293) |  Loss2: (0.0000) | Acc: (92.00%) (34321/37248)
Epoch: 93 | Batch_idx: 300 |  Loss: (0.2302) |  Loss2: (0.0000) | Acc: (92.00%) (35482/38528)
Epoch: 93 | Batch_idx: 310 |  Loss: (0.2305) |  Loss2: (0.0000) | Acc: (92.00%) (36648/39808)
Epoch: 93 | Batch_idx: 320 |  Loss: (0.2298) |  Loss2: (0.0000) | Acc: (92.00%) (37835/41088)
Epoch: 93 | Batch_idx: 330 |  Loss: (0.2297) |  Loss2: (0.0000) | Acc: (92.00%) (39003/42368)
Epoch: 93 | Batch_idx: 340 |  Loss: (0.2292) |  Loss2: (0.0000) | Acc: (92.00%) (40190/43648)
Epoch: 93 | Batch_idx: 350 |  Loss: (0.2296) |  Loss2: (0.0000) | Acc: (92.00%) (41360/44928)
Epoch: 93 | Batch_idx: 360 |  Loss: (0.2290) |  Loss2: (0.0000) | Acc: (92.00%) (42552/46208)
Epoch: 93 | Batch_idx: 370 |  Loss: (0.2295) |  Loss2: (0.0000) | Acc: (92.00%) (43713/47488)
Epoch: 93 | Batch_idx: 380 |  Loss: (0.2297) |  Loss2: (0.0000) | Acc: (92.00%) (44893/48768)
Epoch: 93 | Batch_idx: 390 |  Loss: (0.2300) |  Loss2: (0.0000) | Acc: (92.00%) (46019/50000)
# TEST : Loss: (0.4294) | Acc: (86.00%) (8654/10000)
percent tensor([0.5741, 0.4259], device='cuda:0')
percent tensor([0.5493, 0.4507], device='cuda:0')
percent tensor([0.5356, 0.4644], device='cuda:0')
percent tensor([0.6114, 0.3886], device='cuda:0')
percent tensor([0.5821, 0.4179], device='cuda:0')
percent tensor([0.7024, 0.2976], device='cuda:0')
percent tensor([0.6808, 0.3192], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Epoch: 94 | Batch_idx: 0 |  Loss: (0.3286) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 94 | Batch_idx: 10 |  Loss: (0.2133) |  Loss2: (0.0000) | Acc: (92.00%) (1298/1408)
Epoch: 94 | Batch_idx: 20 |  Loss: (0.2223) |  Loss2: (0.0000) | Acc: (91.00%) (2470/2688)
Epoch: 94 | Batch_idx: 30 |  Loss: (0.2173) |  Loss2: (0.0000) | Acc: (92.00%) (3658/3968)
Epoch: 94 | Batch_idx: 40 |  Loss: (0.2193) |  Loss2: (0.0000) | Acc: (92.00%) (4832/5248)
Epoch: 94 | Batch_idx: 50 |  Loss: (0.2185) |  Loss2: (0.0000) | Acc: (92.00%) (6022/6528)
Epoch: 94 | Batch_idx: 60 |  Loss: (0.2237) |  Loss2: (0.0000) | Acc: (92.00%) (7191/7808)
Epoch: 94 | Batch_idx: 70 |  Loss: (0.2155) |  Loss2: (0.0000) | Acc: (92.00%) (8398/9088)
Epoch: 94 | Batch_idx: 80 |  Loss: (0.2170) |  Loss2: (0.0000) | Acc: (92.00%) (9569/10368)
Epoch: 94 | Batch_idx: 90 |  Loss: (0.2192) |  Loss2: (0.0000) | Acc: (92.00%) (10751/11648)
Epoch: 94 | Batch_idx: 100 |  Loss: (0.2183) |  Loss2: (0.0000) | Acc: (92.00%) (11942/12928)
Epoch: 94 | Batch_idx: 110 |  Loss: (0.2174) |  Loss2: (0.0000) | Acc: (92.00%) (13125/14208)
Epoch: 94 | Batch_idx: 120 |  Loss: (0.2174) |  Loss2: (0.0000) | Acc: (92.00%) (14312/15488)
Epoch: 94 | Batch_idx: 130 |  Loss: (0.2181) |  Loss2: (0.0000) | Acc: (92.00%) (15491/16768)
Epoch: 94 | Batch_idx: 140 |  Loss: (0.2172) |  Loss2: (0.0000) | Acc: (92.00%) (16685/18048)
Epoch: 94 | Batch_idx: 150 |  Loss: (0.2177) |  Loss2: (0.0000) | Acc: (92.00%) (17863/19328)
Epoch: 94 | Batch_idx: 160 |  Loss: (0.2166) |  Loss2: (0.0000) | Acc: (92.00%) (19046/20608)
Epoch: 94 | Batch_idx: 170 |  Loss: (0.2190) |  Loss2: (0.0000) | Acc: (92.00%) (20219/21888)
Epoch: 94 | Batch_idx: 180 |  Loss: (0.2193) |  Loss2: (0.0000) | Acc: (92.00%) (21402/23168)
Epoch: 94 | Batch_idx: 190 |  Loss: (0.2211) |  Loss2: (0.0000) | Acc: (92.00%) (22560/24448)
Epoch: 94 | Batch_idx: 200 |  Loss: (0.2197) |  Loss2: (0.0000) | Acc: (92.00%) (23750/25728)
Epoch: 94 | Batch_idx: 210 |  Loss: (0.2201) |  Loss2: (0.0000) | Acc: (92.00%) (24933/27008)
Epoch: 94 | Batch_idx: 220 |  Loss: (0.2207) |  Loss2: (0.0000) | Acc: (92.00%) (26108/28288)
Epoch: 94 | Batch_idx: 230 |  Loss: (0.2210) |  Loss2: (0.0000) | Acc: (92.00%) (27288/29568)
Epoch: 94 | Batch_idx: 240 |  Loss: (0.2222) |  Loss2: (0.0000) | Acc: (92.00%) (28459/30848)
Epoch: 94 | Batch_idx: 250 |  Loss: (0.2222) |  Loss2: (0.0000) | Acc: (92.00%) (29647/32128)
Epoch: 94 | Batch_idx: 260 |  Loss: (0.2211) |  Loss2: (0.0000) | Acc: (92.00%) (30840/33408)
Epoch: 94 | Batch_idx: 270 |  Loss: (0.2207) |  Loss2: (0.0000) | Acc: (92.00%) (32037/34688)
Epoch: 94 | Batch_idx: 280 |  Loss: (0.2218) |  Loss2: (0.0000) | Acc: (92.00%) (33198/35968)
Epoch: 94 | Batch_idx: 290 |  Loss: (0.2234) |  Loss2: (0.0000) | Acc: (92.00%) (34366/37248)
Epoch: 94 | Batch_idx: 300 |  Loss: (0.2238) |  Loss2: (0.0000) | Acc: (92.00%) (35550/38528)
Epoch: 94 | Batch_idx: 310 |  Loss: (0.2240) |  Loss2: (0.0000) | Acc: (92.00%) (36733/39808)
Epoch: 94 | Batch_idx: 320 |  Loss: (0.2251) |  Loss2: (0.0000) | Acc: (92.00%) (37895/41088)
Epoch: 94 | Batch_idx: 330 |  Loss: (0.2256) |  Loss2: (0.0000) | Acc: (92.00%) (39069/42368)
Epoch: 94 | Batch_idx: 340 |  Loss: (0.2257) |  Loss2: (0.0000) | Acc: (92.00%) (40253/43648)
Epoch: 94 | Batch_idx: 350 |  Loss: (0.2262) |  Loss2: (0.0000) | Acc: (92.00%) (41422/44928)
Epoch: 94 | Batch_idx: 360 |  Loss: (0.2257) |  Loss2: (0.0000) | Acc: (92.00%) (42610/46208)
Epoch: 94 | Batch_idx: 370 |  Loss: (0.2255) |  Loss2: (0.0000) | Acc: (92.00%) (43792/47488)
Epoch: 94 | Batch_idx: 380 |  Loss: (0.2256) |  Loss2: (0.0000) | Acc: (92.00%) (44970/48768)
Epoch: 94 | Batch_idx: 390 |  Loss: (0.2257) |  Loss2: (0.0000) | Acc: (92.00%) (46099/50000)
# TEST : Loss: (0.4436) | Acc: (85.00%) (8574/10000)
percent tensor([0.5742, 0.4258], device='cuda:0')
percent tensor([0.5491, 0.4509], device='cuda:0')
percent tensor([0.5349, 0.4651], device='cuda:0')
percent tensor([0.6096, 0.3904], device='cuda:0')
percent tensor([0.5828, 0.4172], device='cuda:0')
percent tensor([0.7039, 0.2961], device='cuda:0')
percent tensor([0.6870, 0.3130], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 95 | Batch_idx: 0 |  Loss: (0.2143) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 95 | Batch_idx: 10 |  Loss: (0.2413) |  Loss2: (0.0000) | Acc: (91.00%) (1295/1408)
Epoch: 95 | Batch_idx: 20 |  Loss: (0.2423) |  Loss2: (0.0000) | Acc: (91.00%) (2464/2688)
Epoch: 95 | Batch_idx: 30 |  Loss: (0.2526) |  Loss2: (0.0000) | Acc: (91.00%) (3614/3968)
Epoch: 95 | Batch_idx: 40 |  Loss: (0.2563) |  Loss2: (0.0000) | Acc: (91.00%) (4783/5248)
Epoch: 95 | Batch_idx: 50 |  Loss: (0.2648) |  Loss2: (0.0000) | Acc: (90.00%) (5934/6528)
Epoch: 95 | Batch_idx: 60 |  Loss: (0.2703) |  Loss2: (0.0000) | Acc: (90.00%) (7085/7808)
Epoch: 95 | Batch_idx: 70 |  Loss: (0.2735) |  Loss2: (0.0000) | Acc: (90.00%) (8229/9088)
Epoch: 95 | Batch_idx: 80 |  Loss: (0.2755) |  Loss2: (0.0000) | Acc: (90.00%) (9385/10368)
Epoch: 95 | Batch_idx: 90 |  Loss: (0.2763) |  Loss2: (0.0000) | Acc: (90.00%) (10539/11648)
Epoch: 95 | Batch_idx: 100 |  Loss: (0.2771) |  Loss2: (0.0000) | Acc: (90.00%) (11684/12928)
Epoch: 95 | Batch_idx: 110 |  Loss: (0.2774) |  Loss2: (0.0000) | Acc: (90.00%) (12841/14208)
Epoch: 95 | Batch_idx: 120 |  Loss: (0.2753) |  Loss2: (0.0000) | Acc: (90.00%) (14009/15488)
Epoch: 95 | Batch_idx: 130 |  Loss: (0.2745) |  Loss2: (0.0000) | Acc: (90.00%) (15166/16768)
Epoch: 95 | Batch_idx: 140 |  Loss: (0.2738) |  Loss2: (0.0000) | Acc: (90.00%) (16319/18048)
Epoch: 95 | Batch_idx: 150 |  Loss: (0.2725) |  Loss2: (0.0000) | Acc: (90.00%) (17480/19328)
Epoch: 95 | Batch_idx: 160 |  Loss: (0.2721) |  Loss2: (0.0000) | Acc: (90.00%) (18647/20608)
Epoch: 95 | Batch_idx: 170 |  Loss: (0.2716) |  Loss2: (0.0000) | Acc: (90.00%) (19807/21888)
Epoch: 95 | Batch_idx: 180 |  Loss: (0.2697) |  Loss2: (0.0000) | Acc: (90.00%) (20975/23168)
Epoch: 95 | Batch_idx: 190 |  Loss: (0.2692) |  Loss2: (0.0000) | Acc: (90.00%) (22138/24448)
Epoch: 95 | Batch_idx: 200 |  Loss: (0.2679) |  Loss2: (0.0000) | Acc: (90.00%) (23311/25728)
Epoch: 95 | Batch_idx: 210 |  Loss: (0.2656) |  Loss2: (0.0000) | Acc: (90.00%) (24502/27008)
Epoch: 95 | Batch_idx: 220 |  Loss: (0.2634) |  Loss2: (0.0000) | Acc: (90.00%) (25694/28288)
Epoch: 95 | Batch_idx: 230 |  Loss: (0.2632) |  Loss2: (0.0000) | Acc: (90.00%) (26854/29568)
Epoch: 95 | Batch_idx: 240 |  Loss: (0.2638) |  Loss2: (0.0000) | Acc: (90.00%) (28017/30848)
Epoch: 95 | Batch_idx: 250 |  Loss: (0.2644) |  Loss2: (0.0000) | Acc: (90.00%) (29165/32128)
Epoch: 95 | Batch_idx: 260 |  Loss: (0.2644) |  Loss2: (0.0000) | Acc: (90.00%) (30327/33408)
Epoch: 95 | Batch_idx: 270 |  Loss: (0.2645) |  Loss2: (0.0000) | Acc: (90.00%) (31482/34688)
Epoch: 95 | Batch_idx: 280 |  Loss: (0.2629) |  Loss2: (0.0000) | Acc: (90.00%) (32663/35968)
Epoch: 95 | Batch_idx: 290 |  Loss: (0.2632) |  Loss2: (0.0000) | Acc: (90.00%) (33816/37248)
Epoch: 95 | Batch_idx: 300 |  Loss: (0.2621) |  Loss2: (0.0000) | Acc: (90.00%) (34992/38528)
Epoch: 95 | Batch_idx: 310 |  Loss: (0.2614) |  Loss2: (0.0000) | Acc: (90.00%) (36166/39808)
Epoch: 95 | Batch_idx: 320 |  Loss: (0.2618) |  Loss2: (0.0000) | Acc: (90.00%) (37323/41088)
Epoch: 95 | Batch_idx: 330 |  Loss: (0.2616) |  Loss2: (0.0000) | Acc: (90.00%) (38484/42368)
Epoch: 95 | Batch_idx: 340 |  Loss: (0.2624) |  Loss2: (0.0000) | Acc: (90.00%) (39646/43648)
Epoch: 95 | Batch_idx: 350 |  Loss: (0.2618) |  Loss2: (0.0000) | Acc: (90.00%) (40822/44928)
Epoch: 95 | Batch_idx: 360 |  Loss: (0.2620) |  Loss2: (0.0000) | Acc: (90.00%) (41988/46208)
Epoch: 95 | Batch_idx: 370 |  Loss: (0.2622) |  Loss2: (0.0000) | Acc: (90.00%) (43144/47488)
Epoch: 95 | Batch_idx: 380 |  Loss: (0.2621) |  Loss2: (0.0000) | Acc: (90.00%) (44306/48768)
Epoch: 95 | Batch_idx: 390 |  Loss: (0.2615) |  Loss2: (0.0000) | Acc: (90.00%) (45440/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_095.pth.tar'
# TEST : Loss: (0.4631) | Acc: (85.00%) (8543/10000)
percent tensor([0.5709, 0.4291], device='cuda:0')
percent tensor([0.5569, 0.4431], device='cuda:0')
percent tensor([0.5445, 0.4555], device='cuda:0')
percent tensor([0.6023, 0.3977], device='cuda:0')
percent tensor([0.5625, 0.4375], device='cuda:0')
percent tensor([0.6820, 0.3180], device='cuda:0')
percent tensor([0.6899, 0.3101], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Epoch: 96 | Batch_idx: 0 |  Loss: (0.1920) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 96 | Batch_idx: 10 |  Loss: (0.2497) |  Loss2: (0.0000) | Acc: (91.00%) (1291/1408)
Epoch: 96 | Batch_idx: 20 |  Loss: (0.2621) |  Loss2: (0.0000) | Acc: (91.00%) (2449/2688)
Epoch: 96 | Batch_idx: 30 |  Loss: (0.2605) |  Loss2: (0.0000) | Acc: (91.00%) (3614/3968)
Epoch: 96 | Batch_idx: 40 |  Loss: (0.2634) |  Loss2: (0.0000) | Acc: (91.00%) (4776/5248)
Epoch: 96 | Batch_idx: 50 |  Loss: (0.2627) |  Loss2: (0.0000) | Acc: (91.00%) (5942/6528)
Epoch: 96 | Batch_idx: 60 |  Loss: (0.2655) |  Loss2: (0.0000) | Acc: (90.00%) (7086/7808)
Epoch: 96 | Batch_idx: 70 |  Loss: (0.2640) |  Loss2: (0.0000) | Acc: (90.00%) (8267/9088)
Epoch: 96 | Batch_idx: 80 |  Loss: (0.2621) |  Loss2: (0.0000) | Acc: (90.00%) (9434/10368)
Epoch: 96 | Batch_idx: 90 |  Loss: (0.2601) |  Loss2: (0.0000) | Acc: (91.00%) (10604/11648)
Epoch: 96 | Batch_idx: 100 |  Loss: (0.2574) |  Loss2: (0.0000) | Acc: (91.00%) (11784/12928)
Epoch: 96 | Batch_idx: 110 |  Loss: (0.2557) |  Loss2: (0.0000) | Acc: (91.00%) (12960/14208)
Epoch: 96 | Batch_idx: 120 |  Loss: (0.2558) |  Loss2: (0.0000) | Acc: (91.00%) (14128/15488)
Epoch: 96 | Batch_idx: 130 |  Loss: (0.2543) |  Loss2: (0.0000) | Acc: (91.00%) (15305/16768)
Epoch: 96 | Batch_idx: 140 |  Loss: (0.2532) |  Loss2: (0.0000) | Acc: (91.00%) (16484/18048)
Epoch: 96 | Batch_idx: 150 |  Loss: (0.2532) |  Loss2: (0.0000) | Acc: (91.00%) (17644/19328)
Epoch: 96 | Batch_idx: 160 |  Loss: (0.2530) |  Loss2: (0.0000) | Acc: (91.00%) (18818/20608)
Epoch: 96 | Batch_idx: 170 |  Loss: (0.2541) |  Loss2: (0.0000) | Acc: (91.00%) (19983/21888)
Epoch: 96 | Batch_idx: 180 |  Loss: (0.2538) |  Loss2: (0.0000) | Acc: (91.00%) (21147/23168)
Epoch: 96 | Batch_idx: 190 |  Loss: (0.2536) |  Loss2: (0.0000) | Acc: (91.00%) (22305/24448)
Epoch: 96 | Batch_idx: 200 |  Loss: (0.2518) |  Loss2: (0.0000) | Acc: (91.00%) (23485/25728)
Epoch: 96 | Batch_idx: 210 |  Loss: (0.2523) |  Loss2: (0.0000) | Acc: (91.00%) (24641/27008)
Epoch: 96 | Batch_idx: 220 |  Loss: (0.2512) |  Loss2: (0.0000) | Acc: (91.00%) (25810/28288)
Epoch: 96 | Batch_idx: 230 |  Loss: (0.2506) |  Loss2: (0.0000) | Acc: (91.00%) (26985/29568)
Epoch: 96 | Batch_idx: 240 |  Loss: (0.2500) |  Loss2: (0.0000) | Acc: (91.00%) (28154/30848)
Epoch: 96 | Batch_idx: 250 |  Loss: (0.2503) |  Loss2: (0.0000) | Acc: (91.00%) (29324/32128)
Epoch: 96 | Batch_idx: 260 |  Loss: (0.2501) |  Loss2: (0.0000) | Acc: (91.00%) (30494/33408)
Epoch: 96 | Batch_idx: 270 |  Loss: (0.2486) |  Loss2: (0.0000) | Acc: (91.00%) (31688/34688)
Epoch: 96 | Batch_idx: 280 |  Loss: (0.2482) |  Loss2: (0.0000) | Acc: (91.00%) (32862/35968)
Epoch: 96 | Batch_idx: 290 |  Loss: (0.2480) |  Loss2: (0.0000) | Acc: (91.00%) (34039/37248)
Epoch: 96 | Batch_idx: 300 |  Loss: (0.2470) |  Loss2: (0.0000) | Acc: (91.00%) (35223/38528)
Epoch: 96 | Batch_idx: 310 |  Loss: (0.2468) |  Loss2: (0.0000) | Acc: (91.00%) (36397/39808)
Epoch: 96 | Batch_idx: 320 |  Loss: (0.2461) |  Loss2: (0.0000) | Acc: (91.00%) (37569/41088)
Epoch: 96 | Batch_idx: 330 |  Loss: (0.2458) |  Loss2: (0.0000) | Acc: (91.00%) (38743/42368)
Epoch: 96 | Batch_idx: 340 |  Loss: (0.2453) |  Loss2: (0.0000) | Acc: (91.00%) (39922/43648)
Epoch: 96 | Batch_idx: 350 |  Loss: (0.2449) |  Loss2: (0.0000) | Acc: (91.00%) (41102/44928)
Epoch: 96 | Batch_idx: 360 |  Loss: (0.2442) |  Loss2: (0.0000) | Acc: (91.00%) (42283/46208)
Epoch: 96 | Batch_idx: 370 |  Loss: (0.2439) |  Loss2: (0.0000) | Acc: (91.00%) (43455/47488)
Epoch: 96 | Batch_idx: 380 |  Loss: (0.2438) |  Loss2: (0.0000) | Acc: (91.00%) (44637/48768)
Epoch: 96 | Batch_idx: 390 |  Loss: (0.2436) |  Loss2: (0.0000) | Acc: (91.00%) (45779/50000)
# TEST : Loss: (0.4393) | Acc: (86.00%) (8602/10000)
percent tensor([0.5680, 0.4320], device='cuda:0')
percent tensor([0.5604, 0.4396], device='cuda:0')
percent tensor([0.5477, 0.4523], device='cuda:0')
percent tensor([0.6084, 0.3916], device='cuda:0')
percent tensor([0.5609, 0.4391], device='cuda:0')
percent tensor([0.6779, 0.3221], device='cuda:0')
percent tensor([0.6942, 0.3058], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 97 | Batch_idx: 0 |  Loss: (0.1721) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 97 | Batch_idx: 10 |  Loss: (0.2082) |  Loss2: (0.0000) | Acc: (92.00%) (1306/1408)
Epoch: 97 | Batch_idx: 20 |  Loss: (0.2094) |  Loss2: (0.0000) | Acc: (92.00%) (2496/2688)
Epoch: 97 | Batch_idx: 30 |  Loss: (0.2168) |  Loss2: (0.0000) | Acc: (92.00%) (3672/3968)
Epoch: 97 | Batch_idx: 40 |  Loss: (0.2181) |  Loss2: (0.0000) | Acc: (92.00%) (4852/5248)
Epoch: 97 | Batch_idx: 50 |  Loss: (0.2239) |  Loss2: (0.0000) | Acc: (92.00%) (6031/6528)
Epoch: 97 | Batch_idx: 60 |  Loss: (0.2243) |  Loss2: (0.0000) | Acc: (92.00%) (7208/7808)
Epoch: 97 | Batch_idx: 70 |  Loss: (0.2249) |  Loss2: (0.0000) | Acc: (92.00%) (8386/9088)
Epoch: 97 | Batch_idx: 80 |  Loss: (0.2271) |  Loss2: (0.0000) | Acc: (92.00%) (9561/10368)
Epoch: 97 | Batch_idx: 90 |  Loss: (0.2250) |  Loss2: (0.0000) | Acc: (92.00%) (10748/11648)
Epoch: 97 | Batch_idx: 100 |  Loss: (0.2259) |  Loss2: (0.0000) | Acc: (92.00%) (11912/12928)
Epoch: 97 | Batch_idx: 110 |  Loss: (0.2286) |  Loss2: (0.0000) | Acc: (92.00%) (13082/14208)
Epoch: 97 | Batch_idx: 120 |  Loss: (0.2272) |  Loss2: (0.0000) | Acc: (92.00%) (14271/15488)
Epoch: 97 | Batch_idx: 130 |  Loss: (0.2304) |  Loss2: (0.0000) | Acc: (92.00%) (15433/16768)
Epoch: 97 | Batch_idx: 140 |  Loss: (0.2313) |  Loss2: (0.0000) | Acc: (92.00%) (16610/18048)
Epoch: 97 | Batch_idx: 150 |  Loss: (0.2320) |  Loss2: (0.0000) | Acc: (92.00%) (17787/19328)
Epoch: 97 | Batch_idx: 160 |  Loss: (0.2349) |  Loss2: (0.0000) | Acc: (91.00%) (18946/20608)
Epoch: 97 | Batch_idx: 170 |  Loss: (0.2331) |  Loss2: (0.0000) | Acc: (91.00%) (20135/21888)
Epoch: 97 | Batch_idx: 180 |  Loss: (0.2310) |  Loss2: (0.0000) | Acc: (92.00%) (21326/23168)
Epoch: 97 | Batch_idx: 190 |  Loss: (0.2325) |  Loss2: (0.0000) | Acc: (91.00%) (22489/24448)
Epoch: 97 | Batch_idx: 200 |  Loss: (0.2333) |  Loss2: (0.0000) | Acc: (91.00%) (23656/25728)
Epoch: 97 | Batch_idx: 210 |  Loss: (0.2341) |  Loss2: (0.0000) | Acc: (91.00%) (24831/27008)
Epoch: 97 | Batch_idx: 220 |  Loss: (0.2343) |  Loss2: (0.0000) | Acc: (91.00%) (26005/28288)
Epoch: 97 | Batch_idx: 230 |  Loss: (0.2340) |  Loss2: (0.0000) | Acc: (91.00%) (27179/29568)
Epoch: 97 | Batch_idx: 240 |  Loss: (0.2344) |  Loss2: (0.0000) | Acc: (91.00%) (28362/30848)
Epoch: 97 | Batch_idx: 250 |  Loss: (0.2336) |  Loss2: (0.0000) | Acc: (91.00%) (29556/32128)
Epoch: 97 | Batch_idx: 260 |  Loss: (0.2342) |  Loss2: (0.0000) | Acc: (91.00%) (30720/33408)
Epoch: 97 | Batch_idx: 270 |  Loss: (0.2338) |  Loss2: (0.0000) | Acc: (91.00%) (31906/34688)
Epoch: 97 | Batch_idx: 280 |  Loss: (0.2344) |  Loss2: (0.0000) | Acc: (91.00%) (33084/35968)
Epoch: 97 | Batch_idx: 290 |  Loss: (0.2343) |  Loss2: (0.0000) | Acc: (91.00%) (34266/37248)
Epoch: 97 | Batch_idx: 300 |  Loss: (0.2345) |  Loss2: (0.0000) | Acc: (91.00%) (35425/38528)
Epoch: 97 | Batch_idx: 310 |  Loss: (0.2339) |  Loss2: (0.0000) | Acc: (91.00%) (36615/39808)
Epoch: 97 | Batch_idx: 320 |  Loss: (0.2336) |  Loss2: (0.0000) | Acc: (91.00%) (37793/41088)
Epoch: 97 | Batch_idx: 330 |  Loss: (0.2338) |  Loss2: (0.0000) | Acc: (91.00%) (38972/42368)
Epoch: 97 | Batch_idx: 340 |  Loss: (0.2335) |  Loss2: (0.0000) | Acc: (91.00%) (40156/43648)
Epoch: 97 | Batch_idx: 350 |  Loss: (0.2326) |  Loss2: (0.0000) | Acc: (92.00%) (41359/44928)
Epoch: 97 | Batch_idx: 360 |  Loss: (0.2319) |  Loss2: (0.0000) | Acc: (92.00%) (42543/46208)
Epoch: 97 | Batch_idx: 370 |  Loss: (0.2320) |  Loss2: (0.0000) | Acc: (92.00%) (43716/47488)
Epoch: 97 | Batch_idx: 380 |  Loss: (0.2320) |  Loss2: (0.0000) | Acc: (92.00%) (44899/48768)
Epoch: 97 | Batch_idx: 390 |  Loss: (0.2328) |  Loss2: (0.0000) | Acc: (92.00%) (46017/50000)
# TEST : Loss: (0.4323) | Acc: (86.00%) (8628/10000)
percent tensor([0.5675, 0.4325], device='cuda:0')
percent tensor([0.5624, 0.4376], device='cuda:0')
percent tensor([0.5482, 0.4518], device='cuda:0')
percent tensor([0.6124, 0.3876], device='cuda:0')
percent tensor([0.5648, 0.4352], device='cuda:0')
percent tensor([0.6806, 0.3194], device='cuda:0')
percent tensor([0.7066, 0.2934], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 98 | Batch_idx: 0 |  Loss: (0.1826) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 98 | Batch_idx: 10 |  Loss: (0.2157) |  Loss2: (0.0000) | Acc: (92.00%) (1300/1408)
Epoch: 98 | Batch_idx: 20 |  Loss: (0.2164) |  Loss2: (0.0000) | Acc: (92.00%) (2481/2688)
Epoch: 98 | Batch_idx: 30 |  Loss: (0.2242) |  Loss2: (0.0000) | Acc: (92.00%) (3654/3968)
Epoch: 98 | Batch_idx: 40 |  Loss: (0.2246) |  Loss2: (0.0000) | Acc: (92.00%) (4830/5248)
Epoch: 98 | Batch_idx: 50 |  Loss: (0.2232) |  Loss2: (0.0000) | Acc: (92.00%) (6020/6528)
Epoch: 98 | Batch_idx: 60 |  Loss: (0.2246) |  Loss2: (0.0000) | Acc: (92.00%) (7208/7808)
Epoch: 98 | Batch_idx: 70 |  Loss: (0.2227) |  Loss2: (0.0000) | Acc: (92.00%) (8400/9088)
Epoch: 98 | Batch_idx: 80 |  Loss: (0.2235) |  Loss2: (0.0000) | Acc: (92.00%) (9582/10368)
Epoch: 98 | Batch_idx: 90 |  Loss: (0.2235) |  Loss2: (0.0000) | Acc: (92.00%) (10770/11648)
Epoch: 98 | Batch_idx: 100 |  Loss: (0.2218) |  Loss2: (0.0000) | Acc: (92.00%) (11957/12928)
Epoch: 98 | Batch_idx: 110 |  Loss: (0.2223) |  Loss2: (0.0000) | Acc: (92.00%) (13143/14208)
Epoch: 98 | Batch_idx: 120 |  Loss: (0.2255) |  Loss2: (0.0000) | Acc: (92.00%) (14305/15488)
Epoch: 98 | Batch_idx: 130 |  Loss: (0.2256) |  Loss2: (0.0000) | Acc: (92.00%) (15477/16768)
Epoch: 98 | Batch_idx: 140 |  Loss: (0.2254) |  Loss2: (0.0000) | Acc: (92.00%) (16663/18048)
Epoch: 98 | Batch_idx: 150 |  Loss: (0.2251) |  Loss2: (0.0000) | Acc: (92.00%) (17846/19328)
Epoch: 98 | Batch_idx: 160 |  Loss: (0.2236) |  Loss2: (0.0000) | Acc: (92.00%) (19031/20608)
Epoch: 98 | Batch_idx: 170 |  Loss: (0.2253) |  Loss2: (0.0000) | Acc: (92.00%) (20198/21888)
Epoch: 98 | Batch_idx: 180 |  Loss: (0.2246) |  Loss2: (0.0000) | Acc: (92.00%) (21382/23168)
Epoch: 98 | Batch_idx: 190 |  Loss: (0.2235) |  Loss2: (0.0000) | Acc: (92.00%) (22569/24448)
Epoch: 98 | Batch_idx: 200 |  Loss: (0.2237) |  Loss2: (0.0000) | Acc: (92.00%) (23746/25728)
Epoch: 98 | Batch_idx: 210 |  Loss: (0.2245) |  Loss2: (0.0000) | Acc: (92.00%) (24921/27008)
Epoch: 98 | Batch_idx: 220 |  Loss: (0.2246) |  Loss2: (0.0000) | Acc: (92.00%) (26117/28288)
Epoch: 98 | Batch_idx: 230 |  Loss: (0.2244) |  Loss2: (0.0000) | Acc: (92.00%) (27293/29568)
Epoch: 98 | Batch_idx: 240 |  Loss: (0.2255) |  Loss2: (0.0000) | Acc: (92.00%) (28458/30848)
Epoch: 98 | Batch_idx: 250 |  Loss: (0.2248) |  Loss2: (0.0000) | Acc: (92.00%) (29648/32128)
Epoch: 98 | Batch_idx: 260 |  Loss: (0.2241) |  Loss2: (0.0000) | Acc: (92.00%) (30845/33408)
Epoch: 98 | Batch_idx: 270 |  Loss: (0.2250) |  Loss2: (0.0000) | Acc: (92.00%) (32001/34688)
Epoch: 98 | Batch_idx: 280 |  Loss: (0.2253) |  Loss2: (0.0000) | Acc: (92.00%) (33169/35968)
Epoch: 98 | Batch_idx: 290 |  Loss: (0.2258) |  Loss2: (0.0000) | Acc: (92.00%) (34341/37248)
Epoch: 98 | Batch_idx: 300 |  Loss: (0.2254) |  Loss2: (0.0000) | Acc: (92.00%) (35528/38528)
Epoch: 98 | Batch_idx: 310 |  Loss: (0.2255) |  Loss2: (0.0000) | Acc: (92.00%) (36711/39808)
Epoch: 98 | Batch_idx: 320 |  Loss: (0.2255) |  Loss2: (0.0000) | Acc: (92.00%) (37896/41088)
Epoch: 98 | Batch_idx: 330 |  Loss: (0.2255) |  Loss2: (0.0000) | Acc: (92.00%) (39074/42368)
Epoch: 98 | Batch_idx: 340 |  Loss: (0.2258) |  Loss2: (0.0000) | Acc: (92.00%) (40248/43648)
Epoch: 98 | Batch_idx: 350 |  Loss: (0.2262) |  Loss2: (0.0000) | Acc: (92.00%) (41416/44928)
Epoch: 98 | Batch_idx: 360 |  Loss: (0.2261) |  Loss2: (0.0000) | Acc: (92.00%) (42598/46208)
Epoch: 98 | Batch_idx: 370 |  Loss: (0.2258) |  Loss2: (0.0000) | Acc: (92.00%) (43784/47488)
Epoch: 98 | Batch_idx: 380 |  Loss: (0.2254) |  Loss2: (0.0000) | Acc: (92.00%) (44975/48768)
Epoch: 98 | Batch_idx: 390 |  Loss: (0.2259) |  Loss2: (0.0000) | Acc: (92.00%) (46103/50000)
# TEST : Loss: (0.4272) | Acc: (86.00%) (8642/10000)
percent tensor([0.5675, 0.4325], device='cuda:0')
percent tensor([0.5602, 0.4398], device='cuda:0')
percent tensor([0.5494, 0.4506], device='cuda:0')
percent tensor([0.6104, 0.3896], device='cuda:0')
percent tensor([0.5697, 0.4303], device='cuda:0')
percent tensor([0.6863, 0.3137], device='cuda:0')
percent tensor([0.7055, 0.2945], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 99 | Batch_idx: 0 |  Loss: (0.2770) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 99 | Batch_idx: 10 |  Loss: (0.2422) |  Loss2: (0.0000) | Acc: (92.00%) (1297/1408)
Epoch: 99 | Batch_idx: 20 |  Loss: (0.2423) |  Loss2: (0.0000) | Acc: (91.00%) (2465/2688)
Epoch: 99 | Batch_idx: 30 |  Loss: (0.2328) |  Loss2: (0.0000) | Acc: (92.00%) (3653/3968)
Epoch: 99 | Batch_idx: 40 |  Loss: (0.2342) |  Loss2: (0.0000) | Acc: (92.00%) (4832/5248)
Epoch: 99 | Batch_idx: 50 |  Loss: (0.2280) |  Loss2: (0.0000) | Acc: (92.00%) (6025/6528)
Epoch: 99 | Batch_idx: 60 |  Loss: (0.2293) |  Loss2: (0.0000) | Acc: (92.00%) (7199/7808)
Epoch: 99 | Batch_idx: 70 |  Loss: (0.2264) |  Loss2: (0.0000) | Acc: (92.00%) (8382/9088)
Epoch: 99 | Batch_idx: 80 |  Loss: (0.2237) |  Loss2: (0.0000) | Acc: (92.00%) (9569/10368)
Epoch: 99 | Batch_idx: 90 |  Loss: (0.2275) |  Loss2: (0.0000) | Acc: (92.00%) (10737/11648)
Epoch: 99 | Batch_idx: 100 |  Loss: (0.2278) |  Loss2: (0.0000) | Acc: (92.00%) (11913/12928)
Epoch: 99 | Batch_idx: 110 |  Loss: (0.2281) |  Loss2: (0.0000) | Acc: (92.00%) (13099/14208)
Epoch: 99 | Batch_idx: 120 |  Loss: (0.2289) |  Loss2: (0.0000) | Acc: (92.00%) (14264/15488)
Epoch: 99 | Batch_idx: 130 |  Loss: (0.2305) |  Loss2: (0.0000) | Acc: (91.00%) (15425/16768)
Epoch: 99 | Batch_idx: 140 |  Loss: (0.2280) |  Loss2: (0.0000) | Acc: (92.00%) (16618/18048)
Epoch: 99 | Batch_idx: 150 |  Loss: (0.2259) |  Loss2: (0.0000) | Acc: (92.00%) (17804/19328)
Epoch: 99 | Batch_idx: 160 |  Loss: (0.2270) |  Loss2: (0.0000) | Acc: (92.00%) (18982/20608)
Epoch: 99 | Batch_idx: 170 |  Loss: (0.2262) |  Loss2: (0.0000) | Acc: (92.00%) (20161/21888)
Epoch: 99 | Batch_idx: 180 |  Loss: (0.2250) |  Loss2: (0.0000) | Acc: (92.00%) (21349/23168)
Epoch: 99 | Batch_idx: 190 |  Loss: (0.2257) |  Loss2: (0.0000) | Acc: (92.00%) (22531/24448)
Epoch: 99 | Batch_idx: 200 |  Loss: (0.2244) |  Loss2: (0.0000) | Acc: (92.00%) (23722/25728)
Epoch: 99 | Batch_idx: 210 |  Loss: (0.2242) |  Loss2: (0.0000) | Acc: (92.00%) (24915/27008)
Epoch: 99 | Batch_idx: 220 |  Loss: (0.2228) |  Loss2: (0.0000) | Acc: (92.00%) (26109/28288)
Epoch: 99 | Batch_idx: 230 |  Loss: (0.2224) |  Loss2: (0.0000) | Acc: (92.00%) (27303/29568)
Epoch: 99 | Batch_idx: 240 |  Loss: (0.2228) |  Loss2: (0.0000) | Acc: (92.00%) (28484/30848)
Epoch: 99 | Batch_idx: 250 |  Loss: (0.2241) |  Loss2: (0.0000) | Acc: (92.00%) (29643/32128)
Epoch: 99 | Batch_idx: 260 |  Loss: (0.2231) |  Loss2: (0.0000) | Acc: (92.00%) (30833/33408)
Epoch: 99 | Batch_idx: 270 |  Loss: (0.2237) |  Loss2: (0.0000) | Acc: (92.00%) (32006/34688)
Epoch: 99 | Batch_idx: 280 |  Loss: (0.2238) |  Loss2: (0.0000) | Acc: (92.00%) (33187/35968)
Epoch: 99 | Batch_idx: 290 |  Loss: (0.2247) |  Loss2: (0.0000) | Acc: (92.00%) (34360/37248)
Epoch: 99 | Batch_idx: 300 |  Loss: (0.2240) |  Loss2: (0.0000) | Acc: (92.00%) (35552/38528)
Epoch: 99 | Batch_idx: 310 |  Loss: (0.2242) |  Loss2: (0.0000) | Acc: (92.00%) (36739/39808)
Epoch: 99 | Batch_idx: 320 |  Loss: (0.2236) |  Loss2: (0.0000) | Acc: (92.00%) (37933/41088)
Epoch: 99 | Batch_idx: 330 |  Loss: (0.2236) |  Loss2: (0.0000) | Acc: (92.00%) (39122/42368)
Epoch: 99 | Batch_idx: 340 |  Loss: (0.2228) |  Loss2: (0.0000) | Acc: (92.00%) (40314/43648)
Epoch: 99 | Batch_idx: 350 |  Loss: (0.2221) |  Loss2: (0.0000) | Acc: (92.00%) (41512/44928)
Epoch: 99 | Batch_idx: 360 |  Loss: (0.2221) |  Loss2: (0.0000) | Acc: (92.00%) (42696/46208)
Epoch: 99 | Batch_idx: 370 |  Loss: (0.2220) |  Loss2: (0.0000) | Acc: (92.00%) (43884/47488)
Epoch: 99 | Batch_idx: 380 |  Loss: (0.2220) |  Loss2: (0.0000) | Acc: (92.00%) (45061/48768)
Epoch: 99 | Batch_idx: 390 |  Loss: (0.2227) |  Loss2: (0.0000) | Acc: (92.00%) (46189/50000)
# TEST : Loss: (0.4183) | Acc: (86.00%) (8671/10000)
percent tensor([0.5693, 0.4307], device='cuda:0')
percent tensor([0.5597, 0.4403], device='cuda:0')
percent tensor([0.5496, 0.4504], device='cuda:0')
percent tensor([0.6118, 0.3882], device='cuda:0')
percent tensor([0.5734, 0.4266], device='cuda:0')
percent tensor([0.6904, 0.3096], device='cuda:0')
percent tensor([0.7078, 0.2922], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 100 | Batch_idx: 0 |  Loss: (0.1994) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 100 | Batch_idx: 10 |  Loss: (0.2233) |  Loss2: (0.0000) | Acc: (91.00%) (1287/1408)
Epoch: 100 | Batch_idx: 20 |  Loss: (0.2109) |  Loss2: (0.0000) | Acc: (92.00%) (2493/2688)
Epoch: 100 | Batch_idx: 30 |  Loss: (0.2125) |  Loss2: (0.0000) | Acc: (92.00%) (3680/3968)
Epoch: 100 | Batch_idx: 40 |  Loss: (0.2193) |  Loss2: (0.0000) | Acc: (92.00%) (4849/5248)
Epoch: 100 | Batch_idx: 50 |  Loss: (0.2173) |  Loss2: (0.0000) | Acc: (92.00%) (6032/6528)
Epoch: 100 | Batch_idx: 60 |  Loss: (0.2214) |  Loss2: (0.0000) | Acc: (92.00%) (7202/7808)
Epoch: 100 | Batch_idx: 70 |  Loss: (0.2215) |  Loss2: (0.0000) | Acc: (92.00%) (8387/9088)
Epoch: 100 | Batch_idx: 80 |  Loss: (0.2193) |  Loss2: (0.0000) | Acc: (92.00%) (9575/10368)
Epoch: 100 | Batch_idx: 90 |  Loss: (0.2200) |  Loss2: (0.0000) | Acc: (92.00%) (10768/11648)
Epoch: 100 | Batch_idx: 100 |  Loss: (0.2205) |  Loss2: (0.0000) | Acc: (92.00%) (11959/12928)
Epoch: 100 | Batch_idx: 110 |  Loss: (0.2194) |  Loss2: (0.0000) | Acc: (92.00%) (13147/14208)
Epoch: 100 | Batch_idx: 120 |  Loss: (0.2200) |  Loss2: (0.0000) | Acc: (92.00%) (14319/15488)
Epoch: 100 | Batch_idx: 130 |  Loss: (0.2204) |  Loss2: (0.0000) | Acc: (92.00%) (15504/16768)
Epoch: 100 | Batch_idx: 140 |  Loss: (0.2202) |  Loss2: (0.0000) | Acc: (92.00%) (16689/18048)
Epoch: 100 | Batch_idx: 150 |  Loss: (0.2201) |  Loss2: (0.0000) | Acc: (92.00%) (17876/19328)
Epoch: 100 | Batch_idx: 160 |  Loss: (0.2204) |  Loss2: (0.0000) | Acc: (92.00%) (19039/20608)
Epoch: 100 | Batch_idx: 170 |  Loss: (0.2203) |  Loss2: (0.0000) | Acc: (92.00%) (20222/21888)
Epoch: 100 | Batch_idx: 180 |  Loss: (0.2209) |  Loss2: (0.0000) | Acc: (92.00%) (21389/23168)
Epoch: 100 | Batch_idx: 190 |  Loss: (0.2215) |  Loss2: (0.0000) | Acc: (92.00%) (22567/24448)
Epoch: 100 | Batch_idx: 200 |  Loss: (0.2228) |  Loss2: (0.0000) | Acc: (92.00%) (23735/25728)
Epoch: 100 | Batch_idx: 210 |  Loss: (0.2238) |  Loss2: (0.0000) | Acc: (92.00%) (24907/27008)
Epoch: 100 | Batch_idx: 220 |  Loss: (0.2242) |  Loss2: (0.0000) | Acc: (92.00%) (26078/28288)
Epoch: 100 | Batch_idx: 230 |  Loss: (0.2236) |  Loss2: (0.0000) | Acc: (92.00%) (27265/29568)
Epoch: 100 | Batch_idx: 240 |  Loss: (0.2242) |  Loss2: (0.0000) | Acc: (92.00%) (28438/30848)
Epoch: 100 | Batch_idx: 250 |  Loss: (0.2252) |  Loss2: (0.0000) | Acc: (92.00%) (29612/32128)
Epoch: 100 | Batch_idx: 260 |  Loss: (0.2250) |  Loss2: (0.0000) | Acc: (92.00%) (30799/33408)
Epoch: 100 | Batch_idx: 270 |  Loss: (0.2259) |  Loss2: (0.0000) | Acc: (92.00%) (31967/34688)
Epoch: 100 | Batch_idx: 280 |  Loss: (0.2257) |  Loss2: (0.0000) | Acc: (92.00%) (33156/35968)
Epoch: 100 | Batch_idx: 290 |  Loss: (0.2261) |  Loss2: (0.0000) | Acc: (92.00%) (34324/37248)
Epoch: 100 | Batch_idx: 300 |  Loss: (0.2260) |  Loss2: (0.0000) | Acc: (92.00%) (35517/38528)
Epoch: 100 | Batch_idx: 310 |  Loss: (0.2269) |  Loss2: (0.0000) | Acc: (92.00%) (36686/39808)
Epoch: 100 | Batch_idx: 320 |  Loss: (0.2265) |  Loss2: (0.0000) | Acc: (92.00%) (37882/41088)
Epoch: 100 | Batch_idx: 330 |  Loss: (0.2266) |  Loss2: (0.0000) | Acc: (92.00%) (39058/42368)
Epoch: 100 | Batch_idx: 340 |  Loss: (0.2261) |  Loss2: (0.0000) | Acc: (92.00%) (40241/43648)
Epoch: 100 | Batch_idx: 350 |  Loss: (0.2261) |  Loss2: (0.0000) | Acc: (92.00%) (41413/44928)
Epoch: 100 | Batch_idx: 360 |  Loss: (0.2259) |  Loss2: (0.0000) | Acc: (92.00%) (42599/46208)
Epoch: 100 | Batch_idx: 370 |  Loss: (0.2258) |  Loss2: (0.0000) | Acc: (92.00%) (43785/47488)
Epoch: 100 | Batch_idx: 380 |  Loss: (0.2264) |  Loss2: (0.0000) | Acc: (92.00%) (44953/48768)
Epoch: 100 | Batch_idx: 390 |  Loss: (0.2275) |  Loss2: (0.0000) | Acc: (92.00%) (46061/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_100.pth.tar'
# TEST : Loss: (0.5097) | Acc: (84.00%) (8491/10000)
percent tensor([0.5696, 0.4304], device='cuda:0')
percent tensor([0.5594, 0.4406], device='cuda:0')
percent tensor([0.5490, 0.4510], device='cuda:0')
percent tensor([0.6099, 0.3901], device='cuda:0')
percent tensor([0.5739, 0.4261], device='cuda:0')
percent tensor([0.6881, 0.3119], device='cuda:0')
percent tensor([0.7105, 0.2895], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(179.2844, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(814.4030, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(811.3518, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1524.3298, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(493.6793, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2244.6790, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4281.6304, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1393.6150, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6155.2432, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11871.3867, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3931.6592, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16639.9258, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 101 | Batch_idx: 0 |  Loss: (0.2492) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 101 | Batch_idx: 10 |  Loss: (0.2208) |  Loss2: (0.0000) | Acc: (92.00%) (1301/1408)
Epoch: 101 | Batch_idx: 20 |  Loss: (0.2089) |  Loss2: (0.0000) | Acc: (92.00%) (2492/2688)
Epoch: 101 | Batch_idx: 30 |  Loss: (0.2170) |  Loss2: (0.0000) | Acc: (92.00%) (3683/3968)
Epoch: 101 | Batch_idx: 40 |  Loss: (0.2117) |  Loss2: (0.0000) | Acc: (92.00%) (4875/5248)
Epoch: 101 | Batch_idx: 50 |  Loss: (0.2167) |  Loss2: (0.0000) | Acc: (92.00%) (6044/6528)
Epoch: 101 | Batch_idx: 60 |  Loss: (0.2140) |  Loss2: (0.0000) | Acc: (92.00%) (7234/7808)
Epoch: 101 | Batch_idx: 70 |  Loss: (0.2133) |  Loss2: (0.0000) | Acc: (92.00%) (8426/9088)
Epoch: 101 | Batch_idx: 80 |  Loss: (0.2129) |  Loss2: (0.0000) | Acc: (92.00%) (9615/10368)
Epoch: 101 | Batch_idx: 90 |  Loss: (0.2123) |  Loss2: (0.0000) | Acc: (92.00%) (10810/11648)
Epoch: 101 | Batch_idx: 100 |  Loss: (0.2114) |  Loss2: (0.0000) | Acc: (92.00%) (12005/12928)
Epoch: 101 | Batch_idx: 110 |  Loss: (0.2096) |  Loss2: (0.0000) | Acc: (92.00%) (13195/14208)
Epoch: 101 | Batch_idx: 120 |  Loss: (0.2104) |  Loss2: (0.0000) | Acc: (92.00%) (14367/15488)
Epoch: 101 | Batch_idx: 130 |  Loss: (0.2093) |  Loss2: (0.0000) | Acc: (92.00%) (15559/16768)
Epoch: 101 | Batch_idx: 140 |  Loss: (0.2067) |  Loss2: (0.0000) | Acc: (92.00%) (16766/18048)
Epoch: 101 | Batch_idx: 150 |  Loss: (0.2057) |  Loss2: (0.0000) | Acc: (92.00%) (17961/19328)
Epoch: 101 | Batch_idx: 160 |  Loss: (0.2071) |  Loss2: (0.0000) | Acc: (92.00%) (19144/20608)
Epoch: 101 | Batch_idx: 170 |  Loss: (0.2073) |  Loss2: (0.0000) | Acc: (92.00%) (20331/21888)
Epoch: 101 | Batch_idx: 180 |  Loss: (0.2067) |  Loss2: (0.0000) | Acc: (92.00%) (21527/23168)
Epoch: 101 | Batch_idx: 190 |  Loss: (0.2067) |  Loss2: (0.0000) | Acc: (92.00%) (22718/24448)
Epoch: 101 | Batch_idx: 200 |  Loss: (0.2072) |  Loss2: (0.0000) | Acc: (92.00%) (23914/25728)
Epoch: 101 | Batch_idx: 210 |  Loss: (0.2067) |  Loss2: (0.0000) | Acc: (92.00%) (25108/27008)
Epoch: 101 | Batch_idx: 220 |  Loss: (0.2073) |  Loss2: (0.0000) | Acc: (92.00%) (26284/28288)
Epoch: 101 | Batch_idx: 230 |  Loss: (0.2079) |  Loss2: (0.0000) | Acc: (92.00%) (27463/29568)
Epoch: 101 | Batch_idx: 240 |  Loss: (0.2078) |  Loss2: (0.0000) | Acc: (92.00%) (28652/30848)
Epoch: 101 | Batch_idx: 250 |  Loss: (0.2086) |  Loss2: (0.0000) | Acc: (92.00%) (29831/32128)
Epoch: 101 | Batch_idx: 260 |  Loss: (0.2108) |  Loss2: (0.0000) | Acc: (92.00%) (30991/33408)
Epoch: 101 | Batch_idx: 270 |  Loss: (0.2123) |  Loss2: (0.0000) | Acc: (92.00%) (32169/34688)
Epoch: 101 | Batch_idx: 280 |  Loss: (0.2139) |  Loss2: (0.0000) | Acc: (92.00%) (33332/35968)
Epoch: 101 | Batch_idx: 290 |  Loss: (0.2142) |  Loss2: (0.0000) | Acc: (92.00%) (34513/37248)
Epoch: 101 | Batch_idx: 300 |  Loss: (0.2147) |  Loss2: (0.0000) | Acc: (92.00%) (35701/38528)
Epoch: 101 | Batch_idx: 310 |  Loss: (0.2146) |  Loss2: (0.0000) | Acc: (92.00%) (36886/39808)
Epoch: 101 | Batch_idx: 320 |  Loss: (0.2147) |  Loss2: (0.0000) | Acc: (92.00%) (38077/41088)
Epoch: 101 | Batch_idx: 330 |  Loss: (0.2146) |  Loss2: (0.0000) | Acc: (92.00%) (39255/42368)
Epoch: 101 | Batch_idx: 340 |  Loss: (0.2148) |  Loss2: (0.0000) | Acc: (92.00%) (40445/43648)
Epoch: 101 | Batch_idx: 350 |  Loss: (0.2148) |  Loss2: (0.0000) | Acc: (92.00%) (41627/44928)
Epoch: 101 | Batch_idx: 360 |  Loss: (0.2146) |  Loss2: (0.0000) | Acc: (92.00%) (42824/46208)
Epoch: 101 | Batch_idx: 370 |  Loss: (0.2147) |  Loss2: (0.0000) | Acc: (92.00%) (44010/47488)
Epoch: 101 | Batch_idx: 380 |  Loss: (0.2146) |  Loss2: (0.0000) | Acc: (92.00%) (45200/48768)
Epoch: 101 | Batch_idx: 390 |  Loss: (0.2154) |  Loss2: (0.0000) | Acc: (92.00%) (46324/50000)
# TEST : Loss: (0.4416) | Acc: (86.00%) (8633/10000)
percent tensor([0.5699, 0.4301], device='cuda:0')
percent tensor([0.5602, 0.4398], device='cuda:0')
percent tensor([0.5492, 0.4508], device='cuda:0')
percent tensor([0.6118, 0.3882], device='cuda:0')
percent tensor([0.5743, 0.4257], device='cuda:0')
percent tensor([0.6890, 0.3110], device='cuda:0')
percent tensor([0.7082, 0.2918], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 102 | Batch_idx: 0 |  Loss: (0.2495) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 102 | Batch_idx: 10 |  Loss: (0.1939) |  Loss2: (0.0000) | Acc: (93.00%) (1314/1408)
Epoch: 102 | Batch_idx: 20 |  Loss: (0.2099) |  Loss2: (0.0000) | Acc: (92.00%) (2484/2688)
Epoch: 102 | Batch_idx: 30 |  Loss: (0.2097) |  Loss2: (0.0000) | Acc: (92.00%) (3663/3968)
Epoch: 102 | Batch_idx: 40 |  Loss: (0.2126) |  Loss2: (0.0000) | Acc: (92.00%) (4847/5248)
Epoch: 102 | Batch_idx: 50 |  Loss: (0.2094) |  Loss2: (0.0000) | Acc: (92.00%) (6036/6528)
Epoch: 102 | Batch_idx: 60 |  Loss: (0.2070) |  Loss2: (0.0000) | Acc: (92.00%) (7231/7808)
Epoch: 102 | Batch_idx: 70 |  Loss: (0.2051) |  Loss2: (0.0000) | Acc: (92.00%) (8424/9088)
Epoch: 102 | Batch_idx: 80 |  Loss: (0.2056) |  Loss2: (0.0000) | Acc: (92.00%) (9611/10368)
Epoch: 102 | Batch_idx: 90 |  Loss: (0.2062) |  Loss2: (0.0000) | Acc: (92.00%) (10794/11648)
Epoch: 102 | Batch_idx: 100 |  Loss: (0.2067) |  Loss2: (0.0000) | Acc: (92.00%) (11978/12928)
Epoch: 102 | Batch_idx: 110 |  Loss: (0.2063) |  Loss2: (0.0000) | Acc: (92.00%) (13177/14208)
Epoch: 102 | Batch_idx: 120 |  Loss: (0.2065) |  Loss2: (0.0000) | Acc: (92.00%) (14369/15488)
Epoch: 102 | Batch_idx: 130 |  Loss: (0.2092) |  Loss2: (0.0000) | Acc: (92.00%) (15549/16768)
Epoch: 102 | Batch_idx: 140 |  Loss: (0.2092) |  Loss2: (0.0000) | Acc: (92.00%) (16743/18048)
Epoch: 102 | Batch_idx: 150 |  Loss: (0.2102) |  Loss2: (0.0000) | Acc: (92.00%) (17934/19328)
Epoch: 102 | Batch_idx: 160 |  Loss: (0.2096) |  Loss2: (0.0000) | Acc: (92.00%) (19114/20608)
Epoch: 102 | Batch_idx: 170 |  Loss: (0.2108) |  Loss2: (0.0000) | Acc: (92.00%) (20301/21888)
Epoch: 102 | Batch_idx: 180 |  Loss: (0.2110) |  Loss2: (0.0000) | Acc: (92.00%) (21486/23168)
Epoch: 102 | Batch_idx: 190 |  Loss: (0.2102) |  Loss2: (0.0000) | Acc: (92.00%) (22686/24448)
Epoch: 102 | Batch_idx: 200 |  Loss: (0.2092) |  Loss2: (0.0000) | Acc: (92.00%) (23879/25728)
Epoch: 102 | Batch_idx: 210 |  Loss: (0.2104) |  Loss2: (0.0000) | Acc: (92.00%) (25064/27008)
Epoch: 102 | Batch_idx: 220 |  Loss: (0.2123) |  Loss2: (0.0000) | Acc: (92.00%) (26223/28288)
Epoch: 102 | Batch_idx: 230 |  Loss: (0.2131) |  Loss2: (0.0000) | Acc: (92.00%) (27406/29568)
Epoch: 102 | Batch_idx: 240 |  Loss: (0.2140) |  Loss2: (0.0000) | Acc: (92.00%) (28586/30848)
Epoch: 102 | Batch_idx: 250 |  Loss: (0.2150) |  Loss2: (0.0000) | Acc: (92.00%) (29764/32128)
Epoch: 102 | Batch_idx: 260 |  Loss: (0.2151) |  Loss2: (0.0000) | Acc: (92.00%) (30941/33408)
Epoch: 102 | Batch_idx: 270 |  Loss: (0.2146) |  Loss2: (0.0000) | Acc: (92.00%) (32127/34688)
Epoch: 102 | Batch_idx: 280 |  Loss: (0.2150) |  Loss2: (0.0000) | Acc: (92.00%) (33317/35968)
Epoch: 102 | Batch_idx: 290 |  Loss: (0.2149) |  Loss2: (0.0000) | Acc: (92.00%) (34505/37248)
Epoch: 102 | Batch_idx: 300 |  Loss: (0.2144) |  Loss2: (0.0000) | Acc: (92.00%) (35694/38528)
Epoch: 102 | Batch_idx: 310 |  Loss: (0.2141) |  Loss2: (0.0000) | Acc: (92.00%) (36879/39808)
Epoch: 102 | Batch_idx: 320 |  Loss: (0.2143) |  Loss2: (0.0000) | Acc: (92.00%) (38066/41088)
Epoch: 102 | Batch_idx: 330 |  Loss: (0.2143) |  Loss2: (0.0000) | Acc: (92.00%) (39255/42368)
Epoch: 102 | Batch_idx: 340 |  Loss: (0.2143) |  Loss2: (0.0000) | Acc: (92.00%) (40439/43648)
Epoch: 102 | Batch_idx: 350 |  Loss: (0.2144) |  Loss2: (0.0000) | Acc: (92.00%) (41611/44928)
Epoch: 102 | Batch_idx: 360 |  Loss: (0.2142) |  Loss2: (0.0000) | Acc: (92.00%) (42804/46208)
Epoch: 102 | Batch_idx: 370 |  Loss: (0.2143) |  Loss2: (0.0000) | Acc: (92.00%) (43989/47488)
Epoch: 102 | Batch_idx: 380 |  Loss: (0.2147) |  Loss2: (0.0000) | Acc: (92.00%) (45168/48768)
Epoch: 102 | Batch_idx: 390 |  Loss: (0.2150) |  Loss2: (0.0000) | Acc: (92.00%) (46307/50000)
# TEST : Loss: (0.5002) | Acc: (84.00%) (8480/10000)
percent tensor([0.5701, 0.4299], device='cuda:0')
percent tensor([0.5599, 0.4401], device='cuda:0')
percent tensor([0.5495, 0.4505], device='cuda:0')
percent tensor([0.6112, 0.3888], device='cuda:0')
percent tensor([0.5738, 0.4262], device='cuda:0')
percent tensor([0.6888, 0.3112], device='cuda:0')
percent tensor([0.7062, 0.2938], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 103 | Batch_idx: 0 |  Loss: (0.1979) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 103 | Batch_idx: 10 |  Loss: (0.2084) |  Loss2: (0.0000) | Acc: (92.00%) (1308/1408)
Epoch: 103 | Batch_idx: 20 |  Loss: (0.1986) |  Loss2: (0.0000) | Acc: (93.00%) (2507/2688)
Epoch: 103 | Batch_idx: 30 |  Loss: (0.1924) |  Loss2: (0.0000) | Acc: (93.00%) (3704/3968)
Epoch: 103 | Batch_idx: 40 |  Loss: (0.1922) |  Loss2: (0.0000) | Acc: (93.00%) (4904/5248)
Epoch: 103 | Batch_idx: 50 |  Loss: (0.1986) |  Loss2: (0.0000) | Acc: (93.00%) (6086/6528)
Epoch: 103 | Batch_idx: 60 |  Loss: (0.1992) |  Loss2: (0.0000) | Acc: (93.00%) (7276/7808)
Epoch: 103 | Batch_idx: 70 |  Loss: (0.1992) |  Loss2: (0.0000) | Acc: (93.00%) (8480/9088)
Epoch: 103 | Batch_idx: 80 |  Loss: (0.2005) |  Loss2: (0.0000) | Acc: (93.00%) (9663/10368)
Epoch: 103 | Batch_idx: 90 |  Loss: (0.2003) |  Loss2: (0.0000) | Acc: (93.00%) (10853/11648)
Epoch: 103 | Batch_idx: 100 |  Loss: (0.1992) |  Loss2: (0.0000) | Acc: (93.00%) (12061/12928)
Epoch: 103 | Batch_idx: 110 |  Loss: (0.1990) |  Loss2: (0.0000) | Acc: (93.00%) (13256/14208)
Epoch: 103 | Batch_idx: 120 |  Loss: (0.1989) |  Loss2: (0.0000) | Acc: (93.00%) (14459/15488)
Epoch: 103 | Batch_idx: 130 |  Loss: (0.1972) |  Loss2: (0.0000) | Acc: (93.00%) (15663/16768)
Epoch: 103 | Batch_idx: 140 |  Loss: (0.1974) |  Loss2: (0.0000) | Acc: (93.00%) (16862/18048)
Epoch: 103 | Batch_idx: 150 |  Loss: (0.1986) |  Loss2: (0.0000) | Acc: (93.00%) (18045/19328)
Epoch: 103 | Batch_idx: 160 |  Loss: (0.1991) |  Loss2: (0.0000) | Acc: (93.00%) (19235/20608)
Epoch: 103 | Batch_idx: 170 |  Loss: (0.1989) |  Loss2: (0.0000) | Acc: (93.00%) (20432/21888)
Epoch: 103 | Batch_idx: 180 |  Loss: (0.2001) |  Loss2: (0.0000) | Acc: (93.00%) (21622/23168)
Epoch: 103 | Batch_idx: 190 |  Loss: (0.2004) |  Loss2: (0.0000) | Acc: (93.00%) (22818/24448)
Epoch: 103 | Batch_idx: 200 |  Loss: (0.2008) |  Loss2: (0.0000) | Acc: (93.00%) (24009/25728)
Epoch: 103 | Batch_idx: 210 |  Loss: (0.2010) |  Loss2: (0.0000) | Acc: (93.00%) (25204/27008)
Epoch: 103 | Batch_idx: 220 |  Loss: (0.2010) |  Loss2: (0.0000) | Acc: (93.00%) (26395/28288)
Epoch: 103 | Batch_idx: 230 |  Loss: (0.2008) |  Loss2: (0.0000) | Acc: (93.00%) (27586/29568)
Epoch: 103 | Batch_idx: 240 |  Loss: (0.2009) |  Loss2: (0.0000) | Acc: (93.00%) (28788/30848)
Epoch: 103 | Batch_idx: 250 |  Loss: (0.2002) |  Loss2: (0.0000) | Acc: (93.00%) (29982/32128)
Epoch: 103 | Batch_idx: 260 |  Loss: (0.2014) |  Loss2: (0.0000) | Acc: (93.00%) (31165/33408)
Epoch: 103 | Batch_idx: 270 |  Loss: (0.2014) |  Loss2: (0.0000) | Acc: (93.00%) (32343/34688)
Epoch: 103 | Batch_idx: 280 |  Loss: (0.2025) |  Loss2: (0.0000) | Acc: (93.00%) (33518/35968)
Epoch: 103 | Batch_idx: 290 |  Loss: (0.2033) |  Loss2: (0.0000) | Acc: (93.00%) (34702/37248)
Epoch: 103 | Batch_idx: 300 |  Loss: (0.2039) |  Loss2: (0.0000) | Acc: (93.00%) (35887/38528)
Epoch: 103 | Batch_idx: 310 |  Loss: (0.2050) |  Loss2: (0.0000) | Acc: (93.00%) (37052/39808)
Epoch: 103 | Batch_idx: 320 |  Loss: (0.2060) |  Loss2: (0.0000) | Acc: (93.00%) (38239/41088)
Epoch: 103 | Batch_idx: 330 |  Loss: (0.2069) |  Loss2: (0.0000) | Acc: (93.00%) (39415/42368)
Epoch: 103 | Batch_idx: 340 |  Loss: (0.2072) |  Loss2: (0.0000) | Acc: (93.00%) (40602/43648)
Epoch: 103 | Batch_idx: 350 |  Loss: (0.2071) |  Loss2: (0.0000) | Acc: (93.00%) (41784/44928)
Epoch: 103 | Batch_idx: 360 |  Loss: (0.2072) |  Loss2: (0.0000) | Acc: (92.00%) (42965/46208)
Epoch: 103 | Batch_idx: 370 |  Loss: (0.2075) |  Loss2: (0.0000) | Acc: (92.00%) (44148/47488)
Epoch: 103 | Batch_idx: 380 |  Loss: (0.2073) |  Loss2: (0.0000) | Acc: (92.00%) (45339/48768)
Epoch: 103 | Batch_idx: 390 |  Loss: (0.2078) |  Loss2: (0.0000) | Acc: (92.00%) (46472/50000)
# TEST : Loss: (0.4583) | Acc: (85.00%) (8598/10000)
percent tensor([0.5694, 0.4306], device='cuda:0')
percent tensor([0.5607, 0.4393], device='cuda:0')
percent tensor([0.5500, 0.4500], device='cuda:0')
percent tensor([0.6108, 0.3892], device='cuda:0')
percent tensor([0.5730, 0.4270], device='cuda:0')
percent tensor([0.6884, 0.3116], device='cuda:0')
percent tensor([0.7096, 0.2904], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 104 | Batch_idx: 0 |  Loss: (0.1658) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 104 | Batch_idx: 10 |  Loss: (0.1829) |  Loss2: (0.0000) | Acc: (94.00%) (1326/1408)
Epoch: 104 | Batch_idx: 20 |  Loss: (0.1920) |  Loss2: (0.0000) | Acc: (93.00%) (2509/2688)
Epoch: 104 | Batch_idx: 30 |  Loss: (0.1941) |  Loss2: (0.0000) | Acc: (93.00%) (3713/3968)
Epoch: 104 | Batch_idx: 40 |  Loss: (0.1914) |  Loss2: (0.0000) | Acc: (93.00%) (4908/5248)
Epoch: 104 | Batch_idx: 50 |  Loss: (0.1914) |  Loss2: (0.0000) | Acc: (93.00%) (6104/6528)
Epoch: 104 | Batch_idx: 60 |  Loss: (0.1877) |  Loss2: (0.0000) | Acc: (93.00%) (7316/7808)
Epoch: 104 | Batch_idx: 70 |  Loss: (0.1888) |  Loss2: (0.0000) | Acc: (93.00%) (8513/9088)
Epoch: 104 | Batch_idx: 80 |  Loss: (0.1909) |  Loss2: (0.0000) | Acc: (93.00%) (9700/10368)
Epoch: 104 | Batch_idx: 90 |  Loss: (0.1921) |  Loss2: (0.0000) | Acc: (93.00%) (10889/11648)
Epoch: 104 | Batch_idx: 100 |  Loss: (0.1948) |  Loss2: (0.0000) | Acc: (93.00%) (12066/12928)
Epoch: 104 | Batch_idx: 110 |  Loss: (0.1924) |  Loss2: (0.0000) | Acc: (93.00%) (13270/14208)
Epoch: 104 | Batch_idx: 120 |  Loss: (0.1940) |  Loss2: (0.0000) | Acc: (93.00%) (14463/15488)
Epoch: 104 | Batch_idx: 130 |  Loss: (0.1948) |  Loss2: (0.0000) | Acc: (93.00%) (15646/16768)
Epoch: 104 | Batch_idx: 140 |  Loss: (0.1955) |  Loss2: (0.0000) | Acc: (93.00%) (16838/18048)
Epoch: 104 | Batch_idx: 150 |  Loss: (0.1969) |  Loss2: (0.0000) | Acc: (93.00%) (18029/19328)
Epoch: 104 | Batch_idx: 160 |  Loss: (0.1965) |  Loss2: (0.0000) | Acc: (93.00%) (19225/20608)
Epoch: 104 | Batch_idx: 170 |  Loss: (0.1966) |  Loss2: (0.0000) | Acc: (93.00%) (20429/21888)
Epoch: 104 | Batch_idx: 180 |  Loss: (0.1970) |  Loss2: (0.0000) | Acc: (93.00%) (21612/23168)
Epoch: 104 | Batch_idx: 190 |  Loss: (0.1964) |  Loss2: (0.0000) | Acc: (93.00%) (22808/24448)
Epoch: 104 | Batch_idx: 200 |  Loss: (0.1968) |  Loss2: (0.0000) | Acc: (93.00%) (23996/25728)
Epoch: 104 | Batch_idx: 210 |  Loss: (0.1970) |  Loss2: (0.0000) | Acc: (93.00%) (25189/27008)
Epoch: 104 | Batch_idx: 220 |  Loss: (0.1969) |  Loss2: (0.0000) | Acc: (93.00%) (26389/28288)
Epoch: 104 | Batch_idx: 230 |  Loss: (0.1966) |  Loss2: (0.0000) | Acc: (93.00%) (27586/29568)
Epoch: 104 | Batch_idx: 240 |  Loss: (0.1966) |  Loss2: (0.0000) | Acc: (93.00%) (28781/30848)
Epoch: 104 | Batch_idx: 250 |  Loss: (0.1964) |  Loss2: (0.0000) | Acc: (93.00%) (29974/32128)
Epoch: 104 | Batch_idx: 260 |  Loss: (0.1977) |  Loss2: (0.0000) | Acc: (93.00%) (31150/33408)
Epoch: 104 | Batch_idx: 270 |  Loss: (0.1971) |  Loss2: (0.0000) | Acc: (93.00%) (32357/34688)
Epoch: 104 | Batch_idx: 280 |  Loss: (0.1979) |  Loss2: (0.0000) | Acc: (93.00%) (33532/35968)
Epoch: 104 | Batch_idx: 290 |  Loss: (0.1981) |  Loss2: (0.0000) | Acc: (93.00%) (34713/37248)
Epoch: 104 | Batch_idx: 300 |  Loss: (0.1983) |  Loss2: (0.0000) | Acc: (93.00%) (35905/38528)
Epoch: 104 | Batch_idx: 310 |  Loss: (0.1983) |  Loss2: (0.0000) | Acc: (93.00%) (37097/39808)
Epoch: 104 | Batch_idx: 320 |  Loss: (0.1983) |  Loss2: (0.0000) | Acc: (93.00%) (38287/41088)
Epoch: 104 | Batch_idx: 330 |  Loss: (0.1989) |  Loss2: (0.0000) | Acc: (93.00%) (39459/42368)
Epoch: 104 | Batch_idx: 340 |  Loss: (0.1992) |  Loss2: (0.0000) | Acc: (93.00%) (40643/43648)
Epoch: 104 | Batch_idx: 350 |  Loss: (0.1996) |  Loss2: (0.0000) | Acc: (93.00%) (41822/44928)
Epoch: 104 | Batch_idx: 360 |  Loss: (0.1999) |  Loss2: (0.0000) | Acc: (93.00%) (43013/46208)
Epoch: 104 | Batch_idx: 370 |  Loss: (0.1998) |  Loss2: (0.0000) | Acc: (93.00%) (44200/47488)
Epoch: 104 | Batch_idx: 380 |  Loss: (0.2002) |  Loss2: (0.0000) | Acc: (93.00%) (45384/48768)
Epoch: 104 | Batch_idx: 390 |  Loss: (0.2009) |  Loss2: (0.0000) | Acc: (93.00%) (46524/50000)
# TEST : Loss: (0.4244) | Acc: (86.00%) (8646/10000)
percent tensor([0.5703, 0.4297], device='cuda:0')
percent tensor([0.5602, 0.4398], device='cuda:0')
percent tensor([0.5496, 0.4504], device='cuda:0')
percent tensor([0.6111, 0.3889], device='cuda:0')
percent tensor([0.5734, 0.4266], device='cuda:0')
percent tensor([0.6889, 0.3111], device='cuda:0')
percent tensor([0.7053, 0.2947], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 105 | Batch_idx: 0 |  Loss: (0.2210) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 105 | Batch_idx: 10 |  Loss: (0.1955) |  Loss2: (0.0000) | Acc: (93.00%) (1315/1408)
Epoch: 105 | Batch_idx: 20 |  Loss: (0.2085) |  Loss2: (0.0000) | Acc: (92.00%) (2487/2688)
Epoch: 105 | Batch_idx: 30 |  Loss: (0.2152) |  Loss2: (0.0000) | Acc: (92.00%) (3655/3968)
Epoch: 105 | Batch_idx: 40 |  Loss: (0.2316) |  Loss2: (0.0000) | Acc: (91.00%) (4812/5248)
Epoch: 105 | Batch_idx: 50 |  Loss: (0.2272) |  Loss2: (0.0000) | Acc: (92.00%) (6008/6528)
Epoch: 105 | Batch_idx: 60 |  Loss: (0.2323) |  Loss2: (0.0000) | Acc: (91.00%) (7165/7808)
Epoch: 105 | Batch_idx: 70 |  Loss: (0.2320) |  Loss2: (0.0000) | Acc: (91.00%) (8329/9088)
Epoch: 105 | Batch_idx: 80 |  Loss: (0.2322) |  Loss2: (0.0000) | Acc: (91.00%) (9507/10368)
Epoch: 105 | Batch_idx: 90 |  Loss: (0.2324) |  Loss2: (0.0000) | Acc: (91.00%) (10683/11648)
Epoch: 105 | Batch_idx: 100 |  Loss: (0.2307) |  Loss2: (0.0000) | Acc: (91.00%) (11858/12928)
Epoch: 105 | Batch_idx: 110 |  Loss: (0.2324) |  Loss2: (0.0000) | Acc: (91.00%) (13025/14208)
Epoch: 105 | Batch_idx: 120 |  Loss: (0.2329) |  Loss2: (0.0000) | Acc: (91.00%) (14189/15488)
Epoch: 105 | Batch_idx: 130 |  Loss: (0.2310) |  Loss2: (0.0000) | Acc: (91.00%) (15376/16768)
Epoch: 105 | Batch_idx: 140 |  Loss: (0.2313) |  Loss2: (0.0000) | Acc: (91.00%) (16550/18048)
Epoch: 105 | Batch_idx: 150 |  Loss: (0.2315) |  Loss2: (0.0000) | Acc: (91.00%) (17728/19328)
Epoch: 105 | Batch_idx: 160 |  Loss: (0.2320) |  Loss2: (0.0000) | Acc: (91.00%) (18897/20608)
Epoch: 105 | Batch_idx: 170 |  Loss: (0.2306) |  Loss2: (0.0000) | Acc: (91.00%) (20082/21888)
Epoch: 105 | Batch_idx: 180 |  Loss: (0.2306) |  Loss2: (0.0000) | Acc: (91.00%) (21274/23168)
Epoch: 105 | Batch_idx: 190 |  Loss: (0.2310) |  Loss2: (0.0000) | Acc: (91.00%) (22438/24448)
Epoch: 105 | Batch_idx: 200 |  Loss: (0.2302) |  Loss2: (0.0000) | Acc: (91.00%) (23629/25728)
Epoch: 105 | Batch_idx: 210 |  Loss: (0.2295) |  Loss2: (0.0000) | Acc: (91.00%) (24820/27008)
Epoch: 105 | Batch_idx: 220 |  Loss: (0.2304) |  Loss2: (0.0000) | Acc: (91.00%) (25981/28288)
Epoch: 105 | Batch_idx: 230 |  Loss: (0.2299) |  Loss2: (0.0000) | Acc: (91.00%) (27151/29568)
Epoch: 105 | Batch_idx: 240 |  Loss: (0.2290) |  Loss2: (0.0000) | Acc: (91.00%) (28338/30848)
Epoch: 105 | Batch_idx: 250 |  Loss: (0.2285) |  Loss2: (0.0000) | Acc: (91.00%) (29515/32128)
Epoch: 105 | Batch_idx: 260 |  Loss: (0.2275) |  Loss2: (0.0000) | Acc: (91.00%) (30700/33408)
Epoch: 105 | Batch_idx: 270 |  Loss: (0.2274) |  Loss2: (0.0000) | Acc: (91.00%) (31881/34688)
Epoch: 105 | Batch_idx: 280 |  Loss: (0.2271) |  Loss2: (0.0000) | Acc: (91.00%) (33060/35968)
Epoch: 105 | Batch_idx: 290 |  Loss: (0.2263) |  Loss2: (0.0000) | Acc: (91.00%) (34249/37248)
Epoch: 105 | Batch_idx: 300 |  Loss: (0.2260) |  Loss2: (0.0000) | Acc: (91.00%) (35431/38528)
Epoch: 105 | Batch_idx: 310 |  Loss: (0.2259) |  Loss2: (0.0000) | Acc: (91.00%) (36618/39808)
Epoch: 105 | Batch_idx: 320 |  Loss: (0.2254) |  Loss2: (0.0000) | Acc: (92.00%) (37819/41088)
Epoch: 105 | Batch_idx: 330 |  Loss: (0.2251) |  Loss2: (0.0000) | Acc: (92.00%) (39002/42368)
Epoch: 105 | Batch_idx: 340 |  Loss: (0.2241) |  Loss2: (0.0000) | Acc: (92.00%) (40189/43648)
Epoch: 105 | Batch_idx: 350 |  Loss: (0.2233) |  Loss2: (0.0000) | Acc: (92.00%) (41386/44928)
Epoch: 105 | Batch_idx: 360 |  Loss: (0.2238) |  Loss2: (0.0000) | Acc: (92.00%) (42555/46208)
Epoch: 105 | Batch_idx: 370 |  Loss: (0.2242) |  Loss2: (0.0000) | Acc: (92.00%) (43717/47488)
Epoch: 105 | Batch_idx: 380 |  Loss: (0.2235) |  Loss2: (0.0000) | Acc: (92.00%) (44901/48768)
Epoch: 105 | Batch_idx: 390 |  Loss: (0.2244) |  Loss2: (0.0000) | Acc: (92.00%) (46015/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_105.pth.tar'
# TEST : Loss: (0.4269) | Acc: (86.00%) (8647/10000)
percent tensor([0.5831, 0.4169], device='cuda:0')
percent tensor([0.5667, 0.4333], device='cuda:0')
percent tensor([0.5439, 0.4561], device='cuda:0')
percent tensor([0.6239, 0.3761], device='cuda:0')
percent tensor([0.5826, 0.4174], device='cuda:0')
percent tensor([0.6681, 0.3319], device='cuda:0')
percent tensor([0.6692, 0.3308], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 106 | Batch_idx: 0 |  Loss: (0.3481) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 106 | Batch_idx: 10 |  Loss: (0.2179) |  Loss2: (0.0000) | Acc: (92.00%) (1307/1408)
Epoch: 106 | Batch_idx: 20 |  Loss: (0.2140) |  Loss2: (0.0000) | Acc: (92.00%) (2495/2688)
Epoch: 106 | Batch_idx: 30 |  Loss: (0.2051) |  Loss2: (0.0000) | Acc: (92.00%) (3690/3968)
Epoch: 106 | Batch_idx: 40 |  Loss: (0.2093) |  Loss2: (0.0000) | Acc: (92.00%) (4875/5248)
Epoch: 106 | Batch_idx: 50 |  Loss: (0.2146) |  Loss2: (0.0000) | Acc: (92.00%) (6049/6528)
Epoch: 106 | Batch_idx: 60 |  Loss: (0.2119) |  Loss2: (0.0000) | Acc: (92.00%) (7239/7808)
Epoch: 106 | Batch_idx: 70 |  Loss: (0.2124) |  Loss2: (0.0000) | Acc: (92.00%) (8421/9088)
Epoch: 106 | Batch_idx: 80 |  Loss: (0.2102) |  Loss2: (0.0000) | Acc: (92.00%) (9616/10368)
Epoch: 106 | Batch_idx: 90 |  Loss: (0.2076) |  Loss2: (0.0000) | Acc: (92.00%) (10810/11648)
Epoch: 106 | Batch_idx: 100 |  Loss: (0.2068) |  Loss2: (0.0000) | Acc: (92.00%) (11999/12928)
Epoch: 106 | Batch_idx: 110 |  Loss: (0.2075) |  Loss2: (0.0000) | Acc: (92.00%) (13183/14208)
Epoch: 106 | Batch_idx: 120 |  Loss: (0.2073) |  Loss2: (0.0000) | Acc: (92.00%) (14362/15488)
Epoch: 106 | Batch_idx: 130 |  Loss: (0.2064) |  Loss2: (0.0000) | Acc: (92.00%) (15550/16768)
Epoch: 106 | Batch_idx: 140 |  Loss: (0.2059) |  Loss2: (0.0000) | Acc: (92.00%) (16740/18048)
Epoch: 106 | Batch_idx: 150 |  Loss: (0.2081) |  Loss2: (0.0000) | Acc: (92.00%) (17922/19328)
Epoch: 106 | Batch_idx: 160 |  Loss: (0.2072) |  Loss2: (0.0000) | Acc: (92.00%) (19124/20608)
Epoch: 106 | Batch_idx: 170 |  Loss: (0.2056) |  Loss2: (0.0000) | Acc: (92.00%) (20321/21888)
Epoch: 106 | Batch_idx: 180 |  Loss: (0.2072) |  Loss2: (0.0000) | Acc: (92.00%) (21488/23168)
Epoch: 106 | Batch_idx: 190 |  Loss: (0.2080) |  Loss2: (0.0000) | Acc: (92.00%) (22668/24448)
Epoch: 106 | Batch_idx: 200 |  Loss: (0.2085) |  Loss2: (0.0000) | Acc: (92.00%) (23844/25728)
Epoch: 106 | Batch_idx: 210 |  Loss: (0.2088) |  Loss2: (0.0000) | Acc: (92.00%) (25036/27008)
Epoch: 106 | Batch_idx: 220 |  Loss: (0.2096) |  Loss2: (0.0000) | Acc: (92.00%) (26217/28288)
Epoch: 106 | Batch_idx: 230 |  Loss: (0.2097) |  Loss2: (0.0000) | Acc: (92.00%) (27411/29568)
Epoch: 106 | Batch_idx: 240 |  Loss: (0.2103) |  Loss2: (0.0000) | Acc: (92.00%) (28596/30848)
Epoch: 106 | Batch_idx: 250 |  Loss: (0.2101) |  Loss2: (0.0000) | Acc: (92.00%) (29786/32128)
Epoch: 106 | Batch_idx: 260 |  Loss: (0.2103) |  Loss2: (0.0000) | Acc: (92.00%) (30973/33408)
Epoch: 106 | Batch_idx: 270 |  Loss: (0.2099) |  Loss2: (0.0000) | Acc: (92.00%) (32160/34688)
Epoch: 106 | Batch_idx: 280 |  Loss: (0.2100) |  Loss2: (0.0000) | Acc: (92.00%) (33345/35968)
Epoch: 106 | Batch_idx: 290 |  Loss: (0.2092) |  Loss2: (0.0000) | Acc: (92.00%) (34542/37248)
Epoch: 106 | Batch_idx: 300 |  Loss: (0.2095) |  Loss2: (0.0000) | Acc: (92.00%) (35728/38528)
Epoch: 106 | Batch_idx: 310 |  Loss: (0.2092) |  Loss2: (0.0000) | Acc: (92.00%) (36917/39808)
Epoch: 106 | Batch_idx: 320 |  Loss: (0.2096) |  Loss2: (0.0000) | Acc: (92.00%) (38094/41088)
Epoch: 106 | Batch_idx: 330 |  Loss: (0.2094) |  Loss2: (0.0000) | Acc: (92.00%) (39286/42368)
Epoch: 106 | Batch_idx: 340 |  Loss: (0.2096) |  Loss2: (0.0000) | Acc: (92.00%) (40476/43648)
Epoch: 106 | Batch_idx: 350 |  Loss: (0.2091) |  Loss2: (0.0000) | Acc: (92.00%) (41667/44928)
Epoch: 106 | Batch_idx: 360 |  Loss: (0.2091) |  Loss2: (0.0000) | Acc: (92.00%) (42851/46208)
Epoch: 106 | Batch_idx: 370 |  Loss: (0.2092) |  Loss2: (0.0000) | Acc: (92.00%) (44043/47488)
Epoch: 106 | Batch_idx: 380 |  Loss: (0.2088) |  Loss2: (0.0000) | Acc: (92.00%) (45239/48768)
Epoch: 106 | Batch_idx: 390 |  Loss: (0.2087) |  Loss2: (0.0000) | Acc: (92.00%) (46390/50000)
# TEST : Loss: (0.4134) | Acc: (86.00%) (8670/10000)
percent tensor([0.5847, 0.4153], device='cuda:0')
percent tensor([0.5716, 0.4284], device='cuda:0')
percent tensor([0.5438, 0.4562], device='cuda:0')
percent tensor([0.6216, 0.3784], device='cuda:0')
percent tensor([0.5878, 0.4122], device='cuda:0')
percent tensor([0.6710, 0.3290], device='cuda:0')
percent tensor([0.6668, 0.3332], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 107 | Batch_idx: 0 |  Loss: (0.1848) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 107 | Batch_idx: 10 |  Loss: (0.2148) |  Loss2: (0.0000) | Acc: (92.00%) (1307/1408)
Epoch: 107 | Batch_idx: 20 |  Loss: (0.2084) |  Loss2: (0.0000) | Acc: (92.00%) (2497/2688)
Epoch: 107 | Batch_idx: 30 |  Loss: (0.2004) |  Loss2: (0.0000) | Acc: (93.00%) (3697/3968)
Epoch: 107 | Batch_idx: 40 |  Loss: (0.1987) |  Loss2: (0.0000) | Acc: (93.00%) (4891/5248)
Epoch: 107 | Batch_idx: 50 |  Loss: (0.1986) |  Loss2: (0.0000) | Acc: (93.00%) (6083/6528)
Epoch: 107 | Batch_idx: 60 |  Loss: (0.2026) |  Loss2: (0.0000) | Acc: (93.00%) (7263/7808)
Epoch: 107 | Batch_idx: 70 |  Loss: (0.1999) |  Loss2: (0.0000) | Acc: (93.00%) (8469/9088)
Epoch: 107 | Batch_idx: 80 |  Loss: (0.1986) |  Loss2: (0.0000) | Acc: (93.00%) (9660/10368)
Epoch: 107 | Batch_idx: 90 |  Loss: (0.2001) |  Loss2: (0.0000) | Acc: (93.00%) (10839/11648)
Epoch: 107 | Batch_idx: 100 |  Loss: (0.1975) |  Loss2: (0.0000) | Acc: (93.00%) (12045/12928)
Epoch: 107 | Batch_idx: 110 |  Loss: (0.1981) |  Loss2: (0.0000) | Acc: (93.00%) (13233/14208)
Epoch: 107 | Batch_idx: 120 |  Loss: (0.1964) |  Loss2: (0.0000) | Acc: (93.00%) (14437/15488)
Epoch: 107 | Batch_idx: 130 |  Loss: (0.1973) |  Loss2: (0.0000) | Acc: (93.00%) (15627/16768)
Epoch: 107 | Batch_idx: 140 |  Loss: (0.1991) |  Loss2: (0.0000) | Acc: (93.00%) (16813/18048)
Epoch: 107 | Batch_idx: 150 |  Loss: (0.2006) |  Loss2: (0.0000) | Acc: (93.00%) (17993/19328)
Epoch: 107 | Batch_idx: 160 |  Loss: (0.2000) |  Loss2: (0.0000) | Acc: (93.00%) (19189/20608)
Epoch: 107 | Batch_idx: 170 |  Loss: (0.1993) |  Loss2: (0.0000) | Acc: (93.00%) (20390/21888)
Epoch: 107 | Batch_idx: 180 |  Loss: (0.1975) |  Loss2: (0.0000) | Acc: (93.00%) (21602/23168)
Epoch: 107 | Batch_idx: 190 |  Loss: (0.1977) |  Loss2: (0.0000) | Acc: (93.00%) (22792/24448)
Epoch: 107 | Batch_idx: 200 |  Loss: (0.1977) |  Loss2: (0.0000) | Acc: (93.00%) (23981/25728)
Epoch: 107 | Batch_idx: 210 |  Loss: (0.1968) |  Loss2: (0.0000) | Acc: (93.00%) (25185/27008)
Epoch: 107 | Batch_idx: 220 |  Loss: (0.1978) |  Loss2: (0.0000) | Acc: (93.00%) (26372/28288)
Epoch: 107 | Batch_idx: 230 |  Loss: (0.1974) |  Loss2: (0.0000) | Acc: (93.00%) (27563/29568)
Epoch: 107 | Batch_idx: 240 |  Loss: (0.1962) |  Loss2: (0.0000) | Acc: (93.00%) (28776/30848)
Epoch: 107 | Batch_idx: 250 |  Loss: (0.1960) |  Loss2: (0.0000) | Acc: (93.00%) (29967/32128)
Epoch: 107 | Batch_idx: 260 |  Loss: (0.1965) |  Loss2: (0.0000) | Acc: (93.00%) (31148/33408)
Epoch: 107 | Batch_idx: 270 |  Loss: (0.1963) |  Loss2: (0.0000) | Acc: (93.00%) (32351/34688)
Epoch: 107 | Batch_idx: 280 |  Loss: (0.1966) |  Loss2: (0.0000) | Acc: (93.00%) (33543/35968)
Epoch: 107 | Batch_idx: 290 |  Loss: (0.1977) |  Loss2: (0.0000) | Acc: (93.00%) (34728/37248)
Epoch: 107 | Batch_idx: 300 |  Loss: (0.1980) |  Loss2: (0.0000) | Acc: (93.00%) (35920/38528)
Epoch: 107 | Batch_idx: 310 |  Loss: (0.1971) |  Loss2: (0.0000) | Acc: (93.00%) (37125/39808)
Epoch: 107 | Batch_idx: 320 |  Loss: (0.1984) |  Loss2: (0.0000) | Acc: (93.00%) (38295/41088)
Epoch: 107 | Batch_idx: 330 |  Loss: (0.1989) |  Loss2: (0.0000) | Acc: (93.00%) (39483/42368)
Epoch: 107 | Batch_idx: 340 |  Loss: (0.1992) |  Loss2: (0.0000) | Acc: (93.00%) (40670/43648)
Epoch: 107 | Batch_idx: 350 |  Loss: (0.1990) |  Loss2: (0.0000) | Acc: (93.00%) (41865/44928)
Epoch: 107 | Batch_idx: 360 |  Loss: (0.1993) |  Loss2: (0.0000) | Acc: (93.00%) (43060/46208)
Epoch: 107 | Batch_idx: 370 |  Loss: (0.1991) |  Loss2: (0.0000) | Acc: (93.00%) (44260/47488)
Epoch: 107 | Batch_idx: 380 |  Loss: (0.1992) |  Loss2: (0.0000) | Acc: (93.00%) (45448/48768)
Epoch: 107 | Batch_idx: 390 |  Loss: (0.1990) |  Loss2: (0.0000) | Acc: (93.00%) (46599/50000)
# TEST : Loss: (0.4025) | Acc: (87.00%) (8715/10000)
percent tensor([0.5861, 0.4139], device='cuda:0')
percent tensor([0.5708, 0.4292], device='cuda:0')
percent tensor([0.5439, 0.4561], device='cuda:0')
percent tensor([0.6271, 0.3729], device='cuda:0')
percent tensor([0.5876, 0.4124], device='cuda:0')
percent tensor([0.6675, 0.3325], device='cuda:0')
percent tensor([0.6745, 0.3255], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 108 | Batch_idx: 0 |  Loss: (0.1134) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 108 | Batch_idx: 10 |  Loss: (0.1900) |  Loss2: (0.0000) | Acc: (93.00%) (1310/1408)
Epoch: 108 | Batch_idx: 20 |  Loss: (0.2151) |  Loss2: (0.0000) | Acc: (92.00%) (2480/2688)
Epoch: 108 | Batch_idx: 30 |  Loss: (0.2092) |  Loss2: (0.0000) | Acc: (92.00%) (3671/3968)
Epoch: 108 | Batch_idx: 40 |  Loss: (0.2027) |  Loss2: (0.0000) | Acc: (92.00%) (4866/5248)
Epoch: 108 | Batch_idx: 50 |  Loss: (0.1977) |  Loss2: (0.0000) | Acc: (92.00%) (6060/6528)
Epoch: 108 | Batch_idx: 60 |  Loss: (0.2005) |  Loss2: (0.0000) | Acc: (92.00%) (7249/7808)
Epoch: 108 | Batch_idx: 70 |  Loss: (0.2034) |  Loss2: (0.0000) | Acc: (92.00%) (8423/9088)
Epoch: 108 | Batch_idx: 80 |  Loss: (0.2029) |  Loss2: (0.0000) | Acc: (92.00%) (9617/10368)
Epoch: 108 | Batch_idx: 90 |  Loss: (0.2001) |  Loss2: (0.0000) | Acc: (92.00%) (10819/11648)
Epoch: 108 | Batch_idx: 100 |  Loss: (0.2005) |  Loss2: (0.0000) | Acc: (92.00%) (12011/12928)
Epoch: 108 | Batch_idx: 110 |  Loss: (0.1994) |  Loss2: (0.0000) | Acc: (93.00%) (13219/14208)
Epoch: 108 | Batch_idx: 120 |  Loss: (0.1999) |  Loss2: (0.0000) | Acc: (93.00%) (14408/15488)
Epoch: 108 | Batch_idx: 130 |  Loss: (0.2000) |  Loss2: (0.0000) | Acc: (93.00%) (15595/16768)
Epoch: 108 | Batch_idx: 140 |  Loss: (0.2010) |  Loss2: (0.0000) | Acc: (92.00%) (16772/18048)
Epoch: 108 | Batch_idx: 150 |  Loss: (0.2000) |  Loss2: (0.0000) | Acc: (92.00%) (17967/19328)
Epoch: 108 | Batch_idx: 160 |  Loss: (0.1999) |  Loss2: (0.0000) | Acc: (92.00%) (19164/20608)
Epoch: 108 | Batch_idx: 170 |  Loss: (0.1992) |  Loss2: (0.0000) | Acc: (92.00%) (20351/21888)
Epoch: 108 | Batch_idx: 180 |  Loss: (0.1994) |  Loss2: (0.0000) | Acc: (92.00%) (21538/23168)
Epoch: 108 | Batch_idx: 190 |  Loss: (0.1982) |  Loss2: (0.0000) | Acc: (93.00%) (22750/24448)
Epoch: 108 | Batch_idx: 200 |  Loss: (0.1987) |  Loss2: (0.0000) | Acc: (93.00%) (23938/25728)
Epoch: 108 | Batch_idx: 210 |  Loss: (0.1985) |  Loss2: (0.0000) | Acc: (93.00%) (25128/27008)
Epoch: 108 | Batch_idx: 220 |  Loss: (0.1990) |  Loss2: (0.0000) | Acc: (93.00%) (26314/28288)
Epoch: 108 | Batch_idx: 230 |  Loss: (0.1984) |  Loss2: (0.0000) | Acc: (93.00%) (27504/29568)
Epoch: 108 | Batch_idx: 240 |  Loss: (0.1983) |  Loss2: (0.0000) | Acc: (93.00%) (28698/30848)
Epoch: 108 | Batch_idx: 250 |  Loss: (0.1981) |  Loss2: (0.0000) | Acc: (93.00%) (29895/32128)
Epoch: 108 | Batch_idx: 260 |  Loss: (0.1977) |  Loss2: (0.0000) | Acc: (93.00%) (31096/33408)
Epoch: 108 | Batch_idx: 270 |  Loss: (0.1979) |  Loss2: (0.0000) | Acc: (93.00%) (32293/34688)
Epoch: 108 | Batch_idx: 280 |  Loss: (0.1976) |  Loss2: (0.0000) | Acc: (93.00%) (33491/35968)
Epoch: 108 | Batch_idx: 290 |  Loss: (0.1982) |  Loss2: (0.0000) | Acc: (93.00%) (34677/37248)
Epoch: 108 | Batch_idx: 300 |  Loss: (0.1980) |  Loss2: (0.0000) | Acc: (93.00%) (35878/38528)
Epoch: 108 | Batch_idx: 310 |  Loss: (0.1979) |  Loss2: (0.0000) | Acc: (93.00%) (37064/39808)
Epoch: 108 | Batch_idx: 320 |  Loss: (0.1981) |  Loss2: (0.0000) | Acc: (93.00%) (38252/41088)
Epoch: 108 | Batch_idx: 330 |  Loss: (0.1978) |  Loss2: (0.0000) | Acc: (93.00%) (39448/42368)
Epoch: 108 | Batch_idx: 340 |  Loss: (0.1973) |  Loss2: (0.0000) | Acc: (93.00%) (40653/43648)
Epoch: 108 | Batch_idx: 350 |  Loss: (0.1978) |  Loss2: (0.0000) | Acc: (93.00%) (41851/44928)
Epoch: 108 | Batch_idx: 360 |  Loss: (0.1982) |  Loss2: (0.0000) | Acc: (93.00%) (43033/46208)
Epoch: 108 | Batch_idx: 370 |  Loss: (0.1978) |  Loss2: (0.0000) | Acc: (93.00%) (44237/47488)
Epoch: 108 | Batch_idx: 380 |  Loss: (0.1978) |  Loss2: (0.0000) | Acc: (93.00%) (45430/48768)
Epoch: 108 | Batch_idx: 390 |  Loss: (0.1976) |  Loss2: (0.0000) | Acc: (93.00%) (46579/50000)
# TEST : Loss: (0.3934) | Acc: (87.00%) (8742/10000)
percent tensor([0.5834, 0.4166], device='cuda:0')
percent tensor([0.5693, 0.4307], device='cuda:0')
percent tensor([0.5437, 0.4563], device='cuda:0')
percent tensor([0.6234, 0.3766], device='cuda:0')
percent tensor([0.5846, 0.4154], device='cuda:0')
percent tensor([0.6724, 0.3276], device='cuda:0')
percent tensor([0.6802, 0.3198], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 109 | Batch_idx: 0 |  Loss: (0.2478) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 109 | Batch_idx: 10 |  Loss: (0.1843) |  Loss2: (0.0000) | Acc: (92.00%) (1308/1408)
Epoch: 109 | Batch_idx: 20 |  Loss: (0.1786) |  Loss2: (0.0000) | Acc: (93.00%) (2516/2688)
Epoch: 109 | Batch_idx: 30 |  Loss: (0.1838) |  Loss2: (0.0000) | Acc: (93.00%) (3708/3968)
Epoch: 109 | Batch_idx: 40 |  Loss: (0.1874) |  Loss2: (0.0000) | Acc: (93.00%) (4897/5248)
Epoch: 109 | Batch_idx: 50 |  Loss: (0.1919) |  Loss2: (0.0000) | Acc: (93.00%) (6085/6528)
Epoch: 109 | Batch_idx: 60 |  Loss: (0.1928) |  Loss2: (0.0000) | Acc: (93.00%) (7272/7808)
Epoch: 109 | Batch_idx: 70 |  Loss: (0.1920) |  Loss2: (0.0000) | Acc: (93.00%) (8467/9088)
Epoch: 109 | Batch_idx: 80 |  Loss: (0.1939) |  Loss2: (0.0000) | Acc: (93.00%) (9649/10368)
Epoch: 109 | Batch_idx: 90 |  Loss: (0.1908) |  Loss2: (0.0000) | Acc: (93.00%) (10857/11648)
Epoch: 109 | Batch_idx: 100 |  Loss: (0.1922) |  Loss2: (0.0000) | Acc: (93.00%) (12050/12928)
Epoch: 109 | Batch_idx: 110 |  Loss: (0.1904) |  Loss2: (0.0000) | Acc: (93.00%) (13255/14208)
Epoch: 109 | Batch_idx: 120 |  Loss: (0.1879) |  Loss2: (0.0000) | Acc: (93.00%) (14472/15488)
Epoch: 109 | Batch_idx: 130 |  Loss: (0.1858) |  Loss2: (0.0000) | Acc: (93.00%) (15679/16768)
Epoch: 109 | Batch_idx: 140 |  Loss: (0.1869) |  Loss2: (0.0000) | Acc: (93.00%) (16871/18048)
Epoch: 109 | Batch_idx: 150 |  Loss: (0.1859) |  Loss2: (0.0000) | Acc: (93.00%) (18071/19328)
Epoch: 109 | Batch_idx: 160 |  Loss: (0.1871) |  Loss2: (0.0000) | Acc: (93.00%) (19265/20608)
Epoch: 109 | Batch_idx: 170 |  Loss: (0.1895) |  Loss2: (0.0000) | Acc: (93.00%) (20442/21888)
Epoch: 109 | Batch_idx: 180 |  Loss: (0.1886) |  Loss2: (0.0000) | Acc: (93.00%) (21647/23168)
Epoch: 109 | Batch_idx: 190 |  Loss: (0.1886) |  Loss2: (0.0000) | Acc: (93.00%) (22843/24448)
Epoch: 109 | Batch_idx: 200 |  Loss: (0.1894) |  Loss2: (0.0000) | Acc: (93.00%) (24038/25728)
Epoch: 109 | Batch_idx: 210 |  Loss: (0.1901) |  Loss2: (0.0000) | Acc: (93.00%) (25219/27008)
Epoch: 109 | Batch_idx: 220 |  Loss: (0.1901) |  Loss2: (0.0000) | Acc: (93.00%) (26413/28288)
Epoch: 109 | Batch_idx: 230 |  Loss: (0.1902) |  Loss2: (0.0000) | Acc: (93.00%) (27607/29568)
Epoch: 109 | Batch_idx: 240 |  Loss: (0.1903) |  Loss2: (0.0000) | Acc: (93.00%) (28800/30848)
Epoch: 109 | Batch_idx: 250 |  Loss: (0.1894) |  Loss2: (0.0000) | Acc: (93.00%) (30008/32128)
Epoch: 109 | Batch_idx: 260 |  Loss: (0.1900) |  Loss2: (0.0000) | Acc: (93.00%) (31194/33408)
Epoch: 109 | Batch_idx: 270 |  Loss: (0.1905) |  Loss2: (0.0000) | Acc: (93.00%) (32384/34688)
Epoch: 109 | Batch_idx: 280 |  Loss: (0.1898) |  Loss2: (0.0000) | Acc: (93.00%) (33587/35968)
Epoch: 109 | Batch_idx: 290 |  Loss: (0.1898) |  Loss2: (0.0000) | Acc: (93.00%) (34790/37248)
Epoch: 109 | Batch_idx: 300 |  Loss: (0.1901) |  Loss2: (0.0000) | Acc: (93.00%) (35973/38528)
Epoch: 109 | Batch_idx: 310 |  Loss: (0.1894) |  Loss2: (0.0000) | Acc: (93.00%) (37181/39808)
Epoch: 109 | Batch_idx: 320 |  Loss: (0.1900) |  Loss2: (0.0000) | Acc: (93.00%) (38377/41088)
Epoch: 109 | Batch_idx: 330 |  Loss: (0.1899) |  Loss2: (0.0000) | Acc: (93.00%) (39583/42368)
Epoch: 109 | Batch_idx: 340 |  Loss: (0.1903) |  Loss2: (0.0000) | Acc: (93.00%) (40771/43648)
Epoch: 109 | Batch_idx: 350 |  Loss: (0.1908) |  Loss2: (0.0000) | Acc: (93.00%) (41960/44928)
Epoch: 109 | Batch_idx: 360 |  Loss: (0.1913) |  Loss2: (0.0000) | Acc: (93.00%) (43148/46208)
Epoch: 109 | Batch_idx: 370 |  Loss: (0.1910) |  Loss2: (0.0000) | Acc: (93.00%) (44349/47488)
Epoch: 109 | Batch_idx: 380 |  Loss: (0.1913) |  Loss2: (0.0000) | Acc: (93.00%) (45544/48768)
Epoch: 109 | Batch_idx: 390 |  Loss: (0.1912) |  Loss2: (0.0000) | Acc: (93.00%) (46694/50000)
# TEST : Loss: (0.3933) | Acc: (87.00%) (8725/10000)
percent tensor([0.5848, 0.4152], device='cuda:0')
percent tensor([0.5699, 0.4301], device='cuda:0')
percent tensor([0.5463, 0.4537], device='cuda:0')
percent tensor([0.6259, 0.3741], device='cuda:0')
percent tensor([0.5922, 0.4078], device='cuda:0')
percent tensor([0.6727, 0.3273], device='cuda:0')
percent tensor([0.6868, 0.3132], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 110 | Batch_idx: 0 |  Loss: (0.1171) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 110 | Batch_idx: 10 |  Loss: (0.1677) |  Loss2: (0.0000) | Acc: (94.00%) (1330/1408)
Epoch: 110 | Batch_idx: 20 |  Loss: (0.1843) |  Loss2: (0.0000) | Acc: (93.00%) (2523/2688)
Epoch: 110 | Batch_idx: 30 |  Loss: (0.1859) |  Loss2: (0.0000) | Acc: (93.00%) (3711/3968)
Epoch: 110 | Batch_idx: 40 |  Loss: (0.1890) |  Loss2: (0.0000) | Acc: (93.00%) (4902/5248)
Epoch: 110 | Batch_idx: 50 |  Loss: (0.1881) |  Loss2: (0.0000) | Acc: (93.00%) (6097/6528)
Epoch: 110 | Batch_idx: 60 |  Loss: (0.1877) |  Loss2: (0.0000) | Acc: (93.00%) (7290/7808)
Epoch: 110 | Batch_idx: 70 |  Loss: (0.1898) |  Loss2: (0.0000) | Acc: (93.00%) (8472/9088)
Epoch: 110 | Batch_idx: 80 |  Loss: (0.1870) |  Loss2: (0.0000) | Acc: (93.00%) (9674/10368)
Epoch: 110 | Batch_idx: 90 |  Loss: (0.1926) |  Loss2: (0.0000) | Acc: (93.00%) (10857/11648)
Epoch: 110 | Batch_idx: 100 |  Loss: (0.1925) |  Loss2: (0.0000) | Acc: (93.00%) (12053/12928)
Epoch: 110 | Batch_idx: 110 |  Loss: (0.1918) |  Loss2: (0.0000) | Acc: (93.00%) (13254/14208)
Epoch: 110 | Batch_idx: 120 |  Loss: (0.1926) |  Loss2: (0.0000) | Acc: (93.00%) (14442/15488)
Epoch: 110 | Batch_idx: 130 |  Loss: (0.1931) |  Loss2: (0.0000) | Acc: (93.00%) (15625/16768)
Epoch: 110 | Batch_idx: 140 |  Loss: (0.1954) |  Loss2: (0.0000) | Acc: (93.00%) (16789/18048)
Epoch: 110 | Batch_idx: 150 |  Loss: (0.1966) |  Loss2: (0.0000) | Acc: (92.00%) (17973/19328)
Epoch: 110 | Batch_idx: 160 |  Loss: (0.1983) |  Loss2: (0.0000) | Acc: (92.00%) (19151/20608)
Epoch: 110 | Batch_idx: 170 |  Loss: (0.1986) |  Loss2: (0.0000) | Acc: (92.00%) (20346/21888)
Epoch: 110 | Batch_idx: 180 |  Loss: (0.1993) |  Loss2: (0.0000) | Acc: (92.00%) (21532/23168)
Epoch: 110 | Batch_idx: 190 |  Loss: (0.1993) |  Loss2: (0.0000) | Acc: (92.00%) (22721/24448)
Epoch: 110 | Batch_idx: 200 |  Loss: (0.2007) |  Loss2: (0.0000) | Acc: (92.00%) (23902/25728)
Epoch: 110 | Batch_idx: 210 |  Loss: (0.2013) |  Loss2: (0.0000) | Acc: (92.00%) (25093/27008)
Epoch: 110 | Batch_idx: 220 |  Loss: (0.2018) |  Loss2: (0.0000) | Acc: (92.00%) (26290/28288)
Epoch: 110 | Batch_idx: 230 |  Loss: (0.2018) |  Loss2: (0.0000) | Acc: (92.00%) (27479/29568)
Epoch: 110 | Batch_idx: 240 |  Loss: (0.2028) |  Loss2: (0.0000) | Acc: (92.00%) (28654/30848)
Epoch: 110 | Batch_idx: 250 |  Loss: (0.2023) |  Loss2: (0.0000) | Acc: (92.00%) (29849/32128)
Epoch: 110 | Batch_idx: 260 |  Loss: (0.2023) |  Loss2: (0.0000) | Acc: (92.00%) (31044/33408)
Epoch: 110 | Batch_idx: 270 |  Loss: (0.2011) |  Loss2: (0.0000) | Acc: (92.00%) (32250/34688)
Epoch: 110 | Batch_idx: 280 |  Loss: (0.2015) |  Loss2: (0.0000) | Acc: (92.00%) (33438/35968)
Epoch: 110 | Batch_idx: 290 |  Loss: (0.2018) |  Loss2: (0.0000) | Acc: (92.00%) (34618/37248)
Epoch: 110 | Batch_idx: 300 |  Loss: (0.2021) |  Loss2: (0.0000) | Acc: (92.00%) (35796/38528)
Epoch: 110 | Batch_idx: 310 |  Loss: (0.2015) |  Loss2: (0.0000) | Acc: (92.00%) (36999/39808)
Epoch: 110 | Batch_idx: 320 |  Loss: (0.2015) |  Loss2: (0.0000) | Acc: (92.00%) (38187/41088)
Epoch: 110 | Batch_idx: 330 |  Loss: (0.2015) |  Loss2: (0.0000) | Acc: (92.00%) (39382/42368)
Epoch: 110 | Batch_idx: 340 |  Loss: (0.2017) |  Loss2: (0.0000) | Acc: (92.00%) (40568/43648)
Epoch: 110 | Batch_idx: 350 |  Loss: (0.2014) |  Loss2: (0.0000) | Acc: (92.00%) (41759/44928)
Epoch: 110 | Batch_idx: 360 |  Loss: (0.2016) |  Loss2: (0.0000) | Acc: (92.00%) (42949/46208)
Epoch: 110 | Batch_idx: 370 |  Loss: (0.2013) |  Loss2: (0.0000) | Acc: (92.00%) (44145/47488)
Epoch: 110 | Batch_idx: 380 |  Loss: (0.2016) |  Loss2: (0.0000) | Acc: (92.00%) (45323/48768)
Epoch: 110 | Batch_idx: 390 |  Loss: (0.2017) |  Loss2: (0.0000) | Acc: (92.00%) (46467/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_110.pth.tar'
# TEST : Loss: (0.4289) | Acc: (86.00%) (8696/10000)
percent tensor([0.5847, 0.4153], device='cuda:0')
percent tensor([0.5700, 0.4300], device='cuda:0')
percent tensor([0.5465, 0.4535], device='cuda:0')
percent tensor([0.6256, 0.3744], device='cuda:0')
percent tensor([0.5926, 0.4074], device='cuda:0')
percent tensor([0.6724, 0.3276], device='cuda:0')
percent tensor([0.6868, 0.3132], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(179.9081, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(816.4470, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(814.3657, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1523.6339, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(492.0988, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2252.2583, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4278.4106, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1388.8553, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6168.2754, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11836.4551, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3916.5063, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16572.6699, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 111 | Batch_idx: 0 |  Loss: (0.2645) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 111 | Batch_idx: 10 |  Loss: (0.1925) |  Loss2: (0.0000) | Acc: (92.00%) (1307/1408)
Epoch: 111 | Batch_idx: 20 |  Loss: (0.1932) |  Loss2: (0.0000) | Acc: (93.00%) (2503/2688)
Epoch: 111 | Batch_idx: 30 |  Loss: (0.1921) |  Loss2: (0.0000) | Acc: (93.00%) (3703/3968)
Epoch: 111 | Batch_idx: 40 |  Loss: (0.1844) |  Loss2: (0.0000) | Acc: (93.00%) (4918/5248)
Epoch: 111 | Batch_idx: 50 |  Loss: (0.1881) |  Loss2: (0.0000) | Acc: (93.00%) (6112/6528)
Epoch: 111 | Batch_idx: 60 |  Loss: (0.1877) |  Loss2: (0.0000) | Acc: (93.00%) (7301/7808)
Epoch: 111 | Batch_idx: 70 |  Loss: (0.1911) |  Loss2: (0.0000) | Acc: (93.00%) (8491/9088)
Epoch: 111 | Batch_idx: 80 |  Loss: (0.1909) |  Loss2: (0.0000) | Acc: (93.00%) (9684/10368)
Epoch: 111 | Batch_idx: 90 |  Loss: (0.1912) |  Loss2: (0.0000) | Acc: (93.00%) (10882/11648)
Epoch: 111 | Batch_idx: 100 |  Loss: (0.1908) |  Loss2: (0.0000) | Acc: (93.00%) (12091/12928)
Epoch: 111 | Batch_idx: 110 |  Loss: (0.1911) |  Loss2: (0.0000) | Acc: (93.00%) (13287/14208)
Epoch: 111 | Batch_idx: 120 |  Loss: (0.1896) |  Loss2: (0.0000) | Acc: (93.00%) (14490/15488)
Epoch: 111 | Batch_idx: 130 |  Loss: (0.1885) |  Loss2: (0.0000) | Acc: (93.00%) (15684/16768)
Epoch: 111 | Batch_idx: 140 |  Loss: (0.1889) |  Loss2: (0.0000) | Acc: (93.00%) (16883/18048)
Epoch: 111 | Batch_idx: 150 |  Loss: (0.1888) |  Loss2: (0.0000) | Acc: (93.00%) (18076/19328)
Epoch: 111 | Batch_idx: 160 |  Loss: (0.1889) |  Loss2: (0.0000) | Acc: (93.00%) (19276/20608)
Epoch: 111 | Batch_idx: 170 |  Loss: (0.1893) |  Loss2: (0.0000) | Acc: (93.00%) (20473/21888)
Epoch: 111 | Batch_idx: 180 |  Loss: (0.1888) |  Loss2: (0.0000) | Acc: (93.00%) (21678/23168)
Epoch: 111 | Batch_idx: 190 |  Loss: (0.1890) |  Loss2: (0.0000) | Acc: (93.00%) (22875/24448)
Epoch: 111 | Batch_idx: 200 |  Loss: (0.1887) |  Loss2: (0.0000) | Acc: (93.00%) (24074/25728)
Epoch: 111 | Batch_idx: 210 |  Loss: (0.1887) |  Loss2: (0.0000) | Acc: (93.00%) (25275/27008)
Epoch: 111 | Batch_idx: 220 |  Loss: (0.1885) |  Loss2: (0.0000) | Acc: (93.00%) (26472/28288)
Epoch: 111 | Batch_idx: 230 |  Loss: (0.1882) |  Loss2: (0.0000) | Acc: (93.00%) (27677/29568)
Epoch: 111 | Batch_idx: 240 |  Loss: (0.1883) |  Loss2: (0.0000) | Acc: (93.00%) (28868/30848)
Epoch: 111 | Batch_idx: 250 |  Loss: (0.1888) |  Loss2: (0.0000) | Acc: (93.00%) (30072/32128)
Epoch: 111 | Batch_idx: 260 |  Loss: (0.1885) |  Loss2: (0.0000) | Acc: (93.00%) (31265/33408)
Epoch: 111 | Batch_idx: 270 |  Loss: (0.1889) |  Loss2: (0.0000) | Acc: (93.00%) (32459/34688)
Epoch: 111 | Batch_idx: 280 |  Loss: (0.1885) |  Loss2: (0.0000) | Acc: (93.00%) (33660/35968)
Epoch: 111 | Batch_idx: 290 |  Loss: (0.1894) |  Loss2: (0.0000) | Acc: (93.00%) (34835/37248)
Epoch: 111 | Batch_idx: 300 |  Loss: (0.1898) |  Loss2: (0.0000) | Acc: (93.00%) (36025/38528)
Epoch: 111 | Batch_idx: 310 |  Loss: (0.1899) |  Loss2: (0.0000) | Acc: (93.00%) (37215/39808)
Epoch: 111 | Batch_idx: 320 |  Loss: (0.1905) |  Loss2: (0.0000) | Acc: (93.00%) (38406/41088)
Epoch: 111 | Batch_idx: 330 |  Loss: (0.1903) |  Loss2: (0.0000) | Acc: (93.00%) (39606/42368)
Epoch: 111 | Batch_idx: 340 |  Loss: (0.1905) |  Loss2: (0.0000) | Acc: (93.00%) (40807/43648)
Epoch: 111 | Batch_idx: 350 |  Loss: (0.1904) |  Loss2: (0.0000) | Acc: (93.00%) (42005/44928)
Epoch: 111 | Batch_idx: 360 |  Loss: (0.1905) |  Loss2: (0.0000) | Acc: (93.00%) (43196/46208)
Epoch: 111 | Batch_idx: 370 |  Loss: (0.1912) |  Loss2: (0.0000) | Acc: (93.00%) (44387/47488)
Epoch: 111 | Batch_idx: 380 |  Loss: (0.1915) |  Loss2: (0.0000) | Acc: (93.00%) (45590/48768)
Epoch: 111 | Batch_idx: 390 |  Loss: (0.1918) |  Loss2: (0.0000) | Acc: (93.00%) (46733/50000)
# TEST : Loss: (0.4527) | Acc: (86.00%) (8646/10000)
percent tensor([0.5852, 0.4148], device='cuda:0')
percent tensor([0.5699, 0.4301], device='cuda:0')
percent tensor([0.5467, 0.4533], device='cuda:0')
percent tensor([0.6263, 0.3737], device='cuda:0')
percent tensor([0.5916, 0.4084], device='cuda:0')
percent tensor([0.6721, 0.3279], device='cuda:0')
percent tensor([0.6857, 0.3143], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 112 | Batch_idx: 0 |  Loss: (0.2003) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 112 | Batch_idx: 10 |  Loss: (0.1734) |  Loss2: (0.0000) | Acc: (94.00%) (1329/1408)
Epoch: 112 | Batch_idx: 20 |  Loss: (0.1853) |  Loss2: (0.0000) | Acc: (93.00%) (2518/2688)
Epoch: 112 | Batch_idx: 30 |  Loss: (0.1860) |  Loss2: (0.0000) | Acc: (93.00%) (3712/3968)
Epoch: 112 | Batch_idx: 40 |  Loss: (0.1948) |  Loss2: (0.0000) | Acc: (93.00%) (4899/5248)
Epoch: 112 | Batch_idx: 50 |  Loss: (0.1917) |  Loss2: (0.0000) | Acc: (93.00%) (6105/6528)
Epoch: 112 | Batch_idx: 60 |  Loss: (0.1909) |  Loss2: (0.0000) | Acc: (93.00%) (7304/7808)
Epoch: 112 | Batch_idx: 70 |  Loss: (0.1910) |  Loss2: (0.0000) | Acc: (93.00%) (8500/9088)
Epoch: 112 | Batch_idx: 80 |  Loss: (0.1895) |  Loss2: (0.0000) | Acc: (93.00%) (9704/10368)
Epoch: 112 | Batch_idx: 90 |  Loss: (0.1877) |  Loss2: (0.0000) | Acc: (93.00%) (10899/11648)
Epoch: 112 | Batch_idx: 100 |  Loss: (0.1875) |  Loss2: (0.0000) | Acc: (93.00%) (12090/12928)
Epoch: 112 | Batch_idx: 110 |  Loss: (0.1875) |  Loss2: (0.0000) | Acc: (93.00%) (13284/14208)
Epoch: 112 | Batch_idx: 120 |  Loss: (0.1889) |  Loss2: (0.0000) | Acc: (93.00%) (14464/15488)
Epoch: 112 | Batch_idx: 130 |  Loss: (0.1870) |  Loss2: (0.0000) | Acc: (93.00%) (15666/16768)
Epoch: 112 | Batch_idx: 140 |  Loss: (0.1884) |  Loss2: (0.0000) | Acc: (93.00%) (16858/18048)
Epoch: 112 | Batch_idx: 150 |  Loss: (0.1905) |  Loss2: (0.0000) | Acc: (93.00%) (18043/19328)
Epoch: 112 | Batch_idx: 160 |  Loss: (0.1904) |  Loss2: (0.0000) | Acc: (93.00%) (19240/20608)
Epoch: 112 | Batch_idx: 170 |  Loss: (0.1900) |  Loss2: (0.0000) | Acc: (93.00%) (20438/21888)
Epoch: 112 | Batch_idx: 180 |  Loss: (0.1901) |  Loss2: (0.0000) | Acc: (93.00%) (21631/23168)
Epoch: 112 | Batch_idx: 190 |  Loss: (0.1899) |  Loss2: (0.0000) | Acc: (93.00%) (22833/24448)
Epoch: 112 | Batch_idx: 200 |  Loss: (0.1906) |  Loss2: (0.0000) | Acc: (93.00%) (24031/25728)
Epoch: 112 | Batch_idx: 210 |  Loss: (0.1896) |  Loss2: (0.0000) | Acc: (93.00%) (25232/27008)
Epoch: 112 | Batch_idx: 220 |  Loss: (0.1896) |  Loss2: (0.0000) | Acc: (93.00%) (26427/28288)
Epoch: 112 | Batch_idx: 230 |  Loss: (0.1904) |  Loss2: (0.0000) | Acc: (93.00%) (27619/29568)
Epoch: 112 | Batch_idx: 240 |  Loss: (0.1899) |  Loss2: (0.0000) | Acc: (93.00%) (28826/30848)
Epoch: 112 | Batch_idx: 250 |  Loss: (0.1901) |  Loss2: (0.0000) | Acc: (93.00%) (30018/32128)
Epoch: 112 | Batch_idx: 260 |  Loss: (0.1897) |  Loss2: (0.0000) | Acc: (93.00%) (31219/33408)
Epoch: 112 | Batch_idx: 270 |  Loss: (0.1904) |  Loss2: (0.0000) | Acc: (93.00%) (32404/34688)
Epoch: 112 | Batch_idx: 280 |  Loss: (0.1900) |  Loss2: (0.0000) | Acc: (93.00%) (33598/35968)
Epoch: 112 | Batch_idx: 290 |  Loss: (0.1909) |  Loss2: (0.0000) | Acc: (93.00%) (34787/37248)
Epoch: 112 | Batch_idx: 300 |  Loss: (0.1907) |  Loss2: (0.0000) | Acc: (93.00%) (35980/38528)
Epoch: 112 | Batch_idx: 310 |  Loss: (0.1915) |  Loss2: (0.0000) | Acc: (93.00%) (37175/39808)
Epoch: 112 | Batch_idx: 320 |  Loss: (0.1911) |  Loss2: (0.0000) | Acc: (93.00%) (38370/41088)
Epoch: 112 | Batch_idx: 330 |  Loss: (0.1918) |  Loss2: (0.0000) | Acc: (93.00%) (39562/42368)
Epoch: 112 | Batch_idx: 340 |  Loss: (0.1911) |  Loss2: (0.0000) | Acc: (93.00%) (40774/43648)
Epoch: 112 | Batch_idx: 350 |  Loss: (0.1914) |  Loss2: (0.0000) | Acc: (93.00%) (41960/44928)
Epoch: 112 | Batch_idx: 360 |  Loss: (0.1913) |  Loss2: (0.0000) | Acc: (93.00%) (43155/46208)
Epoch: 112 | Batch_idx: 370 |  Loss: (0.1910) |  Loss2: (0.0000) | Acc: (93.00%) (44350/47488)
Epoch: 112 | Batch_idx: 380 |  Loss: (0.1913) |  Loss2: (0.0000) | Acc: (93.00%) (45546/48768)
Epoch: 112 | Batch_idx: 390 |  Loss: (0.1914) |  Loss2: (0.0000) | Acc: (93.00%) (46689/50000)
# TEST : Loss: (0.4007) | Acc: (87.00%) (8781/10000)
percent tensor([0.5858, 0.4142], device='cuda:0')
percent tensor([0.5692, 0.4308], device='cuda:0')
percent tensor([0.5467, 0.4533], device='cuda:0')
percent tensor([0.6235, 0.3765], device='cuda:0')
percent tensor([0.5915, 0.4085], device='cuda:0')
percent tensor([0.6718, 0.3282], device='cuda:0')
percent tensor([0.6915, 0.3085], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 113 | Batch_idx: 0 |  Loss: (0.1714) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 113 | Batch_idx: 10 |  Loss: (0.1571) |  Loss2: (0.0000) | Acc: (94.00%) (1335/1408)
Epoch: 113 | Batch_idx: 20 |  Loss: (0.1572) |  Loss2: (0.0000) | Acc: (94.00%) (2544/2688)
Epoch: 113 | Batch_idx: 30 |  Loss: (0.1671) |  Loss2: (0.0000) | Acc: (94.00%) (3734/3968)
Epoch: 113 | Batch_idx: 40 |  Loss: (0.1717) |  Loss2: (0.0000) | Acc: (94.00%) (4935/5248)
Epoch: 113 | Batch_idx: 50 |  Loss: (0.1749) |  Loss2: (0.0000) | Acc: (93.00%) (6132/6528)
Epoch: 113 | Batch_idx: 60 |  Loss: (0.1773) |  Loss2: (0.0000) | Acc: (93.00%) (7327/7808)
Epoch: 113 | Batch_idx: 70 |  Loss: (0.1787) |  Loss2: (0.0000) | Acc: (93.00%) (8524/9088)
Epoch: 113 | Batch_idx: 80 |  Loss: (0.1796) |  Loss2: (0.0000) | Acc: (93.00%) (9719/10368)
Epoch: 113 | Batch_idx: 90 |  Loss: (0.1812) |  Loss2: (0.0000) | Acc: (93.00%) (10921/11648)
Epoch: 113 | Batch_idx: 100 |  Loss: (0.1795) |  Loss2: (0.0000) | Acc: (93.00%) (12129/12928)
Epoch: 113 | Batch_idx: 110 |  Loss: (0.1823) |  Loss2: (0.0000) | Acc: (93.00%) (13317/14208)
Epoch: 113 | Batch_idx: 120 |  Loss: (0.1848) |  Loss2: (0.0000) | Acc: (93.00%) (14513/15488)
Epoch: 113 | Batch_idx: 130 |  Loss: (0.1862) |  Loss2: (0.0000) | Acc: (93.00%) (15708/16768)
Epoch: 113 | Batch_idx: 140 |  Loss: (0.1868) |  Loss2: (0.0000) | Acc: (93.00%) (16901/18048)
Epoch: 113 | Batch_idx: 150 |  Loss: (0.1848) |  Loss2: (0.0000) | Acc: (93.00%) (18109/19328)
Epoch: 113 | Batch_idx: 160 |  Loss: (0.1844) |  Loss2: (0.0000) | Acc: (93.00%) (19312/20608)
Epoch: 113 | Batch_idx: 170 |  Loss: (0.1834) |  Loss2: (0.0000) | Acc: (93.00%) (20514/21888)
Epoch: 113 | Batch_idx: 180 |  Loss: (0.1838) |  Loss2: (0.0000) | Acc: (93.00%) (21722/23168)
Epoch: 113 | Batch_idx: 190 |  Loss: (0.1839) |  Loss2: (0.0000) | Acc: (93.00%) (22926/24448)
Epoch: 113 | Batch_idx: 200 |  Loss: (0.1845) |  Loss2: (0.0000) | Acc: (93.00%) (24107/25728)
Epoch: 113 | Batch_idx: 210 |  Loss: (0.1842) |  Loss2: (0.0000) | Acc: (93.00%) (25312/27008)
Epoch: 113 | Batch_idx: 220 |  Loss: (0.1851) |  Loss2: (0.0000) | Acc: (93.00%) (26505/28288)
Epoch: 113 | Batch_idx: 230 |  Loss: (0.1844) |  Loss2: (0.0000) | Acc: (93.00%) (27715/29568)
Epoch: 113 | Batch_idx: 240 |  Loss: (0.1841) |  Loss2: (0.0000) | Acc: (93.00%) (28917/30848)
Epoch: 113 | Batch_idx: 250 |  Loss: (0.1833) |  Loss2: (0.0000) | Acc: (93.00%) (30119/32128)
Epoch: 113 | Batch_idx: 260 |  Loss: (0.1833) |  Loss2: (0.0000) | Acc: (93.00%) (31316/33408)
Epoch: 113 | Batch_idx: 270 |  Loss: (0.1834) |  Loss2: (0.0000) | Acc: (93.00%) (32511/34688)
Epoch: 113 | Batch_idx: 280 |  Loss: (0.1829) |  Loss2: (0.0000) | Acc: (93.00%) (33722/35968)
Epoch: 113 | Batch_idx: 290 |  Loss: (0.1834) |  Loss2: (0.0000) | Acc: (93.00%) (34920/37248)
Epoch: 113 | Batch_idx: 300 |  Loss: (0.1827) |  Loss2: (0.0000) | Acc: (93.00%) (36129/38528)
Epoch: 113 | Batch_idx: 310 |  Loss: (0.1829) |  Loss2: (0.0000) | Acc: (93.00%) (37320/39808)
Epoch: 113 | Batch_idx: 320 |  Loss: (0.1824) |  Loss2: (0.0000) | Acc: (93.00%) (38524/41088)
Epoch: 113 | Batch_idx: 330 |  Loss: (0.1816) |  Loss2: (0.0000) | Acc: (93.00%) (39735/42368)
Epoch: 113 | Batch_idx: 340 |  Loss: (0.1819) |  Loss2: (0.0000) | Acc: (93.00%) (40931/43648)
Epoch: 113 | Batch_idx: 350 |  Loss: (0.1821) |  Loss2: (0.0000) | Acc: (93.00%) (42125/44928)
Epoch: 113 | Batch_idx: 360 |  Loss: (0.1823) |  Loss2: (0.0000) | Acc: (93.00%) (43321/46208)
Epoch: 113 | Batch_idx: 370 |  Loss: (0.1833) |  Loss2: (0.0000) | Acc: (93.00%) (44503/47488)
Epoch: 113 | Batch_idx: 380 |  Loss: (0.1831) |  Loss2: (0.0000) | Acc: (93.00%) (45711/48768)
Epoch: 113 | Batch_idx: 390 |  Loss: (0.1836) |  Loss2: (0.0000) | Acc: (93.00%) (46853/50000)
# TEST : Loss: (0.4444) | Acc: (86.00%) (8652/10000)
percent tensor([0.5853, 0.4147], device='cuda:0')
percent tensor([0.5701, 0.4299], device='cuda:0')
percent tensor([0.5465, 0.4535], device='cuda:0')
percent tensor([0.6246, 0.3754], device='cuda:0')
percent tensor([0.5922, 0.4078], device='cuda:0')
percent tensor([0.6730, 0.3270], device='cuda:0')
percent tensor([0.6910, 0.3090], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 114 | Batch_idx: 0 |  Loss: (0.1490) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 114 | Batch_idx: 10 |  Loss: (0.1728) |  Loss2: (0.0000) | Acc: (93.00%) (1317/1408)
Epoch: 114 | Batch_idx: 20 |  Loss: (0.1714) |  Loss2: (0.0000) | Acc: (93.00%) (2524/2688)
Epoch: 114 | Batch_idx: 30 |  Loss: (0.1673) |  Loss2: (0.0000) | Acc: (94.00%) (3731/3968)
Epoch: 114 | Batch_idx: 40 |  Loss: (0.1743) |  Loss2: (0.0000) | Acc: (93.00%) (4929/5248)
Epoch: 114 | Batch_idx: 50 |  Loss: (0.1776) |  Loss2: (0.0000) | Acc: (93.00%) (6117/6528)
Epoch: 114 | Batch_idx: 60 |  Loss: (0.1746) |  Loss2: (0.0000) | Acc: (93.00%) (7332/7808)
Epoch: 114 | Batch_idx: 70 |  Loss: (0.1736) |  Loss2: (0.0000) | Acc: (93.00%) (8538/9088)
Epoch: 114 | Batch_idx: 80 |  Loss: (0.1731) |  Loss2: (0.0000) | Acc: (93.00%) (9744/10368)
Epoch: 114 | Batch_idx: 90 |  Loss: (0.1751) |  Loss2: (0.0000) | Acc: (93.00%) (10934/11648)
Epoch: 114 | Batch_idx: 100 |  Loss: (0.1739) |  Loss2: (0.0000) | Acc: (93.00%) (12140/12928)
Epoch: 114 | Batch_idx: 110 |  Loss: (0.1734) |  Loss2: (0.0000) | Acc: (93.00%) (13343/14208)
Epoch: 114 | Batch_idx: 120 |  Loss: (0.1732) |  Loss2: (0.0000) | Acc: (93.00%) (14541/15488)
Epoch: 114 | Batch_idx: 130 |  Loss: (0.1731) |  Loss2: (0.0000) | Acc: (93.00%) (15744/16768)
Epoch: 114 | Batch_idx: 140 |  Loss: (0.1713) |  Loss2: (0.0000) | Acc: (93.00%) (16961/18048)
Epoch: 114 | Batch_idx: 150 |  Loss: (0.1718) |  Loss2: (0.0000) | Acc: (93.00%) (18161/19328)
Epoch: 114 | Batch_idx: 160 |  Loss: (0.1722) |  Loss2: (0.0000) | Acc: (93.00%) (19368/20608)
Epoch: 114 | Batch_idx: 170 |  Loss: (0.1732) |  Loss2: (0.0000) | Acc: (93.00%) (20561/21888)
Epoch: 114 | Batch_idx: 180 |  Loss: (0.1732) |  Loss2: (0.0000) | Acc: (93.00%) (21764/23168)
Epoch: 114 | Batch_idx: 190 |  Loss: (0.1726) |  Loss2: (0.0000) | Acc: (93.00%) (22976/24448)
Epoch: 114 | Batch_idx: 200 |  Loss: (0.1731) |  Loss2: (0.0000) | Acc: (93.00%) (24178/25728)
Epoch: 114 | Batch_idx: 210 |  Loss: (0.1749) |  Loss2: (0.0000) | Acc: (93.00%) (25372/27008)
Epoch: 114 | Batch_idx: 220 |  Loss: (0.1743) |  Loss2: (0.0000) | Acc: (93.00%) (26579/28288)
Epoch: 114 | Batch_idx: 230 |  Loss: (0.1744) |  Loss2: (0.0000) | Acc: (93.00%) (27784/29568)
Epoch: 114 | Batch_idx: 240 |  Loss: (0.1749) |  Loss2: (0.0000) | Acc: (93.00%) (28991/30848)
Epoch: 114 | Batch_idx: 250 |  Loss: (0.1751) |  Loss2: (0.0000) | Acc: (93.00%) (30196/32128)
Epoch: 114 | Batch_idx: 260 |  Loss: (0.1750) |  Loss2: (0.0000) | Acc: (93.00%) (31403/33408)
Epoch: 114 | Batch_idx: 270 |  Loss: (0.1754) |  Loss2: (0.0000) | Acc: (93.00%) (32602/34688)
Epoch: 114 | Batch_idx: 280 |  Loss: (0.1757) |  Loss2: (0.0000) | Acc: (93.00%) (33803/35968)
Epoch: 114 | Batch_idx: 290 |  Loss: (0.1759) |  Loss2: (0.0000) | Acc: (93.00%) (35004/37248)
Epoch: 114 | Batch_idx: 300 |  Loss: (0.1764) |  Loss2: (0.0000) | Acc: (93.00%) (36199/38528)
Epoch: 114 | Batch_idx: 310 |  Loss: (0.1765) |  Loss2: (0.0000) | Acc: (93.00%) (37402/39808)
Epoch: 114 | Batch_idx: 320 |  Loss: (0.1768) |  Loss2: (0.0000) | Acc: (93.00%) (38603/41088)
Epoch: 114 | Batch_idx: 330 |  Loss: (0.1773) |  Loss2: (0.0000) | Acc: (93.00%) (39796/42368)
Epoch: 114 | Batch_idx: 340 |  Loss: (0.1772) |  Loss2: (0.0000) | Acc: (93.00%) (41006/43648)
Epoch: 114 | Batch_idx: 350 |  Loss: (0.1772) |  Loss2: (0.0000) | Acc: (93.00%) (42203/44928)
Epoch: 114 | Batch_idx: 360 |  Loss: (0.1775) |  Loss2: (0.0000) | Acc: (93.00%) (43397/46208)
Epoch: 114 | Batch_idx: 370 |  Loss: (0.1776) |  Loss2: (0.0000) | Acc: (93.00%) (44594/47488)
Epoch: 114 | Batch_idx: 380 |  Loss: (0.1783) |  Loss2: (0.0000) | Acc: (93.00%) (45782/48768)
Epoch: 114 | Batch_idx: 390 |  Loss: (0.1785) |  Loss2: (0.0000) | Acc: (93.00%) (46943/50000)
# TEST : Loss: (0.4608) | Acc: (85.00%) (8583/10000)
percent tensor([0.5851, 0.4149], device='cuda:0')
percent tensor([0.5692, 0.4308], device='cuda:0')
percent tensor([0.5458, 0.4542], device='cuda:0')
percent tensor([0.6246, 0.3754], device='cuda:0')
percent tensor([0.5911, 0.4089], device='cuda:0')
percent tensor([0.6684, 0.3316], device='cuda:0')
percent tensor([0.6819, 0.3181], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 115 | Batch_idx: 0 |  Loss: (0.1438) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 115 | Batch_idx: 10 |  Loss: (0.1933) |  Loss2: (0.0000) | Acc: (93.00%) (1322/1408)
Epoch: 115 | Batch_idx: 20 |  Loss: (0.1944) |  Loss2: (0.0000) | Acc: (93.00%) (2515/2688)
Epoch: 115 | Batch_idx: 30 |  Loss: (0.2034) |  Loss2: (0.0000) | Acc: (93.00%) (3694/3968)
Epoch: 115 | Batch_idx: 40 |  Loss: (0.2024) |  Loss2: (0.0000) | Acc: (92.00%) (4877/5248)
Epoch: 115 | Batch_idx: 50 |  Loss: (0.2070) |  Loss2: (0.0000) | Acc: (92.00%) (6056/6528)
Epoch: 115 | Batch_idx: 60 |  Loss: (0.2107) |  Loss2: (0.0000) | Acc: (92.00%) (7245/7808)
Epoch: 115 | Batch_idx: 70 |  Loss: (0.2121) |  Loss2: (0.0000) | Acc: (92.00%) (8426/9088)
Epoch: 115 | Batch_idx: 80 |  Loss: (0.2126) |  Loss2: (0.0000) | Acc: (92.00%) (9609/10368)
Epoch: 115 | Batch_idx: 90 |  Loss: (0.2134) |  Loss2: (0.0000) | Acc: (92.00%) (10792/11648)
Epoch: 115 | Batch_idx: 100 |  Loss: (0.2133) |  Loss2: (0.0000) | Acc: (92.00%) (11972/12928)
Epoch: 115 | Batch_idx: 110 |  Loss: (0.2117) |  Loss2: (0.0000) | Acc: (92.00%) (13167/14208)
Epoch: 115 | Batch_idx: 120 |  Loss: (0.2117) |  Loss2: (0.0000) | Acc: (92.00%) (14353/15488)
Epoch: 115 | Batch_idx: 130 |  Loss: (0.2144) |  Loss2: (0.0000) | Acc: (92.00%) (15526/16768)
Epoch: 115 | Batch_idx: 140 |  Loss: (0.2154) |  Loss2: (0.0000) | Acc: (92.00%) (16696/18048)
Epoch: 115 | Batch_idx: 150 |  Loss: (0.2155) |  Loss2: (0.0000) | Acc: (92.00%) (17873/19328)
Epoch: 115 | Batch_idx: 160 |  Loss: (0.2171) |  Loss2: (0.0000) | Acc: (92.00%) (19046/20608)
Epoch: 115 | Batch_idx: 170 |  Loss: (0.2160) |  Loss2: (0.0000) | Acc: (92.00%) (20236/21888)
Epoch: 115 | Batch_idx: 180 |  Loss: (0.2145) |  Loss2: (0.0000) | Acc: (92.00%) (21441/23168)
Epoch: 115 | Batch_idx: 190 |  Loss: (0.2150) |  Loss2: (0.0000) | Acc: (92.00%) (22625/24448)
Epoch: 115 | Batch_idx: 200 |  Loss: (0.2164) |  Loss2: (0.0000) | Acc: (92.00%) (23789/25728)
Epoch: 115 | Batch_idx: 210 |  Loss: (0.2157) |  Loss2: (0.0000) | Acc: (92.00%) (24973/27008)
Epoch: 115 | Batch_idx: 220 |  Loss: (0.2156) |  Loss2: (0.0000) | Acc: (92.00%) (26149/28288)
Epoch: 115 | Batch_idx: 230 |  Loss: (0.2156) |  Loss2: (0.0000) | Acc: (92.00%) (27332/29568)
Epoch: 115 | Batch_idx: 240 |  Loss: (0.2156) |  Loss2: (0.0000) | Acc: (92.00%) (28518/30848)
Epoch: 115 | Batch_idx: 250 |  Loss: (0.2154) |  Loss2: (0.0000) | Acc: (92.00%) (29709/32128)
Epoch: 115 | Batch_idx: 260 |  Loss: (0.2160) |  Loss2: (0.0000) | Acc: (92.00%) (30891/33408)
Epoch: 115 | Batch_idx: 270 |  Loss: (0.2154) |  Loss2: (0.0000) | Acc: (92.00%) (32090/34688)
Epoch: 115 | Batch_idx: 280 |  Loss: (0.2153) |  Loss2: (0.0000) | Acc: (92.00%) (33274/35968)
Epoch: 115 | Batch_idx: 290 |  Loss: (0.2143) |  Loss2: (0.0000) | Acc: (92.00%) (34466/37248)
Epoch: 115 | Batch_idx: 300 |  Loss: (0.2140) |  Loss2: (0.0000) | Acc: (92.00%) (35656/38528)
Epoch: 115 | Batch_idx: 310 |  Loss: (0.2140) |  Loss2: (0.0000) | Acc: (92.00%) (36846/39808)
Epoch: 115 | Batch_idx: 320 |  Loss: (0.2136) |  Loss2: (0.0000) | Acc: (92.00%) (38036/41088)
Epoch: 115 | Batch_idx: 330 |  Loss: (0.2131) |  Loss2: (0.0000) | Acc: (92.00%) (39224/42368)
Epoch: 115 | Batch_idx: 340 |  Loss: (0.2136) |  Loss2: (0.0000) | Acc: (92.00%) (40399/43648)
Epoch: 115 | Batch_idx: 350 |  Loss: (0.2123) |  Loss2: (0.0000) | Acc: (92.00%) (41604/44928)
Epoch: 115 | Batch_idx: 360 |  Loss: (0.2115) |  Loss2: (0.0000) | Acc: (92.00%) (42805/46208)
Epoch: 115 | Batch_idx: 370 |  Loss: (0.2113) |  Loss2: (0.0000) | Acc: (92.00%) (44001/47488)
Epoch: 115 | Batch_idx: 380 |  Loss: (0.2108) |  Loss2: (0.0000) | Acc: (92.00%) (45194/48768)
Epoch: 115 | Batch_idx: 390 |  Loss: (0.2109) |  Loss2: (0.0000) | Acc: (92.00%) (46338/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_115.pth.tar'
# TEST : Loss: (0.4362) | Acc: (86.00%) (8682/10000)
percent tensor([0.5774, 0.4226], device='cuda:0')
percent tensor([0.5812, 0.4188], device='cuda:0')
percent tensor([0.5430, 0.4570], device='cuda:0')
percent tensor([0.6367, 0.3633], device='cuda:0')
percent tensor([0.5964, 0.4036], device='cuda:0')
percent tensor([0.6346, 0.3654], device='cuda:0')
percent tensor([0.6925, 0.3075], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 116 | Batch_idx: 0 |  Loss: (0.2311) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 116 | Batch_idx: 10 |  Loss: (0.2122) |  Loss2: (0.0000) | Acc: (92.00%) (1309/1408)
Epoch: 116 | Batch_idx: 20 |  Loss: (0.2255) |  Loss2: (0.0000) | Acc: (92.00%) (2489/2688)
Epoch: 116 | Batch_idx: 30 |  Loss: (0.2113) |  Loss2: (0.0000) | Acc: (92.00%) (3684/3968)
Epoch: 116 | Batch_idx: 40 |  Loss: (0.2093) |  Loss2: (0.0000) | Acc: (93.00%) (4883/5248)
Epoch: 116 | Batch_idx: 50 |  Loss: (0.2070) |  Loss2: (0.0000) | Acc: (93.00%) (6078/6528)
Epoch: 116 | Batch_idx: 60 |  Loss: (0.2034) |  Loss2: (0.0000) | Acc: (93.00%) (7273/7808)
Epoch: 116 | Batch_idx: 70 |  Loss: (0.2019) |  Loss2: (0.0000) | Acc: (93.00%) (8467/9088)
Epoch: 116 | Batch_idx: 80 |  Loss: (0.2014) |  Loss2: (0.0000) | Acc: (93.00%) (9652/10368)
Epoch: 116 | Batch_idx: 90 |  Loss: (0.2006) |  Loss2: (0.0000) | Acc: (93.00%) (10854/11648)
Epoch: 116 | Batch_idx: 100 |  Loss: (0.1986) |  Loss2: (0.0000) | Acc: (93.00%) (12051/12928)
Epoch: 116 | Batch_idx: 110 |  Loss: (0.1966) |  Loss2: (0.0000) | Acc: (93.00%) (13254/14208)
Epoch: 116 | Batch_idx: 120 |  Loss: (0.2000) |  Loss2: (0.0000) | Acc: (93.00%) (14435/15488)
Epoch: 116 | Batch_idx: 130 |  Loss: (0.1999) |  Loss2: (0.0000) | Acc: (93.00%) (15623/16768)
Epoch: 116 | Batch_idx: 140 |  Loss: (0.1999) |  Loss2: (0.0000) | Acc: (93.00%) (16805/18048)
Epoch: 116 | Batch_idx: 150 |  Loss: (0.1992) |  Loss2: (0.0000) | Acc: (93.00%) (17999/19328)
Epoch: 116 | Batch_idx: 160 |  Loss: (0.1991) |  Loss2: (0.0000) | Acc: (93.00%) (19190/20608)
Epoch: 116 | Batch_idx: 170 |  Loss: (0.1983) |  Loss2: (0.0000) | Acc: (93.00%) (20395/21888)
Epoch: 116 | Batch_idx: 180 |  Loss: (0.1988) |  Loss2: (0.0000) | Acc: (93.00%) (21584/23168)
Epoch: 116 | Batch_idx: 190 |  Loss: (0.1983) |  Loss2: (0.0000) | Acc: (93.00%) (22783/24448)
Epoch: 116 | Batch_idx: 200 |  Loss: (0.1993) |  Loss2: (0.0000) | Acc: (93.00%) (23968/25728)
Epoch: 116 | Batch_idx: 210 |  Loss: (0.1992) |  Loss2: (0.0000) | Acc: (93.00%) (25153/27008)
Epoch: 116 | Batch_idx: 220 |  Loss: (0.1987) |  Loss2: (0.0000) | Acc: (93.00%) (26360/28288)
Epoch: 116 | Batch_idx: 230 |  Loss: (0.1976) |  Loss2: (0.0000) | Acc: (93.00%) (27569/29568)
Epoch: 116 | Batch_idx: 240 |  Loss: (0.1980) |  Loss2: (0.0000) | Acc: (93.00%) (28752/30848)
Epoch: 116 | Batch_idx: 250 |  Loss: (0.1976) |  Loss2: (0.0000) | Acc: (93.00%) (29950/32128)
Epoch: 116 | Batch_idx: 260 |  Loss: (0.1980) |  Loss2: (0.0000) | Acc: (93.00%) (31137/33408)
Epoch: 116 | Batch_idx: 270 |  Loss: (0.1972) |  Loss2: (0.0000) | Acc: (93.00%) (32339/34688)
Epoch: 116 | Batch_idx: 280 |  Loss: (0.1973) |  Loss2: (0.0000) | Acc: (93.00%) (33534/35968)
Epoch: 116 | Batch_idx: 290 |  Loss: (0.1977) |  Loss2: (0.0000) | Acc: (93.00%) (34713/37248)
Epoch: 116 | Batch_idx: 300 |  Loss: (0.1976) |  Loss2: (0.0000) | Acc: (93.00%) (35905/38528)
Epoch: 116 | Batch_idx: 310 |  Loss: (0.1976) |  Loss2: (0.0000) | Acc: (93.00%) (37092/39808)
Epoch: 116 | Batch_idx: 320 |  Loss: (0.1974) |  Loss2: (0.0000) | Acc: (93.00%) (38291/41088)
Epoch: 116 | Batch_idx: 330 |  Loss: (0.1968) |  Loss2: (0.0000) | Acc: (93.00%) (39491/42368)
Epoch: 116 | Batch_idx: 340 |  Loss: (0.1967) |  Loss2: (0.0000) | Acc: (93.00%) (40688/43648)
Epoch: 116 | Batch_idx: 350 |  Loss: (0.1969) |  Loss2: (0.0000) | Acc: (93.00%) (41885/44928)
Epoch: 116 | Batch_idx: 360 |  Loss: (0.1963) |  Loss2: (0.0000) | Acc: (93.00%) (43087/46208)
Epoch: 116 | Batch_idx: 370 |  Loss: (0.1960) |  Loss2: (0.0000) | Acc: (93.00%) (44284/47488)
Epoch: 116 | Batch_idx: 380 |  Loss: (0.1965) |  Loss2: (0.0000) | Acc: (93.00%) (45467/48768)
Epoch: 116 | Batch_idx: 390 |  Loss: (0.1961) |  Loss2: (0.0000) | Acc: (93.00%) (46626/50000)
# TEST : Loss: (0.4177) | Acc: (87.00%) (8742/10000)
percent tensor([0.5756, 0.4244], device='cuda:0')
percent tensor([0.5842, 0.4158], device='cuda:0')
percent tensor([0.5477, 0.4523], device='cuda:0')
percent tensor([0.6373, 0.3627], device='cuda:0')
percent tensor([0.5897, 0.4103], device='cuda:0')
percent tensor([0.6430, 0.3570], device='cuda:0')
percent tensor([0.6937, 0.3063], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 117 | Batch_idx: 0 |  Loss: (0.1881) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 117 | Batch_idx: 10 |  Loss: (0.1741) |  Loss2: (0.0000) | Acc: (93.00%) (1321/1408)
Epoch: 117 | Batch_idx: 20 |  Loss: (0.1608) |  Loss2: (0.0000) | Acc: (94.00%) (2540/2688)
Epoch: 117 | Batch_idx: 30 |  Loss: (0.1648) |  Loss2: (0.0000) | Acc: (94.00%) (3741/3968)
Epoch: 117 | Batch_idx: 40 |  Loss: (0.1696) |  Loss2: (0.0000) | Acc: (93.00%) (4933/5248)
Epoch: 117 | Batch_idx: 50 |  Loss: (0.1698) |  Loss2: (0.0000) | Acc: (94.00%) (6143/6528)
Epoch: 117 | Batch_idx: 60 |  Loss: (0.1704) |  Loss2: (0.0000) | Acc: (94.00%) (7348/7808)
Epoch: 117 | Batch_idx: 70 |  Loss: (0.1725) |  Loss2: (0.0000) | Acc: (94.00%) (8547/9088)
Epoch: 117 | Batch_idx: 80 |  Loss: (0.1770) |  Loss2: (0.0000) | Acc: (93.00%) (9737/10368)
Epoch: 117 | Batch_idx: 90 |  Loss: (0.1792) |  Loss2: (0.0000) | Acc: (93.00%) (10927/11648)
Epoch: 117 | Batch_idx: 100 |  Loss: (0.1786) |  Loss2: (0.0000) | Acc: (93.00%) (12128/12928)
Epoch: 117 | Batch_idx: 110 |  Loss: (0.1790) |  Loss2: (0.0000) | Acc: (93.00%) (13328/14208)
Epoch: 117 | Batch_idx: 120 |  Loss: (0.1794) |  Loss2: (0.0000) | Acc: (93.00%) (14530/15488)
Epoch: 117 | Batch_idx: 130 |  Loss: (0.1795) |  Loss2: (0.0000) | Acc: (93.00%) (15722/16768)
Epoch: 117 | Batch_idx: 140 |  Loss: (0.1785) |  Loss2: (0.0000) | Acc: (93.00%) (16933/18048)
Epoch: 117 | Batch_idx: 150 |  Loss: (0.1771) |  Loss2: (0.0000) | Acc: (93.00%) (18151/19328)
Epoch: 117 | Batch_idx: 160 |  Loss: (0.1774) |  Loss2: (0.0000) | Acc: (93.00%) (19346/20608)
Epoch: 117 | Batch_idx: 170 |  Loss: (0.1762) |  Loss2: (0.0000) | Acc: (93.00%) (20557/21888)
Epoch: 117 | Batch_idx: 180 |  Loss: (0.1758) |  Loss2: (0.0000) | Acc: (93.00%) (21764/23168)
Epoch: 117 | Batch_idx: 190 |  Loss: (0.1780) |  Loss2: (0.0000) | Acc: (93.00%) (22944/24448)
Epoch: 117 | Batch_idx: 200 |  Loss: (0.1783) |  Loss2: (0.0000) | Acc: (93.00%) (24155/25728)
Epoch: 117 | Batch_idx: 210 |  Loss: (0.1790) |  Loss2: (0.0000) | Acc: (93.00%) (25349/27008)
Epoch: 117 | Batch_idx: 220 |  Loss: (0.1801) |  Loss2: (0.0000) | Acc: (93.00%) (26540/28288)
Epoch: 117 | Batch_idx: 230 |  Loss: (0.1801) |  Loss2: (0.0000) | Acc: (93.00%) (27733/29568)
Epoch: 117 | Batch_idx: 240 |  Loss: (0.1801) |  Loss2: (0.0000) | Acc: (93.00%) (28934/30848)
Epoch: 117 | Batch_idx: 250 |  Loss: (0.1801) |  Loss2: (0.0000) | Acc: (93.00%) (30142/32128)
Epoch: 117 | Batch_idx: 260 |  Loss: (0.1796) |  Loss2: (0.0000) | Acc: (93.00%) (31352/33408)
Epoch: 117 | Batch_idx: 270 |  Loss: (0.1792) |  Loss2: (0.0000) | Acc: (93.00%) (32561/34688)
Epoch: 117 | Batch_idx: 280 |  Loss: (0.1798) |  Loss2: (0.0000) | Acc: (93.00%) (33751/35968)
Epoch: 117 | Batch_idx: 290 |  Loss: (0.1802) |  Loss2: (0.0000) | Acc: (93.00%) (34955/37248)
Epoch: 117 | Batch_idx: 300 |  Loss: (0.1805) |  Loss2: (0.0000) | Acc: (93.00%) (36150/38528)
Epoch: 117 | Batch_idx: 310 |  Loss: (0.1804) |  Loss2: (0.0000) | Acc: (93.00%) (37355/39808)
Epoch: 117 | Batch_idx: 320 |  Loss: (0.1807) |  Loss2: (0.0000) | Acc: (93.00%) (38549/41088)
Epoch: 117 | Batch_idx: 330 |  Loss: (0.1809) |  Loss2: (0.0000) | Acc: (93.00%) (39745/42368)
Epoch: 117 | Batch_idx: 340 |  Loss: (0.1824) |  Loss2: (0.0000) | Acc: (93.00%) (40917/43648)
Epoch: 117 | Batch_idx: 350 |  Loss: (0.1833) |  Loss2: (0.0000) | Acc: (93.00%) (42096/44928)
Epoch: 117 | Batch_idx: 360 |  Loss: (0.1834) |  Loss2: (0.0000) | Acc: (93.00%) (43293/46208)
Epoch: 117 | Batch_idx: 370 |  Loss: (0.1833) |  Loss2: (0.0000) | Acc: (93.00%) (44496/47488)
Epoch: 117 | Batch_idx: 380 |  Loss: (0.1840) |  Loss2: (0.0000) | Acc: (93.00%) (45680/48768)
Epoch: 117 | Batch_idx: 390 |  Loss: (0.1840) |  Loss2: (0.0000) | Acc: (93.00%) (46837/50000)
# TEST : Loss: (0.4097) | Acc: (87.00%) (8766/10000)
percent tensor([0.5776, 0.4224], device='cuda:0')
percent tensor([0.5846, 0.4154], device='cuda:0')
percent tensor([0.5458, 0.4542], device='cuda:0')
percent tensor([0.6343, 0.3657], device='cuda:0')
percent tensor([0.5947, 0.4053], device='cuda:0')
percent tensor([0.6441, 0.3559], device='cuda:0')
percent tensor([0.7062, 0.2938], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 118 | Batch_idx: 0 |  Loss: (0.1886) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 118 | Batch_idx: 10 |  Loss: (0.2082) |  Loss2: (0.0000) | Acc: (92.00%) (1306/1408)
Epoch: 118 | Batch_idx: 20 |  Loss: (0.1985) |  Loss2: (0.0000) | Acc: (93.00%) (2501/2688)
Epoch: 118 | Batch_idx: 30 |  Loss: (0.1971) |  Loss2: (0.0000) | Acc: (93.00%) (3697/3968)
Epoch: 118 | Batch_idx: 40 |  Loss: (0.1914) |  Loss2: (0.0000) | Acc: (93.00%) (4905/5248)
Epoch: 118 | Batch_idx: 50 |  Loss: (0.1873) |  Loss2: (0.0000) | Acc: (93.00%) (6113/6528)
Epoch: 118 | Batch_idx: 60 |  Loss: (0.1870) |  Loss2: (0.0000) | Acc: (93.00%) (7305/7808)
Epoch: 118 | Batch_idx: 70 |  Loss: (0.1877) |  Loss2: (0.0000) | Acc: (93.00%) (8511/9088)
Epoch: 118 | Batch_idx: 80 |  Loss: (0.1847) |  Loss2: (0.0000) | Acc: (93.00%) (9724/10368)
Epoch: 118 | Batch_idx: 90 |  Loss: (0.1839) |  Loss2: (0.0000) | Acc: (93.00%) (10934/11648)
Epoch: 118 | Batch_idx: 100 |  Loss: (0.1833) |  Loss2: (0.0000) | Acc: (93.00%) (12137/12928)
Epoch: 118 | Batch_idx: 110 |  Loss: (0.1838) |  Loss2: (0.0000) | Acc: (93.00%) (13336/14208)
Epoch: 118 | Batch_idx: 120 |  Loss: (0.1813) |  Loss2: (0.0000) | Acc: (93.00%) (14541/15488)
Epoch: 118 | Batch_idx: 130 |  Loss: (0.1791) |  Loss2: (0.0000) | Acc: (93.00%) (15760/16768)
Epoch: 118 | Batch_idx: 140 |  Loss: (0.1783) |  Loss2: (0.0000) | Acc: (94.00%) (16970/18048)
Epoch: 118 | Batch_idx: 150 |  Loss: (0.1792) |  Loss2: (0.0000) | Acc: (93.00%) (18164/19328)
Epoch: 118 | Batch_idx: 160 |  Loss: (0.1798) |  Loss2: (0.0000) | Acc: (93.00%) (19358/20608)
Epoch: 118 | Batch_idx: 170 |  Loss: (0.1806) |  Loss2: (0.0000) | Acc: (93.00%) (20553/21888)
Epoch: 118 | Batch_idx: 180 |  Loss: (0.1814) |  Loss2: (0.0000) | Acc: (93.00%) (21753/23168)
Epoch: 118 | Batch_idx: 190 |  Loss: (0.1810) |  Loss2: (0.0000) | Acc: (93.00%) (22957/24448)
Epoch: 118 | Batch_idx: 200 |  Loss: (0.1799) |  Loss2: (0.0000) | Acc: (93.00%) (24159/25728)
Epoch: 118 | Batch_idx: 210 |  Loss: (0.1794) |  Loss2: (0.0000) | Acc: (93.00%) (25363/27008)
Epoch: 118 | Batch_idx: 220 |  Loss: (0.1809) |  Loss2: (0.0000) | Acc: (93.00%) (26560/28288)
Epoch: 118 | Batch_idx: 230 |  Loss: (0.1809) |  Loss2: (0.0000) | Acc: (93.00%) (27760/29568)
Epoch: 118 | Batch_idx: 240 |  Loss: (0.1804) |  Loss2: (0.0000) | Acc: (93.00%) (28960/30848)
Epoch: 118 | Batch_idx: 250 |  Loss: (0.1803) |  Loss2: (0.0000) | Acc: (93.00%) (30158/32128)
Epoch: 118 | Batch_idx: 260 |  Loss: (0.1799) |  Loss2: (0.0000) | Acc: (93.00%) (31363/33408)
Epoch: 118 | Batch_idx: 270 |  Loss: (0.1810) |  Loss2: (0.0000) | Acc: (93.00%) (32545/34688)
Epoch: 118 | Batch_idx: 280 |  Loss: (0.1806) |  Loss2: (0.0000) | Acc: (93.00%) (33753/35968)
Epoch: 118 | Batch_idx: 290 |  Loss: (0.1803) |  Loss2: (0.0000) | Acc: (93.00%) (34956/37248)
Epoch: 118 | Batch_idx: 300 |  Loss: (0.1803) |  Loss2: (0.0000) | Acc: (93.00%) (36151/38528)
Epoch: 118 | Batch_idx: 310 |  Loss: (0.1792) |  Loss2: (0.0000) | Acc: (93.00%) (37367/39808)
Epoch: 118 | Batch_idx: 320 |  Loss: (0.1799) |  Loss2: (0.0000) | Acc: (93.00%) (38545/41088)
Epoch: 118 | Batch_idx: 330 |  Loss: (0.1798) |  Loss2: (0.0000) | Acc: (93.00%) (39748/42368)
Epoch: 118 | Batch_idx: 340 |  Loss: (0.1792) |  Loss2: (0.0000) | Acc: (93.00%) (40961/43648)
Epoch: 118 | Batch_idx: 350 |  Loss: (0.1792) |  Loss2: (0.0000) | Acc: (93.00%) (42163/44928)
Epoch: 118 | Batch_idx: 360 |  Loss: (0.1792) |  Loss2: (0.0000) | Acc: (93.00%) (43364/46208)
Epoch: 118 | Batch_idx: 370 |  Loss: (0.1789) |  Loss2: (0.0000) | Acc: (93.00%) (44575/47488)
Epoch: 118 | Batch_idx: 380 |  Loss: (0.1791) |  Loss2: (0.0000) | Acc: (93.00%) (45778/48768)
Epoch: 118 | Batch_idx: 390 |  Loss: (0.1791) |  Loss2: (0.0000) | Acc: (93.00%) (46942/50000)
# TEST : Loss: (0.4041) | Acc: (87.00%) (8769/10000)
percent tensor([0.5773, 0.4227], device='cuda:0')
percent tensor([0.5820, 0.4180], device='cuda:0')
percent tensor([0.5459, 0.4541], device='cuda:0')
percent tensor([0.6364, 0.3636], device='cuda:0')
percent tensor([0.5946, 0.4054], device='cuda:0')
percent tensor([0.6421, 0.3579], device='cuda:0')
percent tensor([0.7099, 0.2901], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 119 | Batch_idx: 0 |  Loss: (0.1825) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 119 | Batch_idx: 10 |  Loss: (0.1992) |  Loss2: (0.0000) | Acc: (93.00%) (1314/1408)
Epoch: 119 | Batch_idx: 20 |  Loss: (0.1878) |  Loss2: (0.0000) | Acc: (93.00%) (2514/2688)
Epoch: 119 | Batch_idx: 30 |  Loss: (0.1793) |  Loss2: (0.0000) | Acc: (93.00%) (3724/3968)
Epoch: 119 | Batch_idx: 40 |  Loss: (0.1738) |  Loss2: (0.0000) | Acc: (93.00%) (4930/5248)
Epoch: 119 | Batch_idx: 50 |  Loss: (0.1785) |  Loss2: (0.0000) | Acc: (93.00%) (6123/6528)
Epoch: 119 | Batch_idx: 60 |  Loss: (0.1777) |  Loss2: (0.0000) | Acc: (93.00%) (7335/7808)
Epoch: 119 | Batch_idx: 70 |  Loss: (0.1761) |  Loss2: (0.0000) | Acc: (93.00%) (8541/9088)
Epoch: 119 | Batch_idx: 80 |  Loss: (0.1760) |  Loss2: (0.0000) | Acc: (94.00%) (9749/10368)
Epoch: 119 | Batch_idx: 90 |  Loss: (0.1736) |  Loss2: (0.0000) | Acc: (94.00%) (10957/11648)
Epoch: 119 | Batch_idx: 100 |  Loss: (0.1721) |  Loss2: (0.0000) | Acc: (94.00%) (12170/12928)
Epoch: 119 | Batch_idx: 110 |  Loss: (0.1712) |  Loss2: (0.0000) | Acc: (94.00%) (13379/14208)
Epoch: 119 | Batch_idx: 120 |  Loss: (0.1712) |  Loss2: (0.0000) | Acc: (94.00%) (14586/15488)
Epoch: 119 | Batch_idx: 130 |  Loss: (0.1733) |  Loss2: (0.0000) | Acc: (94.00%) (15775/16768)
Epoch: 119 | Batch_idx: 140 |  Loss: (0.1722) |  Loss2: (0.0000) | Acc: (94.00%) (16995/18048)
Epoch: 119 | Batch_idx: 150 |  Loss: (0.1726) |  Loss2: (0.0000) | Acc: (94.00%) (18194/19328)
Epoch: 119 | Batch_idx: 160 |  Loss: (0.1747) |  Loss2: (0.0000) | Acc: (94.00%) (19381/20608)
Epoch: 119 | Batch_idx: 170 |  Loss: (0.1748) |  Loss2: (0.0000) | Acc: (94.00%) (20577/21888)
Epoch: 119 | Batch_idx: 180 |  Loss: (0.1743) |  Loss2: (0.0000) | Acc: (94.00%) (21793/23168)
Epoch: 119 | Batch_idx: 190 |  Loss: (0.1740) |  Loss2: (0.0000) | Acc: (94.00%) (22998/24448)
Epoch: 119 | Batch_idx: 200 |  Loss: (0.1722) |  Loss2: (0.0000) | Acc: (94.00%) (24220/25728)
Epoch: 119 | Batch_idx: 210 |  Loss: (0.1719) |  Loss2: (0.0000) | Acc: (94.00%) (25436/27008)
Epoch: 119 | Batch_idx: 220 |  Loss: (0.1727) |  Loss2: (0.0000) | Acc: (94.00%) (26634/28288)
Epoch: 119 | Batch_idx: 230 |  Loss: (0.1731) |  Loss2: (0.0000) | Acc: (94.00%) (27847/29568)
Epoch: 119 | Batch_idx: 240 |  Loss: (0.1727) |  Loss2: (0.0000) | Acc: (94.00%) (29057/30848)
Epoch: 119 | Batch_idx: 250 |  Loss: (0.1732) |  Loss2: (0.0000) | Acc: (94.00%) (30249/32128)
Epoch: 119 | Batch_idx: 260 |  Loss: (0.1724) |  Loss2: (0.0000) | Acc: (94.00%) (31457/33408)
Epoch: 119 | Batch_idx: 270 |  Loss: (0.1723) |  Loss2: (0.0000) | Acc: (94.00%) (32665/34688)
Epoch: 119 | Batch_idx: 280 |  Loss: (0.1725) |  Loss2: (0.0000) | Acc: (94.00%) (33865/35968)
Epoch: 119 | Batch_idx: 290 |  Loss: (0.1725) |  Loss2: (0.0000) | Acc: (94.00%) (35069/37248)
Epoch: 119 | Batch_idx: 300 |  Loss: (0.1725) |  Loss2: (0.0000) | Acc: (94.00%) (36269/38528)
Epoch: 119 | Batch_idx: 310 |  Loss: (0.1733) |  Loss2: (0.0000) | Acc: (94.00%) (37467/39808)
Epoch: 119 | Batch_idx: 320 |  Loss: (0.1737) |  Loss2: (0.0000) | Acc: (94.00%) (38656/41088)
Epoch: 119 | Batch_idx: 330 |  Loss: (0.1733) |  Loss2: (0.0000) | Acc: (94.00%) (39867/42368)
Epoch: 119 | Batch_idx: 340 |  Loss: (0.1730) |  Loss2: (0.0000) | Acc: (94.00%) (41075/43648)
Epoch: 119 | Batch_idx: 350 |  Loss: (0.1733) |  Loss2: (0.0000) | Acc: (94.00%) (42277/44928)
Epoch: 119 | Batch_idx: 360 |  Loss: (0.1730) |  Loss2: (0.0000) | Acc: (94.00%) (43493/46208)
Epoch: 119 | Batch_idx: 370 |  Loss: (0.1735) |  Loss2: (0.0000) | Acc: (94.00%) (44682/47488)
Epoch: 119 | Batch_idx: 380 |  Loss: (0.1728) |  Loss2: (0.0000) | Acc: (94.00%) (45909/48768)
Epoch: 119 | Batch_idx: 390 |  Loss: (0.1727) |  Loss2: (0.0000) | Acc: (94.00%) (47083/50000)
# TEST : Loss: (0.4025) | Acc: (87.00%) (8770/10000)
percent tensor([0.5766, 0.4234], device='cuda:0')
percent tensor([0.5806, 0.4194], device='cuda:0')
percent tensor([0.5473, 0.4527], device='cuda:0')
percent tensor([0.6358, 0.3642], device='cuda:0')
percent tensor([0.5937, 0.4063], device='cuda:0')
percent tensor([0.6510, 0.3490], device='cuda:0')
percent tensor([0.7150, 0.2850], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 120 | Batch_idx: 0 |  Loss: (0.1685) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 120 | Batch_idx: 10 |  Loss: (0.1567) |  Loss2: (0.0000) | Acc: (94.00%) (1333/1408)
Epoch: 120 | Batch_idx: 20 |  Loss: (0.1631) |  Loss2: (0.0000) | Acc: (94.00%) (2545/2688)
Epoch: 120 | Batch_idx: 30 |  Loss: (0.1546) |  Loss2: (0.0000) | Acc: (94.00%) (3765/3968)
Epoch: 120 | Batch_idx: 40 |  Loss: (0.1636) |  Loss2: (0.0000) | Acc: (94.00%) (4956/5248)
Epoch: 120 | Batch_idx: 50 |  Loss: (0.1639) |  Loss2: (0.0000) | Acc: (94.00%) (6159/6528)
Epoch: 120 | Batch_idx: 60 |  Loss: (0.1630) |  Loss2: (0.0000) | Acc: (94.00%) (7369/7808)
Epoch: 120 | Batch_idx: 70 |  Loss: (0.1640) |  Loss2: (0.0000) | Acc: (94.00%) (8577/9088)
Epoch: 120 | Batch_idx: 80 |  Loss: (0.1645) |  Loss2: (0.0000) | Acc: (94.00%) (9784/10368)
Epoch: 120 | Batch_idx: 90 |  Loss: (0.1680) |  Loss2: (0.0000) | Acc: (94.00%) (10979/11648)
Epoch: 120 | Batch_idx: 100 |  Loss: (0.1686) |  Loss2: (0.0000) | Acc: (94.00%) (12184/12928)
Epoch: 120 | Batch_idx: 110 |  Loss: (0.1679) |  Loss2: (0.0000) | Acc: (94.00%) (13389/14208)
Epoch: 120 | Batch_idx: 120 |  Loss: (0.1682) |  Loss2: (0.0000) | Acc: (94.00%) (14588/15488)
Epoch: 120 | Batch_idx: 130 |  Loss: (0.1720) |  Loss2: (0.0000) | Acc: (94.00%) (15769/16768)
Epoch: 120 | Batch_idx: 140 |  Loss: (0.1729) |  Loss2: (0.0000) | Acc: (93.00%) (16964/18048)
Epoch: 120 | Batch_idx: 150 |  Loss: (0.1745) |  Loss2: (0.0000) | Acc: (93.00%) (18155/19328)
Epoch: 120 | Batch_idx: 160 |  Loss: (0.1743) |  Loss2: (0.0000) | Acc: (93.00%) (19357/20608)
Epoch: 120 | Batch_idx: 170 |  Loss: (0.1738) |  Loss2: (0.0000) | Acc: (93.00%) (20561/21888)
Epoch: 120 | Batch_idx: 180 |  Loss: (0.1739) |  Loss2: (0.0000) | Acc: (93.00%) (21773/23168)
Epoch: 120 | Batch_idx: 190 |  Loss: (0.1753) |  Loss2: (0.0000) | Acc: (93.00%) (22969/24448)
Epoch: 120 | Batch_idx: 200 |  Loss: (0.1752) |  Loss2: (0.0000) | Acc: (93.00%) (24180/25728)
Epoch: 120 | Batch_idx: 210 |  Loss: (0.1755) |  Loss2: (0.0000) | Acc: (93.00%) (25370/27008)
Epoch: 120 | Batch_idx: 220 |  Loss: (0.1752) |  Loss2: (0.0000) | Acc: (93.00%) (26571/28288)
Epoch: 120 | Batch_idx: 230 |  Loss: (0.1765) |  Loss2: (0.0000) | Acc: (93.00%) (27760/29568)
Epoch: 120 | Batch_idx: 240 |  Loss: (0.1775) |  Loss2: (0.0000) | Acc: (93.00%) (28958/30848)
Epoch: 120 | Batch_idx: 250 |  Loss: (0.1789) |  Loss2: (0.0000) | Acc: (93.00%) (30136/32128)
Epoch: 120 | Batch_idx: 260 |  Loss: (0.1786) |  Loss2: (0.0000) | Acc: (93.00%) (31346/33408)
Epoch: 120 | Batch_idx: 270 |  Loss: (0.1800) |  Loss2: (0.0000) | Acc: (93.00%) (32531/34688)
Epoch: 120 | Batch_idx: 280 |  Loss: (0.1796) |  Loss2: (0.0000) | Acc: (93.00%) (33738/35968)
Epoch: 120 | Batch_idx: 290 |  Loss: (0.1802) |  Loss2: (0.0000) | Acc: (93.00%) (34935/37248)
Epoch: 120 | Batch_idx: 300 |  Loss: (0.1802) |  Loss2: (0.0000) | Acc: (93.00%) (36131/38528)
Epoch: 120 | Batch_idx: 310 |  Loss: (0.1805) |  Loss2: (0.0000) | Acc: (93.00%) (37324/39808)
Epoch: 120 | Batch_idx: 320 |  Loss: (0.1800) |  Loss2: (0.0000) | Acc: (93.00%) (38539/41088)
Epoch: 120 | Batch_idx: 330 |  Loss: (0.1789) |  Loss2: (0.0000) | Acc: (93.00%) (39759/42368)
Epoch: 120 | Batch_idx: 340 |  Loss: (0.1790) |  Loss2: (0.0000) | Acc: (93.00%) (40960/43648)
Epoch: 120 | Batch_idx: 350 |  Loss: (0.1791) |  Loss2: (0.0000) | Acc: (93.00%) (42147/44928)
Epoch: 120 | Batch_idx: 360 |  Loss: (0.1789) |  Loss2: (0.0000) | Acc: (93.00%) (43359/46208)
Epoch: 120 | Batch_idx: 370 |  Loss: (0.1798) |  Loss2: (0.0000) | Acc: (93.00%) (44543/47488)
Epoch: 120 | Batch_idx: 380 |  Loss: (0.1804) |  Loss2: (0.0000) | Acc: (93.00%) (45726/48768)
Epoch: 120 | Batch_idx: 390 |  Loss: (0.1799) |  Loss2: (0.0000) | Acc: (93.00%) (46889/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_120.pth.tar'
# TEST : Loss: (0.4365) | Acc: (86.00%) (8696/10000)
percent tensor([0.5757, 0.4243], device='cuda:0')
percent tensor([0.5810, 0.4190], device='cuda:0')
percent tensor([0.5475, 0.4525], device='cuda:0')
percent tensor([0.6358, 0.3642], device='cuda:0')
percent tensor([0.5941, 0.4059], device='cuda:0')
percent tensor([0.6524, 0.3476], device='cuda:0')
percent tensor([0.7171, 0.2829], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(180.4536, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(818.5090, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(816.9260, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1523.0393, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(490.3608, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2259.7393, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4275.3574, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1383.9344, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6180.3467, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11801.0225, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3901.3828, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16505.6094, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 121 | Batch_idx: 0 |  Loss: (0.2271) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 121 | Batch_idx: 10 |  Loss: (0.1660) |  Loss2: (0.0000) | Acc: (94.00%) (1327/1408)
Epoch: 121 | Batch_idx: 20 |  Loss: (0.1736) |  Loss2: (0.0000) | Acc: (94.00%) (2529/2688)
Epoch: 121 | Batch_idx: 30 |  Loss: (0.1666) |  Loss2: (0.0000) | Acc: (94.00%) (3741/3968)
Epoch: 121 | Batch_idx: 40 |  Loss: (0.1696) |  Loss2: (0.0000) | Acc: (93.00%) (4933/5248)
Epoch: 121 | Batch_idx: 50 |  Loss: (0.1728) |  Loss2: (0.0000) | Acc: (93.00%) (6135/6528)
Epoch: 121 | Batch_idx: 60 |  Loss: (0.1703) |  Loss2: (0.0000) | Acc: (94.00%) (7351/7808)
Epoch: 121 | Batch_idx: 70 |  Loss: (0.1684) |  Loss2: (0.0000) | Acc: (94.00%) (8560/9088)
Epoch: 121 | Batch_idx: 80 |  Loss: (0.1703) |  Loss2: (0.0000) | Acc: (94.00%) (9760/10368)
Epoch: 121 | Batch_idx: 90 |  Loss: (0.1679) |  Loss2: (0.0000) | Acc: (94.00%) (10975/11648)
Epoch: 121 | Batch_idx: 100 |  Loss: (0.1701) |  Loss2: (0.0000) | Acc: (94.00%) (12177/12928)
Epoch: 121 | Batch_idx: 110 |  Loss: (0.1715) |  Loss2: (0.0000) | Acc: (94.00%) (13366/14208)
Epoch: 121 | Batch_idx: 120 |  Loss: (0.1727) |  Loss2: (0.0000) | Acc: (94.00%) (14561/15488)
Epoch: 121 | Batch_idx: 130 |  Loss: (0.1727) |  Loss2: (0.0000) | Acc: (94.00%) (15768/16768)
Epoch: 121 | Batch_idx: 140 |  Loss: (0.1730) |  Loss2: (0.0000) | Acc: (94.00%) (16969/18048)
Epoch: 121 | Batch_idx: 150 |  Loss: (0.1728) |  Loss2: (0.0000) | Acc: (94.00%) (18177/19328)
Epoch: 121 | Batch_idx: 160 |  Loss: (0.1722) |  Loss2: (0.0000) | Acc: (94.00%) (19379/20608)
Epoch: 121 | Batch_idx: 170 |  Loss: (0.1725) |  Loss2: (0.0000) | Acc: (94.00%) (20579/21888)
Epoch: 121 | Batch_idx: 180 |  Loss: (0.1716) |  Loss2: (0.0000) | Acc: (94.00%) (21795/23168)
Epoch: 121 | Batch_idx: 190 |  Loss: (0.1728) |  Loss2: (0.0000) | Acc: (94.00%) (22990/24448)
Epoch: 121 | Batch_idx: 200 |  Loss: (0.1721) |  Loss2: (0.0000) | Acc: (94.00%) (24197/25728)
Epoch: 121 | Batch_idx: 210 |  Loss: (0.1718) |  Loss2: (0.0000) | Acc: (94.00%) (25397/27008)
Epoch: 121 | Batch_idx: 220 |  Loss: (0.1717) |  Loss2: (0.0000) | Acc: (94.00%) (26607/28288)
Epoch: 121 | Batch_idx: 230 |  Loss: (0.1734) |  Loss2: (0.0000) | Acc: (94.00%) (27796/29568)
Epoch: 121 | Batch_idx: 240 |  Loss: (0.1736) |  Loss2: (0.0000) | Acc: (93.00%) (28994/30848)
Epoch: 121 | Batch_idx: 250 |  Loss: (0.1731) |  Loss2: (0.0000) | Acc: (94.00%) (30202/32128)
Epoch: 121 | Batch_idx: 260 |  Loss: (0.1726) |  Loss2: (0.0000) | Acc: (94.00%) (31409/33408)
Epoch: 121 | Batch_idx: 270 |  Loss: (0.1727) |  Loss2: (0.0000) | Acc: (94.00%) (32607/34688)
Epoch: 121 | Batch_idx: 280 |  Loss: (0.1726) |  Loss2: (0.0000) | Acc: (93.00%) (33807/35968)
Epoch: 121 | Batch_idx: 290 |  Loss: (0.1724) |  Loss2: (0.0000) | Acc: (93.00%) (35009/37248)
Epoch: 121 | Batch_idx: 300 |  Loss: (0.1729) |  Loss2: (0.0000) | Acc: (93.00%) (36198/38528)
Epoch: 121 | Batch_idx: 310 |  Loss: (0.1729) |  Loss2: (0.0000) | Acc: (93.00%) (37399/39808)
Epoch: 121 | Batch_idx: 320 |  Loss: (0.1726) |  Loss2: (0.0000) | Acc: (93.00%) (38610/41088)
Epoch: 121 | Batch_idx: 330 |  Loss: (0.1729) |  Loss2: (0.0000) | Acc: (93.00%) (39807/42368)
Epoch: 121 | Batch_idx: 340 |  Loss: (0.1732) |  Loss2: (0.0000) | Acc: (93.00%) (41008/43648)
Epoch: 121 | Batch_idx: 350 |  Loss: (0.1742) |  Loss2: (0.0000) | Acc: (93.00%) (42192/44928)
Epoch: 121 | Batch_idx: 360 |  Loss: (0.1749) |  Loss2: (0.0000) | Acc: (93.00%) (43389/46208)
Epoch: 121 | Batch_idx: 370 |  Loss: (0.1748) |  Loss2: (0.0000) | Acc: (93.00%) (44584/47488)
Epoch: 121 | Batch_idx: 380 |  Loss: (0.1748) |  Loss2: (0.0000) | Acc: (93.00%) (45788/48768)
Epoch: 121 | Batch_idx: 390 |  Loss: (0.1746) |  Loss2: (0.0000) | Acc: (93.00%) (46946/50000)
# TEST : Loss: (0.4210) | Acc: (87.00%) (8733/10000)
percent tensor([0.5764, 0.4236], device='cuda:0')
percent tensor([0.5805, 0.4195], device='cuda:0')
percent tensor([0.5478, 0.4522], device='cuda:0')
percent tensor([0.6348, 0.3652], device='cuda:0')
percent tensor([0.5936, 0.4064], device='cuda:0')
percent tensor([0.6515, 0.3485], device='cuda:0')
percent tensor([0.7168, 0.2832], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 122 | Batch_idx: 0 |  Loss: (0.1261) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 122 | Batch_idx: 10 |  Loss: (0.1864) |  Loss2: (0.0000) | Acc: (93.00%) (1313/1408)
Epoch: 122 | Batch_idx: 20 |  Loss: (0.1725) |  Loss2: (0.0000) | Acc: (94.00%) (2528/2688)
Epoch: 122 | Batch_idx: 30 |  Loss: (0.1696) |  Loss2: (0.0000) | Acc: (94.00%) (3736/3968)
Epoch: 122 | Batch_idx: 40 |  Loss: (0.1638) |  Loss2: (0.0000) | Acc: (94.00%) (4952/5248)
Epoch: 122 | Batch_idx: 50 |  Loss: (0.1695) |  Loss2: (0.0000) | Acc: (94.00%) (6145/6528)
Epoch: 122 | Batch_idx: 60 |  Loss: (0.1698) |  Loss2: (0.0000) | Acc: (94.00%) (7349/7808)
Epoch: 122 | Batch_idx: 70 |  Loss: (0.1669) |  Loss2: (0.0000) | Acc: (94.00%) (8557/9088)
Epoch: 122 | Batch_idx: 80 |  Loss: (0.1641) |  Loss2: (0.0000) | Acc: (94.00%) (9778/10368)
Epoch: 122 | Batch_idx: 90 |  Loss: (0.1632) |  Loss2: (0.0000) | Acc: (94.00%) (10995/11648)
Epoch: 122 | Batch_idx: 100 |  Loss: (0.1639) |  Loss2: (0.0000) | Acc: (94.00%) (12200/12928)
Epoch: 122 | Batch_idx: 110 |  Loss: (0.1641) |  Loss2: (0.0000) | Acc: (94.00%) (13408/14208)
Epoch: 122 | Batch_idx: 120 |  Loss: (0.1668) |  Loss2: (0.0000) | Acc: (94.00%) (14590/15488)
Epoch: 122 | Batch_idx: 130 |  Loss: (0.1674) |  Loss2: (0.0000) | Acc: (94.00%) (15788/16768)
Epoch: 122 | Batch_idx: 140 |  Loss: (0.1678) |  Loss2: (0.0000) | Acc: (94.00%) (16987/18048)
Epoch: 122 | Batch_idx: 150 |  Loss: (0.1691) |  Loss2: (0.0000) | Acc: (94.00%) (18187/19328)
Epoch: 122 | Batch_idx: 160 |  Loss: (0.1693) |  Loss2: (0.0000) | Acc: (94.00%) (19385/20608)
Epoch: 122 | Batch_idx: 170 |  Loss: (0.1675) |  Loss2: (0.0000) | Acc: (94.00%) (20605/21888)
Epoch: 122 | Batch_idx: 180 |  Loss: (0.1673) |  Loss2: (0.0000) | Acc: (94.00%) (21814/23168)
Epoch: 122 | Batch_idx: 190 |  Loss: (0.1662) |  Loss2: (0.0000) | Acc: (94.00%) (23027/24448)
Epoch: 122 | Batch_idx: 200 |  Loss: (0.1669) |  Loss2: (0.0000) | Acc: (94.00%) (24230/25728)
Epoch: 122 | Batch_idx: 210 |  Loss: (0.1673) |  Loss2: (0.0000) | Acc: (94.00%) (25431/27008)
Epoch: 122 | Batch_idx: 220 |  Loss: (0.1678) |  Loss2: (0.0000) | Acc: (94.00%) (26635/28288)
Epoch: 122 | Batch_idx: 230 |  Loss: (0.1695) |  Loss2: (0.0000) | Acc: (94.00%) (27822/29568)
Epoch: 122 | Batch_idx: 240 |  Loss: (0.1684) |  Loss2: (0.0000) | Acc: (94.00%) (29039/30848)
Epoch: 122 | Batch_idx: 250 |  Loss: (0.1695) |  Loss2: (0.0000) | Acc: (94.00%) (30228/32128)
Epoch: 122 | Batch_idx: 260 |  Loss: (0.1693) |  Loss2: (0.0000) | Acc: (94.00%) (31441/33408)
Epoch: 122 | Batch_idx: 270 |  Loss: (0.1705) |  Loss2: (0.0000) | Acc: (94.00%) (32633/34688)
Epoch: 122 | Batch_idx: 280 |  Loss: (0.1698) |  Loss2: (0.0000) | Acc: (94.00%) (33853/35968)
Epoch: 122 | Batch_idx: 290 |  Loss: (0.1694) |  Loss2: (0.0000) | Acc: (94.00%) (35061/37248)
Epoch: 122 | Batch_idx: 300 |  Loss: (0.1704) |  Loss2: (0.0000) | Acc: (94.00%) (36248/38528)
Epoch: 122 | Batch_idx: 310 |  Loss: (0.1701) |  Loss2: (0.0000) | Acc: (94.00%) (37456/39808)
Epoch: 122 | Batch_idx: 320 |  Loss: (0.1707) |  Loss2: (0.0000) | Acc: (94.00%) (38648/41088)
Epoch: 122 | Batch_idx: 330 |  Loss: (0.1707) |  Loss2: (0.0000) | Acc: (94.00%) (39854/42368)
Epoch: 122 | Batch_idx: 340 |  Loss: (0.1705) |  Loss2: (0.0000) | Acc: (94.00%) (41058/43648)
Epoch: 122 | Batch_idx: 350 |  Loss: (0.1699) |  Loss2: (0.0000) | Acc: (94.00%) (42271/44928)
Epoch: 122 | Batch_idx: 360 |  Loss: (0.1699) |  Loss2: (0.0000) | Acc: (94.00%) (43475/46208)
Epoch: 122 | Batch_idx: 370 |  Loss: (0.1700) |  Loss2: (0.0000) | Acc: (94.00%) (44678/47488)
Epoch: 122 | Batch_idx: 380 |  Loss: (0.1706) |  Loss2: (0.0000) | Acc: (94.00%) (45874/48768)
Epoch: 122 | Batch_idx: 390 |  Loss: (0.1706) |  Loss2: (0.0000) | Acc: (94.00%) (47020/50000)
# TEST : Loss: (0.4735) | Acc: (85.00%) (8585/10000)
percent tensor([0.5771, 0.4229], device='cuda:0')
percent tensor([0.5811, 0.4189], device='cuda:0')
percent tensor([0.5473, 0.4527], device='cuda:0')
percent tensor([0.6353, 0.3647], device='cuda:0')
percent tensor([0.5922, 0.4078], device='cuda:0')
percent tensor([0.6502, 0.3498], device='cuda:0')
percent tensor([0.7143, 0.2857], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 123 | Batch_idx: 0 |  Loss: (0.1654) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 123 | Batch_idx: 10 |  Loss: (0.1662) |  Loss2: (0.0000) | Acc: (94.00%) (1329/1408)
Epoch: 123 | Batch_idx: 20 |  Loss: (0.1482) |  Loss2: (0.0000) | Acc: (95.00%) (2562/2688)
Epoch: 123 | Batch_idx: 30 |  Loss: (0.1508) |  Loss2: (0.0000) | Acc: (95.00%) (3779/3968)
Epoch: 123 | Batch_idx: 40 |  Loss: (0.1593) |  Loss2: (0.0000) | Acc: (95.00%) (4986/5248)
Epoch: 123 | Batch_idx: 50 |  Loss: (0.1564) |  Loss2: (0.0000) | Acc: (94.00%) (6198/6528)
Epoch: 123 | Batch_idx: 60 |  Loss: (0.1595) |  Loss2: (0.0000) | Acc: (94.00%) (7404/7808)
Epoch: 123 | Batch_idx: 70 |  Loss: (0.1605) |  Loss2: (0.0000) | Acc: (94.00%) (8614/9088)
Epoch: 123 | Batch_idx: 80 |  Loss: (0.1632) |  Loss2: (0.0000) | Acc: (94.00%) (9814/10368)
Epoch: 123 | Batch_idx: 90 |  Loss: (0.1623) |  Loss2: (0.0000) | Acc: (94.00%) (11030/11648)
Epoch: 123 | Batch_idx: 100 |  Loss: (0.1642) |  Loss2: (0.0000) | Acc: (94.00%) (12238/12928)
Epoch: 123 | Batch_idx: 110 |  Loss: (0.1632) |  Loss2: (0.0000) | Acc: (94.00%) (13457/14208)
Epoch: 123 | Batch_idx: 120 |  Loss: (0.1623) |  Loss2: (0.0000) | Acc: (94.00%) (14668/15488)
Epoch: 123 | Batch_idx: 130 |  Loss: (0.1614) |  Loss2: (0.0000) | Acc: (94.00%) (15882/16768)
Epoch: 123 | Batch_idx: 140 |  Loss: (0.1612) |  Loss2: (0.0000) | Acc: (94.00%) (17090/18048)
Epoch: 123 | Batch_idx: 150 |  Loss: (0.1603) |  Loss2: (0.0000) | Acc: (94.00%) (18311/19328)
Epoch: 123 | Batch_idx: 160 |  Loss: (0.1606) |  Loss2: (0.0000) | Acc: (94.00%) (19513/20608)
Epoch: 123 | Batch_idx: 170 |  Loss: (0.1606) |  Loss2: (0.0000) | Acc: (94.00%) (20726/21888)
Epoch: 123 | Batch_idx: 180 |  Loss: (0.1606) |  Loss2: (0.0000) | Acc: (94.00%) (21933/23168)
Epoch: 123 | Batch_idx: 190 |  Loss: (0.1625) |  Loss2: (0.0000) | Acc: (94.00%) (23129/24448)
Epoch: 123 | Batch_idx: 200 |  Loss: (0.1633) |  Loss2: (0.0000) | Acc: (94.00%) (24332/25728)
Epoch: 123 | Batch_idx: 210 |  Loss: (0.1628) |  Loss2: (0.0000) | Acc: (94.00%) (25541/27008)
Epoch: 123 | Batch_idx: 220 |  Loss: (0.1633) |  Loss2: (0.0000) | Acc: (94.00%) (26748/28288)
Epoch: 123 | Batch_idx: 230 |  Loss: (0.1631) |  Loss2: (0.0000) | Acc: (94.00%) (27965/29568)
Epoch: 123 | Batch_idx: 240 |  Loss: (0.1637) |  Loss2: (0.0000) | Acc: (94.00%) (29160/30848)
Epoch: 123 | Batch_idx: 250 |  Loss: (0.1648) |  Loss2: (0.0000) | Acc: (94.00%) (30362/32128)
Epoch: 123 | Batch_idx: 260 |  Loss: (0.1654) |  Loss2: (0.0000) | Acc: (94.00%) (31554/33408)
Epoch: 123 | Batch_idx: 270 |  Loss: (0.1653) |  Loss2: (0.0000) | Acc: (94.00%) (32753/34688)
Epoch: 123 | Batch_idx: 280 |  Loss: (0.1642) |  Loss2: (0.0000) | Acc: (94.00%) (33981/35968)
Epoch: 123 | Batch_idx: 290 |  Loss: (0.1651) |  Loss2: (0.0000) | Acc: (94.00%) (35177/37248)
Epoch: 123 | Batch_idx: 300 |  Loss: (0.1667) |  Loss2: (0.0000) | Acc: (94.00%) (36365/38528)
Epoch: 123 | Batch_idx: 310 |  Loss: (0.1674) |  Loss2: (0.0000) | Acc: (94.00%) (37568/39808)
Epoch: 123 | Batch_idx: 320 |  Loss: (0.1672) |  Loss2: (0.0000) | Acc: (94.00%) (38776/41088)
Epoch: 123 | Batch_idx: 330 |  Loss: (0.1676) |  Loss2: (0.0000) | Acc: (94.00%) (39964/42368)
Epoch: 123 | Batch_idx: 340 |  Loss: (0.1676) |  Loss2: (0.0000) | Acc: (94.00%) (41164/43648)
Epoch: 123 | Batch_idx: 350 |  Loss: (0.1679) |  Loss2: (0.0000) | Acc: (94.00%) (42368/44928)
Epoch: 123 | Batch_idx: 360 |  Loss: (0.1684) |  Loss2: (0.0000) | Acc: (94.00%) (43569/46208)
Epoch: 123 | Batch_idx: 370 |  Loss: (0.1679) |  Loss2: (0.0000) | Acc: (94.00%) (44784/47488)
Epoch: 123 | Batch_idx: 380 |  Loss: (0.1678) |  Loss2: (0.0000) | Acc: (94.00%) (45992/48768)
Epoch: 123 | Batch_idx: 390 |  Loss: (0.1683) |  Loss2: (0.0000) | Acc: (94.00%) (47147/50000)
# TEST : Loss: (0.4836) | Acc: (86.00%) (8603/10000)
percent tensor([0.5763, 0.4237], device='cuda:0')
percent tensor([0.5812, 0.4188], device='cuda:0')
percent tensor([0.5477, 0.4523], device='cuda:0')
percent tensor([0.6357, 0.3643], device='cuda:0')
percent tensor([0.5925, 0.4075], device='cuda:0')
percent tensor([0.6519, 0.3481], device='cuda:0')
percent tensor([0.7137, 0.2863], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 124 | Batch_idx: 0 |  Loss: (0.1533) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 124 | Batch_idx: 10 |  Loss: (0.1579) |  Loss2: (0.0000) | Acc: (94.00%) (1326/1408)
Epoch: 124 | Batch_idx: 20 |  Loss: (0.1583) |  Loss2: (0.0000) | Acc: (93.00%) (2526/2688)
Epoch: 124 | Batch_idx: 30 |  Loss: (0.1620) |  Loss2: (0.0000) | Acc: (93.00%) (3722/3968)
Epoch: 124 | Batch_idx: 40 |  Loss: (0.1589) |  Loss2: (0.0000) | Acc: (94.00%) (4947/5248)
Epoch: 124 | Batch_idx: 50 |  Loss: (0.1562) |  Loss2: (0.0000) | Acc: (94.00%) (6166/6528)
Epoch: 124 | Batch_idx: 60 |  Loss: (0.1550) |  Loss2: (0.0000) | Acc: (94.00%) (7382/7808)
Epoch: 124 | Batch_idx: 70 |  Loss: (0.1555) |  Loss2: (0.0000) | Acc: (94.00%) (8602/9088)
Epoch: 124 | Batch_idx: 80 |  Loss: (0.1535) |  Loss2: (0.0000) | Acc: (94.00%) (9824/10368)
Epoch: 124 | Batch_idx: 90 |  Loss: (0.1564) |  Loss2: (0.0000) | Acc: (94.00%) (11026/11648)
Epoch: 124 | Batch_idx: 100 |  Loss: (0.1575) |  Loss2: (0.0000) | Acc: (94.00%) (12230/12928)
Epoch: 124 | Batch_idx: 110 |  Loss: (0.1576) |  Loss2: (0.0000) | Acc: (94.00%) (13439/14208)
Epoch: 124 | Batch_idx: 120 |  Loss: (0.1599) |  Loss2: (0.0000) | Acc: (94.00%) (14631/15488)
Epoch: 124 | Batch_idx: 130 |  Loss: (0.1593) |  Loss2: (0.0000) | Acc: (94.00%) (15838/16768)
Epoch: 124 | Batch_idx: 140 |  Loss: (0.1602) |  Loss2: (0.0000) | Acc: (94.00%) (17046/18048)
Epoch: 124 | Batch_idx: 150 |  Loss: (0.1596) |  Loss2: (0.0000) | Acc: (94.00%) (18260/19328)
Epoch: 124 | Batch_idx: 160 |  Loss: (0.1599) |  Loss2: (0.0000) | Acc: (94.00%) (19473/20608)
Epoch: 124 | Batch_idx: 170 |  Loss: (0.1609) |  Loss2: (0.0000) | Acc: (94.00%) (20674/21888)
Epoch: 124 | Batch_idx: 180 |  Loss: (0.1614) |  Loss2: (0.0000) | Acc: (94.00%) (21875/23168)
Epoch: 124 | Batch_idx: 190 |  Loss: (0.1614) |  Loss2: (0.0000) | Acc: (94.00%) (23094/24448)
Epoch: 124 | Batch_idx: 200 |  Loss: (0.1618) |  Loss2: (0.0000) | Acc: (94.00%) (24309/25728)
Epoch: 124 | Batch_idx: 210 |  Loss: (0.1638) |  Loss2: (0.0000) | Acc: (94.00%) (25502/27008)
Epoch: 124 | Batch_idx: 220 |  Loss: (0.1632) |  Loss2: (0.0000) | Acc: (94.00%) (26721/28288)
Epoch: 124 | Batch_idx: 230 |  Loss: (0.1622) |  Loss2: (0.0000) | Acc: (94.00%) (27942/29568)
Epoch: 124 | Batch_idx: 240 |  Loss: (0.1627) |  Loss2: (0.0000) | Acc: (94.00%) (29154/30848)
Epoch: 124 | Batch_idx: 250 |  Loss: (0.1630) |  Loss2: (0.0000) | Acc: (94.00%) (30355/32128)
Epoch: 124 | Batch_idx: 260 |  Loss: (0.1630) |  Loss2: (0.0000) | Acc: (94.00%) (31568/33408)
Epoch: 124 | Batch_idx: 270 |  Loss: (0.1635) |  Loss2: (0.0000) | Acc: (94.00%) (32771/34688)
Epoch: 124 | Batch_idx: 280 |  Loss: (0.1629) |  Loss2: (0.0000) | Acc: (94.00%) (33989/35968)
Epoch: 124 | Batch_idx: 290 |  Loss: (0.1622) |  Loss2: (0.0000) | Acc: (94.00%) (35206/37248)
Epoch: 124 | Batch_idx: 300 |  Loss: (0.1627) |  Loss2: (0.0000) | Acc: (94.00%) (36409/38528)
Epoch: 124 | Batch_idx: 310 |  Loss: (0.1625) |  Loss2: (0.0000) | Acc: (94.00%) (37621/39808)
Epoch: 124 | Batch_idx: 320 |  Loss: (0.1620) |  Loss2: (0.0000) | Acc: (94.00%) (38835/41088)
Epoch: 124 | Batch_idx: 330 |  Loss: (0.1619) |  Loss2: (0.0000) | Acc: (94.00%) (40045/42368)
Epoch: 124 | Batch_idx: 340 |  Loss: (0.1613) |  Loss2: (0.0000) | Acc: (94.00%) (41263/43648)
Epoch: 124 | Batch_idx: 350 |  Loss: (0.1616) |  Loss2: (0.0000) | Acc: (94.00%) (42470/44928)
Epoch: 124 | Batch_idx: 360 |  Loss: (0.1627) |  Loss2: (0.0000) | Acc: (94.00%) (43662/46208)
Epoch: 124 | Batch_idx: 370 |  Loss: (0.1627) |  Loss2: (0.0000) | Acc: (94.00%) (44867/47488)
Epoch: 124 | Batch_idx: 380 |  Loss: (0.1624) |  Loss2: (0.0000) | Acc: (94.00%) (46086/48768)
Epoch: 124 | Batch_idx: 390 |  Loss: (0.1626) |  Loss2: (0.0000) | Acc: (94.00%) (47240/50000)
# TEST : Loss: (0.4315) | Acc: (87.00%) (8734/10000)
percent tensor([0.5764, 0.4236], device='cuda:0')
percent tensor([0.5813, 0.4187], device='cuda:0')
percent tensor([0.5470, 0.4530], device='cuda:0')
percent tensor([0.6349, 0.3651], device='cuda:0')
percent tensor([0.5929, 0.4071], device='cuda:0')
percent tensor([0.6532, 0.3468], device='cuda:0')
percent tensor([0.7173, 0.2827], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 125 | Batch_idx: 0 |  Loss: (0.1660) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 125 | Batch_idx: 10 |  Loss: (0.1712) |  Loss2: (0.0000) | Acc: (94.00%) (1327/1408)
Epoch: 125 | Batch_idx: 20 |  Loss: (0.1867) |  Loss2: (0.0000) | Acc: (93.00%) (2514/2688)
Epoch: 125 | Batch_idx: 30 |  Loss: (0.1858) |  Loss2: (0.0000) | Acc: (93.00%) (3709/3968)
Epoch: 125 | Batch_idx: 40 |  Loss: (0.1950) |  Loss2: (0.0000) | Acc: (93.00%) (4881/5248)
Epoch: 125 | Batch_idx: 50 |  Loss: (0.1970) |  Loss2: (0.0000) | Acc: (92.00%) (6068/6528)
Epoch: 125 | Batch_idx: 60 |  Loss: (0.1920) |  Loss2: (0.0000) | Acc: (93.00%) (7272/7808)
Epoch: 125 | Batch_idx: 70 |  Loss: (0.1915) |  Loss2: (0.0000) | Acc: (93.00%) (8455/9088)
Epoch: 125 | Batch_idx: 80 |  Loss: (0.1898) |  Loss2: (0.0000) | Acc: (93.00%) (9656/10368)
Epoch: 125 | Batch_idx: 90 |  Loss: (0.1914) |  Loss2: (0.0000) | Acc: (93.00%) (10844/11648)
Epoch: 125 | Batch_idx: 100 |  Loss: (0.1936) |  Loss2: (0.0000) | Acc: (93.00%) (12024/12928)
Epoch: 125 | Batch_idx: 110 |  Loss: (0.1948) |  Loss2: (0.0000) | Acc: (92.00%) (13213/14208)
Epoch: 125 | Batch_idx: 120 |  Loss: (0.1942) |  Loss2: (0.0000) | Acc: (93.00%) (14416/15488)
Epoch: 125 | Batch_idx: 130 |  Loss: (0.1949) |  Loss2: (0.0000) | Acc: (93.00%) (15598/16768)
Epoch: 125 | Batch_idx: 140 |  Loss: (0.1944) |  Loss2: (0.0000) | Acc: (93.00%) (16791/18048)
Epoch: 125 | Batch_idx: 150 |  Loss: (0.1930) |  Loss2: (0.0000) | Acc: (93.00%) (18000/19328)
Epoch: 125 | Batch_idx: 160 |  Loss: (0.1948) |  Loss2: (0.0000) | Acc: (93.00%) (19180/20608)
Epoch: 125 | Batch_idx: 170 |  Loss: (0.1944) |  Loss2: (0.0000) | Acc: (93.00%) (20380/21888)
Epoch: 125 | Batch_idx: 180 |  Loss: (0.1938) |  Loss2: (0.0000) | Acc: (93.00%) (21579/23168)
Epoch: 125 | Batch_idx: 190 |  Loss: (0.1942) |  Loss2: (0.0000) | Acc: (93.00%) (22765/24448)
Epoch: 125 | Batch_idx: 200 |  Loss: (0.1933) |  Loss2: (0.0000) | Acc: (93.00%) (23968/25728)
Epoch: 125 | Batch_idx: 210 |  Loss: (0.1936) |  Loss2: (0.0000) | Acc: (93.00%) (25163/27008)
Epoch: 125 | Batch_idx: 220 |  Loss: (0.1934) |  Loss2: (0.0000) | Acc: (93.00%) (26349/28288)
Epoch: 125 | Batch_idx: 230 |  Loss: (0.1919) |  Loss2: (0.0000) | Acc: (93.00%) (27560/29568)
Epoch: 125 | Batch_idx: 240 |  Loss: (0.1907) |  Loss2: (0.0000) | Acc: (93.00%) (28771/30848)
Epoch: 125 | Batch_idx: 250 |  Loss: (0.1903) |  Loss2: (0.0000) | Acc: (93.00%) (29968/32128)
Epoch: 125 | Batch_idx: 260 |  Loss: (0.1901) |  Loss2: (0.0000) | Acc: (93.00%) (31171/33408)
Epoch: 125 | Batch_idx: 270 |  Loss: (0.1908) |  Loss2: (0.0000) | Acc: (93.00%) (32360/34688)
Epoch: 125 | Batch_idx: 280 |  Loss: (0.1900) |  Loss2: (0.0000) | Acc: (93.00%) (33568/35968)
Epoch: 125 | Batch_idx: 290 |  Loss: (0.1902) |  Loss2: (0.0000) | Acc: (93.00%) (34763/37248)
Epoch: 125 | Batch_idx: 300 |  Loss: (0.1899) |  Loss2: (0.0000) | Acc: (93.00%) (35952/38528)
Epoch: 125 | Batch_idx: 310 |  Loss: (0.1901) |  Loss2: (0.0000) | Acc: (93.00%) (37142/39808)
Epoch: 125 | Batch_idx: 320 |  Loss: (0.1898) |  Loss2: (0.0000) | Acc: (93.00%) (38344/41088)
Epoch: 125 | Batch_idx: 330 |  Loss: (0.1891) |  Loss2: (0.0000) | Acc: (93.00%) (39559/42368)
Epoch: 125 | Batch_idx: 340 |  Loss: (0.1894) |  Loss2: (0.0000) | Acc: (93.00%) (40744/43648)
Epoch: 125 | Batch_idx: 350 |  Loss: (0.1885) |  Loss2: (0.0000) | Acc: (93.00%) (41957/44928)
Epoch: 125 | Batch_idx: 360 |  Loss: (0.1888) |  Loss2: (0.0000) | Acc: (93.00%) (43143/46208)
Epoch: 125 | Batch_idx: 370 |  Loss: (0.1889) |  Loss2: (0.0000) | Acc: (93.00%) (44340/47488)
Epoch: 125 | Batch_idx: 380 |  Loss: (0.1893) |  Loss2: (0.0000) | Acc: (93.00%) (45531/48768)
Epoch: 125 | Batch_idx: 390 |  Loss: (0.1890) |  Loss2: (0.0000) | Acc: (93.00%) (46689/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_125.pth.tar'
# TEST : Loss: (0.4350) | Acc: (87.00%) (8718/10000)
percent tensor([0.5794, 0.4206], device='cuda:0')
percent tensor([0.5748, 0.4252], device='cuda:0')
percent tensor([0.5470, 0.4530], device='cuda:0')
percent tensor([0.6270, 0.3730], device='cuda:0')
percent tensor([0.5958, 0.4042], device='cuda:0')
percent tensor([0.6575, 0.3425], device='cuda:0')
percent tensor([0.6887, 0.3113], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 126 | Batch_idx: 0 |  Loss: (0.1643) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 126 | Batch_idx: 10 |  Loss: (0.1682) |  Loss2: (0.0000) | Acc: (94.00%) (1330/1408)
Epoch: 126 | Batch_idx: 20 |  Loss: (0.1838) |  Loss2: (0.0000) | Acc: (93.00%) (2520/2688)
Epoch: 126 | Batch_idx: 30 |  Loss: (0.1868) |  Loss2: (0.0000) | Acc: (93.00%) (3716/3968)
Epoch: 126 | Batch_idx: 40 |  Loss: (0.1782) |  Loss2: (0.0000) | Acc: (93.00%) (4929/5248)
Epoch: 126 | Batch_idx: 50 |  Loss: (0.1831) |  Loss2: (0.0000) | Acc: (93.00%) (6118/6528)
Epoch: 126 | Batch_idx: 60 |  Loss: (0.1814) |  Loss2: (0.0000) | Acc: (93.00%) (7316/7808)
Epoch: 126 | Batch_idx: 70 |  Loss: (0.1805) |  Loss2: (0.0000) | Acc: (93.00%) (8517/9088)
Epoch: 126 | Batch_idx: 80 |  Loss: (0.1790) |  Loss2: (0.0000) | Acc: (93.00%) (9723/10368)
Epoch: 126 | Batch_idx: 90 |  Loss: (0.1801) |  Loss2: (0.0000) | Acc: (93.00%) (10910/11648)
Epoch: 126 | Batch_idx: 100 |  Loss: (0.1815) |  Loss2: (0.0000) | Acc: (93.00%) (12097/12928)
Epoch: 126 | Batch_idx: 110 |  Loss: (0.1818) |  Loss2: (0.0000) | Acc: (93.00%) (13295/14208)
Epoch: 126 | Batch_idx: 120 |  Loss: (0.1815) |  Loss2: (0.0000) | Acc: (93.00%) (14488/15488)
Epoch: 126 | Batch_idx: 130 |  Loss: (0.1804) |  Loss2: (0.0000) | Acc: (93.00%) (15689/16768)
Epoch: 126 | Batch_idx: 140 |  Loss: (0.1803) |  Loss2: (0.0000) | Acc: (93.00%) (16888/18048)
Epoch: 126 | Batch_idx: 150 |  Loss: (0.1799) |  Loss2: (0.0000) | Acc: (93.00%) (18085/19328)
Epoch: 126 | Batch_idx: 160 |  Loss: (0.1802) |  Loss2: (0.0000) | Acc: (93.00%) (19278/20608)
Epoch: 126 | Batch_idx: 170 |  Loss: (0.1793) |  Loss2: (0.0000) | Acc: (93.00%) (20481/21888)
Epoch: 126 | Batch_idx: 180 |  Loss: (0.1781) |  Loss2: (0.0000) | Acc: (93.00%) (21691/23168)
Epoch: 126 | Batch_idx: 190 |  Loss: (0.1776) |  Loss2: (0.0000) | Acc: (93.00%) (22895/24448)
Epoch: 126 | Batch_idx: 200 |  Loss: (0.1776) |  Loss2: (0.0000) | Acc: (93.00%) (24096/25728)
Epoch: 126 | Batch_idx: 210 |  Loss: (0.1768) |  Loss2: (0.0000) | Acc: (93.00%) (25308/27008)
Epoch: 126 | Batch_idx: 220 |  Loss: (0.1756) |  Loss2: (0.0000) | Acc: (93.00%) (26521/28288)
Epoch: 126 | Batch_idx: 230 |  Loss: (0.1749) |  Loss2: (0.0000) | Acc: (93.00%) (27722/29568)
Epoch: 126 | Batch_idx: 240 |  Loss: (0.1750) |  Loss2: (0.0000) | Acc: (93.00%) (28920/30848)
Epoch: 126 | Batch_idx: 250 |  Loss: (0.1757) |  Loss2: (0.0000) | Acc: (93.00%) (30115/32128)
Epoch: 126 | Batch_idx: 260 |  Loss: (0.1753) |  Loss2: (0.0000) | Acc: (93.00%) (31320/33408)
Epoch: 126 | Batch_idx: 270 |  Loss: (0.1748) |  Loss2: (0.0000) | Acc: (93.00%) (32531/34688)
Epoch: 126 | Batch_idx: 280 |  Loss: (0.1741) |  Loss2: (0.0000) | Acc: (93.00%) (33741/35968)
Epoch: 126 | Batch_idx: 290 |  Loss: (0.1742) |  Loss2: (0.0000) | Acc: (93.00%) (34939/37248)
Epoch: 126 | Batch_idx: 300 |  Loss: (0.1734) |  Loss2: (0.0000) | Acc: (93.00%) (36147/38528)
Epoch: 126 | Batch_idx: 310 |  Loss: (0.1729) |  Loss2: (0.0000) | Acc: (93.00%) (37350/39808)
Epoch: 126 | Batch_idx: 320 |  Loss: (0.1730) |  Loss2: (0.0000) | Acc: (93.00%) (38547/41088)
Epoch: 126 | Batch_idx: 330 |  Loss: (0.1725) |  Loss2: (0.0000) | Acc: (93.00%) (39749/42368)
Epoch: 126 | Batch_idx: 340 |  Loss: (0.1726) |  Loss2: (0.0000) | Acc: (93.00%) (40950/43648)
Epoch: 126 | Batch_idx: 350 |  Loss: (0.1721) |  Loss2: (0.0000) | Acc: (93.00%) (42163/44928)
Epoch: 126 | Batch_idx: 360 |  Loss: (0.1725) |  Loss2: (0.0000) | Acc: (93.00%) (43358/46208)
Epoch: 126 | Batch_idx: 370 |  Loss: (0.1717) |  Loss2: (0.0000) | Acc: (93.00%) (44575/47488)
Epoch: 126 | Batch_idx: 380 |  Loss: (0.1721) |  Loss2: (0.0000) | Acc: (93.00%) (45761/48768)
Epoch: 126 | Batch_idx: 390 |  Loss: (0.1722) |  Loss2: (0.0000) | Acc: (93.00%) (46915/50000)
# TEST : Loss: (0.4180) | Acc: (87.00%) (8749/10000)
percent tensor([0.5774, 0.4226], device='cuda:0')
percent tensor([0.5748, 0.4252], device='cuda:0')
percent tensor([0.5471, 0.4529], device='cuda:0')
percent tensor([0.6228, 0.3772], device='cuda:0')
percent tensor([0.5982, 0.4018], device='cuda:0')
percent tensor([0.6603, 0.3397], device='cuda:0')
percent tensor([0.6918, 0.3082], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 127 | Batch_idx: 0 |  Loss: (0.1612) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 127 | Batch_idx: 10 |  Loss: (0.1469) |  Loss2: (0.0000) | Acc: (95.00%) (1342/1408)
Epoch: 127 | Batch_idx: 20 |  Loss: (0.1489) |  Loss2: (0.0000) | Acc: (95.00%) (2554/2688)
Epoch: 127 | Batch_idx: 30 |  Loss: (0.1571) |  Loss2: (0.0000) | Acc: (94.00%) (3756/3968)
Epoch: 127 | Batch_idx: 40 |  Loss: (0.1563) |  Loss2: (0.0000) | Acc: (94.00%) (4967/5248)
Epoch: 127 | Batch_idx: 50 |  Loss: (0.1556) |  Loss2: (0.0000) | Acc: (94.00%) (6179/6528)
Epoch: 127 | Batch_idx: 60 |  Loss: (0.1595) |  Loss2: (0.0000) | Acc: (94.00%) (7374/7808)
Epoch: 127 | Batch_idx: 70 |  Loss: (0.1582) |  Loss2: (0.0000) | Acc: (94.00%) (8587/9088)
Epoch: 127 | Batch_idx: 80 |  Loss: (0.1607) |  Loss2: (0.0000) | Acc: (94.00%) (9787/10368)
Epoch: 127 | Batch_idx: 90 |  Loss: (0.1626) |  Loss2: (0.0000) | Acc: (94.00%) (10983/11648)
Epoch: 127 | Batch_idx: 100 |  Loss: (0.1641) |  Loss2: (0.0000) | Acc: (94.00%) (12176/12928)
Epoch: 127 | Batch_idx: 110 |  Loss: (0.1632) |  Loss2: (0.0000) | Acc: (94.00%) (13391/14208)
Epoch: 127 | Batch_idx: 120 |  Loss: (0.1627) |  Loss2: (0.0000) | Acc: (94.00%) (14599/15488)
Epoch: 127 | Batch_idx: 130 |  Loss: (0.1646) |  Loss2: (0.0000) | Acc: (94.00%) (15789/16768)
Epoch: 127 | Batch_idx: 140 |  Loss: (0.1662) |  Loss2: (0.0000) | Acc: (94.00%) (16992/18048)
Epoch: 127 | Batch_idx: 150 |  Loss: (0.1653) |  Loss2: (0.0000) | Acc: (94.00%) (18201/19328)
Epoch: 127 | Batch_idx: 160 |  Loss: (0.1645) |  Loss2: (0.0000) | Acc: (94.00%) (19409/20608)
Epoch: 127 | Batch_idx: 170 |  Loss: (0.1648) |  Loss2: (0.0000) | Acc: (94.00%) (20614/21888)
Epoch: 127 | Batch_idx: 180 |  Loss: (0.1643) |  Loss2: (0.0000) | Acc: (94.00%) (21831/23168)
Epoch: 127 | Batch_idx: 190 |  Loss: (0.1641) |  Loss2: (0.0000) | Acc: (94.00%) (23049/24448)
Epoch: 127 | Batch_idx: 200 |  Loss: (0.1639) |  Loss2: (0.0000) | Acc: (94.00%) (24256/25728)
Epoch: 127 | Batch_idx: 210 |  Loss: (0.1636) |  Loss2: (0.0000) | Acc: (94.00%) (25472/27008)
Epoch: 127 | Batch_idx: 220 |  Loss: (0.1630) |  Loss2: (0.0000) | Acc: (94.00%) (26682/28288)
Epoch: 127 | Batch_idx: 230 |  Loss: (0.1635) |  Loss2: (0.0000) | Acc: (94.00%) (27887/29568)
Epoch: 127 | Batch_idx: 240 |  Loss: (0.1632) |  Loss2: (0.0000) | Acc: (94.00%) (29104/30848)
Epoch: 127 | Batch_idx: 250 |  Loss: (0.1626) |  Loss2: (0.0000) | Acc: (94.00%) (30331/32128)
Epoch: 127 | Batch_idx: 260 |  Loss: (0.1625) |  Loss2: (0.0000) | Acc: (94.00%) (31540/33408)
Epoch: 127 | Batch_idx: 270 |  Loss: (0.1627) |  Loss2: (0.0000) | Acc: (94.00%) (32743/34688)
Epoch: 127 | Batch_idx: 280 |  Loss: (0.1630) |  Loss2: (0.0000) | Acc: (94.00%) (33952/35968)
Epoch: 127 | Batch_idx: 290 |  Loss: (0.1636) |  Loss2: (0.0000) | Acc: (94.00%) (35153/37248)
Epoch: 127 | Batch_idx: 300 |  Loss: (0.1634) |  Loss2: (0.0000) | Acc: (94.00%) (36360/38528)
Epoch: 127 | Batch_idx: 310 |  Loss: (0.1641) |  Loss2: (0.0000) | Acc: (94.00%) (37556/39808)
Epoch: 127 | Batch_idx: 320 |  Loss: (0.1646) |  Loss2: (0.0000) | Acc: (94.00%) (38754/41088)
Epoch: 127 | Batch_idx: 330 |  Loss: (0.1640) |  Loss2: (0.0000) | Acc: (94.00%) (39974/42368)
Epoch: 127 | Batch_idx: 340 |  Loss: (0.1636) |  Loss2: (0.0000) | Acc: (94.00%) (41185/43648)
Epoch: 127 | Batch_idx: 350 |  Loss: (0.1638) |  Loss2: (0.0000) | Acc: (94.00%) (42388/44928)
Epoch: 127 | Batch_idx: 360 |  Loss: (0.1641) |  Loss2: (0.0000) | Acc: (94.00%) (43598/46208)
Epoch: 127 | Batch_idx: 370 |  Loss: (0.1637) |  Loss2: (0.0000) | Acc: (94.00%) (44809/47488)
Epoch: 127 | Batch_idx: 380 |  Loss: (0.1632) |  Loss2: (0.0000) | Acc: (94.00%) (46017/48768)
Epoch: 127 | Batch_idx: 390 |  Loss: (0.1632) |  Loss2: (0.0000) | Acc: (94.00%) (47175/50000)
# TEST : Loss: (0.4042) | Acc: (87.00%) (8765/10000)
percent tensor([0.5797, 0.4203], device='cuda:0')
percent tensor([0.5748, 0.4252], device='cuda:0')
percent tensor([0.5477, 0.4523], device='cuda:0')
percent tensor([0.6273, 0.3727], device='cuda:0')
percent tensor([0.6025, 0.3975], device='cuda:0')
percent tensor([0.6622, 0.3378], device='cuda:0')
percent tensor([0.6956, 0.3044], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 128 | Batch_idx: 0 |  Loss: (0.1785) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 128 | Batch_idx: 10 |  Loss: (0.1423) |  Loss2: (0.0000) | Acc: (94.00%) (1334/1408)
Epoch: 128 | Batch_idx: 20 |  Loss: (0.1640) |  Loss2: (0.0000) | Acc: (93.00%) (2523/2688)
Epoch: 128 | Batch_idx: 30 |  Loss: (0.1598) |  Loss2: (0.0000) | Acc: (94.00%) (3731/3968)
Epoch: 128 | Batch_idx: 40 |  Loss: (0.1577) |  Loss2: (0.0000) | Acc: (94.00%) (4948/5248)
Epoch: 128 | Batch_idx: 50 |  Loss: (0.1551) |  Loss2: (0.0000) | Acc: (94.00%) (6172/6528)
Epoch: 128 | Batch_idx: 60 |  Loss: (0.1571) |  Loss2: (0.0000) | Acc: (94.00%) (7389/7808)
Epoch: 128 | Batch_idx: 70 |  Loss: (0.1551) |  Loss2: (0.0000) | Acc: (94.00%) (8599/9088)
Epoch: 128 | Batch_idx: 80 |  Loss: (0.1552) |  Loss2: (0.0000) | Acc: (94.00%) (9808/10368)
Epoch: 128 | Batch_idx: 90 |  Loss: (0.1561) |  Loss2: (0.0000) | Acc: (94.00%) (11007/11648)
Epoch: 128 | Batch_idx: 100 |  Loss: (0.1586) |  Loss2: (0.0000) | Acc: (94.00%) (12203/12928)
Epoch: 128 | Batch_idx: 110 |  Loss: (0.1596) |  Loss2: (0.0000) | Acc: (94.00%) (13415/14208)
Epoch: 128 | Batch_idx: 120 |  Loss: (0.1610) |  Loss2: (0.0000) | Acc: (94.00%) (14623/15488)
Epoch: 128 | Batch_idx: 130 |  Loss: (0.1607) |  Loss2: (0.0000) | Acc: (94.00%) (15832/16768)
Epoch: 128 | Batch_idx: 140 |  Loss: (0.1598) |  Loss2: (0.0000) | Acc: (94.00%) (17045/18048)
Epoch: 128 | Batch_idx: 150 |  Loss: (0.1607) |  Loss2: (0.0000) | Acc: (94.00%) (18250/19328)
Epoch: 128 | Batch_idx: 160 |  Loss: (0.1609) |  Loss2: (0.0000) | Acc: (94.00%) (19466/20608)
Epoch: 128 | Batch_idx: 170 |  Loss: (0.1607) |  Loss2: (0.0000) | Acc: (94.00%) (20676/21888)
Epoch: 128 | Batch_idx: 180 |  Loss: (0.1613) |  Loss2: (0.0000) | Acc: (94.00%) (21874/23168)
Epoch: 128 | Batch_idx: 190 |  Loss: (0.1631) |  Loss2: (0.0000) | Acc: (94.00%) (23066/24448)
Epoch: 128 | Batch_idx: 200 |  Loss: (0.1628) |  Loss2: (0.0000) | Acc: (94.00%) (24275/25728)
Epoch: 128 | Batch_idx: 210 |  Loss: (0.1616) |  Loss2: (0.0000) | Acc: (94.00%) (25501/27008)
Epoch: 128 | Batch_idx: 220 |  Loss: (0.1610) |  Loss2: (0.0000) | Acc: (94.00%) (26706/28288)
Epoch: 128 | Batch_idx: 230 |  Loss: (0.1616) |  Loss2: (0.0000) | Acc: (94.00%) (27905/29568)
Epoch: 128 | Batch_idx: 240 |  Loss: (0.1602) |  Loss2: (0.0000) | Acc: (94.00%) (29123/30848)
Epoch: 128 | Batch_idx: 250 |  Loss: (0.1614) |  Loss2: (0.0000) | Acc: (94.00%) (30321/32128)
Epoch: 128 | Batch_idx: 260 |  Loss: (0.1611) |  Loss2: (0.0000) | Acc: (94.00%) (31540/33408)
Epoch: 128 | Batch_idx: 270 |  Loss: (0.1609) |  Loss2: (0.0000) | Acc: (94.00%) (32753/34688)
Epoch: 128 | Batch_idx: 280 |  Loss: (0.1602) |  Loss2: (0.0000) | Acc: (94.00%) (33972/35968)
Epoch: 128 | Batch_idx: 290 |  Loss: (0.1594) |  Loss2: (0.0000) | Acc: (94.00%) (35192/37248)
Epoch: 128 | Batch_idx: 300 |  Loss: (0.1595) |  Loss2: (0.0000) | Acc: (94.00%) (36401/38528)
Epoch: 128 | Batch_idx: 310 |  Loss: (0.1593) |  Loss2: (0.0000) | Acc: (94.00%) (37612/39808)
Epoch: 128 | Batch_idx: 320 |  Loss: (0.1588) |  Loss2: (0.0000) | Acc: (94.00%) (38827/41088)
Epoch: 128 | Batch_idx: 330 |  Loss: (0.1583) |  Loss2: (0.0000) | Acc: (94.00%) (40047/42368)
Epoch: 128 | Batch_idx: 340 |  Loss: (0.1579) |  Loss2: (0.0000) | Acc: (94.00%) (41261/43648)
Epoch: 128 | Batch_idx: 350 |  Loss: (0.1579) |  Loss2: (0.0000) | Acc: (94.00%) (42472/44928)
Epoch: 128 | Batch_idx: 360 |  Loss: (0.1576) |  Loss2: (0.0000) | Acc: (94.00%) (43697/46208)
Epoch: 128 | Batch_idx: 370 |  Loss: (0.1578) |  Loss2: (0.0000) | Acc: (94.00%) (44901/47488)
Epoch: 128 | Batch_idx: 380 |  Loss: (0.1577) |  Loss2: (0.0000) | Acc: (94.00%) (46104/48768)
Epoch: 128 | Batch_idx: 390 |  Loss: (0.1579) |  Loss2: (0.0000) | Acc: (94.00%) (47259/50000)
# TEST : Loss: (0.3930) | Acc: (87.00%) (8774/10000)
percent tensor([0.5810, 0.4190], device='cuda:0')
percent tensor([0.5774, 0.4226], device='cuda:0')
percent tensor([0.5500, 0.4500], device='cuda:0')
percent tensor([0.6263, 0.3737], device='cuda:0')
percent tensor([0.5996, 0.4004], device='cuda:0')
percent tensor([0.6652, 0.3348], device='cuda:0')
percent tensor([0.7020, 0.2980], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 129 | Batch_idx: 0 |  Loss: (0.1263) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 129 | Batch_idx: 10 |  Loss: (0.1815) |  Loss2: (0.0000) | Acc: (93.00%) (1318/1408)
Epoch: 129 | Batch_idx: 20 |  Loss: (0.1697) |  Loss2: (0.0000) | Acc: (93.00%) (2523/2688)
Epoch: 129 | Batch_idx: 30 |  Loss: (0.1543) |  Loss2: (0.0000) | Acc: (94.00%) (3754/3968)
Epoch: 129 | Batch_idx: 40 |  Loss: (0.1595) |  Loss2: (0.0000) | Acc: (94.00%) (4960/5248)
Epoch: 129 | Batch_idx: 50 |  Loss: (0.1579) |  Loss2: (0.0000) | Acc: (94.00%) (6166/6528)
Epoch: 129 | Batch_idx: 60 |  Loss: (0.1576) |  Loss2: (0.0000) | Acc: (94.00%) (7372/7808)
Epoch: 129 | Batch_idx: 70 |  Loss: (0.1577) |  Loss2: (0.0000) | Acc: (94.00%) (8581/9088)
Epoch: 129 | Batch_idx: 80 |  Loss: (0.1582) |  Loss2: (0.0000) | Acc: (94.00%) (9790/10368)
Epoch: 129 | Batch_idx: 90 |  Loss: (0.1575) |  Loss2: (0.0000) | Acc: (94.00%) (11002/11648)
Epoch: 129 | Batch_idx: 100 |  Loss: (0.1557) |  Loss2: (0.0000) | Acc: (94.00%) (12217/12928)
Epoch: 129 | Batch_idx: 110 |  Loss: (0.1539) |  Loss2: (0.0000) | Acc: (94.00%) (13440/14208)
Epoch: 129 | Batch_idx: 120 |  Loss: (0.1532) |  Loss2: (0.0000) | Acc: (94.00%) (14657/15488)
Epoch: 129 | Batch_idx: 130 |  Loss: (0.1534) |  Loss2: (0.0000) | Acc: (94.00%) (15865/16768)
Epoch: 129 | Batch_idx: 140 |  Loss: (0.1526) |  Loss2: (0.0000) | Acc: (94.00%) (17087/18048)
Epoch: 129 | Batch_idx: 150 |  Loss: (0.1523) |  Loss2: (0.0000) | Acc: (94.00%) (18301/19328)
Epoch: 129 | Batch_idx: 160 |  Loss: (0.1523) |  Loss2: (0.0000) | Acc: (94.00%) (19514/20608)
Epoch: 129 | Batch_idx: 170 |  Loss: (0.1536) |  Loss2: (0.0000) | Acc: (94.00%) (20721/21888)
Epoch: 129 | Batch_idx: 180 |  Loss: (0.1527) |  Loss2: (0.0000) | Acc: (94.00%) (21937/23168)
Epoch: 129 | Batch_idx: 190 |  Loss: (0.1531) |  Loss2: (0.0000) | Acc: (94.00%) (23149/24448)
Epoch: 129 | Batch_idx: 200 |  Loss: (0.1527) |  Loss2: (0.0000) | Acc: (94.00%) (24368/25728)
Epoch: 129 | Batch_idx: 210 |  Loss: (0.1531) |  Loss2: (0.0000) | Acc: (94.00%) (25575/27008)
Epoch: 129 | Batch_idx: 220 |  Loss: (0.1534) |  Loss2: (0.0000) | Acc: (94.00%) (26788/28288)
Epoch: 129 | Batch_idx: 230 |  Loss: (0.1529) |  Loss2: (0.0000) | Acc: (94.00%) (28008/29568)
Epoch: 129 | Batch_idx: 240 |  Loss: (0.1535) |  Loss2: (0.0000) | Acc: (94.00%) (29211/30848)
Epoch: 129 | Batch_idx: 250 |  Loss: (0.1533) |  Loss2: (0.0000) | Acc: (94.00%) (30429/32128)
Epoch: 129 | Batch_idx: 260 |  Loss: (0.1534) |  Loss2: (0.0000) | Acc: (94.00%) (31643/33408)
Epoch: 129 | Batch_idx: 270 |  Loss: (0.1544) |  Loss2: (0.0000) | Acc: (94.00%) (32839/34688)
Epoch: 129 | Batch_idx: 280 |  Loss: (0.1537) |  Loss2: (0.0000) | Acc: (94.00%) (34072/35968)
Epoch: 129 | Batch_idx: 290 |  Loss: (0.1537) |  Loss2: (0.0000) | Acc: (94.00%) (35286/37248)
Epoch: 129 | Batch_idx: 300 |  Loss: (0.1531) |  Loss2: (0.0000) | Acc: (94.00%) (36516/38528)
Epoch: 129 | Batch_idx: 310 |  Loss: (0.1527) |  Loss2: (0.0000) | Acc: (94.00%) (37728/39808)
Epoch: 129 | Batch_idx: 320 |  Loss: (0.1522) |  Loss2: (0.0000) | Acc: (94.00%) (38951/41088)
Epoch: 129 | Batch_idx: 330 |  Loss: (0.1516) |  Loss2: (0.0000) | Acc: (94.00%) (40168/42368)
Epoch: 129 | Batch_idx: 340 |  Loss: (0.1521) |  Loss2: (0.0000) | Acc: (94.00%) (41371/43648)
Epoch: 129 | Batch_idx: 350 |  Loss: (0.1521) |  Loss2: (0.0000) | Acc: (94.00%) (42584/44928)
Epoch: 129 | Batch_idx: 360 |  Loss: (0.1521) |  Loss2: (0.0000) | Acc: (94.00%) (43800/46208)
Epoch: 129 | Batch_idx: 370 |  Loss: (0.1519) |  Loss2: (0.0000) | Acc: (94.00%) (45017/47488)
Epoch: 129 | Batch_idx: 380 |  Loss: (0.1520) |  Loss2: (0.0000) | Acc: (94.00%) (46224/48768)
Epoch: 129 | Batch_idx: 390 |  Loss: (0.1524) |  Loss2: (0.0000) | Acc: (94.00%) (47378/50000)
# TEST : Loss: (0.3931) | Acc: (87.00%) (8795/10000)
percent tensor([0.5817, 0.4183], device='cuda:0')
percent tensor([0.5758, 0.4242], device='cuda:0')
percent tensor([0.5502, 0.4498], device='cuda:0')
percent tensor([0.6249, 0.3751], device='cuda:0')
percent tensor([0.5999, 0.4001], device='cuda:0')
percent tensor([0.6648, 0.3352], device='cuda:0')
percent tensor([0.7042, 0.2958], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 130 | Batch_idx: 0 |  Loss: (0.0892) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 130 | Batch_idx: 10 |  Loss: (0.1514) |  Loss2: (0.0000) | Acc: (95.00%) (1339/1408)
Epoch: 130 | Batch_idx: 20 |  Loss: (0.1540) |  Loss2: (0.0000) | Acc: (94.00%) (2552/2688)
Epoch: 130 | Batch_idx: 30 |  Loss: (0.1534) |  Loss2: (0.0000) | Acc: (94.00%) (3765/3968)
Epoch: 130 | Batch_idx: 40 |  Loss: (0.1607) |  Loss2: (0.0000) | Acc: (94.00%) (4958/5248)
Epoch: 130 | Batch_idx: 50 |  Loss: (0.1577) |  Loss2: (0.0000) | Acc: (94.00%) (6174/6528)
Epoch: 130 | Batch_idx: 60 |  Loss: (0.1588) |  Loss2: (0.0000) | Acc: (94.00%) (7385/7808)
Epoch: 130 | Batch_idx: 70 |  Loss: (0.1571) |  Loss2: (0.0000) | Acc: (94.00%) (8601/9088)
Epoch: 130 | Batch_idx: 80 |  Loss: (0.1551) |  Loss2: (0.0000) | Acc: (94.00%) (9823/10368)
Epoch: 130 | Batch_idx: 90 |  Loss: (0.1590) |  Loss2: (0.0000) | Acc: (94.00%) (11018/11648)
Epoch: 130 | Batch_idx: 100 |  Loss: (0.1577) |  Loss2: (0.0000) | Acc: (94.00%) (12231/12928)
Epoch: 130 | Batch_idx: 110 |  Loss: (0.1575) |  Loss2: (0.0000) | Acc: (94.00%) (13442/14208)
Epoch: 130 | Batch_idx: 120 |  Loss: (0.1577) |  Loss2: (0.0000) | Acc: (94.00%) (14647/15488)
Epoch: 130 | Batch_idx: 130 |  Loss: (0.1570) |  Loss2: (0.0000) | Acc: (94.00%) (15861/16768)
Epoch: 130 | Batch_idx: 140 |  Loss: (0.1576) |  Loss2: (0.0000) | Acc: (94.00%) (17065/18048)
Epoch: 130 | Batch_idx: 150 |  Loss: (0.1559) |  Loss2: (0.0000) | Acc: (94.00%) (18289/19328)
Epoch: 130 | Batch_idx: 160 |  Loss: (0.1567) |  Loss2: (0.0000) | Acc: (94.00%) (19504/20608)
Epoch: 130 | Batch_idx: 170 |  Loss: (0.1569) |  Loss2: (0.0000) | Acc: (94.00%) (20713/21888)
Epoch: 130 | Batch_idx: 180 |  Loss: (0.1565) |  Loss2: (0.0000) | Acc: (94.00%) (21926/23168)
Epoch: 130 | Batch_idx: 190 |  Loss: (0.1567) |  Loss2: (0.0000) | Acc: (94.00%) (23132/24448)
Epoch: 130 | Batch_idx: 200 |  Loss: (0.1569) |  Loss2: (0.0000) | Acc: (94.00%) (24342/25728)
Epoch: 130 | Batch_idx: 210 |  Loss: (0.1573) |  Loss2: (0.0000) | Acc: (94.00%) (25549/27008)
Epoch: 130 | Batch_idx: 220 |  Loss: (0.1573) |  Loss2: (0.0000) | Acc: (94.00%) (26757/28288)
Epoch: 130 | Batch_idx: 230 |  Loss: (0.1577) |  Loss2: (0.0000) | Acc: (94.00%) (27966/29568)
Epoch: 130 | Batch_idx: 240 |  Loss: (0.1577) |  Loss2: (0.0000) | Acc: (94.00%) (29180/30848)
Epoch: 130 | Batch_idx: 250 |  Loss: (0.1582) |  Loss2: (0.0000) | Acc: (94.00%) (30395/32128)
Epoch: 130 | Batch_idx: 260 |  Loss: (0.1588) |  Loss2: (0.0000) | Acc: (94.00%) (31598/33408)
Epoch: 130 | Batch_idx: 270 |  Loss: (0.1599) |  Loss2: (0.0000) | Acc: (94.00%) (32796/34688)
Epoch: 130 | Batch_idx: 280 |  Loss: (0.1609) |  Loss2: (0.0000) | Acc: (94.00%) (33991/35968)
Epoch: 130 | Batch_idx: 290 |  Loss: (0.1618) |  Loss2: (0.0000) | Acc: (94.00%) (35190/37248)
Epoch: 130 | Batch_idx: 300 |  Loss: (0.1624) |  Loss2: (0.0000) | Acc: (94.00%) (36399/38528)
Epoch: 130 | Batch_idx: 310 |  Loss: (0.1623) |  Loss2: (0.0000) | Acc: (94.00%) (37611/39808)
Epoch: 130 | Batch_idx: 320 |  Loss: (0.1631) |  Loss2: (0.0000) | Acc: (94.00%) (38800/41088)
Epoch: 130 | Batch_idx: 330 |  Loss: (0.1629) |  Loss2: (0.0000) | Acc: (94.00%) (40002/42368)
Epoch: 130 | Batch_idx: 340 |  Loss: (0.1623) |  Loss2: (0.0000) | Acc: (94.00%) (41227/43648)
Epoch: 130 | Batch_idx: 350 |  Loss: (0.1629) |  Loss2: (0.0000) | Acc: (94.00%) (42422/44928)
Epoch: 130 | Batch_idx: 360 |  Loss: (0.1628) |  Loss2: (0.0000) | Acc: (94.00%) (43628/46208)
Epoch: 130 | Batch_idx: 370 |  Loss: (0.1628) |  Loss2: (0.0000) | Acc: (94.00%) (44837/47488)
Epoch: 130 | Batch_idx: 380 |  Loss: (0.1631) |  Loss2: (0.0000) | Acc: (94.00%) (46038/48768)
Epoch: 130 | Batch_idx: 390 |  Loss: (0.1629) |  Loss2: (0.0000) | Acc: (94.00%) (47214/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_130.pth.tar'
# TEST : Loss: (0.4489) | Acc: (86.00%) (8679/10000)
percent tensor([0.5810, 0.4190], device='cuda:0')
percent tensor([0.5758, 0.4242], device='cuda:0')
percent tensor([0.5504, 0.4496], device='cuda:0')
percent tensor([0.6246, 0.3754], device='cuda:0')
percent tensor([0.5985, 0.4015], device='cuda:0')
percent tensor([0.6631, 0.3369], device='cuda:0')
percent tensor([0.7031, 0.2969], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(181.0199, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(820.4230, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(819.4392, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1521.8595, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(488.6646, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2266.7656, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4272.3286, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1379.1045, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6193.2158, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11767.3730, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3886.2905, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16439.4121, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 131 | Batch_idx: 0 |  Loss: (0.2033) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 131 | Batch_idx: 10 |  Loss: (0.1742) |  Loss2: (0.0000) | Acc: (93.00%) (1320/1408)
Epoch: 131 | Batch_idx: 20 |  Loss: (0.1682) |  Loss2: (0.0000) | Acc: (93.00%) (2524/2688)
Epoch: 131 | Batch_idx: 30 |  Loss: (0.1663) |  Loss2: (0.0000) | Acc: (93.00%) (3729/3968)
Epoch: 131 | Batch_idx: 40 |  Loss: (0.1603) |  Loss2: (0.0000) | Acc: (94.00%) (4941/5248)
Epoch: 131 | Batch_idx: 50 |  Loss: (0.1597) |  Loss2: (0.0000) | Acc: (94.00%) (6151/6528)
Epoch: 131 | Batch_idx: 60 |  Loss: (0.1575) |  Loss2: (0.0000) | Acc: (94.00%) (7363/7808)
Epoch: 131 | Batch_idx: 70 |  Loss: (0.1588) |  Loss2: (0.0000) | Acc: (94.00%) (8566/9088)
Epoch: 131 | Batch_idx: 80 |  Loss: (0.1580) |  Loss2: (0.0000) | Acc: (94.00%) (9781/10368)
Epoch: 131 | Batch_idx: 90 |  Loss: (0.1590) |  Loss2: (0.0000) | Acc: (94.00%) (10987/11648)
Epoch: 131 | Batch_idx: 100 |  Loss: (0.1570) |  Loss2: (0.0000) | Acc: (94.00%) (12205/12928)
Epoch: 131 | Batch_idx: 110 |  Loss: (0.1548) |  Loss2: (0.0000) | Acc: (94.00%) (13427/14208)
Epoch: 131 | Batch_idx: 120 |  Loss: (0.1567) |  Loss2: (0.0000) | Acc: (94.00%) (14629/15488)
Epoch: 131 | Batch_idx: 130 |  Loss: (0.1581) |  Loss2: (0.0000) | Acc: (94.00%) (15832/16768)
Epoch: 131 | Batch_idx: 140 |  Loss: (0.1599) |  Loss2: (0.0000) | Acc: (94.00%) (17031/18048)
Epoch: 131 | Batch_idx: 150 |  Loss: (0.1584) |  Loss2: (0.0000) | Acc: (94.00%) (18251/19328)
Epoch: 131 | Batch_idx: 160 |  Loss: (0.1581) |  Loss2: (0.0000) | Acc: (94.00%) (19462/20608)
Epoch: 131 | Batch_idx: 170 |  Loss: (0.1571) |  Loss2: (0.0000) | Acc: (94.00%) (20674/21888)
Epoch: 131 | Batch_idx: 180 |  Loss: (0.1578) |  Loss2: (0.0000) | Acc: (94.00%) (21883/23168)
Epoch: 131 | Batch_idx: 190 |  Loss: (0.1577) |  Loss2: (0.0000) | Acc: (94.00%) (23093/24448)
Epoch: 131 | Batch_idx: 200 |  Loss: (0.1569) |  Loss2: (0.0000) | Acc: (94.00%) (24304/25728)
Epoch: 131 | Batch_idx: 210 |  Loss: (0.1564) |  Loss2: (0.0000) | Acc: (94.00%) (25524/27008)
Epoch: 131 | Batch_idx: 220 |  Loss: (0.1562) |  Loss2: (0.0000) | Acc: (94.00%) (26740/28288)
Epoch: 131 | Batch_idx: 230 |  Loss: (0.1556) |  Loss2: (0.0000) | Acc: (94.00%) (27952/29568)
Epoch: 131 | Batch_idx: 240 |  Loss: (0.1556) |  Loss2: (0.0000) | Acc: (94.00%) (29166/30848)
Epoch: 131 | Batch_idx: 250 |  Loss: (0.1560) |  Loss2: (0.0000) | Acc: (94.00%) (30386/32128)
Epoch: 131 | Batch_idx: 260 |  Loss: (0.1564) |  Loss2: (0.0000) | Acc: (94.00%) (31591/33408)
Epoch: 131 | Batch_idx: 270 |  Loss: (0.1562) |  Loss2: (0.0000) | Acc: (94.00%) (32801/34688)
Epoch: 131 | Batch_idx: 280 |  Loss: (0.1570) |  Loss2: (0.0000) | Acc: (94.00%) (34002/35968)
Epoch: 131 | Batch_idx: 290 |  Loss: (0.1569) |  Loss2: (0.0000) | Acc: (94.00%) (35203/37248)
Epoch: 131 | Batch_idx: 300 |  Loss: (0.1570) |  Loss2: (0.0000) | Acc: (94.00%) (36421/38528)
Epoch: 131 | Batch_idx: 310 |  Loss: (0.1573) |  Loss2: (0.0000) | Acc: (94.00%) (37623/39808)
Epoch: 131 | Batch_idx: 320 |  Loss: (0.1570) |  Loss2: (0.0000) | Acc: (94.00%) (38835/41088)
Epoch: 131 | Batch_idx: 330 |  Loss: (0.1569) |  Loss2: (0.0000) | Acc: (94.00%) (40046/42368)
Epoch: 131 | Batch_idx: 340 |  Loss: (0.1571) |  Loss2: (0.0000) | Acc: (94.00%) (41261/43648)
Epoch: 131 | Batch_idx: 350 |  Loss: (0.1570) |  Loss2: (0.0000) | Acc: (94.00%) (42471/44928)
Epoch: 131 | Batch_idx: 360 |  Loss: (0.1571) |  Loss2: (0.0000) | Acc: (94.00%) (43677/46208)
Epoch: 131 | Batch_idx: 370 |  Loss: (0.1569) |  Loss2: (0.0000) | Acc: (94.00%) (44893/47488)
Epoch: 131 | Batch_idx: 380 |  Loss: (0.1582) |  Loss2: (0.0000) | Acc: (94.00%) (46087/48768)
Epoch: 131 | Batch_idx: 390 |  Loss: (0.1581) |  Loss2: (0.0000) | Acc: (94.00%) (47250/50000)
# TEST : Loss: (0.3997) | Acc: (87.00%) (8776/10000)
percent tensor([0.5814, 0.4186], device='cuda:0')
percent tensor([0.5754, 0.4246], device='cuda:0')
percent tensor([0.5498, 0.4502], device='cuda:0')
percent tensor([0.6246, 0.3754], device='cuda:0')
percent tensor([0.5994, 0.4006], device='cuda:0')
percent tensor([0.6653, 0.3347], device='cuda:0')
percent tensor([0.7027, 0.2973], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 132 | Batch_idx: 0 |  Loss: (0.1574) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 132 | Batch_idx: 10 |  Loss: (0.1741) |  Loss2: (0.0000) | Acc: (93.00%) (1322/1408)
Epoch: 132 | Batch_idx: 20 |  Loss: (0.1603) |  Loss2: (0.0000) | Acc: (94.00%) (2536/2688)
Epoch: 132 | Batch_idx: 30 |  Loss: (0.1643) |  Loss2: (0.0000) | Acc: (94.00%) (3746/3968)
Epoch: 132 | Batch_idx: 40 |  Loss: (0.1655) |  Loss2: (0.0000) | Acc: (94.00%) (4945/5248)
Epoch: 132 | Batch_idx: 50 |  Loss: (0.1598) |  Loss2: (0.0000) | Acc: (94.00%) (6161/6528)
Epoch: 132 | Batch_idx: 60 |  Loss: (0.1570) |  Loss2: (0.0000) | Acc: (94.00%) (7386/7808)
Epoch: 132 | Batch_idx: 70 |  Loss: (0.1590) |  Loss2: (0.0000) | Acc: (94.00%) (8588/9088)
Epoch: 132 | Batch_idx: 80 |  Loss: (0.1576) |  Loss2: (0.0000) | Acc: (94.00%) (9802/10368)
Epoch: 132 | Batch_idx: 90 |  Loss: (0.1589) |  Loss2: (0.0000) | Acc: (94.00%) (11005/11648)
Epoch: 132 | Batch_idx: 100 |  Loss: (0.1586) |  Loss2: (0.0000) | Acc: (94.00%) (12217/12928)
Epoch: 132 | Batch_idx: 110 |  Loss: (0.1597) |  Loss2: (0.0000) | Acc: (94.00%) (13432/14208)
Epoch: 132 | Batch_idx: 120 |  Loss: (0.1577) |  Loss2: (0.0000) | Acc: (94.00%) (14650/15488)
Epoch: 132 | Batch_idx: 130 |  Loss: (0.1574) |  Loss2: (0.0000) | Acc: (94.00%) (15859/16768)
Epoch: 132 | Batch_idx: 140 |  Loss: (0.1558) |  Loss2: (0.0000) | Acc: (94.00%) (17080/18048)
Epoch: 132 | Batch_idx: 150 |  Loss: (0.1568) |  Loss2: (0.0000) | Acc: (94.00%) (18275/19328)
Epoch: 132 | Batch_idx: 160 |  Loss: (0.1572) |  Loss2: (0.0000) | Acc: (94.00%) (19480/20608)
Epoch: 132 | Batch_idx: 170 |  Loss: (0.1577) |  Loss2: (0.0000) | Acc: (94.00%) (20676/21888)
Epoch: 132 | Batch_idx: 180 |  Loss: (0.1563) |  Loss2: (0.0000) | Acc: (94.00%) (21903/23168)
Epoch: 132 | Batch_idx: 190 |  Loss: (0.1555) |  Loss2: (0.0000) | Acc: (94.00%) (23119/24448)
Epoch: 132 | Batch_idx: 200 |  Loss: (0.1548) |  Loss2: (0.0000) | Acc: (94.00%) (24340/25728)
Epoch: 132 | Batch_idx: 210 |  Loss: (0.1562) |  Loss2: (0.0000) | Acc: (94.00%) (25538/27008)
Epoch: 132 | Batch_idx: 220 |  Loss: (0.1572) |  Loss2: (0.0000) | Acc: (94.00%) (26744/28288)
Epoch: 132 | Batch_idx: 230 |  Loss: (0.1573) |  Loss2: (0.0000) | Acc: (94.00%) (27961/29568)
Epoch: 132 | Batch_idx: 240 |  Loss: (0.1567) |  Loss2: (0.0000) | Acc: (94.00%) (29178/30848)
Epoch: 132 | Batch_idx: 250 |  Loss: (0.1564) |  Loss2: (0.0000) | Acc: (94.00%) (30396/32128)
Epoch: 132 | Batch_idx: 260 |  Loss: (0.1566) |  Loss2: (0.0000) | Acc: (94.00%) (31603/33408)
Epoch: 132 | Batch_idx: 270 |  Loss: (0.1574) |  Loss2: (0.0000) | Acc: (94.00%) (32802/34688)
Epoch: 132 | Batch_idx: 280 |  Loss: (0.1576) |  Loss2: (0.0000) | Acc: (94.00%) (34006/35968)
Epoch: 132 | Batch_idx: 290 |  Loss: (0.1575) |  Loss2: (0.0000) | Acc: (94.00%) (35222/37248)
Epoch: 132 | Batch_idx: 300 |  Loss: (0.1579) |  Loss2: (0.0000) | Acc: (94.00%) (36432/38528)
Epoch: 132 | Batch_idx: 310 |  Loss: (0.1582) |  Loss2: (0.0000) | Acc: (94.00%) (37646/39808)
Epoch: 132 | Batch_idx: 320 |  Loss: (0.1580) |  Loss2: (0.0000) | Acc: (94.00%) (38858/41088)
Epoch: 132 | Batch_idx: 330 |  Loss: (0.1580) |  Loss2: (0.0000) | Acc: (94.00%) (40071/42368)
Epoch: 132 | Batch_idx: 340 |  Loss: (0.1585) |  Loss2: (0.0000) | Acc: (94.00%) (41269/43648)
Epoch: 132 | Batch_idx: 350 |  Loss: (0.1578) |  Loss2: (0.0000) | Acc: (94.00%) (42496/44928)
Epoch: 132 | Batch_idx: 360 |  Loss: (0.1573) |  Loss2: (0.0000) | Acc: (94.00%) (43717/46208)
Epoch: 132 | Batch_idx: 370 |  Loss: (0.1572) |  Loss2: (0.0000) | Acc: (94.00%) (44926/47488)
Epoch: 132 | Batch_idx: 380 |  Loss: (0.1574) |  Loss2: (0.0000) | Acc: (94.00%) (46126/48768)
Epoch: 132 | Batch_idx: 390 |  Loss: (0.1575) |  Loss2: (0.0000) | Acc: (94.00%) (47289/50000)
# TEST : Loss: (0.4851) | Acc: (86.00%) (8605/10000)
percent tensor([0.5805, 0.4195], device='cuda:0')
percent tensor([0.5749, 0.4251], device='cuda:0')
percent tensor([0.5500, 0.4500], device='cuda:0')
percent tensor([0.6250, 0.3750], device='cuda:0')
percent tensor([0.5997, 0.4003], device='cuda:0')
percent tensor([0.6641, 0.3359], device='cuda:0')
percent tensor([0.7068, 0.2932], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 133 | Batch_idx: 0 |  Loss: (0.1381) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 133 | Batch_idx: 10 |  Loss: (0.1417) |  Loss2: (0.0000) | Acc: (95.00%) (1345/1408)
Epoch: 133 | Batch_idx: 20 |  Loss: (0.1356) |  Loss2: (0.0000) | Acc: (95.00%) (2569/2688)
Epoch: 133 | Batch_idx: 30 |  Loss: (0.1328) |  Loss2: (0.0000) | Acc: (95.00%) (3799/3968)
Epoch: 133 | Batch_idx: 40 |  Loss: (0.1365) |  Loss2: (0.0000) | Acc: (95.00%) (5021/5248)
Epoch: 133 | Batch_idx: 50 |  Loss: (0.1386) |  Loss2: (0.0000) | Acc: (95.00%) (6243/6528)
Epoch: 133 | Batch_idx: 60 |  Loss: (0.1393) |  Loss2: (0.0000) | Acc: (95.00%) (7463/7808)
Epoch: 133 | Batch_idx: 70 |  Loss: (0.1419) |  Loss2: (0.0000) | Acc: (95.00%) (8681/9088)
Epoch: 133 | Batch_idx: 80 |  Loss: (0.1416) |  Loss2: (0.0000) | Acc: (95.00%) (9898/10368)
Epoch: 133 | Batch_idx: 90 |  Loss: (0.1426) |  Loss2: (0.0000) | Acc: (95.00%) (11113/11648)
Epoch: 133 | Batch_idx: 100 |  Loss: (0.1420) |  Loss2: (0.0000) | Acc: (95.00%) (12332/12928)
Epoch: 133 | Batch_idx: 110 |  Loss: (0.1415) |  Loss2: (0.0000) | Acc: (95.00%) (13558/14208)
Epoch: 133 | Batch_idx: 120 |  Loss: (0.1411) |  Loss2: (0.0000) | Acc: (95.00%) (14774/15488)
Epoch: 133 | Batch_idx: 130 |  Loss: (0.1419) |  Loss2: (0.0000) | Acc: (95.00%) (15986/16768)
Epoch: 133 | Batch_idx: 140 |  Loss: (0.1448) |  Loss2: (0.0000) | Acc: (95.00%) (17183/18048)
Epoch: 133 | Batch_idx: 150 |  Loss: (0.1457) |  Loss2: (0.0000) | Acc: (95.00%) (18399/19328)
Epoch: 133 | Batch_idx: 160 |  Loss: (0.1463) |  Loss2: (0.0000) | Acc: (95.00%) (19606/20608)
Epoch: 133 | Batch_idx: 170 |  Loss: (0.1463) |  Loss2: (0.0000) | Acc: (95.00%) (20821/21888)
Epoch: 133 | Batch_idx: 180 |  Loss: (0.1471) |  Loss2: (0.0000) | Acc: (95.00%) (22022/23168)
Epoch: 133 | Batch_idx: 190 |  Loss: (0.1462) |  Loss2: (0.0000) | Acc: (95.00%) (23255/24448)
Epoch: 133 | Batch_idx: 200 |  Loss: (0.1458) |  Loss2: (0.0000) | Acc: (95.00%) (24473/25728)
Epoch: 133 | Batch_idx: 210 |  Loss: (0.1469) |  Loss2: (0.0000) | Acc: (95.00%) (25685/27008)
Epoch: 133 | Batch_idx: 220 |  Loss: (0.1467) |  Loss2: (0.0000) | Acc: (95.00%) (26905/28288)
Epoch: 133 | Batch_idx: 230 |  Loss: (0.1475) |  Loss2: (0.0000) | Acc: (95.00%) (28108/29568)
Epoch: 133 | Batch_idx: 240 |  Loss: (0.1474) |  Loss2: (0.0000) | Acc: (95.00%) (29325/30848)
Epoch: 133 | Batch_idx: 250 |  Loss: (0.1465) |  Loss2: (0.0000) | Acc: (95.00%) (30547/32128)
Epoch: 133 | Batch_idx: 260 |  Loss: (0.1469) |  Loss2: (0.0000) | Acc: (95.00%) (31761/33408)
Epoch: 133 | Batch_idx: 270 |  Loss: (0.1477) |  Loss2: (0.0000) | Acc: (95.00%) (32970/34688)
Epoch: 133 | Batch_idx: 280 |  Loss: (0.1471) |  Loss2: (0.0000) | Acc: (95.00%) (34191/35968)
Epoch: 133 | Batch_idx: 290 |  Loss: (0.1479) |  Loss2: (0.0000) | Acc: (95.00%) (35399/37248)
Epoch: 133 | Batch_idx: 300 |  Loss: (0.1479) |  Loss2: (0.0000) | Acc: (95.00%) (36608/38528)
Epoch: 133 | Batch_idx: 310 |  Loss: (0.1483) |  Loss2: (0.0000) | Acc: (94.00%) (37815/39808)
Epoch: 133 | Batch_idx: 320 |  Loss: (0.1485) |  Loss2: (0.0000) | Acc: (94.00%) (39024/41088)
Epoch: 133 | Batch_idx: 330 |  Loss: (0.1492) |  Loss2: (0.0000) | Acc: (94.00%) (40227/42368)
Epoch: 133 | Batch_idx: 340 |  Loss: (0.1500) |  Loss2: (0.0000) | Acc: (94.00%) (41425/43648)
Epoch: 133 | Batch_idx: 350 |  Loss: (0.1499) |  Loss2: (0.0000) | Acc: (94.00%) (42642/44928)
Epoch: 133 | Batch_idx: 360 |  Loss: (0.1501) |  Loss2: (0.0000) | Acc: (94.00%) (43851/46208)
Epoch: 133 | Batch_idx: 370 |  Loss: (0.1496) |  Loss2: (0.0000) | Acc: (94.00%) (45065/47488)
Epoch: 133 | Batch_idx: 380 |  Loss: (0.1501) |  Loss2: (0.0000) | Acc: (94.00%) (46277/48768)
Epoch: 133 | Batch_idx: 390 |  Loss: (0.1507) |  Loss2: (0.0000) | Acc: (94.00%) (47433/50000)
# TEST : Loss: (0.4242) | Acc: (87.00%) (8763/10000)
percent tensor([0.5815, 0.4185], device='cuda:0')
percent tensor([0.5750, 0.4250], device='cuda:0')
percent tensor([0.5498, 0.4502], device='cuda:0')
percent tensor([0.6240, 0.3760], device='cuda:0')
percent tensor([0.5991, 0.4009], device='cuda:0')
percent tensor([0.6651, 0.3349], device='cuda:0')
percent tensor([0.7073, 0.2927], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 134 | Batch_idx: 0 |  Loss: (0.1168) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 134 | Batch_idx: 10 |  Loss: (0.1444) |  Loss2: (0.0000) | Acc: (94.00%) (1337/1408)
Epoch: 134 | Batch_idx: 20 |  Loss: (0.1465) |  Loss2: (0.0000) | Acc: (94.00%) (2549/2688)
Epoch: 134 | Batch_idx: 30 |  Loss: (0.1543) |  Loss2: (0.0000) | Acc: (94.00%) (3748/3968)
Epoch: 134 | Batch_idx: 40 |  Loss: (0.1474) |  Loss2: (0.0000) | Acc: (94.00%) (4972/5248)
Epoch: 134 | Batch_idx: 50 |  Loss: (0.1451) |  Loss2: (0.0000) | Acc: (94.00%) (6189/6528)
Epoch: 134 | Batch_idx: 60 |  Loss: (0.1467) |  Loss2: (0.0000) | Acc: (94.00%) (7404/7808)
Epoch: 134 | Batch_idx: 70 |  Loss: (0.1459) |  Loss2: (0.0000) | Acc: (94.00%) (8621/9088)
Epoch: 134 | Batch_idx: 80 |  Loss: (0.1448) |  Loss2: (0.0000) | Acc: (94.00%) (9837/10368)
Epoch: 134 | Batch_idx: 90 |  Loss: (0.1436) |  Loss2: (0.0000) | Acc: (94.00%) (11053/11648)
Epoch: 134 | Batch_idx: 100 |  Loss: (0.1447) |  Loss2: (0.0000) | Acc: (94.00%) (12263/12928)
Epoch: 134 | Batch_idx: 110 |  Loss: (0.1450) |  Loss2: (0.0000) | Acc: (94.00%) (13475/14208)
Epoch: 134 | Batch_idx: 120 |  Loss: (0.1450) |  Loss2: (0.0000) | Acc: (94.00%) (14694/15488)
Epoch: 134 | Batch_idx: 130 |  Loss: (0.1433) |  Loss2: (0.0000) | Acc: (94.00%) (15923/16768)
Epoch: 134 | Batch_idx: 140 |  Loss: (0.1434) |  Loss2: (0.0000) | Acc: (94.00%) (17141/18048)
Epoch: 134 | Batch_idx: 150 |  Loss: (0.1430) |  Loss2: (0.0000) | Acc: (95.00%) (18368/19328)
Epoch: 134 | Batch_idx: 160 |  Loss: (0.1430) |  Loss2: (0.0000) | Acc: (95.00%) (19584/20608)
Epoch: 134 | Batch_idx: 170 |  Loss: (0.1427) |  Loss2: (0.0000) | Acc: (95.00%) (20803/21888)
Epoch: 134 | Batch_idx: 180 |  Loss: (0.1447) |  Loss2: (0.0000) | Acc: (94.00%) (21995/23168)
Epoch: 134 | Batch_idx: 190 |  Loss: (0.1443) |  Loss2: (0.0000) | Acc: (94.00%) (23217/24448)
Epoch: 134 | Batch_idx: 200 |  Loss: (0.1435) |  Loss2: (0.0000) | Acc: (95.00%) (24445/25728)
Epoch: 134 | Batch_idx: 210 |  Loss: (0.1444) |  Loss2: (0.0000) | Acc: (94.00%) (25656/27008)
Epoch: 134 | Batch_idx: 220 |  Loss: (0.1446) |  Loss2: (0.0000) | Acc: (94.00%) (26872/28288)
Epoch: 134 | Batch_idx: 230 |  Loss: (0.1440) |  Loss2: (0.0000) | Acc: (95.00%) (28092/29568)
Epoch: 134 | Batch_idx: 240 |  Loss: (0.1446) |  Loss2: (0.0000) | Acc: (94.00%) (29305/30848)
Epoch: 134 | Batch_idx: 250 |  Loss: (0.1449) |  Loss2: (0.0000) | Acc: (94.00%) (30516/32128)
Epoch: 134 | Batch_idx: 260 |  Loss: (0.1454) |  Loss2: (0.0000) | Acc: (94.00%) (31729/33408)
Epoch: 134 | Batch_idx: 270 |  Loss: (0.1456) |  Loss2: (0.0000) | Acc: (94.00%) (32944/34688)
Epoch: 134 | Batch_idx: 280 |  Loss: (0.1469) |  Loss2: (0.0000) | Acc: (94.00%) (34147/35968)
Epoch: 134 | Batch_idx: 290 |  Loss: (0.1473) |  Loss2: (0.0000) | Acc: (94.00%) (35354/37248)
Epoch: 134 | Batch_idx: 300 |  Loss: (0.1477) |  Loss2: (0.0000) | Acc: (94.00%) (36566/38528)
Epoch: 134 | Batch_idx: 310 |  Loss: (0.1471) |  Loss2: (0.0000) | Acc: (94.00%) (37789/39808)
Epoch: 134 | Batch_idx: 320 |  Loss: (0.1473) |  Loss2: (0.0000) | Acc: (94.00%) (38997/41088)
Epoch: 134 | Batch_idx: 330 |  Loss: (0.1469) |  Loss2: (0.0000) | Acc: (94.00%) (40219/42368)
Epoch: 134 | Batch_idx: 340 |  Loss: (0.1466) |  Loss2: (0.0000) | Acc: (94.00%) (41442/43648)
Epoch: 134 | Batch_idx: 350 |  Loss: (0.1462) |  Loss2: (0.0000) | Acc: (94.00%) (42663/44928)
Epoch: 134 | Batch_idx: 360 |  Loss: (0.1461) |  Loss2: (0.0000) | Acc: (94.00%) (43883/46208)
Epoch: 134 | Batch_idx: 370 |  Loss: (0.1464) |  Loss2: (0.0000) | Acc: (94.00%) (45100/47488)
Epoch: 134 | Batch_idx: 380 |  Loss: (0.1462) |  Loss2: (0.0000) | Acc: (94.00%) (46322/48768)
Epoch: 134 | Batch_idx: 390 |  Loss: (0.1467) |  Loss2: (0.0000) | Acc: (94.00%) (47481/50000)
# TEST : Loss: (0.4392) | Acc: (87.00%) (8735/10000)
percent tensor([0.5810, 0.4190], device='cuda:0')
percent tensor([0.5748, 0.4252], device='cuda:0')
percent tensor([0.5489, 0.4511], device='cuda:0')
percent tensor([0.6251, 0.3749], device='cuda:0')
percent tensor([0.5995, 0.4005], device='cuda:0')
percent tensor([0.6646, 0.3354], device='cuda:0')
percent tensor([0.6999, 0.3001], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 135 | Batch_idx: 0 |  Loss: (0.2500) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 135 | Batch_idx: 10 |  Loss: (0.1684) |  Loss2: (0.0000) | Acc: (94.00%) (1327/1408)
Epoch: 135 | Batch_idx: 20 |  Loss: (0.1742) |  Loss2: (0.0000) | Acc: (93.00%) (2524/2688)
Epoch: 135 | Batch_idx: 30 |  Loss: (0.1785) |  Loss2: (0.0000) | Acc: (93.00%) (3718/3968)
Epoch: 135 | Batch_idx: 40 |  Loss: (0.1821) |  Loss2: (0.0000) | Acc: (93.00%) (4904/5248)
Epoch: 135 | Batch_idx: 50 |  Loss: (0.1829) |  Loss2: (0.0000) | Acc: (93.00%) (6097/6528)
Epoch: 135 | Batch_idx: 60 |  Loss: (0.1839) |  Loss2: (0.0000) | Acc: (93.00%) (7296/7808)
Epoch: 135 | Batch_idx: 70 |  Loss: (0.1816) |  Loss2: (0.0000) | Acc: (93.00%) (8501/9088)
Epoch: 135 | Batch_idx: 80 |  Loss: (0.1852) |  Loss2: (0.0000) | Acc: (93.00%) (9684/10368)
Epoch: 135 | Batch_idx: 90 |  Loss: (0.1843) |  Loss2: (0.0000) | Acc: (93.00%) (10877/11648)
Epoch: 135 | Batch_idx: 100 |  Loss: (0.1844) |  Loss2: (0.0000) | Acc: (93.00%) (12078/12928)
Epoch: 135 | Batch_idx: 110 |  Loss: (0.1832) |  Loss2: (0.0000) | Acc: (93.00%) (13284/14208)
Epoch: 135 | Batch_idx: 120 |  Loss: (0.1847) |  Loss2: (0.0000) | Acc: (93.00%) (14474/15488)
Epoch: 135 | Batch_idx: 130 |  Loss: (0.1861) |  Loss2: (0.0000) | Acc: (93.00%) (15654/16768)
Epoch: 135 | Batch_idx: 140 |  Loss: (0.1880) |  Loss2: (0.0000) | Acc: (93.00%) (16835/18048)
Epoch: 135 | Batch_idx: 150 |  Loss: (0.1866) |  Loss2: (0.0000) | Acc: (93.00%) (18035/19328)
Epoch: 135 | Batch_idx: 160 |  Loss: (0.1863) |  Loss2: (0.0000) | Acc: (93.00%) (19236/20608)
Epoch: 135 | Batch_idx: 170 |  Loss: (0.1868) |  Loss2: (0.0000) | Acc: (93.00%) (20424/21888)
Epoch: 135 | Batch_idx: 180 |  Loss: (0.1879) |  Loss2: (0.0000) | Acc: (93.00%) (21606/23168)
Epoch: 135 | Batch_idx: 190 |  Loss: (0.1865) |  Loss2: (0.0000) | Acc: (93.00%) (22818/24448)
Epoch: 135 | Batch_idx: 200 |  Loss: (0.1861) |  Loss2: (0.0000) | Acc: (93.00%) (24019/25728)
Epoch: 135 | Batch_idx: 210 |  Loss: (0.1862) |  Loss2: (0.0000) | Acc: (93.00%) (25218/27008)
Epoch: 135 | Batch_idx: 220 |  Loss: (0.1855) |  Loss2: (0.0000) | Acc: (93.00%) (26415/28288)
Epoch: 135 | Batch_idx: 230 |  Loss: (0.1846) |  Loss2: (0.0000) | Acc: (93.00%) (27622/29568)
Epoch: 135 | Batch_idx: 240 |  Loss: (0.1832) |  Loss2: (0.0000) | Acc: (93.00%) (28837/30848)
Epoch: 135 | Batch_idx: 250 |  Loss: (0.1835) |  Loss2: (0.0000) | Acc: (93.00%) (30036/32128)
Epoch: 135 | Batch_idx: 260 |  Loss: (0.1824) |  Loss2: (0.0000) | Acc: (93.00%) (31250/33408)
Epoch: 135 | Batch_idx: 270 |  Loss: (0.1822) |  Loss2: (0.0000) | Acc: (93.00%) (32445/34688)
Epoch: 135 | Batch_idx: 280 |  Loss: (0.1819) |  Loss2: (0.0000) | Acc: (93.00%) (33645/35968)
Epoch: 135 | Batch_idx: 290 |  Loss: (0.1818) |  Loss2: (0.0000) | Acc: (93.00%) (34847/37248)
Epoch: 135 | Batch_idx: 300 |  Loss: (0.1813) |  Loss2: (0.0000) | Acc: (93.00%) (36057/38528)
Epoch: 135 | Batch_idx: 310 |  Loss: (0.1804) |  Loss2: (0.0000) | Acc: (93.00%) (37265/39808)
Epoch: 135 | Batch_idx: 320 |  Loss: (0.1804) |  Loss2: (0.0000) | Acc: (93.00%) (38459/41088)
Epoch: 135 | Batch_idx: 330 |  Loss: (0.1794) |  Loss2: (0.0000) | Acc: (93.00%) (39667/42368)
Epoch: 135 | Batch_idx: 340 |  Loss: (0.1794) |  Loss2: (0.0000) | Acc: (93.00%) (40865/43648)
Epoch: 135 | Batch_idx: 350 |  Loss: (0.1789) |  Loss2: (0.0000) | Acc: (93.00%) (42076/44928)
Epoch: 135 | Batch_idx: 360 |  Loss: (0.1789) |  Loss2: (0.0000) | Acc: (93.00%) (43284/46208)
Epoch: 135 | Batch_idx: 370 |  Loss: (0.1791) |  Loss2: (0.0000) | Acc: (93.00%) (44472/47488)
Epoch: 135 | Batch_idx: 380 |  Loss: (0.1783) |  Loss2: (0.0000) | Acc: (93.00%) (45686/48768)
Epoch: 135 | Batch_idx: 390 |  Loss: (0.1776) |  Loss2: (0.0000) | Acc: (93.00%) (46854/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_135.pth.tar'
# TEST : Loss: (0.4261) | Acc: (87.00%) (8733/10000)
percent tensor([0.5779, 0.4221], device='cuda:0')
percent tensor([0.5793, 0.4207], device='cuda:0')
percent tensor([0.5608, 0.4392], device='cuda:0')
percent tensor([0.6418, 0.3582], device='cuda:0')
percent tensor([0.5903, 0.4097], device='cuda:0')
percent tensor([0.6461, 0.3539], device='cuda:0')
percent tensor([0.7111, 0.2889], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 136 | Batch_idx: 0 |  Loss: (0.1307) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 136 | Batch_idx: 10 |  Loss: (0.1668) |  Loss2: (0.0000) | Acc: (94.00%) (1329/1408)
Epoch: 136 | Batch_idx: 20 |  Loss: (0.1651) |  Loss2: (0.0000) | Acc: (94.00%) (2535/2688)
Epoch: 136 | Batch_idx: 30 |  Loss: (0.1711) |  Loss2: (0.0000) | Acc: (94.00%) (3739/3968)
Epoch: 136 | Batch_idx: 40 |  Loss: (0.1699) |  Loss2: (0.0000) | Acc: (94.00%) (4954/5248)
Epoch: 136 | Batch_idx: 50 |  Loss: (0.1685) |  Loss2: (0.0000) | Acc: (94.00%) (6159/6528)
Epoch: 136 | Batch_idx: 60 |  Loss: (0.1696) |  Loss2: (0.0000) | Acc: (94.00%) (7353/7808)
Epoch: 136 | Batch_idx: 70 |  Loss: (0.1628) |  Loss2: (0.0000) | Acc: (94.00%) (8581/9088)
Epoch: 136 | Batch_idx: 80 |  Loss: (0.1622) |  Loss2: (0.0000) | Acc: (94.00%) (9794/10368)
Epoch: 136 | Batch_idx: 90 |  Loss: (0.1605) |  Loss2: (0.0000) | Acc: (94.00%) (11010/11648)
Epoch: 136 | Batch_idx: 100 |  Loss: (0.1625) |  Loss2: (0.0000) | Acc: (94.00%) (12202/12928)
Epoch: 136 | Batch_idx: 110 |  Loss: (0.1620) |  Loss2: (0.0000) | Acc: (94.00%) (13411/14208)
Epoch: 136 | Batch_idx: 120 |  Loss: (0.1596) |  Loss2: (0.0000) | Acc: (94.00%) (14633/15488)
Epoch: 136 | Batch_idx: 130 |  Loss: (0.1602) |  Loss2: (0.0000) | Acc: (94.00%) (15837/16768)
Epoch: 136 | Batch_idx: 140 |  Loss: (0.1611) |  Loss2: (0.0000) | Acc: (94.00%) (17030/18048)
Epoch: 136 | Batch_idx: 150 |  Loss: (0.1613) |  Loss2: (0.0000) | Acc: (94.00%) (18229/19328)
Epoch: 136 | Batch_idx: 160 |  Loss: (0.1614) |  Loss2: (0.0000) | Acc: (94.00%) (19433/20608)
Epoch: 136 | Batch_idx: 170 |  Loss: (0.1637) |  Loss2: (0.0000) | Acc: (94.00%) (20618/21888)
Epoch: 136 | Batch_idx: 180 |  Loss: (0.1652) |  Loss2: (0.0000) | Acc: (94.00%) (21806/23168)
Epoch: 136 | Batch_idx: 190 |  Loss: (0.1645) |  Loss2: (0.0000) | Acc: (94.00%) (23020/24448)
Epoch: 136 | Batch_idx: 200 |  Loss: (0.1652) |  Loss2: (0.0000) | Acc: (94.00%) (24232/25728)
Epoch: 136 | Batch_idx: 210 |  Loss: (0.1644) |  Loss2: (0.0000) | Acc: (94.00%) (25448/27008)
Epoch: 136 | Batch_idx: 220 |  Loss: (0.1638) |  Loss2: (0.0000) | Acc: (94.00%) (26663/28288)
Epoch: 136 | Batch_idx: 230 |  Loss: (0.1640) |  Loss2: (0.0000) | Acc: (94.00%) (27870/29568)
Epoch: 136 | Batch_idx: 240 |  Loss: (0.1642) |  Loss2: (0.0000) | Acc: (94.00%) (29068/30848)
Epoch: 136 | Batch_idx: 250 |  Loss: (0.1647) |  Loss2: (0.0000) | Acc: (94.00%) (30267/32128)
Epoch: 136 | Batch_idx: 260 |  Loss: (0.1644) |  Loss2: (0.0000) | Acc: (94.00%) (31481/33408)
Epoch: 136 | Batch_idx: 270 |  Loss: (0.1646) |  Loss2: (0.0000) | Acc: (94.00%) (32678/34688)
Epoch: 136 | Batch_idx: 280 |  Loss: (0.1645) |  Loss2: (0.0000) | Acc: (94.00%) (33883/35968)
Epoch: 136 | Batch_idx: 290 |  Loss: (0.1641) |  Loss2: (0.0000) | Acc: (94.00%) (35090/37248)
Epoch: 136 | Batch_idx: 300 |  Loss: (0.1631) |  Loss2: (0.0000) | Acc: (94.00%) (36308/38528)
Epoch: 136 | Batch_idx: 310 |  Loss: (0.1623) |  Loss2: (0.0000) | Acc: (94.00%) (37530/39808)
Epoch: 136 | Batch_idx: 320 |  Loss: (0.1617) |  Loss2: (0.0000) | Acc: (94.00%) (38751/41088)
Epoch: 136 | Batch_idx: 330 |  Loss: (0.1617) |  Loss2: (0.0000) | Acc: (94.00%) (39955/42368)
Epoch: 136 | Batch_idx: 340 |  Loss: (0.1619) |  Loss2: (0.0000) | Acc: (94.00%) (41166/43648)
Epoch: 136 | Batch_idx: 350 |  Loss: (0.1611) |  Loss2: (0.0000) | Acc: (94.00%) (42388/44928)
Epoch: 136 | Batch_idx: 360 |  Loss: (0.1605) |  Loss2: (0.0000) | Acc: (94.00%) (43609/46208)
Epoch: 136 | Batch_idx: 370 |  Loss: (0.1601) |  Loss2: (0.0000) | Acc: (94.00%) (44824/47488)
Epoch: 136 | Batch_idx: 380 |  Loss: (0.1599) |  Loss2: (0.0000) | Acc: (94.00%) (46034/48768)
Epoch: 136 | Batch_idx: 390 |  Loss: (0.1598) |  Loss2: (0.0000) | Acc: (94.00%) (47194/50000)
# TEST : Loss: (0.4075) | Acc: (87.00%) (8793/10000)
percent tensor([0.5771, 0.4229], device='cuda:0')
percent tensor([0.5801, 0.4199], device='cuda:0')
percent tensor([0.5659, 0.4341], device='cuda:0')
percent tensor([0.6414, 0.3586], device='cuda:0')
percent tensor([0.5937, 0.4063], device='cuda:0')
percent tensor([0.6483, 0.3517], device='cuda:0')
percent tensor([0.7261, 0.2739], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 137 | Batch_idx: 0 |  Loss: (0.1815) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 137 | Batch_idx: 10 |  Loss: (0.1639) |  Loss2: (0.0000) | Acc: (94.00%) (1326/1408)
Epoch: 137 | Batch_idx: 20 |  Loss: (0.1712) |  Loss2: (0.0000) | Acc: (94.00%) (2528/2688)
Epoch: 137 | Batch_idx: 30 |  Loss: (0.1621) |  Loss2: (0.0000) | Acc: (94.00%) (3740/3968)
Epoch: 137 | Batch_idx: 40 |  Loss: (0.1678) |  Loss2: (0.0000) | Acc: (94.00%) (4939/5248)
Epoch: 137 | Batch_idx: 50 |  Loss: (0.1728) |  Loss2: (0.0000) | Acc: (94.00%) (6138/6528)
Epoch: 137 | Batch_idx: 60 |  Loss: (0.1716) |  Loss2: (0.0000) | Acc: (94.00%) (7344/7808)
Epoch: 137 | Batch_idx: 70 |  Loss: (0.1671) |  Loss2: (0.0000) | Acc: (94.00%) (8560/9088)
Epoch: 137 | Batch_idx: 80 |  Loss: (0.1661) |  Loss2: (0.0000) | Acc: (94.00%) (9767/10368)
Epoch: 137 | Batch_idx: 90 |  Loss: (0.1635) |  Loss2: (0.0000) | Acc: (94.00%) (10980/11648)
Epoch: 137 | Batch_idx: 100 |  Loss: (0.1612) |  Loss2: (0.0000) | Acc: (94.00%) (12201/12928)
Epoch: 137 | Batch_idx: 110 |  Loss: (0.1622) |  Loss2: (0.0000) | Acc: (94.00%) (13404/14208)
Epoch: 137 | Batch_idx: 120 |  Loss: (0.1625) |  Loss2: (0.0000) | Acc: (94.00%) (14609/15488)
Epoch: 137 | Batch_idx: 130 |  Loss: (0.1612) |  Loss2: (0.0000) | Acc: (94.00%) (15826/16768)
Epoch: 137 | Batch_idx: 140 |  Loss: (0.1608) |  Loss2: (0.0000) | Acc: (94.00%) (17035/18048)
Epoch: 137 | Batch_idx: 150 |  Loss: (0.1598) |  Loss2: (0.0000) | Acc: (94.00%) (18255/19328)
Epoch: 137 | Batch_idx: 160 |  Loss: (0.1586) |  Loss2: (0.0000) | Acc: (94.00%) (19477/20608)
Epoch: 137 | Batch_idx: 170 |  Loss: (0.1573) |  Loss2: (0.0000) | Acc: (94.00%) (20699/21888)
Epoch: 137 | Batch_idx: 180 |  Loss: (0.1556) |  Loss2: (0.0000) | Acc: (94.00%) (21923/23168)
Epoch: 137 | Batch_idx: 190 |  Loss: (0.1561) |  Loss2: (0.0000) | Acc: (94.00%) (23129/24448)
Epoch: 137 | Batch_idx: 200 |  Loss: (0.1558) |  Loss2: (0.0000) | Acc: (94.00%) (24338/25728)
Epoch: 137 | Batch_idx: 210 |  Loss: (0.1548) |  Loss2: (0.0000) | Acc: (94.00%) (25562/27008)
Epoch: 137 | Batch_idx: 220 |  Loss: (0.1550) |  Loss2: (0.0000) | Acc: (94.00%) (26772/28288)
Epoch: 137 | Batch_idx: 230 |  Loss: (0.1549) |  Loss2: (0.0000) | Acc: (94.00%) (27993/29568)
Epoch: 137 | Batch_idx: 240 |  Loss: (0.1545) |  Loss2: (0.0000) | Acc: (94.00%) (29209/30848)
Epoch: 137 | Batch_idx: 250 |  Loss: (0.1540) |  Loss2: (0.0000) | Acc: (94.00%) (30430/32128)
Epoch: 137 | Batch_idx: 260 |  Loss: (0.1532) |  Loss2: (0.0000) | Acc: (94.00%) (31650/33408)
Epoch: 137 | Batch_idx: 270 |  Loss: (0.1524) |  Loss2: (0.0000) | Acc: (94.00%) (32870/34688)
Epoch: 137 | Batch_idx: 280 |  Loss: (0.1519) |  Loss2: (0.0000) | Acc: (94.00%) (34088/35968)
Epoch: 137 | Batch_idx: 290 |  Loss: (0.1525) |  Loss2: (0.0000) | Acc: (94.00%) (35293/37248)
Epoch: 137 | Batch_idx: 300 |  Loss: (0.1520) |  Loss2: (0.0000) | Acc: (94.00%) (36512/38528)
Epoch: 137 | Batch_idx: 310 |  Loss: (0.1518) |  Loss2: (0.0000) | Acc: (94.00%) (37727/39808)
Epoch: 137 | Batch_idx: 320 |  Loss: (0.1517) |  Loss2: (0.0000) | Acc: (94.00%) (38945/41088)
Epoch: 137 | Batch_idx: 330 |  Loss: (0.1514) |  Loss2: (0.0000) | Acc: (94.00%) (40163/42368)
Epoch: 137 | Batch_idx: 340 |  Loss: (0.1510) |  Loss2: (0.0000) | Acc: (94.00%) (41386/43648)
Epoch: 137 | Batch_idx: 350 |  Loss: (0.1504) |  Loss2: (0.0000) | Acc: (94.00%) (42613/44928)
Epoch: 137 | Batch_idx: 360 |  Loss: (0.1506) |  Loss2: (0.0000) | Acc: (94.00%) (43815/46208)
Epoch: 137 | Batch_idx: 370 |  Loss: (0.1505) |  Loss2: (0.0000) | Acc: (94.00%) (45039/47488)
Epoch: 137 | Batch_idx: 380 |  Loss: (0.1506) |  Loss2: (0.0000) | Acc: (94.00%) (46248/48768)
Epoch: 137 | Batch_idx: 390 |  Loss: (0.1503) |  Loss2: (0.0000) | Acc: (94.00%) (47425/50000)
# TEST : Loss: (0.4035) | Acc: (88.00%) (8807/10000)
percent tensor([0.5800, 0.4200], device='cuda:0')
percent tensor([0.5825, 0.4175], device='cuda:0')
percent tensor([0.5647, 0.4353], device='cuda:0')
percent tensor([0.6451, 0.3549], device='cuda:0')
percent tensor([0.5962, 0.4038], device='cuda:0')
percent tensor([0.6568, 0.3432], device='cuda:0')
percent tensor([0.7368, 0.2632], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 138 | Batch_idx: 0 |  Loss: (0.1374) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 138 | Batch_idx: 10 |  Loss: (0.1116) |  Loss2: (0.0000) | Acc: (96.00%) (1358/1408)
Epoch: 138 | Batch_idx: 20 |  Loss: (0.1229) |  Loss2: (0.0000) | Acc: (96.00%) (2585/2688)
Epoch: 138 | Batch_idx: 30 |  Loss: (0.1281) |  Loss2: (0.0000) | Acc: (95.00%) (3804/3968)
Epoch: 138 | Batch_idx: 40 |  Loss: (0.1311) |  Loss2: (0.0000) | Acc: (95.00%) (5028/5248)
Epoch: 138 | Batch_idx: 50 |  Loss: (0.1334) |  Loss2: (0.0000) | Acc: (95.00%) (6243/6528)
Epoch: 138 | Batch_idx: 60 |  Loss: (0.1381) |  Loss2: (0.0000) | Acc: (95.00%) (7455/7808)
Epoch: 138 | Batch_idx: 70 |  Loss: (0.1377) |  Loss2: (0.0000) | Acc: (95.00%) (8674/9088)
Epoch: 138 | Batch_idx: 80 |  Loss: (0.1377) |  Loss2: (0.0000) | Acc: (95.00%) (9904/10368)
Epoch: 138 | Batch_idx: 90 |  Loss: (0.1406) |  Loss2: (0.0000) | Acc: (95.00%) (11112/11648)
Epoch: 138 | Batch_idx: 100 |  Loss: (0.1393) |  Loss2: (0.0000) | Acc: (95.00%) (12338/12928)
Epoch: 138 | Batch_idx: 110 |  Loss: (0.1399) |  Loss2: (0.0000) | Acc: (95.00%) (13555/14208)
Epoch: 138 | Batch_idx: 120 |  Loss: (0.1382) |  Loss2: (0.0000) | Acc: (95.00%) (14778/15488)
Epoch: 138 | Batch_idx: 130 |  Loss: (0.1390) |  Loss2: (0.0000) | Acc: (95.00%) (15996/16768)
Epoch: 138 | Batch_idx: 140 |  Loss: (0.1398) |  Loss2: (0.0000) | Acc: (95.00%) (17207/18048)
Epoch: 138 | Batch_idx: 150 |  Loss: (0.1394) |  Loss2: (0.0000) | Acc: (95.00%) (18431/19328)
Epoch: 138 | Batch_idx: 160 |  Loss: (0.1396) |  Loss2: (0.0000) | Acc: (95.00%) (19652/20608)
Epoch: 138 | Batch_idx: 170 |  Loss: (0.1393) |  Loss2: (0.0000) | Acc: (95.00%) (20877/21888)
Epoch: 138 | Batch_idx: 180 |  Loss: (0.1399) |  Loss2: (0.0000) | Acc: (95.00%) (22089/23168)
Epoch: 138 | Batch_idx: 190 |  Loss: (0.1406) |  Loss2: (0.0000) | Acc: (95.00%) (23314/24448)
Epoch: 138 | Batch_idx: 200 |  Loss: (0.1409) |  Loss2: (0.0000) | Acc: (95.00%) (24527/25728)
Epoch: 138 | Batch_idx: 210 |  Loss: (0.1416) |  Loss2: (0.0000) | Acc: (95.00%) (25730/27008)
Epoch: 138 | Batch_idx: 220 |  Loss: (0.1416) |  Loss2: (0.0000) | Acc: (95.00%) (26937/28288)
Epoch: 138 | Batch_idx: 230 |  Loss: (0.1425) |  Loss2: (0.0000) | Acc: (95.00%) (28139/29568)
Epoch: 138 | Batch_idx: 240 |  Loss: (0.1425) |  Loss2: (0.0000) | Acc: (95.00%) (29356/30848)
Epoch: 138 | Batch_idx: 250 |  Loss: (0.1432) |  Loss2: (0.0000) | Acc: (95.00%) (30566/32128)
Epoch: 138 | Batch_idx: 260 |  Loss: (0.1443) |  Loss2: (0.0000) | Acc: (95.00%) (31768/33408)
Epoch: 138 | Batch_idx: 270 |  Loss: (0.1442) |  Loss2: (0.0000) | Acc: (95.00%) (32986/34688)
Epoch: 138 | Batch_idx: 280 |  Loss: (0.1440) |  Loss2: (0.0000) | Acc: (95.00%) (34209/35968)
Epoch: 138 | Batch_idx: 290 |  Loss: (0.1439) |  Loss2: (0.0000) | Acc: (95.00%) (35430/37248)
Epoch: 138 | Batch_idx: 300 |  Loss: (0.1441) |  Loss2: (0.0000) | Acc: (95.00%) (36642/38528)
Epoch: 138 | Batch_idx: 310 |  Loss: (0.1445) |  Loss2: (0.0000) | Acc: (95.00%) (37839/39808)
Epoch: 138 | Batch_idx: 320 |  Loss: (0.1446) |  Loss2: (0.0000) | Acc: (95.00%) (39060/41088)
Epoch: 138 | Batch_idx: 330 |  Loss: (0.1448) |  Loss2: (0.0000) | Acc: (95.00%) (40275/42368)
Epoch: 138 | Batch_idx: 340 |  Loss: (0.1451) |  Loss2: (0.0000) | Acc: (95.00%) (41488/43648)
Epoch: 138 | Batch_idx: 350 |  Loss: (0.1446) |  Loss2: (0.0000) | Acc: (95.00%) (42712/44928)
Epoch: 138 | Batch_idx: 360 |  Loss: (0.1443) |  Loss2: (0.0000) | Acc: (95.00%) (43935/46208)
Epoch: 138 | Batch_idx: 370 |  Loss: (0.1442) |  Loss2: (0.0000) | Acc: (95.00%) (45154/47488)
Epoch: 138 | Batch_idx: 380 |  Loss: (0.1437) |  Loss2: (0.0000) | Acc: (95.00%) (46381/48768)
Epoch: 138 | Batch_idx: 390 |  Loss: (0.1436) |  Loss2: (0.0000) | Acc: (95.00%) (47555/50000)
# TEST : Loss: (0.3944) | Acc: (88.00%) (8829/10000)
percent tensor([0.5823, 0.4177], device='cuda:0')
percent tensor([0.5808, 0.4192], device='cuda:0')
percent tensor([0.5605, 0.4395], device='cuda:0')
percent tensor([0.6416, 0.3584], device='cuda:0')
percent tensor([0.5912, 0.4088], device='cuda:0')
percent tensor([0.6580, 0.3420], device='cuda:0')
percent tensor([0.7324, 0.2676], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 139 | Batch_idx: 0 |  Loss: (0.1171) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 139 | Batch_idx: 10 |  Loss: (0.1271) |  Loss2: (0.0000) | Acc: (95.00%) (1348/1408)
Epoch: 139 | Batch_idx: 20 |  Loss: (0.1371) |  Loss2: (0.0000) | Acc: (95.00%) (2560/2688)
Epoch: 139 | Batch_idx: 30 |  Loss: (0.1410) |  Loss2: (0.0000) | Acc: (95.00%) (3771/3968)
Epoch: 139 | Batch_idx: 40 |  Loss: (0.1370) |  Loss2: (0.0000) | Acc: (95.00%) (4997/5248)
Epoch: 139 | Batch_idx: 50 |  Loss: (0.1424) |  Loss2: (0.0000) | Acc: (94.00%) (6196/6528)
Epoch: 139 | Batch_idx: 60 |  Loss: (0.1417) |  Loss2: (0.0000) | Acc: (94.00%) (7409/7808)
Epoch: 139 | Batch_idx: 70 |  Loss: (0.1395) |  Loss2: (0.0000) | Acc: (94.00%) (8633/9088)
Epoch: 139 | Batch_idx: 80 |  Loss: (0.1411) |  Loss2: (0.0000) | Acc: (94.00%) (9844/10368)
Epoch: 139 | Batch_idx: 90 |  Loss: (0.1387) |  Loss2: (0.0000) | Acc: (95.00%) (11075/11648)
Epoch: 139 | Batch_idx: 100 |  Loss: (0.1384) |  Loss2: (0.0000) | Acc: (95.00%) (12297/12928)
Epoch: 139 | Batch_idx: 110 |  Loss: (0.1388) |  Loss2: (0.0000) | Acc: (95.00%) (13509/14208)
Epoch: 139 | Batch_idx: 120 |  Loss: (0.1396) |  Loss2: (0.0000) | Acc: (95.00%) (14722/15488)
Epoch: 139 | Batch_idx: 130 |  Loss: (0.1393) |  Loss2: (0.0000) | Acc: (95.00%) (15944/16768)
Epoch: 139 | Batch_idx: 140 |  Loss: (0.1397) |  Loss2: (0.0000) | Acc: (95.00%) (17163/18048)
Epoch: 139 | Batch_idx: 150 |  Loss: (0.1396) |  Loss2: (0.0000) | Acc: (95.00%) (18381/19328)
Epoch: 139 | Batch_idx: 160 |  Loss: (0.1393) |  Loss2: (0.0000) | Acc: (95.00%) (19601/20608)
Epoch: 139 | Batch_idx: 170 |  Loss: (0.1397) |  Loss2: (0.0000) | Acc: (95.00%) (20806/21888)
Epoch: 139 | Batch_idx: 180 |  Loss: (0.1398) |  Loss2: (0.0000) | Acc: (95.00%) (22024/23168)
Epoch: 139 | Batch_idx: 190 |  Loss: (0.1407) |  Loss2: (0.0000) | Acc: (95.00%) (23231/24448)
Epoch: 139 | Batch_idx: 200 |  Loss: (0.1389) |  Loss2: (0.0000) | Acc: (95.00%) (24469/25728)
Epoch: 139 | Batch_idx: 210 |  Loss: (0.1386) |  Loss2: (0.0000) | Acc: (95.00%) (25691/27008)
Epoch: 139 | Batch_idx: 220 |  Loss: (0.1396) |  Loss2: (0.0000) | Acc: (95.00%) (26903/28288)
Epoch: 139 | Batch_idx: 230 |  Loss: (0.1403) |  Loss2: (0.0000) | Acc: (95.00%) (28120/29568)
Epoch: 139 | Batch_idx: 240 |  Loss: (0.1416) |  Loss2: (0.0000) | Acc: (95.00%) (29320/30848)
Epoch: 139 | Batch_idx: 250 |  Loss: (0.1410) |  Loss2: (0.0000) | Acc: (95.00%) (30547/32128)
Epoch: 139 | Batch_idx: 260 |  Loss: (0.1405) |  Loss2: (0.0000) | Acc: (95.00%) (31764/33408)
Epoch: 139 | Batch_idx: 270 |  Loss: (0.1399) |  Loss2: (0.0000) | Acc: (95.00%) (32995/34688)
Epoch: 139 | Batch_idx: 280 |  Loss: (0.1395) |  Loss2: (0.0000) | Acc: (95.00%) (34227/35968)
Epoch: 139 | Batch_idx: 290 |  Loss: (0.1394) |  Loss2: (0.0000) | Acc: (95.00%) (35446/37248)
Epoch: 139 | Batch_idx: 300 |  Loss: (0.1396) |  Loss2: (0.0000) | Acc: (95.00%) (36667/38528)
Epoch: 139 | Batch_idx: 310 |  Loss: (0.1395) |  Loss2: (0.0000) | Acc: (95.00%) (37888/39808)
Epoch: 139 | Batch_idx: 320 |  Loss: (0.1396) |  Loss2: (0.0000) | Acc: (95.00%) (39105/41088)
Epoch: 139 | Batch_idx: 330 |  Loss: (0.1390) |  Loss2: (0.0000) | Acc: (95.00%) (40340/42368)
Epoch: 139 | Batch_idx: 340 |  Loss: (0.1388) |  Loss2: (0.0000) | Acc: (95.00%) (41565/43648)
Epoch: 139 | Batch_idx: 350 |  Loss: (0.1387) |  Loss2: (0.0000) | Acc: (95.00%) (42788/44928)
Epoch: 139 | Batch_idx: 360 |  Loss: (0.1386) |  Loss2: (0.0000) | Acc: (95.00%) (44012/46208)
Epoch: 139 | Batch_idx: 370 |  Loss: (0.1384) |  Loss2: (0.0000) | Acc: (95.00%) (45234/47488)
Epoch: 139 | Batch_idx: 380 |  Loss: (0.1385) |  Loss2: (0.0000) | Acc: (95.00%) (46456/48768)
Epoch: 139 | Batch_idx: 390 |  Loss: (0.1388) |  Loss2: (0.0000) | Acc: (95.00%) (47621/50000)
# TEST : Loss: (0.3908) | Acc: (88.00%) (8825/10000)
percent tensor([0.5794, 0.4206], device='cuda:0')
percent tensor([0.5803, 0.4197], device='cuda:0')
percent tensor([0.5604, 0.4396], device='cuda:0')
percent tensor([0.6397, 0.3603], device='cuda:0')
percent tensor([0.5965, 0.4035], device='cuda:0')
percent tensor([0.6630, 0.3370], device='cuda:0')
percent tensor([0.7348, 0.2652], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 140 | Batch_idx: 0 |  Loss: (0.2125) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 140 | Batch_idx: 10 |  Loss: (0.1440) |  Loss2: (0.0000) | Acc: (95.00%) (1338/1408)
Epoch: 140 | Batch_idx: 20 |  Loss: (0.1437) |  Loss2: (0.0000) | Acc: (94.00%) (2549/2688)
Epoch: 140 | Batch_idx: 30 |  Loss: (0.1426) |  Loss2: (0.0000) | Acc: (94.00%) (3762/3968)
Epoch: 140 | Batch_idx: 40 |  Loss: (0.1445) |  Loss2: (0.0000) | Acc: (95.00%) (4988/5248)
Epoch: 140 | Batch_idx: 50 |  Loss: (0.1470) |  Loss2: (0.0000) | Acc: (94.00%) (6201/6528)
Epoch: 140 | Batch_idx: 60 |  Loss: (0.1489) |  Loss2: (0.0000) | Acc: (94.00%) (7412/7808)
Epoch: 140 | Batch_idx: 70 |  Loss: (0.1467) |  Loss2: (0.0000) | Acc: (94.00%) (8630/9088)
Epoch: 140 | Batch_idx: 80 |  Loss: (0.1450) |  Loss2: (0.0000) | Acc: (95.00%) (9853/10368)
Epoch: 140 | Batch_idx: 90 |  Loss: (0.1441) |  Loss2: (0.0000) | Acc: (95.00%) (11075/11648)
Epoch: 140 | Batch_idx: 100 |  Loss: (0.1455) |  Loss2: (0.0000) | Acc: (95.00%) (12283/12928)
Epoch: 140 | Batch_idx: 110 |  Loss: (0.1459) |  Loss2: (0.0000) | Acc: (94.00%) (13494/14208)
Epoch: 140 | Batch_idx: 120 |  Loss: (0.1433) |  Loss2: (0.0000) | Acc: (95.00%) (14729/15488)
Epoch: 140 | Batch_idx: 130 |  Loss: (0.1424) |  Loss2: (0.0000) | Acc: (95.00%) (15953/16768)
Epoch: 140 | Batch_idx: 140 |  Loss: (0.1408) |  Loss2: (0.0000) | Acc: (95.00%) (17182/18048)
Epoch: 140 | Batch_idx: 150 |  Loss: (0.1422) |  Loss2: (0.0000) | Acc: (95.00%) (18384/19328)
Epoch: 140 | Batch_idx: 160 |  Loss: (0.1429) |  Loss2: (0.0000) | Acc: (95.00%) (19604/20608)
Epoch: 140 | Batch_idx: 170 |  Loss: (0.1438) |  Loss2: (0.0000) | Acc: (95.00%) (20809/21888)
Epoch: 140 | Batch_idx: 180 |  Loss: (0.1439) |  Loss2: (0.0000) | Acc: (95.00%) (22029/23168)
Epoch: 140 | Batch_idx: 190 |  Loss: (0.1449) |  Loss2: (0.0000) | Acc: (95.00%) (23238/24448)
Epoch: 140 | Batch_idx: 200 |  Loss: (0.1448) |  Loss2: (0.0000) | Acc: (95.00%) (24450/25728)
Epoch: 140 | Batch_idx: 210 |  Loss: (0.1454) |  Loss2: (0.0000) | Acc: (95.00%) (25663/27008)
Epoch: 140 | Batch_idx: 220 |  Loss: (0.1445) |  Loss2: (0.0000) | Acc: (95.00%) (26887/28288)
Epoch: 140 | Batch_idx: 230 |  Loss: (0.1446) |  Loss2: (0.0000) | Acc: (95.00%) (28103/29568)
Epoch: 140 | Batch_idx: 240 |  Loss: (0.1442) |  Loss2: (0.0000) | Acc: (95.00%) (29331/30848)
Epoch: 140 | Batch_idx: 250 |  Loss: (0.1451) |  Loss2: (0.0000) | Acc: (95.00%) (30538/32128)
Epoch: 140 | Batch_idx: 260 |  Loss: (0.1456) |  Loss2: (0.0000) | Acc: (95.00%) (31743/33408)
Epoch: 140 | Batch_idx: 270 |  Loss: (0.1459) |  Loss2: (0.0000) | Acc: (95.00%) (32958/34688)
Epoch: 140 | Batch_idx: 280 |  Loss: (0.1456) |  Loss2: (0.0000) | Acc: (95.00%) (34177/35968)
Epoch: 140 | Batch_idx: 290 |  Loss: (0.1457) |  Loss2: (0.0000) | Acc: (95.00%) (35394/37248)
Epoch: 140 | Batch_idx: 300 |  Loss: (0.1467) |  Loss2: (0.0000) | Acc: (94.00%) (36591/38528)
Epoch: 140 | Batch_idx: 310 |  Loss: (0.1467) |  Loss2: (0.0000) | Acc: (94.00%) (37803/39808)
Epoch: 140 | Batch_idx: 320 |  Loss: (0.1467) |  Loss2: (0.0000) | Acc: (94.00%) (39025/41088)
Epoch: 140 | Batch_idx: 330 |  Loss: (0.1472) |  Loss2: (0.0000) | Acc: (94.00%) (40227/42368)
Epoch: 140 | Batch_idx: 340 |  Loss: (0.1469) |  Loss2: (0.0000) | Acc: (94.00%) (41450/43648)
Epoch: 140 | Batch_idx: 350 |  Loss: (0.1469) |  Loss2: (0.0000) | Acc: (94.00%) (42659/44928)
Epoch: 140 | Batch_idx: 360 |  Loss: (0.1467) |  Loss2: (0.0000) | Acc: (94.00%) (43876/46208)
Epoch: 140 | Batch_idx: 370 |  Loss: (0.1466) |  Loss2: (0.0000) | Acc: (94.00%) (45090/47488)
Epoch: 140 | Batch_idx: 380 |  Loss: (0.1464) |  Loss2: (0.0000) | Acc: (94.00%) (46307/48768)
Epoch: 140 | Batch_idx: 390 |  Loss: (0.1464) |  Loss2: (0.0000) | Acc: (94.00%) (47479/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_140.pth.tar'
# TEST : Loss: (0.4362) | Acc: (87.00%) (8774/10000)
percent tensor([0.5793, 0.4207], device='cuda:0')
percent tensor([0.5797, 0.4203], device='cuda:0')
percent tensor([0.5607, 0.4393], device='cuda:0')
percent tensor([0.6388, 0.3612], device='cuda:0')
percent tensor([0.5971, 0.4029], device='cuda:0')
percent tensor([0.6628, 0.3372], device='cuda:0')
percent tensor([0.7347, 0.2653], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(181.4400, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(821.9816, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(822.1249, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1521.1364, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(487.0274, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2272.9983, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4269.7402, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1374.2428, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6205.4424, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11733.7725, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3871.2607, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16373.6230, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 141 | Batch_idx: 0 |  Loss: (0.0520) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 141 | Batch_idx: 10 |  Loss: (0.1358) |  Loss2: (0.0000) | Acc: (95.00%) (1346/1408)
Epoch: 141 | Batch_idx: 20 |  Loss: (0.1286) |  Loss2: (0.0000) | Acc: (95.00%) (2575/2688)
Epoch: 141 | Batch_idx: 30 |  Loss: (0.1243) |  Loss2: (0.0000) | Acc: (95.00%) (3802/3968)
Epoch: 141 | Batch_idx: 40 |  Loss: (0.1259) |  Loss2: (0.0000) | Acc: (95.00%) (5018/5248)
Epoch: 141 | Batch_idx: 50 |  Loss: (0.1245) |  Loss2: (0.0000) | Acc: (95.00%) (6248/6528)
Epoch: 141 | Batch_idx: 60 |  Loss: (0.1248) |  Loss2: (0.0000) | Acc: (95.00%) (7468/7808)
Epoch: 141 | Batch_idx: 70 |  Loss: (0.1239) |  Loss2: (0.0000) | Acc: (95.00%) (8703/9088)
Epoch: 141 | Batch_idx: 80 |  Loss: (0.1237) |  Loss2: (0.0000) | Acc: (95.00%) (9926/10368)
Epoch: 141 | Batch_idx: 90 |  Loss: (0.1256) |  Loss2: (0.0000) | Acc: (95.00%) (11141/11648)
Epoch: 141 | Batch_idx: 100 |  Loss: (0.1264) |  Loss2: (0.0000) | Acc: (95.00%) (12366/12928)
Epoch: 141 | Batch_idx: 110 |  Loss: (0.1266) |  Loss2: (0.0000) | Acc: (95.00%) (13589/14208)
Epoch: 141 | Batch_idx: 120 |  Loss: (0.1273) |  Loss2: (0.0000) | Acc: (95.00%) (14799/15488)
Epoch: 141 | Batch_idx: 130 |  Loss: (0.1277) |  Loss2: (0.0000) | Acc: (95.00%) (16024/16768)
Epoch: 141 | Batch_idx: 140 |  Loss: (0.1278) |  Loss2: (0.0000) | Acc: (95.00%) (17251/18048)
Epoch: 141 | Batch_idx: 150 |  Loss: (0.1285) |  Loss2: (0.0000) | Acc: (95.00%) (18476/19328)
Epoch: 141 | Batch_idx: 160 |  Loss: (0.1289) |  Loss2: (0.0000) | Acc: (95.00%) (19692/20608)
Epoch: 141 | Batch_idx: 170 |  Loss: (0.1297) |  Loss2: (0.0000) | Acc: (95.00%) (20911/21888)
Epoch: 141 | Batch_idx: 180 |  Loss: (0.1310) |  Loss2: (0.0000) | Acc: (95.00%) (22120/23168)
Epoch: 141 | Batch_idx: 190 |  Loss: (0.1318) |  Loss2: (0.0000) | Acc: (95.00%) (23336/24448)
Epoch: 141 | Batch_idx: 200 |  Loss: (0.1324) |  Loss2: (0.0000) | Acc: (95.00%) (24556/25728)
Epoch: 141 | Batch_idx: 210 |  Loss: (0.1325) |  Loss2: (0.0000) | Acc: (95.00%) (25771/27008)
Epoch: 141 | Batch_idx: 220 |  Loss: (0.1335) |  Loss2: (0.0000) | Acc: (95.00%) (26983/28288)
Epoch: 141 | Batch_idx: 230 |  Loss: (0.1336) |  Loss2: (0.0000) | Acc: (95.00%) (28212/29568)
Epoch: 141 | Batch_idx: 240 |  Loss: (0.1351) |  Loss2: (0.0000) | Acc: (95.00%) (29416/30848)
Epoch: 141 | Batch_idx: 250 |  Loss: (0.1351) |  Loss2: (0.0000) | Acc: (95.00%) (30632/32128)
Epoch: 141 | Batch_idx: 260 |  Loss: (0.1348) |  Loss2: (0.0000) | Acc: (95.00%) (31856/33408)
Epoch: 141 | Batch_idx: 270 |  Loss: (0.1356) |  Loss2: (0.0000) | Acc: (95.00%) (33067/34688)
Epoch: 141 | Batch_idx: 280 |  Loss: (0.1359) |  Loss2: (0.0000) | Acc: (95.00%) (34286/35968)
Epoch: 141 | Batch_idx: 290 |  Loss: (0.1357) |  Loss2: (0.0000) | Acc: (95.00%) (35513/37248)
Epoch: 141 | Batch_idx: 300 |  Loss: (0.1360) |  Loss2: (0.0000) | Acc: (95.00%) (36727/38528)
Epoch: 141 | Batch_idx: 310 |  Loss: (0.1366) |  Loss2: (0.0000) | Acc: (95.00%) (37934/39808)
Epoch: 141 | Batch_idx: 320 |  Loss: (0.1372) |  Loss2: (0.0000) | Acc: (95.00%) (39150/41088)
Epoch: 141 | Batch_idx: 330 |  Loss: (0.1382) |  Loss2: (0.0000) | Acc: (95.00%) (40359/42368)
Epoch: 141 | Batch_idx: 340 |  Loss: (0.1386) |  Loss2: (0.0000) | Acc: (95.00%) (41577/43648)
Epoch: 141 | Batch_idx: 350 |  Loss: (0.1385) |  Loss2: (0.0000) | Acc: (95.00%) (42794/44928)
Epoch: 141 | Batch_idx: 360 |  Loss: (0.1392) |  Loss2: (0.0000) | Acc: (95.00%) (44003/46208)
Epoch: 141 | Batch_idx: 370 |  Loss: (0.1397) |  Loss2: (0.0000) | Acc: (95.00%) (45210/47488)
Epoch: 141 | Batch_idx: 380 |  Loss: (0.1399) |  Loss2: (0.0000) | Acc: (95.00%) (46424/48768)
Epoch: 141 | Batch_idx: 390 |  Loss: (0.1403) |  Loss2: (0.0000) | Acc: (95.00%) (47591/50000)
# TEST : Loss: (0.4210) | Acc: (88.00%) (8808/10000)
percent tensor([0.5791, 0.4209], device='cuda:0')
percent tensor([0.5794, 0.4206], device='cuda:0')
percent tensor([0.5608, 0.4392], device='cuda:0')
percent tensor([0.6381, 0.3619], device='cuda:0')
percent tensor([0.5961, 0.4039], device='cuda:0')
percent tensor([0.6612, 0.3388], device='cuda:0')
percent tensor([0.7333, 0.2667], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 142 | Batch_idx: 0 |  Loss: (0.1374) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 142 | Batch_idx: 10 |  Loss: (0.1125) |  Loss2: (0.0000) | Acc: (96.00%) (1359/1408)
Epoch: 142 | Batch_idx: 20 |  Loss: (0.1121) |  Loss2: (0.0000) | Acc: (96.00%) (2597/2688)
Epoch: 142 | Batch_idx: 30 |  Loss: (0.1154) |  Loss2: (0.0000) | Acc: (96.00%) (3827/3968)
Epoch: 142 | Batch_idx: 40 |  Loss: (0.1242) |  Loss2: (0.0000) | Acc: (95.00%) (5037/5248)
Epoch: 142 | Batch_idx: 50 |  Loss: (0.1238) |  Loss2: (0.0000) | Acc: (95.00%) (6258/6528)
Epoch: 142 | Batch_idx: 60 |  Loss: (0.1221) |  Loss2: (0.0000) | Acc: (95.00%) (7490/7808)
Epoch: 142 | Batch_idx: 70 |  Loss: (0.1224) |  Loss2: (0.0000) | Acc: (95.00%) (8716/9088)
Epoch: 142 | Batch_idx: 80 |  Loss: (0.1222) |  Loss2: (0.0000) | Acc: (95.00%) (9936/10368)
Epoch: 142 | Batch_idx: 90 |  Loss: (0.1230) |  Loss2: (0.0000) | Acc: (95.00%) (11155/11648)
Epoch: 142 | Batch_idx: 100 |  Loss: (0.1229) |  Loss2: (0.0000) | Acc: (95.00%) (12379/12928)
Epoch: 142 | Batch_idx: 110 |  Loss: (0.1231) |  Loss2: (0.0000) | Acc: (95.00%) (13611/14208)
Epoch: 142 | Batch_idx: 120 |  Loss: (0.1234) |  Loss2: (0.0000) | Acc: (95.00%) (14832/15488)
Epoch: 142 | Batch_idx: 130 |  Loss: (0.1252) |  Loss2: (0.0000) | Acc: (95.00%) (16051/16768)
Epoch: 142 | Batch_idx: 140 |  Loss: (0.1256) |  Loss2: (0.0000) | Acc: (95.00%) (17274/18048)
Epoch: 142 | Batch_idx: 150 |  Loss: (0.1258) |  Loss2: (0.0000) | Acc: (95.00%) (18497/19328)
Epoch: 142 | Batch_idx: 160 |  Loss: (0.1269) |  Loss2: (0.0000) | Acc: (95.00%) (19720/20608)
Epoch: 142 | Batch_idx: 170 |  Loss: (0.1286) |  Loss2: (0.0000) | Acc: (95.00%) (20928/21888)
Epoch: 142 | Batch_idx: 180 |  Loss: (0.1285) |  Loss2: (0.0000) | Acc: (95.00%) (22152/23168)
Epoch: 142 | Batch_idx: 190 |  Loss: (0.1291) |  Loss2: (0.0000) | Acc: (95.00%) (23362/24448)
Epoch: 142 | Batch_idx: 200 |  Loss: (0.1302) |  Loss2: (0.0000) | Acc: (95.00%) (24570/25728)
Epoch: 142 | Batch_idx: 210 |  Loss: (0.1313) |  Loss2: (0.0000) | Acc: (95.00%) (25778/27008)
Epoch: 142 | Batch_idx: 220 |  Loss: (0.1322) |  Loss2: (0.0000) | Acc: (95.00%) (26984/28288)
Epoch: 142 | Batch_idx: 230 |  Loss: (0.1326) |  Loss2: (0.0000) | Acc: (95.00%) (28206/29568)
Epoch: 142 | Batch_idx: 240 |  Loss: (0.1330) |  Loss2: (0.0000) | Acc: (95.00%) (29418/30848)
Epoch: 142 | Batch_idx: 250 |  Loss: (0.1336) |  Loss2: (0.0000) | Acc: (95.00%) (30635/32128)
Epoch: 142 | Batch_idx: 260 |  Loss: (0.1337) |  Loss2: (0.0000) | Acc: (95.00%) (31861/33408)
Epoch: 142 | Batch_idx: 270 |  Loss: (0.1337) |  Loss2: (0.0000) | Acc: (95.00%) (33090/34688)
Epoch: 142 | Batch_idx: 280 |  Loss: (0.1339) |  Loss2: (0.0000) | Acc: (95.00%) (34308/35968)
Epoch: 142 | Batch_idx: 290 |  Loss: (0.1344) |  Loss2: (0.0000) | Acc: (95.00%) (35512/37248)
Epoch: 142 | Batch_idx: 300 |  Loss: (0.1353) |  Loss2: (0.0000) | Acc: (95.00%) (36717/38528)
Epoch: 142 | Batch_idx: 310 |  Loss: (0.1355) |  Loss2: (0.0000) | Acc: (95.00%) (37923/39808)
Epoch: 142 | Batch_idx: 320 |  Loss: (0.1365) |  Loss2: (0.0000) | Acc: (95.00%) (39118/41088)
Epoch: 142 | Batch_idx: 330 |  Loss: (0.1362) |  Loss2: (0.0000) | Acc: (95.00%) (40340/42368)
Epoch: 142 | Batch_idx: 340 |  Loss: (0.1367) |  Loss2: (0.0000) | Acc: (95.00%) (41561/43648)
Epoch: 142 | Batch_idx: 350 |  Loss: (0.1367) |  Loss2: (0.0000) | Acc: (95.00%) (42784/44928)
Epoch: 142 | Batch_idx: 360 |  Loss: (0.1369) |  Loss2: (0.0000) | Acc: (95.00%) (44000/46208)
Epoch: 142 | Batch_idx: 370 |  Loss: (0.1372) |  Loss2: (0.0000) | Acc: (95.00%) (45220/47488)
Epoch: 142 | Batch_idx: 380 |  Loss: (0.1374) |  Loss2: (0.0000) | Acc: (95.00%) (46437/48768)
Epoch: 142 | Batch_idx: 390 |  Loss: (0.1381) |  Loss2: (0.0000) | Acc: (95.00%) (47606/50000)
# TEST : Loss: (0.4860) | Acc: (86.00%) (8648/10000)
percent tensor([0.5794, 0.4206], device='cuda:0')
percent tensor([0.5807, 0.4193], device='cuda:0')
percent tensor([0.5605, 0.4395], device='cuda:0')
percent tensor([0.6383, 0.3617], device='cuda:0')
percent tensor([0.5965, 0.4035], device='cuda:0')
percent tensor([0.6593, 0.3407], device='cuda:0')
percent tensor([0.7348, 0.2652], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 143 | Batch_idx: 0 |  Loss: (0.1285) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 143 | Batch_idx: 10 |  Loss: (0.1280) |  Loss2: (0.0000) | Acc: (95.00%) (1351/1408)
Epoch: 143 | Batch_idx: 20 |  Loss: (0.1158) |  Loss2: (0.0000) | Acc: (96.00%) (2586/2688)
Epoch: 143 | Batch_idx: 30 |  Loss: (0.1264) |  Loss2: (0.0000) | Acc: (95.00%) (3805/3968)
Epoch: 143 | Batch_idx: 40 |  Loss: (0.1225) |  Loss2: (0.0000) | Acc: (95.00%) (5037/5248)
Epoch: 143 | Batch_idx: 50 |  Loss: (0.1258) |  Loss2: (0.0000) | Acc: (95.00%) (6255/6528)
Epoch: 143 | Batch_idx: 60 |  Loss: (0.1263) |  Loss2: (0.0000) | Acc: (95.00%) (7477/7808)
Epoch: 143 | Batch_idx: 70 |  Loss: (0.1247) |  Loss2: (0.0000) | Acc: (95.00%) (8708/9088)
Epoch: 143 | Batch_idx: 80 |  Loss: (0.1252) |  Loss2: (0.0000) | Acc: (95.00%) (9932/10368)
Epoch: 143 | Batch_idx: 90 |  Loss: (0.1247) |  Loss2: (0.0000) | Acc: (95.00%) (11168/11648)
Epoch: 143 | Batch_idx: 100 |  Loss: (0.1242) |  Loss2: (0.0000) | Acc: (95.00%) (12395/12928)
Epoch: 143 | Batch_idx: 110 |  Loss: (0.1238) |  Loss2: (0.0000) | Acc: (95.00%) (13629/14208)
Epoch: 143 | Batch_idx: 120 |  Loss: (0.1245) |  Loss2: (0.0000) | Acc: (95.00%) (14854/15488)
Epoch: 143 | Batch_idx: 130 |  Loss: (0.1250) |  Loss2: (0.0000) | Acc: (95.00%) (16076/16768)
Epoch: 143 | Batch_idx: 140 |  Loss: (0.1241) |  Loss2: (0.0000) | Acc: (95.00%) (17308/18048)
Epoch: 143 | Batch_idx: 150 |  Loss: (0.1250) |  Loss2: (0.0000) | Acc: (95.00%) (18528/19328)
Epoch: 143 | Batch_idx: 160 |  Loss: (0.1260) |  Loss2: (0.0000) | Acc: (95.00%) (19747/20608)
Epoch: 143 | Batch_idx: 170 |  Loss: (0.1257) |  Loss2: (0.0000) | Acc: (95.00%) (20973/21888)
Epoch: 143 | Batch_idx: 180 |  Loss: (0.1259) |  Loss2: (0.0000) | Acc: (95.00%) (22189/23168)
Epoch: 143 | Batch_idx: 190 |  Loss: (0.1255) |  Loss2: (0.0000) | Acc: (95.00%) (23424/24448)
Epoch: 143 | Batch_idx: 200 |  Loss: (0.1258) |  Loss2: (0.0000) | Acc: (95.00%) (24646/25728)
Epoch: 143 | Batch_idx: 210 |  Loss: (0.1257) |  Loss2: (0.0000) | Acc: (95.00%) (25868/27008)
Epoch: 143 | Batch_idx: 220 |  Loss: (0.1259) |  Loss2: (0.0000) | Acc: (95.00%) (27094/28288)
Epoch: 143 | Batch_idx: 230 |  Loss: (0.1261) |  Loss2: (0.0000) | Acc: (95.00%) (28318/29568)
Epoch: 143 | Batch_idx: 240 |  Loss: (0.1270) |  Loss2: (0.0000) | Acc: (95.00%) (29522/30848)
Epoch: 143 | Batch_idx: 250 |  Loss: (0.1278) |  Loss2: (0.0000) | Acc: (95.00%) (30737/32128)
Epoch: 143 | Batch_idx: 260 |  Loss: (0.1282) |  Loss2: (0.0000) | Acc: (95.00%) (31959/33408)
Epoch: 143 | Batch_idx: 270 |  Loss: (0.1282) |  Loss2: (0.0000) | Acc: (95.00%) (33180/34688)
Epoch: 143 | Batch_idx: 280 |  Loss: (0.1284) |  Loss2: (0.0000) | Acc: (95.00%) (34401/35968)
Epoch: 143 | Batch_idx: 290 |  Loss: (0.1288) |  Loss2: (0.0000) | Acc: (95.00%) (35619/37248)
Epoch: 143 | Batch_idx: 300 |  Loss: (0.1283) |  Loss2: (0.0000) | Acc: (95.00%) (36856/38528)
Epoch: 143 | Batch_idx: 310 |  Loss: (0.1290) |  Loss2: (0.0000) | Acc: (95.00%) (38072/39808)
Epoch: 143 | Batch_idx: 320 |  Loss: (0.1295) |  Loss2: (0.0000) | Acc: (95.00%) (39290/41088)
Epoch: 143 | Batch_idx: 330 |  Loss: (0.1300) |  Loss2: (0.0000) | Acc: (95.00%) (40504/42368)
Epoch: 143 | Batch_idx: 340 |  Loss: (0.1305) |  Loss2: (0.0000) | Acc: (95.00%) (41715/43648)
Epoch: 143 | Batch_idx: 350 |  Loss: (0.1309) |  Loss2: (0.0000) | Acc: (95.00%) (42934/44928)
Epoch: 143 | Batch_idx: 360 |  Loss: (0.1307) |  Loss2: (0.0000) | Acc: (95.00%) (44158/46208)
Epoch: 143 | Batch_idx: 370 |  Loss: (0.1316) |  Loss2: (0.0000) | Acc: (95.00%) (45360/47488)
Epoch: 143 | Batch_idx: 380 |  Loss: (0.1312) |  Loss2: (0.0000) | Acc: (95.00%) (46590/48768)
Epoch: 143 | Batch_idx: 390 |  Loss: (0.1315) |  Loss2: (0.0000) | Acc: (95.00%) (47759/50000)
# TEST : Loss: (0.4464) | Acc: (87.00%) (8739/10000)
percent tensor([0.5789, 0.4211], device='cuda:0')
percent tensor([0.5797, 0.4203], device='cuda:0')
percent tensor([0.5614, 0.4386], device='cuda:0')
percent tensor([0.6379, 0.3621], device='cuda:0')
percent tensor([0.5965, 0.4035], device='cuda:0')
percent tensor([0.6606, 0.3394], device='cuda:0')
percent tensor([0.7352, 0.2648], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 144 | Batch_idx: 0 |  Loss: (0.1254) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 144 | Batch_idx: 10 |  Loss: (0.1310) |  Loss2: (0.0000) | Acc: (95.00%) (1347/1408)
Epoch: 144 | Batch_idx: 20 |  Loss: (0.1221) |  Loss2: (0.0000) | Acc: (95.00%) (2579/2688)
Epoch: 144 | Batch_idx: 30 |  Loss: (0.1237) |  Loss2: (0.0000) | Acc: (95.00%) (3805/3968)
Epoch: 144 | Batch_idx: 40 |  Loss: (0.1232) |  Loss2: (0.0000) | Acc: (95.00%) (5034/5248)
Epoch: 144 | Batch_idx: 50 |  Loss: (0.1239) |  Loss2: (0.0000) | Acc: (95.00%) (6251/6528)
Epoch: 144 | Batch_idx: 60 |  Loss: (0.1203) |  Loss2: (0.0000) | Acc: (95.00%) (7483/7808)
Epoch: 144 | Batch_idx: 70 |  Loss: (0.1204) |  Loss2: (0.0000) | Acc: (95.00%) (8706/9088)
Epoch: 144 | Batch_idx: 80 |  Loss: (0.1201) |  Loss2: (0.0000) | Acc: (95.00%) (9941/10368)
Epoch: 144 | Batch_idx: 90 |  Loss: (0.1208) |  Loss2: (0.0000) | Acc: (95.00%) (11161/11648)
Epoch: 144 | Batch_idx: 100 |  Loss: (0.1227) |  Loss2: (0.0000) | Acc: (95.00%) (12373/12928)
Epoch: 144 | Batch_idx: 110 |  Loss: (0.1231) |  Loss2: (0.0000) | Acc: (95.00%) (13599/14208)
Epoch: 144 | Batch_idx: 120 |  Loss: (0.1233) |  Loss2: (0.0000) | Acc: (95.00%) (14829/15488)
Epoch: 144 | Batch_idx: 130 |  Loss: (0.1226) |  Loss2: (0.0000) | Acc: (95.00%) (16058/16768)
Epoch: 144 | Batch_idx: 140 |  Loss: (0.1238) |  Loss2: (0.0000) | Acc: (95.00%) (17275/18048)
Epoch: 144 | Batch_idx: 150 |  Loss: (0.1236) |  Loss2: (0.0000) | Acc: (95.00%) (18499/19328)
Epoch: 144 | Batch_idx: 160 |  Loss: (0.1241) |  Loss2: (0.0000) | Acc: (95.00%) (19714/20608)
Epoch: 144 | Batch_idx: 170 |  Loss: (0.1255) |  Loss2: (0.0000) | Acc: (95.00%) (20918/21888)
Epoch: 144 | Batch_idx: 180 |  Loss: (0.1267) |  Loss2: (0.0000) | Acc: (95.00%) (22137/23168)
Epoch: 144 | Batch_idx: 190 |  Loss: (0.1280) |  Loss2: (0.0000) | Acc: (95.00%) (23348/24448)
Epoch: 144 | Batch_idx: 200 |  Loss: (0.1288) |  Loss2: (0.0000) | Acc: (95.00%) (24567/25728)
Epoch: 144 | Batch_idx: 210 |  Loss: (0.1293) |  Loss2: (0.0000) | Acc: (95.00%) (25791/27008)
Epoch: 144 | Batch_idx: 220 |  Loss: (0.1289) |  Loss2: (0.0000) | Acc: (95.00%) (27018/28288)
Epoch: 144 | Batch_idx: 230 |  Loss: (0.1282) |  Loss2: (0.0000) | Acc: (95.00%) (28253/29568)
Epoch: 144 | Batch_idx: 240 |  Loss: (0.1281) |  Loss2: (0.0000) | Acc: (95.00%) (29482/30848)
Epoch: 144 | Batch_idx: 250 |  Loss: (0.1281) |  Loss2: (0.0000) | Acc: (95.00%) (30696/32128)
Epoch: 144 | Batch_idx: 260 |  Loss: (0.1288) |  Loss2: (0.0000) | Acc: (95.00%) (31910/33408)
Epoch: 144 | Batch_idx: 270 |  Loss: (0.1287) |  Loss2: (0.0000) | Acc: (95.00%) (33134/34688)
Epoch: 144 | Batch_idx: 280 |  Loss: (0.1289) |  Loss2: (0.0000) | Acc: (95.00%) (34357/35968)
Epoch: 144 | Batch_idx: 290 |  Loss: (0.1293) |  Loss2: (0.0000) | Acc: (95.00%) (35578/37248)
Epoch: 144 | Batch_idx: 300 |  Loss: (0.1293) |  Loss2: (0.0000) | Acc: (95.00%) (36795/38528)
Epoch: 144 | Batch_idx: 310 |  Loss: (0.1299) |  Loss2: (0.0000) | Acc: (95.00%) (38011/39808)
Epoch: 144 | Batch_idx: 320 |  Loss: (0.1294) |  Loss2: (0.0000) | Acc: (95.00%) (39233/41088)
Epoch: 144 | Batch_idx: 330 |  Loss: (0.1301) |  Loss2: (0.0000) | Acc: (95.00%) (40457/42368)
Epoch: 144 | Batch_idx: 340 |  Loss: (0.1308) |  Loss2: (0.0000) | Acc: (95.00%) (41660/43648)
Epoch: 144 | Batch_idx: 350 |  Loss: (0.1312) |  Loss2: (0.0000) | Acc: (95.00%) (42874/44928)
Epoch: 144 | Batch_idx: 360 |  Loss: (0.1311) |  Loss2: (0.0000) | Acc: (95.00%) (44108/46208)
Epoch: 144 | Batch_idx: 370 |  Loss: (0.1310) |  Loss2: (0.0000) | Acc: (95.00%) (45325/47488)
Epoch: 144 | Batch_idx: 380 |  Loss: (0.1313) |  Loss2: (0.0000) | Acc: (95.00%) (46552/48768)
Epoch: 144 | Batch_idx: 390 |  Loss: (0.1317) |  Loss2: (0.0000) | Acc: (95.00%) (47718/50000)
# TEST : Loss: (0.4528) | Acc: (87.00%) (8740/10000)
percent tensor([0.5787, 0.4213], device='cuda:0')
percent tensor([0.5803, 0.4197], device='cuda:0')
percent tensor([0.5598, 0.4402], device='cuda:0')
percent tensor([0.6392, 0.3608], device='cuda:0')
percent tensor([0.5971, 0.4029], device='cuda:0')
percent tensor([0.6604, 0.3396], device='cuda:0')
percent tensor([0.7336, 0.2664], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 145 | Batch_idx: 0 |  Loss: (0.0887) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 145 | Batch_idx: 10 |  Loss: (0.1263) |  Loss2: (0.0000) | Acc: (96.00%) (1353/1408)
Epoch: 145 | Batch_idx: 20 |  Loss: (0.1450) |  Loss2: (0.0000) | Acc: (95.00%) (2564/2688)
Epoch: 145 | Batch_idx: 30 |  Loss: (0.1530) |  Loss2: (0.0000) | Acc: (94.00%) (3764/3968)
Epoch: 145 | Batch_idx: 40 |  Loss: (0.1544) |  Loss2: (0.0000) | Acc: (94.00%) (4969/5248)
Epoch: 145 | Batch_idx: 50 |  Loss: (0.1558) |  Loss2: (0.0000) | Acc: (94.00%) (6176/6528)
Epoch: 145 | Batch_idx: 60 |  Loss: (0.1607) |  Loss2: (0.0000) | Acc: (94.00%) (7368/7808)
Epoch: 145 | Batch_idx: 70 |  Loss: (0.1586) |  Loss2: (0.0000) | Acc: (94.00%) (8579/9088)
Epoch: 145 | Batch_idx: 80 |  Loss: (0.1603) |  Loss2: (0.0000) | Acc: (94.00%) (9786/10368)
Epoch: 145 | Batch_idx: 90 |  Loss: (0.1601) |  Loss2: (0.0000) | Acc: (94.00%) (10997/11648)
Epoch: 145 | Batch_idx: 100 |  Loss: (0.1607) |  Loss2: (0.0000) | Acc: (94.00%) (12205/12928)
Epoch: 145 | Batch_idx: 110 |  Loss: (0.1602) |  Loss2: (0.0000) | Acc: (94.00%) (13411/14208)
Epoch: 145 | Batch_idx: 120 |  Loss: (0.1599) |  Loss2: (0.0000) | Acc: (94.00%) (14622/15488)
Epoch: 145 | Batch_idx: 130 |  Loss: (0.1610) |  Loss2: (0.0000) | Acc: (94.00%) (15825/16768)
Epoch: 145 | Batch_idx: 140 |  Loss: (0.1624) |  Loss2: (0.0000) | Acc: (94.00%) (17027/18048)
Epoch: 145 | Batch_idx: 150 |  Loss: (0.1614) |  Loss2: (0.0000) | Acc: (94.00%) (18238/19328)
Epoch: 145 | Batch_idx: 160 |  Loss: (0.1617) |  Loss2: (0.0000) | Acc: (94.00%) (19442/20608)
Epoch: 145 | Batch_idx: 170 |  Loss: (0.1616) |  Loss2: (0.0000) | Acc: (94.00%) (20651/21888)
Epoch: 145 | Batch_idx: 180 |  Loss: (0.1616) |  Loss2: (0.0000) | Acc: (94.00%) (21854/23168)
Epoch: 145 | Batch_idx: 190 |  Loss: (0.1611) |  Loss2: (0.0000) | Acc: (94.00%) (23061/24448)
Epoch: 145 | Batch_idx: 200 |  Loss: (0.1628) |  Loss2: (0.0000) | Acc: (94.00%) (24247/25728)
Epoch: 145 | Batch_idx: 210 |  Loss: (0.1627) |  Loss2: (0.0000) | Acc: (94.00%) (25460/27008)
Epoch: 145 | Batch_idx: 220 |  Loss: (0.1623) |  Loss2: (0.0000) | Acc: (94.00%) (26669/28288)
Epoch: 145 | Batch_idx: 230 |  Loss: (0.1625) |  Loss2: (0.0000) | Acc: (94.00%) (27878/29568)
Epoch: 145 | Batch_idx: 240 |  Loss: (0.1616) |  Loss2: (0.0000) | Acc: (94.00%) (29096/30848)
Epoch: 145 | Batch_idx: 250 |  Loss: (0.1603) |  Loss2: (0.0000) | Acc: (94.00%) (30330/32128)
Epoch: 145 | Batch_idx: 260 |  Loss: (0.1602) |  Loss2: (0.0000) | Acc: (94.00%) (31530/33408)
Epoch: 145 | Batch_idx: 270 |  Loss: (0.1599) |  Loss2: (0.0000) | Acc: (94.00%) (32740/34688)
Epoch: 145 | Batch_idx: 280 |  Loss: (0.1594) |  Loss2: (0.0000) | Acc: (94.00%) (33957/35968)
Epoch: 145 | Batch_idx: 290 |  Loss: (0.1595) |  Loss2: (0.0000) | Acc: (94.00%) (35155/37248)
Epoch: 145 | Batch_idx: 300 |  Loss: (0.1589) |  Loss2: (0.0000) | Acc: (94.00%) (36375/38528)
Epoch: 145 | Batch_idx: 310 |  Loss: (0.1583) |  Loss2: (0.0000) | Acc: (94.00%) (37587/39808)
Epoch: 145 | Batch_idx: 320 |  Loss: (0.1582) |  Loss2: (0.0000) | Acc: (94.00%) (38800/41088)
Epoch: 145 | Batch_idx: 330 |  Loss: (0.1577) |  Loss2: (0.0000) | Acc: (94.00%) (40016/42368)
Epoch: 145 | Batch_idx: 340 |  Loss: (0.1569) |  Loss2: (0.0000) | Acc: (94.00%) (41240/43648)
Epoch: 145 | Batch_idx: 350 |  Loss: (0.1569) |  Loss2: (0.0000) | Acc: (94.00%) (42454/44928)
Epoch: 145 | Batch_idx: 360 |  Loss: (0.1566) |  Loss2: (0.0000) | Acc: (94.00%) (43665/46208)
Epoch: 145 | Batch_idx: 370 |  Loss: (0.1557) |  Loss2: (0.0000) | Acc: (94.00%) (44890/47488)
Epoch: 145 | Batch_idx: 380 |  Loss: (0.1560) |  Loss2: (0.0000) | Acc: (94.00%) (46103/48768)
Epoch: 145 | Batch_idx: 390 |  Loss: (0.1562) |  Loss2: (0.0000) | Acc: (94.00%) (47265/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_145.pth.tar'
# TEST : Loss: (0.4289) | Acc: (87.00%) (8756/10000)
percent tensor([0.5660, 0.4340], device='cuda:0')
percent tensor([0.5778, 0.4222], device='cuda:0')
percent tensor([0.5653, 0.4347], device='cuda:0')
percent tensor([0.6571, 0.3429], device='cuda:0')
percent tensor([0.5992, 0.4008], device='cuda:0')
percent tensor([0.6719, 0.3281], device='cuda:0')
percent tensor([0.7433, 0.2567], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 146 | Batch_idx: 0 |  Loss: (0.1252) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 146 | Batch_idx: 10 |  Loss: (0.1528) |  Loss2: (0.0000) | Acc: (94.00%) (1327/1408)
Epoch: 146 | Batch_idx: 20 |  Loss: (0.1546) |  Loss2: (0.0000) | Acc: (94.00%) (2529/2688)
Epoch: 146 | Batch_idx: 30 |  Loss: (0.1526) |  Loss2: (0.0000) | Acc: (94.00%) (3749/3968)
Epoch: 146 | Batch_idx: 40 |  Loss: (0.1500) |  Loss2: (0.0000) | Acc: (94.00%) (4960/5248)
Epoch: 146 | Batch_idx: 50 |  Loss: (0.1518) |  Loss2: (0.0000) | Acc: (94.00%) (6170/6528)
Epoch: 146 | Batch_idx: 60 |  Loss: (0.1508) |  Loss2: (0.0000) | Acc: (94.00%) (7383/7808)
Epoch: 146 | Batch_idx: 70 |  Loss: (0.1489) |  Loss2: (0.0000) | Acc: (94.00%) (8603/9088)
Epoch: 146 | Batch_idx: 80 |  Loss: (0.1493) |  Loss2: (0.0000) | Acc: (94.00%) (9817/10368)
Epoch: 146 | Batch_idx: 90 |  Loss: (0.1478) |  Loss2: (0.0000) | Acc: (94.00%) (11043/11648)
Epoch: 146 | Batch_idx: 100 |  Loss: (0.1493) |  Loss2: (0.0000) | Acc: (94.00%) (12251/12928)
Epoch: 146 | Batch_idx: 110 |  Loss: (0.1490) |  Loss2: (0.0000) | Acc: (94.00%) (13469/14208)
Epoch: 146 | Batch_idx: 120 |  Loss: (0.1493) |  Loss2: (0.0000) | Acc: (94.00%) (14688/15488)
Epoch: 146 | Batch_idx: 130 |  Loss: (0.1501) |  Loss2: (0.0000) | Acc: (94.00%) (15902/16768)
Epoch: 146 | Batch_idx: 140 |  Loss: (0.1496) |  Loss2: (0.0000) | Acc: (94.00%) (17113/18048)
Epoch: 146 | Batch_idx: 150 |  Loss: (0.1480) |  Loss2: (0.0000) | Acc: (94.00%) (18342/19328)
Epoch: 146 | Batch_idx: 160 |  Loss: (0.1472) |  Loss2: (0.0000) | Acc: (94.00%) (19560/20608)
Epoch: 146 | Batch_idx: 170 |  Loss: (0.1474) |  Loss2: (0.0000) | Acc: (94.00%) (20773/21888)
Epoch: 146 | Batch_idx: 180 |  Loss: (0.1481) |  Loss2: (0.0000) | Acc: (94.00%) (21989/23168)
Epoch: 146 | Batch_idx: 190 |  Loss: (0.1478) |  Loss2: (0.0000) | Acc: (94.00%) (23202/24448)
Epoch: 146 | Batch_idx: 200 |  Loss: (0.1475) |  Loss2: (0.0000) | Acc: (94.00%) (24420/25728)
Epoch: 146 | Batch_idx: 210 |  Loss: (0.1472) |  Loss2: (0.0000) | Acc: (94.00%) (25636/27008)
Epoch: 146 | Batch_idx: 220 |  Loss: (0.1468) |  Loss2: (0.0000) | Acc: (94.00%) (26853/28288)
Epoch: 146 | Batch_idx: 230 |  Loss: (0.1468) |  Loss2: (0.0000) | Acc: (94.00%) (28064/29568)
Epoch: 146 | Batch_idx: 240 |  Loss: (0.1462) |  Loss2: (0.0000) | Acc: (94.00%) (29280/30848)
Epoch: 146 | Batch_idx: 250 |  Loss: (0.1458) |  Loss2: (0.0000) | Acc: (94.00%) (30500/32128)
Epoch: 146 | Batch_idx: 260 |  Loss: (0.1450) |  Loss2: (0.0000) | Acc: (94.00%) (31726/33408)
Epoch: 146 | Batch_idx: 270 |  Loss: (0.1446) |  Loss2: (0.0000) | Acc: (94.00%) (32946/34688)
Epoch: 146 | Batch_idx: 280 |  Loss: (0.1449) |  Loss2: (0.0000) | Acc: (94.00%) (34162/35968)
Epoch: 146 | Batch_idx: 290 |  Loss: (0.1443) |  Loss2: (0.0000) | Acc: (94.00%) (35383/37248)
Epoch: 146 | Batch_idx: 300 |  Loss: (0.1448) |  Loss2: (0.0000) | Acc: (94.00%) (36589/38528)
Epoch: 146 | Batch_idx: 310 |  Loss: (0.1440) |  Loss2: (0.0000) | Acc: (94.00%) (37816/39808)
Epoch: 146 | Batch_idx: 320 |  Loss: (0.1443) |  Loss2: (0.0000) | Acc: (94.00%) (39026/41088)
Epoch: 146 | Batch_idx: 330 |  Loss: (0.1439) |  Loss2: (0.0000) | Acc: (95.00%) (40258/42368)
Epoch: 146 | Batch_idx: 340 |  Loss: (0.1435) |  Loss2: (0.0000) | Acc: (95.00%) (41483/43648)
Epoch: 146 | Batch_idx: 350 |  Loss: (0.1435) |  Loss2: (0.0000) | Acc: (95.00%) (42698/44928)
Epoch: 146 | Batch_idx: 360 |  Loss: (0.1439) |  Loss2: (0.0000) | Acc: (95.00%) (43903/46208)
Epoch: 146 | Batch_idx: 370 |  Loss: (0.1436) |  Loss2: (0.0000) | Acc: (95.00%) (45121/47488)
Epoch: 146 | Batch_idx: 380 |  Loss: (0.1434) |  Loss2: (0.0000) | Acc: (95.00%) (46341/48768)
Epoch: 146 | Batch_idx: 390 |  Loss: (0.1432) |  Loss2: (0.0000) | Acc: (95.00%) (47514/50000)
# TEST : Loss: (0.4182) | Acc: (87.00%) (8788/10000)
percent tensor([0.5715, 0.4285], device='cuda:0')
percent tensor([0.5769, 0.4231], device='cuda:0')
percent tensor([0.5664, 0.4336], device='cuda:0')
percent tensor([0.6581, 0.3419], device='cuda:0')
percent tensor([0.5974, 0.4026], device='cuda:0')
percent tensor([0.6693, 0.3307], device='cuda:0')
percent tensor([0.7550, 0.2450], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 147 | Batch_idx: 0 |  Loss: (0.1317) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 147 | Batch_idx: 10 |  Loss: (0.1390) |  Loss2: (0.0000) | Acc: (94.00%) (1337/1408)
Epoch: 147 | Batch_idx: 20 |  Loss: (0.1354) |  Loss2: (0.0000) | Acc: (95.00%) (2557/2688)
Epoch: 147 | Batch_idx: 30 |  Loss: (0.1359) |  Loss2: (0.0000) | Acc: (95.00%) (3775/3968)
Epoch: 147 | Batch_idx: 40 |  Loss: (0.1394) |  Loss2: (0.0000) | Acc: (95.00%) (4993/5248)
Epoch: 147 | Batch_idx: 50 |  Loss: (0.1419) |  Loss2: (0.0000) | Acc: (95.00%) (6205/6528)
Epoch: 147 | Batch_idx: 60 |  Loss: (0.1419) |  Loss2: (0.0000) | Acc: (95.00%) (7426/7808)
Epoch: 147 | Batch_idx: 70 |  Loss: (0.1437) |  Loss2: (0.0000) | Acc: (95.00%) (8638/9088)
Epoch: 147 | Batch_idx: 80 |  Loss: (0.1447) |  Loss2: (0.0000) | Acc: (95.00%) (9850/10368)
Epoch: 147 | Batch_idx: 90 |  Loss: (0.1432) |  Loss2: (0.0000) | Acc: (95.00%) (11083/11648)
Epoch: 147 | Batch_idx: 100 |  Loss: (0.1430) |  Loss2: (0.0000) | Acc: (95.00%) (12300/12928)
Epoch: 147 | Batch_idx: 110 |  Loss: (0.1432) |  Loss2: (0.0000) | Acc: (95.00%) (13516/14208)
Epoch: 147 | Batch_idx: 120 |  Loss: (0.1414) |  Loss2: (0.0000) | Acc: (95.00%) (14751/15488)
Epoch: 147 | Batch_idx: 130 |  Loss: (0.1414) |  Loss2: (0.0000) | Acc: (95.00%) (15961/16768)
Epoch: 147 | Batch_idx: 140 |  Loss: (0.1422) |  Loss2: (0.0000) | Acc: (95.00%) (17171/18048)
Epoch: 147 | Batch_idx: 150 |  Loss: (0.1406) |  Loss2: (0.0000) | Acc: (95.00%) (18395/19328)
Epoch: 147 | Batch_idx: 160 |  Loss: (0.1387) |  Loss2: (0.0000) | Acc: (95.00%) (19621/20608)
Epoch: 147 | Batch_idx: 170 |  Loss: (0.1384) |  Loss2: (0.0000) | Acc: (95.00%) (20848/21888)
Epoch: 147 | Batch_idx: 180 |  Loss: (0.1380) |  Loss2: (0.0000) | Acc: (95.00%) (22072/23168)
Epoch: 147 | Batch_idx: 190 |  Loss: (0.1368) |  Loss2: (0.0000) | Acc: (95.00%) (23305/24448)
Epoch: 147 | Batch_idx: 200 |  Loss: (0.1362) |  Loss2: (0.0000) | Acc: (95.00%) (24533/25728)
Epoch: 147 | Batch_idx: 210 |  Loss: (0.1362) |  Loss2: (0.0000) | Acc: (95.00%) (25758/27008)
Epoch: 147 | Batch_idx: 220 |  Loss: (0.1365) |  Loss2: (0.0000) | Acc: (95.00%) (26981/28288)
Epoch: 147 | Batch_idx: 230 |  Loss: (0.1358) |  Loss2: (0.0000) | Acc: (95.00%) (28206/29568)
Epoch: 147 | Batch_idx: 240 |  Loss: (0.1364) |  Loss2: (0.0000) | Acc: (95.00%) (29421/30848)
Epoch: 147 | Batch_idx: 250 |  Loss: (0.1370) |  Loss2: (0.0000) | Acc: (95.00%) (30632/32128)
Epoch: 147 | Batch_idx: 260 |  Loss: (0.1364) |  Loss2: (0.0000) | Acc: (95.00%) (31862/33408)
Epoch: 147 | Batch_idx: 270 |  Loss: (0.1364) |  Loss2: (0.0000) | Acc: (95.00%) (33077/34688)
Epoch: 147 | Batch_idx: 280 |  Loss: (0.1358) |  Loss2: (0.0000) | Acc: (95.00%) (34302/35968)
Epoch: 147 | Batch_idx: 290 |  Loss: (0.1357) |  Loss2: (0.0000) | Acc: (95.00%) (35521/37248)
Epoch: 147 | Batch_idx: 300 |  Loss: (0.1354) |  Loss2: (0.0000) | Acc: (95.00%) (36746/38528)
Epoch: 147 | Batch_idx: 310 |  Loss: (0.1345) |  Loss2: (0.0000) | Acc: (95.00%) (37987/39808)
Epoch: 147 | Batch_idx: 320 |  Loss: (0.1352) |  Loss2: (0.0000) | Acc: (95.00%) (39199/41088)
Epoch: 147 | Batch_idx: 330 |  Loss: (0.1352) |  Loss2: (0.0000) | Acc: (95.00%) (40423/42368)
Epoch: 147 | Batch_idx: 340 |  Loss: (0.1349) |  Loss2: (0.0000) | Acc: (95.00%) (41649/43648)
Epoch: 147 | Batch_idx: 350 |  Loss: (0.1351) |  Loss2: (0.0000) | Acc: (95.00%) (42871/44928)
Epoch: 147 | Batch_idx: 360 |  Loss: (0.1352) |  Loss2: (0.0000) | Acc: (95.00%) (44090/46208)
Epoch: 147 | Batch_idx: 370 |  Loss: (0.1351) |  Loss2: (0.0000) | Acc: (95.00%) (45310/47488)
Epoch: 147 | Batch_idx: 380 |  Loss: (0.1350) |  Loss2: (0.0000) | Acc: (95.00%) (46526/48768)
Epoch: 147 | Batch_idx: 390 |  Loss: (0.1347) |  Loss2: (0.0000) | Acc: (95.00%) (47710/50000)
# TEST : Loss: (0.4107) | Acc: (88.00%) (8803/10000)
percent tensor([0.5718, 0.4282], device='cuda:0')
percent tensor([0.5757, 0.4243], device='cuda:0')
percent tensor([0.5630, 0.4370], device='cuda:0')
percent tensor([0.6594, 0.3406], device='cuda:0')
percent tensor([0.5963, 0.4037], device='cuda:0')
percent tensor([0.6727, 0.3273], device='cuda:0')
percent tensor([0.7570, 0.2430], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 148 | Batch_idx: 0 |  Loss: (0.1712) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 148 | Batch_idx: 10 |  Loss: (0.1217) |  Loss2: (0.0000) | Acc: (95.00%) (1351/1408)
Epoch: 148 | Batch_idx: 20 |  Loss: (0.1297) |  Loss2: (0.0000) | Acc: (95.00%) (2563/2688)
Epoch: 148 | Batch_idx: 30 |  Loss: (0.1301) |  Loss2: (0.0000) | Acc: (95.00%) (3778/3968)
Epoch: 148 | Batch_idx: 40 |  Loss: (0.1330) |  Loss2: (0.0000) | Acc: (95.00%) (4995/5248)
Epoch: 148 | Batch_idx: 50 |  Loss: (0.1352) |  Loss2: (0.0000) | Acc: (95.00%) (6209/6528)
Epoch: 148 | Batch_idx: 60 |  Loss: (0.1314) |  Loss2: (0.0000) | Acc: (95.00%) (7440/7808)
Epoch: 148 | Batch_idx: 70 |  Loss: (0.1317) |  Loss2: (0.0000) | Acc: (95.00%) (8665/9088)
Epoch: 148 | Batch_idx: 80 |  Loss: (0.1309) |  Loss2: (0.0000) | Acc: (95.00%) (9893/10368)
Epoch: 148 | Batch_idx: 90 |  Loss: (0.1309) |  Loss2: (0.0000) | Acc: (95.00%) (11118/11648)
Epoch: 148 | Batch_idx: 100 |  Loss: (0.1312) |  Loss2: (0.0000) | Acc: (95.00%) (12338/12928)
Epoch: 148 | Batch_idx: 110 |  Loss: (0.1302) |  Loss2: (0.0000) | Acc: (95.00%) (13565/14208)
Epoch: 148 | Batch_idx: 120 |  Loss: (0.1292) |  Loss2: (0.0000) | Acc: (95.00%) (14794/15488)
Epoch: 148 | Batch_idx: 130 |  Loss: (0.1290) |  Loss2: (0.0000) | Acc: (95.00%) (16018/16768)
Epoch: 148 | Batch_idx: 140 |  Loss: (0.1295) |  Loss2: (0.0000) | Acc: (95.00%) (17236/18048)
Epoch: 148 | Batch_idx: 150 |  Loss: (0.1307) |  Loss2: (0.0000) | Acc: (95.00%) (18451/19328)
Epoch: 148 | Batch_idx: 160 |  Loss: (0.1298) |  Loss2: (0.0000) | Acc: (95.00%) (19681/20608)
Epoch: 148 | Batch_idx: 170 |  Loss: (0.1301) |  Loss2: (0.0000) | Acc: (95.00%) (20900/21888)
Epoch: 148 | Batch_idx: 180 |  Loss: (0.1299) |  Loss2: (0.0000) | Acc: (95.00%) (22129/23168)
Epoch: 148 | Batch_idx: 190 |  Loss: (0.1298) |  Loss2: (0.0000) | Acc: (95.00%) (23350/24448)
Epoch: 148 | Batch_idx: 200 |  Loss: (0.1294) |  Loss2: (0.0000) | Acc: (95.00%) (24582/25728)
Epoch: 148 | Batch_idx: 210 |  Loss: (0.1280) |  Loss2: (0.0000) | Acc: (95.00%) (25821/27008)
Epoch: 148 | Batch_idx: 220 |  Loss: (0.1286) |  Loss2: (0.0000) | Acc: (95.00%) (27035/28288)
Epoch: 148 | Batch_idx: 230 |  Loss: (0.1288) |  Loss2: (0.0000) | Acc: (95.00%) (28255/29568)
Epoch: 148 | Batch_idx: 240 |  Loss: (0.1280) |  Loss2: (0.0000) | Acc: (95.00%) (29484/30848)
Epoch: 148 | Batch_idx: 250 |  Loss: (0.1282) |  Loss2: (0.0000) | Acc: (95.00%) (30700/32128)
Epoch: 148 | Batch_idx: 260 |  Loss: (0.1284) |  Loss2: (0.0000) | Acc: (95.00%) (31921/33408)
Epoch: 148 | Batch_idx: 270 |  Loss: (0.1285) |  Loss2: (0.0000) | Acc: (95.00%) (33145/34688)
Epoch: 148 | Batch_idx: 280 |  Loss: (0.1285) |  Loss2: (0.0000) | Acc: (95.00%) (34367/35968)
Epoch: 148 | Batch_idx: 290 |  Loss: (0.1284) |  Loss2: (0.0000) | Acc: (95.00%) (35591/37248)
Epoch: 148 | Batch_idx: 300 |  Loss: (0.1288) |  Loss2: (0.0000) | Acc: (95.00%) (36813/38528)
Epoch: 148 | Batch_idx: 310 |  Loss: (0.1292) |  Loss2: (0.0000) | Acc: (95.00%) (38027/39808)
Epoch: 148 | Batch_idx: 320 |  Loss: (0.1288) |  Loss2: (0.0000) | Acc: (95.00%) (39259/41088)
Epoch: 148 | Batch_idx: 330 |  Loss: (0.1290) |  Loss2: (0.0000) | Acc: (95.00%) (40481/42368)
Epoch: 148 | Batch_idx: 340 |  Loss: (0.1288) |  Loss2: (0.0000) | Acc: (95.00%) (41706/43648)
Epoch: 148 | Batch_idx: 350 |  Loss: (0.1288) |  Loss2: (0.0000) | Acc: (95.00%) (42935/44928)
Epoch: 148 | Batch_idx: 360 |  Loss: (0.1286) |  Loss2: (0.0000) | Acc: (95.00%) (44162/46208)
Epoch: 148 | Batch_idx: 370 |  Loss: (0.1285) |  Loss2: (0.0000) | Acc: (95.00%) (45386/47488)
Epoch: 148 | Batch_idx: 380 |  Loss: (0.1290) |  Loss2: (0.0000) | Acc: (95.00%) (46605/48768)
Epoch: 148 | Batch_idx: 390 |  Loss: (0.1288) |  Loss2: (0.0000) | Acc: (95.00%) (47787/50000)
# TEST : Loss: (0.4067) | Acc: (88.00%) (8829/10000)
percent tensor([0.5719, 0.4281], device='cuda:0')
percent tensor([0.5771, 0.4229], device='cuda:0')
percent tensor([0.5640, 0.4360], device='cuda:0')
percent tensor([0.6582, 0.3418], device='cuda:0')
percent tensor([0.5976, 0.4024], device='cuda:0')
percent tensor([0.6708, 0.3292], device='cuda:0')
percent tensor([0.7593, 0.2407], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 149 | Batch_idx: 0 |  Loss: (0.1387) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 149 | Batch_idx: 10 |  Loss: (0.1322) |  Loss2: (0.0000) | Acc: (95.00%) (1338/1408)
Epoch: 149 | Batch_idx: 20 |  Loss: (0.1261) |  Loss2: (0.0000) | Acc: (95.00%) (2563/2688)
Epoch: 149 | Batch_idx: 30 |  Loss: (0.1280) |  Loss2: (0.0000) | Acc: (95.00%) (3786/3968)
Epoch: 149 | Batch_idx: 40 |  Loss: (0.1263) |  Loss2: (0.0000) | Acc: (95.00%) (5014/5248)
Epoch: 149 | Batch_idx: 50 |  Loss: (0.1260) |  Loss2: (0.0000) | Acc: (95.00%) (6233/6528)
Epoch: 149 | Batch_idx: 60 |  Loss: (0.1249) |  Loss2: (0.0000) | Acc: (95.00%) (7463/7808)
Epoch: 149 | Batch_idx: 70 |  Loss: (0.1239) |  Loss2: (0.0000) | Acc: (95.00%) (8692/9088)
Epoch: 149 | Batch_idx: 80 |  Loss: (0.1242) |  Loss2: (0.0000) | Acc: (95.00%) (9923/10368)
Epoch: 149 | Batch_idx: 90 |  Loss: (0.1258) |  Loss2: (0.0000) | Acc: (95.00%) (11137/11648)
Epoch: 149 | Batch_idx: 100 |  Loss: (0.1250) |  Loss2: (0.0000) | Acc: (95.00%) (12375/12928)
Epoch: 149 | Batch_idx: 110 |  Loss: (0.1243) |  Loss2: (0.0000) | Acc: (95.00%) (13599/14208)
Epoch: 149 | Batch_idx: 120 |  Loss: (0.1243) |  Loss2: (0.0000) | Acc: (95.00%) (14827/15488)
Epoch: 149 | Batch_idx: 130 |  Loss: (0.1255) |  Loss2: (0.0000) | Acc: (95.00%) (16043/16768)
Epoch: 149 | Batch_idx: 140 |  Loss: (0.1254) |  Loss2: (0.0000) | Acc: (95.00%) (17266/18048)
Epoch: 149 | Batch_idx: 150 |  Loss: (0.1262) |  Loss2: (0.0000) | Acc: (95.00%) (18484/19328)
Epoch: 149 | Batch_idx: 160 |  Loss: (0.1265) |  Loss2: (0.0000) | Acc: (95.00%) (19703/20608)
Epoch: 149 | Batch_idx: 170 |  Loss: (0.1267) |  Loss2: (0.0000) | Acc: (95.00%) (20927/21888)
Epoch: 149 | Batch_idx: 180 |  Loss: (0.1270) |  Loss2: (0.0000) | Acc: (95.00%) (22153/23168)
Epoch: 149 | Batch_idx: 190 |  Loss: (0.1281) |  Loss2: (0.0000) | Acc: (95.00%) (23361/24448)
Epoch: 149 | Batch_idx: 200 |  Loss: (0.1279) |  Loss2: (0.0000) | Acc: (95.00%) (24577/25728)
Epoch: 149 | Batch_idx: 210 |  Loss: (0.1272) |  Loss2: (0.0000) | Acc: (95.00%) (25808/27008)
Epoch: 149 | Batch_idx: 220 |  Loss: (0.1259) |  Loss2: (0.0000) | Acc: (95.00%) (27044/28288)
Epoch: 149 | Batch_idx: 230 |  Loss: (0.1248) |  Loss2: (0.0000) | Acc: (95.00%) (28282/29568)
Epoch: 149 | Batch_idx: 240 |  Loss: (0.1250) |  Loss2: (0.0000) | Acc: (95.00%) (29502/30848)
Epoch: 149 | Batch_idx: 250 |  Loss: (0.1248) |  Loss2: (0.0000) | Acc: (95.00%) (30729/32128)
Epoch: 149 | Batch_idx: 260 |  Loss: (0.1238) |  Loss2: (0.0000) | Acc: (95.00%) (31970/33408)
Epoch: 149 | Batch_idx: 270 |  Loss: (0.1242) |  Loss2: (0.0000) | Acc: (95.00%) (33199/34688)
Epoch: 149 | Batch_idx: 280 |  Loss: (0.1240) |  Loss2: (0.0000) | Acc: (95.00%) (34426/35968)
Epoch: 149 | Batch_idx: 290 |  Loss: (0.1241) |  Loss2: (0.0000) | Acc: (95.00%) (35651/37248)
Epoch: 149 | Batch_idx: 300 |  Loss: (0.1243) |  Loss2: (0.0000) | Acc: (95.00%) (36866/38528)
Epoch: 149 | Batch_idx: 310 |  Loss: (0.1241) |  Loss2: (0.0000) | Acc: (95.00%) (38096/39808)
Epoch: 149 | Batch_idx: 320 |  Loss: (0.1240) |  Loss2: (0.0000) | Acc: (95.00%) (39322/41088)
Epoch: 149 | Batch_idx: 330 |  Loss: (0.1246) |  Loss2: (0.0000) | Acc: (95.00%) (40537/42368)
Epoch: 149 | Batch_idx: 340 |  Loss: (0.1251) |  Loss2: (0.0000) | Acc: (95.00%) (41751/43648)
Epoch: 149 | Batch_idx: 350 |  Loss: (0.1254) |  Loss2: (0.0000) | Acc: (95.00%) (42973/44928)
Epoch: 149 | Batch_idx: 360 |  Loss: (0.1256) |  Loss2: (0.0000) | Acc: (95.00%) (44200/46208)
Epoch: 149 | Batch_idx: 370 |  Loss: (0.1254) |  Loss2: (0.0000) | Acc: (95.00%) (45429/47488)
Epoch: 149 | Batch_idx: 380 |  Loss: (0.1248) |  Loss2: (0.0000) | Acc: (95.00%) (46658/48768)
Epoch: 149 | Batch_idx: 390 |  Loss: (0.1244) |  Loss2: (0.0000) | Acc: (95.00%) (47850/50000)
# TEST : Loss: (0.4054) | Acc: (88.00%) (8829/10000)
percent tensor([0.5714, 0.4286], device='cuda:0')
percent tensor([0.5764, 0.4236], device='cuda:0')
percent tensor([0.5673, 0.4327], device='cuda:0')
percent tensor([0.6560, 0.3440], device='cuda:0')
percent tensor([0.5983, 0.4017], device='cuda:0')
percent tensor([0.6620, 0.3380], device='cuda:0')
percent tensor([0.7583, 0.2417], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 150 | Batch_idx: 0 |  Loss: (0.1709) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 150 | Batch_idx: 10 |  Loss: (0.1111) |  Loss2: (0.0000) | Acc: (96.00%) (1355/1408)
Epoch: 150 | Batch_idx: 20 |  Loss: (0.1145) |  Loss2: (0.0000) | Acc: (95.00%) (2577/2688)
Epoch: 150 | Batch_idx: 30 |  Loss: (0.1190) |  Loss2: (0.0000) | Acc: (95.00%) (3806/3968)
Epoch: 150 | Batch_idx: 40 |  Loss: (0.1206) |  Loss2: (0.0000) | Acc: (95.00%) (5032/5248)
Epoch: 150 | Batch_idx: 50 |  Loss: (0.1201) |  Loss2: (0.0000) | Acc: (95.00%) (6263/6528)
Epoch: 150 | Batch_idx: 60 |  Loss: (0.1213) |  Loss2: (0.0000) | Acc: (95.00%) (7493/7808)
Epoch: 150 | Batch_idx: 70 |  Loss: (0.1224) |  Loss2: (0.0000) | Acc: (95.00%) (8714/9088)
Epoch: 150 | Batch_idx: 80 |  Loss: (0.1231) |  Loss2: (0.0000) | Acc: (95.00%) (9932/10368)
Epoch: 150 | Batch_idx: 90 |  Loss: (0.1234) |  Loss2: (0.0000) | Acc: (95.00%) (11159/11648)
Epoch: 150 | Batch_idx: 100 |  Loss: (0.1232) |  Loss2: (0.0000) | Acc: (95.00%) (12386/12928)
Epoch: 150 | Batch_idx: 110 |  Loss: (0.1251) |  Loss2: (0.0000) | Acc: (95.00%) (13609/14208)
Epoch: 150 | Batch_idx: 120 |  Loss: (0.1263) |  Loss2: (0.0000) | Acc: (95.00%) (14828/15488)
Epoch: 150 | Batch_idx: 130 |  Loss: (0.1255) |  Loss2: (0.0000) | Acc: (95.00%) (16054/16768)
Epoch: 150 | Batch_idx: 140 |  Loss: (0.1259) |  Loss2: (0.0000) | Acc: (95.00%) (17273/18048)
Epoch: 150 | Batch_idx: 150 |  Loss: (0.1278) |  Loss2: (0.0000) | Acc: (95.00%) (18490/19328)
Epoch: 150 | Batch_idx: 160 |  Loss: (0.1291) |  Loss2: (0.0000) | Acc: (95.00%) (19703/20608)
Epoch: 150 | Batch_idx: 170 |  Loss: (0.1297) |  Loss2: (0.0000) | Acc: (95.00%) (20920/21888)
Epoch: 150 | Batch_idx: 180 |  Loss: (0.1296) |  Loss2: (0.0000) | Acc: (95.00%) (22139/23168)
Epoch: 150 | Batch_idx: 190 |  Loss: (0.1306) |  Loss2: (0.0000) | Acc: (95.00%) (23355/24448)
Epoch: 150 | Batch_idx: 200 |  Loss: (0.1314) |  Loss2: (0.0000) | Acc: (95.00%) (24580/25728)
Epoch: 150 | Batch_idx: 210 |  Loss: (0.1310) |  Loss2: (0.0000) | Acc: (95.00%) (25806/27008)
Epoch: 150 | Batch_idx: 220 |  Loss: (0.1310) |  Loss2: (0.0000) | Acc: (95.00%) (27026/28288)
Epoch: 150 | Batch_idx: 230 |  Loss: (0.1310) |  Loss2: (0.0000) | Acc: (95.00%) (28247/29568)
Epoch: 150 | Batch_idx: 240 |  Loss: (0.1313) |  Loss2: (0.0000) | Acc: (95.00%) (29465/30848)
Epoch: 150 | Batch_idx: 250 |  Loss: (0.1318) |  Loss2: (0.0000) | Acc: (95.00%) (30681/32128)
Epoch: 150 | Batch_idx: 260 |  Loss: (0.1313) |  Loss2: (0.0000) | Acc: (95.00%) (31908/33408)
Epoch: 150 | Batch_idx: 270 |  Loss: (0.1310) |  Loss2: (0.0000) | Acc: (95.00%) (33131/34688)
Epoch: 150 | Batch_idx: 280 |  Loss: (0.1314) |  Loss2: (0.0000) | Acc: (95.00%) (34346/35968)
Epoch: 150 | Batch_idx: 290 |  Loss: (0.1314) |  Loss2: (0.0000) | Acc: (95.00%) (35572/37248)
Epoch: 150 | Batch_idx: 300 |  Loss: (0.1310) |  Loss2: (0.0000) | Acc: (95.00%) (36794/38528)
Epoch: 150 | Batch_idx: 310 |  Loss: (0.1309) |  Loss2: (0.0000) | Acc: (95.00%) (38017/39808)
Epoch: 150 | Batch_idx: 320 |  Loss: (0.1313) |  Loss2: (0.0000) | Acc: (95.00%) (39232/41088)
Epoch: 150 | Batch_idx: 330 |  Loss: (0.1319) |  Loss2: (0.0000) | Acc: (95.00%) (40451/42368)
Epoch: 150 | Batch_idx: 340 |  Loss: (0.1332) |  Loss2: (0.0000) | Acc: (95.00%) (41656/43648)
Epoch: 150 | Batch_idx: 350 |  Loss: (0.1330) |  Loss2: (0.0000) | Acc: (95.00%) (42878/44928)
Epoch: 150 | Batch_idx: 360 |  Loss: (0.1326) |  Loss2: (0.0000) | Acc: (95.00%) (44105/46208)
Epoch: 150 | Batch_idx: 370 |  Loss: (0.1335) |  Loss2: (0.0000) | Acc: (95.00%) (45323/47488)
Epoch: 150 | Batch_idx: 380 |  Loss: (0.1334) |  Loss2: (0.0000) | Acc: (95.00%) (46547/48768)
Epoch: 150 | Batch_idx: 390 |  Loss: (0.1333) |  Loss2: (0.0000) | Acc: (95.00%) (47721/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_150.pth.tar'
# TEST : Loss: (0.4636) | Acc: (87.00%) (8723/10000)
percent tensor([0.5724, 0.4276], device='cuda:0')
percent tensor([0.5768, 0.4232], device='cuda:0')
percent tensor([0.5679, 0.4321], device='cuda:0')
percent tensor([0.6557, 0.3443], device='cuda:0')
percent tensor([0.5979, 0.4021], device='cuda:0')
percent tensor([0.6618, 0.3382], device='cuda:0')
percent tensor([0.7645, 0.2355], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(181.8157, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(823.4608, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(823.8359, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1520.4581, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(485.3317, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2279.1204, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4266.6323, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1369.4167, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6217.1787, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11700.4521, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3856.3081, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16307.9277, device='cuda:0', grad_fn=<NormBackward0>)
