Files already downloaded and verified
USE 1 GPUs!
Epoch: 0 | Batch_idx: 0 |  Loss: (2.3180) |  Loss2: (0.0000) | Acc: (8.00%) (11/128)
Epoch: 0 | Batch_idx: 10 |  Loss: (2.3123) |  Loss2: (0.0000) | Acc: (8.00%) (116/1408)
Epoch: 0 | Batch_idx: 20 |  Loss: (2.3021) |  Loss2: (0.0000) | Acc: (9.00%) (257/2688)
Epoch: 0 | Batch_idx: 30 |  Loss: (2.2945) |  Loss2: (0.0000) | Acc: (11.00%) (449/3968)
Epoch: 0 | Batch_idx: 40 |  Loss: (2.2871) |  Loss2: (0.0000) | Acc: (13.00%) (694/5248)
Epoch: 0 | Batch_idx: 50 |  Loss: (2.2803) |  Loss2: (0.0000) | Acc: (14.00%) (960/6528)
Epoch: 0 | Batch_idx: 60 |  Loss: (2.2729) |  Loss2: (0.0000) | Acc: (15.00%) (1243/7808)
Epoch: 0 | Batch_idx: 70 |  Loss: (2.2647) |  Loss2: (0.0000) | Acc: (16.00%) (1544/9088)
Epoch: 0 | Batch_idx: 80 |  Loss: (2.2567) |  Loss2: (0.0000) | Acc: (17.00%) (1833/10368)
Epoch: 0 | Batch_idx: 90 |  Loss: (2.2497) |  Loss2: (0.0000) | Acc: (18.00%) (2114/11648)
Epoch: 0 | Batch_idx: 100 |  Loss: (2.2407) |  Loss2: (0.0000) | Acc: (18.00%) (2410/12928)
Epoch: 0 | Batch_idx: 110 |  Loss: (2.2310) |  Loss2: (0.0000) | Acc: (19.00%) (2719/14208)
Epoch: 0 | Batch_idx: 120 |  Loss: (2.2216) |  Loss2: (0.0000) | Acc: (19.00%) (3020/15488)
Epoch: 0 | Batch_idx: 130 |  Loss: (2.2134) |  Loss2: (0.0000) | Acc: (19.00%) (3314/16768)
Epoch: 0 | Batch_idx: 140 |  Loss: (2.2060) |  Loss2: (0.0000) | Acc: (19.00%) (3606/18048)
Epoch: 0 | Batch_idx: 150 |  Loss: (2.1973) |  Loss2: (0.0000) | Acc: (20.00%) (3936/19328)
Epoch: 0 | Batch_idx: 160 |  Loss: (2.1891) |  Loss2: (0.0000) | Acc: (20.00%) (4238/20608)
Epoch: 0 | Batch_idx: 170 |  Loss: (2.1815) |  Loss2: (0.0000) | Acc: (20.00%) (4571/21888)
Epoch: 0 | Batch_idx: 180 |  Loss: (2.1745) |  Loss2: (0.0000) | Acc: (21.00%) (4919/23168)
Epoch: 0 | Batch_idx: 190 |  Loss: (2.1664) |  Loss2: (0.0000) | Acc: (21.00%) (5265/24448)
Epoch: 0 | Batch_idx: 200 |  Loss: (2.1585) |  Loss2: (0.0000) | Acc: (21.00%) (5625/25728)
Epoch: 0 | Batch_idx: 210 |  Loss: (2.1502) |  Loss2: (0.0000) | Acc: (22.00%) (5972/27008)
Epoch: 0 | Batch_idx: 220 |  Loss: (2.1424) |  Loss2: (0.0000) | Acc: (22.00%) (6325/28288)
Epoch: 0 | Batch_idx: 230 |  Loss: (2.1359) |  Loss2: (0.0000) | Acc: (22.00%) (6672/29568)
Epoch: 0 | Batch_idx: 240 |  Loss: (2.1288) |  Loss2: (0.0000) | Acc: (22.00%) (7018/30848)
Epoch: 0 | Batch_idx: 250 |  Loss: (2.1226) |  Loss2: (0.0000) | Acc: (22.00%) (7353/32128)
Epoch: 0 | Batch_idx: 260 |  Loss: (2.1152) |  Loss2: (0.0000) | Acc: (23.00%) (7747/33408)
Epoch: 0 | Batch_idx: 270 |  Loss: (2.1086) |  Loss2: (0.0000) | Acc: (23.00%) (8146/34688)
Epoch: 0 | Batch_idx: 280 |  Loss: (2.1020) |  Loss2: (0.0000) | Acc: (23.00%) (8557/35968)
Epoch: 0 | Batch_idx: 290 |  Loss: (2.0960) |  Loss2: (0.0000) | Acc: (24.00%) (8951/37248)
Epoch: 0 | Batch_idx: 300 |  Loss: (2.0893) |  Loss2: (0.0000) | Acc: (24.00%) (9373/38528)
Epoch: 0 | Batch_idx: 310 |  Loss: (2.0822) |  Loss2: (0.0000) | Acc: (24.00%) (9784/39808)
Epoch: 0 | Batch_idx: 320 |  Loss: (2.0764) |  Loss2: (0.0000) | Acc: (24.00%) (10195/41088)
Epoch: 0 | Batch_idx: 330 |  Loss: (2.0705) |  Loss2: (0.0000) | Acc: (24.00%) (10585/42368)
Epoch: 0 | Batch_idx: 340 |  Loss: (2.0649) |  Loss2: (0.0000) | Acc: (25.00%) (10993/43648)
Epoch: 0 | Batch_idx: 350 |  Loss: (2.0583) |  Loss2: (0.0000) | Acc: (25.00%) (11426/44928)
Epoch: 0 | Batch_idx: 360 |  Loss: (2.0538) |  Loss2: (0.0000) | Acc: (25.00%) (11813/46208)
Epoch: 0 | Batch_idx: 370 |  Loss: (2.0479) |  Loss2: (0.0000) | Acc: (25.00%) (12252/47488)
Epoch: 0 | Batch_idx: 380 |  Loss: (2.0429) |  Loss2: (0.0000) | Acc: (25.00%) (12677/48768)
Epoch: 0 | Batch_idx: 390 |  Loss: (2.0378) |  Loss2: (0.0000) | Acc: (26.00%) (13101/50000)
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type ResNet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type BasicBlock. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type _Gate. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_000.pth.tar'
# TEST : Loss: (1.7863) | Acc: (34.00%) (3443/10000)
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(168.1901, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(772.8687, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(767.9380, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1531.8798, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(520.6130, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2172.8420, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4334.5688, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1453.8347, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6141.6021, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12280.2295, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4097.2881, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(17365.8711, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 1 | Batch_idx: 0 |  Loss: (1.8560) |  Loss2: (0.0000) | Acc: (35.00%) (45/128)
Epoch: 1 | Batch_idx: 10 |  Loss: (1.8010) |  Loss2: (0.0000) | Acc: (34.00%) (490/1408)
Epoch: 1 | Batch_idx: 20 |  Loss: (1.7947) |  Loss2: (0.0000) | Acc: (35.00%) (967/2688)
Epoch: 1 | Batch_idx: 30 |  Loss: (1.7924) |  Loss2: (0.0000) | Acc: (35.00%) (1413/3968)
Epoch: 1 | Batch_idx: 40 |  Loss: (1.7903) |  Loss2: (0.0000) | Acc: (35.00%) (1885/5248)
Epoch: 1 | Batch_idx: 50 |  Loss: (1.7811) |  Loss2: (0.0000) | Acc: (36.00%) (2367/6528)
Epoch: 1 | Batch_idx: 60 |  Loss: (1.7807) |  Loss2: (0.0000) | Acc: (36.00%) (2822/7808)
Epoch: 1 | Batch_idx: 70 |  Loss: (1.7801) |  Loss2: (0.0000) | Acc: (35.00%) (3269/9088)
Epoch: 1 | Batch_idx: 80 |  Loss: (1.7783) |  Loss2: (0.0000) | Acc: (35.00%) (3721/10368)
Epoch: 1 | Batch_idx: 90 |  Loss: (1.7728) |  Loss2: (0.0000) | Acc: (35.00%) (4186/11648)
Epoch: 1 | Batch_idx: 100 |  Loss: (1.7679) |  Loss2: (0.0000) | Acc: (36.00%) (4671/12928)
Epoch: 1 | Batch_idx: 110 |  Loss: (1.7634) |  Loss2: (0.0000) | Acc: (36.00%) (5175/14208)
Epoch: 1 | Batch_idx: 120 |  Loss: (1.7610) |  Loss2: (0.0000) | Acc: (36.00%) (5612/15488)
Epoch: 1 | Batch_idx: 130 |  Loss: (1.7589) |  Loss2: (0.0000) | Acc: (36.00%) (6061/16768)
Epoch: 1 | Batch_idx: 140 |  Loss: (1.7546) |  Loss2: (0.0000) | Acc: (36.00%) (6524/18048)
Epoch: 1 | Batch_idx: 150 |  Loss: (1.7512) |  Loss2: (0.0000) | Acc: (36.00%) (6995/19328)
Epoch: 1 | Batch_idx: 160 |  Loss: (1.7477) |  Loss2: (0.0000) | Acc: (36.00%) (7471/20608)
Epoch: 1 | Batch_idx: 170 |  Loss: (1.7459) |  Loss2: (0.0000) | Acc: (36.00%) (7926/21888)
Epoch: 1 | Batch_idx: 180 |  Loss: (1.7428) |  Loss2: (0.0000) | Acc: (36.00%) (8417/23168)
Epoch: 1 | Batch_idx: 190 |  Loss: (1.7390) |  Loss2: (0.0000) | Acc: (36.00%) (8892/24448)
Epoch: 1 | Batch_idx: 200 |  Loss: (1.7365) |  Loss2: (0.0000) | Acc: (36.00%) (9383/25728)
Epoch: 1 | Batch_idx: 210 |  Loss: (1.7334) |  Loss2: (0.0000) | Acc: (36.00%) (9853/27008)
Epoch: 1 | Batch_idx: 220 |  Loss: (1.7305) |  Loss2: (0.0000) | Acc: (36.00%) (10327/28288)
Epoch: 1 | Batch_idx: 230 |  Loss: (1.7279) |  Loss2: (0.0000) | Acc: (36.00%) (10817/29568)
Epoch: 1 | Batch_idx: 240 |  Loss: (1.7247) |  Loss2: (0.0000) | Acc: (36.00%) (11326/30848)
Epoch: 1 | Batch_idx: 250 |  Loss: (1.7225) |  Loss2: (0.0000) | Acc: (36.00%) (11819/32128)
Epoch: 1 | Batch_idx: 260 |  Loss: (1.7200) |  Loss2: (0.0000) | Acc: (36.00%) (12302/33408)
Epoch: 1 | Batch_idx: 270 |  Loss: (1.7174) |  Loss2: (0.0000) | Acc: (36.00%) (12808/34688)
Epoch: 1 | Batch_idx: 280 |  Loss: (1.7143) |  Loss2: (0.0000) | Acc: (37.00%) (13317/35968)
Epoch: 1 | Batch_idx: 290 |  Loss: (1.7126) |  Loss2: (0.0000) | Acc: (37.00%) (13832/37248)
Epoch: 1 | Batch_idx: 300 |  Loss: (1.7094) |  Loss2: (0.0000) | Acc: (37.00%) (14348/38528)
Epoch: 1 | Batch_idx: 310 |  Loss: (1.7064) |  Loss2: (0.0000) | Acc: (37.00%) (14844/39808)
Epoch: 1 | Batch_idx: 320 |  Loss: (1.7045) |  Loss2: (0.0000) | Acc: (37.00%) (15338/41088)
Epoch: 1 | Batch_idx: 330 |  Loss: (1.7017) |  Loss2: (0.0000) | Acc: (37.00%) (15843/42368)
Epoch: 1 | Batch_idx: 340 |  Loss: (1.6991) |  Loss2: (0.0000) | Acc: (37.00%) (16372/43648)
Epoch: 1 | Batch_idx: 350 |  Loss: (1.6964) |  Loss2: (0.0000) | Acc: (37.00%) (16896/44928)
Epoch: 1 | Batch_idx: 360 |  Loss: (1.6943) |  Loss2: (0.0000) | Acc: (37.00%) (17393/46208)
Epoch: 1 | Batch_idx: 370 |  Loss: (1.6916) |  Loss2: (0.0000) | Acc: (37.00%) (17930/47488)
Epoch: 1 | Batch_idx: 380 |  Loss: (1.6885) |  Loss2: (0.0000) | Acc: (37.00%) (18439/48768)
Epoch: 1 | Batch_idx: 390 |  Loss: (1.6864) |  Loss2: (0.0000) | Acc: (37.00%) (18919/50000)
# TEST : Loss: (1.5386) | Acc: (41.00%) (4168/10000)
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
Epoch: 2 | Batch_idx: 0 |  Loss: (1.6477) |  Loss2: (0.0000) | Acc: (35.00%) (45/128)
Epoch: 2 | Batch_idx: 10 |  Loss: (1.5670) |  Loss2: (0.0000) | Acc: (41.00%) (578/1408)
Epoch: 2 | Batch_idx: 20 |  Loss: (1.5605) |  Loss2: (0.0000) | Acc: (41.00%) (1124/2688)
Epoch: 2 | Batch_idx: 30 |  Loss: (1.5672) |  Loss2: (0.0000) | Acc: (42.00%) (1673/3968)
Epoch: 2 | Batch_idx: 40 |  Loss: (1.5732) |  Loss2: (0.0000) | Acc: (42.00%) (2206/5248)
Epoch: 2 | Batch_idx: 50 |  Loss: (1.5677) |  Loss2: (0.0000) | Acc: (42.00%) (2758/6528)
Epoch: 2 | Batch_idx: 60 |  Loss: (1.5632) |  Loss2: (0.0000) | Acc: (42.00%) (3309/7808)
Epoch: 2 | Batch_idx: 70 |  Loss: (1.5599) |  Loss2: (0.0000) | Acc: (42.00%) (3884/9088)
Epoch: 2 | Batch_idx: 80 |  Loss: (1.5619) |  Loss2: (0.0000) | Acc: (42.00%) (4433/10368)
Epoch: 2 | Batch_idx: 90 |  Loss: (1.5602) |  Loss2: (0.0000) | Acc: (42.00%) (4998/11648)
Epoch: 2 | Batch_idx: 100 |  Loss: (1.5590) |  Loss2: (0.0000) | Acc: (42.00%) (5547/12928)
Epoch: 2 | Batch_idx: 110 |  Loss: (1.5550) |  Loss2: (0.0000) | Acc: (42.00%) (6104/14208)
Epoch: 2 | Batch_idx: 120 |  Loss: (1.5520) |  Loss2: (0.0000) | Acc: (42.00%) (6657/15488)
Epoch: 2 | Batch_idx: 130 |  Loss: (1.5514) |  Loss2: (0.0000) | Acc: (43.00%) (7225/16768)
Epoch: 2 | Batch_idx: 140 |  Loss: (1.5507) |  Loss2: (0.0000) | Acc: (43.00%) (7772/18048)
Epoch: 2 | Batch_idx: 150 |  Loss: (1.5497) |  Loss2: (0.0000) | Acc: (43.00%) (8329/19328)
Epoch: 2 | Batch_idx: 160 |  Loss: (1.5492) |  Loss2: (0.0000) | Acc: (43.00%) (8895/20608)
Epoch: 2 | Batch_idx: 170 |  Loss: (1.5472) |  Loss2: (0.0000) | Acc: (43.00%) (9463/21888)
Epoch: 2 | Batch_idx: 180 |  Loss: (1.5456) |  Loss2: (0.0000) | Acc: (43.00%) (10060/23168)
Epoch: 2 | Batch_idx: 190 |  Loss: (1.5439) |  Loss2: (0.0000) | Acc: (43.00%) (10629/24448)
Epoch: 2 | Batch_idx: 200 |  Loss: (1.5425) |  Loss2: (0.0000) | Acc: (43.00%) (11164/25728)
Epoch: 2 | Batch_idx: 210 |  Loss: (1.5400) |  Loss2: (0.0000) | Acc: (43.00%) (11768/27008)
Epoch: 2 | Batch_idx: 220 |  Loss: (1.5382) |  Loss2: (0.0000) | Acc: (43.00%) (12344/28288)
Epoch: 2 | Batch_idx: 230 |  Loss: (1.5360) |  Loss2: (0.0000) | Acc: (43.00%) (12925/29568)
Epoch: 2 | Batch_idx: 240 |  Loss: (1.5329) |  Loss2: (0.0000) | Acc: (43.00%) (13510/30848)
Epoch: 2 | Batch_idx: 250 |  Loss: (1.5312) |  Loss2: (0.0000) | Acc: (43.00%) (14110/32128)
Epoch: 2 | Batch_idx: 260 |  Loss: (1.5275) |  Loss2: (0.0000) | Acc: (44.00%) (14749/33408)
Epoch: 2 | Batch_idx: 270 |  Loss: (1.5253) |  Loss2: (0.0000) | Acc: (44.00%) (15324/34688)
Epoch: 2 | Batch_idx: 280 |  Loss: (1.5237) |  Loss2: (0.0000) | Acc: (44.00%) (15898/35968)
Epoch: 2 | Batch_idx: 290 |  Loss: (1.5214) |  Loss2: (0.0000) | Acc: (44.00%) (16483/37248)
Epoch: 2 | Batch_idx: 300 |  Loss: (1.5192) |  Loss2: (0.0000) | Acc: (44.00%) (17079/38528)
Epoch: 2 | Batch_idx: 310 |  Loss: (1.5175) |  Loss2: (0.0000) | Acc: (44.00%) (17688/39808)
Epoch: 2 | Batch_idx: 320 |  Loss: (1.5157) |  Loss2: (0.0000) | Acc: (44.00%) (18299/41088)
Epoch: 2 | Batch_idx: 330 |  Loss: (1.5135) |  Loss2: (0.0000) | Acc: (44.00%) (18883/42368)
Epoch: 2 | Batch_idx: 340 |  Loss: (1.5115) |  Loss2: (0.0000) | Acc: (44.00%) (19455/43648)
Epoch: 2 | Batch_idx: 350 |  Loss: (1.5093) |  Loss2: (0.0000) | Acc: (44.00%) (20049/44928)
Epoch: 2 | Batch_idx: 360 |  Loss: (1.5080) |  Loss2: (0.0000) | Acc: (44.00%) (20609/46208)
Epoch: 2 | Batch_idx: 370 |  Loss: (1.5053) |  Loss2: (0.0000) | Acc: (44.00%) (21228/47488)
Epoch: 2 | Batch_idx: 380 |  Loss: (1.5025) |  Loss2: (0.0000) | Acc: (44.00%) (21868/48768)
Epoch: 2 | Batch_idx: 390 |  Loss: (1.5007) |  Loss2: (0.0000) | Acc: (44.00%) (22453/50000)
# TEST : Loss: (1.4041) | Acc: (48.00%) (4827/10000)
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
Epoch: 3 | Batch_idx: 0 |  Loss: (1.3655) |  Loss2: (0.0000) | Acc: (53.00%) (68/128)
Epoch: 3 | Batch_idx: 10 |  Loss: (1.4271) |  Loss2: (0.0000) | Acc: (49.00%) (693/1408)
Epoch: 3 | Batch_idx: 20 |  Loss: (1.4019) |  Loss2: (0.0000) | Acc: (49.00%) (1340/2688)
Epoch: 3 | Batch_idx: 30 |  Loss: (1.3983) |  Loss2: (0.0000) | Acc: (49.00%) (1949/3968)
Epoch: 3 | Batch_idx: 40 |  Loss: (1.3950) |  Loss2: (0.0000) | Acc: (48.00%) (2567/5248)
Epoch: 3 | Batch_idx: 50 |  Loss: (1.3928) |  Loss2: (0.0000) | Acc: (49.00%) (3214/6528)
Epoch: 3 | Batch_idx: 60 |  Loss: (1.3989) |  Loss2: (0.0000) | Acc: (48.00%) (3819/7808)
Epoch: 3 | Batch_idx: 70 |  Loss: (1.3993) |  Loss2: (0.0000) | Acc: (48.00%) (4414/9088)
Epoch: 3 | Batch_idx: 80 |  Loss: (1.3998) |  Loss2: (0.0000) | Acc: (48.00%) (5036/10368)
Epoch: 3 | Batch_idx: 90 |  Loss: (1.3966) |  Loss2: (0.0000) | Acc: (48.00%) (5643/11648)
Epoch: 3 | Batch_idx: 100 |  Loss: (1.3948) |  Loss2: (0.0000) | Acc: (48.00%) (6267/12928)
Epoch: 3 | Batch_idx: 110 |  Loss: (1.3963) |  Loss2: (0.0000) | Acc: (48.00%) (6896/14208)
Epoch: 3 | Batch_idx: 120 |  Loss: (1.3955) |  Loss2: (0.0000) | Acc: (48.00%) (7528/15488)
Epoch: 3 | Batch_idx: 130 |  Loss: (1.3946) |  Loss2: (0.0000) | Acc: (48.00%) (8143/16768)
Epoch: 3 | Batch_idx: 140 |  Loss: (1.3935) |  Loss2: (0.0000) | Acc: (48.00%) (8799/18048)
Epoch: 3 | Batch_idx: 150 |  Loss: (1.3913) |  Loss2: (0.0000) | Acc: (48.00%) (9455/19328)
Epoch: 3 | Batch_idx: 160 |  Loss: (1.3900) |  Loss2: (0.0000) | Acc: (49.00%) (10101/20608)
Epoch: 3 | Batch_idx: 170 |  Loss: (1.3914) |  Loss2: (0.0000) | Acc: (48.00%) (10703/21888)
Epoch: 3 | Batch_idx: 180 |  Loss: (1.3914) |  Loss2: (0.0000) | Acc: (49.00%) (11357/23168)
Epoch: 3 | Batch_idx: 190 |  Loss: (1.3890) |  Loss2: (0.0000) | Acc: (49.00%) (12025/24448)
Epoch: 3 | Batch_idx: 200 |  Loss: (1.3883) |  Loss2: (0.0000) | Acc: (49.00%) (12663/25728)
Epoch: 3 | Batch_idx: 210 |  Loss: (1.3888) |  Loss2: (0.0000) | Acc: (49.00%) (13293/27008)
Epoch: 3 | Batch_idx: 220 |  Loss: (1.3851) |  Loss2: (0.0000) | Acc: (49.00%) (13981/28288)
Epoch: 3 | Batch_idx: 230 |  Loss: (1.3822) |  Loss2: (0.0000) | Acc: (49.00%) (14620/29568)
Epoch: 3 | Batch_idx: 240 |  Loss: (1.3812) |  Loss2: (0.0000) | Acc: (49.00%) (15273/30848)
Epoch: 3 | Batch_idx: 250 |  Loss: (1.3808) |  Loss2: (0.0000) | Acc: (49.00%) (15915/32128)
Epoch: 3 | Batch_idx: 260 |  Loss: (1.3784) |  Loss2: (0.0000) | Acc: (49.00%) (16601/33408)
Epoch: 3 | Batch_idx: 270 |  Loss: (1.3778) |  Loss2: (0.0000) | Acc: (49.00%) (17240/34688)
Epoch: 3 | Batch_idx: 280 |  Loss: (1.3779) |  Loss2: (0.0000) | Acc: (49.00%) (17871/35968)
Epoch: 3 | Batch_idx: 290 |  Loss: (1.3764) |  Loss2: (0.0000) | Acc: (49.00%) (18550/37248)
Epoch: 3 | Batch_idx: 300 |  Loss: (1.3743) |  Loss2: (0.0000) | Acc: (49.00%) (19208/38528)
Epoch: 3 | Batch_idx: 310 |  Loss: (1.3725) |  Loss2: (0.0000) | Acc: (49.00%) (19888/39808)
Epoch: 3 | Batch_idx: 320 |  Loss: (1.3705) |  Loss2: (0.0000) | Acc: (50.00%) (20555/41088)
Epoch: 3 | Batch_idx: 330 |  Loss: (1.3693) |  Loss2: (0.0000) | Acc: (50.00%) (21228/42368)
Epoch: 3 | Batch_idx: 340 |  Loss: (1.3674) |  Loss2: (0.0000) | Acc: (50.00%) (21899/43648)
Epoch: 3 | Batch_idx: 350 |  Loss: (1.3666) |  Loss2: (0.0000) | Acc: (50.00%) (22566/44928)
Epoch: 3 | Batch_idx: 360 |  Loss: (1.3644) |  Loss2: (0.0000) | Acc: (50.00%) (23258/46208)
Epoch: 3 | Batch_idx: 370 |  Loss: (1.3619) |  Loss2: (0.0000) | Acc: (50.00%) (23973/47488)
Epoch: 3 | Batch_idx: 380 |  Loss: (1.3608) |  Loss2: (0.0000) | Acc: (50.00%) (24650/48768)
Epoch: 3 | Batch_idx: 390 |  Loss: (1.3602) |  Loss2: (0.0000) | Acc: (50.00%) (25280/50000)
# TEST : Loss: (1.2917) | Acc: (52.00%) (5209/10000)
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
Epoch: 4 | Batch_idx: 0 |  Loss: (1.4297) |  Loss2: (0.0000) | Acc: (45.00%) (58/128)
Epoch: 4 | Batch_idx: 10 |  Loss: (1.3256) |  Loss2: (0.0000) | Acc: (51.00%) (732/1408)
Epoch: 4 | Batch_idx: 20 |  Loss: (1.3220) |  Loss2: (0.0000) | Acc: (52.00%) (1412/2688)
Epoch: 4 | Batch_idx: 30 |  Loss: (1.3204) |  Loss2: (0.0000) | Acc: (52.00%) (2075/3968)
Epoch: 4 | Batch_idx: 40 |  Loss: (1.3101) |  Loss2: (0.0000) | Acc: (52.00%) (2771/5248)
Epoch: 4 | Batch_idx: 50 |  Loss: (1.3043) |  Loss2: (0.0000) | Acc: (52.00%) (3434/6528)
Epoch: 4 | Batch_idx: 60 |  Loss: (1.3074) |  Loss2: (0.0000) | Acc: (52.00%) (4093/7808)
Epoch: 4 | Batch_idx: 70 |  Loss: (1.3082) |  Loss2: (0.0000) | Acc: (52.00%) (4784/9088)
Epoch: 4 | Batch_idx: 80 |  Loss: (1.3021) |  Loss2: (0.0000) | Acc: (53.00%) (5496/10368)
Epoch: 4 | Batch_idx: 90 |  Loss: (1.2936) |  Loss2: (0.0000) | Acc: (53.00%) (6224/11648)
Epoch: 4 | Batch_idx: 100 |  Loss: (1.2938) |  Loss2: (0.0000) | Acc: (53.00%) (6918/12928)
Epoch: 4 | Batch_idx: 110 |  Loss: (1.2934) |  Loss2: (0.0000) | Acc: (53.00%) (7597/14208)
Epoch: 4 | Batch_idx: 120 |  Loss: (1.2889) |  Loss2: (0.0000) | Acc: (53.00%) (8301/15488)
Epoch: 4 | Batch_idx: 130 |  Loss: (1.2883) |  Loss2: (0.0000) | Acc: (53.00%) (8996/16768)
Epoch: 4 | Batch_idx: 140 |  Loss: (1.2879) |  Loss2: (0.0000) | Acc: (53.00%) (9698/18048)
Epoch: 4 | Batch_idx: 150 |  Loss: (1.2842) |  Loss2: (0.0000) | Acc: (53.00%) (10410/19328)
Epoch: 4 | Batch_idx: 160 |  Loss: (1.2823) |  Loss2: (0.0000) | Acc: (53.00%) (11110/20608)
Epoch: 4 | Batch_idx: 170 |  Loss: (1.2823) |  Loss2: (0.0000) | Acc: (53.00%) (11805/21888)
Epoch: 4 | Batch_idx: 180 |  Loss: (1.2786) |  Loss2: (0.0000) | Acc: (54.00%) (12529/23168)
Epoch: 4 | Batch_idx: 190 |  Loss: (1.2777) |  Loss2: (0.0000) | Acc: (54.00%) (13221/24448)
Epoch: 4 | Batch_idx: 200 |  Loss: (1.2746) |  Loss2: (0.0000) | Acc: (54.00%) (13952/25728)
Epoch: 4 | Batch_idx: 210 |  Loss: (1.2733) |  Loss2: (0.0000) | Acc: (54.00%) (14656/27008)
Epoch: 4 | Batch_idx: 220 |  Loss: (1.2713) |  Loss2: (0.0000) | Acc: (54.00%) (15385/28288)
Epoch: 4 | Batch_idx: 230 |  Loss: (1.2688) |  Loss2: (0.0000) | Acc: (54.00%) (16102/29568)
Epoch: 4 | Batch_idx: 240 |  Loss: (1.2668) |  Loss2: (0.0000) | Acc: (54.00%) (16820/30848)
Epoch: 4 | Batch_idx: 250 |  Loss: (1.2651) |  Loss2: (0.0000) | Acc: (54.00%) (17519/32128)
Epoch: 4 | Batch_idx: 260 |  Loss: (1.2626) |  Loss2: (0.0000) | Acc: (54.00%) (18236/33408)
Epoch: 4 | Batch_idx: 270 |  Loss: (1.2620) |  Loss2: (0.0000) | Acc: (54.00%) (18931/34688)
Epoch: 4 | Batch_idx: 280 |  Loss: (1.2614) |  Loss2: (0.0000) | Acc: (54.00%) (19669/35968)
Epoch: 4 | Batch_idx: 290 |  Loss: (1.2608) |  Loss2: (0.0000) | Acc: (54.00%) (20376/37248)
Epoch: 4 | Batch_idx: 300 |  Loss: (1.2578) |  Loss2: (0.0000) | Acc: (54.00%) (21123/38528)
Epoch: 4 | Batch_idx: 310 |  Loss: (1.2559) |  Loss2: (0.0000) | Acc: (54.00%) (21869/39808)
Epoch: 4 | Batch_idx: 320 |  Loss: (1.2539) |  Loss2: (0.0000) | Acc: (55.00%) (22615/41088)
Epoch: 4 | Batch_idx: 330 |  Loss: (1.2519) |  Loss2: (0.0000) | Acc: (55.00%) (23347/42368)
Epoch: 4 | Batch_idx: 340 |  Loss: (1.2514) |  Loss2: (0.0000) | Acc: (55.00%) (24060/43648)
Epoch: 4 | Batch_idx: 350 |  Loss: (1.2519) |  Loss2: (0.0000) | Acc: (55.00%) (24763/44928)
Epoch: 4 | Batch_idx: 360 |  Loss: (1.2497) |  Loss2: (0.0000) | Acc: (55.00%) (25504/46208)
Epoch: 4 | Batch_idx: 370 |  Loss: (1.2479) |  Loss2: (0.0000) | Acc: (55.00%) (26238/47488)
Epoch: 4 | Batch_idx: 380 |  Loss: (1.2480) |  Loss2: (0.0000) | Acc: (55.00%) (26943/48768)
Epoch: 4 | Batch_idx: 390 |  Loss: (1.2466) |  Loss2: (0.0000) | Acc: (55.00%) (27658/50000)
# TEST : Loss: (1.2948) | Acc: (53.00%) (5335/10000)
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 5 | Batch_idx: 0 |  Loss: (1.0548) |  Loss2: (0.0000) | Acc: (60.00%) (77/128)
Epoch: 5 | Batch_idx: 10 |  Loss: (1.2142) |  Loss2: (0.0000) | Acc: (55.00%) (775/1408)
Epoch: 5 | Batch_idx: 20 |  Loss: (1.3016) |  Loss2: (0.0000) | Acc: (52.00%) (1417/2688)
Epoch: 5 | Batch_idx: 30 |  Loss: (1.3438) |  Loss2: (0.0000) | Acc: (51.00%) (2062/3968)
Epoch: 5 | Batch_idx: 40 |  Loss: (1.3748) |  Loss2: (0.0000) | Acc: (50.00%) (2672/5248)
Epoch: 5 | Batch_idx: 50 |  Loss: (1.3793) |  Loss2: (0.0000) | Acc: (50.00%) (3318/6528)
Epoch: 5 | Batch_idx: 60 |  Loss: (1.3879) |  Loss2: (0.0000) | Acc: (50.00%) (3927/7808)
Epoch: 5 | Batch_idx: 70 |  Loss: (1.3840) |  Loss2: (0.0000) | Acc: (50.00%) (4595/9088)
Epoch: 5 | Batch_idx: 80 |  Loss: (1.3827) |  Loss2: (0.0000) | Acc: (50.00%) (5228/10368)
Epoch: 5 | Batch_idx: 90 |  Loss: (1.3785) |  Loss2: (0.0000) | Acc: (50.00%) (5885/11648)
Epoch: 5 | Batch_idx: 100 |  Loss: (1.3754) |  Loss2: (0.0000) | Acc: (50.00%) (6524/12928)
Epoch: 5 | Batch_idx: 110 |  Loss: (1.3697) |  Loss2: (0.0000) | Acc: (50.00%) (7209/14208)
Epoch: 5 | Batch_idx: 120 |  Loss: (1.3634) |  Loss2: (0.0000) | Acc: (51.00%) (7899/15488)
Epoch: 5 | Batch_idx: 130 |  Loss: (1.3652) |  Loss2: (0.0000) | Acc: (50.00%) (8533/16768)
Epoch: 5 | Batch_idx: 140 |  Loss: (1.3576) |  Loss2: (0.0000) | Acc: (51.00%) (9238/18048)
Epoch: 5 | Batch_idx: 150 |  Loss: (1.3527) |  Loss2: (0.0000) | Acc: (51.00%) (9945/19328)
Epoch: 5 | Batch_idx: 160 |  Loss: (1.3498) |  Loss2: (0.0000) | Acc: (51.00%) (10613/20608)
Epoch: 5 | Batch_idx: 170 |  Loss: (1.3457) |  Loss2: (0.0000) | Acc: (51.00%) (11305/21888)
Epoch: 5 | Batch_idx: 180 |  Loss: (1.3439) |  Loss2: (0.0000) | Acc: (51.00%) (11969/23168)
Epoch: 5 | Batch_idx: 190 |  Loss: (1.3404) |  Loss2: (0.0000) | Acc: (51.00%) (12662/24448)
Epoch: 5 | Batch_idx: 200 |  Loss: (1.3377) |  Loss2: (0.0000) | Acc: (51.00%) (13338/25728)
Epoch: 5 | Batch_idx: 210 |  Loss: (1.3362) |  Loss2: (0.0000) | Acc: (51.00%) (14023/27008)
Epoch: 5 | Batch_idx: 220 |  Loss: (1.3336) |  Loss2: (0.0000) | Acc: (52.00%) (14722/28288)
Epoch: 5 | Batch_idx: 230 |  Loss: (1.3329) |  Loss2: (0.0000) | Acc: (52.00%) (15399/29568)
Epoch: 5 | Batch_idx: 240 |  Loss: (1.3303) |  Loss2: (0.0000) | Acc: (52.00%) (16072/30848)
Epoch: 5 | Batch_idx: 250 |  Loss: (1.3293) |  Loss2: (0.0000) | Acc: (52.00%) (16742/32128)
Epoch: 5 | Batch_idx: 260 |  Loss: (1.3271) |  Loss2: (0.0000) | Acc: (52.00%) (17456/33408)
Epoch: 5 | Batch_idx: 270 |  Loss: (1.3262) |  Loss2: (0.0000) | Acc: (52.00%) (18124/34688)
Epoch: 5 | Batch_idx: 280 |  Loss: (1.3234) |  Loss2: (0.0000) | Acc: (52.00%) (18826/35968)
Epoch: 5 | Batch_idx: 290 |  Loss: (1.3234) |  Loss2: (0.0000) | Acc: (52.00%) (19467/37248)
Epoch: 5 | Batch_idx: 300 |  Loss: (1.3221) |  Loss2: (0.0000) | Acc: (52.00%) (20146/38528)
Epoch: 5 | Batch_idx: 310 |  Loss: (1.3217) |  Loss2: (0.0000) | Acc: (52.00%) (20827/39808)
Epoch: 5 | Batch_idx: 320 |  Loss: (1.3197) |  Loss2: (0.0000) | Acc: (52.00%) (21509/41088)
Epoch: 5 | Batch_idx: 330 |  Loss: (1.3179) |  Loss2: (0.0000) | Acc: (52.00%) (22195/42368)
Epoch: 5 | Batch_idx: 340 |  Loss: (1.3164) |  Loss2: (0.0000) | Acc: (52.00%) (22877/43648)
Epoch: 5 | Batch_idx: 350 |  Loss: (1.3136) |  Loss2: (0.0000) | Acc: (52.00%) (23595/44928)
Epoch: 5 | Batch_idx: 360 |  Loss: (1.3121) |  Loss2: (0.0000) | Acc: (52.00%) (24305/46208)
Epoch: 5 | Batch_idx: 370 |  Loss: (1.3106) |  Loss2: (0.0000) | Acc: (52.00%) (25000/47488)
Epoch: 5 | Batch_idx: 380 |  Loss: (1.3097) |  Loss2: (0.0000) | Acc: (52.00%) (25689/48768)
Epoch: 5 | Batch_idx: 390 |  Loss: (1.3080) |  Loss2: (0.0000) | Acc: (52.00%) (26370/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_005.pth.tar'
# TEST : Loss: (1.2201) | Acc: (55.00%) (5503/10000)
percent tensor([0.5051, 0.4949], device='cuda:0')
percent tensor([0.4906, 0.5094], device='cuda:0')
percent tensor([0.4842, 0.5158], device='cuda:0')
percent tensor([0.4862, 0.5138], device='cuda:0')
percent tensor([0.4837, 0.5163], device='cuda:0')
percent tensor([0.5011, 0.4989], device='cuda:0')
percent tensor([0.5079, 0.4921], device='cuda:0')
percent tensor([0.5846, 0.4154], device='cuda:0')
Epoch: 6 | Batch_idx: 0 |  Loss: (1.2930) |  Loss2: (0.0000) | Acc: (50.00%) (64/128)
Epoch: 6 | Batch_idx: 10 |  Loss: (1.2423) |  Loss2: (0.0000) | Acc: (55.00%) (775/1408)
Epoch: 6 | Batch_idx: 20 |  Loss: (1.2482) |  Loss2: (0.0000) | Acc: (54.00%) (1473/2688)
Epoch: 6 | Batch_idx: 30 |  Loss: (1.2469) |  Loss2: (0.0000) | Acc: (55.00%) (2183/3968)
Epoch: 6 | Batch_idx: 40 |  Loss: (1.2370) |  Loss2: (0.0000) | Acc: (55.00%) (2910/5248)
Epoch: 6 | Batch_idx: 50 |  Loss: (1.2397) |  Loss2: (0.0000) | Acc: (55.00%) (3607/6528)
Epoch: 6 | Batch_idx: 60 |  Loss: (1.2381) |  Loss2: (0.0000) | Acc: (55.00%) (4299/7808)
Epoch: 6 | Batch_idx: 70 |  Loss: (1.2319) |  Loss2: (0.0000) | Acc: (55.00%) (5016/9088)
Epoch: 6 | Batch_idx: 80 |  Loss: (1.2314) |  Loss2: (0.0000) | Acc: (55.00%) (5729/10368)
Epoch: 6 | Batch_idx: 90 |  Loss: (1.2301) |  Loss2: (0.0000) | Acc: (55.00%) (6418/11648)
Epoch: 6 | Batch_idx: 100 |  Loss: (1.2304) |  Loss2: (0.0000) | Acc: (55.00%) (7112/12928)
Epoch: 6 | Batch_idx: 110 |  Loss: (1.2350) |  Loss2: (0.0000) | Acc: (54.00%) (7803/14208)
Epoch: 6 | Batch_idx: 120 |  Loss: (1.2321) |  Loss2: (0.0000) | Acc: (55.00%) (8528/15488)
Epoch: 6 | Batch_idx: 130 |  Loss: (1.2297) |  Loss2: (0.0000) | Acc: (55.00%) (9242/16768)
Epoch: 6 | Batch_idx: 140 |  Loss: (1.2262) |  Loss2: (0.0000) | Acc: (55.00%) (9974/18048)
Epoch: 6 | Batch_idx: 150 |  Loss: (1.2277) |  Loss2: (0.0000) | Acc: (55.00%) (10676/19328)
Epoch: 6 | Batch_idx: 160 |  Loss: (1.2274) |  Loss2: (0.0000) | Acc: (55.00%) (11381/20608)
Epoch: 6 | Batch_idx: 170 |  Loss: (1.2261) |  Loss2: (0.0000) | Acc: (55.00%) (12105/21888)
Epoch: 6 | Batch_idx: 180 |  Loss: (1.2267) |  Loss2: (0.0000) | Acc: (55.00%) (12831/23168)
Epoch: 6 | Batch_idx: 190 |  Loss: (1.2271) |  Loss2: (0.0000) | Acc: (55.00%) (13541/24448)
Epoch: 6 | Batch_idx: 200 |  Loss: (1.2278) |  Loss2: (0.0000) | Acc: (55.00%) (14257/25728)
Epoch: 6 | Batch_idx: 210 |  Loss: (1.2273) |  Loss2: (0.0000) | Acc: (55.00%) (14986/27008)
Epoch: 6 | Batch_idx: 220 |  Loss: (1.2282) |  Loss2: (0.0000) | Acc: (55.00%) (15675/28288)
Epoch: 6 | Batch_idx: 230 |  Loss: (1.2266) |  Loss2: (0.0000) | Acc: (55.00%) (16406/29568)
Epoch: 6 | Batch_idx: 240 |  Loss: (1.2267) |  Loss2: (0.0000) | Acc: (55.00%) (17110/30848)
Epoch: 6 | Batch_idx: 250 |  Loss: (1.2269) |  Loss2: (0.0000) | Acc: (55.00%) (17844/32128)
Epoch: 6 | Batch_idx: 260 |  Loss: (1.2251) |  Loss2: (0.0000) | Acc: (55.00%) (18572/33408)
Epoch: 6 | Batch_idx: 270 |  Loss: (1.2255) |  Loss2: (0.0000) | Acc: (55.00%) (19269/34688)
Epoch: 6 | Batch_idx: 280 |  Loss: (1.2264) |  Loss2: (0.0000) | Acc: (55.00%) (19975/35968)
Epoch: 6 | Batch_idx: 290 |  Loss: (1.2279) |  Loss2: (0.0000) | Acc: (55.00%) (20665/37248)
Epoch: 6 | Batch_idx: 300 |  Loss: (1.2267) |  Loss2: (0.0000) | Acc: (55.00%) (21384/38528)
Epoch: 6 | Batch_idx: 310 |  Loss: (1.2259) |  Loss2: (0.0000) | Acc: (55.00%) (22119/39808)
Epoch: 6 | Batch_idx: 320 |  Loss: (1.2247) |  Loss2: (0.0000) | Acc: (55.00%) (22843/41088)
Epoch: 6 | Batch_idx: 330 |  Loss: (1.2241) |  Loss2: (0.0000) | Acc: (55.00%) (23571/42368)
Epoch: 6 | Batch_idx: 340 |  Loss: (1.2235) |  Loss2: (0.0000) | Acc: (55.00%) (24282/43648)
Epoch: 6 | Batch_idx: 350 |  Loss: (1.2229) |  Loss2: (0.0000) | Acc: (55.00%) (25010/44928)
Epoch: 6 | Batch_idx: 360 |  Loss: (1.2212) |  Loss2: (0.0000) | Acc: (55.00%) (25743/46208)
Epoch: 6 | Batch_idx: 370 |  Loss: (1.2203) |  Loss2: (0.0000) | Acc: (55.00%) (26476/47488)
Epoch: 6 | Batch_idx: 380 |  Loss: (1.2186) |  Loss2: (0.0000) | Acc: (55.00%) (27225/48768)
Epoch: 6 | Batch_idx: 390 |  Loss: (1.2179) |  Loss2: (0.0000) | Acc: (55.00%) (27932/50000)
# TEST : Loss: (1.1856) | Acc: (56.00%) (5668/10000)
percent tensor([0.5111, 0.4889], device='cuda:0')
percent tensor([0.4921, 0.5079], device='cuda:0')
percent tensor([0.4807, 0.5193], device='cuda:0')
percent tensor([0.4859, 0.5141], device='cuda:0')
percent tensor([0.4890, 0.5110], device='cuda:0')
percent tensor([0.5054, 0.4946], device='cuda:0')
percent tensor([0.5151, 0.4849], device='cuda:0')
percent tensor([0.6943, 0.3057], device='cuda:0')
Epoch: 7 | Batch_idx: 0 |  Loss: (1.1268) |  Loss2: (0.0000) | Acc: (60.00%) (78/128)
Epoch: 7 | Batch_idx: 10 |  Loss: (1.2106) |  Loss2: (0.0000) | Acc: (55.00%) (776/1408)
Epoch: 7 | Batch_idx: 20 |  Loss: (1.2078) |  Loss2: (0.0000) | Acc: (56.00%) (1509/2688)
Epoch: 7 | Batch_idx: 30 |  Loss: (1.2075) |  Loss2: (0.0000) | Acc: (56.00%) (2229/3968)
Epoch: 7 | Batch_idx: 40 |  Loss: (1.2077) |  Loss2: (0.0000) | Acc: (56.00%) (2954/5248)
Epoch: 7 | Batch_idx: 50 |  Loss: (1.2035) |  Loss2: (0.0000) | Acc: (56.00%) (3665/6528)
Epoch: 7 | Batch_idx: 60 |  Loss: (1.2102) |  Loss2: (0.0000) | Acc: (55.00%) (4372/7808)
Epoch: 7 | Batch_idx: 70 |  Loss: (1.2056) |  Loss2: (0.0000) | Acc: (56.00%) (5109/9088)
Epoch: 7 | Batch_idx: 80 |  Loss: (1.2058) |  Loss2: (0.0000) | Acc: (56.00%) (5829/10368)
Epoch: 7 | Batch_idx: 90 |  Loss: (1.2086) |  Loss2: (0.0000) | Acc: (56.00%) (6549/11648)
Epoch: 7 | Batch_idx: 100 |  Loss: (1.2061) |  Loss2: (0.0000) | Acc: (56.00%) (7263/12928)
Epoch: 7 | Batch_idx: 110 |  Loss: (1.2051) |  Loss2: (0.0000) | Acc: (56.00%) (7975/14208)
Epoch: 7 | Batch_idx: 120 |  Loss: (1.2059) |  Loss2: (0.0000) | Acc: (56.00%) (8689/15488)
Epoch: 7 | Batch_idx: 130 |  Loss: (1.2062) |  Loss2: (0.0000) | Acc: (56.00%) (9401/16768)
Epoch: 7 | Batch_idx: 140 |  Loss: (1.2066) |  Loss2: (0.0000) | Acc: (56.00%) (10120/18048)
Epoch: 7 | Batch_idx: 150 |  Loss: (1.2060) |  Loss2: (0.0000) | Acc: (56.00%) (10832/19328)
Epoch: 7 | Batch_idx: 160 |  Loss: (1.2032) |  Loss2: (0.0000) | Acc: (56.00%) (11570/20608)
Epoch: 7 | Batch_idx: 170 |  Loss: (1.2000) |  Loss2: (0.0000) | Acc: (56.00%) (12324/21888)
Epoch: 7 | Batch_idx: 180 |  Loss: (1.1960) |  Loss2: (0.0000) | Acc: (56.00%) (13074/23168)
Epoch: 7 | Batch_idx: 190 |  Loss: (1.1967) |  Loss2: (0.0000) | Acc: (56.00%) (13781/24448)
Epoch: 7 | Batch_idx: 200 |  Loss: (1.1964) |  Loss2: (0.0000) | Acc: (56.00%) (14501/25728)
Epoch: 7 | Batch_idx: 210 |  Loss: (1.1948) |  Loss2: (0.0000) | Acc: (56.00%) (15237/27008)
Epoch: 7 | Batch_idx: 220 |  Loss: (1.1945) |  Loss2: (0.0000) | Acc: (56.00%) (15950/28288)
Epoch: 7 | Batch_idx: 230 |  Loss: (1.1931) |  Loss2: (0.0000) | Acc: (56.00%) (16692/29568)
Epoch: 7 | Batch_idx: 240 |  Loss: (1.1943) |  Loss2: (0.0000) | Acc: (56.00%) (17385/30848)
Epoch: 7 | Batch_idx: 250 |  Loss: (1.1938) |  Loss2: (0.0000) | Acc: (56.00%) (18124/32128)
Epoch: 7 | Batch_idx: 260 |  Loss: (1.1944) |  Loss2: (0.0000) | Acc: (56.00%) (18851/33408)
Epoch: 7 | Batch_idx: 270 |  Loss: (1.1931) |  Loss2: (0.0000) | Acc: (56.00%) (19586/34688)
Epoch: 7 | Batch_idx: 280 |  Loss: (1.1915) |  Loss2: (0.0000) | Acc: (56.00%) (20330/35968)
Epoch: 7 | Batch_idx: 290 |  Loss: (1.1924) |  Loss2: (0.0000) | Acc: (56.00%) (21033/37248)
Epoch: 7 | Batch_idx: 300 |  Loss: (1.1918) |  Loss2: (0.0000) | Acc: (56.00%) (21766/38528)
Epoch: 7 | Batch_idx: 310 |  Loss: (1.1923) |  Loss2: (0.0000) | Acc: (56.00%) (22484/39808)
Epoch: 7 | Batch_idx: 320 |  Loss: (1.1923) |  Loss2: (0.0000) | Acc: (56.00%) (23198/41088)
Epoch: 7 | Batch_idx: 330 |  Loss: (1.1912) |  Loss2: (0.0000) | Acc: (56.00%) (23950/42368)
Epoch: 7 | Batch_idx: 340 |  Loss: (1.1911) |  Loss2: (0.0000) | Acc: (56.00%) (24686/43648)
Epoch: 7 | Batch_idx: 350 |  Loss: (1.1897) |  Loss2: (0.0000) | Acc: (56.00%) (25451/44928)
Epoch: 7 | Batch_idx: 360 |  Loss: (1.1906) |  Loss2: (0.0000) | Acc: (56.00%) (26168/46208)
Epoch: 7 | Batch_idx: 370 |  Loss: (1.1920) |  Loss2: (0.0000) | Acc: (56.00%) (26865/47488)
Epoch: 7 | Batch_idx: 380 |  Loss: (1.1921) |  Loss2: (0.0000) | Acc: (56.00%) (27563/48768)
Epoch: 7 | Batch_idx: 390 |  Loss: (1.1919) |  Loss2: (0.0000) | Acc: (56.00%) (28273/50000)
# TEST : Loss: (1.1739) | Acc: (57.00%) (5763/10000)
percent tensor([0.5156, 0.4844], device='cuda:0')
percent tensor([0.4952, 0.5048], device='cuda:0')
percent tensor([0.4814, 0.5186], device='cuda:0')
percent tensor([0.4902, 0.5098], device='cuda:0')
percent tensor([0.4977, 0.5023], device='cuda:0')
percent tensor([0.5087, 0.4913], device='cuda:0')
percent tensor([0.5201, 0.4799], device='cuda:0')
percent tensor([0.7420, 0.2580], device='cuda:0')
Epoch: 8 | Batch_idx: 0 |  Loss: (1.1608) |  Loss2: (0.0000) | Acc: (57.00%) (74/128)
Epoch: 8 | Batch_idx: 10 |  Loss: (1.1840) |  Loss2: (0.0000) | Acc: (57.00%) (805/1408)
Epoch: 8 | Batch_idx: 20 |  Loss: (1.1713) |  Loss2: (0.0000) | Acc: (57.00%) (1534/2688)
Epoch: 8 | Batch_idx: 30 |  Loss: (1.1943) |  Loss2: (0.0000) | Acc: (56.00%) (2230/3968)
Epoch: 8 | Batch_idx: 40 |  Loss: (1.1939) |  Loss2: (0.0000) | Acc: (56.00%) (2942/5248)
Epoch: 8 | Batch_idx: 50 |  Loss: (1.1899) |  Loss2: (0.0000) | Acc: (56.00%) (3671/6528)
Epoch: 8 | Batch_idx: 60 |  Loss: (1.1918) |  Loss2: (0.0000) | Acc: (56.00%) (4398/7808)
Epoch: 8 | Batch_idx: 70 |  Loss: (1.1849) |  Loss2: (0.0000) | Acc: (56.00%) (5141/9088)
Epoch: 8 | Batch_idx: 80 |  Loss: (1.1896) |  Loss2: (0.0000) | Acc: (56.00%) (5855/10368)
Epoch: 8 | Batch_idx: 90 |  Loss: (1.1895) |  Loss2: (0.0000) | Acc: (56.00%) (6576/11648)
Epoch: 8 | Batch_idx: 100 |  Loss: (1.1932) |  Loss2: (0.0000) | Acc: (56.00%) (7271/12928)
Epoch: 8 | Batch_idx: 110 |  Loss: (1.1922) |  Loss2: (0.0000) | Acc: (56.00%) (7976/14208)
Epoch: 8 | Batch_idx: 120 |  Loss: (1.1903) |  Loss2: (0.0000) | Acc: (56.00%) (8730/15488)
Epoch: 8 | Batch_idx: 130 |  Loss: (1.1886) |  Loss2: (0.0000) | Acc: (56.00%) (9480/16768)
Epoch: 8 | Batch_idx: 140 |  Loss: (1.1850) |  Loss2: (0.0000) | Acc: (56.00%) (10213/18048)
Epoch: 8 | Batch_idx: 150 |  Loss: (1.1880) |  Loss2: (0.0000) | Acc: (56.00%) (10927/19328)
Epoch: 8 | Batch_idx: 160 |  Loss: (1.1888) |  Loss2: (0.0000) | Acc: (56.00%) (11663/20608)
Epoch: 8 | Batch_idx: 170 |  Loss: (1.1873) |  Loss2: (0.0000) | Acc: (56.00%) (12417/21888)
Epoch: 8 | Batch_idx: 180 |  Loss: (1.1888) |  Loss2: (0.0000) | Acc: (56.00%) (13131/23168)
Epoch: 8 | Batch_idx: 190 |  Loss: (1.1866) |  Loss2: (0.0000) | Acc: (56.00%) (13868/24448)
Epoch: 8 | Batch_idx: 200 |  Loss: (1.1867) |  Loss2: (0.0000) | Acc: (56.00%) (14599/25728)
Epoch: 8 | Batch_idx: 210 |  Loss: (1.1854) |  Loss2: (0.0000) | Acc: (56.00%) (15335/27008)
Epoch: 8 | Batch_idx: 220 |  Loss: (1.1853) |  Loss2: (0.0000) | Acc: (56.00%) (16067/28288)
Epoch: 8 | Batch_idx: 230 |  Loss: (1.1842) |  Loss2: (0.0000) | Acc: (56.00%) (16801/29568)
Epoch: 8 | Batch_idx: 240 |  Loss: (1.1851) |  Loss2: (0.0000) | Acc: (56.00%) (17517/30848)
Epoch: 8 | Batch_idx: 250 |  Loss: (1.1864) |  Loss2: (0.0000) | Acc: (56.00%) (18231/32128)
Epoch: 8 | Batch_idx: 260 |  Loss: (1.1849) |  Loss2: (0.0000) | Acc: (56.00%) (18956/33408)
Epoch: 8 | Batch_idx: 270 |  Loss: (1.1849) |  Loss2: (0.0000) | Acc: (56.00%) (19689/34688)
Epoch: 8 | Batch_idx: 280 |  Loss: (1.1862) |  Loss2: (0.0000) | Acc: (56.00%) (20388/35968)
Epoch: 8 | Batch_idx: 290 |  Loss: (1.1862) |  Loss2: (0.0000) | Acc: (56.00%) (21119/37248)
Epoch: 8 | Batch_idx: 300 |  Loss: (1.1855) |  Loss2: (0.0000) | Acc: (56.00%) (21868/38528)
Epoch: 8 | Batch_idx: 310 |  Loss: (1.1868) |  Loss2: (0.0000) | Acc: (56.00%) (22588/39808)
Epoch: 8 | Batch_idx: 320 |  Loss: (1.1866) |  Loss2: (0.0000) | Acc: (56.00%) (23312/41088)
Epoch: 8 | Batch_idx: 330 |  Loss: (1.1860) |  Loss2: (0.0000) | Acc: (56.00%) (24033/42368)
Epoch: 8 | Batch_idx: 340 |  Loss: (1.1850) |  Loss2: (0.0000) | Acc: (56.00%) (24796/43648)
Epoch: 8 | Batch_idx: 350 |  Loss: (1.1830) |  Loss2: (0.0000) | Acc: (56.00%) (25562/44928)
Epoch: 8 | Batch_idx: 360 |  Loss: (1.1822) |  Loss2: (0.0000) | Acc: (56.00%) (26298/46208)
Epoch: 8 | Batch_idx: 370 |  Loss: (1.1814) |  Loss2: (0.0000) | Acc: (56.00%) (27045/47488)
Epoch: 8 | Batch_idx: 380 |  Loss: (1.1799) |  Loss2: (0.0000) | Acc: (56.00%) (27797/48768)
Epoch: 8 | Batch_idx: 390 |  Loss: (1.1793) |  Loss2: (0.0000) | Acc: (57.00%) (28502/50000)
# TEST : Loss: (1.1619) | Acc: (57.00%) (5792/10000)
percent tensor([0.5172, 0.4828], device='cuda:0')
percent tensor([0.4965, 0.5035], device='cuda:0')
percent tensor([0.4823, 0.5177], device='cuda:0')
percent tensor([0.4945, 0.5055], device='cuda:0')
percent tensor([0.5055, 0.4945], device='cuda:0')
percent tensor([0.5119, 0.4881], device='cuda:0')
percent tensor([0.5260, 0.4740], device='cuda:0')
percent tensor([0.7711, 0.2289], device='cuda:0')
Epoch: 9 | Batch_idx: 0 |  Loss: (1.1442) |  Loss2: (0.0000) | Acc: (55.00%) (71/128)
Epoch: 9 | Batch_idx: 10 |  Loss: (1.1314) |  Loss2: (0.0000) | Acc: (58.00%) (822/1408)
Epoch: 9 | Batch_idx: 20 |  Loss: (1.1520) |  Loss2: (0.0000) | Acc: (58.00%) (1573/2688)
Epoch: 9 | Batch_idx: 30 |  Loss: (1.1590) |  Loss2: (0.0000) | Acc: (58.00%) (2323/3968)
Epoch: 9 | Batch_idx: 40 |  Loss: (1.1562) |  Loss2: (0.0000) | Acc: (58.00%) (3058/5248)
Epoch: 9 | Batch_idx: 50 |  Loss: (1.1545) |  Loss2: (0.0000) | Acc: (58.00%) (3821/6528)
Epoch: 9 | Batch_idx: 60 |  Loss: (1.1538) |  Loss2: (0.0000) | Acc: (58.00%) (4562/7808)
Epoch: 9 | Batch_idx: 70 |  Loss: (1.1542) |  Loss2: (0.0000) | Acc: (58.00%) (5311/9088)
Epoch: 9 | Batch_idx: 80 |  Loss: (1.1597) |  Loss2: (0.0000) | Acc: (58.00%) (6035/10368)
Epoch: 9 | Batch_idx: 90 |  Loss: (1.1619) |  Loss2: (0.0000) | Acc: (57.00%) (6754/11648)
Epoch: 9 | Batch_idx: 100 |  Loss: (1.1669) |  Loss2: (0.0000) | Acc: (57.00%) (7455/12928)
Epoch: 9 | Batch_idx: 110 |  Loss: (1.1691) |  Loss2: (0.0000) | Acc: (57.00%) (8186/14208)
Epoch: 9 | Batch_idx: 120 |  Loss: (1.1756) |  Loss2: (0.0000) | Acc: (57.00%) (8855/15488)
Epoch: 9 | Batch_idx: 130 |  Loss: (1.1752) |  Loss2: (0.0000) | Acc: (57.00%) (9607/16768)
Epoch: 9 | Batch_idx: 140 |  Loss: (1.1727) |  Loss2: (0.0000) | Acc: (57.00%) (10360/18048)
Epoch: 9 | Batch_idx: 150 |  Loss: (1.1722) |  Loss2: (0.0000) | Acc: (57.00%) (11108/19328)
Epoch: 9 | Batch_idx: 160 |  Loss: (1.1704) |  Loss2: (0.0000) | Acc: (57.00%) (11830/20608)
Epoch: 9 | Batch_idx: 170 |  Loss: (1.1696) |  Loss2: (0.0000) | Acc: (57.00%) (12567/21888)
Epoch: 9 | Batch_idx: 180 |  Loss: (1.1696) |  Loss2: (0.0000) | Acc: (57.00%) (13317/23168)
Epoch: 9 | Batch_idx: 190 |  Loss: (1.1712) |  Loss2: (0.0000) | Acc: (57.00%) (14020/24448)
Epoch: 9 | Batch_idx: 200 |  Loss: (1.1714) |  Loss2: (0.0000) | Acc: (57.00%) (14769/25728)
Epoch: 9 | Batch_idx: 210 |  Loss: (1.1713) |  Loss2: (0.0000) | Acc: (57.00%) (15500/27008)
Epoch: 9 | Batch_idx: 220 |  Loss: (1.1705) |  Loss2: (0.0000) | Acc: (57.00%) (16236/28288)
Epoch: 9 | Batch_idx: 230 |  Loss: (1.1719) |  Loss2: (0.0000) | Acc: (57.00%) (16932/29568)
Epoch: 9 | Batch_idx: 240 |  Loss: (1.1715) |  Loss2: (0.0000) | Acc: (57.00%) (17662/30848)
Epoch: 9 | Batch_idx: 250 |  Loss: (1.1718) |  Loss2: (0.0000) | Acc: (57.00%) (18404/32128)
Epoch: 9 | Batch_idx: 260 |  Loss: (1.1727) |  Loss2: (0.0000) | Acc: (57.00%) (19127/33408)
Epoch: 9 | Batch_idx: 270 |  Loss: (1.1729) |  Loss2: (0.0000) | Acc: (57.00%) (19849/34688)
Epoch: 9 | Batch_idx: 280 |  Loss: (1.1729) |  Loss2: (0.0000) | Acc: (57.00%) (20564/35968)
Epoch: 9 | Batch_idx: 290 |  Loss: (1.1719) |  Loss2: (0.0000) | Acc: (57.00%) (21322/37248)
Epoch: 9 | Batch_idx: 300 |  Loss: (1.1720) |  Loss2: (0.0000) | Acc: (57.00%) (22061/38528)
Epoch: 9 | Batch_idx: 310 |  Loss: (1.1712) |  Loss2: (0.0000) | Acc: (57.00%) (22801/39808)
Epoch: 9 | Batch_idx: 320 |  Loss: (1.1723) |  Loss2: (0.0000) | Acc: (57.00%) (23517/41088)
Epoch: 9 | Batch_idx: 330 |  Loss: (1.1706) |  Loss2: (0.0000) | Acc: (57.00%) (24289/42368)
Epoch: 9 | Batch_idx: 340 |  Loss: (1.1689) |  Loss2: (0.0000) | Acc: (57.00%) (25049/43648)
Epoch: 9 | Batch_idx: 350 |  Loss: (1.1689) |  Loss2: (0.0000) | Acc: (57.00%) (25778/44928)
Epoch: 9 | Batch_idx: 360 |  Loss: (1.1689) |  Loss2: (0.0000) | Acc: (57.00%) (26529/46208)
Epoch: 9 | Batch_idx: 370 |  Loss: (1.1683) |  Loss2: (0.0000) | Acc: (57.00%) (27265/47488)
Epoch: 9 | Batch_idx: 380 |  Loss: (1.1693) |  Loss2: (0.0000) | Acc: (57.00%) (28001/48768)
Epoch: 9 | Batch_idx: 390 |  Loss: (1.1697) |  Loss2: (0.0000) | Acc: (57.00%) (28691/50000)
# TEST : Loss: (1.1592) | Acc: (58.00%) (5813/10000)
percent tensor([0.5177, 0.4823], device='cuda:0')
percent tensor([0.4971, 0.5029], device='cuda:0')
percent tensor([0.4829, 0.5171], device='cuda:0')
percent tensor([0.4981, 0.5019], device='cuda:0')
percent tensor([0.5129, 0.4871], device='cuda:0')
percent tensor([0.5151, 0.4849], device='cuda:0')
percent tensor([0.5313, 0.4687], device='cuda:0')
percent tensor([0.7864, 0.2136], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 10 | Batch_idx: 0 |  Loss: (1.2573) |  Loss2: (0.0000) | Acc: (56.00%) (72/128)
Epoch: 10 | Batch_idx: 10 |  Loss: (1.2284) |  Loss2: (0.0000) | Acc: (57.00%) (803/1408)
Epoch: 10 | Batch_idx: 20 |  Loss: (1.2202) |  Loss2: (0.0000) | Acc: (56.00%) (1522/2688)
Epoch: 10 | Batch_idx: 30 |  Loss: (1.2198) |  Loss2: (0.0000) | Acc: (56.00%) (2238/3968)
Epoch: 10 | Batch_idx: 40 |  Loss: (1.1971) |  Loss2: (0.0000) | Acc: (56.00%) (2983/5248)
Epoch: 10 | Batch_idx: 50 |  Loss: (1.1969) |  Loss2: (0.0000) | Acc: (56.00%) (3712/6528)
Epoch: 10 | Batch_idx: 60 |  Loss: (1.1897) |  Loss2: (0.0000) | Acc: (57.00%) (4463/7808)
Epoch: 10 | Batch_idx: 70 |  Loss: (1.1806) |  Loss2: (0.0000) | Acc: (57.00%) (5212/9088)
Epoch: 10 | Batch_idx: 80 |  Loss: (1.1736) |  Loss2: (0.0000) | Acc: (57.00%) (5981/10368)
Epoch: 10 | Batch_idx: 90 |  Loss: (1.1663) |  Loss2: (0.0000) | Acc: (58.00%) (6760/11648)
Epoch: 10 | Batch_idx: 100 |  Loss: (1.1625) |  Loss2: (0.0000) | Acc: (58.00%) (7530/12928)
Epoch: 10 | Batch_idx: 110 |  Loss: (1.1598) |  Loss2: (0.0000) | Acc: (58.00%) (8289/14208)
Epoch: 10 | Batch_idx: 120 |  Loss: (1.1577) |  Loss2: (0.0000) | Acc: (58.00%) (9041/15488)
Epoch: 10 | Batch_idx: 130 |  Loss: (1.1527) |  Loss2: (0.0000) | Acc: (58.00%) (9814/16768)
Epoch: 10 | Batch_idx: 140 |  Loss: (1.1541) |  Loss2: (0.0000) | Acc: (58.00%) (10544/18048)
Epoch: 10 | Batch_idx: 150 |  Loss: (1.1530) |  Loss2: (0.0000) | Acc: (58.00%) (11300/19328)
Epoch: 10 | Batch_idx: 160 |  Loss: (1.1544) |  Loss2: (0.0000) | Acc: (58.00%) (12033/20608)
Epoch: 10 | Batch_idx: 170 |  Loss: (1.1536) |  Loss2: (0.0000) | Acc: (58.00%) (12789/21888)
Epoch: 10 | Batch_idx: 180 |  Loss: (1.1514) |  Loss2: (0.0000) | Acc: (58.00%) (13571/23168)
Epoch: 10 | Batch_idx: 190 |  Loss: (1.1512) |  Loss2: (0.0000) | Acc: (58.00%) (14324/24448)
Epoch: 10 | Batch_idx: 200 |  Loss: (1.1482) |  Loss2: (0.0000) | Acc: (58.00%) (15106/25728)
Epoch: 10 | Batch_idx: 210 |  Loss: (1.1460) |  Loss2: (0.0000) | Acc: (58.00%) (15893/27008)
Epoch: 10 | Batch_idx: 220 |  Loss: (1.1454) |  Loss2: (0.0000) | Acc: (58.00%) (16650/28288)
Epoch: 10 | Batch_idx: 230 |  Loss: (1.1441) |  Loss2: (0.0000) | Acc: (58.00%) (17419/29568)
Epoch: 10 | Batch_idx: 240 |  Loss: (1.1408) |  Loss2: (0.0000) | Acc: (59.00%) (18202/30848)
Epoch: 10 | Batch_idx: 250 |  Loss: (1.1386) |  Loss2: (0.0000) | Acc: (59.00%) (18982/32128)
Epoch: 10 | Batch_idx: 260 |  Loss: (1.1361) |  Loss2: (0.0000) | Acc: (59.00%) (19785/33408)
Epoch: 10 | Batch_idx: 270 |  Loss: (1.1360) |  Loss2: (0.0000) | Acc: (59.00%) (20534/34688)
Epoch: 10 | Batch_idx: 280 |  Loss: (1.1340) |  Loss2: (0.0000) | Acc: (59.00%) (21319/35968)
Epoch: 10 | Batch_idx: 290 |  Loss: (1.1336) |  Loss2: (0.0000) | Acc: (59.00%) (22081/37248)
Epoch: 10 | Batch_idx: 300 |  Loss: (1.1328) |  Loss2: (0.0000) | Acc: (59.00%) (22839/38528)
Epoch: 10 | Batch_idx: 310 |  Loss: (1.1312) |  Loss2: (0.0000) | Acc: (59.00%) (23612/39808)
Epoch: 10 | Batch_idx: 320 |  Loss: (1.1287) |  Loss2: (0.0000) | Acc: (59.00%) (24418/41088)
Epoch: 10 | Batch_idx: 330 |  Loss: (1.1261) |  Loss2: (0.0000) | Acc: (59.00%) (25231/42368)
Epoch: 10 | Batch_idx: 340 |  Loss: (1.1241) |  Loss2: (0.0000) | Acc: (59.00%) (26039/43648)
Epoch: 10 | Batch_idx: 350 |  Loss: (1.1235) |  Loss2: (0.0000) | Acc: (59.00%) (26787/44928)
Epoch: 10 | Batch_idx: 360 |  Loss: (1.1223) |  Loss2: (0.0000) | Acc: (59.00%) (27564/46208)
Epoch: 10 | Batch_idx: 370 |  Loss: (1.1210) |  Loss2: (0.0000) | Acc: (59.00%) (28351/47488)
Epoch: 10 | Batch_idx: 380 |  Loss: (1.1205) |  Loss2: (0.0000) | Acc: (59.00%) (29138/48768)
Epoch: 10 | Batch_idx: 390 |  Loss: (1.1188) |  Loss2: (0.0000) | Acc: (59.00%) (29901/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_010.pth.tar'
# TEST : Loss: (1.0776) | Acc: (60.00%) (6068/10000)
percent tensor([0.5175, 0.4825], device='cuda:0')
percent tensor([0.4971, 0.5029], device='cuda:0')
percent tensor([0.4834, 0.5166], device='cuda:0')
percent tensor([0.4981, 0.5019], device='cuda:0')
percent tensor([0.5132, 0.4868], device='cuda:0')
percent tensor([0.5154, 0.4846], device='cuda:0')
percent tensor([0.5332, 0.4668], device='cuda:0')
percent tensor([0.7918, 0.2082], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(168.7254, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(778.6592, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(771.5042, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1527.7139, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(518.8765, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2167.2141, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4317.1187, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1448.1422, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6111.4082, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12226.0586, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4080.1460, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(17279.5762, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 11 | Batch_idx: 0 |  Loss: (0.9706) |  Loss2: (0.0000) | Acc: (63.00%) (81/128)
Epoch: 11 | Batch_idx: 10 |  Loss: (1.0712) |  Loss2: (0.0000) | Acc: (62.00%) (880/1408)
Epoch: 11 | Batch_idx: 20 |  Loss: (1.0550) |  Loss2: (0.0000) | Acc: (62.00%) (1684/2688)
Epoch: 11 | Batch_idx: 30 |  Loss: (1.0572) |  Loss2: (0.0000) | Acc: (62.00%) (2479/3968)
Epoch: 11 | Batch_idx: 40 |  Loss: (1.0652) |  Loss2: (0.0000) | Acc: (62.00%) (3275/5248)
Epoch: 11 | Batch_idx: 50 |  Loss: (1.0726) |  Loss2: (0.0000) | Acc: (61.00%) (4045/6528)
Epoch: 11 | Batch_idx: 60 |  Loss: (1.0619) |  Loss2: (0.0000) | Acc: (62.00%) (4901/7808)
Epoch: 11 | Batch_idx: 70 |  Loss: (1.0616) |  Loss2: (0.0000) | Acc: (62.00%) (5694/9088)
Epoch: 11 | Batch_idx: 80 |  Loss: (1.0620) |  Loss2: (0.0000) | Acc: (62.00%) (6505/10368)
Epoch: 11 | Batch_idx: 90 |  Loss: (1.0601) |  Loss2: (0.0000) | Acc: (62.00%) (7285/11648)
Epoch: 11 | Batch_idx: 100 |  Loss: (1.0596) |  Loss2: (0.0000) | Acc: (62.00%) (8068/12928)
Epoch: 11 | Batch_idx: 110 |  Loss: (1.0577) |  Loss2: (0.0000) | Acc: (62.00%) (8878/14208)
Epoch: 11 | Batch_idx: 120 |  Loss: (1.0591) |  Loss2: (0.0000) | Acc: (62.00%) (9663/15488)
Epoch: 11 | Batch_idx: 130 |  Loss: (1.0592) |  Loss2: (0.0000) | Acc: (62.00%) (10447/16768)
Epoch: 11 | Batch_idx: 140 |  Loss: (1.0572) |  Loss2: (0.0000) | Acc: (62.00%) (11261/18048)
Epoch: 11 | Batch_idx: 150 |  Loss: (1.0549) |  Loss2: (0.0000) | Acc: (62.00%) (12069/19328)
Epoch: 11 | Batch_idx: 160 |  Loss: (1.0519) |  Loss2: (0.0000) | Acc: (62.00%) (12892/20608)
Epoch: 11 | Batch_idx: 170 |  Loss: (1.0499) |  Loss2: (0.0000) | Acc: (62.00%) (13707/21888)
Epoch: 11 | Batch_idx: 180 |  Loss: (1.0471) |  Loss2: (0.0000) | Acc: (62.00%) (14548/23168)
Epoch: 11 | Batch_idx: 190 |  Loss: (1.0473) |  Loss2: (0.0000) | Acc: (62.00%) (15349/24448)
Epoch: 11 | Batch_idx: 200 |  Loss: (1.0467) |  Loss2: (0.0000) | Acc: (62.00%) (16150/25728)
Epoch: 11 | Batch_idx: 210 |  Loss: (1.0468) |  Loss2: (0.0000) | Acc: (62.00%) (16949/27008)
Epoch: 11 | Batch_idx: 220 |  Loss: (1.0452) |  Loss2: (0.0000) | Acc: (62.00%) (17783/28288)
Epoch: 11 | Batch_idx: 230 |  Loss: (1.0427) |  Loss2: (0.0000) | Acc: (62.00%) (18612/29568)
Epoch: 11 | Batch_idx: 240 |  Loss: (1.0414) |  Loss2: (0.0000) | Acc: (63.00%) (19438/30848)
Epoch: 11 | Batch_idx: 250 |  Loss: (1.0398) |  Loss2: (0.0000) | Acc: (63.00%) (20248/32128)
Epoch: 11 | Batch_idx: 260 |  Loss: (1.0358) |  Loss2: (0.0000) | Acc: (63.00%) (21105/33408)
Epoch: 11 | Batch_idx: 270 |  Loss: (1.0347) |  Loss2: (0.0000) | Acc: (63.00%) (21922/34688)
Epoch: 11 | Batch_idx: 280 |  Loss: (1.0326) |  Loss2: (0.0000) | Acc: (63.00%) (22754/35968)
Epoch: 11 | Batch_idx: 290 |  Loss: (1.0304) |  Loss2: (0.0000) | Acc: (63.00%) (23581/37248)
Epoch: 11 | Batch_idx: 300 |  Loss: (1.0283) |  Loss2: (0.0000) | Acc: (63.00%) (24417/38528)
Epoch: 11 | Batch_idx: 310 |  Loss: (1.0279) |  Loss2: (0.0000) | Acc: (63.00%) (25248/39808)
Epoch: 11 | Batch_idx: 320 |  Loss: (1.0273) |  Loss2: (0.0000) | Acc: (63.00%) (26057/41088)
Epoch: 11 | Batch_idx: 330 |  Loss: (1.0263) |  Loss2: (0.0000) | Acc: (63.00%) (26889/42368)
Epoch: 11 | Batch_idx: 340 |  Loss: (1.0252) |  Loss2: (0.0000) | Acc: (63.00%) (27707/43648)
Epoch: 11 | Batch_idx: 350 |  Loss: (1.0235) |  Loss2: (0.0000) | Acc: (63.00%) (28551/44928)
Epoch: 11 | Batch_idx: 360 |  Loss: (1.0220) |  Loss2: (0.0000) | Acc: (63.00%) (29365/46208)
Epoch: 11 | Batch_idx: 370 |  Loss: (1.0209) |  Loss2: (0.0000) | Acc: (63.00%) (30198/47488)
Epoch: 11 | Batch_idx: 380 |  Loss: (1.0200) |  Loss2: (0.0000) | Acc: (63.00%) (31019/48768)
Epoch: 11 | Batch_idx: 390 |  Loss: (1.0191) |  Loss2: (0.0000) | Acc: (63.00%) (31802/50000)
# TEST : Loss: (1.1754) | Acc: (58.00%) (5832/10000)
percent tensor([0.5172, 0.4828], device='cuda:0')
percent tensor([0.4971, 0.5029], device='cuda:0')
percent tensor([0.4829, 0.5171], device='cuda:0')
percent tensor([0.4981, 0.5019], device='cuda:0')
percent tensor([0.5134, 0.4866], device='cuda:0')
percent tensor([0.5167, 0.4833], device='cuda:0')
percent tensor([0.5326, 0.4674], device='cuda:0')
percent tensor([0.8073, 0.1927], device='cuda:0')
Epoch: 12 | Batch_idx: 0 |  Loss: (0.9802) |  Loss2: (0.0000) | Acc: (66.00%) (85/128)
Epoch: 12 | Batch_idx: 10 |  Loss: (1.0186) |  Loss2: (0.0000) | Acc: (62.00%) (876/1408)
Epoch: 12 | Batch_idx: 20 |  Loss: (1.0100) |  Loss2: (0.0000) | Acc: (63.00%) (1697/2688)
Epoch: 12 | Batch_idx: 30 |  Loss: (0.9892) |  Loss2: (0.0000) | Acc: (64.00%) (2545/3968)
Epoch: 12 | Batch_idx: 40 |  Loss: (0.9964) |  Loss2: (0.0000) | Acc: (64.00%) (3370/5248)
Epoch: 12 | Batch_idx: 50 |  Loss: (0.9957) |  Loss2: (0.0000) | Acc: (64.00%) (4189/6528)
Epoch: 12 | Batch_idx: 60 |  Loss: (0.9846) |  Loss2: (0.0000) | Acc: (64.00%) (5058/7808)
Epoch: 12 | Batch_idx: 70 |  Loss: (0.9820) |  Loss2: (0.0000) | Acc: (64.00%) (5884/9088)
Epoch: 12 | Batch_idx: 80 |  Loss: (0.9813) |  Loss2: (0.0000) | Acc: (64.00%) (6719/10368)
Epoch: 12 | Batch_idx: 90 |  Loss: (0.9748) |  Loss2: (0.0000) | Acc: (65.00%) (7598/11648)
Epoch: 12 | Batch_idx: 100 |  Loss: (0.9748) |  Loss2: (0.0000) | Acc: (65.00%) (8431/12928)
Epoch: 12 | Batch_idx: 110 |  Loss: (0.9690) |  Loss2: (0.0000) | Acc: (65.00%) (9275/14208)
Epoch: 12 | Batch_idx: 120 |  Loss: (0.9704) |  Loss2: (0.0000) | Acc: (65.00%) (10107/15488)
Epoch: 12 | Batch_idx: 130 |  Loss: (0.9708) |  Loss2: (0.0000) | Acc: (65.00%) (10937/16768)
Epoch: 12 | Batch_idx: 140 |  Loss: (0.9699) |  Loss2: (0.0000) | Acc: (65.00%) (11792/18048)
Epoch: 12 | Batch_idx: 150 |  Loss: (0.9683) |  Loss2: (0.0000) | Acc: (65.00%) (12639/19328)
Epoch: 12 | Batch_idx: 160 |  Loss: (0.9637) |  Loss2: (0.0000) | Acc: (65.00%) (13515/20608)
Epoch: 12 | Batch_idx: 170 |  Loss: (0.9619) |  Loss2: (0.0000) | Acc: (65.00%) (14384/21888)
Epoch: 12 | Batch_idx: 180 |  Loss: (0.9627) |  Loss2: (0.0000) | Acc: (65.00%) (15221/23168)
Epoch: 12 | Batch_idx: 190 |  Loss: (0.9589) |  Loss2: (0.0000) | Acc: (65.00%) (16095/24448)
Epoch: 12 | Batch_idx: 200 |  Loss: (0.9602) |  Loss2: (0.0000) | Acc: (65.00%) (16929/25728)
Epoch: 12 | Batch_idx: 210 |  Loss: (0.9611) |  Loss2: (0.0000) | Acc: (65.00%) (17758/27008)
Epoch: 12 | Batch_idx: 220 |  Loss: (0.9605) |  Loss2: (0.0000) | Acc: (65.00%) (18625/28288)
Epoch: 12 | Batch_idx: 230 |  Loss: (0.9575) |  Loss2: (0.0000) | Acc: (65.00%) (19490/29568)
Epoch: 12 | Batch_idx: 240 |  Loss: (0.9577) |  Loss2: (0.0000) | Acc: (65.00%) (20323/30848)
Epoch: 12 | Batch_idx: 250 |  Loss: (0.9565) |  Loss2: (0.0000) | Acc: (65.00%) (21174/32128)
Epoch: 12 | Batch_idx: 260 |  Loss: (0.9553) |  Loss2: (0.0000) | Acc: (65.00%) (22037/33408)
Epoch: 12 | Batch_idx: 270 |  Loss: (0.9545) |  Loss2: (0.0000) | Acc: (66.00%) (22895/34688)
Epoch: 12 | Batch_idx: 280 |  Loss: (0.9555) |  Loss2: (0.0000) | Acc: (65.00%) (23724/35968)
Epoch: 12 | Batch_idx: 290 |  Loss: (0.9541) |  Loss2: (0.0000) | Acc: (65.00%) (24580/37248)
Epoch: 12 | Batch_idx: 300 |  Loss: (0.9541) |  Loss2: (0.0000) | Acc: (66.00%) (25438/38528)
Epoch: 12 | Batch_idx: 310 |  Loss: (0.9541) |  Loss2: (0.0000) | Acc: (66.00%) (26279/39808)
Epoch: 12 | Batch_idx: 320 |  Loss: (0.9536) |  Loss2: (0.0000) | Acc: (66.00%) (27125/41088)
Epoch: 12 | Batch_idx: 330 |  Loss: (0.9531) |  Loss2: (0.0000) | Acc: (66.00%) (27979/42368)
Epoch: 12 | Batch_idx: 340 |  Loss: (0.9528) |  Loss2: (0.0000) | Acc: (66.00%) (28844/43648)
Epoch: 12 | Batch_idx: 350 |  Loss: (0.9525) |  Loss2: (0.0000) | Acc: (66.00%) (29707/44928)
Epoch: 12 | Batch_idx: 360 |  Loss: (0.9522) |  Loss2: (0.0000) | Acc: (66.00%) (30547/46208)
Epoch: 12 | Batch_idx: 370 |  Loss: (0.9514) |  Loss2: (0.0000) | Acc: (66.00%) (31412/47488)
Epoch: 12 | Batch_idx: 380 |  Loss: (0.9497) |  Loss2: (0.0000) | Acc: (66.00%) (32288/48768)
Epoch: 12 | Batch_idx: 390 |  Loss: (0.9476) |  Loss2: (0.0000) | Acc: (66.00%) (33142/50000)
# TEST : Loss: (1.0360) | Acc: (63.00%) (6339/10000)
percent tensor([0.5173, 0.4827], device='cuda:0')
percent tensor([0.4972, 0.5028], device='cuda:0')
percent tensor([0.4824, 0.5176], device='cuda:0')
percent tensor([0.4981, 0.5019], device='cuda:0')
percent tensor([0.5135, 0.4865], device='cuda:0')
percent tensor([0.5168, 0.4832], device='cuda:0')
percent tensor([0.5343, 0.4657], device='cuda:0')
percent tensor([0.8005, 0.1995], device='cuda:0')
Epoch: 13 | Batch_idx: 0 |  Loss: (0.7668) |  Loss2: (0.0000) | Acc: (72.00%) (93/128)
Epoch: 13 | Batch_idx: 10 |  Loss: (0.9448) |  Loss2: (0.0000) | Acc: (66.00%) (934/1408)
Epoch: 13 | Batch_idx: 20 |  Loss: (0.9423) |  Loss2: (0.0000) | Acc: (66.00%) (1789/2688)
Epoch: 13 | Batch_idx: 30 |  Loss: (0.9363) |  Loss2: (0.0000) | Acc: (66.00%) (2653/3968)
Epoch: 13 | Batch_idx: 40 |  Loss: (0.9271) |  Loss2: (0.0000) | Acc: (67.00%) (3518/5248)
Epoch: 13 | Batch_idx: 50 |  Loss: (0.9176) |  Loss2: (0.0000) | Acc: (67.00%) (4407/6528)
Epoch: 13 | Batch_idx: 60 |  Loss: (0.9094) |  Loss2: (0.0000) | Acc: (67.00%) (5300/7808)
Epoch: 13 | Batch_idx: 70 |  Loss: (0.9036) |  Loss2: (0.0000) | Acc: (68.00%) (6191/9088)
Epoch: 13 | Batch_idx: 80 |  Loss: (0.9042) |  Loss2: (0.0000) | Acc: (67.00%) (7047/10368)
Epoch: 13 | Batch_idx: 90 |  Loss: (0.9005) |  Loss2: (0.0000) | Acc: (68.00%) (7932/11648)
Epoch: 13 | Batch_idx: 100 |  Loss: (0.8999) |  Loss2: (0.0000) | Acc: (68.00%) (8805/12928)
Epoch: 13 | Batch_idx: 110 |  Loss: (0.8970) |  Loss2: (0.0000) | Acc: (68.00%) (9696/14208)
Epoch: 13 | Batch_idx: 120 |  Loss: (0.8924) |  Loss2: (0.0000) | Acc: (68.00%) (10594/15488)
Epoch: 13 | Batch_idx: 130 |  Loss: (0.8921) |  Loss2: (0.0000) | Acc: (68.00%) (11467/16768)
Epoch: 13 | Batch_idx: 140 |  Loss: (0.8913) |  Loss2: (0.0000) | Acc: (68.00%) (12335/18048)
Epoch: 13 | Batch_idx: 150 |  Loss: (0.8887) |  Loss2: (0.0000) | Acc: (68.00%) (13228/19328)
Epoch: 13 | Batch_idx: 160 |  Loss: (0.8885) |  Loss2: (0.0000) | Acc: (68.00%) (14104/20608)
Epoch: 13 | Batch_idx: 170 |  Loss: (0.8892) |  Loss2: (0.0000) | Acc: (68.00%) (14969/21888)
Epoch: 13 | Batch_idx: 180 |  Loss: (0.8876) |  Loss2: (0.0000) | Acc: (68.00%) (15871/23168)
Epoch: 13 | Batch_idx: 190 |  Loss: (0.8874) |  Loss2: (0.0000) | Acc: (68.00%) (16739/24448)
Epoch: 13 | Batch_idx: 200 |  Loss: (0.8882) |  Loss2: (0.0000) | Acc: (68.00%) (17598/25728)
Epoch: 13 | Batch_idx: 210 |  Loss: (0.8872) |  Loss2: (0.0000) | Acc: (68.00%) (18487/27008)
Epoch: 13 | Batch_idx: 220 |  Loss: (0.8861) |  Loss2: (0.0000) | Acc: (68.00%) (19389/28288)
Epoch: 13 | Batch_idx: 230 |  Loss: (0.8871) |  Loss2: (0.0000) | Acc: (68.00%) (20267/29568)
Epoch: 13 | Batch_idx: 240 |  Loss: (0.8848) |  Loss2: (0.0000) | Acc: (68.00%) (21165/30848)
Epoch: 13 | Batch_idx: 250 |  Loss: (0.8858) |  Loss2: (0.0000) | Acc: (68.00%) (22036/32128)
Epoch: 13 | Batch_idx: 260 |  Loss: (0.8860) |  Loss2: (0.0000) | Acc: (68.00%) (22898/33408)
Epoch: 13 | Batch_idx: 270 |  Loss: (0.8862) |  Loss2: (0.0000) | Acc: (68.00%) (23761/34688)
Epoch: 13 | Batch_idx: 280 |  Loss: (0.8859) |  Loss2: (0.0000) | Acc: (68.00%) (24652/35968)
Epoch: 13 | Batch_idx: 290 |  Loss: (0.8857) |  Loss2: (0.0000) | Acc: (68.00%) (25546/37248)
Epoch: 13 | Batch_idx: 300 |  Loss: (0.8840) |  Loss2: (0.0000) | Acc: (68.00%) (26463/38528)
Epoch: 13 | Batch_idx: 310 |  Loss: (0.8829) |  Loss2: (0.0000) | Acc: (68.00%) (27371/39808)
Epoch: 13 | Batch_idx: 320 |  Loss: (0.8844) |  Loss2: (0.0000) | Acc: (68.00%) (28222/41088)
Epoch: 13 | Batch_idx: 330 |  Loss: (0.8834) |  Loss2: (0.0000) | Acc: (68.00%) (29128/42368)
Epoch: 13 | Batch_idx: 340 |  Loss: (0.8835) |  Loss2: (0.0000) | Acc: (68.00%) (30028/43648)
Epoch: 13 | Batch_idx: 350 |  Loss: (0.8827) |  Loss2: (0.0000) | Acc: (68.00%) (30916/44928)
Epoch: 13 | Batch_idx: 360 |  Loss: (0.8830) |  Loss2: (0.0000) | Acc: (68.00%) (31784/46208)
Epoch: 13 | Batch_idx: 370 |  Loss: (0.8831) |  Loss2: (0.0000) | Acc: (68.00%) (32651/47488)
Epoch: 13 | Batch_idx: 380 |  Loss: (0.8835) |  Loss2: (0.0000) | Acc: (68.00%) (33515/48768)
Epoch: 13 | Batch_idx: 390 |  Loss: (0.8842) |  Loss2: (0.0000) | Acc: (68.00%) (34341/50000)
# TEST : Loss: (0.9239) | Acc: (67.00%) (6742/10000)
percent tensor([0.5171, 0.4829], device='cuda:0')
percent tensor([0.4971, 0.5029], device='cuda:0')
percent tensor([0.4823, 0.5177], device='cuda:0')
percent tensor([0.4981, 0.5019], device='cuda:0')
percent tensor([0.5136, 0.4864], device='cuda:0')
percent tensor([0.5167, 0.4833], device='cuda:0')
percent tensor([0.5332, 0.4668], device='cuda:0')
percent tensor([0.8002, 0.1998], device='cuda:0')
Epoch: 14 | Batch_idx: 0 |  Loss: (0.8574) |  Loss2: (0.0000) | Acc: (70.00%) (90/128)
Epoch: 14 | Batch_idx: 10 |  Loss: (0.8217) |  Loss2: (0.0000) | Acc: (70.00%) (995/1408)
Epoch: 14 | Batch_idx: 20 |  Loss: (0.8317) |  Loss2: (0.0000) | Acc: (70.00%) (1894/2688)
Epoch: 14 | Batch_idx: 30 |  Loss: (0.8581) |  Loss2: (0.0000) | Acc: (69.00%) (2750/3968)
Epoch: 14 | Batch_idx: 40 |  Loss: (0.8550) |  Loss2: (0.0000) | Acc: (69.00%) (3650/5248)
Epoch: 14 | Batch_idx: 50 |  Loss: (0.8558) |  Loss2: (0.0000) | Acc: (69.00%) (4530/6528)
Epoch: 14 | Batch_idx: 60 |  Loss: (0.8590) |  Loss2: (0.0000) | Acc: (69.00%) (5408/7808)
Epoch: 14 | Batch_idx: 70 |  Loss: (0.8577) |  Loss2: (0.0000) | Acc: (69.00%) (6301/9088)
Epoch: 14 | Batch_idx: 80 |  Loss: (0.8566) |  Loss2: (0.0000) | Acc: (69.00%) (7194/10368)
Epoch: 14 | Batch_idx: 90 |  Loss: (0.8559) |  Loss2: (0.0000) | Acc: (69.00%) (8104/11648)
Epoch: 14 | Batch_idx: 100 |  Loss: (0.8571) |  Loss2: (0.0000) | Acc: (69.00%) (8990/12928)
Epoch: 14 | Batch_idx: 110 |  Loss: (0.8551) |  Loss2: (0.0000) | Acc: (69.00%) (9903/14208)
Epoch: 14 | Batch_idx: 120 |  Loss: (0.8526) |  Loss2: (0.0000) | Acc: (69.00%) (10815/15488)
Epoch: 14 | Batch_idx: 130 |  Loss: (0.8527) |  Loss2: (0.0000) | Acc: (69.00%) (11719/16768)
Epoch: 14 | Batch_idx: 140 |  Loss: (0.8545) |  Loss2: (0.0000) | Acc: (69.00%) (12594/18048)
Epoch: 14 | Batch_idx: 150 |  Loss: (0.8505) |  Loss2: (0.0000) | Acc: (69.00%) (13508/19328)
Epoch: 14 | Batch_idx: 160 |  Loss: (0.8502) |  Loss2: (0.0000) | Acc: (69.00%) (14406/20608)
Epoch: 14 | Batch_idx: 170 |  Loss: (0.8498) |  Loss2: (0.0000) | Acc: (69.00%) (15297/21888)
Epoch: 14 | Batch_idx: 180 |  Loss: (0.8483) |  Loss2: (0.0000) | Acc: (69.00%) (16205/23168)
Epoch: 14 | Batch_idx: 190 |  Loss: (0.8504) |  Loss2: (0.0000) | Acc: (69.00%) (17093/24448)
Epoch: 14 | Batch_idx: 200 |  Loss: (0.8490) |  Loss2: (0.0000) | Acc: (70.00%) (18015/25728)
Epoch: 14 | Batch_idx: 210 |  Loss: (0.8472) |  Loss2: (0.0000) | Acc: (70.00%) (18944/27008)
Epoch: 14 | Batch_idx: 220 |  Loss: (0.8464) |  Loss2: (0.0000) | Acc: (70.00%) (19840/28288)
Epoch: 14 | Batch_idx: 230 |  Loss: (0.8459) |  Loss2: (0.0000) | Acc: (70.00%) (20745/29568)
Epoch: 14 | Batch_idx: 240 |  Loss: (0.8433) |  Loss2: (0.0000) | Acc: (70.00%) (21666/30848)
Epoch: 14 | Batch_idx: 250 |  Loss: (0.8423) |  Loss2: (0.0000) | Acc: (70.00%) (22592/32128)
Epoch: 14 | Batch_idx: 260 |  Loss: (0.8416) |  Loss2: (0.0000) | Acc: (70.00%) (23503/33408)
Epoch: 14 | Batch_idx: 270 |  Loss: (0.8409) |  Loss2: (0.0000) | Acc: (70.00%) (24415/34688)
Epoch: 14 | Batch_idx: 280 |  Loss: (0.8388) |  Loss2: (0.0000) | Acc: (70.00%) (25330/35968)
Epoch: 14 | Batch_idx: 290 |  Loss: (0.8377) |  Loss2: (0.0000) | Acc: (70.00%) (26235/37248)
Epoch: 14 | Batch_idx: 300 |  Loss: (0.8364) |  Loss2: (0.0000) | Acc: (70.00%) (27150/38528)
Epoch: 14 | Batch_idx: 310 |  Loss: (0.8359) |  Loss2: (0.0000) | Acc: (70.00%) (28050/39808)
Epoch: 14 | Batch_idx: 320 |  Loss: (0.8355) |  Loss2: (0.0000) | Acc: (70.00%) (28955/41088)
Epoch: 14 | Batch_idx: 330 |  Loss: (0.8345) |  Loss2: (0.0000) | Acc: (70.00%) (29889/42368)
Epoch: 14 | Batch_idx: 340 |  Loss: (0.8351) |  Loss2: (0.0000) | Acc: (70.00%) (30781/43648)
Epoch: 14 | Batch_idx: 350 |  Loss: (0.8358) |  Loss2: (0.0000) | Acc: (70.00%) (31675/44928)
Epoch: 14 | Batch_idx: 360 |  Loss: (0.8354) |  Loss2: (0.0000) | Acc: (70.00%) (32585/46208)
Epoch: 14 | Batch_idx: 370 |  Loss: (0.8341) |  Loss2: (0.0000) | Acc: (70.00%) (33519/47488)
Epoch: 14 | Batch_idx: 380 |  Loss: (0.8342) |  Loss2: (0.0000) | Acc: (70.00%) (34430/48768)
Epoch: 14 | Batch_idx: 390 |  Loss: (0.8338) |  Loss2: (0.0000) | Acc: (70.00%) (35288/50000)
# TEST : Loss: (0.8658) | Acc: (69.00%) (6915/10000)
percent tensor([0.5169, 0.4831], device='cuda:0')
percent tensor([0.4971, 0.5029], device='cuda:0')
percent tensor([0.4824, 0.5176], device='cuda:0')
percent tensor([0.4982, 0.5018], device='cuda:0')
percent tensor([0.5135, 0.4865], device='cuda:0')
percent tensor([0.5162, 0.4838], device='cuda:0')
percent tensor([0.5339, 0.4661], device='cuda:0')
percent tensor([0.8030, 0.1970], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 15 | Batch_idx: 0 |  Loss: (0.8983) |  Loss2: (0.0000) | Acc: (71.00%) (91/128)
Epoch: 15 | Batch_idx: 10 |  Loss: (0.9020) |  Loss2: (0.0000) | Acc: (68.00%) (958/1408)
Epoch: 15 | Batch_idx: 20 |  Loss: (0.9349) |  Loss2: (0.0000) | Acc: (66.00%) (1788/2688)
Epoch: 15 | Batch_idx: 30 |  Loss: (0.9451) |  Loss2: (0.0000) | Acc: (66.00%) (2636/3968)
Epoch: 15 | Batch_idx: 40 |  Loss: (0.9647) |  Loss2: (0.0000) | Acc: (65.00%) (3457/5248)
Epoch: 15 | Batch_idx: 50 |  Loss: (0.9775) |  Loss2: (0.0000) | Acc: (65.00%) (4275/6528)
Epoch: 15 | Batch_idx: 60 |  Loss: (0.9785) |  Loss2: (0.0000) | Acc: (65.00%) (5088/7808)
Epoch: 15 | Batch_idx: 70 |  Loss: (0.9747) |  Loss2: (0.0000) | Acc: (65.00%) (5956/9088)
Epoch: 15 | Batch_idx: 80 |  Loss: (0.9745) |  Loss2: (0.0000) | Acc: (65.00%) (6791/10368)
Epoch: 15 | Batch_idx: 90 |  Loss: (0.9690) |  Loss2: (0.0000) | Acc: (65.00%) (7657/11648)
Epoch: 15 | Batch_idx: 100 |  Loss: (0.9674) |  Loss2: (0.0000) | Acc: (65.00%) (8493/12928)
Epoch: 15 | Batch_idx: 110 |  Loss: (0.9597) |  Loss2: (0.0000) | Acc: (65.00%) (9360/14208)
Epoch: 15 | Batch_idx: 120 |  Loss: (0.9562) |  Loss2: (0.0000) | Acc: (65.00%) (10213/15488)
Epoch: 15 | Batch_idx: 130 |  Loss: (0.9568) |  Loss2: (0.0000) | Acc: (65.00%) (11040/16768)
Epoch: 15 | Batch_idx: 140 |  Loss: (0.9491) |  Loss2: (0.0000) | Acc: (66.00%) (11941/18048)
Epoch: 15 | Batch_idx: 150 |  Loss: (0.9483) |  Loss2: (0.0000) | Acc: (66.00%) (12799/19328)
Epoch: 15 | Batch_idx: 160 |  Loss: (0.9457) |  Loss2: (0.0000) | Acc: (66.00%) (13661/20608)
Epoch: 15 | Batch_idx: 170 |  Loss: (0.9411) |  Loss2: (0.0000) | Acc: (66.00%) (14563/21888)
Epoch: 15 | Batch_idx: 180 |  Loss: (0.9415) |  Loss2: (0.0000) | Acc: (66.00%) (15411/23168)
Epoch: 15 | Batch_idx: 190 |  Loss: (0.9385) |  Loss2: (0.0000) | Acc: (66.00%) (16295/24448)
Epoch: 15 | Batch_idx: 200 |  Loss: (0.9365) |  Loss2: (0.0000) | Acc: (66.00%) (17155/25728)
Epoch: 15 | Batch_idx: 210 |  Loss: (0.9336) |  Loss2: (0.0000) | Acc: (66.00%) (18021/27008)
Epoch: 15 | Batch_idx: 220 |  Loss: (0.9319) |  Loss2: (0.0000) | Acc: (66.00%) (18901/28288)
Epoch: 15 | Batch_idx: 230 |  Loss: (0.9292) |  Loss2: (0.0000) | Acc: (66.00%) (19790/29568)
Epoch: 15 | Batch_idx: 240 |  Loss: (0.9261) |  Loss2: (0.0000) | Acc: (67.00%) (20680/30848)
Epoch: 15 | Batch_idx: 250 |  Loss: (0.9222) |  Loss2: (0.0000) | Acc: (67.00%) (21568/32128)
Epoch: 15 | Batch_idx: 260 |  Loss: (0.9193) |  Loss2: (0.0000) | Acc: (67.00%) (22478/33408)
Epoch: 15 | Batch_idx: 270 |  Loss: (0.9180) |  Loss2: (0.0000) | Acc: (67.00%) (23356/34688)
Epoch: 15 | Batch_idx: 280 |  Loss: (0.9180) |  Loss2: (0.0000) | Acc: (67.00%) (24211/35968)
Epoch: 15 | Batch_idx: 290 |  Loss: (0.9182) |  Loss2: (0.0000) | Acc: (67.00%) (25072/37248)
Epoch: 15 | Batch_idx: 300 |  Loss: (0.9169) |  Loss2: (0.0000) | Acc: (67.00%) (25944/38528)
Epoch: 15 | Batch_idx: 310 |  Loss: (0.9155) |  Loss2: (0.0000) | Acc: (67.00%) (26826/39808)
Epoch: 15 | Batch_idx: 320 |  Loss: (0.9127) |  Loss2: (0.0000) | Acc: (67.00%) (27735/41088)
Epoch: 15 | Batch_idx: 330 |  Loss: (0.9099) |  Loss2: (0.0000) | Acc: (67.00%) (28627/42368)
Epoch: 15 | Batch_idx: 340 |  Loss: (0.9082) |  Loss2: (0.0000) | Acc: (67.00%) (29537/43648)
Epoch: 15 | Batch_idx: 350 |  Loss: (0.9066) |  Loss2: (0.0000) | Acc: (67.00%) (30418/44928)
Epoch: 15 | Batch_idx: 360 |  Loss: (0.9053) |  Loss2: (0.0000) | Acc: (67.00%) (31303/46208)
Epoch: 15 | Batch_idx: 370 |  Loss: (0.9032) |  Loss2: (0.0000) | Acc: (67.00%) (32231/47488)
Epoch: 15 | Batch_idx: 380 |  Loss: (0.9023) |  Loss2: (0.0000) | Acc: (67.00%) (33106/48768)
Epoch: 15 | Batch_idx: 390 |  Loss: (0.9009) |  Loss2: (0.0000) | Acc: (67.00%) (33971/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_015.pth.tar'
# TEST : Loss: (0.8844) | Acc: (68.00%) (6895/10000)
percent tensor([0.5056, 0.4944], device='cuda:0')
percent tensor([0.4818, 0.5182], device='cuda:0')
percent tensor([0.4738, 0.5262], device='cuda:0')
percent tensor([0.4928, 0.5072], device='cuda:0')
percent tensor([0.4848, 0.5152], device='cuda:0')
percent tensor([0.5345, 0.4655], device='cuda:0')
percent tensor([0.5496, 0.4504], device='cuda:0')
percent tensor([0.8512, 0.1488], device='cuda:0')
Epoch: 16 | Batch_idx: 0 |  Loss: (0.8154) |  Loss2: (0.0000) | Acc: (70.00%) (90/128)
Epoch: 16 | Batch_idx: 10 |  Loss: (0.8686) |  Loss2: (0.0000) | Acc: (69.00%) (978/1408)
Epoch: 16 | Batch_idx: 20 |  Loss: (0.8764) |  Loss2: (0.0000) | Acc: (68.00%) (1853/2688)
Epoch: 16 | Batch_idx: 30 |  Loss: (0.8755) |  Loss2: (0.0000) | Acc: (68.00%) (2729/3968)
Epoch: 16 | Batch_idx: 40 |  Loss: (0.8742) |  Loss2: (0.0000) | Acc: (68.00%) (3595/5248)
Epoch: 16 | Batch_idx: 50 |  Loss: (0.8598) |  Loss2: (0.0000) | Acc: (68.00%) (4499/6528)
Epoch: 16 | Batch_idx: 60 |  Loss: (0.8573) |  Loss2: (0.0000) | Acc: (68.00%) (5385/7808)
Epoch: 16 | Batch_idx: 70 |  Loss: (0.8552) |  Loss2: (0.0000) | Acc: (69.00%) (6280/9088)
Epoch: 16 | Batch_idx: 80 |  Loss: (0.8585) |  Loss2: (0.0000) | Acc: (69.00%) (7165/10368)
Epoch: 16 | Batch_idx: 90 |  Loss: (0.8585) |  Loss2: (0.0000) | Acc: (69.00%) (8051/11648)
Epoch: 16 | Batch_idx: 100 |  Loss: (0.8589) |  Loss2: (0.0000) | Acc: (69.00%) (8937/12928)
Epoch: 16 | Batch_idx: 110 |  Loss: (0.8578) |  Loss2: (0.0000) | Acc: (69.00%) (9833/14208)
Epoch: 16 | Batch_idx: 120 |  Loss: (0.8573) |  Loss2: (0.0000) | Acc: (69.00%) (10729/15488)
Epoch: 16 | Batch_idx: 130 |  Loss: (0.8520) |  Loss2: (0.0000) | Acc: (69.00%) (11664/16768)
Epoch: 16 | Batch_idx: 140 |  Loss: (0.8501) |  Loss2: (0.0000) | Acc: (69.00%) (12567/18048)
Epoch: 16 | Batch_idx: 150 |  Loss: (0.8497) |  Loss2: (0.0000) | Acc: (69.00%) (13450/19328)
Epoch: 16 | Batch_idx: 160 |  Loss: (0.8484) |  Loss2: (0.0000) | Acc: (69.00%) (14348/20608)
Epoch: 16 | Batch_idx: 170 |  Loss: (0.8458) |  Loss2: (0.0000) | Acc: (69.00%) (15272/21888)
Epoch: 16 | Batch_idx: 180 |  Loss: (0.8438) |  Loss2: (0.0000) | Acc: (69.00%) (16179/23168)
Epoch: 16 | Batch_idx: 190 |  Loss: (0.8458) |  Loss2: (0.0000) | Acc: (69.00%) (17061/24448)
Epoch: 16 | Batch_idx: 200 |  Loss: (0.8448) |  Loss2: (0.0000) | Acc: (69.00%) (17987/25728)
Epoch: 16 | Batch_idx: 210 |  Loss: (0.8432) |  Loss2: (0.0000) | Acc: (69.00%) (18886/27008)
Epoch: 16 | Batch_idx: 220 |  Loss: (0.8413) |  Loss2: (0.0000) | Acc: (70.00%) (19810/28288)
Epoch: 16 | Batch_idx: 230 |  Loss: (0.8423) |  Loss2: (0.0000) | Acc: (70.00%) (20708/29568)
Epoch: 16 | Batch_idx: 240 |  Loss: (0.8415) |  Loss2: (0.0000) | Acc: (70.00%) (21622/30848)
Epoch: 16 | Batch_idx: 250 |  Loss: (0.8387) |  Loss2: (0.0000) | Acc: (70.00%) (22545/32128)
Epoch: 16 | Batch_idx: 260 |  Loss: (0.8402) |  Loss2: (0.0000) | Acc: (70.00%) (23444/33408)
Epoch: 16 | Batch_idx: 270 |  Loss: (0.8403) |  Loss2: (0.0000) | Acc: (70.00%) (24335/34688)
Epoch: 16 | Batch_idx: 280 |  Loss: (0.8394) |  Loss2: (0.0000) | Acc: (70.00%) (25239/35968)
Epoch: 16 | Batch_idx: 290 |  Loss: (0.8386) |  Loss2: (0.0000) | Acc: (70.00%) (26157/37248)
Epoch: 16 | Batch_idx: 300 |  Loss: (0.8370) |  Loss2: (0.0000) | Acc: (70.00%) (27074/38528)
Epoch: 16 | Batch_idx: 310 |  Loss: (0.8367) |  Loss2: (0.0000) | Acc: (70.00%) (27973/39808)
Epoch: 16 | Batch_idx: 320 |  Loss: (0.8370) |  Loss2: (0.0000) | Acc: (70.00%) (28865/41088)
Epoch: 16 | Batch_idx: 330 |  Loss: (0.8357) |  Loss2: (0.0000) | Acc: (70.00%) (29779/42368)
Epoch: 16 | Batch_idx: 340 |  Loss: (0.8353) |  Loss2: (0.0000) | Acc: (70.00%) (30671/43648)
Epoch: 16 | Batch_idx: 350 |  Loss: (0.8351) |  Loss2: (0.0000) | Acc: (70.00%) (31566/44928)
Epoch: 16 | Batch_idx: 360 |  Loss: (0.8336) |  Loss2: (0.0000) | Acc: (70.00%) (32488/46208)
Epoch: 16 | Batch_idx: 370 |  Loss: (0.8347) |  Loss2: (0.0000) | Acc: (70.00%) (33371/47488)
Epoch: 16 | Batch_idx: 380 |  Loss: (0.8344) |  Loss2: (0.0000) | Acc: (70.00%) (34293/48768)
Epoch: 16 | Batch_idx: 390 |  Loss: (0.8332) |  Loss2: (0.0000) | Acc: (70.00%) (35175/50000)
# TEST : Loss: (0.8596) | Acc: (70.00%) (7013/10000)
percent tensor([0.5080, 0.4920], device='cuda:0')
percent tensor([0.4814, 0.5186], device='cuda:0')
percent tensor([0.4784, 0.5216], device='cuda:0')
percent tensor([0.4993, 0.5007], device='cuda:0')
percent tensor([0.4832, 0.5168], device='cuda:0')
percent tensor([0.5567, 0.4433], device='cuda:0')
percent tensor([0.5639, 0.4361], device='cuda:0')
percent tensor([0.8990, 0.1010], device='cuda:0')
Epoch: 17 | Batch_idx: 0 |  Loss: (0.7031) |  Loss2: (0.0000) | Acc: (73.00%) (94/128)
Epoch: 17 | Batch_idx: 10 |  Loss: (0.7507) |  Loss2: (0.0000) | Acc: (73.00%) (1029/1408)
Epoch: 17 | Batch_idx: 20 |  Loss: (0.7872) |  Loss2: (0.0000) | Acc: (72.00%) (1937/2688)
Epoch: 17 | Batch_idx: 30 |  Loss: (0.8040) |  Loss2: (0.0000) | Acc: (71.00%) (2841/3968)
Epoch: 17 | Batch_idx: 40 |  Loss: (0.8188) |  Loss2: (0.0000) | Acc: (71.00%) (3727/5248)
Epoch: 17 | Batch_idx: 50 |  Loss: (0.8142) |  Loss2: (0.0000) | Acc: (71.00%) (4655/6528)
Epoch: 17 | Batch_idx: 60 |  Loss: (0.8152) |  Loss2: (0.0000) | Acc: (71.00%) (5544/7808)
Epoch: 17 | Batch_idx: 70 |  Loss: (0.8130) |  Loss2: (0.0000) | Acc: (71.00%) (6461/9088)
Epoch: 17 | Batch_idx: 80 |  Loss: (0.8106) |  Loss2: (0.0000) | Acc: (71.00%) (7387/10368)
Epoch: 17 | Batch_idx: 90 |  Loss: (0.8125) |  Loss2: (0.0000) | Acc: (71.00%) (8273/11648)
Epoch: 17 | Batch_idx: 100 |  Loss: (0.8120) |  Loss2: (0.0000) | Acc: (71.00%) (9184/12928)
Epoch: 17 | Batch_idx: 110 |  Loss: (0.8144) |  Loss2: (0.0000) | Acc: (70.00%) (10085/14208)
Epoch: 17 | Batch_idx: 120 |  Loss: (0.8120) |  Loss2: (0.0000) | Acc: (71.00%) (11010/15488)
Epoch: 17 | Batch_idx: 130 |  Loss: (0.8118) |  Loss2: (0.0000) | Acc: (70.00%) (11901/16768)
Epoch: 17 | Batch_idx: 140 |  Loss: (0.8117) |  Loss2: (0.0000) | Acc: (71.00%) (12822/18048)
Epoch: 17 | Batch_idx: 150 |  Loss: (0.8128) |  Loss2: (0.0000) | Acc: (71.00%) (13726/19328)
Epoch: 17 | Batch_idx: 160 |  Loss: (0.8128) |  Loss2: (0.0000) | Acc: (71.00%) (14636/20608)
Epoch: 17 | Batch_idx: 170 |  Loss: (0.8148) |  Loss2: (0.0000) | Acc: (70.00%) (15524/21888)
Epoch: 17 | Batch_idx: 180 |  Loss: (0.8157) |  Loss2: (0.0000) | Acc: (70.00%) (16425/23168)
Epoch: 17 | Batch_idx: 190 |  Loss: (0.8160) |  Loss2: (0.0000) | Acc: (70.00%) (17344/24448)
Epoch: 17 | Batch_idx: 200 |  Loss: (0.8169) |  Loss2: (0.0000) | Acc: (70.00%) (18249/25728)
Epoch: 17 | Batch_idx: 210 |  Loss: (0.8159) |  Loss2: (0.0000) | Acc: (70.00%) (19158/27008)
Epoch: 17 | Batch_idx: 220 |  Loss: (0.8151) |  Loss2: (0.0000) | Acc: (70.00%) (20067/28288)
Epoch: 17 | Batch_idx: 230 |  Loss: (0.8176) |  Loss2: (0.0000) | Acc: (70.00%) (20955/29568)
Epoch: 17 | Batch_idx: 240 |  Loss: (0.8183) |  Loss2: (0.0000) | Acc: (70.00%) (21849/30848)
Epoch: 17 | Batch_idx: 250 |  Loss: (0.8181) |  Loss2: (0.0000) | Acc: (70.00%) (22762/32128)
Epoch: 17 | Batch_idx: 260 |  Loss: (0.8180) |  Loss2: (0.0000) | Acc: (70.00%) (23671/33408)
Epoch: 17 | Batch_idx: 270 |  Loss: (0.8173) |  Loss2: (0.0000) | Acc: (70.00%) (24585/34688)
Epoch: 17 | Batch_idx: 280 |  Loss: (0.8187) |  Loss2: (0.0000) | Acc: (70.00%) (25491/35968)
Epoch: 17 | Batch_idx: 290 |  Loss: (0.8173) |  Loss2: (0.0000) | Acc: (70.00%) (26415/37248)
Epoch: 17 | Batch_idx: 300 |  Loss: (0.8164) |  Loss2: (0.0000) | Acc: (70.00%) (27338/38528)
Epoch: 17 | Batch_idx: 310 |  Loss: (0.8167) |  Loss2: (0.0000) | Acc: (70.00%) (28239/39808)
Epoch: 17 | Batch_idx: 320 |  Loss: (0.8183) |  Loss2: (0.0000) | Acc: (70.00%) (29127/41088)
Epoch: 17 | Batch_idx: 330 |  Loss: (0.8182) |  Loss2: (0.0000) | Acc: (70.00%) (30019/42368)
Epoch: 17 | Batch_idx: 340 |  Loss: (0.8184) |  Loss2: (0.0000) | Acc: (70.00%) (30922/43648)
Epoch: 17 | Batch_idx: 350 |  Loss: (0.8182) |  Loss2: (0.0000) | Acc: (70.00%) (31819/44928)
Epoch: 17 | Batch_idx: 360 |  Loss: (0.8176) |  Loss2: (0.0000) | Acc: (70.00%) (32750/46208)
Epoch: 17 | Batch_idx: 370 |  Loss: (0.8171) |  Loss2: (0.0000) | Acc: (70.00%) (33668/47488)
Epoch: 17 | Batch_idx: 380 |  Loss: (0.8185) |  Loss2: (0.0000) | Acc: (70.00%) (34552/48768)
Epoch: 17 | Batch_idx: 390 |  Loss: (0.8174) |  Loss2: (0.0000) | Acc: (70.00%) (35439/50000)
# TEST : Loss: (0.8463) | Acc: (70.00%) (7052/10000)
percent tensor([0.5102, 0.4898], device='cuda:0')
percent tensor([0.4838, 0.5162], device='cuda:0')
percent tensor([0.4807, 0.5193], device='cuda:0')
percent tensor([0.5057, 0.4943], device='cuda:0')
percent tensor([0.4848, 0.5152], device='cuda:0')
percent tensor([0.5738, 0.4262], device='cuda:0')
percent tensor([0.5713, 0.4287], device='cuda:0')
percent tensor([0.9140, 0.0860], device='cuda:0')
Epoch: 18 | Batch_idx: 0 |  Loss: (0.7626) |  Loss2: (0.0000) | Acc: (68.00%) (88/128)
Epoch: 18 | Batch_idx: 10 |  Loss: (0.7682) |  Loss2: (0.0000) | Acc: (71.00%) (1012/1408)
Epoch: 18 | Batch_idx: 20 |  Loss: (0.7778) |  Loss2: (0.0000) | Acc: (72.00%) (1946/2688)
Epoch: 18 | Batch_idx: 30 |  Loss: (0.7914) |  Loss2: (0.0000) | Acc: (71.00%) (2850/3968)
Epoch: 18 | Batch_idx: 40 |  Loss: (0.7935) |  Loss2: (0.0000) | Acc: (71.00%) (3762/5248)
Epoch: 18 | Batch_idx: 50 |  Loss: (0.7934) |  Loss2: (0.0000) | Acc: (71.00%) (4676/6528)
Epoch: 18 | Batch_idx: 60 |  Loss: (0.7999) |  Loss2: (0.0000) | Acc: (71.00%) (5590/7808)
Epoch: 18 | Batch_idx: 70 |  Loss: (0.8062) |  Loss2: (0.0000) | Acc: (71.00%) (6480/9088)
Epoch: 18 | Batch_idx: 80 |  Loss: (0.7974) |  Loss2: (0.0000) | Acc: (71.00%) (7417/10368)
Epoch: 18 | Batch_idx: 90 |  Loss: (0.8006) |  Loss2: (0.0000) | Acc: (71.00%) (8327/11648)
Epoch: 18 | Batch_idx: 100 |  Loss: (0.7993) |  Loss2: (0.0000) | Acc: (71.00%) (9245/12928)
Epoch: 18 | Batch_idx: 110 |  Loss: (0.8022) |  Loss2: (0.0000) | Acc: (71.00%) (10161/14208)
Epoch: 18 | Batch_idx: 120 |  Loss: (0.8030) |  Loss2: (0.0000) | Acc: (71.00%) (11078/15488)
Epoch: 18 | Batch_idx: 130 |  Loss: (0.8029) |  Loss2: (0.0000) | Acc: (71.00%) (12000/16768)
Epoch: 18 | Batch_idx: 140 |  Loss: (0.8031) |  Loss2: (0.0000) | Acc: (71.00%) (12900/18048)
Epoch: 18 | Batch_idx: 150 |  Loss: (0.8057) |  Loss2: (0.0000) | Acc: (71.00%) (13799/19328)
Epoch: 18 | Batch_idx: 160 |  Loss: (0.8036) |  Loss2: (0.0000) | Acc: (71.00%) (14713/20608)
Epoch: 18 | Batch_idx: 170 |  Loss: (0.8027) |  Loss2: (0.0000) | Acc: (71.00%) (15621/21888)
Epoch: 18 | Batch_idx: 180 |  Loss: (0.8018) |  Loss2: (0.0000) | Acc: (71.00%) (16552/23168)
Epoch: 18 | Batch_idx: 190 |  Loss: (0.8045) |  Loss2: (0.0000) | Acc: (71.00%) (17450/24448)
Epoch: 18 | Batch_idx: 200 |  Loss: (0.8039) |  Loss2: (0.0000) | Acc: (71.00%) (18371/25728)
Epoch: 18 | Batch_idx: 210 |  Loss: (0.8045) |  Loss2: (0.0000) | Acc: (71.00%) (19287/27008)
Epoch: 18 | Batch_idx: 220 |  Loss: (0.8033) |  Loss2: (0.0000) | Acc: (71.00%) (20220/28288)
Epoch: 18 | Batch_idx: 230 |  Loss: (0.8027) |  Loss2: (0.0000) | Acc: (71.00%) (21120/29568)
Epoch: 18 | Batch_idx: 240 |  Loss: (0.8045) |  Loss2: (0.0000) | Acc: (71.00%) (22027/30848)
Epoch: 18 | Batch_idx: 250 |  Loss: (0.8031) |  Loss2: (0.0000) | Acc: (71.00%) (22957/32128)
Epoch: 18 | Batch_idx: 260 |  Loss: (0.8030) |  Loss2: (0.0000) | Acc: (71.00%) (23862/33408)
Epoch: 18 | Batch_idx: 270 |  Loss: (0.8030) |  Loss2: (0.0000) | Acc: (71.00%) (24773/34688)
Epoch: 18 | Batch_idx: 280 |  Loss: (0.8032) |  Loss2: (0.0000) | Acc: (71.00%) (25677/35968)
Epoch: 18 | Batch_idx: 290 |  Loss: (0.8032) |  Loss2: (0.0000) | Acc: (71.00%) (26592/37248)
Epoch: 18 | Batch_idx: 300 |  Loss: (0.8039) |  Loss2: (0.0000) | Acc: (71.00%) (27481/38528)
Epoch: 18 | Batch_idx: 310 |  Loss: (0.8031) |  Loss2: (0.0000) | Acc: (71.00%) (28408/39808)
Epoch: 18 | Batch_idx: 320 |  Loss: (0.8022) |  Loss2: (0.0000) | Acc: (71.00%) (29328/41088)
Epoch: 18 | Batch_idx: 330 |  Loss: (0.8034) |  Loss2: (0.0000) | Acc: (71.00%) (30196/42368)
Epoch: 18 | Batch_idx: 340 |  Loss: (0.8042) |  Loss2: (0.0000) | Acc: (71.00%) (31111/43648)
Epoch: 18 | Batch_idx: 350 |  Loss: (0.8037) |  Loss2: (0.0000) | Acc: (71.00%) (32029/44928)
Epoch: 18 | Batch_idx: 360 |  Loss: (0.8031) |  Loss2: (0.0000) | Acc: (71.00%) (32954/46208)
Epoch: 18 | Batch_idx: 370 |  Loss: (0.8043) |  Loss2: (0.0000) | Acc: (71.00%) (33865/47488)
Epoch: 18 | Batch_idx: 380 |  Loss: (0.8044) |  Loss2: (0.0000) | Acc: (71.00%) (34763/48768)
Epoch: 18 | Batch_idx: 390 |  Loss: (0.8051) |  Loss2: (0.0000) | Acc: (71.00%) (35620/50000)
# TEST : Loss: (0.8359) | Acc: (70.00%) (7081/10000)
percent tensor([0.5149, 0.4851], device='cuda:0')
percent tensor([0.4855, 0.5145], device='cuda:0')
percent tensor([0.4817, 0.5183], device='cuda:0')
percent tensor([0.5108, 0.4892], device='cuda:0')
percent tensor([0.4873, 0.5127], device='cuda:0')
percent tensor([0.5894, 0.4106], device='cuda:0')
percent tensor([0.5747, 0.4253], device='cuda:0')
percent tensor([0.9279, 0.0721], device='cuda:0')
Epoch: 19 | Batch_idx: 0 |  Loss: (0.7422) |  Loss2: (0.0000) | Acc: (77.00%) (99/128)
Epoch: 19 | Batch_idx: 10 |  Loss: (0.8174) |  Loss2: (0.0000) | Acc: (70.00%) (992/1408)
Epoch: 19 | Batch_idx: 20 |  Loss: (0.8056) |  Loss2: (0.0000) | Acc: (70.00%) (1900/2688)
Epoch: 19 | Batch_idx: 30 |  Loss: (0.7957) |  Loss2: (0.0000) | Acc: (71.00%) (2835/3968)
Epoch: 19 | Batch_idx: 40 |  Loss: (0.7842) |  Loss2: (0.0000) | Acc: (71.00%) (3762/5248)
Epoch: 19 | Batch_idx: 50 |  Loss: (0.7857) |  Loss2: (0.0000) | Acc: (71.00%) (4682/6528)
Epoch: 19 | Batch_idx: 60 |  Loss: (0.7855) |  Loss2: (0.0000) | Acc: (71.00%) (5606/7808)
Epoch: 19 | Batch_idx: 70 |  Loss: (0.7901) |  Loss2: (0.0000) | Acc: (71.00%) (6517/9088)
Epoch: 19 | Batch_idx: 80 |  Loss: (0.7894) |  Loss2: (0.0000) | Acc: (71.00%) (7443/10368)
Epoch: 19 | Batch_idx: 90 |  Loss: (0.7859) |  Loss2: (0.0000) | Acc: (71.00%) (8377/11648)
Epoch: 19 | Batch_idx: 100 |  Loss: (0.7899) |  Loss2: (0.0000) | Acc: (71.00%) (9290/12928)
Epoch: 19 | Batch_idx: 110 |  Loss: (0.7882) |  Loss2: (0.0000) | Acc: (71.00%) (10227/14208)
Epoch: 19 | Batch_idx: 120 |  Loss: (0.7909) |  Loss2: (0.0000) | Acc: (72.00%) (11162/15488)
Epoch: 19 | Batch_idx: 130 |  Loss: (0.7950) |  Loss2: (0.0000) | Acc: (71.00%) (12061/16768)
Epoch: 19 | Batch_idx: 140 |  Loss: (0.7961) |  Loss2: (0.0000) | Acc: (71.00%) (12973/18048)
Epoch: 19 | Batch_idx: 150 |  Loss: (0.8005) |  Loss2: (0.0000) | Acc: (71.00%) (13860/19328)
Epoch: 19 | Batch_idx: 160 |  Loss: (0.8015) |  Loss2: (0.0000) | Acc: (71.00%) (14767/20608)
Epoch: 19 | Batch_idx: 170 |  Loss: (0.8004) |  Loss2: (0.0000) | Acc: (71.00%) (15697/21888)
Epoch: 19 | Batch_idx: 180 |  Loss: (0.8016) |  Loss2: (0.0000) | Acc: (71.00%) (16599/23168)
Epoch: 19 | Batch_idx: 190 |  Loss: (0.8007) |  Loss2: (0.0000) | Acc: (71.00%) (17524/24448)
Epoch: 19 | Batch_idx: 200 |  Loss: (0.7992) |  Loss2: (0.0000) | Acc: (71.00%) (18470/25728)
Epoch: 19 | Batch_idx: 210 |  Loss: (0.7988) |  Loss2: (0.0000) | Acc: (71.00%) (19398/27008)
Epoch: 19 | Batch_idx: 220 |  Loss: (0.7981) |  Loss2: (0.0000) | Acc: (71.00%) (20337/28288)
Epoch: 19 | Batch_idx: 230 |  Loss: (0.7999) |  Loss2: (0.0000) | Acc: (71.00%) (21233/29568)
Epoch: 19 | Batch_idx: 240 |  Loss: (0.8006) |  Loss2: (0.0000) | Acc: (71.00%) (22133/30848)
Epoch: 19 | Batch_idx: 250 |  Loss: (0.7996) |  Loss2: (0.0000) | Acc: (71.00%) (23078/32128)
Epoch: 19 | Batch_idx: 260 |  Loss: (0.8004) |  Loss2: (0.0000) | Acc: (71.00%) (23969/33408)
Epoch: 19 | Batch_idx: 270 |  Loss: (0.8002) |  Loss2: (0.0000) | Acc: (71.00%) (24873/34688)
Epoch: 19 | Batch_idx: 280 |  Loss: (0.8000) |  Loss2: (0.0000) | Acc: (71.00%) (25789/35968)
Epoch: 19 | Batch_idx: 290 |  Loss: (0.8007) |  Loss2: (0.0000) | Acc: (71.00%) (26700/37248)
Epoch: 19 | Batch_idx: 300 |  Loss: (0.8011) |  Loss2: (0.0000) | Acc: (71.00%) (27624/38528)
Epoch: 19 | Batch_idx: 310 |  Loss: (0.8018) |  Loss2: (0.0000) | Acc: (71.00%) (28521/39808)
Epoch: 19 | Batch_idx: 320 |  Loss: (0.8023) |  Loss2: (0.0000) | Acc: (71.00%) (29424/41088)
Epoch: 19 | Batch_idx: 330 |  Loss: (0.8026) |  Loss2: (0.0000) | Acc: (71.00%) (30344/42368)
Epoch: 19 | Batch_idx: 340 |  Loss: (0.8007) |  Loss2: (0.0000) | Acc: (71.00%) (31285/43648)
Epoch: 19 | Batch_idx: 350 |  Loss: (0.8007) |  Loss2: (0.0000) | Acc: (71.00%) (32205/44928)
Epoch: 19 | Batch_idx: 360 |  Loss: (0.7997) |  Loss2: (0.0000) | Acc: (71.00%) (33139/46208)
Epoch: 19 | Batch_idx: 370 |  Loss: (0.7993) |  Loss2: (0.0000) | Acc: (71.00%) (34086/47488)
Epoch: 19 | Batch_idx: 380 |  Loss: (0.7978) |  Loss2: (0.0000) | Acc: (71.00%) (35032/48768)
Epoch: 19 | Batch_idx: 390 |  Loss: (0.7983) |  Loss2: (0.0000) | Acc: (71.00%) (35924/50000)
# TEST : Loss: (0.8294) | Acc: (71.00%) (7109/10000)
percent tensor([0.5174, 0.4826], device='cuda:0')
percent tensor([0.4867, 0.5133], device='cuda:0')
percent tensor([0.4823, 0.5177], device='cuda:0')
percent tensor([0.5139, 0.4861], device='cuda:0')
percent tensor([0.4895, 0.5105], device='cuda:0')
percent tensor([0.6003, 0.3997], device='cuda:0')
percent tensor([0.5767, 0.4233], device='cuda:0')
percent tensor([0.9370, 0.0630], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 20 | Batch_idx: 0 |  Loss: (0.7470) |  Loss2: (0.0000) | Acc: (75.00%) (97/128)
Epoch: 20 | Batch_idx: 10 |  Loss: (0.8168) |  Loss2: (0.0000) | Acc: (70.00%) (990/1408)
Epoch: 20 | Batch_idx: 20 |  Loss: (0.8151) |  Loss2: (0.0000) | Acc: (71.00%) (1920/2688)
Epoch: 20 | Batch_idx: 30 |  Loss: (0.8209) |  Loss2: (0.0000) | Acc: (71.00%) (2824/3968)
Epoch: 20 | Batch_idx: 40 |  Loss: (0.8227) |  Loss2: (0.0000) | Acc: (70.00%) (3716/5248)
Epoch: 20 | Batch_idx: 50 |  Loss: (0.8223) |  Loss2: (0.0000) | Acc: (70.00%) (4614/6528)
Epoch: 20 | Batch_idx: 60 |  Loss: (0.8167) |  Loss2: (0.0000) | Acc: (70.00%) (5539/7808)
Epoch: 20 | Batch_idx: 70 |  Loss: (0.8194) |  Loss2: (0.0000) | Acc: (70.00%) (6437/9088)
Epoch: 20 | Batch_idx: 80 |  Loss: (0.8195) |  Loss2: (0.0000) | Acc: (70.00%) (7340/10368)
Epoch: 20 | Batch_idx: 90 |  Loss: (0.8205) |  Loss2: (0.0000) | Acc: (70.00%) (8249/11648)
Epoch: 20 | Batch_idx: 100 |  Loss: (0.8188) |  Loss2: (0.0000) | Acc: (70.00%) (9145/12928)
Epoch: 20 | Batch_idx: 110 |  Loss: (0.8211) |  Loss2: (0.0000) | Acc: (70.00%) (10045/14208)
Epoch: 20 | Batch_idx: 120 |  Loss: (0.8195) |  Loss2: (0.0000) | Acc: (70.00%) (10967/15488)
Epoch: 20 | Batch_idx: 130 |  Loss: (0.8162) |  Loss2: (0.0000) | Acc: (70.00%) (11898/16768)
Epoch: 20 | Batch_idx: 140 |  Loss: (0.8112) |  Loss2: (0.0000) | Acc: (71.00%) (12858/18048)
Epoch: 20 | Batch_idx: 150 |  Loss: (0.8075) |  Loss2: (0.0000) | Acc: (71.00%) (13799/19328)
Epoch: 20 | Batch_idx: 160 |  Loss: (0.8071) |  Loss2: (0.0000) | Acc: (71.00%) (14725/20608)
Epoch: 20 | Batch_idx: 170 |  Loss: (0.8062) |  Loss2: (0.0000) | Acc: (71.00%) (15637/21888)
Epoch: 20 | Batch_idx: 180 |  Loss: (0.8044) |  Loss2: (0.0000) | Acc: (71.00%) (16571/23168)
Epoch: 20 | Batch_idx: 190 |  Loss: (0.8044) |  Loss2: (0.0000) | Acc: (71.00%) (17490/24448)
Epoch: 20 | Batch_idx: 200 |  Loss: (0.8020) |  Loss2: (0.0000) | Acc: (71.00%) (18441/25728)
Epoch: 20 | Batch_idx: 210 |  Loss: (0.8016) |  Loss2: (0.0000) | Acc: (71.00%) (19374/27008)
Epoch: 20 | Batch_idx: 220 |  Loss: (0.8002) |  Loss2: (0.0000) | Acc: (71.00%) (20308/28288)
Epoch: 20 | Batch_idx: 230 |  Loss: (0.7985) |  Loss2: (0.0000) | Acc: (71.00%) (21231/29568)
Epoch: 20 | Batch_idx: 240 |  Loss: (0.7975) |  Loss2: (0.0000) | Acc: (71.00%) (22171/30848)
Epoch: 20 | Batch_idx: 250 |  Loss: (0.7962) |  Loss2: (0.0000) | Acc: (71.00%) (23092/32128)
Epoch: 20 | Batch_idx: 260 |  Loss: (0.7963) |  Loss2: (0.0000) | Acc: (71.00%) (24024/33408)
Epoch: 20 | Batch_idx: 270 |  Loss: (0.7973) |  Loss2: (0.0000) | Acc: (71.00%) (24939/34688)
Epoch: 20 | Batch_idx: 280 |  Loss: (0.7955) |  Loss2: (0.0000) | Acc: (71.00%) (25878/35968)
Epoch: 20 | Batch_idx: 290 |  Loss: (0.7942) |  Loss2: (0.0000) | Acc: (71.00%) (26810/37248)
Epoch: 20 | Batch_idx: 300 |  Loss: (0.7924) |  Loss2: (0.0000) | Acc: (72.00%) (27762/38528)
Epoch: 20 | Batch_idx: 310 |  Loss: (0.7912) |  Loss2: (0.0000) | Acc: (72.00%) (28710/39808)
Epoch: 20 | Batch_idx: 320 |  Loss: (0.7914) |  Loss2: (0.0000) | Acc: (72.00%) (29631/41088)
Epoch: 20 | Batch_idx: 330 |  Loss: (0.7913) |  Loss2: (0.0000) | Acc: (72.00%) (30545/42368)
Epoch: 20 | Batch_idx: 340 |  Loss: (0.7903) |  Loss2: (0.0000) | Acc: (72.00%) (31487/43648)
Epoch: 20 | Batch_idx: 350 |  Loss: (0.7914) |  Loss2: (0.0000) | Acc: (72.00%) (32365/44928)
Epoch: 20 | Batch_idx: 360 |  Loss: (0.7908) |  Loss2: (0.0000) | Acc: (72.00%) (33297/46208)
Epoch: 20 | Batch_idx: 370 |  Loss: (0.7900) |  Loss2: (0.0000) | Acc: (72.00%) (34238/47488)
Epoch: 20 | Batch_idx: 380 |  Loss: (0.7902) |  Loss2: (0.0000) | Acc: (72.00%) (35154/48768)
Epoch: 20 | Batch_idx: 390 |  Loss: (0.7897) |  Loss2: (0.0000) | Acc: (72.00%) (36052/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_020.pth.tar'
# TEST : Loss: (0.8517) | Acc: (71.00%) (7114/10000)
percent tensor([0.5174, 0.4826], device='cuda:0')
percent tensor([0.4868, 0.5132], device='cuda:0')
percent tensor([0.4825, 0.5175], device='cuda:0')
percent tensor([0.5137, 0.4863], device='cuda:0')
percent tensor([0.4894, 0.5106], device='cuda:0')
percent tensor([0.6013, 0.3987], device='cuda:0')
percent tensor([0.5752, 0.4248], device='cuda:0')
percent tensor([0.9364, 0.0636], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(170.9641, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(787.3159, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(778.2573, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1525.5526, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(517.2247, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2172.2354, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4304.9199, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1442.8206, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6095.0156, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12176.5576, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4064.1052, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(17199.0586, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 21 | Batch_idx: 0 |  Loss: (0.6720) |  Loss2: (0.0000) | Acc: (76.00%) (98/128)
Epoch: 21 | Batch_idx: 10 |  Loss: (0.7189) |  Loss2: (0.0000) | Acc: (74.00%) (1053/1408)
Epoch: 21 | Batch_idx: 20 |  Loss: (0.7200) |  Loss2: (0.0000) | Acc: (74.00%) (2007/2688)
Epoch: 21 | Batch_idx: 30 |  Loss: (0.7138) |  Loss2: (0.0000) | Acc: (74.00%) (2975/3968)
Epoch: 21 | Batch_idx: 40 |  Loss: (0.7096) |  Loss2: (0.0000) | Acc: (75.00%) (3955/5248)
Epoch: 21 | Batch_idx: 50 |  Loss: (0.7132) |  Loss2: (0.0000) | Acc: (75.00%) (4899/6528)
Epoch: 21 | Batch_idx: 60 |  Loss: (0.7202) |  Loss2: (0.0000) | Acc: (74.00%) (5841/7808)
Epoch: 21 | Batch_idx: 70 |  Loss: (0.7210) |  Loss2: (0.0000) | Acc: (74.00%) (6799/9088)
Epoch: 21 | Batch_idx: 80 |  Loss: (0.7259) |  Loss2: (0.0000) | Acc: (74.00%) (7725/10368)
Epoch: 21 | Batch_idx: 90 |  Loss: (0.7260) |  Loss2: (0.0000) | Acc: (74.00%) (8680/11648)
Epoch: 21 | Batch_idx: 100 |  Loss: (0.7310) |  Loss2: (0.0000) | Acc: (74.00%) (9608/12928)
Epoch: 21 | Batch_idx: 110 |  Loss: (0.7304) |  Loss2: (0.0000) | Acc: (74.00%) (10554/14208)
Epoch: 21 | Batch_idx: 120 |  Loss: (0.7345) |  Loss2: (0.0000) | Acc: (74.00%) (11486/15488)
Epoch: 21 | Batch_idx: 130 |  Loss: (0.7312) |  Loss2: (0.0000) | Acc: (74.00%) (12451/16768)
Epoch: 21 | Batch_idx: 140 |  Loss: (0.7293) |  Loss2: (0.0000) | Acc: (74.00%) (13423/18048)
Epoch: 21 | Batch_idx: 150 |  Loss: (0.7299) |  Loss2: (0.0000) | Acc: (74.00%) (14375/19328)
Epoch: 21 | Batch_idx: 160 |  Loss: (0.7337) |  Loss2: (0.0000) | Acc: (74.00%) (15306/20608)
Epoch: 21 | Batch_idx: 170 |  Loss: (0.7357) |  Loss2: (0.0000) | Acc: (74.00%) (16234/21888)
Epoch: 21 | Batch_idx: 180 |  Loss: (0.7344) |  Loss2: (0.0000) | Acc: (74.00%) (17186/23168)
Epoch: 21 | Batch_idx: 190 |  Loss: (0.7323) |  Loss2: (0.0000) | Acc: (74.00%) (18153/24448)
Epoch: 21 | Batch_idx: 200 |  Loss: (0.7325) |  Loss2: (0.0000) | Acc: (74.00%) (19107/25728)
Epoch: 21 | Batch_idx: 210 |  Loss: (0.7323) |  Loss2: (0.0000) | Acc: (74.00%) (20069/27008)
Epoch: 21 | Batch_idx: 220 |  Loss: (0.7354) |  Loss2: (0.0000) | Acc: (74.00%) (20977/28288)
Epoch: 21 | Batch_idx: 230 |  Loss: (0.7370) |  Loss2: (0.0000) | Acc: (74.00%) (21901/29568)
Epoch: 21 | Batch_idx: 240 |  Loss: (0.7364) |  Loss2: (0.0000) | Acc: (74.00%) (22849/30848)
Epoch: 21 | Batch_idx: 250 |  Loss: (0.7361) |  Loss2: (0.0000) | Acc: (74.00%) (23811/32128)
Epoch: 21 | Batch_idx: 260 |  Loss: (0.7359) |  Loss2: (0.0000) | Acc: (74.00%) (24754/33408)
Epoch: 21 | Batch_idx: 270 |  Loss: (0.7377) |  Loss2: (0.0000) | Acc: (74.00%) (25683/34688)
Epoch: 21 | Batch_idx: 280 |  Loss: (0.7362) |  Loss2: (0.0000) | Acc: (74.00%) (26665/35968)
Epoch: 21 | Batch_idx: 290 |  Loss: (0.7363) |  Loss2: (0.0000) | Acc: (74.00%) (27617/37248)
Epoch: 21 | Batch_idx: 300 |  Loss: (0.7359) |  Loss2: (0.0000) | Acc: (74.00%) (28584/38528)
Epoch: 21 | Batch_idx: 310 |  Loss: (0.7358) |  Loss2: (0.0000) | Acc: (74.00%) (29514/39808)
Epoch: 21 | Batch_idx: 320 |  Loss: (0.7347) |  Loss2: (0.0000) | Acc: (74.00%) (30475/41088)
Epoch: 21 | Batch_idx: 330 |  Loss: (0.7362) |  Loss2: (0.0000) | Acc: (74.00%) (31417/42368)
Epoch: 21 | Batch_idx: 340 |  Loss: (0.7355) |  Loss2: (0.0000) | Acc: (74.00%) (32373/43648)
Epoch: 21 | Batch_idx: 350 |  Loss: (0.7356) |  Loss2: (0.0000) | Acc: (74.00%) (33316/44928)
Epoch: 21 | Batch_idx: 360 |  Loss: (0.7360) |  Loss2: (0.0000) | Acc: (74.00%) (34262/46208)
Epoch: 21 | Batch_idx: 370 |  Loss: (0.7361) |  Loss2: (0.0000) | Acc: (74.00%) (35209/47488)
Epoch: 21 | Batch_idx: 380 |  Loss: (0.7367) |  Loss2: (0.0000) | Acc: (74.00%) (36139/48768)
Epoch: 21 | Batch_idx: 390 |  Loss: (0.7359) |  Loss2: (0.0000) | Acc: (74.00%) (37054/50000)
# TEST : Loss: (0.7980) | Acc: (72.00%) (7237/10000)
percent tensor([0.5173, 0.4827], device='cuda:0')
percent tensor([0.4867, 0.5133], device='cuda:0')
percent tensor([0.4820, 0.5180], device='cuda:0')
percent tensor([0.5138, 0.4862], device='cuda:0')
percent tensor([0.4893, 0.5107], device='cuda:0')
percent tensor([0.6028, 0.3972], device='cuda:0')
percent tensor([0.5801, 0.4199], device='cuda:0')
percent tensor([0.9444, 0.0556], device='cuda:0')
Epoch: 22 | Batch_idx: 0 |  Loss: (0.7335) |  Loss2: (0.0000) | Acc: (75.00%) (96/128)
Epoch: 22 | Batch_idx: 10 |  Loss: (0.7312) |  Loss2: (0.0000) | Acc: (74.00%) (1053/1408)
Epoch: 22 | Batch_idx: 20 |  Loss: (0.6843) |  Loss2: (0.0000) | Acc: (76.00%) (2057/2688)
Epoch: 22 | Batch_idx: 30 |  Loss: (0.6915) |  Loss2: (0.0000) | Acc: (75.00%) (3011/3968)
Epoch: 22 | Batch_idx: 40 |  Loss: (0.6922) |  Loss2: (0.0000) | Acc: (75.00%) (3980/5248)
Epoch: 22 | Batch_idx: 50 |  Loss: (0.6932) |  Loss2: (0.0000) | Acc: (75.00%) (4948/6528)
Epoch: 22 | Batch_idx: 60 |  Loss: (0.7002) |  Loss2: (0.0000) | Acc: (75.00%) (5897/7808)
Epoch: 22 | Batch_idx: 70 |  Loss: (0.7024) |  Loss2: (0.0000) | Acc: (75.00%) (6871/9088)
Epoch: 22 | Batch_idx: 80 |  Loss: (0.6962) |  Loss2: (0.0000) | Acc: (75.00%) (7867/10368)
Epoch: 22 | Batch_idx: 90 |  Loss: (0.6958) |  Loss2: (0.0000) | Acc: (75.00%) (8831/11648)
Epoch: 22 | Batch_idx: 100 |  Loss: (0.6961) |  Loss2: (0.0000) | Acc: (75.00%) (9799/12928)
Epoch: 22 | Batch_idx: 110 |  Loss: (0.6964) |  Loss2: (0.0000) | Acc: (75.00%) (10752/14208)
Epoch: 22 | Batch_idx: 120 |  Loss: (0.6984) |  Loss2: (0.0000) | Acc: (75.00%) (11716/15488)
Epoch: 22 | Batch_idx: 130 |  Loss: (0.7001) |  Loss2: (0.0000) | Acc: (75.00%) (12679/16768)
Epoch: 22 | Batch_idx: 140 |  Loss: (0.6977) |  Loss2: (0.0000) | Acc: (75.00%) (13669/18048)
Epoch: 22 | Batch_idx: 150 |  Loss: (0.6966) |  Loss2: (0.0000) | Acc: (75.00%) (14652/19328)
Epoch: 22 | Batch_idx: 160 |  Loss: (0.6987) |  Loss2: (0.0000) | Acc: (75.00%) (15609/20608)
Epoch: 22 | Batch_idx: 170 |  Loss: (0.6986) |  Loss2: (0.0000) | Acc: (75.00%) (16568/21888)
Epoch: 22 | Batch_idx: 180 |  Loss: (0.6987) |  Loss2: (0.0000) | Acc: (75.00%) (17533/23168)
Epoch: 22 | Batch_idx: 190 |  Loss: (0.6974) |  Loss2: (0.0000) | Acc: (75.00%) (18519/24448)
Epoch: 22 | Batch_idx: 200 |  Loss: (0.6972) |  Loss2: (0.0000) | Acc: (75.00%) (19476/25728)
Epoch: 22 | Batch_idx: 210 |  Loss: (0.6957) |  Loss2: (0.0000) | Acc: (75.00%) (20444/27008)
Epoch: 22 | Batch_idx: 220 |  Loss: (0.6948) |  Loss2: (0.0000) | Acc: (75.00%) (21431/28288)
Epoch: 22 | Batch_idx: 230 |  Loss: (0.6961) |  Loss2: (0.0000) | Acc: (75.00%) (22401/29568)
Epoch: 22 | Batch_idx: 240 |  Loss: (0.6962) |  Loss2: (0.0000) | Acc: (75.00%) (23372/30848)
Epoch: 22 | Batch_idx: 250 |  Loss: (0.6963) |  Loss2: (0.0000) | Acc: (75.00%) (24340/32128)
Epoch: 22 | Batch_idx: 260 |  Loss: (0.6970) |  Loss2: (0.0000) | Acc: (75.00%) (25295/33408)
Epoch: 22 | Batch_idx: 270 |  Loss: (0.6963) |  Loss2: (0.0000) | Acc: (75.00%) (26257/34688)
Epoch: 22 | Batch_idx: 280 |  Loss: (0.6964) |  Loss2: (0.0000) | Acc: (75.00%) (27228/35968)
Epoch: 22 | Batch_idx: 290 |  Loss: (0.6964) |  Loss2: (0.0000) | Acc: (75.00%) (28190/37248)
Epoch: 22 | Batch_idx: 300 |  Loss: (0.6964) |  Loss2: (0.0000) | Acc: (75.00%) (29164/38528)
Epoch: 22 | Batch_idx: 310 |  Loss: (0.6970) |  Loss2: (0.0000) | Acc: (75.00%) (30119/39808)
Epoch: 22 | Batch_idx: 320 |  Loss: (0.6957) |  Loss2: (0.0000) | Acc: (75.00%) (31111/41088)
Epoch: 22 | Batch_idx: 330 |  Loss: (0.6962) |  Loss2: (0.0000) | Acc: (75.00%) (32065/42368)
Epoch: 22 | Batch_idx: 340 |  Loss: (0.6952) |  Loss2: (0.0000) | Acc: (75.00%) (33058/43648)
Epoch: 22 | Batch_idx: 350 |  Loss: (0.6952) |  Loss2: (0.0000) | Acc: (75.00%) (34025/44928)
Epoch: 22 | Batch_idx: 360 |  Loss: (0.6937) |  Loss2: (0.0000) | Acc: (75.00%) (35032/46208)
Epoch: 22 | Batch_idx: 370 |  Loss: (0.6925) |  Loss2: (0.0000) | Acc: (75.00%) (36018/47488)
Epoch: 22 | Batch_idx: 380 |  Loss: (0.6912) |  Loss2: (0.0000) | Acc: (75.00%) (37002/48768)
Epoch: 22 | Batch_idx: 390 |  Loss: (0.6903) |  Loss2: (0.0000) | Acc: (75.00%) (37949/50000)
# TEST : Loss: (0.7912) | Acc: (73.00%) (7318/10000)
percent tensor([0.5173, 0.4827], device='cuda:0')
percent tensor([0.4867, 0.5133], device='cuda:0')
percent tensor([0.4824, 0.5176], device='cuda:0')
percent tensor([0.5134, 0.4866], device='cuda:0')
percent tensor([0.4891, 0.5109], device='cuda:0')
percent tensor([0.6018, 0.3982], device='cuda:0')
percent tensor([0.5792, 0.4208], device='cuda:0')
percent tensor([0.9422, 0.0578], device='cuda:0')
Epoch: 23 | Batch_idx: 0 |  Loss: (0.6047) |  Loss2: (0.0000) | Acc: (75.00%) (96/128)
Epoch: 23 | Batch_idx: 10 |  Loss: (0.6894) |  Loss2: (0.0000) | Acc: (76.00%) (1071/1408)
Epoch: 23 | Batch_idx: 20 |  Loss: (0.6515) |  Loss2: (0.0000) | Acc: (77.00%) (2087/2688)
Epoch: 23 | Batch_idx: 30 |  Loss: (0.6540) |  Loss2: (0.0000) | Acc: (77.00%) (3071/3968)
Epoch: 23 | Batch_idx: 40 |  Loss: (0.6606) |  Loss2: (0.0000) | Acc: (77.00%) (4045/5248)
Epoch: 23 | Batch_idx: 50 |  Loss: (0.6582) |  Loss2: (0.0000) | Acc: (77.00%) (5038/6528)
Epoch: 23 | Batch_idx: 60 |  Loss: (0.6557) |  Loss2: (0.0000) | Acc: (77.00%) (6031/7808)
Epoch: 23 | Batch_idx: 70 |  Loss: (0.6544) |  Loss2: (0.0000) | Acc: (77.00%) (7034/9088)
Epoch: 23 | Batch_idx: 80 |  Loss: (0.6476) |  Loss2: (0.0000) | Acc: (77.00%) (8059/10368)
Epoch: 23 | Batch_idx: 90 |  Loss: (0.6495) |  Loss2: (0.0000) | Acc: (77.00%) (9039/11648)
Epoch: 23 | Batch_idx: 100 |  Loss: (0.6540) |  Loss2: (0.0000) | Acc: (77.00%) (9996/12928)
Epoch: 23 | Batch_idx: 110 |  Loss: (0.6546) |  Loss2: (0.0000) | Acc: (77.00%) (10982/14208)
Epoch: 23 | Batch_idx: 120 |  Loss: (0.6555) |  Loss2: (0.0000) | Acc: (77.00%) (11951/15488)
Epoch: 23 | Batch_idx: 130 |  Loss: (0.6537) |  Loss2: (0.0000) | Acc: (77.00%) (12971/16768)
Epoch: 23 | Batch_idx: 140 |  Loss: (0.6545) |  Loss2: (0.0000) | Acc: (77.00%) (13956/18048)
Epoch: 23 | Batch_idx: 150 |  Loss: (0.6536) |  Loss2: (0.0000) | Acc: (77.00%) (14941/19328)
Epoch: 23 | Batch_idx: 160 |  Loss: (0.6560) |  Loss2: (0.0000) | Acc: (77.00%) (15927/20608)
Epoch: 23 | Batch_idx: 170 |  Loss: (0.6598) |  Loss2: (0.0000) | Acc: (77.00%) (16882/21888)
Epoch: 23 | Batch_idx: 180 |  Loss: (0.6603) |  Loss2: (0.0000) | Acc: (77.00%) (17880/23168)
Epoch: 23 | Batch_idx: 190 |  Loss: (0.6593) |  Loss2: (0.0000) | Acc: (77.00%) (18877/24448)
Epoch: 23 | Batch_idx: 200 |  Loss: (0.6570) |  Loss2: (0.0000) | Acc: (77.00%) (19876/25728)
Epoch: 23 | Batch_idx: 210 |  Loss: (0.6558) |  Loss2: (0.0000) | Acc: (77.00%) (20882/27008)
Epoch: 23 | Batch_idx: 220 |  Loss: (0.6558) |  Loss2: (0.0000) | Acc: (77.00%) (21868/28288)
Epoch: 23 | Batch_idx: 230 |  Loss: (0.6543) |  Loss2: (0.0000) | Acc: (77.00%) (22862/29568)
Epoch: 23 | Batch_idx: 240 |  Loss: (0.6544) |  Loss2: (0.0000) | Acc: (77.00%) (23851/30848)
Epoch: 23 | Batch_idx: 250 |  Loss: (0.6541) |  Loss2: (0.0000) | Acc: (77.00%) (24842/32128)
Epoch: 23 | Batch_idx: 260 |  Loss: (0.6534) |  Loss2: (0.0000) | Acc: (77.00%) (25846/33408)
Epoch: 23 | Batch_idx: 270 |  Loss: (0.6532) |  Loss2: (0.0000) | Acc: (77.00%) (26826/34688)
Epoch: 23 | Batch_idx: 280 |  Loss: (0.6515) |  Loss2: (0.0000) | Acc: (77.00%) (27837/35968)
Epoch: 23 | Batch_idx: 290 |  Loss: (0.6489) |  Loss2: (0.0000) | Acc: (77.00%) (28849/37248)
Epoch: 23 | Batch_idx: 300 |  Loss: (0.6493) |  Loss2: (0.0000) | Acc: (77.00%) (29830/38528)
Epoch: 23 | Batch_idx: 310 |  Loss: (0.6505) |  Loss2: (0.0000) | Acc: (77.00%) (30801/39808)
Epoch: 23 | Batch_idx: 320 |  Loss: (0.6513) |  Loss2: (0.0000) | Acc: (77.00%) (31778/41088)
Epoch: 23 | Batch_idx: 330 |  Loss: (0.6524) |  Loss2: (0.0000) | Acc: (77.00%) (32766/42368)
Epoch: 23 | Batch_idx: 340 |  Loss: (0.6529) |  Loss2: (0.0000) | Acc: (77.00%) (33747/43648)
Epoch: 23 | Batch_idx: 350 |  Loss: (0.6527) |  Loss2: (0.0000) | Acc: (77.00%) (34719/44928)
Epoch: 23 | Batch_idx: 360 |  Loss: (0.6538) |  Loss2: (0.0000) | Acc: (77.00%) (35696/46208)
Epoch: 23 | Batch_idx: 370 |  Loss: (0.6533) |  Loss2: (0.0000) | Acc: (77.00%) (36688/47488)
Epoch: 23 | Batch_idx: 380 |  Loss: (0.6525) |  Loss2: (0.0000) | Acc: (77.00%) (37683/48768)
Epoch: 23 | Batch_idx: 390 |  Loss: (0.6527) |  Loss2: (0.0000) | Acc: (77.00%) (38635/50000)
# TEST : Loss: (0.6967) | Acc: (75.00%) (7561/10000)
percent tensor([0.5174, 0.4826], device='cuda:0')
percent tensor([0.4870, 0.5130], device='cuda:0')
percent tensor([0.4827, 0.5173], device='cuda:0')
percent tensor([0.5137, 0.4863], device='cuda:0')
percent tensor([0.4893, 0.5107], device='cuda:0')
percent tensor([0.6001, 0.3999], device='cuda:0')
percent tensor([0.5772, 0.4228], device='cuda:0')
percent tensor([0.9363, 0.0637], device='cuda:0')
Epoch: 24 | Batch_idx: 0 |  Loss: (0.6117) |  Loss2: (0.0000) | Acc: (77.00%) (99/128)
Epoch: 24 | Batch_idx: 10 |  Loss: (0.6316) |  Loss2: (0.0000) | Acc: (77.00%) (1090/1408)
Epoch: 24 | Batch_idx: 20 |  Loss: (0.6340) |  Loss2: (0.0000) | Acc: (77.00%) (2082/2688)
Epoch: 24 | Batch_idx: 30 |  Loss: (0.6247) |  Loss2: (0.0000) | Acc: (78.00%) (3115/3968)
Epoch: 24 | Batch_idx: 40 |  Loss: (0.6378) |  Loss2: (0.0000) | Acc: (77.00%) (4092/5248)
Epoch: 24 | Batch_idx: 50 |  Loss: (0.6394) |  Loss2: (0.0000) | Acc: (77.00%) (5086/6528)
Epoch: 24 | Batch_idx: 60 |  Loss: (0.6361) |  Loss2: (0.0000) | Acc: (78.00%) (6099/7808)
Epoch: 24 | Batch_idx: 70 |  Loss: (0.6372) |  Loss2: (0.0000) | Acc: (78.00%) (7109/9088)
Epoch: 24 | Batch_idx: 80 |  Loss: (0.6355) |  Loss2: (0.0000) | Acc: (78.00%) (8109/10368)
Epoch: 24 | Batch_idx: 90 |  Loss: (0.6344) |  Loss2: (0.0000) | Acc: (78.00%) (9100/11648)
Epoch: 24 | Batch_idx: 100 |  Loss: (0.6343) |  Loss2: (0.0000) | Acc: (78.00%) (10103/12928)
Epoch: 24 | Batch_idx: 110 |  Loss: (0.6356) |  Loss2: (0.0000) | Acc: (78.00%) (11109/14208)
Epoch: 24 | Batch_idx: 120 |  Loss: (0.6320) |  Loss2: (0.0000) | Acc: (78.00%) (12111/15488)
Epoch: 24 | Batch_idx: 130 |  Loss: (0.6289) |  Loss2: (0.0000) | Acc: (78.00%) (13119/16768)
Epoch: 24 | Batch_idx: 140 |  Loss: (0.6293) |  Loss2: (0.0000) | Acc: (78.00%) (14106/18048)
Epoch: 24 | Batch_idx: 150 |  Loss: (0.6260) |  Loss2: (0.0000) | Acc: (78.00%) (15124/19328)
Epoch: 24 | Batch_idx: 160 |  Loss: (0.6275) |  Loss2: (0.0000) | Acc: (78.00%) (16106/20608)
Epoch: 24 | Batch_idx: 170 |  Loss: (0.6285) |  Loss2: (0.0000) | Acc: (78.00%) (17089/21888)
Epoch: 24 | Batch_idx: 180 |  Loss: (0.6273) |  Loss2: (0.0000) | Acc: (78.00%) (18083/23168)
Epoch: 24 | Batch_idx: 190 |  Loss: (0.6283) |  Loss2: (0.0000) | Acc: (77.00%) (19066/24448)
Epoch: 24 | Batch_idx: 200 |  Loss: (0.6290) |  Loss2: (0.0000) | Acc: (78.00%) (20070/25728)
Epoch: 24 | Batch_idx: 210 |  Loss: (0.6283) |  Loss2: (0.0000) | Acc: (78.00%) (21086/27008)
Epoch: 24 | Batch_idx: 220 |  Loss: (0.6269) |  Loss2: (0.0000) | Acc: (78.00%) (22109/28288)
Epoch: 24 | Batch_idx: 230 |  Loss: (0.6266) |  Loss2: (0.0000) | Acc: (78.00%) (23125/29568)
Epoch: 24 | Batch_idx: 240 |  Loss: (0.6262) |  Loss2: (0.0000) | Acc: (78.00%) (24146/30848)
Epoch: 24 | Batch_idx: 250 |  Loss: (0.6278) |  Loss2: (0.0000) | Acc: (78.00%) (25128/32128)
Epoch: 24 | Batch_idx: 260 |  Loss: (0.6271) |  Loss2: (0.0000) | Acc: (78.00%) (26130/33408)
Epoch: 24 | Batch_idx: 270 |  Loss: (0.6277) |  Loss2: (0.0000) | Acc: (78.00%) (27129/34688)
Epoch: 24 | Batch_idx: 280 |  Loss: (0.6292) |  Loss2: (0.0000) | Acc: (78.00%) (28107/35968)
Epoch: 24 | Batch_idx: 290 |  Loss: (0.6287) |  Loss2: (0.0000) | Acc: (78.00%) (29116/37248)
Epoch: 24 | Batch_idx: 300 |  Loss: (0.6285) |  Loss2: (0.0000) | Acc: (78.00%) (30114/38528)
Epoch: 24 | Batch_idx: 310 |  Loss: (0.6276) |  Loss2: (0.0000) | Acc: (78.00%) (31143/39808)
Epoch: 24 | Batch_idx: 320 |  Loss: (0.6266) |  Loss2: (0.0000) | Acc: (78.00%) (32151/41088)
Epoch: 24 | Batch_idx: 330 |  Loss: (0.6257) |  Loss2: (0.0000) | Acc: (78.00%) (33174/42368)
Epoch: 24 | Batch_idx: 340 |  Loss: (0.6245) |  Loss2: (0.0000) | Acc: (78.00%) (34187/43648)
Epoch: 24 | Batch_idx: 350 |  Loss: (0.6246) |  Loss2: (0.0000) | Acc: (78.00%) (35187/44928)
Epoch: 24 | Batch_idx: 360 |  Loss: (0.6246) |  Loss2: (0.0000) | Acc: (78.00%) (36190/46208)
Epoch: 24 | Batch_idx: 370 |  Loss: (0.6246) |  Loss2: (0.0000) | Acc: (78.00%) (37197/47488)
Epoch: 24 | Batch_idx: 380 |  Loss: (0.6244) |  Loss2: (0.0000) | Acc: (78.00%) (38206/48768)
Epoch: 24 | Batch_idx: 390 |  Loss: (0.6243) |  Loss2: (0.0000) | Acc: (78.00%) (39169/50000)
# TEST : Loss: (0.6404) | Acc: (77.00%) (7794/10000)
percent tensor([0.5174, 0.4826], device='cuda:0')
percent tensor([0.4869, 0.5131], device='cuda:0')
percent tensor([0.4825, 0.5175], device='cuda:0')
percent tensor([0.5140, 0.4860], device='cuda:0')
percent tensor([0.4892, 0.5108], device='cuda:0')
percent tensor([0.6014, 0.3986], device='cuda:0')
percent tensor([0.5774, 0.4226], device='cuda:0')
percent tensor([0.9367, 0.0633], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 25 | Batch_idx: 0 |  Loss: (0.6104) |  Loss2: (0.0000) | Acc: (75.00%) (97/128)
Epoch: 25 | Batch_idx: 10 |  Loss: (0.6709) |  Loss2: (0.0000) | Acc: (75.00%) (1067/1408)
Epoch: 25 | Batch_idx: 20 |  Loss: (0.7366) |  Loss2: (0.0000) | Acc: (73.00%) (1976/2688)
Epoch: 25 | Batch_idx: 30 |  Loss: (0.7820) |  Loss2: (0.0000) | Acc: (72.00%) (2875/3968)
Epoch: 25 | Batch_idx: 40 |  Loss: (0.7851) |  Loss2: (0.0000) | Acc: (72.00%) (3807/5248)
Epoch: 25 | Batch_idx: 50 |  Loss: (0.7886) |  Loss2: (0.0000) | Acc: (72.00%) (4727/6528)
Epoch: 25 | Batch_idx: 60 |  Loss: (0.7875) |  Loss2: (0.0000) | Acc: (72.00%) (5657/7808)
Epoch: 25 | Batch_idx: 70 |  Loss: (0.7812) |  Loss2: (0.0000) | Acc: (72.00%) (6613/9088)
Epoch: 25 | Batch_idx: 80 |  Loss: (0.7769) |  Loss2: (0.0000) | Acc: (72.00%) (7545/10368)
Epoch: 25 | Batch_idx: 90 |  Loss: (0.7728) |  Loss2: (0.0000) | Acc: (72.00%) (8498/11648)
Epoch: 25 | Batch_idx: 100 |  Loss: (0.7773) |  Loss2: (0.0000) | Acc: (72.00%) (9419/12928)
Epoch: 25 | Batch_idx: 110 |  Loss: (0.7747) |  Loss2: (0.0000) | Acc: (72.00%) (10364/14208)
Epoch: 25 | Batch_idx: 120 |  Loss: (0.7705) |  Loss2: (0.0000) | Acc: (73.00%) (11316/15488)
Epoch: 25 | Batch_idx: 130 |  Loss: (0.7702) |  Loss2: (0.0000) | Acc: (73.00%) (12253/16768)
Epoch: 25 | Batch_idx: 140 |  Loss: (0.7709) |  Loss2: (0.0000) | Acc: (73.00%) (13187/18048)
Epoch: 25 | Batch_idx: 150 |  Loss: (0.7684) |  Loss2: (0.0000) | Acc: (73.00%) (14146/19328)
Epoch: 25 | Batch_idx: 160 |  Loss: (0.7686) |  Loss2: (0.0000) | Acc: (73.00%) (15079/20608)
Epoch: 25 | Batch_idx: 170 |  Loss: (0.7672) |  Loss2: (0.0000) | Acc: (73.00%) (16039/21888)
Epoch: 25 | Batch_idx: 180 |  Loss: (0.7642) |  Loss2: (0.0000) | Acc: (73.00%) (16990/23168)
Epoch: 25 | Batch_idx: 190 |  Loss: (0.7631) |  Loss2: (0.0000) | Acc: (73.00%) (17930/24448)
Epoch: 25 | Batch_idx: 200 |  Loss: (0.7629) |  Loss2: (0.0000) | Acc: (73.00%) (18866/25728)
Epoch: 25 | Batch_idx: 210 |  Loss: (0.7623) |  Loss2: (0.0000) | Acc: (73.00%) (19813/27008)
Epoch: 25 | Batch_idx: 220 |  Loss: (0.7619) |  Loss2: (0.0000) | Acc: (73.00%) (20757/28288)
Epoch: 25 | Batch_idx: 230 |  Loss: (0.7608) |  Loss2: (0.0000) | Acc: (73.00%) (21716/29568)
Epoch: 25 | Batch_idx: 240 |  Loss: (0.7573) |  Loss2: (0.0000) | Acc: (73.00%) (22698/30848)
Epoch: 25 | Batch_idx: 250 |  Loss: (0.7531) |  Loss2: (0.0000) | Acc: (73.00%) (23698/32128)
Epoch: 25 | Batch_idx: 260 |  Loss: (0.7521) |  Loss2: (0.0000) | Acc: (73.00%) (24659/33408)
Epoch: 25 | Batch_idx: 270 |  Loss: (0.7505) |  Loss2: (0.0000) | Acc: (73.00%) (25619/34688)
Epoch: 25 | Batch_idx: 280 |  Loss: (0.7472) |  Loss2: (0.0000) | Acc: (74.00%) (26619/35968)
Epoch: 25 | Batch_idx: 290 |  Loss: (0.7456) |  Loss2: (0.0000) | Acc: (74.00%) (27598/37248)
Epoch: 25 | Batch_idx: 300 |  Loss: (0.7454) |  Loss2: (0.0000) | Acc: (74.00%) (28555/38528)
Epoch: 25 | Batch_idx: 310 |  Loss: (0.7436) |  Loss2: (0.0000) | Acc: (74.00%) (29530/39808)
Epoch: 25 | Batch_idx: 320 |  Loss: (0.7414) |  Loss2: (0.0000) | Acc: (74.00%) (30534/41088)
Epoch: 25 | Batch_idx: 330 |  Loss: (0.7402) |  Loss2: (0.0000) | Acc: (74.00%) (31488/42368)
Epoch: 25 | Batch_idx: 340 |  Loss: (0.7385) |  Loss2: (0.0000) | Acc: (74.00%) (32465/43648)
Epoch: 25 | Batch_idx: 350 |  Loss: (0.7359) |  Loss2: (0.0000) | Acc: (74.00%) (33452/44928)
Epoch: 25 | Batch_idx: 360 |  Loss: (0.7351) |  Loss2: (0.0000) | Acc: (74.00%) (34435/46208)
Epoch: 25 | Batch_idx: 370 |  Loss: (0.7351) |  Loss2: (0.0000) | Acc: (74.00%) (35391/47488)
Epoch: 25 | Batch_idx: 380 |  Loss: (0.7330) |  Loss2: (0.0000) | Acc: (74.00%) (36387/48768)
Epoch: 25 | Batch_idx: 390 |  Loss: (0.7318) |  Loss2: (0.0000) | Acc: (74.00%) (37329/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_025.pth.tar'
# TEST : Loss: (0.7136) | Acc: (75.00%) (7583/10000)
percent tensor([0.5278, 0.4722], device='cuda:0')
percent tensor([0.4920, 0.5080], device='cuda:0')
percent tensor([0.4661, 0.5339], device='cuda:0')
percent tensor([0.5256, 0.4744], device='cuda:0')
percent tensor([0.5075, 0.4925], device='cuda:0')
percent tensor([0.6156, 0.3844], device='cuda:0')
percent tensor([0.5726, 0.4274], device='cuda:0')
percent tensor([0.9492, 0.0508], device='cuda:0')
Epoch: 26 | Batch_idx: 0 |  Loss: (0.6829) |  Loss2: (0.0000) | Acc: (75.00%) (96/128)
Epoch: 26 | Batch_idx: 10 |  Loss: (0.7277) |  Loss2: (0.0000) | Acc: (74.00%) (1046/1408)
Epoch: 26 | Batch_idx: 20 |  Loss: (0.7040) |  Loss2: (0.0000) | Acc: (75.00%) (2025/2688)
Epoch: 26 | Batch_idx: 30 |  Loss: (0.6927) |  Loss2: (0.0000) | Acc: (75.00%) (3003/3968)
Epoch: 26 | Batch_idx: 40 |  Loss: (0.6830) |  Loss2: (0.0000) | Acc: (76.00%) (4002/5248)
Epoch: 26 | Batch_idx: 50 |  Loss: (0.6883) |  Loss2: (0.0000) | Acc: (76.00%) (4968/6528)
Epoch: 26 | Batch_idx: 60 |  Loss: (0.6813) |  Loss2: (0.0000) | Acc: (76.00%) (5958/7808)
Epoch: 26 | Batch_idx: 70 |  Loss: (0.6795) |  Loss2: (0.0000) | Acc: (76.00%) (6936/9088)
Epoch: 26 | Batch_idx: 80 |  Loss: (0.6772) |  Loss2: (0.0000) | Acc: (76.00%) (7916/10368)
Epoch: 26 | Batch_idx: 90 |  Loss: (0.6723) |  Loss2: (0.0000) | Acc: (76.00%) (8919/11648)
Epoch: 26 | Batch_idx: 100 |  Loss: (0.6706) |  Loss2: (0.0000) | Acc: (76.00%) (9901/12928)
Epoch: 26 | Batch_idx: 110 |  Loss: (0.6706) |  Loss2: (0.0000) | Acc: (76.00%) (10898/14208)
Epoch: 26 | Batch_idx: 120 |  Loss: (0.6708) |  Loss2: (0.0000) | Acc: (76.00%) (11875/15488)
Epoch: 26 | Batch_idx: 130 |  Loss: (0.6696) |  Loss2: (0.0000) | Acc: (76.00%) (12881/16768)
Epoch: 26 | Batch_idx: 140 |  Loss: (0.6681) |  Loss2: (0.0000) | Acc: (76.00%) (13870/18048)
Epoch: 26 | Batch_idx: 150 |  Loss: (0.6688) |  Loss2: (0.0000) | Acc: (76.00%) (14851/19328)
Epoch: 26 | Batch_idx: 160 |  Loss: (0.6671) |  Loss2: (0.0000) | Acc: (76.00%) (15844/20608)
Epoch: 26 | Batch_idx: 170 |  Loss: (0.6696) |  Loss2: (0.0000) | Acc: (76.00%) (16802/21888)
Epoch: 26 | Batch_idx: 180 |  Loss: (0.6676) |  Loss2: (0.0000) | Acc: (76.00%) (17797/23168)
Epoch: 26 | Batch_idx: 190 |  Loss: (0.6686) |  Loss2: (0.0000) | Acc: (76.00%) (18788/24448)
Epoch: 26 | Batch_idx: 200 |  Loss: (0.6691) |  Loss2: (0.0000) | Acc: (76.00%) (19762/25728)
Epoch: 26 | Batch_idx: 210 |  Loss: (0.6671) |  Loss2: (0.0000) | Acc: (76.00%) (20753/27008)
Epoch: 26 | Batch_idx: 220 |  Loss: (0.6653) |  Loss2: (0.0000) | Acc: (76.00%) (21747/28288)
Epoch: 26 | Batch_idx: 230 |  Loss: (0.6634) |  Loss2: (0.0000) | Acc: (76.00%) (22742/29568)
Epoch: 26 | Batch_idx: 240 |  Loss: (0.6636) |  Loss2: (0.0000) | Acc: (76.00%) (23721/30848)
Epoch: 26 | Batch_idx: 250 |  Loss: (0.6619) |  Loss2: (0.0000) | Acc: (76.00%) (24734/32128)
Epoch: 26 | Batch_idx: 260 |  Loss: (0.6604) |  Loss2: (0.0000) | Acc: (77.00%) (25743/33408)
Epoch: 26 | Batch_idx: 270 |  Loss: (0.6603) |  Loss2: (0.0000) | Acc: (77.00%) (26746/34688)
Epoch: 26 | Batch_idx: 280 |  Loss: (0.6604) |  Loss2: (0.0000) | Acc: (77.00%) (27729/35968)
Epoch: 26 | Batch_idx: 290 |  Loss: (0.6599) |  Loss2: (0.0000) | Acc: (77.00%) (28722/37248)
Epoch: 26 | Batch_idx: 300 |  Loss: (0.6578) |  Loss2: (0.0000) | Acc: (77.00%) (29738/38528)
Epoch: 26 | Batch_idx: 310 |  Loss: (0.6588) |  Loss2: (0.0000) | Acc: (77.00%) (30693/39808)
Epoch: 26 | Batch_idx: 320 |  Loss: (0.6594) |  Loss2: (0.0000) | Acc: (77.00%) (31684/41088)
Epoch: 26 | Batch_idx: 330 |  Loss: (0.6585) |  Loss2: (0.0000) | Acc: (77.00%) (32689/42368)
Epoch: 26 | Batch_idx: 340 |  Loss: (0.6575) |  Loss2: (0.0000) | Acc: (77.00%) (33674/43648)
Epoch: 26 | Batch_idx: 350 |  Loss: (0.6563) |  Loss2: (0.0000) | Acc: (77.00%) (34687/44928)
Epoch: 26 | Batch_idx: 360 |  Loss: (0.6549) |  Loss2: (0.0000) | Acc: (77.00%) (35692/46208)
Epoch: 26 | Batch_idx: 370 |  Loss: (0.6544) |  Loss2: (0.0000) | Acc: (77.00%) (36685/47488)
Epoch: 26 | Batch_idx: 380 |  Loss: (0.6544) |  Loss2: (0.0000) | Acc: (77.00%) (37677/48768)
Epoch: 26 | Batch_idx: 390 |  Loss: (0.6542) |  Loss2: (0.0000) | Acc: (77.00%) (38624/50000)
# TEST : Loss: (0.6752) | Acc: (77.00%) (7718/10000)
percent tensor([0.5262, 0.4738], device='cuda:0')
percent tensor([0.4940, 0.5060], device='cuda:0')
percent tensor([0.4640, 0.5360], device='cuda:0')
percent tensor([0.5306, 0.4694], device='cuda:0')
percent tensor([0.5164, 0.4836], device='cuda:0')
percent tensor([0.6314, 0.3686], device='cuda:0')
percent tensor([0.5862, 0.4138], device='cuda:0')
percent tensor([0.9669, 0.0331], device='cuda:0')
Epoch: 27 | Batch_idx: 0 |  Loss: (0.6244) |  Loss2: (0.0000) | Acc: (78.00%) (100/128)
Epoch: 27 | Batch_idx: 10 |  Loss: (0.6381) |  Loss2: (0.0000) | Acc: (77.00%) (1091/1408)
Epoch: 27 | Batch_idx: 20 |  Loss: (0.6411) |  Loss2: (0.0000) | Acc: (77.00%) (2072/2688)
Epoch: 27 | Batch_idx: 30 |  Loss: (0.6392) |  Loss2: (0.0000) | Acc: (77.00%) (3060/3968)
Epoch: 27 | Batch_idx: 40 |  Loss: (0.6529) |  Loss2: (0.0000) | Acc: (76.00%) (4025/5248)
Epoch: 27 | Batch_idx: 50 |  Loss: (0.6543) |  Loss2: (0.0000) | Acc: (76.00%) (5015/6528)
Epoch: 27 | Batch_idx: 60 |  Loss: (0.6505) |  Loss2: (0.0000) | Acc: (77.00%) (6015/7808)
Epoch: 27 | Batch_idx: 70 |  Loss: (0.6490) |  Loss2: (0.0000) | Acc: (77.00%) (7005/9088)
Epoch: 27 | Batch_idx: 80 |  Loss: (0.6517) |  Loss2: (0.0000) | Acc: (77.00%) (7989/10368)
Epoch: 27 | Batch_idx: 90 |  Loss: (0.6517) |  Loss2: (0.0000) | Acc: (77.00%) (8984/11648)
Epoch: 27 | Batch_idx: 100 |  Loss: (0.6528) |  Loss2: (0.0000) | Acc: (77.00%) (9972/12928)
Epoch: 27 | Batch_idx: 110 |  Loss: (0.6529) |  Loss2: (0.0000) | Acc: (77.00%) (10979/14208)
Epoch: 27 | Batch_idx: 120 |  Loss: (0.6478) |  Loss2: (0.0000) | Acc: (77.00%) (12000/15488)
Epoch: 27 | Batch_idx: 130 |  Loss: (0.6444) |  Loss2: (0.0000) | Acc: (77.00%) (13021/16768)
Epoch: 27 | Batch_idx: 140 |  Loss: (0.6429) |  Loss2: (0.0000) | Acc: (77.00%) (14035/18048)
Epoch: 27 | Batch_idx: 150 |  Loss: (0.6456) |  Loss2: (0.0000) | Acc: (77.00%) (14993/19328)
Epoch: 27 | Batch_idx: 160 |  Loss: (0.6476) |  Loss2: (0.0000) | Acc: (77.00%) (15977/20608)
Epoch: 27 | Batch_idx: 170 |  Loss: (0.6484) |  Loss2: (0.0000) | Acc: (77.00%) (16968/21888)
Epoch: 27 | Batch_idx: 180 |  Loss: (0.6509) |  Loss2: (0.0000) | Acc: (77.00%) (17936/23168)
Epoch: 27 | Batch_idx: 190 |  Loss: (0.6507) |  Loss2: (0.0000) | Acc: (77.00%) (18920/24448)
Epoch: 27 | Batch_idx: 200 |  Loss: (0.6473) |  Loss2: (0.0000) | Acc: (77.00%) (19944/25728)
Epoch: 27 | Batch_idx: 210 |  Loss: (0.6455) |  Loss2: (0.0000) | Acc: (77.00%) (20955/27008)
Epoch: 27 | Batch_idx: 220 |  Loss: (0.6463) |  Loss2: (0.0000) | Acc: (77.00%) (21952/28288)
Epoch: 27 | Batch_idx: 230 |  Loss: (0.6445) |  Loss2: (0.0000) | Acc: (77.00%) (22972/29568)
Epoch: 27 | Batch_idx: 240 |  Loss: (0.6436) |  Loss2: (0.0000) | Acc: (77.00%) (23981/30848)
Epoch: 27 | Batch_idx: 250 |  Loss: (0.6434) |  Loss2: (0.0000) | Acc: (77.00%) (24958/32128)
Epoch: 27 | Batch_idx: 260 |  Loss: (0.6417) |  Loss2: (0.0000) | Acc: (77.00%) (25985/33408)
Epoch: 27 | Batch_idx: 270 |  Loss: (0.6409) |  Loss2: (0.0000) | Acc: (77.00%) (26981/34688)
Epoch: 27 | Batch_idx: 280 |  Loss: (0.6408) |  Loss2: (0.0000) | Acc: (77.00%) (27979/35968)
Epoch: 27 | Batch_idx: 290 |  Loss: (0.6412) |  Loss2: (0.0000) | Acc: (77.00%) (28978/37248)
Epoch: 27 | Batch_idx: 300 |  Loss: (0.6413) |  Loss2: (0.0000) | Acc: (77.00%) (29966/38528)
Epoch: 27 | Batch_idx: 310 |  Loss: (0.6393) |  Loss2: (0.0000) | Acc: (77.00%) (30996/39808)
Epoch: 27 | Batch_idx: 320 |  Loss: (0.6388) |  Loss2: (0.0000) | Acc: (77.00%) (32019/41088)
Epoch: 27 | Batch_idx: 330 |  Loss: (0.6381) |  Loss2: (0.0000) | Acc: (77.00%) (33040/42368)
Epoch: 27 | Batch_idx: 340 |  Loss: (0.6367) |  Loss2: (0.0000) | Acc: (78.00%) (34065/43648)
Epoch: 27 | Batch_idx: 350 |  Loss: (0.6356) |  Loss2: (0.0000) | Acc: (78.00%) (35074/44928)
Epoch: 27 | Batch_idx: 360 |  Loss: (0.6338) |  Loss2: (0.0000) | Acc: (78.00%) (36090/46208)
Epoch: 27 | Batch_idx: 370 |  Loss: (0.6355) |  Loss2: (0.0000) | Acc: (78.00%) (37051/47488)
Epoch: 27 | Batch_idx: 380 |  Loss: (0.6361) |  Loss2: (0.0000) | Acc: (78.00%) (38057/48768)
Epoch: 27 | Batch_idx: 390 |  Loss: (0.6350) |  Loss2: (0.0000) | Acc: (78.00%) (39029/50000)
# TEST : Loss: (0.6562) | Acc: (77.00%) (7762/10000)
percent tensor([0.5270, 0.4730], device='cuda:0')
percent tensor([0.4942, 0.5058], device='cuda:0')
percent tensor([0.4633, 0.5367], device='cuda:0')
percent tensor([0.5322, 0.4678], device='cuda:0')
percent tensor([0.5236, 0.4764], device='cuda:0')
percent tensor([0.6350, 0.3650], device='cuda:0')
percent tensor([0.5975, 0.4025], device='cuda:0')
percent tensor([0.9758, 0.0242], device='cuda:0')
Epoch: 28 | Batch_idx: 0 |  Loss: (0.6241) |  Loss2: (0.0000) | Acc: (76.00%) (98/128)
Epoch: 28 | Batch_idx: 10 |  Loss: (0.6063) |  Loss2: (0.0000) | Acc: (79.00%) (1116/1408)
Epoch: 28 | Batch_idx: 20 |  Loss: (0.6014) |  Loss2: (0.0000) | Acc: (78.00%) (2121/2688)
Epoch: 28 | Batch_idx: 30 |  Loss: (0.6127) |  Loss2: (0.0000) | Acc: (78.00%) (3117/3968)
Epoch: 28 | Batch_idx: 40 |  Loss: (0.6109) |  Loss2: (0.0000) | Acc: (78.00%) (4115/5248)
Epoch: 28 | Batch_idx: 50 |  Loss: (0.6046) |  Loss2: (0.0000) | Acc: (78.00%) (5138/6528)
Epoch: 28 | Batch_idx: 60 |  Loss: (0.6060) |  Loss2: (0.0000) | Acc: (78.00%) (6148/7808)
Epoch: 28 | Batch_idx: 70 |  Loss: (0.6057) |  Loss2: (0.0000) | Acc: (78.00%) (7161/9088)
Epoch: 28 | Batch_idx: 80 |  Loss: (0.6089) |  Loss2: (0.0000) | Acc: (78.00%) (8160/10368)
Epoch: 28 | Batch_idx: 90 |  Loss: (0.6077) |  Loss2: (0.0000) | Acc: (78.00%) (9182/11648)
Epoch: 28 | Batch_idx: 100 |  Loss: (0.6103) |  Loss2: (0.0000) | Acc: (78.00%) (10178/12928)
Epoch: 28 | Batch_idx: 110 |  Loss: (0.6114) |  Loss2: (0.0000) | Acc: (78.00%) (11172/14208)
Epoch: 28 | Batch_idx: 120 |  Loss: (0.6153) |  Loss2: (0.0000) | Acc: (78.00%) (12169/15488)
Epoch: 28 | Batch_idx: 130 |  Loss: (0.6171) |  Loss2: (0.0000) | Acc: (78.00%) (13151/16768)
Epoch: 28 | Batch_idx: 140 |  Loss: (0.6177) |  Loss2: (0.0000) | Acc: (78.00%) (14149/18048)
Epoch: 28 | Batch_idx: 150 |  Loss: (0.6182) |  Loss2: (0.0000) | Acc: (78.00%) (15145/19328)
Epoch: 28 | Batch_idx: 160 |  Loss: (0.6188) |  Loss2: (0.0000) | Acc: (78.00%) (16137/20608)
Epoch: 28 | Batch_idx: 170 |  Loss: (0.6182) |  Loss2: (0.0000) | Acc: (78.00%) (17150/21888)
Epoch: 28 | Batch_idx: 180 |  Loss: (0.6192) |  Loss2: (0.0000) | Acc: (78.00%) (18155/23168)
Epoch: 28 | Batch_idx: 190 |  Loss: (0.6186) |  Loss2: (0.0000) | Acc: (78.00%) (19151/24448)
Epoch: 28 | Batch_idx: 200 |  Loss: (0.6202) |  Loss2: (0.0000) | Acc: (78.00%) (20139/25728)
Epoch: 28 | Batch_idx: 210 |  Loss: (0.6213) |  Loss2: (0.0000) | Acc: (78.00%) (21130/27008)
Epoch: 28 | Batch_idx: 220 |  Loss: (0.6212) |  Loss2: (0.0000) | Acc: (78.00%) (22160/28288)
Epoch: 28 | Batch_idx: 230 |  Loss: (0.6205) |  Loss2: (0.0000) | Acc: (78.00%) (23153/29568)
Epoch: 28 | Batch_idx: 240 |  Loss: (0.6193) |  Loss2: (0.0000) | Acc: (78.00%) (24170/30848)
Epoch: 28 | Batch_idx: 250 |  Loss: (0.6183) |  Loss2: (0.0000) | Acc: (78.00%) (25187/32128)
Epoch: 28 | Batch_idx: 260 |  Loss: (0.6194) |  Loss2: (0.0000) | Acc: (78.00%) (26166/33408)
Epoch: 28 | Batch_idx: 270 |  Loss: (0.6194) |  Loss2: (0.0000) | Acc: (78.00%) (27158/34688)
Epoch: 28 | Batch_idx: 280 |  Loss: (0.6199) |  Loss2: (0.0000) | Acc: (78.00%) (28153/35968)
Epoch: 28 | Batch_idx: 290 |  Loss: (0.6205) |  Loss2: (0.0000) | Acc: (78.00%) (29141/37248)
Epoch: 28 | Batch_idx: 300 |  Loss: (0.6183) |  Loss2: (0.0000) | Acc: (78.00%) (30172/38528)
Epoch: 28 | Batch_idx: 310 |  Loss: (0.6201) |  Loss2: (0.0000) | Acc: (78.00%) (31145/39808)
Epoch: 28 | Batch_idx: 320 |  Loss: (0.6187) |  Loss2: (0.0000) | Acc: (78.00%) (32162/41088)
Epoch: 28 | Batch_idx: 330 |  Loss: (0.6182) |  Loss2: (0.0000) | Acc: (78.00%) (33169/42368)
Epoch: 28 | Batch_idx: 340 |  Loss: (0.6175) |  Loss2: (0.0000) | Acc: (78.00%) (34172/43648)
Epoch: 28 | Batch_idx: 350 |  Loss: (0.6169) |  Loss2: (0.0000) | Acc: (78.00%) (35183/44928)
Epoch: 28 | Batch_idx: 360 |  Loss: (0.6174) |  Loss2: (0.0000) | Acc: (78.00%) (36197/46208)
Epoch: 28 | Batch_idx: 370 |  Loss: (0.6161) |  Loss2: (0.0000) | Acc: (78.00%) (37232/47488)
Epoch: 28 | Batch_idx: 380 |  Loss: (0.6156) |  Loss2: (0.0000) | Acc: (78.00%) (38254/48768)
Epoch: 28 | Batch_idx: 390 |  Loss: (0.6163) |  Loss2: (0.0000) | Acc: (78.00%) (39198/50000)
# TEST : Loss: (0.6476) | Acc: (77.00%) (7793/10000)
percent tensor([0.5255, 0.4745], device='cuda:0')
percent tensor([0.4960, 0.5040], device='cuda:0')
percent tensor([0.4625, 0.5375], device='cuda:0')
percent tensor([0.5338, 0.4662], device='cuda:0')
percent tensor([0.5264, 0.4736], device='cuda:0')
percent tensor([0.6338, 0.3662], device='cuda:0')
percent tensor([0.6079, 0.3921], device='cuda:0')
percent tensor([0.9820, 0.0180], device='cuda:0')
Epoch: 29 | Batch_idx: 0 |  Loss: (0.5662) |  Loss2: (0.0000) | Acc: (77.00%) (99/128)
Epoch: 29 | Batch_idx: 10 |  Loss: (0.6122) |  Loss2: (0.0000) | Acc: (77.00%) (1089/1408)
Epoch: 29 | Batch_idx: 20 |  Loss: (0.6260) |  Loss2: (0.0000) | Acc: (77.00%) (2081/2688)
Epoch: 29 | Batch_idx: 30 |  Loss: (0.6299) |  Loss2: (0.0000) | Acc: (77.00%) (3086/3968)
Epoch: 29 | Batch_idx: 40 |  Loss: (0.6294) |  Loss2: (0.0000) | Acc: (78.00%) (4101/5248)
Epoch: 29 | Batch_idx: 50 |  Loss: (0.6303) |  Loss2: (0.0000) | Acc: (78.00%) (5113/6528)
Epoch: 29 | Batch_idx: 60 |  Loss: (0.6250) |  Loss2: (0.0000) | Acc: (78.00%) (6131/7808)
Epoch: 29 | Batch_idx: 70 |  Loss: (0.6237) |  Loss2: (0.0000) | Acc: (78.00%) (7133/9088)
Epoch: 29 | Batch_idx: 80 |  Loss: (0.6213) |  Loss2: (0.0000) | Acc: (78.00%) (8153/10368)
Epoch: 29 | Batch_idx: 90 |  Loss: (0.6245) |  Loss2: (0.0000) | Acc: (78.00%) (9131/11648)
Epoch: 29 | Batch_idx: 100 |  Loss: (0.6215) |  Loss2: (0.0000) | Acc: (78.00%) (10135/12928)
Epoch: 29 | Batch_idx: 110 |  Loss: (0.6219) |  Loss2: (0.0000) | Acc: (78.00%) (11130/14208)
Epoch: 29 | Batch_idx: 120 |  Loss: (0.6237) |  Loss2: (0.0000) | Acc: (78.00%) (12128/15488)
Epoch: 29 | Batch_idx: 130 |  Loss: (0.6226) |  Loss2: (0.0000) | Acc: (78.00%) (13127/16768)
Epoch: 29 | Batch_idx: 140 |  Loss: (0.6218) |  Loss2: (0.0000) | Acc: (78.00%) (14136/18048)
Epoch: 29 | Batch_idx: 150 |  Loss: (0.6221) |  Loss2: (0.0000) | Acc: (78.00%) (15133/19328)
Epoch: 29 | Batch_idx: 160 |  Loss: (0.6240) |  Loss2: (0.0000) | Acc: (78.00%) (16121/20608)
Epoch: 29 | Batch_idx: 170 |  Loss: (0.6235) |  Loss2: (0.0000) | Acc: (78.00%) (17130/21888)
Epoch: 29 | Batch_idx: 180 |  Loss: (0.6219) |  Loss2: (0.0000) | Acc: (78.00%) (18156/23168)
Epoch: 29 | Batch_idx: 190 |  Loss: (0.6215) |  Loss2: (0.0000) | Acc: (78.00%) (19160/24448)
Epoch: 29 | Batch_idx: 200 |  Loss: (0.6220) |  Loss2: (0.0000) | Acc: (78.00%) (20150/25728)
Epoch: 29 | Batch_idx: 210 |  Loss: (0.6214) |  Loss2: (0.0000) | Acc: (78.00%) (21177/27008)
Epoch: 29 | Batch_idx: 220 |  Loss: (0.6214) |  Loss2: (0.0000) | Acc: (78.00%) (22187/28288)
Epoch: 29 | Batch_idx: 230 |  Loss: (0.6216) |  Loss2: (0.0000) | Acc: (78.00%) (23189/29568)
Epoch: 29 | Batch_idx: 240 |  Loss: (0.6199) |  Loss2: (0.0000) | Acc: (78.00%) (24219/30848)
Epoch: 29 | Batch_idx: 250 |  Loss: (0.6187) |  Loss2: (0.0000) | Acc: (78.00%) (25250/32128)
Epoch: 29 | Batch_idx: 260 |  Loss: (0.6188) |  Loss2: (0.0000) | Acc: (78.00%) (26246/33408)
Epoch: 29 | Batch_idx: 270 |  Loss: (0.6182) |  Loss2: (0.0000) | Acc: (78.00%) (27259/34688)
Epoch: 29 | Batch_idx: 280 |  Loss: (0.6175) |  Loss2: (0.0000) | Acc: (78.00%) (28280/35968)
Epoch: 29 | Batch_idx: 290 |  Loss: (0.6168) |  Loss2: (0.0000) | Acc: (78.00%) (29295/37248)
Epoch: 29 | Batch_idx: 300 |  Loss: (0.6150) |  Loss2: (0.0000) | Acc: (78.00%) (30325/38528)
Epoch: 29 | Batch_idx: 310 |  Loss: (0.6151) |  Loss2: (0.0000) | Acc: (78.00%) (31320/39808)
Epoch: 29 | Batch_idx: 320 |  Loss: (0.6161) |  Loss2: (0.0000) | Acc: (78.00%) (32335/41088)
Epoch: 29 | Batch_idx: 330 |  Loss: (0.6174) |  Loss2: (0.0000) | Acc: (78.00%) (33322/42368)
Epoch: 29 | Batch_idx: 340 |  Loss: (0.6172) |  Loss2: (0.0000) | Acc: (78.00%) (34332/43648)
Epoch: 29 | Batch_idx: 350 |  Loss: (0.6180) |  Loss2: (0.0000) | Acc: (78.00%) (35321/44928)
Epoch: 29 | Batch_idx: 360 |  Loss: (0.6174) |  Loss2: (0.0000) | Acc: (78.00%) (36332/46208)
Epoch: 29 | Batch_idx: 370 |  Loss: (0.6165) |  Loss2: (0.0000) | Acc: (78.00%) (37349/47488)
Epoch: 29 | Batch_idx: 380 |  Loss: (0.6170) |  Loss2: (0.0000) | Acc: (78.00%) (38345/48768)
Epoch: 29 | Batch_idx: 390 |  Loss: (0.6159) |  Loss2: (0.0000) | Acc: (78.00%) (39321/50000)
# TEST : Loss: (0.6430) | Acc: (78.00%) (7807/10000)
percent tensor([0.5276, 0.4724], device='cuda:0')
percent tensor([0.4960, 0.5040], device='cuda:0')
percent tensor([0.4654, 0.5346], device='cuda:0')
percent tensor([0.5364, 0.4636], device='cuda:0')
percent tensor([0.5302, 0.4698], device='cuda:0')
percent tensor([0.6361, 0.3639], device='cuda:0')
percent tensor([0.6136, 0.3864], device='cuda:0')
percent tensor([0.9855, 0.0145], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 30 | Batch_idx: 0 |  Loss: (0.6319) |  Loss2: (0.0000) | Acc: (79.00%) (102/128)
Epoch: 30 | Batch_idx: 10 |  Loss: (0.5996) |  Loss2: (0.0000) | Acc: (79.00%) (1122/1408)
Epoch: 30 | Batch_idx: 20 |  Loss: (0.6078) |  Loss2: (0.0000) | Acc: (78.00%) (2117/2688)
Epoch: 30 | Batch_idx: 30 |  Loss: (0.6133) |  Loss2: (0.0000) | Acc: (78.00%) (3112/3968)
Epoch: 30 | Batch_idx: 40 |  Loss: (0.5999) |  Loss2: (0.0000) | Acc: (78.00%) (4141/5248)
Epoch: 30 | Batch_idx: 50 |  Loss: (0.6067) |  Loss2: (0.0000) | Acc: (78.00%) (5155/6528)
Epoch: 30 | Batch_idx: 60 |  Loss: (0.6045) |  Loss2: (0.0000) | Acc: (79.00%) (6188/7808)
Epoch: 30 | Batch_idx: 70 |  Loss: (0.6094) |  Loss2: (0.0000) | Acc: (79.00%) (7188/9088)
Epoch: 30 | Batch_idx: 80 |  Loss: (0.6143) |  Loss2: (0.0000) | Acc: (78.00%) (8168/10368)
Epoch: 30 | Batch_idx: 90 |  Loss: (0.6166) |  Loss2: (0.0000) | Acc: (78.00%) (9166/11648)
Epoch: 30 | Batch_idx: 100 |  Loss: (0.6132) |  Loss2: (0.0000) | Acc: (78.00%) (10189/12928)
Epoch: 30 | Batch_idx: 110 |  Loss: (0.6131) |  Loss2: (0.0000) | Acc: (78.00%) (11204/14208)
Epoch: 30 | Batch_idx: 120 |  Loss: (0.6096) |  Loss2: (0.0000) | Acc: (78.00%) (12218/15488)
Epoch: 30 | Batch_idx: 130 |  Loss: (0.6091) |  Loss2: (0.0000) | Acc: (78.00%) (13228/16768)
Epoch: 30 | Batch_idx: 140 |  Loss: (0.6108) |  Loss2: (0.0000) | Acc: (78.00%) (14234/18048)
Epoch: 30 | Batch_idx: 150 |  Loss: (0.6144) |  Loss2: (0.0000) | Acc: (78.00%) (15218/19328)
Epoch: 30 | Batch_idx: 160 |  Loss: (0.6154) |  Loss2: (0.0000) | Acc: (78.00%) (16210/20608)
Epoch: 30 | Batch_idx: 170 |  Loss: (0.6137) |  Loss2: (0.0000) | Acc: (78.00%) (17231/21888)
Epoch: 30 | Batch_idx: 180 |  Loss: (0.6116) |  Loss2: (0.0000) | Acc: (78.00%) (18267/23168)
Epoch: 30 | Batch_idx: 190 |  Loss: (0.6094) |  Loss2: (0.0000) | Acc: (78.00%) (19295/24448)
Epoch: 30 | Batch_idx: 200 |  Loss: (0.6070) |  Loss2: (0.0000) | Acc: (79.00%) (20326/25728)
Epoch: 30 | Batch_idx: 210 |  Loss: (0.6063) |  Loss2: (0.0000) | Acc: (78.00%) (21336/27008)
Epoch: 30 | Batch_idx: 220 |  Loss: (0.6056) |  Loss2: (0.0000) | Acc: (78.00%) (22333/28288)
Epoch: 30 | Batch_idx: 230 |  Loss: (0.6035) |  Loss2: (0.0000) | Acc: (78.00%) (23352/29568)
Epoch: 30 | Batch_idx: 240 |  Loss: (0.6040) |  Loss2: (0.0000) | Acc: (78.00%) (24362/30848)
Epoch: 30 | Batch_idx: 250 |  Loss: (0.6028) |  Loss2: (0.0000) | Acc: (78.00%) (25379/32128)
Epoch: 30 | Batch_idx: 260 |  Loss: (0.6019) |  Loss2: (0.0000) | Acc: (79.00%) (26401/33408)
Epoch: 30 | Batch_idx: 270 |  Loss: (0.6022) |  Loss2: (0.0000) | Acc: (79.00%) (27419/34688)
Epoch: 30 | Batch_idx: 280 |  Loss: (0.6009) |  Loss2: (0.0000) | Acc: (79.00%) (28442/35968)
Epoch: 30 | Batch_idx: 290 |  Loss: (0.6006) |  Loss2: (0.0000) | Acc: (79.00%) (29459/37248)
Epoch: 30 | Batch_idx: 300 |  Loss: (0.6000) |  Loss2: (0.0000) | Acc: (79.00%) (30490/38528)
Epoch: 30 | Batch_idx: 310 |  Loss: (0.6008) |  Loss2: (0.0000) | Acc: (79.00%) (31494/39808)
Epoch: 30 | Batch_idx: 320 |  Loss: (0.6012) |  Loss2: (0.0000) | Acc: (79.00%) (32498/41088)
Epoch: 30 | Batch_idx: 330 |  Loss: (0.6014) |  Loss2: (0.0000) | Acc: (79.00%) (33517/42368)
Epoch: 30 | Batch_idx: 340 |  Loss: (0.6024) |  Loss2: (0.0000) | Acc: (79.00%) (34507/43648)
Epoch: 30 | Batch_idx: 350 |  Loss: (0.6018) |  Loss2: (0.0000) | Acc: (79.00%) (35524/44928)
Epoch: 30 | Batch_idx: 360 |  Loss: (0.6010) |  Loss2: (0.0000) | Acc: (79.00%) (36567/46208)
Epoch: 30 | Batch_idx: 370 |  Loss: (0.6005) |  Loss2: (0.0000) | Acc: (79.00%) (37575/47488)
Epoch: 30 | Batch_idx: 380 |  Loss: (0.6003) |  Loss2: (0.0000) | Acc: (79.00%) (38586/48768)
Epoch: 30 | Batch_idx: 390 |  Loss: (0.6001) |  Loss2: (0.0000) | Acc: (79.00%) (39570/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_030.pth.tar'
# TEST : Loss: (0.7046) | Acc: (75.00%) (7594/10000)
percent tensor([0.5268, 0.4732], device='cuda:0')
percent tensor([0.4961, 0.5039], device='cuda:0')
percent tensor([0.4660, 0.5340], device='cuda:0')
percent tensor([0.5370, 0.4630], device='cuda:0')
percent tensor([0.5307, 0.4693], device='cuda:0')
percent tensor([0.6367, 0.3633], device='cuda:0')
percent tensor([0.6101, 0.3899], device='cuda:0')
percent tensor([0.9851, 0.0149], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(172.8340, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(794.4263, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(785.2326, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1523.5641, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(515.6425, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2181.0874, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4296.8647, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1437.6057, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6092.9346, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12132.3027, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4048.1936, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(17123.6445, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 31 | Batch_idx: 0 |  Loss: (0.4886) |  Loss2: (0.0000) | Acc: (79.00%) (102/128)
Epoch: 31 | Batch_idx: 10 |  Loss: (0.5464) |  Loss2: (0.0000) | Acc: (79.00%) (1126/1408)
Epoch: 31 | Batch_idx: 20 |  Loss: (0.5657) |  Loss2: (0.0000) | Acc: (79.00%) (2145/2688)
Epoch: 31 | Batch_idx: 30 |  Loss: (0.5784) |  Loss2: (0.0000) | Acc: (79.00%) (3153/3968)
Epoch: 31 | Batch_idx: 40 |  Loss: (0.5834) |  Loss2: (0.0000) | Acc: (79.00%) (4164/5248)
Epoch: 31 | Batch_idx: 50 |  Loss: (0.5849) |  Loss2: (0.0000) | Acc: (79.00%) (5189/6528)
Epoch: 31 | Batch_idx: 60 |  Loss: (0.5822) |  Loss2: (0.0000) | Acc: (79.00%) (6212/7808)
Epoch: 31 | Batch_idx: 70 |  Loss: (0.5759) |  Loss2: (0.0000) | Acc: (80.00%) (7276/9088)
Epoch: 31 | Batch_idx: 80 |  Loss: (0.5779) |  Loss2: (0.0000) | Acc: (80.00%) (8303/10368)
Epoch: 31 | Batch_idx: 90 |  Loss: (0.5747) |  Loss2: (0.0000) | Acc: (80.00%) (9348/11648)
Epoch: 31 | Batch_idx: 100 |  Loss: (0.5747) |  Loss2: (0.0000) | Acc: (80.00%) (10359/12928)
Epoch: 31 | Batch_idx: 110 |  Loss: (0.5767) |  Loss2: (0.0000) | Acc: (80.00%) (11375/14208)
Epoch: 31 | Batch_idx: 120 |  Loss: (0.5726) |  Loss2: (0.0000) | Acc: (80.00%) (12426/15488)
Epoch: 31 | Batch_idx: 130 |  Loss: (0.5708) |  Loss2: (0.0000) | Acc: (80.00%) (13459/16768)
Epoch: 31 | Batch_idx: 140 |  Loss: (0.5726) |  Loss2: (0.0000) | Acc: (80.00%) (14484/18048)
Epoch: 31 | Batch_idx: 150 |  Loss: (0.5705) |  Loss2: (0.0000) | Acc: (80.00%) (15505/19328)
Epoch: 31 | Batch_idx: 160 |  Loss: (0.5683) |  Loss2: (0.0000) | Acc: (80.00%) (16540/20608)
Epoch: 31 | Batch_idx: 170 |  Loss: (0.5697) |  Loss2: (0.0000) | Acc: (80.00%) (17566/21888)
Epoch: 31 | Batch_idx: 180 |  Loss: (0.5678) |  Loss2: (0.0000) | Acc: (80.00%) (18624/23168)
Epoch: 31 | Batch_idx: 190 |  Loss: (0.5669) |  Loss2: (0.0000) | Acc: (80.00%) (19661/24448)
Epoch: 31 | Batch_idx: 200 |  Loss: (0.5676) |  Loss2: (0.0000) | Acc: (80.00%) (20684/25728)
Epoch: 31 | Batch_idx: 210 |  Loss: (0.5684) |  Loss2: (0.0000) | Acc: (80.00%) (21710/27008)
Epoch: 31 | Batch_idx: 220 |  Loss: (0.5685) |  Loss2: (0.0000) | Acc: (80.00%) (22742/28288)
Epoch: 31 | Batch_idx: 230 |  Loss: (0.5681) |  Loss2: (0.0000) | Acc: (80.00%) (23768/29568)
Epoch: 31 | Batch_idx: 240 |  Loss: (0.5669) |  Loss2: (0.0000) | Acc: (80.00%) (24808/30848)
Epoch: 31 | Batch_idx: 250 |  Loss: (0.5700) |  Loss2: (0.0000) | Acc: (80.00%) (25801/32128)
Epoch: 31 | Batch_idx: 260 |  Loss: (0.5689) |  Loss2: (0.0000) | Acc: (80.00%) (26846/33408)
Epoch: 31 | Batch_idx: 270 |  Loss: (0.5695) |  Loss2: (0.0000) | Acc: (80.00%) (27864/34688)
Epoch: 31 | Batch_idx: 280 |  Loss: (0.5709) |  Loss2: (0.0000) | Acc: (80.00%) (28884/35968)
Epoch: 31 | Batch_idx: 290 |  Loss: (0.5713) |  Loss2: (0.0000) | Acc: (80.00%) (29909/37248)
Epoch: 31 | Batch_idx: 300 |  Loss: (0.5718) |  Loss2: (0.0000) | Acc: (80.00%) (30926/38528)
Epoch: 31 | Batch_idx: 310 |  Loss: (0.5709) |  Loss2: (0.0000) | Acc: (80.00%) (31968/39808)
Epoch: 31 | Batch_idx: 320 |  Loss: (0.5716) |  Loss2: (0.0000) | Acc: (80.00%) (32981/41088)
Epoch: 31 | Batch_idx: 330 |  Loss: (0.5736) |  Loss2: (0.0000) | Acc: (80.00%) (33988/42368)
Epoch: 31 | Batch_idx: 340 |  Loss: (0.5721) |  Loss2: (0.0000) | Acc: (80.00%) (35037/43648)
Epoch: 31 | Batch_idx: 350 |  Loss: (0.5715) |  Loss2: (0.0000) | Acc: (80.00%) (36072/44928)
Epoch: 31 | Batch_idx: 360 |  Loss: (0.5711) |  Loss2: (0.0000) | Acc: (80.00%) (37120/46208)
Epoch: 31 | Batch_idx: 370 |  Loss: (0.5696) |  Loss2: (0.0000) | Acc: (80.00%) (38181/47488)
Epoch: 31 | Batch_idx: 380 |  Loss: (0.5692) |  Loss2: (0.0000) | Acc: (80.00%) (39210/48768)
Epoch: 31 | Batch_idx: 390 |  Loss: (0.5682) |  Loss2: (0.0000) | Acc: (80.00%) (40207/50000)
# TEST : Loss: (0.7063) | Acc: (76.00%) (7617/10000)
percent tensor([0.5271, 0.4729], device='cuda:0')
percent tensor([0.4961, 0.5039], device='cuda:0')
percent tensor([0.4669, 0.5331], device='cuda:0')
percent tensor([0.5367, 0.4633], device='cuda:0')
percent tensor([0.5304, 0.4696], device='cuda:0')
percent tensor([0.6361, 0.3639], device='cuda:0')
percent tensor([0.6124, 0.3876], device='cuda:0')
percent tensor([0.9869, 0.0131], device='cuda:0')
Epoch: 32 | Batch_idx: 0 |  Loss: (0.5135) |  Loss2: (0.0000) | Acc: (84.00%) (108/128)
Epoch: 32 | Batch_idx: 10 |  Loss: (0.5327) |  Loss2: (0.0000) | Acc: (81.00%) (1142/1408)
Epoch: 32 | Batch_idx: 20 |  Loss: (0.5333) |  Loss2: (0.0000) | Acc: (81.00%) (2197/2688)
Epoch: 32 | Batch_idx: 30 |  Loss: (0.5305) |  Loss2: (0.0000) | Acc: (81.00%) (3233/3968)
Epoch: 32 | Batch_idx: 40 |  Loss: (0.5414) |  Loss2: (0.0000) | Acc: (81.00%) (4256/5248)
Epoch: 32 | Batch_idx: 50 |  Loss: (0.5296) |  Loss2: (0.0000) | Acc: (81.00%) (5327/6528)
Epoch: 32 | Batch_idx: 60 |  Loss: (0.5345) |  Loss2: (0.0000) | Acc: (81.00%) (6350/7808)
Epoch: 32 | Batch_idx: 70 |  Loss: (0.5335) |  Loss2: (0.0000) | Acc: (81.00%) (7395/9088)
Epoch: 32 | Batch_idx: 80 |  Loss: (0.5284) |  Loss2: (0.0000) | Acc: (81.00%) (8460/10368)
Epoch: 32 | Batch_idx: 90 |  Loss: (0.5317) |  Loss2: (0.0000) | Acc: (81.00%) (9506/11648)
Epoch: 32 | Batch_idx: 100 |  Loss: (0.5308) |  Loss2: (0.0000) | Acc: (81.00%) (10557/12928)
Epoch: 32 | Batch_idx: 110 |  Loss: (0.5367) |  Loss2: (0.0000) | Acc: (81.00%) (11580/14208)
Epoch: 32 | Batch_idx: 120 |  Loss: (0.5352) |  Loss2: (0.0000) | Acc: (81.00%) (12633/15488)
Epoch: 32 | Batch_idx: 130 |  Loss: (0.5392) |  Loss2: (0.0000) | Acc: (81.00%) (13652/16768)
Epoch: 32 | Batch_idx: 140 |  Loss: (0.5442) |  Loss2: (0.0000) | Acc: (81.00%) (14658/18048)
Epoch: 32 | Batch_idx: 150 |  Loss: (0.5420) |  Loss2: (0.0000) | Acc: (81.00%) (15716/19328)
Epoch: 32 | Batch_idx: 160 |  Loss: (0.5441) |  Loss2: (0.0000) | Acc: (81.00%) (16740/20608)
Epoch: 32 | Batch_idx: 170 |  Loss: (0.5454) |  Loss2: (0.0000) | Acc: (81.00%) (17780/21888)
Epoch: 32 | Batch_idx: 180 |  Loss: (0.5440) |  Loss2: (0.0000) | Acc: (81.00%) (18833/23168)
Epoch: 32 | Batch_idx: 190 |  Loss: (0.5436) |  Loss2: (0.0000) | Acc: (81.00%) (19876/24448)
Epoch: 32 | Batch_idx: 200 |  Loss: (0.5436) |  Loss2: (0.0000) | Acc: (81.00%) (20904/25728)
Epoch: 32 | Batch_idx: 210 |  Loss: (0.5409) |  Loss2: (0.0000) | Acc: (81.00%) (21968/27008)
Epoch: 32 | Batch_idx: 220 |  Loss: (0.5409) |  Loss2: (0.0000) | Acc: (81.00%) (23015/28288)
Epoch: 32 | Batch_idx: 230 |  Loss: (0.5403) |  Loss2: (0.0000) | Acc: (81.00%) (24077/29568)
Epoch: 32 | Batch_idx: 240 |  Loss: (0.5400) |  Loss2: (0.0000) | Acc: (81.00%) (25116/30848)
Epoch: 32 | Batch_idx: 250 |  Loss: (0.5409) |  Loss2: (0.0000) | Acc: (81.00%) (26154/32128)
Epoch: 32 | Batch_idx: 260 |  Loss: (0.5417) |  Loss2: (0.0000) | Acc: (81.00%) (27178/33408)
Epoch: 32 | Batch_idx: 270 |  Loss: (0.5422) |  Loss2: (0.0000) | Acc: (81.00%) (28230/34688)
Epoch: 32 | Batch_idx: 280 |  Loss: (0.5455) |  Loss2: (0.0000) | Acc: (81.00%) (29218/35968)
Epoch: 32 | Batch_idx: 290 |  Loss: (0.5440) |  Loss2: (0.0000) | Acc: (81.00%) (30289/37248)
Epoch: 32 | Batch_idx: 300 |  Loss: (0.5435) |  Loss2: (0.0000) | Acc: (81.00%) (31332/38528)
Epoch: 32 | Batch_idx: 310 |  Loss: (0.5436) |  Loss2: (0.0000) | Acc: (81.00%) (32359/39808)
Epoch: 32 | Batch_idx: 320 |  Loss: (0.5440) |  Loss2: (0.0000) | Acc: (81.00%) (33384/41088)
Epoch: 32 | Batch_idx: 330 |  Loss: (0.5442) |  Loss2: (0.0000) | Acc: (81.00%) (34436/42368)
Epoch: 32 | Batch_idx: 340 |  Loss: (0.5432) |  Loss2: (0.0000) | Acc: (81.00%) (35488/43648)
Epoch: 32 | Batch_idx: 350 |  Loss: (0.5433) |  Loss2: (0.0000) | Acc: (81.00%) (36526/44928)
Epoch: 32 | Batch_idx: 360 |  Loss: (0.5428) |  Loss2: (0.0000) | Acc: (81.00%) (37564/46208)
Epoch: 32 | Batch_idx: 370 |  Loss: (0.5417) |  Loss2: (0.0000) | Acc: (81.00%) (38627/47488)
Epoch: 32 | Batch_idx: 380 |  Loss: (0.5418) |  Loss2: (0.0000) | Acc: (81.00%) (39661/48768)
Epoch: 32 | Batch_idx: 390 |  Loss: (0.5406) |  Loss2: (0.0000) | Acc: (81.00%) (40675/50000)
# TEST : Loss: (0.6897) | Acc: (76.00%) (7643/10000)
percent tensor([0.5269, 0.4731], device='cuda:0')
percent tensor([0.4961, 0.5039], device='cuda:0')
percent tensor([0.4656, 0.5344], device='cuda:0')
percent tensor([0.5364, 0.4636], device='cuda:0')
percent tensor([0.5299, 0.4701], device='cuda:0')
percent tensor([0.6276, 0.3724], device='cuda:0')
percent tensor([0.6089, 0.3911], device='cuda:0')
percent tensor([0.9832, 0.0168], device='cuda:0')
Epoch: 33 | Batch_idx: 0 |  Loss: (0.3821) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 33 | Batch_idx: 10 |  Loss: (0.5179) |  Loss2: (0.0000) | Acc: (82.00%) (1158/1408)
Epoch: 33 | Batch_idx: 20 |  Loss: (0.5295) |  Loss2: (0.0000) | Acc: (81.00%) (2203/2688)
Epoch: 33 | Batch_idx: 30 |  Loss: (0.5234) |  Loss2: (0.0000) | Acc: (82.00%) (3254/3968)
Epoch: 33 | Batch_idx: 40 |  Loss: (0.5233) |  Loss2: (0.0000) | Acc: (81.00%) (4296/5248)
Epoch: 33 | Batch_idx: 50 |  Loss: (0.5221) |  Loss2: (0.0000) | Acc: (81.00%) (5350/6528)
Epoch: 33 | Batch_idx: 60 |  Loss: (0.5264) |  Loss2: (0.0000) | Acc: (81.00%) (6384/7808)
Epoch: 33 | Batch_idx: 70 |  Loss: (0.5266) |  Loss2: (0.0000) | Acc: (81.00%) (7437/9088)
Epoch: 33 | Batch_idx: 80 |  Loss: (0.5214) |  Loss2: (0.0000) | Acc: (82.00%) (8506/10368)
Epoch: 33 | Batch_idx: 90 |  Loss: (0.5240) |  Loss2: (0.0000) | Acc: (81.00%) (9538/11648)
Epoch: 33 | Batch_idx: 100 |  Loss: (0.5229) |  Loss2: (0.0000) | Acc: (81.00%) (10581/12928)
Epoch: 33 | Batch_idx: 110 |  Loss: (0.5242) |  Loss2: (0.0000) | Acc: (81.00%) (11627/14208)
Epoch: 33 | Batch_idx: 120 |  Loss: (0.5264) |  Loss2: (0.0000) | Acc: (81.00%) (12679/15488)
Epoch: 33 | Batch_idx: 130 |  Loss: (0.5275) |  Loss2: (0.0000) | Acc: (81.00%) (13722/16768)
Epoch: 33 | Batch_idx: 140 |  Loss: (0.5277) |  Loss2: (0.0000) | Acc: (81.00%) (14767/18048)
Epoch: 33 | Batch_idx: 150 |  Loss: (0.5265) |  Loss2: (0.0000) | Acc: (81.00%) (15807/19328)
Epoch: 33 | Batch_idx: 160 |  Loss: (0.5276) |  Loss2: (0.0000) | Acc: (81.00%) (16851/20608)
Epoch: 33 | Batch_idx: 170 |  Loss: (0.5305) |  Loss2: (0.0000) | Acc: (81.00%) (17888/21888)
Epoch: 33 | Batch_idx: 180 |  Loss: (0.5304) |  Loss2: (0.0000) | Acc: (81.00%) (18938/23168)
Epoch: 33 | Batch_idx: 190 |  Loss: (0.5308) |  Loss2: (0.0000) | Acc: (81.00%) (19969/24448)
Epoch: 33 | Batch_idx: 200 |  Loss: (0.5289) |  Loss2: (0.0000) | Acc: (81.00%) (21043/25728)
Epoch: 33 | Batch_idx: 210 |  Loss: (0.5272) |  Loss2: (0.0000) | Acc: (81.00%) (22111/27008)
Epoch: 33 | Batch_idx: 220 |  Loss: (0.5262) |  Loss2: (0.0000) | Acc: (81.00%) (23169/28288)
Epoch: 33 | Batch_idx: 230 |  Loss: (0.5258) |  Loss2: (0.0000) | Acc: (81.00%) (24214/29568)
Epoch: 33 | Batch_idx: 240 |  Loss: (0.5235) |  Loss2: (0.0000) | Acc: (81.00%) (25283/30848)
Epoch: 33 | Batch_idx: 250 |  Loss: (0.5231) |  Loss2: (0.0000) | Acc: (81.00%) (26327/32128)
Epoch: 33 | Batch_idx: 260 |  Loss: (0.5231) |  Loss2: (0.0000) | Acc: (81.00%) (27377/33408)
Epoch: 33 | Batch_idx: 270 |  Loss: (0.5211) |  Loss2: (0.0000) | Acc: (82.00%) (28456/34688)
Epoch: 33 | Batch_idx: 280 |  Loss: (0.5199) |  Loss2: (0.0000) | Acc: (82.00%) (29518/35968)
Epoch: 33 | Batch_idx: 290 |  Loss: (0.5207) |  Loss2: (0.0000) | Acc: (82.00%) (30563/37248)
Epoch: 33 | Batch_idx: 300 |  Loss: (0.5204) |  Loss2: (0.0000) | Acc: (82.00%) (31622/38528)
Epoch: 33 | Batch_idx: 310 |  Loss: (0.5204) |  Loss2: (0.0000) | Acc: (82.00%) (32657/39808)
Epoch: 33 | Batch_idx: 320 |  Loss: (0.5211) |  Loss2: (0.0000) | Acc: (82.00%) (33695/41088)
Epoch: 33 | Batch_idx: 330 |  Loss: (0.5210) |  Loss2: (0.0000) | Acc: (82.00%) (34755/42368)
Epoch: 33 | Batch_idx: 340 |  Loss: (0.5215) |  Loss2: (0.0000) | Acc: (81.00%) (35781/43648)
Epoch: 33 | Batch_idx: 350 |  Loss: (0.5204) |  Loss2: (0.0000) | Acc: (82.00%) (36847/44928)
Epoch: 33 | Batch_idx: 360 |  Loss: (0.5205) |  Loss2: (0.0000) | Acc: (82.00%) (37901/46208)
Epoch: 33 | Batch_idx: 370 |  Loss: (0.5203) |  Loss2: (0.0000) | Acc: (82.00%) (38962/47488)
Epoch: 33 | Batch_idx: 380 |  Loss: (0.5205) |  Loss2: (0.0000) | Acc: (82.00%) (40014/48768)
Epoch: 33 | Batch_idx: 390 |  Loss: (0.5210) |  Loss2: (0.0000) | Acc: (82.00%) (41023/50000)
# TEST : Loss: (0.6170) | Acc: (79.00%) (7932/10000)
percent tensor([0.5272, 0.4728], device='cuda:0')
percent tensor([0.4961, 0.5039], device='cuda:0')
percent tensor([0.4659, 0.5341], device='cuda:0')
percent tensor([0.5366, 0.4634], device='cuda:0')
percent tensor([0.5309, 0.4691], device='cuda:0')
percent tensor([0.6348, 0.3652], device='cuda:0')
percent tensor([0.6119, 0.3881], device='cuda:0')
percent tensor([0.9845, 0.0155], device='cuda:0')
Epoch: 34 | Batch_idx: 0 |  Loss: (0.4431) |  Loss2: (0.0000) | Acc: (86.00%) (111/128)
Epoch: 34 | Batch_idx: 10 |  Loss: (0.5186) |  Loss2: (0.0000) | Acc: (81.00%) (1152/1408)
Epoch: 34 | Batch_idx: 20 |  Loss: (0.5151) |  Loss2: (0.0000) | Acc: (82.00%) (2210/2688)
Epoch: 34 | Batch_idx: 30 |  Loss: (0.5079) |  Loss2: (0.0000) | Acc: (82.00%) (3280/3968)
Epoch: 34 | Batch_idx: 40 |  Loss: (0.5020) |  Loss2: (0.0000) | Acc: (82.00%) (4351/5248)
Epoch: 34 | Batch_idx: 50 |  Loss: (0.4932) |  Loss2: (0.0000) | Acc: (83.00%) (5427/6528)
Epoch: 34 | Batch_idx: 60 |  Loss: (0.4925) |  Loss2: (0.0000) | Acc: (83.00%) (6495/7808)
Epoch: 34 | Batch_idx: 70 |  Loss: (0.4898) |  Loss2: (0.0000) | Acc: (83.00%) (7576/9088)
Epoch: 34 | Batch_idx: 80 |  Loss: (0.4943) |  Loss2: (0.0000) | Acc: (83.00%) (8634/10368)
Epoch: 34 | Batch_idx: 90 |  Loss: (0.4978) |  Loss2: (0.0000) | Acc: (83.00%) (9691/11648)
Epoch: 34 | Batch_idx: 100 |  Loss: (0.4987) |  Loss2: (0.0000) | Acc: (83.00%) (10746/12928)
Epoch: 34 | Batch_idx: 110 |  Loss: (0.4983) |  Loss2: (0.0000) | Acc: (83.00%) (11800/14208)
Epoch: 34 | Batch_idx: 120 |  Loss: (0.4991) |  Loss2: (0.0000) | Acc: (82.00%) (12854/15488)
Epoch: 34 | Batch_idx: 130 |  Loss: (0.4991) |  Loss2: (0.0000) | Acc: (82.00%) (13905/16768)
Epoch: 34 | Batch_idx: 140 |  Loss: (0.4971) |  Loss2: (0.0000) | Acc: (83.00%) (14988/18048)
Epoch: 34 | Batch_idx: 150 |  Loss: (0.4985) |  Loss2: (0.0000) | Acc: (83.00%) (16050/19328)
Epoch: 34 | Batch_idx: 160 |  Loss: (0.4989) |  Loss2: (0.0000) | Acc: (83.00%) (17115/20608)
Epoch: 34 | Batch_idx: 170 |  Loss: (0.5014) |  Loss2: (0.0000) | Acc: (82.00%) (18158/21888)
Epoch: 34 | Batch_idx: 180 |  Loss: (0.5004) |  Loss2: (0.0000) | Acc: (83.00%) (19233/23168)
Epoch: 34 | Batch_idx: 190 |  Loss: (0.5018) |  Loss2: (0.0000) | Acc: (83.00%) (20292/24448)
Epoch: 34 | Batch_idx: 200 |  Loss: (0.5035) |  Loss2: (0.0000) | Acc: (82.00%) (21324/25728)
Epoch: 34 | Batch_idx: 210 |  Loss: (0.5066) |  Loss2: (0.0000) | Acc: (82.00%) (22351/27008)
Epoch: 34 | Batch_idx: 220 |  Loss: (0.5061) |  Loss2: (0.0000) | Acc: (82.00%) (23408/28288)
Epoch: 34 | Batch_idx: 230 |  Loss: (0.5059) |  Loss2: (0.0000) | Acc: (82.00%) (24466/29568)
Epoch: 34 | Batch_idx: 240 |  Loss: (0.5055) |  Loss2: (0.0000) | Acc: (82.00%) (25527/30848)
Epoch: 34 | Batch_idx: 250 |  Loss: (0.5041) |  Loss2: (0.0000) | Acc: (82.00%) (26602/32128)
Epoch: 34 | Batch_idx: 260 |  Loss: (0.5037) |  Loss2: (0.0000) | Acc: (82.00%) (27671/33408)
Epoch: 34 | Batch_idx: 270 |  Loss: (0.5024) |  Loss2: (0.0000) | Acc: (82.00%) (28749/34688)
Epoch: 34 | Batch_idx: 280 |  Loss: (0.5032) |  Loss2: (0.0000) | Acc: (82.00%) (29790/35968)
Epoch: 34 | Batch_idx: 290 |  Loss: (0.5036) |  Loss2: (0.0000) | Acc: (82.00%) (30841/37248)
Epoch: 34 | Batch_idx: 300 |  Loss: (0.5019) |  Loss2: (0.0000) | Acc: (82.00%) (31907/38528)
Epoch: 34 | Batch_idx: 310 |  Loss: (0.5013) |  Loss2: (0.0000) | Acc: (82.00%) (32989/39808)
Epoch: 34 | Batch_idx: 320 |  Loss: (0.5017) |  Loss2: (0.0000) | Acc: (82.00%) (34044/41088)
Epoch: 34 | Batch_idx: 330 |  Loss: (0.5021) |  Loss2: (0.0000) | Acc: (82.00%) (35086/42368)
Epoch: 34 | Batch_idx: 340 |  Loss: (0.5022) |  Loss2: (0.0000) | Acc: (82.00%) (36152/43648)
Epoch: 34 | Batch_idx: 350 |  Loss: (0.5019) |  Loss2: (0.0000) | Acc: (82.00%) (37215/44928)
Epoch: 34 | Batch_idx: 360 |  Loss: (0.5014) |  Loss2: (0.0000) | Acc: (82.00%) (38283/46208)
Epoch: 34 | Batch_idx: 370 |  Loss: (0.5016) |  Loss2: (0.0000) | Acc: (82.00%) (39343/47488)
Epoch: 34 | Batch_idx: 380 |  Loss: (0.5019) |  Loss2: (0.0000) | Acc: (82.00%) (40384/48768)
Epoch: 34 | Batch_idx: 390 |  Loss: (0.5017) |  Loss2: (0.0000) | Acc: (82.00%) (41405/50000)
# TEST : Loss: (0.6709) | Acc: (77.00%) (7789/10000)
percent tensor([0.5269, 0.4731], device='cuda:0')
percent tensor([0.4961, 0.5039], device='cuda:0')
percent tensor([0.4659, 0.5341], device='cuda:0')
percent tensor([0.5359, 0.4641], device='cuda:0')
percent tensor([0.5313, 0.4687], device='cuda:0')
percent tensor([0.6375, 0.3625], device='cuda:0')
percent tensor([0.6189, 0.3811], device='cuda:0')
percent tensor([0.9870, 0.0130], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 35 | Batch_idx: 0 |  Loss: (0.5847) |  Loss2: (0.0000) | Acc: (84.00%) (108/128)
Epoch: 35 | Batch_idx: 10 |  Loss: (0.5969) |  Loss2: (0.0000) | Acc: (79.00%) (1114/1408)
Epoch: 35 | Batch_idx: 20 |  Loss: (0.6391) |  Loss2: (0.0000) | Acc: (77.00%) (2081/2688)
Epoch: 35 | Batch_idx: 30 |  Loss: (0.6791) |  Loss2: (0.0000) | Acc: (76.00%) (3026/3968)
Epoch: 35 | Batch_idx: 40 |  Loss: (0.6876) |  Loss2: (0.0000) | Acc: (76.00%) (3990/5248)
Epoch: 35 | Batch_idx: 50 |  Loss: (0.6855) |  Loss2: (0.0000) | Acc: (76.00%) (4964/6528)
Epoch: 35 | Batch_idx: 60 |  Loss: (0.6912) |  Loss2: (0.0000) | Acc: (76.00%) (5945/7808)
Epoch: 35 | Batch_idx: 70 |  Loss: (0.6894) |  Loss2: (0.0000) | Acc: (76.00%) (6942/9088)
Epoch: 35 | Batch_idx: 80 |  Loss: (0.6949) |  Loss2: (0.0000) | Acc: (76.00%) (7894/10368)
Epoch: 35 | Batch_idx: 90 |  Loss: (0.6940) |  Loss2: (0.0000) | Acc: (76.00%) (8864/11648)
Epoch: 35 | Batch_idx: 100 |  Loss: (0.6895) |  Loss2: (0.0000) | Acc: (76.00%) (9865/12928)
Epoch: 35 | Batch_idx: 110 |  Loss: (0.6851) |  Loss2: (0.0000) | Acc: (76.00%) (10862/14208)
Epoch: 35 | Batch_idx: 120 |  Loss: (0.6749) |  Loss2: (0.0000) | Acc: (76.00%) (11895/15488)
Epoch: 35 | Batch_idx: 130 |  Loss: (0.6715) |  Loss2: (0.0000) | Acc: (76.00%) (12883/16768)
Epoch: 35 | Batch_idx: 140 |  Loss: (0.6665) |  Loss2: (0.0000) | Acc: (77.00%) (13904/18048)
Epoch: 35 | Batch_idx: 150 |  Loss: (0.6624) |  Loss2: (0.0000) | Acc: (77.00%) (14926/19328)
Epoch: 35 | Batch_idx: 160 |  Loss: (0.6582) |  Loss2: (0.0000) | Acc: (77.00%) (15950/20608)
Epoch: 35 | Batch_idx: 170 |  Loss: (0.6545) |  Loss2: (0.0000) | Acc: (77.00%) (16971/21888)
Epoch: 35 | Batch_idx: 180 |  Loss: (0.6526) |  Loss2: (0.0000) | Acc: (77.00%) (17964/23168)
Epoch: 35 | Batch_idx: 190 |  Loss: (0.6509) |  Loss2: (0.0000) | Acc: (77.00%) (18962/24448)
Epoch: 35 | Batch_idx: 200 |  Loss: (0.6500) |  Loss2: (0.0000) | Acc: (77.00%) (19964/25728)
Epoch: 35 | Batch_idx: 210 |  Loss: (0.6468) |  Loss2: (0.0000) | Acc: (77.00%) (20985/27008)
Epoch: 35 | Batch_idx: 220 |  Loss: (0.6457) |  Loss2: (0.0000) | Acc: (77.00%) (21975/28288)
Epoch: 35 | Batch_idx: 230 |  Loss: (0.6427) |  Loss2: (0.0000) | Acc: (77.00%) (23010/29568)
Epoch: 35 | Batch_idx: 240 |  Loss: (0.6389) |  Loss2: (0.0000) | Acc: (77.00%) (24053/30848)
Epoch: 35 | Batch_idx: 250 |  Loss: (0.6365) |  Loss2: (0.0000) | Acc: (78.00%) (25078/32128)
Epoch: 35 | Batch_idx: 260 |  Loss: (0.6356) |  Loss2: (0.0000) | Acc: (78.00%) (26083/33408)
Epoch: 35 | Batch_idx: 270 |  Loss: (0.6342) |  Loss2: (0.0000) | Acc: (78.00%) (27107/34688)
Epoch: 35 | Batch_idx: 280 |  Loss: (0.6330) |  Loss2: (0.0000) | Acc: (78.00%) (28121/35968)
Epoch: 35 | Batch_idx: 290 |  Loss: (0.6333) |  Loss2: (0.0000) | Acc: (78.00%) (29108/37248)
Epoch: 35 | Batch_idx: 300 |  Loss: (0.6313) |  Loss2: (0.0000) | Acc: (78.00%) (30132/38528)
Epoch: 35 | Batch_idx: 310 |  Loss: (0.6292) |  Loss2: (0.0000) | Acc: (78.00%) (31155/39808)
Epoch: 35 | Batch_idx: 320 |  Loss: (0.6274) |  Loss2: (0.0000) | Acc: (78.00%) (32184/41088)
Epoch: 35 | Batch_idx: 330 |  Loss: (0.6264) |  Loss2: (0.0000) | Acc: (78.00%) (33195/42368)
Epoch: 35 | Batch_idx: 340 |  Loss: (0.6249) |  Loss2: (0.0000) | Acc: (78.00%) (34225/43648)
Epoch: 35 | Batch_idx: 350 |  Loss: (0.6236) |  Loss2: (0.0000) | Acc: (78.00%) (35253/44928)
Epoch: 35 | Batch_idx: 360 |  Loss: (0.6213) |  Loss2: (0.0000) | Acc: (78.00%) (36299/46208)
Epoch: 35 | Batch_idx: 370 |  Loss: (0.6201) |  Loss2: (0.0000) | Acc: (78.00%) (37333/47488)
Epoch: 35 | Batch_idx: 380 |  Loss: (0.6200) |  Loss2: (0.0000) | Acc: (78.00%) (38334/48768)
Epoch: 35 | Batch_idx: 390 |  Loss: (0.6194) |  Loss2: (0.0000) | Acc: (78.00%) (39315/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_035.pth.tar'
# TEST : Loss: (0.6273) | Acc: (78.00%) (7875/10000)
percent tensor([0.5046, 0.4954], device='cuda:0')
percent tensor([0.4889, 0.5111], device='cuda:0')
percent tensor([0.4663, 0.5337], device='cuda:0')
percent tensor([0.5566, 0.4434], device='cuda:0')
percent tensor([0.5478, 0.4522], device='cuda:0')
percent tensor([0.5900, 0.4100], device='cuda:0')
percent tensor([0.6314, 0.3686], device='cuda:0')
percent tensor([0.9844, 0.0156], device='cuda:0')
Epoch: 36 | Batch_idx: 0 |  Loss: (0.4838) |  Loss2: (0.0000) | Acc: (78.00%) (101/128)
Epoch: 36 | Batch_idx: 10 |  Loss: (0.5606) |  Loss2: (0.0000) | Acc: (79.00%) (1126/1408)
Epoch: 36 | Batch_idx: 20 |  Loss: (0.5425) |  Loss2: (0.0000) | Acc: (80.00%) (2160/2688)
Epoch: 36 | Batch_idx: 30 |  Loss: (0.5455) |  Loss2: (0.0000) | Acc: (80.00%) (3195/3968)
Epoch: 36 | Batch_idx: 40 |  Loss: (0.5643) |  Loss2: (0.0000) | Acc: (79.00%) (4198/5248)
Epoch: 36 | Batch_idx: 50 |  Loss: (0.5705) |  Loss2: (0.0000) | Acc: (79.00%) (5189/6528)
Epoch: 36 | Batch_idx: 60 |  Loss: (0.5685) |  Loss2: (0.0000) | Acc: (79.00%) (6222/7808)
Epoch: 36 | Batch_idx: 70 |  Loss: (0.5622) |  Loss2: (0.0000) | Acc: (79.00%) (7261/9088)
Epoch: 36 | Batch_idx: 80 |  Loss: (0.5600) |  Loss2: (0.0000) | Acc: (80.00%) (8307/10368)
Epoch: 36 | Batch_idx: 90 |  Loss: (0.5644) |  Loss2: (0.0000) | Acc: (79.00%) (9293/11648)
Epoch: 36 | Batch_idx: 100 |  Loss: (0.5683) |  Loss2: (0.0000) | Acc: (79.00%) (10296/12928)
Epoch: 36 | Batch_idx: 110 |  Loss: (0.5667) |  Loss2: (0.0000) | Acc: (79.00%) (11332/14208)
Epoch: 36 | Batch_idx: 120 |  Loss: (0.5644) |  Loss2: (0.0000) | Acc: (79.00%) (12373/15488)
Epoch: 36 | Batch_idx: 130 |  Loss: (0.5647) |  Loss2: (0.0000) | Acc: (79.00%) (13401/16768)
Epoch: 36 | Batch_idx: 140 |  Loss: (0.5641) |  Loss2: (0.0000) | Acc: (80.00%) (14441/18048)
Epoch: 36 | Batch_idx: 150 |  Loss: (0.5645) |  Loss2: (0.0000) | Acc: (80.00%) (15473/19328)
Epoch: 36 | Batch_idx: 160 |  Loss: (0.5624) |  Loss2: (0.0000) | Acc: (80.00%) (16526/20608)
Epoch: 36 | Batch_idx: 170 |  Loss: (0.5646) |  Loss2: (0.0000) | Acc: (80.00%) (17542/21888)
Epoch: 36 | Batch_idx: 180 |  Loss: (0.5632) |  Loss2: (0.0000) | Acc: (80.00%) (18573/23168)
Epoch: 36 | Batch_idx: 190 |  Loss: (0.5640) |  Loss2: (0.0000) | Acc: (80.00%) (19604/24448)
Epoch: 36 | Batch_idx: 200 |  Loss: (0.5639) |  Loss2: (0.0000) | Acc: (80.00%) (20626/25728)
Epoch: 36 | Batch_idx: 210 |  Loss: (0.5652) |  Loss2: (0.0000) | Acc: (80.00%) (21659/27008)
Epoch: 36 | Batch_idx: 220 |  Loss: (0.5637) |  Loss2: (0.0000) | Acc: (80.00%) (22694/28288)
Epoch: 36 | Batch_idx: 230 |  Loss: (0.5640) |  Loss2: (0.0000) | Acc: (80.00%) (23730/29568)
Epoch: 36 | Batch_idx: 240 |  Loss: (0.5625) |  Loss2: (0.0000) | Acc: (80.00%) (24777/30848)
Epoch: 36 | Batch_idx: 250 |  Loss: (0.5617) |  Loss2: (0.0000) | Acc: (80.00%) (25803/32128)
Epoch: 36 | Batch_idx: 260 |  Loss: (0.5621) |  Loss2: (0.0000) | Acc: (80.00%) (26831/33408)
Epoch: 36 | Batch_idx: 270 |  Loss: (0.5632) |  Loss2: (0.0000) | Acc: (80.00%) (27842/34688)
Epoch: 36 | Batch_idx: 280 |  Loss: (0.5609) |  Loss2: (0.0000) | Acc: (80.00%) (28891/35968)
Epoch: 36 | Batch_idx: 290 |  Loss: (0.5602) |  Loss2: (0.0000) | Acc: (80.00%) (29941/37248)
Epoch: 36 | Batch_idx: 300 |  Loss: (0.5605) |  Loss2: (0.0000) | Acc: (80.00%) (30970/38528)
Epoch: 36 | Batch_idx: 310 |  Loss: (0.5594) |  Loss2: (0.0000) | Acc: (80.00%) (32022/39808)
Epoch: 36 | Batch_idx: 320 |  Loss: (0.5581) |  Loss2: (0.0000) | Acc: (80.00%) (33078/41088)
Epoch: 36 | Batch_idx: 330 |  Loss: (0.5568) |  Loss2: (0.0000) | Acc: (80.00%) (34147/42368)
Epoch: 36 | Batch_idx: 340 |  Loss: (0.5575) |  Loss2: (0.0000) | Acc: (80.00%) (35173/43648)
Epoch: 36 | Batch_idx: 350 |  Loss: (0.5576) |  Loss2: (0.0000) | Acc: (80.00%) (36194/44928)
Epoch: 36 | Batch_idx: 360 |  Loss: (0.5579) |  Loss2: (0.0000) | Acc: (80.00%) (37227/46208)
Epoch: 36 | Batch_idx: 370 |  Loss: (0.5573) |  Loss2: (0.0000) | Acc: (80.00%) (38272/47488)
Epoch: 36 | Batch_idx: 380 |  Loss: (0.5570) |  Loss2: (0.0000) | Acc: (80.00%) (39316/48768)
Epoch: 36 | Batch_idx: 390 |  Loss: (0.5566) |  Loss2: (0.0000) | Acc: (80.00%) (40317/50000)
# TEST : Loss: (0.5957) | Acc: (79.00%) (7970/10000)
percent tensor([0.5034, 0.4966], device='cuda:0')
percent tensor([0.4869, 0.5131], device='cuda:0')
percent tensor([0.4695, 0.5305], device='cuda:0')
percent tensor([0.5646, 0.4354], device='cuda:0')
percent tensor([0.5497, 0.4503], device='cuda:0')
percent tensor([0.5883, 0.4117], device='cuda:0')
percent tensor([0.6478, 0.3522], device='cuda:0')
percent tensor([0.9860, 0.0140], device='cuda:0')
Epoch: 37 | Batch_idx: 0 |  Loss: (0.4939) |  Loss2: (0.0000) | Acc: (82.00%) (105/128)
Epoch: 37 | Batch_idx: 10 |  Loss: (0.5485) |  Loss2: (0.0000) | Acc: (80.00%) (1139/1408)
Epoch: 37 | Batch_idx: 20 |  Loss: (0.5632) |  Loss2: (0.0000) | Acc: (80.00%) (2172/2688)
Epoch: 37 | Batch_idx: 30 |  Loss: (0.5527) |  Loss2: (0.0000) | Acc: (81.00%) (3223/3968)
Epoch: 37 | Batch_idx: 40 |  Loss: (0.5391) |  Loss2: (0.0000) | Acc: (81.00%) (4298/5248)
Epoch: 37 | Batch_idx: 50 |  Loss: (0.5337) |  Loss2: (0.0000) | Acc: (81.00%) (5351/6528)
Epoch: 37 | Batch_idx: 60 |  Loss: (0.5378) |  Loss2: (0.0000) | Acc: (81.00%) (6368/7808)
Epoch: 37 | Batch_idx: 70 |  Loss: (0.5440) |  Loss2: (0.0000) | Acc: (81.00%) (7401/9088)
Epoch: 37 | Batch_idx: 80 |  Loss: (0.5487) |  Loss2: (0.0000) | Acc: (81.00%) (8417/10368)
Epoch: 37 | Batch_idx: 90 |  Loss: (0.5489) |  Loss2: (0.0000) | Acc: (81.00%) (9447/11648)
Epoch: 37 | Batch_idx: 100 |  Loss: (0.5469) |  Loss2: (0.0000) | Acc: (81.00%) (10495/12928)
Epoch: 37 | Batch_idx: 110 |  Loss: (0.5485) |  Loss2: (0.0000) | Acc: (81.00%) (11526/14208)
Epoch: 37 | Batch_idx: 120 |  Loss: (0.5477) |  Loss2: (0.0000) | Acc: (81.00%) (12569/15488)
Epoch: 37 | Batch_idx: 130 |  Loss: (0.5467) |  Loss2: (0.0000) | Acc: (81.00%) (13614/16768)
Epoch: 37 | Batch_idx: 140 |  Loss: (0.5460) |  Loss2: (0.0000) | Acc: (81.00%) (14675/18048)
Epoch: 37 | Batch_idx: 150 |  Loss: (0.5463) |  Loss2: (0.0000) | Acc: (81.00%) (15710/19328)
Epoch: 37 | Batch_idx: 160 |  Loss: (0.5436) |  Loss2: (0.0000) | Acc: (81.00%) (16773/20608)
Epoch: 37 | Batch_idx: 170 |  Loss: (0.5420) |  Loss2: (0.0000) | Acc: (81.00%) (17818/21888)
Epoch: 37 | Batch_idx: 180 |  Loss: (0.5448) |  Loss2: (0.0000) | Acc: (81.00%) (18842/23168)
Epoch: 37 | Batch_idx: 190 |  Loss: (0.5442) |  Loss2: (0.0000) | Acc: (81.00%) (19891/24448)
Epoch: 37 | Batch_idx: 200 |  Loss: (0.5425) |  Loss2: (0.0000) | Acc: (81.00%) (20938/25728)
Epoch: 37 | Batch_idx: 210 |  Loss: (0.5442) |  Loss2: (0.0000) | Acc: (81.00%) (21960/27008)
Epoch: 37 | Batch_idx: 220 |  Loss: (0.5421) |  Loss2: (0.0000) | Acc: (81.00%) (23009/28288)
Epoch: 37 | Batch_idx: 230 |  Loss: (0.5407) |  Loss2: (0.0000) | Acc: (81.00%) (24060/29568)
Epoch: 37 | Batch_idx: 240 |  Loss: (0.5409) |  Loss2: (0.0000) | Acc: (81.00%) (25092/30848)
Epoch: 37 | Batch_idx: 250 |  Loss: (0.5408) |  Loss2: (0.0000) | Acc: (81.00%) (26129/32128)
Epoch: 37 | Batch_idx: 260 |  Loss: (0.5411) |  Loss2: (0.0000) | Acc: (81.00%) (27152/33408)
Epoch: 37 | Batch_idx: 270 |  Loss: (0.5391) |  Loss2: (0.0000) | Acc: (81.00%) (28220/34688)
Epoch: 37 | Batch_idx: 280 |  Loss: (0.5369) |  Loss2: (0.0000) | Acc: (81.00%) (29299/35968)
Epoch: 37 | Batch_idx: 290 |  Loss: (0.5358) |  Loss2: (0.0000) | Acc: (81.00%) (30352/37248)
Epoch: 37 | Batch_idx: 300 |  Loss: (0.5358) |  Loss2: (0.0000) | Acc: (81.00%) (31410/38528)
Epoch: 37 | Batch_idx: 310 |  Loss: (0.5366) |  Loss2: (0.0000) | Acc: (81.00%) (32437/39808)
Epoch: 37 | Batch_idx: 320 |  Loss: (0.5369) |  Loss2: (0.0000) | Acc: (81.00%) (33492/41088)
Epoch: 37 | Batch_idx: 330 |  Loss: (0.5370) |  Loss2: (0.0000) | Acc: (81.00%) (34525/42368)
Epoch: 37 | Batch_idx: 340 |  Loss: (0.5365) |  Loss2: (0.0000) | Acc: (81.00%) (35578/43648)
Epoch: 37 | Batch_idx: 350 |  Loss: (0.5361) |  Loss2: (0.0000) | Acc: (81.00%) (36631/44928)
Epoch: 37 | Batch_idx: 360 |  Loss: (0.5356) |  Loss2: (0.0000) | Acc: (81.00%) (37681/46208)
Epoch: 37 | Batch_idx: 370 |  Loss: (0.5354) |  Loss2: (0.0000) | Acc: (81.00%) (38732/47488)
Epoch: 37 | Batch_idx: 380 |  Loss: (0.5347) |  Loss2: (0.0000) | Acc: (81.00%) (39790/48768)
Epoch: 37 | Batch_idx: 390 |  Loss: (0.5347) |  Loss2: (0.0000) | Acc: (81.00%) (40777/50000)
# TEST : Loss: (0.5843) | Acc: (80.00%) (8011/10000)
percent tensor([0.5042, 0.4958], device='cuda:0')
percent tensor([0.4856, 0.5144], device='cuda:0')
percent tensor([0.4695, 0.5305], device='cuda:0')
percent tensor([0.5658, 0.4342], device='cuda:0')
percent tensor([0.5493, 0.4507], device='cuda:0')
percent tensor([0.5930, 0.4070], device='cuda:0')
percent tensor([0.6598, 0.3402], device='cuda:0')
percent tensor([0.9882, 0.0118], device='cuda:0')
Epoch: 38 | Batch_idx: 0 |  Loss: (0.3419) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 38 | Batch_idx: 10 |  Loss: (0.4893) |  Loss2: (0.0000) | Acc: (83.00%) (1176/1408)
Epoch: 38 | Batch_idx: 20 |  Loss: (0.5088) |  Loss2: (0.0000) | Acc: (82.00%) (2214/2688)
Epoch: 38 | Batch_idx: 30 |  Loss: (0.5213) |  Loss2: (0.0000) | Acc: (81.00%) (3250/3968)
Epoch: 38 | Batch_idx: 40 |  Loss: (0.5098) |  Loss2: (0.0000) | Acc: (82.00%) (4326/5248)
Epoch: 38 | Batch_idx: 50 |  Loss: (0.5092) |  Loss2: (0.0000) | Acc: (82.00%) (5370/6528)
Epoch: 38 | Batch_idx: 60 |  Loss: (0.5059) |  Loss2: (0.0000) | Acc: (82.00%) (6437/7808)
Epoch: 38 | Batch_idx: 70 |  Loss: (0.5122) |  Loss2: (0.0000) | Acc: (82.00%) (7456/9088)
Epoch: 38 | Batch_idx: 80 |  Loss: (0.5104) |  Loss2: (0.0000) | Acc: (82.00%) (8514/10368)
Epoch: 38 | Batch_idx: 90 |  Loss: (0.5141) |  Loss2: (0.0000) | Acc: (82.00%) (9555/11648)
Epoch: 38 | Batch_idx: 100 |  Loss: (0.5164) |  Loss2: (0.0000) | Acc: (82.00%) (10605/12928)
Epoch: 38 | Batch_idx: 110 |  Loss: (0.5220) |  Loss2: (0.0000) | Acc: (81.00%) (11631/14208)
Epoch: 38 | Batch_idx: 120 |  Loss: (0.5199) |  Loss2: (0.0000) | Acc: (81.00%) (12684/15488)
Epoch: 38 | Batch_idx: 130 |  Loss: (0.5218) |  Loss2: (0.0000) | Acc: (81.00%) (13721/16768)
Epoch: 38 | Batch_idx: 140 |  Loss: (0.5215) |  Loss2: (0.0000) | Acc: (81.00%) (14759/18048)
Epoch: 38 | Batch_idx: 150 |  Loss: (0.5223) |  Loss2: (0.0000) | Acc: (81.00%) (15797/19328)
Epoch: 38 | Batch_idx: 160 |  Loss: (0.5209) |  Loss2: (0.0000) | Acc: (81.00%) (16859/20608)
Epoch: 38 | Batch_idx: 170 |  Loss: (0.5198) |  Loss2: (0.0000) | Acc: (81.00%) (17915/21888)
Epoch: 38 | Batch_idx: 180 |  Loss: (0.5212) |  Loss2: (0.0000) | Acc: (81.00%) (18948/23168)
Epoch: 38 | Batch_idx: 190 |  Loss: (0.5220) |  Loss2: (0.0000) | Acc: (81.00%) (19989/24448)
Epoch: 38 | Batch_idx: 200 |  Loss: (0.5225) |  Loss2: (0.0000) | Acc: (81.00%) (21031/25728)
Epoch: 38 | Batch_idx: 210 |  Loss: (0.5234) |  Loss2: (0.0000) | Acc: (81.00%) (22075/27008)
Epoch: 38 | Batch_idx: 220 |  Loss: (0.5240) |  Loss2: (0.0000) | Acc: (81.00%) (23119/28288)
Epoch: 38 | Batch_idx: 230 |  Loss: (0.5241) |  Loss2: (0.0000) | Acc: (81.00%) (24158/29568)
Epoch: 38 | Batch_idx: 240 |  Loss: (0.5228) |  Loss2: (0.0000) | Acc: (81.00%) (25219/30848)
Epoch: 38 | Batch_idx: 250 |  Loss: (0.5224) |  Loss2: (0.0000) | Acc: (81.00%) (26270/32128)
Epoch: 38 | Batch_idx: 260 |  Loss: (0.5224) |  Loss2: (0.0000) | Acc: (81.00%) (27317/33408)
Epoch: 38 | Batch_idx: 270 |  Loss: (0.5219) |  Loss2: (0.0000) | Acc: (81.00%) (28360/34688)
Epoch: 38 | Batch_idx: 280 |  Loss: (0.5217) |  Loss2: (0.0000) | Acc: (81.00%) (29415/35968)
Epoch: 38 | Batch_idx: 290 |  Loss: (0.5216) |  Loss2: (0.0000) | Acc: (81.00%) (30459/37248)
Epoch: 38 | Batch_idx: 300 |  Loss: (0.5210) |  Loss2: (0.0000) | Acc: (81.00%) (31512/38528)
Epoch: 38 | Batch_idx: 310 |  Loss: (0.5213) |  Loss2: (0.0000) | Acc: (81.00%) (32563/39808)
Epoch: 38 | Batch_idx: 320 |  Loss: (0.5220) |  Loss2: (0.0000) | Acc: (81.00%) (33604/41088)
Epoch: 38 | Batch_idx: 330 |  Loss: (0.5211) |  Loss2: (0.0000) | Acc: (81.00%) (34671/42368)
Epoch: 38 | Batch_idx: 340 |  Loss: (0.5220) |  Loss2: (0.0000) | Acc: (81.00%) (35717/43648)
Epoch: 38 | Batch_idx: 350 |  Loss: (0.5217) |  Loss2: (0.0000) | Acc: (81.00%) (36774/44928)
Epoch: 38 | Batch_idx: 360 |  Loss: (0.5216) |  Loss2: (0.0000) | Acc: (81.00%) (37815/46208)
Epoch: 38 | Batch_idx: 370 |  Loss: (0.5218) |  Loss2: (0.0000) | Acc: (81.00%) (38848/47488)
Epoch: 38 | Batch_idx: 380 |  Loss: (0.5210) |  Loss2: (0.0000) | Acc: (81.00%) (39907/48768)
Epoch: 38 | Batch_idx: 390 |  Loss: (0.5216) |  Loss2: (0.0000) | Acc: (81.00%) (40898/50000)
# TEST : Loss: (0.5743) | Acc: (80.00%) (8068/10000)
percent tensor([0.5041, 0.4959], device='cuda:0')
percent tensor([0.4857, 0.5143], device='cuda:0')
percent tensor([0.4719, 0.5281], device='cuda:0')
percent tensor([0.5647, 0.4353], device='cuda:0')
percent tensor([0.5544, 0.4456], device='cuda:0')
percent tensor([0.6011, 0.3989], device='cuda:0')
percent tensor([0.6661, 0.3339], device='cuda:0')
percent tensor([0.9903, 0.0097], device='cuda:0')
Epoch: 39 | Batch_idx: 0 |  Loss: (0.4939) |  Loss2: (0.0000) | Acc: (82.00%) (106/128)
Epoch: 39 | Batch_idx: 10 |  Loss: (0.5039) |  Loss2: (0.0000) | Acc: (83.00%) (1180/1408)
Epoch: 39 | Batch_idx: 20 |  Loss: (0.4993) |  Loss2: (0.0000) | Acc: (83.00%) (2250/2688)
Epoch: 39 | Batch_idx: 30 |  Loss: (0.5022) |  Loss2: (0.0000) | Acc: (83.00%) (3302/3968)
Epoch: 39 | Batch_idx: 40 |  Loss: (0.5082) |  Loss2: (0.0000) | Acc: (82.00%) (4341/5248)
Epoch: 39 | Batch_idx: 50 |  Loss: (0.5056) |  Loss2: (0.0000) | Acc: (82.00%) (5399/6528)
Epoch: 39 | Batch_idx: 60 |  Loss: (0.5088) |  Loss2: (0.0000) | Acc: (82.00%) (6438/7808)
Epoch: 39 | Batch_idx: 70 |  Loss: (0.5138) |  Loss2: (0.0000) | Acc: (82.00%) (7478/9088)
Epoch: 39 | Batch_idx: 80 |  Loss: (0.5082) |  Loss2: (0.0000) | Acc: (82.00%) (8550/10368)
Epoch: 39 | Batch_idx: 90 |  Loss: (0.5076) |  Loss2: (0.0000) | Acc: (82.00%) (9598/11648)
Epoch: 39 | Batch_idx: 100 |  Loss: (0.5098) |  Loss2: (0.0000) | Acc: (82.00%) (10641/12928)
Epoch: 39 | Batch_idx: 110 |  Loss: (0.5067) |  Loss2: (0.0000) | Acc: (82.00%) (11691/14208)
Epoch: 39 | Batch_idx: 120 |  Loss: (0.5092) |  Loss2: (0.0000) | Acc: (82.00%) (12716/15488)
Epoch: 39 | Batch_idx: 130 |  Loss: (0.5068) |  Loss2: (0.0000) | Acc: (82.00%) (13788/16768)
Epoch: 39 | Batch_idx: 140 |  Loss: (0.5046) |  Loss2: (0.0000) | Acc: (82.00%) (14858/18048)
Epoch: 39 | Batch_idx: 150 |  Loss: (0.5077) |  Loss2: (0.0000) | Acc: (82.00%) (15887/19328)
Epoch: 39 | Batch_idx: 160 |  Loss: (0.5062) |  Loss2: (0.0000) | Acc: (82.00%) (16965/20608)
Epoch: 39 | Batch_idx: 170 |  Loss: (0.5060) |  Loss2: (0.0000) | Acc: (82.00%) (18022/21888)
Epoch: 39 | Batch_idx: 180 |  Loss: (0.5079) |  Loss2: (0.0000) | Acc: (82.00%) (19075/23168)
Epoch: 39 | Batch_idx: 190 |  Loss: (0.5067) |  Loss2: (0.0000) | Acc: (82.00%) (20147/24448)
Epoch: 39 | Batch_idx: 200 |  Loss: (0.5068) |  Loss2: (0.0000) | Acc: (82.00%) (21200/25728)
Epoch: 39 | Batch_idx: 210 |  Loss: (0.5081) |  Loss2: (0.0000) | Acc: (82.00%) (22233/27008)
Epoch: 39 | Batch_idx: 220 |  Loss: (0.5080) |  Loss2: (0.0000) | Acc: (82.00%) (23293/28288)
Epoch: 39 | Batch_idx: 230 |  Loss: (0.5079) |  Loss2: (0.0000) | Acc: (82.00%) (24350/29568)
Epoch: 39 | Batch_idx: 240 |  Loss: (0.5071) |  Loss2: (0.0000) | Acc: (82.00%) (25413/30848)
Epoch: 39 | Batch_idx: 250 |  Loss: (0.5068) |  Loss2: (0.0000) | Acc: (82.00%) (26488/32128)
Epoch: 39 | Batch_idx: 260 |  Loss: (0.5060) |  Loss2: (0.0000) | Acc: (82.00%) (27559/33408)
Epoch: 39 | Batch_idx: 270 |  Loss: (0.5083) |  Loss2: (0.0000) | Acc: (82.00%) (28574/34688)
Epoch: 39 | Batch_idx: 280 |  Loss: (0.5074) |  Loss2: (0.0000) | Acc: (82.00%) (29638/35968)
Epoch: 39 | Batch_idx: 290 |  Loss: (0.5079) |  Loss2: (0.0000) | Acc: (82.00%) (30689/37248)
Epoch: 39 | Batch_idx: 300 |  Loss: (0.5094) |  Loss2: (0.0000) | Acc: (82.00%) (31722/38528)
Epoch: 39 | Batch_idx: 310 |  Loss: (0.5090) |  Loss2: (0.0000) | Acc: (82.00%) (32794/39808)
Epoch: 39 | Batch_idx: 320 |  Loss: (0.5089) |  Loss2: (0.0000) | Acc: (82.00%) (33846/41088)
Epoch: 39 | Batch_idx: 330 |  Loss: (0.5106) |  Loss2: (0.0000) | Acc: (82.00%) (34866/42368)
Epoch: 39 | Batch_idx: 340 |  Loss: (0.5110) |  Loss2: (0.0000) | Acc: (82.00%) (35911/43648)
Epoch: 39 | Batch_idx: 350 |  Loss: (0.5117) |  Loss2: (0.0000) | Acc: (82.00%) (36949/44928)
Epoch: 39 | Batch_idx: 360 |  Loss: (0.5118) |  Loss2: (0.0000) | Acc: (82.00%) (37994/46208)
Epoch: 39 | Batch_idx: 370 |  Loss: (0.5120) |  Loss2: (0.0000) | Acc: (82.00%) (39048/47488)
Epoch: 39 | Batch_idx: 380 |  Loss: (0.5117) |  Loss2: (0.0000) | Acc: (82.00%) (40112/48768)
Epoch: 39 | Batch_idx: 390 |  Loss: (0.5118) |  Loss2: (0.0000) | Acc: (82.00%) (41120/50000)
# TEST : Loss: (0.5648) | Acc: (80.00%) (8081/10000)
percent tensor([0.5035, 0.4965], device='cuda:0')
percent tensor([0.4857, 0.5143], device='cuda:0')
percent tensor([0.4728, 0.5272], device='cuda:0')
percent tensor([0.5655, 0.4345], device='cuda:0')
percent tensor([0.5534, 0.4466], device='cuda:0')
percent tensor([0.6041, 0.3959], device='cuda:0')
percent tensor([0.6681, 0.3319], device='cuda:0')
percent tensor([0.9916, 0.0084], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 40 | Batch_idx: 0 |  Loss: (0.4788) |  Loss2: (0.0000) | Acc: (82.00%) (106/128)
Epoch: 40 | Batch_idx: 10 |  Loss: (0.4824) |  Loss2: (0.0000) | Acc: (83.00%) (1173/1408)
Epoch: 40 | Batch_idx: 20 |  Loss: (0.4821) |  Loss2: (0.0000) | Acc: (83.00%) (2244/2688)
Epoch: 40 | Batch_idx: 30 |  Loss: (0.4802) |  Loss2: (0.0000) | Acc: (83.00%) (3320/3968)
Epoch: 40 | Batch_idx: 40 |  Loss: (0.4839) |  Loss2: (0.0000) | Acc: (83.00%) (4366/5248)
Epoch: 40 | Batch_idx: 50 |  Loss: (0.4893) |  Loss2: (0.0000) | Acc: (82.00%) (5416/6528)
Epoch: 40 | Batch_idx: 60 |  Loss: (0.4844) |  Loss2: (0.0000) | Acc: (83.00%) (6487/7808)
Epoch: 40 | Batch_idx: 70 |  Loss: (0.4902) |  Loss2: (0.0000) | Acc: (83.00%) (7544/9088)
Epoch: 40 | Batch_idx: 80 |  Loss: (0.4932) |  Loss2: (0.0000) | Acc: (83.00%) (8608/10368)
Epoch: 40 | Batch_idx: 90 |  Loss: (0.4936) |  Loss2: (0.0000) | Acc: (83.00%) (9671/11648)
Epoch: 40 | Batch_idx: 100 |  Loss: (0.4924) |  Loss2: (0.0000) | Acc: (83.00%) (10735/12928)
Epoch: 40 | Batch_idx: 110 |  Loss: (0.4920) |  Loss2: (0.0000) | Acc: (83.00%) (11811/14208)
Epoch: 40 | Batch_idx: 120 |  Loss: (0.4895) |  Loss2: (0.0000) | Acc: (83.00%) (12879/15488)
Epoch: 40 | Batch_idx: 130 |  Loss: (0.4890) |  Loss2: (0.0000) | Acc: (83.00%) (13953/16768)
Epoch: 40 | Batch_idx: 140 |  Loss: (0.4883) |  Loss2: (0.0000) | Acc: (83.00%) (15013/18048)
Epoch: 40 | Batch_idx: 150 |  Loss: (0.4888) |  Loss2: (0.0000) | Acc: (83.00%) (16057/19328)
Epoch: 40 | Batch_idx: 160 |  Loss: (0.4867) |  Loss2: (0.0000) | Acc: (83.00%) (17143/20608)
Epoch: 40 | Batch_idx: 170 |  Loss: (0.4867) |  Loss2: (0.0000) | Acc: (83.00%) (18211/21888)
Epoch: 40 | Batch_idx: 180 |  Loss: (0.4887) |  Loss2: (0.0000) | Acc: (83.00%) (19267/23168)
Epoch: 40 | Batch_idx: 190 |  Loss: (0.4882) |  Loss2: (0.0000) | Acc: (83.00%) (20334/24448)
Epoch: 40 | Batch_idx: 200 |  Loss: (0.4890) |  Loss2: (0.0000) | Acc: (83.00%) (21386/25728)
Epoch: 40 | Batch_idx: 210 |  Loss: (0.4905) |  Loss2: (0.0000) | Acc: (83.00%) (22444/27008)
Epoch: 40 | Batch_idx: 220 |  Loss: (0.4915) |  Loss2: (0.0000) | Acc: (83.00%) (23488/28288)
Epoch: 40 | Batch_idx: 230 |  Loss: (0.4923) |  Loss2: (0.0000) | Acc: (83.00%) (24554/29568)
Epoch: 40 | Batch_idx: 240 |  Loss: (0.4928) |  Loss2: (0.0000) | Acc: (83.00%) (25618/30848)
Epoch: 40 | Batch_idx: 250 |  Loss: (0.4946) |  Loss2: (0.0000) | Acc: (82.00%) (26658/32128)
Epoch: 40 | Batch_idx: 260 |  Loss: (0.4945) |  Loss2: (0.0000) | Acc: (82.00%) (27721/33408)
Epoch: 40 | Batch_idx: 270 |  Loss: (0.4942) |  Loss2: (0.0000) | Acc: (82.00%) (28788/34688)
Epoch: 40 | Batch_idx: 280 |  Loss: (0.4937) |  Loss2: (0.0000) | Acc: (82.00%) (29844/35968)
Epoch: 40 | Batch_idx: 290 |  Loss: (0.4940) |  Loss2: (0.0000) | Acc: (82.00%) (30910/37248)
Epoch: 40 | Batch_idx: 300 |  Loss: (0.4957) |  Loss2: (0.0000) | Acc: (82.00%) (31958/38528)
Epoch: 40 | Batch_idx: 310 |  Loss: (0.4961) |  Loss2: (0.0000) | Acc: (82.00%) (33006/39808)
Epoch: 40 | Batch_idx: 320 |  Loss: (0.4952) |  Loss2: (0.0000) | Acc: (82.00%) (34070/41088)
Epoch: 40 | Batch_idx: 330 |  Loss: (0.4958) |  Loss2: (0.0000) | Acc: (82.00%) (35125/42368)
Epoch: 40 | Batch_idx: 340 |  Loss: (0.4959) |  Loss2: (0.0000) | Acc: (82.00%) (36177/43648)
Epoch: 40 | Batch_idx: 350 |  Loss: (0.4952) |  Loss2: (0.0000) | Acc: (82.00%) (37264/44928)
Epoch: 40 | Batch_idx: 360 |  Loss: (0.4958) |  Loss2: (0.0000) | Acc: (82.00%) (38309/46208)
Epoch: 40 | Batch_idx: 370 |  Loss: (0.4960) |  Loss2: (0.0000) | Acc: (82.00%) (39363/47488)
Epoch: 40 | Batch_idx: 380 |  Loss: (0.4959) |  Loss2: (0.0000) | Acc: (82.00%) (40428/48768)
Epoch: 40 | Batch_idx: 390 |  Loss: (0.4957) |  Loss2: (0.0000) | Acc: (82.00%) (41460/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_040.pth.tar'
# TEST : Loss: (0.6512) | Acc: (78.00%) (7842/10000)
percent tensor([0.5035, 0.4965], device='cuda:0')
percent tensor([0.4859, 0.5141], device='cuda:0')
percent tensor([0.4729, 0.5271], device='cuda:0')
percent tensor([0.5656, 0.4344], device='cuda:0')
percent tensor([0.5552, 0.4448], device='cuda:0')
percent tensor([0.6047, 0.3953], device='cuda:0')
percent tensor([0.6698, 0.3302], device='cuda:0')
percent tensor([0.9922, 0.0078], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(174.4092, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(799.4547, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(790.2896, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1521.2834, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(513.8759, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2189.9766, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4291.5176, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1432.3967, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6097.1411, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12091.4668, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4032.4648, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(17051.7754, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 41 | Batch_idx: 0 |  Loss: (0.6150) |  Loss2: (0.0000) | Acc: (77.00%) (99/128)
Epoch: 41 | Batch_idx: 10 |  Loss: (0.4987) |  Loss2: (0.0000) | Acc: (83.00%) (1177/1408)
Epoch: 41 | Batch_idx: 20 |  Loss: (0.4905) |  Loss2: (0.0000) | Acc: (83.00%) (2244/2688)
Epoch: 41 | Batch_idx: 30 |  Loss: (0.4867) |  Loss2: (0.0000) | Acc: (83.00%) (3320/3968)
Epoch: 41 | Batch_idx: 40 |  Loss: (0.4712) |  Loss2: (0.0000) | Acc: (84.00%) (4417/5248)
Epoch: 41 | Batch_idx: 50 |  Loss: (0.4700) |  Loss2: (0.0000) | Acc: (84.00%) (5486/6528)
Epoch: 41 | Batch_idx: 60 |  Loss: (0.4727) |  Loss2: (0.0000) | Acc: (83.00%) (6554/7808)
Epoch: 41 | Batch_idx: 70 |  Loss: (0.4709) |  Loss2: (0.0000) | Acc: (84.00%) (7641/9088)
Epoch: 41 | Batch_idx: 80 |  Loss: (0.4735) |  Loss2: (0.0000) | Acc: (83.00%) (8704/10368)
Epoch: 41 | Batch_idx: 90 |  Loss: (0.4740) |  Loss2: (0.0000) | Acc: (83.00%) (9765/11648)
Epoch: 41 | Batch_idx: 100 |  Loss: (0.4716) |  Loss2: (0.0000) | Acc: (83.00%) (10841/12928)
Epoch: 41 | Batch_idx: 110 |  Loss: (0.4724) |  Loss2: (0.0000) | Acc: (83.00%) (11907/14208)
Epoch: 41 | Batch_idx: 120 |  Loss: (0.4725) |  Loss2: (0.0000) | Acc: (83.00%) (12978/15488)
Epoch: 41 | Batch_idx: 130 |  Loss: (0.4727) |  Loss2: (0.0000) | Acc: (83.00%) (14048/16768)
Epoch: 41 | Batch_idx: 140 |  Loss: (0.4758) |  Loss2: (0.0000) | Acc: (83.00%) (15109/18048)
Epoch: 41 | Batch_idx: 150 |  Loss: (0.4755) |  Loss2: (0.0000) | Acc: (83.00%) (16183/19328)
Epoch: 41 | Batch_idx: 160 |  Loss: (0.4744) |  Loss2: (0.0000) | Acc: (83.00%) (17260/20608)
Epoch: 41 | Batch_idx: 170 |  Loss: (0.4757) |  Loss2: (0.0000) | Acc: (83.00%) (18316/21888)
Epoch: 41 | Batch_idx: 180 |  Loss: (0.4755) |  Loss2: (0.0000) | Acc: (83.00%) (19405/23168)
Epoch: 41 | Batch_idx: 190 |  Loss: (0.4746) |  Loss2: (0.0000) | Acc: (83.00%) (20466/24448)
Epoch: 41 | Batch_idx: 200 |  Loss: (0.4750) |  Loss2: (0.0000) | Acc: (83.00%) (21529/25728)
Epoch: 41 | Batch_idx: 210 |  Loss: (0.4745) |  Loss2: (0.0000) | Acc: (83.00%) (22594/27008)
Epoch: 41 | Batch_idx: 220 |  Loss: (0.4739) |  Loss2: (0.0000) | Acc: (83.00%) (23670/28288)
Epoch: 41 | Batch_idx: 230 |  Loss: (0.4732) |  Loss2: (0.0000) | Acc: (83.00%) (24756/29568)
Epoch: 41 | Batch_idx: 240 |  Loss: (0.4735) |  Loss2: (0.0000) | Acc: (83.00%) (25825/30848)
Epoch: 41 | Batch_idx: 250 |  Loss: (0.4737) |  Loss2: (0.0000) | Acc: (83.00%) (26898/32128)
Epoch: 41 | Batch_idx: 260 |  Loss: (0.4744) |  Loss2: (0.0000) | Acc: (83.00%) (27959/33408)
Epoch: 41 | Batch_idx: 270 |  Loss: (0.4740) |  Loss2: (0.0000) | Acc: (83.00%) (29023/34688)
Epoch: 41 | Batch_idx: 280 |  Loss: (0.4756) |  Loss2: (0.0000) | Acc: (83.00%) (30072/35968)
Epoch: 41 | Batch_idx: 290 |  Loss: (0.4760) |  Loss2: (0.0000) | Acc: (83.00%) (31148/37248)
Epoch: 41 | Batch_idx: 300 |  Loss: (0.4774) |  Loss2: (0.0000) | Acc: (83.00%) (32205/38528)
Epoch: 41 | Batch_idx: 310 |  Loss: (0.4772) |  Loss2: (0.0000) | Acc: (83.00%) (33280/39808)
Epoch: 41 | Batch_idx: 320 |  Loss: (0.4767) |  Loss2: (0.0000) | Acc: (83.00%) (34360/41088)
Epoch: 41 | Batch_idx: 330 |  Loss: (0.4763) |  Loss2: (0.0000) | Acc: (83.00%) (35434/42368)
Epoch: 41 | Batch_idx: 340 |  Loss: (0.4761) |  Loss2: (0.0000) | Acc: (83.00%) (36504/43648)
Epoch: 41 | Batch_idx: 350 |  Loss: (0.4767) |  Loss2: (0.0000) | Acc: (83.00%) (37563/44928)
Epoch: 41 | Batch_idx: 360 |  Loss: (0.4770) |  Loss2: (0.0000) | Acc: (83.00%) (38629/46208)
Epoch: 41 | Batch_idx: 370 |  Loss: (0.4762) |  Loss2: (0.0000) | Acc: (83.00%) (39713/47488)
Epoch: 41 | Batch_idx: 380 |  Loss: (0.4755) |  Loss2: (0.0000) | Acc: (83.00%) (40787/48768)
Epoch: 41 | Batch_idx: 390 |  Loss: (0.4743) |  Loss2: (0.0000) | Acc: (83.00%) (41838/50000)
# TEST : Loss: (0.6159) | Acc: (79.00%) (7978/10000)
percent tensor([0.5035, 0.4965], device='cuda:0')
percent tensor([0.4861, 0.5139], device='cuda:0')
percent tensor([0.4732, 0.5268], device='cuda:0')
percent tensor([0.5655, 0.4345], device='cuda:0')
percent tensor([0.5544, 0.4456], device='cuda:0')
percent tensor([0.6068, 0.3932], device='cuda:0')
percent tensor([0.6708, 0.3292], device='cuda:0')
percent tensor([0.9926, 0.0074], device='cuda:0')
Epoch: 42 | Batch_idx: 0 |  Loss: (0.4127) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 42 | Batch_idx: 10 |  Loss: (0.4082) |  Loss2: (0.0000) | Acc: (86.00%) (1214/1408)
Epoch: 42 | Batch_idx: 20 |  Loss: (0.4321) |  Loss2: (0.0000) | Acc: (85.00%) (2295/2688)
Epoch: 42 | Batch_idx: 30 |  Loss: (0.4465) |  Loss2: (0.0000) | Acc: (84.00%) (3360/3968)
Epoch: 42 | Batch_idx: 40 |  Loss: (0.4512) |  Loss2: (0.0000) | Acc: (84.00%) (4433/5248)
Epoch: 42 | Batch_idx: 50 |  Loss: (0.4488) |  Loss2: (0.0000) | Acc: (84.00%) (5525/6528)
Epoch: 42 | Batch_idx: 60 |  Loss: (0.4520) |  Loss2: (0.0000) | Acc: (84.00%) (6601/7808)
Epoch: 42 | Batch_idx: 70 |  Loss: (0.4559) |  Loss2: (0.0000) | Acc: (84.00%) (7683/9088)
Epoch: 42 | Batch_idx: 80 |  Loss: (0.4543) |  Loss2: (0.0000) | Acc: (84.00%) (8783/10368)
Epoch: 42 | Batch_idx: 90 |  Loss: (0.4574) |  Loss2: (0.0000) | Acc: (84.00%) (9838/11648)
Epoch: 42 | Batch_idx: 100 |  Loss: (0.4587) |  Loss2: (0.0000) | Acc: (84.00%) (10904/12928)
Epoch: 42 | Batch_idx: 110 |  Loss: (0.4578) |  Loss2: (0.0000) | Acc: (84.00%) (11984/14208)
Epoch: 42 | Batch_idx: 120 |  Loss: (0.4599) |  Loss2: (0.0000) | Acc: (84.00%) (13045/15488)
Epoch: 42 | Batch_idx: 130 |  Loss: (0.4581) |  Loss2: (0.0000) | Acc: (84.00%) (14128/16768)
Epoch: 42 | Batch_idx: 140 |  Loss: (0.4572) |  Loss2: (0.0000) | Acc: (84.00%) (15202/18048)
Epoch: 42 | Batch_idx: 150 |  Loss: (0.4560) |  Loss2: (0.0000) | Acc: (84.00%) (16285/19328)
Epoch: 42 | Batch_idx: 160 |  Loss: (0.4566) |  Loss2: (0.0000) | Acc: (84.00%) (17357/20608)
Epoch: 42 | Batch_idx: 170 |  Loss: (0.4582) |  Loss2: (0.0000) | Acc: (84.00%) (18412/21888)
Epoch: 42 | Batch_idx: 180 |  Loss: (0.4567) |  Loss2: (0.0000) | Acc: (84.00%) (19510/23168)
Epoch: 42 | Batch_idx: 190 |  Loss: (0.4570) |  Loss2: (0.0000) | Acc: (84.00%) (20593/24448)
Epoch: 42 | Batch_idx: 200 |  Loss: (0.4552) |  Loss2: (0.0000) | Acc: (84.00%) (21705/25728)
Epoch: 42 | Batch_idx: 210 |  Loss: (0.4552) |  Loss2: (0.0000) | Acc: (84.00%) (22774/27008)
Epoch: 42 | Batch_idx: 220 |  Loss: (0.4555) |  Loss2: (0.0000) | Acc: (84.00%) (23852/28288)
Epoch: 42 | Batch_idx: 230 |  Loss: (0.4564) |  Loss2: (0.0000) | Acc: (84.00%) (24920/29568)
Epoch: 42 | Batch_idx: 240 |  Loss: (0.4567) |  Loss2: (0.0000) | Acc: (84.00%) (25998/30848)
Epoch: 42 | Batch_idx: 250 |  Loss: (0.4551) |  Loss2: (0.0000) | Acc: (84.00%) (27088/32128)
Epoch: 42 | Batch_idx: 260 |  Loss: (0.4550) |  Loss2: (0.0000) | Acc: (84.00%) (28161/33408)
Epoch: 42 | Batch_idx: 270 |  Loss: (0.4559) |  Loss2: (0.0000) | Acc: (84.00%) (29230/34688)
Epoch: 42 | Batch_idx: 280 |  Loss: (0.4589) |  Loss2: (0.0000) | Acc: (84.00%) (30281/35968)
Epoch: 42 | Batch_idx: 290 |  Loss: (0.4576) |  Loss2: (0.0000) | Acc: (84.00%) (31372/37248)
Epoch: 42 | Batch_idx: 300 |  Loss: (0.4574) |  Loss2: (0.0000) | Acc: (84.00%) (32460/38528)
Epoch: 42 | Batch_idx: 310 |  Loss: (0.4564) |  Loss2: (0.0000) | Acc: (84.00%) (33551/39808)
Epoch: 42 | Batch_idx: 320 |  Loss: (0.4562) |  Loss2: (0.0000) | Acc: (84.00%) (34634/41088)
Epoch: 42 | Batch_idx: 330 |  Loss: (0.4556) |  Loss2: (0.0000) | Acc: (84.00%) (35712/42368)
Epoch: 42 | Batch_idx: 340 |  Loss: (0.4550) |  Loss2: (0.0000) | Acc: (84.00%) (36797/43648)
Epoch: 42 | Batch_idx: 350 |  Loss: (0.4559) |  Loss2: (0.0000) | Acc: (84.00%) (37853/44928)
Epoch: 42 | Batch_idx: 360 |  Loss: (0.4564) |  Loss2: (0.0000) | Acc: (84.00%) (38924/46208)
Epoch: 42 | Batch_idx: 370 |  Loss: (0.4561) |  Loss2: (0.0000) | Acc: (84.00%) (40018/47488)
Epoch: 42 | Batch_idx: 380 |  Loss: (0.4564) |  Loss2: (0.0000) | Acc: (84.00%) (41098/48768)
Epoch: 42 | Batch_idx: 390 |  Loss: (0.4562) |  Loss2: (0.0000) | Acc: (84.00%) (42155/50000)
# TEST : Loss: (0.6649) | Acc: (79.00%) (7917/10000)
percent tensor([0.5035, 0.4965], device='cuda:0')
percent tensor([0.4862, 0.5138], device='cuda:0')
percent tensor([0.4727, 0.5273], device='cuda:0')
percent tensor([0.5658, 0.4342], device='cuda:0')
percent tensor([0.5551, 0.4449], device='cuda:0')
percent tensor([0.6026, 0.3974], device='cuda:0')
percent tensor([0.6663, 0.3337], device='cuda:0')
percent tensor([0.9923, 0.0077], device='cuda:0')
Epoch: 43 | Batch_idx: 0 |  Loss: (0.4122) |  Loss2: (0.0000) | Acc: (85.00%) (110/128)
Epoch: 43 | Batch_idx: 10 |  Loss: (0.4713) |  Loss2: (0.0000) | Acc: (83.00%) (1176/1408)
Epoch: 43 | Batch_idx: 20 |  Loss: (0.4668) |  Loss2: (0.0000) | Acc: (83.00%) (2244/2688)
Epoch: 43 | Batch_idx: 30 |  Loss: (0.4668) |  Loss2: (0.0000) | Acc: (83.00%) (3319/3968)
Epoch: 43 | Batch_idx: 40 |  Loss: (0.4571) |  Loss2: (0.0000) | Acc: (83.00%) (4407/5248)
Epoch: 43 | Batch_idx: 50 |  Loss: (0.4452) |  Loss2: (0.0000) | Acc: (84.00%) (5507/6528)
Epoch: 43 | Batch_idx: 60 |  Loss: (0.4449) |  Loss2: (0.0000) | Acc: (84.00%) (6592/7808)
Epoch: 43 | Batch_idx: 70 |  Loss: (0.4416) |  Loss2: (0.0000) | Acc: (84.00%) (7690/9088)
Epoch: 43 | Batch_idx: 80 |  Loss: (0.4421) |  Loss2: (0.0000) | Acc: (84.00%) (8776/10368)
Epoch: 43 | Batch_idx: 90 |  Loss: (0.4426) |  Loss2: (0.0000) | Acc: (84.00%) (9854/11648)
Epoch: 43 | Batch_idx: 100 |  Loss: (0.4416) |  Loss2: (0.0000) | Acc: (84.00%) (10940/12928)
Epoch: 43 | Batch_idx: 110 |  Loss: (0.4401) |  Loss2: (0.0000) | Acc: (84.00%) (12042/14208)
Epoch: 43 | Batch_idx: 120 |  Loss: (0.4404) |  Loss2: (0.0000) | Acc: (84.00%) (13132/15488)
Epoch: 43 | Batch_idx: 130 |  Loss: (0.4401) |  Loss2: (0.0000) | Acc: (84.00%) (14218/16768)
Epoch: 43 | Batch_idx: 140 |  Loss: (0.4409) |  Loss2: (0.0000) | Acc: (84.00%) (15281/18048)
Epoch: 43 | Batch_idx: 150 |  Loss: (0.4404) |  Loss2: (0.0000) | Acc: (84.00%) (16358/19328)
Epoch: 43 | Batch_idx: 160 |  Loss: (0.4393) |  Loss2: (0.0000) | Acc: (84.00%) (17437/20608)
Epoch: 43 | Batch_idx: 170 |  Loss: (0.4400) |  Loss2: (0.0000) | Acc: (84.00%) (18514/21888)
Epoch: 43 | Batch_idx: 180 |  Loss: (0.4394) |  Loss2: (0.0000) | Acc: (84.00%) (19602/23168)
Epoch: 43 | Batch_idx: 190 |  Loss: (0.4383) |  Loss2: (0.0000) | Acc: (84.00%) (20715/24448)
Epoch: 43 | Batch_idx: 200 |  Loss: (0.4374) |  Loss2: (0.0000) | Acc: (84.00%) (21815/25728)
Epoch: 43 | Batch_idx: 210 |  Loss: (0.4366) |  Loss2: (0.0000) | Acc: (84.00%) (22898/27008)
Epoch: 43 | Batch_idx: 220 |  Loss: (0.4378) |  Loss2: (0.0000) | Acc: (84.00%) (23958/28288)
Epoch: 43 | Batch_idx: 230 |  Loss: (0.4371) |  Loss2: (0.0000) | Acc: (84.00%) (25063/29568)
Epoch: 43 | Batch_idx: 240 |  Loss: (0.4367) |  Loss2: (0.0000) | Acc: (84.00%) (26149/30848)
Epoch: 43 | Batch_idx: 250 |  Loss: (0.4368) |  Loss2: (0.0000) | Acc: (84.00%) (27240/32128)
Epoch: 43 | Batch_idx: 260 |  Loss: (0.4366) |  Loss2: (0.0000) | Acc: (84.00%) (28330/33408)
Epoch: 43 | Batch_idx: 270 |  Loss: (0.4359) |  Loss2: (0.0000) | Acc: (84.00%) (29418/34688)
Epoch: 43 | Batch_idx: 280 |  Loss: (0.4352) |  Loss2: (0.0000) | Acc: (84.00%) (30515/35968)
Epoch: 43 | Batch_idx: 290 |  Loss: (0.4353) |  Loss2: (0.0000) | Acc: (84.00%) (31608/37248)
Epoch: 43 | Batch_idx: 300 |  Loss: (0.4361) |  Loss2: (0.0000) | Acc: (84.00%) (32676/38528)
Epoch: 43 | Batch_idx: 310 |  Loss: (0.4363) |  Loss2: (0.0000) | Acc: (84.00%) (33768/39808)
Epoch: 43 | Batch_idx: 320 |  Loss: (0.4365) |  Loss2: (0.0000) | Acc: (84.00%) (34862/41088)
Epoch: 43 | Batch_idx: 330 |  Loss: (0.4369) |  Loss2: (0.0000) | Acc: (84.00%) (35932/42368)
Epoch: 43 | Batch_idx: 340 |  Loss: (0.4363) |  Loss2: (0.0000) | Acc: (84.00%) (37035/43648)
Epoch: 43 | Batch_idx: 350 |  Loss: (0.4359) |  Loss2: (0.0000) | Acc: (84.00%) (38134/44928)
Epoch: 43 | Batch_idx: 360 |  Loss: (0.4366) |  Loss2: (0.0000) | Acc: (84.00%) (39203/46208)
Epoch: 43 | Batch_idx: 370 |  Loss: (0.4359) |  Loss2: (0.0000) | Acc: (84.00%) (40309/47488)
Epoch: 43 | Batch_idx: 380 |  Loss: (0.4359) |  Loss2: (0.0000) | Acc: (84.00%) (41403/48768)
Epoch: 43 | Batch_idx: 390 |  Loss: (0.4366) |  Loss2: (0.0000) | Acc: (84.00%) (42435/50000)
# TEST : Loss: (0.5353) | Acc: (81.00%) (8195/10000)
percent tensor([0.5035, 0.4965], device='cuda:0')
percent tensor([0.4861, 0.5139], device='cuda:0')
percent tensor([0.4727, 0.5273], device='cuda:0')
percent tensor([0.5663, 0.4337], device='cuda:0')
percent tensor([0.5536, 0.4464], device='cuda:0')
percent tensor([0.6021, 0.3979], device='cuda:0')
percent tensor([0.6644, 0.3356], device='cuda:0')
percent tensor([0.9914, 0.0086], device='cuda:0')
Epoch: 44 | Batch_idx: 0 |  Loss: (0.3876) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 44 | Batch_idx: 10 |  Loss: (0.4077) |  Loss2: (0.0000) | Acc: (84.00%) (1195/1408)
Epoch: 44 | Batch_idx: 20 |  Loss: (0.4077) |  Loss2: (0.0000) | Acc: (84.00%) (2279/2688)
Epoch: 44 | Batch_idx: 30 |  Loss: (0.4115) |  Loss2: (0.0000) | Acc: (85.00%) (3390/3968)
Epoch: 44 | Batch_idx: 40 |  Loss: (0.4133) |  Loss2: (0.0000) | Acc: (85.00%) (4472/5248)
Epoch: 44 | Batch_idx: 50 |  Loss: (0.4154) |  Loss2: (0.0000) | Acc: (85.00%) (5549/6528)
Epoch: 44 | Batch_idx: 60 |  Loss: (0.4133) |  Loss2: (0.0000) | Acc: (85.00%) (6644/7808)
Epoch: 44 | Batch_idx: 70 |  Loss: (0.4165) |  Loss2: (0.0000) | Acc: (85.00%) (7730/9088)
Epoch: 44 | Batch_idx: 80 |  Loss: (0.4189) |  Loss2: (0.0000) | Acc: (85.00%) (8814/10368)
Epoch: 44 | Batch_idx: 90 |  Loss: (0.4179) |  Loss2: (0.0000) | Acc: (85.00%) (9916/11648)
Epoch: 44 | Batch_idx: 100 |  Loss: (0.4216) |  Loss2: (0.0000) | Acc: (85.00%) (10994/12928)
Epoch: 44 | Batch_idx: 110 |  Loss: (0.4202) |  Loss2: (0.0000) | Acc: (85.00%) (12106/14208)
Epoch: 44 | Batch_idx: 120 |  Loss: (0.4242) |  Loss2: (0.0000) | Acc: (85.00%) (13181/15488)
Epoch: 44 | Batch_idx: 130 |  Loss: (0.4243) |  Loss2: (0.0000) | Acc: (85.00%) (14284/16768)
Epoch: 44 | Batch_idx: 140 |  Loss: (0.4250) |  Loss2: (0.0000) | Acc: (85.00%) (15375/18048)
Epoch: 44 | Batch_idx: 150 |  Loss: (0.4231) |  Loss2: (0.0000) | Acc: (85.00%) (16473/19328)
Epoch: 44 | Batch_idx: 160 |  Loss: (0.4260) |  Loss2: (0.0000) | Acc: (85.00%) (17547/20608)
Epoch: 44 | Batch_idx: 170 |  Loss: (0.4264) |  Loss2: (0.0000) | Acc: (85.00%) (18630/21888)
Epoch: 44 | Batch_idx: 180 |  Loss: (0.4260) |  Loss2: (0.0000) | Acc: (85.00%) (19725/23168)
Epoch: 44 | Batch_idx: 190 |  Loss: (0.4253) |  Loss2: (0.0000) | Acc: (85.00%) (20830/24448)
Epoch: 44 | Batch_idx: 200 |  Loss: (0.4234) |  Loss2: (0.0000) | Acc: (85.00%) (21939/25728)
Epoch: 44 | Batch_idx: 210 |  Loss: (0.4232) |  Loss2: (0.0000) | Acc: (85.00%) (23027/27008)
Epoch: 44 | Batch_idx: 220 |  Loss: (0.4246) |  Loss2: (0.0000) | Acc: (85.00%) (24121/28288)
Epoch: 44 | Batch_idx: 230 |  Loss: (0.4251) |  Loss2: (0.0000) | Acc: (85.00%) (25207/29568)
Epoch: 44 | Batch_idx: 240 |  Loss: (0.4251) |  Loss2: (0.0000) | Acc: (85.00%) (26298/30848)
Epoch: 44 | Batch_idx: 250 |  Loss: (0.4255) |  Loss2: (0.0000) | Acc: (85.00%) (27384/32128)
Epoch: 44 | Batch_idx: 260 |  Loss: (0.4246) |  Loss2: (0.0000) | Acc: (85.00%) (28492/33408)
Epoch: 44 | Batch_idx: 270 |  Loss: (0.4249) |  Loss2: (0.0000) | Acc: (85.00%) (29580/34688)
Epoch: 44 | Batch_idx: 280 |  Loss: (0.4240) |  Loss2: (0.0000) | Acc: (85.00%) (30679/35968)
Epoch: 44 | Batch_idx: 290 |  Loss: (0.4250) |  Loss2: (0.0000) | Acc: (85.00%) (31763/37248)
Epoch: 44 | Batch_idx: 300 |  Loss: (0.4252) |  Loss2: (0.0000) | Acc: (85.00%) (32850/38528)
Epoch: 44 | Batch_idx: 310 |  Loss: (0.4249) |  Loss2: (0.0000) | Acc: (85.00%) (33951/39808)
Epoch: 44 | Batch_idx: 320 |  Loss: (0.4248) |  Loss2: (0.0000) | Acc: (85.00%) (35044/41088)
Epoch: 44 | Batch_idx: 330 |  Loss: (0.4250) |  Loss2: (0.0000) | Acc: (85.00%) (36135/42368)
Epoch: 44 | Batch_idx: 340 |  Loss: (0.4251) |  Loss2: (0.0000) | Acc: (85.00%) (37216/43648)
Epoch: 44 | Batch_idx: 350 |  Loss: (0.4241) |  Loss2: (0.0000) | Acc: (85.00%) (38334/44928)
Epoch: 44 | Batch_idx: 360 |  Loss: (0.4237) |  Loss2: (0.0000) | Acc: (85.00%) (39435/46208)
Epoch: 44 | Batch_idx: 370 |  Loss: (0.4239) |  Loss2: (0.0000) | Acc: (85.00%) (40535/47488)
Epoch: 44 | Batch_idx: 380 |  Loss: (0.4242) |  Loss2: (0.0000) | Acc: (85.00%) (41619/48768)
Epoch: 44 | Batch_idx: 390 |  Loss: (0.4240) |  Loss2: (0.0000) | Acc: (85.00%) (42676/50000)
# TEST : Loss: (0.5286) | Acc: (82.00%) (8239/10000)
percent tensor([0.5034, 0.4966], device='cuda:0')
percent tensor([0.4864, 0.5136], device='cuda:0')
percent tensor([0.4728, 0.5272], device='cuda:0')
percent tensor([0.5651, 0.4349], device='cuda:0')
percent tensor([0.5549, 0.4451], device='cuda:0')
percent tensor([0.6045, 0.3955], device='cuda:0')
percent tensor([0.6740, 0.3260], device='cuda:0')
percent tensor([0.9923, 0.0077], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 45 | Batch_idx: 0 |  Loss: (0.4352) |  Loss2: (0.0000) | Acc: (84.00%) (108/128)
Epoch: 45 | Batch_idx: 10 |  Loss: (0.4309) |  Loss2: (0.0000) | Acc: (84.00%) (1194/1408)
Epoch: 45 | Batch_idx: 20 |  Loss: (0.4648) |  Loss2: (0.0000) | Acc: (83.00%) (2234/2688)
Epoch: 45 | Batch_idx: 30 |  Loss: (0.4748) |  Loss2: (0.0000) | Acc: (82.00%) (3283/3968)
Epoch: 45 | Batch_idx: 40 |  Loss: (0.4912) |  Loss2: (0.0000) | Acc: (82.00%) (4333/5248)
Epoch: 45 | Batch_idx: 50 |  Loss: (0.4999) |  Loss2: (0.0000) | Acc: (82.00%) (5371/6528)
Epoch: 45 | Batch_idx: 60 |  Loss: (0.4982) |  Loss2: (0.0000) | Acc: (82.00%) (6418/7808)
Epoch: 45 | Batch_idx: 70 |  Loss: (0.5016) |  Loss2: (0.0000) | Acc: (82.00%) (7475/9088)
Epoch: 45 | Batch_idx: 80 |  Loss: (0.4997) |  Loss2: (0.0000) | Acc: (82.00%) (8538/10368)
Epoch: 45 | Batch_idx: 90 |  Loss: (0.5055) |  Loss2: (0.0000) | Acc: (82.00%) (9580/11648)
Epoch: 45 | Batch_idx: 100 |  Loss: (0.5113) |  Loss2: (0.0000) | Acc: (82.00%) (10607/12928)
Epoch: 45 | Batch_idx: 110 |  Loss: (0.5085) |  Loss2: (0.0000) | Acc: (82.00%) (11663/14208)
Epoch: 45 | Batch_idx: 120 |  Loss: (0.5087) |  Loss2: (0.0000) | Acc: (82.00%) (12729/15488)
Epoch: 45 | Batch_idx: 130 |  Loss: (0.5074) |  Loss2: (0.0000) | Acc: (82.00%) (13783/16768)
Epoch: 45 | Batch_idx: 140 |  Loss: (0.5073) |  Loss2: (0.0000) | Acc: (82.00%) (14839/18048)
Epoch: 45 | Batch_idx: 150 |  Loss: (0.5041) |  Loss2: (0.0000) | Acc: (82.00%) (15903/19328)
Epoch: 45 | Batch_idx: 160 |  Loss: (0.5025) |  Loss2: (0.0000) | Acc: (82.00%) (16960/20608)
Epoch: 45 | Batch_idx: 170 |  Loss: (0.5030) |  Loss2: (0.0000) | Acc: (82.00%) (18023/21888)
Epoch: 45 | Batch_idx: 180 |  Loss: (0.5015) |  Loss2: (0.0000) | Acc: (82.00%) (19081/23168)
Epoch: 45 | Batch_idx: 190 |  Loss: (0.5002) |  Loss2: (0.0000) | Acc: (82.00%) (20146/24448)
Epoch: 45 | Batch_idx: 200 |  Loss: (0.4980) |  Loss2: (0.0000) | Acc: (82.00%) (21221/25728)
Epoch: 45 | Batch_idx: 210 |  Loss: (0.4948) |  Loss2: (0.0000) | Acc: (82.00%) (22307/27008)
Epoch: 45 | Batch_idx: 220 |  Loss: (0.4936) |  Loss2: (0.0000) | Acc: (82.00%) (23388/28288)
Epoch: 45 | Batch_idx: 230 |  Loss: (0.4914) |  Loss2: (0.0000) | Acc: (82.00%) (24458/29568)
Epoch: 45 | Batch_idx: 240 |  Loss: (0.4913) |  Loss2: (0.0000) | Acc: (82.00%) (25529/30848)
Epoch: 45 | Batch_idx: 250 |  Loss: (0.4914) |  Loss2: (0.0000) | Acc: (82.00%) (26600/32128)
Epoch: 45 | Batch_idx: 260 |  Loss: (0.4907) |  Loss2: (0.0000) | Acc: (82.00%) (27662/33408)
Epoch: 45 | Batch_idx: 270 |  Loss: (0.4902) |  Loss2: (0.0000) | Acc: (82.00%) (28725/34688)
Epoch: 45 | Batch_idx: 280 |  Loss: (0.4891) |  Loss2: (0.0000) | Acc: (82.00%) (29802/35968)
Epoch: 45 | Batch_idx: 290 |  Loss: (0.4893) |  Loss2: (0.0000) | Acc: (82.00%) (30849/37248)
Epoch: 45 | Batch_idx: 300 |  Loss: (0.4885) |  Loss2: (0.0000) | Acc: (82.00%) (31926/38528)
Epoch: 45 | Batch_idx: 310 |  Loss: (0.4888) |  Loss2: (0.0000) | Acc: (82.00%) (32988/39808)
Epoch: 45 | Batch_idx: 320 |  Loss: (0.4880) |  Loss2: (0.0000) | Acc: (82.00%) (34059/41088)
Epoch: 45 | Batch_idx: 330 |  Loss: (0.4878) |  Loss2: (0.0000) | Acc: (82.00%) (35120/42368)
Epoch: 45 | Batch_idx: 340 |  Loss: (0.4864) |  Loss2: (0.0000) | Acc: (82.00%) (36203/43648)
Epoch: 45 | Batch_idx: 350 |  Loss: (0.4858) |  Loss2: (0.0000) | Acc: (82.00%) (37277/44928)
Epoch: 45 | Batch_idx: 360 |  Loss: (0.4849) |  Loss2: (0.0000) | Acc: (83.00%) (38356/46208)
Epoch: 45 | Batch_idx: 370 |  Loss: (0.4843) |  Loss2: (0.0000) | Acc: (83.00%) (39435/47488)
Epoch: 45 | Batch_idx: 380 |  Loss: (0.4839) |  Loss2: (0.0000) | Acc: (83.00%) (40502/48768)
Epoch: 45 | Batch_idx: 390 |  Loss: (0.4826) |  Loss2: (0.0000) | Acc: (83.00%) (41547/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_045.pth.tar'
# TEST : Loss: (0.5539) | Acc: (81.00%) (8156/10000)
percent tensor([0.5063, 0.4937], device='cuda:0')
percent tensor([0.4925, 0.5075], device='cuda:0')
percent tensor([0.4732, 0.5268], device='cuda:0')
percent tensor([0.5708, 0.4292], device='cuda:0')
percent tensor([0.5554, 0.4446], device='cuda:0')
percent tensor([0.6060, 0.3940], device='cuda:0')
percent tensor([0.6822, 0.3178], device='cuda:0')
percent tensor([0.9919, 0.0081], device='cuda:0')
Epoch: 46 | Batch_idx: 0 |  Loss: (0.4350) |  Loss2: (0.0000) | Acc: (84.00%) (108/128)
Epoch: 46 | Batch_idx: 10 |  Loss: (0.4070) |  Loss2: (0.0000) | Acc: (85.00%) (1205/1408)
Epoch: 46 | Batch_idx: 20 |  Loss: (0.4301) |  Loss2: (0.0000) | Acc: (84.00%) (2279/2688)
Epoch: 46 | Batch_idx: 30 |  Loss: (0.4423) |  Loss2: (0.0000) | Acc: (84.00%) (3349/3968)
Epoch: 46 | Batch_idx: 40 |  Loss: (0.4498) |  Loss2: (0.0000) | Acc: (84.00%) (4410/5248)
Epoch: 46 | Batch_idx: 50 |  Loss: (0.4550) |  Loss2: (0.0000) | Acc: (83.00%) (5468/6528)
Epoch: 46 | Batch_idx: 60 |  Loss: (0.4569) |  Loss2: (0.0000) | Acc: (83.00%) (6544/7808)
Epoch: 46 | Batch_idx: 70 |  Loss: (0.4591) |  Loss2: (0.0000) | Acc: (83.00%) (7611/9088)
Epoch: 46 | Batch_idx: 80 |  Loss: (0.4629) |  Loss2: (0.0000) | Acc: (83.00%) (8671/10368)
Epoch: 46 | Batch_idx: 90 |  Loss: (0.4605) |  Loss2: (0.0000) | Acc: (83.00%) (9763/11648)
Epoch: 46 | Batch_idx: 100 |  Loss: (0.4577) |  Loss2: (0.0000) | Acc: (83.00%) (10855/12928)
Epoch: 46 | Batch_idx: 110 |  Loss: (0.4594) |  Loss2: (0.0000) | Acc: (83.00%) (11923/14208)
Epoch: 46 | Batch_idx: 120 |  Loss: (0.4582) |  Loss2: (0.0000) | Acc: (83.00%) (13000/15488)
Epoch: 46 | Batch_idx: 130 |  Loss: (0.4574) |  Loss2: (0.0000) | Acc: (83.00%) (14076/16768)
Epoch: 46 | Batch_idx: 140 |  Loss: (0.4582) |  Loss2: (0.0000) | Acc: (83.00%) (15158/18048)
Epoch: 46 | Batch_idx: 150 |  Loss: (0.4590) |  Loss2: (0.0000) | Acc: (83.00%) (16219/19328)
Epoch: 46 | Batch_idx: 160 |  Loss: (0.4590) |  Loss2: (0.0000) | Acc: (83.00%) (17286/20608)
Epoch: 46 | Batch_idx: 170 |  Loss: (0.4569) |  Loss2: (0.0000) | Acc: (83.00%) (18377/21888)
Epoch: 46 | Batch_idx: 180 |  Loss: (0.4560) |  Loss2: (0.0000) | Acc: (83.00%) (19454/23168)
Epoch: 46 | Batch_idx: 190 |  Loss: (0.4588) |  Loss2: (0.0000) | Acc: (83.00%) (20509/24448)
Epoch: 46 | Batch_idx: 200 |  Loss: (0.4576) |  Loss2: (0.0000) | Acc: (83.00%) (21593/25728)
Epoch: 46 | Batch_idx: 210 |  Loss: (0.4554) |  Loss2: (0.0000) | Acc: (84.00%) (22694/27008)
Epoch: 46 | Batch_idx: 220 |  Loss: (0.4546) |  Loss2: (0.0000) | Acc: (84.00%) (23776/28288)
Epoch: 46 | Batch_idx: 230 |  Loss: (0.4532) |  Loss2: (0.0000) | Acc: (84.00%) (24869/29568)
Epoch: 46 | Batch_idx: 240 |  Loss: (0.4523) |  Loss2: (0.0000) | Acc: (84.00%) (25964/30848)
Epoch: 46 | Batch_idx: 250 |  Loss: (0.4531) |  Loss2: (0.0000) | Acc: (84.00%) (27038/32128)
Epoch: 46 | Batch_idx: 260 |  Loss: (0.4524) |  Loss2: (0.0000) | Acc: (84.00%) (28124/33408)
Epoch: 46 | Batch_idx: 270 |  Loss: (0.4525) |  Loss2: (0.0000) | Acc: (84.00%) (29197/34688)
Epoch: 46 | Batch_idx: 280 |  Loss: (0.4519) |  Loss2: (0.0000) | Acc: (84.00%) (30279/35968)
Epoch: 46 | Batch_idx: 290 |  Loss: (0.4505) |  Loss2: (0.0000) | Acc: (84.00%) (31378/37248)
Epoch: 46 | Batch_idx: 300 |  Loss: (0.4520) |  Loss2: (0.0000) | Acc: (84.00%) (32434/38528)
Epoch: 46 | Batch_idx: 310 |  Loss: (0.4516) |  Loss2: (0.0000) | Acc: (84.00%) (33518/39808)
Epoch: 46 | Batch_idx: 320 |  Loss: (0.4504) |  Loss2: (0.0000) | Acc: (84.00%) (34602/41088)
Epoch: 46 | Batch_idx: 330 |  Loss: (0.4502) |  Loss2: (0.0000) | Acc: (84.00%) (35688/42368)
Epoch: 46 | Batch_idx: 340 |  Loss: (0.4501) |  Loss2: (0.0000) | Acc: (84.00%) (36772/43648)
Epoch: 46 | Batch_idx: 350 |  Loss: (0.4510) |  Loss2: (0.0000) | Acc: (84.00%) (37849/44928)
Epoch: 46 | Batch_idx: 360 |  Loss: (0.4501) |  Loss2: (0.0000) | Acc: (84.00%) (38942/46208)
Epoch: 46 | Batch_idx: 370 |  Loss: (0.4493) |  Loss2: (0.0000) | Acc: (84.00%) (40021/47488)
Epoch: 46 | Batch_idx: 380 |  Loss: (0.4498) |  Loss2: (0.0000) | Acc: (84.00%) (41099/48768)
Epoch: 46 | Batch_idx: 390 |  Loss: (0.4492) |  Loss2: (0.0000) | Acc: (84.00%) (42159/50000)
# TEST : Loss: (0.5257) | Acc: (82.00%) (8240/10000)
percent tensor([0.5068, 0.4932], device='cuda:0')
percent tensor([0.4941, 0.5059], device='cuda:0')
percent tensor([0.4749, 0.5251], device='cuda:0')
percent tensor([0.5727, 0.4273], device='cuda:0')
percent tensor([0.5588, 0.4412], device='cuda:0')
percent tensor([0.6042, 0.3958], device='cuda:0')
percent tensor([0.6952, 0.3048], device='cuda:0')
percent tensor([0.9925, 0.0075], device='cuda:0')
Epoch: 47 | Batch_idx: 0 |  Loss: (0.3478) |  Loss2: (0.0000) | Acc: (86.00%) (111/128)
Epoch: 47 | Batch_idx: 10 |  Loss: (0.4160) |  Loss2: (0.0000) | Acc: (86.00%) (1211/1408)
Epoch: 47 | Batch_idx: 20 |  Loss: (0.4148) |  Loss2: (0.0000) | Acc: (85.00%) (2299/2688)
Epoch: 47 | Batch_idx: 30 |  Loss: (0.4309) |  Loss2: (0.0000) | Acc: (85.00%) (3377/3968)
Epoch: 47 | Batch_idx: 40 |  Loss: (0.4370) |  Loss2: (0.0000) | Acc: (84.00%) (4457/5248)
Epoch: 47 | Batch_idx: 50 |  Loss: (0.4257) |  Loss2: (0.0000) | Acc: (85.00%) (5568/6528)
Epoch: 47 | Batch_idx: 60 |  Loss: (0.4233) |  Loss2: (0.0000) | Acc: (85.00%) (6658/7808)
Epoch: 47 | Batch_idx: 70 |  Loss: (0.4261) |  Loss2: (0.0000) | Acc: (85.00%) (7741/9088)
Epoch: 47 | Batch_idx: 80 |  Loss: (0.4289) |  Loss2: (0.0000) | Acc: (85.00%) (8818/10368)
Epoch: 47 | Batch_idx: 90 |  Loss: (0.4278) |  Loss2: (0.0000) | Acc: (85.00%) (9921/11648)
Epoch: 47 | Batch_idx: 100 |  Loss: (0.4277) |  Loss2: (0.0000) | Acc: (85.00%) (11008/12928)
Epoch: 47 | Batch_idx: 110 |  Loss: (0.4250) |  Loss2: (0.0000) | Acc: (85.00%) (12103/14208)
Epoch: 47 | Batch_idx: 120 |  Loss: (0.4266) |  Loss2: (0.0000) | Acc: (85.00%) (13187/15488)
Epoch: 47 | Batch_idx: 130 |  Loss: (0.4260) |  Loss2: (0.0000) | Acc: (85.00%) (14275/16768)
Epoch: 47 | Batch_idx: 140 |  Loss: (0.4259) |  Loss2: (0.0000) | Acc: (85.00%) (15356/18048)
Epoch: 47 | Batch_idx: 150 |  Loss: (0.4248) |  Loss2: (0.0000) | Acc: (85.00%) (16445/19328)
Epoch: 47 | Batch_idx: 160 |  Loss: (0.4238) |  Loss2: (0.0000) | Acc: (85.00%) (17553/20608)
Epoch: 47 | Batch_idx: 170 |  Loss: (0.4248) |  Loss2: (0.0000) | Acc: (85.00%) (18643/21888)
Epoch: 47 | Batch_idx: 180 |  Loss: (0.4240) |  Loss2: (0.0000) | Acc: (85.00%) (19748/23168)
Epoch: 47 | Batch_idx: 190 |  Loss: (0.4234) |  Loss2: (0.0000) | Acc: (85.00%) (20845/24448)
Epoch: 47 | Batch_idx: 200 |  Loss: (0.4234) |  Loss2: (0.0000) | Acc: (85.00%) (21931/25728)
Epoch: 47 | Batch_idx: 210 |  Loss: (0.4228) |  Loss2: (0.0000) | Acc: (85.00%) (23016/27008)
Epoch: 47 | Batch_idx: 220 |  Loss: (0.4227) |  Loss2: (0.0000) | Acc: (85.00%) (24111/28288)
Epoch: 47 | Batch_idx: 230 |  Loss: (0.4244) |  Loss2: (0.0000) | Acc: (85.00%) (25200/29568)
Epoch: 47 | Batch_idx: 240 |  Loss: (0.4266) |  Loss2: (0.0000) | Acc: (85.00%) (26264/30848)
Epoch: 47 | Batch_idx: 250 |  Loss: (0.4259) |  Loss2: (0.0000) | Acc: (85.00%) (27357/32128)
Epoch: 47 | Batch_idx: 260 |  Loss: (0.4253) |  Loss2: (0.0000) | Acc: (85.00%) (28462/33408)
Epoch: 47 | Batch_idx: 270 |  Loss: (0.4267) |  Loss2: (0.0000) | Acc: (85.00%) (29545/34688)
Epoch: 47 | Batch_idx: 280 |  Loss: (0.4280) |  Loss2: (0.0000) | Acc: (85.00%) (30606/35968)
Epoch: 47 | Batch_idx: 290 |  Loss: (0.4282) |  Loss2: (0.0000) | Acc: (85.00%) (31688/37248)
Epoch: 47 | Batch_idx: 300 |  Loss: (0.4289) |  Loss2: (0.0000) | Acc: (85.00%) (32776/38528)
Epoch: 47 | Batch_idx: 310 |  Loss: (0.4287) |  Loss2: (0.0000) | Acc: (85.00%) (33874/39808)
Epoch: 47 | Batch_idx: 320 |  Loss: (0.4292) |  Loss2: (0.0000) | Acc: (85.00%) (34961/41088)
Epoch: 47 | Batch_idx: 330 |  Loss: (0.4292) |  Loss2: (0.0000) | Acc: (85.00%) (36057/42368)
Epoch: 47 | Batch_idx: 340 |  Loss: (0.4300) |  Loss2: (0.0000) | Acc: (85.00%) (37136/43648)
Epoch: 47 | Batch_idx: 350 |  Loss: (0.4314) |  Loss2: (0.0000) | Acc: (85.00%) (38200/44928)
Epoch: 47 | Batch_idx: 360 |  Loss: (0.4315) |  Loss2: (0.0000) | Acc: (85.00%) (39278/46208)
Epoch: 47 | Batch_idx: 370 |  Loss: (0.4310) |  Loss2: (0.0000) | Acc: (85.00%) (40377/47488)
Epoch: 47 | Batch_idx: 380 |  Loss: (0.4305) |  Loss2: (0.0000) | Acc: (85.00%) (41489/48768)
Epoch: 47 | Batch_idx: 390 |  Loss: (0.4308) |  Loss2: (0.0000) | Acc: (85.00%) (42524/50000)
# TEST : Loss: (0.5137) | Acc: (82.00%) (8270/10000)
percent tensor([0.5067, 0.4933], device='cuda:0')
percent tensor([0.4928, 0.5072], device='cuda:0')
percent tensor([0.4759, 0.5241], device='cuda:0')
percent tensor([0.5754, 0.4246], device='cuda:0')
percent tensor([0.5635, 0.4365], device='cuda:0')
percent tensor([0.6088, 0.3912], device='cuda:0')
percent tensor([0.6971, 0.3029], device='cuda:0')
percent tensor([0.9934, 0.0066], device='cuda:0')
Epoch: 48 | Batch_idx: 0 |  Loss: (0.4527) |  Loss2: (0.0000) | Acc: (85.00%) (110/128)
Epoch: 48 | Batch_idx: 10 |  Loss: (0.4240) |  Loss2: (0.0000) | Acc: (85.00%) (1198/1408)
Epoch: 48 | Batch_idx: 20 |  Loss: (0.4247) |  Loss2: (0.0000) | Acc: (85.00%) (2288/2688)
Epoch: 48 | Batch_idx: 30 |  Loss: (0.4344) |  Loss2: (0.0000) | Acc: (84.00%) (3362/3968)
Epoch: 48 | Batch_idx: 40 |  Loss: (0.4239) |  Loss2: (0.0000) | Acc: (85.00%) (4469/5248)
Epoch: 48 | Batch_idx: 50 |  Loss: (0.4220) |  Loss2: (0.0000) | Acc: (85.00%) (5561/6528)
Epoch: 48 | Batch_idx: 60 |  Loss: (0.4184) |  Loss2: (0.0000) | Acc: (85.00%) (6663/7808)
Epoch: 48 | Batch_idx: 70 |  Loss: (0.4165) |  Loss2: (0.0000) | Acc: (85.00%) (7763/9088)
Epoch: 48 | Batch_idx: 80 |  Loss: (0.4192) |  Loss2: (0.0000) | Acc: (85.00%) (8852/10368)
Epoch: 48 | Batch_idx: 90 |  Loss: (0.4211) |  Loss2: (0.0000) | Acc: (85.00%) (9939/11648)
Epoch: 48 | Batch_idx: 100 |  Loss: (0.4231) |  Loss2: (0.0000) | Acc: (85.00%) (11030/12928)
Epoch: 48 | Batch_idx: 110 |  Loss: (0.4247) |  Loss2: (0.0000) | Acc: (85.00%) (12116/14208)
Epoch: 48 | Batch_idx: 120 |  Loss: (0.4229) |  Loss2: (0.0000) | Acc: (85.00%) (13222/15488)
Epoch: 48 | Batch_idx: 130 |  Loss: (0.4236) |  Loss2: (0.0000) | Acc: (85.00%) (14304/16768)
Epoch: 48 | Batch_idx: 140 |  Loss: (0.4228) |  Loss2: (0.0000) | Acc: (85.00%) (15402/18048)
Epoch: 48 | Batch_idx: 150 |  Loss: (0.4218) |  Loss2: (0.0000) | Acc: (85.00%) (16503/19328)
Epoch: 48 | Batch_idx: 160 |  Loss: (0.4223) |  Loss2: (0.0000) | Acc: (85.00%) (17598/20608)
Epoch: 48 | Batch_idx: 170 |  Loss: (0.4250) |  Loss2: (0.0000) | Acc: (85.00%) (18672/21888)
Epoch: 48 | Batch_idx: 180 |  Loss: (0.4272) |  Loss2: (0.0000) | Acc: (85.00%) (19747/23168)
Epoch: 48 | Batch_idx: 190 |  Loss: (0.4256) |  Loss2: (0.0000) | Acc: (85.00%) (20854/24448)
Epoch: 48 | Batch_idx: 200 |  Loss: (0.4261) |  Loss2: (0.0000) | Acc: (85.00%) (21950/25728)
Epoch: 48 | Batch_idx: 210 |  Loss: (0.4285) |  Loss2: (0.0000) | Acc: (85.00%) (23011/27008)
Epoch: 48 | Batch_idx: 220 |  Loss: (0.4285) |  Loss2: (0.0000) | Acc: (85.00%) (24105/28288)
Epoch: 48 | Batch_idx: 230 |  Loss: (0.4290) |  Loss2: (0.0000) | Acc: (85.00%) (25199/29568)
Epoch: 48 | Batch_idx: 240 |  Loss: (0.4288) |  Loss2: (0.0000) | Acc: (85.00%) (26294/30848)
Epoch: 48 | Batch_idx: 250 |  Loss: (0.4266) |  Loss2: (0.0000) | Acc: (85.00%) (27415/32128)
Epoch: 48 | Batch_idx: 260 |  Loss: (0.4251) |  Loss2: (0.0000) | Acc: (85.00%) (28535/33408)
Epoch: 48 | Batch_idx: 270 |  Loss: (0.4246) |  Loss2: (0.0000) | Acc: (85.00%) (29639/34688)
Epoch: 48 | Batch_idx: 280 |  Loss: (0.4253) |  Loss2: (0.0000) | Acc: (85.00%) (30726/35968)
Epoch: 48 | Batch_idx: 290 |  Loss: (0.4254) |  Loss2: (0.0000) | Acc: (85.00%) (31815/37248)
Epoch: 48 | Batch_idx: 300 |  Loss: (0.4253) |  Loss2: (0.0000) | Acc: (85.00%) (32910/38528)
Epoch: 48 | Batch_idx: 310 |  Loss: (0.4250) |  Loss2: (0.0000) | Acc: (85.00%) (34000/39808)
Epoch: 48 | Batch_idx: 320 |  Loss: (0.4236) |  Loss2: (0.0000) | Acc: (85.00%) (35107/41088)
Epoch: 48 | Batch_idx: 330 |  Loss: (0.4228) |  Loss2: (0.0000) | Acc: (85.00%) (36205/42368)
Epoch: 48 | Batch_idx: 340 |  Loss: (0.4221) |  Loss2: (0.0000) | Acc: (85.00%) (37317/43648)
Epoch: 48 | Batch_idx: 350 |  Loss: (0.4221) |  Loss2: (0.0000) | Acc: (85.00%) (38402/44928)
Epoch: 48 | Batch_idx: 360 |  Loss: (0.4217) |  Loss2: (0.0000) | Acc: (85.00%) (39511/46208)
Epoch: 48 | Batch_idx: 370 |  Loss: (0.4217) |  Loss2: (0.0000) | Acc: (85.00%) (40602/47488)
Epoch: 48 | Batch_idx: 380 |  Loss: (0.4212) |  Loss2: (0.0000) | Acc: (85.00%) (41700/48768)
Epoch: 48 | Batch_idx: 390 |  Loss: (0.4214) |  Loss2: (0.0000) | Acc: (85.00%) (42740/50000)
# TEST : Loss: (0.5045) | Acc: (83.00%) (8302/10000)
percent tensor([0.5076, 0.4924], device='cuda:0')
percent tensor([0.4920, 0.5080], device='cuda:0')
percent tensor([0.4787, 0.5213], device='cuda:0')
percent tensor([0.5789, 0.4211], device='cuda:0')
percent tensor([0.5642, 0.4358], device='cuda:0')
percent tensor([0.6077, 0.3923], device='cuda:0')
percent tensor([0.7002, 0.2998], device='cuda:0')
percent tensor([0.9943, 0.0057], device='cuda:0')
Epoch: 49 | Batch_idx: 0 |  Loss: (0.3690) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 49 | Batch_idx: 10 |  Loss: (0.4056) |  Loss2: (0.0000) | Acc: (86.00%) (1220/1408)
Epoch: 49 | Batch_idx: 20 |  Loss: (0.4175) |  Loss2: (0.0000) | Acc: (85.00%) (2308/2688)
Epoch: 49 | Batch_idx: 30 |  Loss: (0.4319) |  Loss2: (0.0000) | Acc: (85.00%) (3386/3968)
Epoch: 49 | Batch_idx: 40 |  Loss: (0.4212) |  Loss2: (0.0000) | Acc: (85.00%) (4499/5248)
Epoch: 49 | Batch_idx: 50 |  Loss: (0.4172) |  Loss2: (0.0000) | Acc: (85.00%) (5600/6528)
Epoch: 49 | Batch_idx: 60 |  Loss: (0.4125) |  Loss2: (0.0000) | Acc: (85.00%) (6710/7808)
Epoch: 49 | Batch_idx: 70 |  Loss: (0.4154) |  Loss2: (0.0000) | Acc: (85.00%) (7796/9088)
Epoch: 49 | Batch_idx: 80 |  Loss: (0.4141) |  Loss2: (0.0000) | Acc: (85.00%) (8901/10368)
Epoch: 49 | Batch_idx: 90 |  Loss: (0.4137) |  Loss2: (0.0000) | Acc: (85.00%) (10012/11648)
Epoch: 49 | Batch_idx: 100 |  Loss: (0.4120) |  Loss2: (0.0000) | Acc: (86.00%) (11120/12928)
Epoch: 49 | Batch_idx: 110 |  Loss: (0.4137) |  Loss2: (0.0000) | Acc: (85.00%) (12211/14208)
Epoch: 49 | Batch_idx: 120 |  Loss: (0.4148) |  Loss2: (0.0000) | Acc: (85.00%) (13300/15488)
Epoch: 49 | Batch_idx: 130 |  Loss: (0.4145) |  Loss2: (0.0000) | Acc: (85.00%) (14395/16768)
Epoch: 49 | Batch_idx: 140 |  Loss: (0.4158) |  Loss2: (0.0000) | Acc: (85.00%) (15491/18048)
Epoch: 49 | Batch_idx: 150 |  Loss: (0.4176) |  Loss2: (0.0000) | Acc: (85.00%) (16576/19328)
Epoch: 49 | Batch_idx: 160 |  Loss: (0.4176) |  Loss2: (0.0000) | Acc: (85.00%) (17679/20608)
Epoch: 49 | Batch_idx: 170 |  Loss: (0.4163) |  Loss2: (0.0000) | Acc: (85.00%) (18782/21888)
Epoch: 49 | Batch_idx: 180 |  Loss: (0.4152) |  Loss2: (0.0000) | Acc: (85.00%) (19888/23168)
Epoch: 49 | Batch_idx: 190 |  Loss: (0.4142) |  Loss2: (0.0000) | Acc: (85.00%) (20993/24448)
Epoch: 49 | Batch_idx: 200 |  Loss: (0.4148) |  Loss2: (0.0000) | Acc: (85.00%) (22081/25728)
Epoch: 49 | Batch_idx: 210 |  Loss: (0.4140) |  Loss2: (0.0000) | Acc: (85.00%) (23186/27008)
Epoch: 49 | Batch_idx: 220 |  Loss: (0.4137) |  Loss2: (0.0000) | Acc: (85.00%) (24291/28288)
Epoch: 49 | Batch_idx: 230 |  Loss: (0.4140) |  Loss2: (0.0000) | Acc: (85.00%) (25390/29568)
Epoch: 49 | Batch_idx: 240 |  Loss: (0.4141) |  Loss2: (0.0000) | Acc: (85.00%) (26493/30848)
Epoch: 49 | Batch_idx: 250 |  Loss: (0.4150) |  Loss2: (0.0000) | Acc: (85.00%) (27579/32128)
Epoch: 49 | Batch_idx: 260 |  Loss: (0.4145) |  Loss2: (0.0000) | Acc: (85.00%) (28682/33408)
Epoch: 49 | Batch_idx: 270 |  Loss: (0.4138) |  Loss2: (0.0000) | Acc: (85.00%) (29779/34688)
Epoch: 49 | Batch_idx: 280 |  Loss: (0.4135) |  Loss2: (0.0000) | Acc: (85.00%) (30887/35968)
Epoch: 49 | Batch_idx: 290 |  Loss: (0.4144) |  Loss2: (0.0000) | Acc: (85.00%) (31959/37248)
Epoch: 49 | Batch_idx: 300 |  Loss: (0.4145) |  Loss2: (0.0000) | Acc: (85.00%) (33055/38528)
Epoch: 49 | Batch_idx: 310 |  Loss: (0.4143) |  Loss2: (0.0000) | Acc: (85.00%) (34156/39808)
Epoch: 49 | Batch_idx: 320 |  Loss: (0.4145) |  Loss2: (0.0000) | Acc: (85.00%) (35254/41088)
Epoch: 49 | Batch_idx: 330 |  Loss: (0.4150) |  Loss2: (0.0000) | Acc: (85.00%) (36340/42368)
Epoch: 49 | Batch_idx: 340 |  Loss: (0.4144) |  Loss2: (0.0000) | Acc: (85.00%) (37442/43648)
Epoch: 49 | Batch_idx: 350 |  Loss: (0.4134) |  Loss2: (0.0000) | Acc: (85.00%) (38555/44928)
Epoch: 49 | Batch_idx: 360 |  Loss: (0.4129) |  Loss2: (0.0000) | Acc: (85.00%) (39658/46208)
Epoch: 49 | Batch_idx: 370 |  Loss: (0.4132) |  Loss2: (0.0000) | Acc: (85.00%) (40739/47488)
Epoch: 49 | Batch_idx: 380 |  Loss: (0.4137) |  Loss2: (0.0000) | Acc: (85.00%) (41829/48768)
Epoch: 49 | Batch_idx: 390 |  Loss: (0.4143) |  Loss2: (0.0000) | Acc: (85.00%) (42879/50000)
# TEST : Loss: (0.5002) | Acc: (83.00%) (8329/10000)
percent tensor([0.5077, 0.4923], device='cuda:0')
percent tensor([0.4906, 0.5094], device='cuda:0')
percent tensor([0.4806, 0.5194], device='cuda:0')
percent tensor([0.5794, 0.4206], device='cuda:0')
percent tensor([0.5662, 0.4338], device='cuda:0')
percent tensor([0.6077, 0.3923], device='cuda:0')
percent tensor([0.7054, 0.2946], device='cuda:0')
percent tensor([0.9951, 0.0049], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 50 | Batch_idx: 0 |  Loss: (0.4727) |  Loss2: (0.0000) | Acc: (82.00%) (106/128)
Epoch: 50 | Batch_idx: 10 |  Loss: (0.4349) |  Loss2: (0.0000) | Acc: (84.00%) (1192/1408)
Epoch: 50 | Batch_idx: 20 |  Loss: (0.4075) |  Loss2: (0.0000) | Acc: (86.00%) (2321/2688)
Epoch: 50 | Batch_idx: 30 |  Loss: (0.4033) |  Loss2: (0.0000) | Acc: (86.00%) (3428/3968)
Epoch: 50 | Batch_idx: 40 |  Loss: (0.4097) |  Loss2: (0.0000) | Acc: (86.00%) (4517/5248)
Epoch: 50 | Batch_idx: 50 |  Loss: (0.4113) |  Loss2: (0.0000) | Acc: (86.00%) (5617/6528)
Epoch: 50 | Batch_idx: 60 |  Loss: (0.4093) |  Loss2: (0.0000) | Acc: (86.00%) (6725/7808)
Epoch: 50 | Batch_idx: 70 |  Loss: (0.4151) |  Loss2: (0.0000) | Acc: (85.00%) (7805/9088)
Epoch: 50 | Batch_idx: 80 |  Loss: (0.4139) |  Loss2: (0.0000) | Acc: (85.00%) (8895/10368)
Epoch: 50 | Batch_idx: 90 |  Loss: (0.4169) |  Loss2: (0.0000) | Acc: (85.00%) (9985/11648)
Epoch: 50 | Batch_idx: 100 |  Loss: (0.4163) |  Loss2: (0.0000) | Acc: (85.00%) (11083/12928)
Epoch: 50 | Batch_idx: 110 |  Loss: (0.4142) |  Loss2: (0.0000) | Acc: (85.00%) (12187/14208)
Epoch: 50 | Batch_idx: 120 |  Loss: (0.4151) |  Loss2: (0.0000) | Acc: (85.00%) (13287/15488)
Epoch: 50 | Batch_idx: 130 |  Loss: (0.4114) |  Loss2: (0.0000) | Acc: (85.00%) (14410/16768)
Epoch: 50 | Batch_idx: 140 |  Loss: (0.4133) |  Loss2: (0.0000) | Acc: (85.00%) (15485/18048)
Epoch: 50 | Batch_idx: 150 |  Loss: (0.4134) |  Loss2: (0.0000) | Acc: (85.00%) (16597/19328)
Epoch: 50 | Batch_idx: 160 |  Loss: (0.4119) |  Loss2: (0.0000) | Acc: (85.00%) (17716/20608)
Epoch: 50 | Batch_idx: 170 |  Loss: (0.4118) |  Loss2: (0.0000) | Acc: (85.00%) (18817/21888)
Epoch: 50 | Batch_idx: 180 |  Loss: (0.4122) |  Loss2: (0.0000) | Acc: (85.00%) (19910/23168)
Epoch: 50 | Batch_idx: 190 |  Loss: (0.4123) |  Loss2: (0.0000) | Acc: (85.00%) (21000/24448)
Epoch: 50 | Batch_idx: 200 |  Loss: (0.4108) |  Loss2: (0.0000) | Acc: (85.00%) (22115/25728)
Epoch: 50 | Batch_idx: 210 |  Loss: (0.4086) |  Loss2: (0.0000) | Acc: (86.00%) (23240/27008)
Epoch: 50 | Batch_idx: 220 |  Loss: (0.4104) |  Loss2: (0.0000) | Acc: (85.00%) (24323/28288)
Epoch: 50 | Batch_idx: 230 |  Loss: (0.4122) |  Loss2: (0.0000) | Acc: (85.00%) (25404/29568)
Epoch: 50 | Batch_idx: 240 |  Loss: (0.4124) |  Loss2: (0.0000) | Acc: (85.00%) (26500/30848)
Epoch: 50 | Batch_idx: 250 |  Loss: (0.4133) |  Loss2: (0.0000) | Acc: (85.00%) (27581/32128)
Epoch: 50 | Batch_idx: 260 |  Loss: (0.4147) |  Loss2: (0.0000) | Acc: (85.00%) (28659/33408)
Epoch: 50 | Batch_idx: 270 |  Loss: (0.4169) |  Loss2: (0.0000) | Acc: (85.00%) (29745/34688)
Epoch: 50 | Batch_idx: 280 |  Loss: (0.4177) |  Loss2: (0.0000) | Acc: (85.00%) (30827/35968)
Epoch: 50 | Batch_idx: 290 |  Loss: (0.4183) |  Loss2: (0.0000) | Acc: (85.00%) (31932/37248)
Epoch: 50 | Batch_idx: 300 |  Loss: (0.4175) |  Loss2: (0.0000) | Acc: (85.00%) (33020/38528)
Epoch: 50 | Batch_idx: 310 |  Loss: (0.4181) |  Loss2: (0.0000) | Acc: (85.00%) (34116/39808)
Epoch: 50 | Batch_idx: 320 |  Loss: (0.4182) |  Loss2: (0.0000) | Acc: (85.00%) (35196/41088)
Epoch: 50 | Batch_idx: 330 |  Loss: (0.4172) |  Loss2: (0.0000) | Acc: (85.00%) (36299/42368)
Epoch: 50 | Batch_idx: 340 |  Loss: (0.4166) |  Loss2: (0.0000) | Acc: (85.00%) (37404/43648)
Epoch: 50 | Batch_idx: 350 |  Loss: (0.4167) |  Loss2: (0.0000) | Acc: (85.00%) (38498/44928)
Epoch: 50 | Batch_idx: 360 |  Loss: (0.4166) |  Loss2: (0.0000) | Acc: (85.00%) (39594/46208)
Epoch: 50 | Batch_idx: 370 |  Loss: (0.4164) |  Loss2: (0.0000) | Acc: (85.00%) (40703/47488)
Epoch: 50 | Batch_idx: 380 |  Loss: (0.4175) |  Loss2: (0.0000) | Acc: (85.00%) (41773/48768)
Epoch: 50 | Batch_idx: 390 |  Loss: (0.4173) |  Loss2: (0.0000) | Acc: (85.00%) (42837/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_050.pth.tar'
# TEST : Loss: (0.4943) | Acc: (83.00%) (8346/10000)
percent tensor([0.5078, 0.4922], device='cuda:0')
percent tensor([0.4906, 0.5094], device='cuda:0')
percent tensor([0.4802, 0.5198], device='cuda:0')
percent tensor([0.5804, 0.4196], device='cuda:0')
percent tensor([0.5657, 0.4343], device='cuda:0')
percent tensor([0.6067, 0.3933], device='cuda:0')
percent tensor([0.7026, 0.2974], device='cuda:0')
percent tensor([0.9950, 0.0050], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(175.7307, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(803.8704, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(794.6786, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1519.3600, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(512.0760, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2198.8411, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4286.9521, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1427.3369, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6103.7212, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12052.9170, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4016.8042, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16981.1602, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 51 | Batch_idx: 0 |  Loss: (0.4485) |  Loss2: (0.0000) | Acc: (85.00%) (109/128)
Epoch: 51 | Batch_idx: 10 |  Loss: (0.3930) |  Loss2: (0.0000) | Acc: (86.00%) (1219/1408)
Epoch: 51 | Batch_idx: 20 |  Loss: (0.3842) |  Loss2: (0.0000) | Acc: (86.00%) (2329/2688)
Epoch: 51 | Batch_idx: 30 |  Loss: (0.3862) |  Loss2: (0.0000) | Acc: (86.00%) (3433/3968)
Epoch: 51 | Batch_idx: 40 |  Loss: (0.3818) |  Loss2: (0.0000) | Acc: (86.00%) (4563/5248)
Epoch: 51 | Batch_idx: 50 |  Loss: (0.3913) |  Loss2: (0.0000) | Acc: (86.00%) (5659/6528)
Epoch: 51 | Batch_idx: 60 |  Loss: (0.3926) |  Loss2: (0.0000) | Acc: (86.00%) (6753/7808)
Epoch: 51 | Batch_idx: 70 |  Loss: (0.3918) |  Loss2: (0.0000) | Acc: (86.00%) (7865/9088)
Epoch: 51 | Batch_idx: 80 |  Loss: (0.3922) |  Loss2: (0.0000) | Acc: (86.00%) (8964/10368)
Epoch: 51 | Batch_idx: 90 |  Loss: (0.3933) |  Loss2: (0.0000) | Acc: (86.00%) (10076/11648)
Epoch: 51 | Batch_idx: 100 |  Loss: (0.3955) |  Loss2: (0.0000) | Acc: (86.00%) (11170/12928)
Epoch: 51 | Batch_idx: 110 |  Loss: (0.3983) |  Loss2: (0.0000) | Acc: (86.00%) (12252/14208)
Epoch: 51 | Batch_idx: 120 |  Loss: (0.3978) |  Loss2: (0.0000) | Acc: (86.00%) (13356/15488)
Epoch: 51 | Batch_idx: 130 |  Loss: (0.3946) |  Loss2: (0.0000) | Acc: (86.00%) (14484/16768)
Epoch: 51 | Batch_idx: 140 |  Loss: (0.3948) |  Loss2: (0.0000) | Acc: (86.00%) (15590/18048)
Epoch: 51 | Batch_idx: 150 |  Loss: (0.3954) |  Loss2: (0.0000) | Acc: (86.00%) (16695/19328)
Epoch: 51 | Batch_idx: 160 |  Loss: (0.3931) |  Loss2: (0.0000) | Acc: (86.00%) (17819/20608)
Epoch: 51 | Batch_idx: 170 |  Loss: (0.3941) |  Loss2: (0.0000) | Acc: (86.00%) (18933/21888)
Epoch: 51 | Batch_idx: 180 |  Loss: (0.3930) |  Loss2: (0.0000) | Acc: (86.00%) (20048/23168)
Epoch: 51 | Batch_idx: 190 |  Loss: (0.3937) |  Loss2: (0.0000) | Acc: (86.00%) (21154/24448)
Epoch: 51 | Batch_idx: 200 |  Loss: (0.3931) |  Loss2: (0.0000) | Acc: (86.00%) (22275/25728)
Epoch: 51 | Batch_idx: 210 |  Loss: (0.3941) |  Loss2: (0.0000) | Acc: (86.00%) (23368/27008)
Epoch: 51 | Batch_idx: 220 |  Loss: (0.3959) |  Loss2: (0.0000) | Acc: (86.00%) (24453/28288)
Epoch: 51 | Batch_idx: 230 |  Loss: (0.3959) |  Loss2: (0.0000) | Acc: (86.00%) (25561/29568)
Epoch: 51 | Batch_idx: 240 |  Loss: (0.3974) |  Loss2: (0.0000) | Acc: (86.00%) (26660/30848)
Epoch: 51 | Batch_idx: 250 |  Loss: (0.3968) |  Loss2: (0.0000) | Acc: (86.00%) (27779/32128)
Epoch: 51 | Batch_idx: 260 |  Loss: (0.3980) |  Loss2: (0.0000) | Acc: (86.00%) (28882/33408)
Epoch: 51 | Batch_idx: 270 |  Loss: (0.3983) |  Loss2: (0.0000) | Acc: (86.00%) (29960/34688)
Epoch: 51 | Batch_idx: 280 |  Loss: (0.3979) |  Loss2: (0.0000) | Acc: (86.00%) (31060/35968)
Epoch: 51 | Batch_idx: 290 |  Loss: (0.3980) |  Loss2: (0.0000) | Acc: (86.00%) (32167/37248)
Epoch: 51 | Batch_idx: 300 |  Loss: (0.3984) |  Loss2: (0.0000) | Acc: (86.00%) (33251/38528)
Epoch: 51 | Batch_idx: 310 |  Loss: (0.3973) |  Loss2: (0.0000) | Acc: (86.00%) (34386/39808)
Epoch: 51 | Batch_idx: 320 |  Loss: (0.3977) |  Loss2: (0.0000) | Acc: (86.00%) (35480/41088)
Epoch: 51 | Batch_idx: 330 |  Loss: (0.3969) |  Loss2: (0.0000) | Acc: (86.00%) (36593/42368)
Epoch: 51 | Batch_idx: 340 |  Loss: (0.3969) |  Loss2: (0.0000) | Acc: (86.00%) (37694/43648)
Epoch: 51 | Batch_idx: 350 |  Loss: (0.3974) |  Loss2: (0.0000) | Acc: (86.00%) (38781/44928)
Epoch: 51 | Batch_idx: 360 |  Loss: (0.3988) |  Loss2: (0.0000) | Acc: (86.00%) (39855/46208)
Epoch: 51 | Batch_idx: 370 |  Loss: (0.3997) |  Loss2: (0.0000) | Acc: (86.00%) (40948/47488)
Epoch: 51 | Batch_idx: 380 |  Loss: (0.4002) |  Loss2: (0.0000) | Acc: (86.00%) (42047/48768)
Epoch: 51 | Batch_idx: 390 |  Loss: (0.4001) |  Loss2: (0.0000) | Acc: (86.00%) (43097/50000)
# TEST : Loss: (0.6194) | Acc: (80.00%) (8005/10000)
percent tensor([0.5078, 0.4922], device='cuda:0')
percent tensor([0.4906, 0.5094], device='cuda:0')
percent tensor([0.4804, 0.5196], device='cuda:0')
percent tensor([0.5798, 0.4202], device='cuda:0')
percent tensor([0.5675, 0.4325], device='cuda:0')
percent tensor([0.6074, 0.3926], device='cuda:0')
percent tensor([0.7055, 0.2945], device='cuda:0')
percent tensor([0.9957, 0.0043], device='cuda:0')
Epoch: 52 | Batch_idx: 0 |  Loss: (0.3348) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 52 | Batch_idx: 10 |  Loss: (0.3707) |  Loss2: (0.0000) | Acc: (87.00%) (1228/1408)
Epoch: 52 | Batch_idx: 20 |  Loss: (0.3730) |  Loss2: (0.0000) | Acc: (87.00%) (2355/2688)
Epoch: 52 | Batch_idx: 30 |  Loss: (0.3742) |  Loss2: (0.0000) | Acc: (87.00%) (3470/3968)
Epoch: 52 | Batch_idx: 40 |  Loss: (0.3785) |  Loss2: (0.0000) | Acc: (87.00%) (4586/5248)
Epoch: 52 | Batch_idx: 50 |  Loss: (0.3827) |  Loss2: (0.0000) | Acc: (87.00%) (5699/6528)
Epoch: 52 | Batch_idx: 60 |  Loss: (0.3821) |  Loss2: (0.0000) | Acc: (87.00%) (6810/7808)
Epoch: 52 | Batch_idx: 70 |  Loss: (0.3794) |  Loss2: (0.0000) | Acc: (87.00%) (7926/9088)
Epoch: 52 | Batch_idx: 80 |  Loss: (0.3792) |  Loss2: (0.0000) | Acc: (87.00%) (9030/10368)
Epoch: 52 | Batch_idx: 90 |  Loss: (0.3803) |  Loss2: (0.0000) | Acc: (87.00%) (10142/11648)
Epoch: 52 | Batch_idx: 100 |  Loss: (0.3791) |  Loss2: (0.0000) | Acc: (87.00%) (11252/12928)
Epoch: 52 | Batch_idx: 110 |  Loss: (0.3760) |  Loss2: (0.0000) | Acc: (87.00%) (12384/14208)
Epoch: 52 | Batch_idx: 120 |  Loss: (0.3763) |  Loss2: (0.0000) | Acc: (87.00%) (13496/15488)
Epoch: 52 | Batch_idx: 130 |  Loss: (0.3766) |  Loss2: (0.0000) | Acc: (87.00%) (14612/16768)
Epoch: 52 | Batch_idx: 140 |  Loss: (0.3767) |  Loss2: (0.0000) | Acc: (87.00%) (15716/18048)
Epoch: 52 | Batch_idx: 150 |  Loss: (0.3791) |  Loss2: (0.0000) | Acc: (87.00%) (16821/19328)
Epoch: 52 | Batch_idx: 160 |  Loss: (0.3782) |  Loss2: (0.0000) | Acc: (87.00%) (17932/20608)
Epoch: 52 | Batch_idx: 170 |  Loss: (0.3802) |  Loss2: (0.0000) | Acc: (86.00%) (19024/21888)
Epoch: 52 | Batch_idx: 180 |  Loss: (0.3789) |  Loss2: (0.0000) | Acc: (86.00%) (20141/23168)
Epoch: 52 | Batch_idx: 190 |  Loss: (0.3796) |  Loss2: (0.0000) | Acc: (86.00%) (21240/24448)
Epoch: 52 | Batch_idx: 200 |  Loss: (0.3809) |  Loss2: (0.0000) | Acc: (86.00%) (22335/25728)
Epoch: 52 | Batch_idx: 210 |  Loss: (0.3806) |  Loss2: (0.0000) | Acc: (86.00%) (23460/27008)
Epoch: 52 | Batch_idx: 220 |  Loss: (0.3794) |  Loss2: (0.0000) | Acc: (86.00%) (24586/28288)
Epoch: 52 | Batch_idx: 230 |  Loss: (0.3796) |  Loss2: (0.0000) | Acc: (86.00%) (25699/29568)
Epoch: 52 | Batch_idx: 240 |  Loss: (0.3814) |  Loss2: (0.0000) | Acc: (86.00%) (26787/30848)
Epoch: 52 | Batch_idx: 250 |  Loss: (0.3820) |  Loss2: (0.0000) | Acc: (86.00%) (27871/32128)
Epoch: 52 | Batch_idx: 260 |  Loss: (0.3822) |  Loss2: (0.0000) | Acc: (86.00%) (28980/33408)
Epoch: 52 | Batch_idx: 270 |  Loss: (0.3835) |  Loss2: (0.0000) | Acc: (86.00%) (30089/34688)
Epoch: 52 | Batch_idx: 280 |  Loss: (0.3829) |  Loss2: (0.0000) | Acc: (86.00%) (31212/35968)
Epoch: 52 | Batch_idx: 290 |  Loss: (0.3832) |  Loss2: (0.0000) | Acc: (86.00%) (32321/37248)
Epoch: 52 | Batch_idx: 300 |  Loss: (0.3846) |  Loss2: (0.0000) | Acc: (86.00%) (33414/38528)
Epoch: 52 | Batch_idx: 310 |  Loss: (0.3863) |  Loss2: (0.0000) | Acc: (86.00%) (34501/39808)
Epoch: 52 | Batch_idx: 320 |  Loss: (0.3868) |  Loss2: (0.0000) | Acc: (86.00%) (35604/41088)
Epoch: 52 | Batch_idx: 330 |  Loss: (0.3876) |  Loss2: (0.0000) | Acc: (86.00%) (36694/42368)
Epoch: 52 | Batch_idx: 340 |  Loss: (0.3869) |  Loss2: (0.0000) | Acc: (86.00%) (37803/43648)
Epoch: 52 | Batch_idx: 350 |  Loss: (0.3874) |  Loss2: (0.0000) | Acc: (86.00%) (38900/44928)
Epoch: 52 | Batch_idx: 360 |  Loss: (0.3888) |  Loss2: (0.0000) | Acc: (86.00%) (39984/46208)
Epoch: 52 | Batch_idx: 370 |  Loss: (0.3887) |  Loss2: (0.0000) | Acc: (86.00%) (41097/47488)
Epoch: 52 | Batch_idx: 380 |  Loss: (0.3883) |  Loss2: (0.0000) | Acc: (86.00%) (42216/48768)
Epoch: 52 | Batch_idx: 390 |  Loss: (0.3890) |  Loss2: (0.0000) | Acc: (86.00%) (43275/50000)
# TEST : Loss: (0.5894) | Acc: (81.00%) (8147/10000)
percent tensor([0.5077, 0.4923], device='cuda:0')
percent tensor([0.4907, 0.5093], device='cuda:0')
percent tensor([0.4807, 0.5193], device='cuda:0')
percent tensor([0.5809, 0.4191], device='cuda:0')
percent tensor([0.5670, 0.4330], device='cuda:0')
percent tensor([0.6058, 0.3942], device='cuda:0')
percent tensor([0.7029, 0.2971], device='cuda:0')
percent tensor([0.9947, 0.0053], device='cuda:0')
Epoch: 53 | Batch_idx: 0 |  Loss: (0.3664) |  Loss2: (0.0000) | Acc: (87.00%) (112/128)
Epoch: 53 | Batch_idx: 10 |  Loss: (0.3888) |  Loss2: (0.0000) | Acc: (85.00%) (1204/1408)
Epoch: 53 | Batch_idx: 20 |  Loss: (0.3694) |  Loss2: (0.0000) | Acc: (87.00%) (2342/2688)
Epoch: 53 | Batch_idx: 30 |  Loss: (0.3695) |  Loss2: (0.0000) | Acc: (86.00%) (3452/3968)
Epoch: 53 | Batch_idx: 40 |  Loss: (0.3668) |  Loss2: (0.0000) | Acc: (87.00%) (4578/5248)
Epoch: 53 | Batch_idx: 50 |  Loss: (0.3710) |  Loss2: (0.0000) | Acc: (87.00%) (5691/6528)
Epoch: 53 | Batch_idx: 60 |  Loss: (0.3724) |  Loss2: (0.0000) | Acc: (87.00%) (6809/7808)
Epoch: 53 | Batch_idx: 70 |  Loss: (0.3726) |  Loss2: (0.0000) | Acc: (87.00%) (7929/9088)
Epoch: 53 | Batch_idx: 80 |  Loss: (0.3731) |  Loss2: (0.0000) | Acc: (87.00%) (9045/10368)
Epoch: 53 | Batch_idx: 90 |  Loss: (0.3712) |  Loss2: (0.0000) | Acc: (87.00%) (10173/11648)
Epoch: 53 | Batch_idx: 100 |  Loss: (0.3732) |  Loss2: (0.0000) | Acc: (87.00%) (11265/12928)
Epoch: 53 | Batch_idx: 110 |  Loss: (0.3734) |  Loss2: (0.0000) | Acc: (86.00%) (12360/14208)
Epoch: 53 | Batch_idx: 120 |  Loss: (0.3733) |  Loss2: (0.0000) | Acc: (87.00%) (13493/15488)
Epoch: 53 | Batch_idx: 130 |  Loss: (0.3737) |  Loss2: (0.0000) | Acc: (87.00%) (14609/16768)
Epoch: 53 | Batch_idx: 140 |  Loss: (0.3741) |  Loss2: (0.0000) | Acc: (87.00%) (15720/18048)
Epoch: 53 | Batch_idx: 150 |  Loss: (0.3755) |  Loss2: (0.0000) | Acc: (87.00%) (16831/19328)
Epoch: 53 | Batch_idx: 160 |  Loss: (0.3758) |  Loss2: (0.0000) | Acc: (86.00%) (17925/20608)
Epoch: 53 | Batch_idx: 170 |  Loss: (0.3762) |  Loss2: (0.0000) | Acc: (86.00%) (19036/21888)
Epoch: 53 | Batch_idx: 180 |  Loss: (0.3740) |  Loss2: (0.0000) | Acc: (87.00%) (20158/23168)
Epoch: 53 | Batch_idx: 190 |  Loss: (0.3767) |  Loss2: (0.0000) | Acc: (86.00%) (21268/24448)
Epoch: 53 | Batch_idx: 200 |  Loss: (0.3761) |  Loss2: (0.0000) | Acc: (87.00%) (22391/25728)
Epoch: 53 | Batch_idx: 210 |  Loss: (0.3757) |  Loss2: (0.0000) | Acc: (87.00%) (23508/27008)
Epoch: 53 | Batch_idx: 220 |  Loss: (0.3749) |  Loss2: (0.0000) | Acc: (87.00%) (24620/28288)
Epoch: 53 | Batch_idx: 230 |  Loss: (0.3749) |  Loss2: (0.0000) | Acc: (87.00%) (25737/29568)
Epoch: 53 | Batch_idx: 240 |  Loss: (0.3754) |  Loss2: (0.0000) | Acc: (87.00%) (26844/30848)
Epoch: 53 | Batch_idx: 250 |  Loss: (0.3758) |  Loss2: (0.0000) | Acc: (87.00%) (27961/32128)
Epoch: 53 | Batch_idx: 260 |  Loss: (0.3767) |  Loss2: (0.0000) | Acc: (86.00%) (29060/33408)
Epoch: 53 | Batch_idx: 270 |  Loss: (0.3774) |  Loss2: (0.0000) | Acc: (86.00%) (30165/34688)
Epoch: 53 | Batch_idx: 280 |  Loss: (0.3783) |  Loss2: (0.0000) | Acc: (86.00%) (31270/35968)
Epoch: 53 | Batch_idx: 290 |  Loss: (0.3779) |  Loss2: (0.0000) | Acc: (86.00%) (32389/37248)
Epoch: 53 | Batch_idx: 300 |  Loss: (0.3780) |  Loss2: (0.0000) | Acc: (86.00%) (33505/38528)
Epoch: 53 | Batch_idx: 310 |  Loss: (0.3789) |  Loss2: (0.0000) | Acc: (86.00%) (34608/39808)
Epoch: 53 | Batch_idx: 320 |  Loss: (0.3783) |  Loss2: (0.0000) | Acc: (86.00%) (35743/41088)
Epoch: 53 | Batch_idx: 330 |  Loss: (0.3783) |  Loss2: (0.0000) | Acc: (86.00%) (36853/42368)
Epoch: 53 | Batch_idx: 340 |  Loss: (0.3796) |  Loss2: (0.0000) | Acc: (86.00%) (37952/43648)
Epoch: 53 | Batch_idx: 350 |  Loss: (0.3793) |  Loss2: (0.0000) | Acc: (86.00%) (39068/44928)
Epoch: 53 | Batch_idx: 360 |  Loss: (0.3795) |  Loss2: (0.0000) | Acc: (86.00%) (40188/46208)
Epoch: 53 | Batch_idx: 370 |  Loss: (0.3792) |  Loss2: (0.0000) | Acc: (87.00%) (41319/47488)
Epoch: 53 | Batch_idx: 380 |  Loss: (0.3788) |  Loss2: (0.0000) | Acc: (86.00%) (42426/48768)
Epoch: 53 | Batch_idx: 390 |  Loss: (0.3785) |  Loss2: (0.0000) | Acc: (87.00%) (43512/50000)
# TEST : Loss: (0.4622) | Acc: (84.00%) (8444/10000)
percent tensor([0.5077, 0.4923], device='cuda:0')
percent tensor([0.4906, 0.5094], device='cuda:0')
percent tensor([0.4809, 0.5191], device='cuda:0')
percent tensor([0.5805, 0.4195], device='cuda:0')
percent tensor([0.5677, 0.4323], device='cuda:0')
percent tensor([0.6075, 0.3925], device='cuda:0')
percent tensor([0.7031, 0.2969], device='cuda:0')
percent tensor([0.9952, 0.0048], device='cuda:0')
Epoch: 54 | Batch_idx: 0 |  Loss: (0.4024) |  Loss2: (0.0000) | Acc: (85.00%) (109/128)
Epoch: 54 | Batch_idx: 10 |  Loss: (0.3753) |  Loss2: (0.0000) | Acc: (87.00%) (1226/1408)
Epoch: 54 | Batch_idx: 20 |  Loss: (0.3628) |  Loss2: (0.0000) | Acc: (87.00%) (2365/2688)
Epoch: 54 | Batch_idx: 30 |  Loss: (0.3572) |  Loss2: (0.0000) | Acc: (87.00%) (3483/3968)
Epoch: 54 | Batch_idx: 40 |  Loss: (0.3548) |  Loss2: (0.0000) | Acc: (87.00%) (4606/5248)
Epoch: 54 | Batch_idx: 50 |  Loss: (0.3560) |  Loss2: (0.0000) | Acc: (87.00%) (5717/6528)
Epoch: 54 | Batch_idx: 60 |  Loss: (0.3580) |  Loss2: (0.0000) | Acc: (87.00%) (6826/7808)
Epoch: 54 | Batch_idx: 70 |  Loss: (0.3582) |  Loss2: (0.0000) | Acc: (87.00%) (7949/9088)
Epoch: 54 | Batch_idx: 80 |  Loss: (0.3566) |  Loss2: (0.0000) | Acc: (87.00%) (9072/10368)
Epoch: 54 | Batch_idx: 90 |  Loss: (0.3572) |  Loss2: (0.0000) | Acc: (87.00%) (10193/11648)
Epoch: 54 | Batch_idx: 100 |  Loss: (0.3597) |  Loss2: (0.0000) | Acc: (87.00%) (11295/12928)
Epoch: 54 | Batch_idx: 110 |  Loss: (0.3616) |  Loss2: (0.0000) | Acc: (87.00%) (12412/14208)
Epoch: 54 | Batch_idx: 120 |  Loss: (0.3598) |  Loss2: (0.0000) | Acc: (87.00%) (13545/15488)
Epoch: 54 | Batch_idx: 130 |  Loss: (0.3618) |  Loss2: (0.0000) | Acc: (87.00%) (14648/16768)
Epoch: 54 | Batch_idx: 140 |  Loss: (0.3624) |  Loss2: (0.0000) | Acc: (87.00%) (15762/18048)
Epoch: 54 | Batch_idx: 150 |  Loss: (0.3632) |  Loss2: (0.0000) | Acc: (87.00%) (16876/19328)
Epoch: 54 | Batch_idx: 160 |  Loss: (0.3637) |  Loss2: (0.0000) | Acc: (87.00%) (17991/20608)
Epoch: 54 | Batch_idx: 170 |  Loss: (0.3635) |  Loss2: (0.0000) | Acc: (87.00%) (19110/21888)
Epoch: 54 | Batch_idx: 180 |  Loss: (0.3641) |  Loss2: (0.0000) | Acc: (87.00%) (20223/23168)
Epoch: 54 | Batch_idx: 190 |  Loss: (0.3645) |  Loss2: (0.0000) | Acc: (87.00%) (21351/24448)
Epoch: 54 | Batch_idx: 200 |  Loss: (0.3637) |  Loss2: (0.0000) | Acc: (87.00%) (22481/25728)
Epoch: 54 | Batch_idx: 210 |  Loss: (0.3631) |  Loss2: (0.0000) | Acc: (87.00%) (23599/27008)
Epoch: 54 | Batch_idx: 220 |  Loss: (0.3627) |  Loss2: (0.0000) | Acc: (87.00%) (24726/28288)
Epoch: 54 | Batch_idx: 230 |  Loss: (0.3651) |  Loss2: (0.0000) | Acc: (87.00%) (25824/29568)
Epoch: 54 | Batch_idx: 240 |  Loss: (0.3649) |  Loss2: (0.0000) | Acc: (87.00%) (26963/30848)
Epoch: 54 | Batch_idx: 250 |  Loss: (0.3638) |  Loss2: (0.0000) | Acc: (87.00%) (28095/32128)
Epoch: 54 | Batch_idx: 260 |  Loss: (0.3645) |  Loss2: (0.0000) | Acc: (87.00%) (29209/33408)
Epoch: 54 | Batch_idx: 270 |  Loss: (0.3654) |  Loss2: (0.0000) | Acc: (87.00%) (30321/34688)
Epoch: 54 | Batch_idx: 280 |  Loss: (0.3649) |  Loss2: (0.0000) | Acc: (87.00%) (31439/35968)
Epoch: 54 | Batch_idx: 290 |  Loss: (0.3666) |  Loss2: (0.0000) | Acc: (87.00%) (32545/37248)
Epoch: 54 | Batch_idx: 300 |  Loss: (0.3662) |  Loss2: (0.0000) | Acc: (87.00%) (33674/38528)
Epoch: 54 | Batch_idx: 310 |  Loss: (0.3652) |  Loss2: (0.0000) | Acc: (87.00%) (34803/39808)
Epoch: 54 | Batch_idx: 320 |  Loss: (0.3646) |  Loss2: (0.0000) | Acc: (87.00%) (35935/41088)
Epoch: 54 | Batch_idx: 330 |  Loss: (0.3653) |  Loss2: (0.0000) | Acc: (87.00%) (37044/42368)
Epoch: 54 | Batch_idx: 340 |  Loss: (0.3652) |  Loss2: (0.0000) | Acc: (87.00%) (38158/43648)
Epoch: 54 | Batch_idx: 350 |  Loss: (0.3647) |  Loss2: (0.0000) | Acc: (87.00%) (39280/44928)
Epoch: 54 | Batch_idx: 360 |  Loss: (0.3652) |  Loss2: (0.0000) | Acc: (87.00%) (40403/46208)
Epoch: 54 | Batch_idx: 370 |  Loss: (0.3646) |  Loss2: (0.0000) | Acc: (87.00%) (41539/47488)
Epoch: 54 | Batch_idx: 380 |  Loss: (0.3650) |  Loss2: (0.0000) | Acc: (87.00%) (42651/48768)
Epoch: 54 | Batch_idx: 390 |  Loss: (0.3649) |  Loss2: (0.0000) | Acc: (87.00%) (43732/50000)
# TEST : Loss: (0.5503) | Acc: (82.00%) (8236/10000)
percent tensor([0.5077, 0.4923], device='cuda:0')
percent tensor([0.4908, 0.5092], device='cuda:0')
percent tensor([0.4803, 0.5197], device='cuda:0')
percent tensor([0.5793, 0.4207], device='cuda:0')
percent tensor([0.5658, 0.4342], device='cuda:0')
percent tensor([0.6060, 0.3940], device='cuda:0')
percent tensor([0.7011, 0.2989], device='cuda:0')
percent tensor([0.9947, 0.0053], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 55 | Batch_idx: 0 |  Loss: (0.3859) |  Loss2: (0.0000) | Acc: (85.00%) (110/128)
Epoch: 55 | Batch_idx: 10 |  Loss: (0.3878) |  Loss2: (0.0000) | Acc: (86.00%) (1222/1408)
Epoch: 55 | Batch_idx: 20 |  Loss: (0.4195) |  Loss2: (0.0000) | Acc: (86.00%) (2312/2688)
Epoch: 55 | Batch_idx: 30 |  Loss: (0.4226) |  Loss2: (0.0000) | Acc: (85.00%) (3391/3968)
Epoch: 55 | Batch_idx: 40 |  Loss: (0.4342) |  Loss2: (0.0000) | Acc: (84.00%) (4451/5248)
Epoch: 55 | Batch_idx: 50 |  Loss: (0.4455) |  Loss2: (0.0000) | Acc: (84.00%) (5504/6528)
Epoch: 55 | Batch_idx: 60 |  Loss: (0.4489) |  Loss2: (0.0000) | Acc: (84.00%) (6588/7808)
Epoch: 55 | Batch_idx: 70 |  Loss: (0.4569) |  Loss2: (0.0000) | Acc: (84.00%) (7648/9088)
Epoch: 55 | Batch_idx: 80 |  Loss: (0.4565) |  Loss2: (0.0000) | Acc: (84.00%) (8731/10368)
Epoch: 55 | Batch_idx: 90 |  Loss: (0.4563) |  Loss2: (0.0000) | Acc: (84.00%) (9809/11648)
Epoch: 55 | Batch_idx: 100 |  Loss: (0.4537) |  Loss2: (0.0000) | Acc: (84.00%) (10895/12928)
Epoch: 55 | Batch_idx: 110 |  Loss: (0.4503) |  Loss2: (0.0000) | Acc: (84.00%) (11988/14208)
Epoch: 55 | Batch_idx: 120 |  Loss: (0.4525) |  Loss2: (0.0000) | Acc: (84.00%) (13045/15488)
Epoch: 55 | Batch_idx: 130 |  Loss: (0.4522) |  Loss2: (0.0000) | Acc: (84.00%) (14129/16768)
Epoch: 55 | Batch_idx: 140 |  Loss: (0.4527) |  Loss2: (0.0000) | Acc: (84.00%) (15222/18048)
Epoch: 55 | Batch_idx: 150 |  Loss: (0.4492) |  Loss2: (0.0000) | Acc: (84.00%) (16323/19328)
Epoch: 55 | Batch_idx: 160 |  Loss: (0.4470) |  Loss2: (0.0000) | Acc: (84.00%) (17416/20608)
Epoch: 55 | Batch_idx: 170 |  Loss: (0.4463) |  Loss2: (0.0000) | Acc: (84.00%) (18508/21888)
Epoch: 55 | Batch_idx: 180 |  Loss: (0.4444) |  Loss2: (0.0000) | Acc: (84.00%) (19610/23168)
Epoch: 55 | Batch_idx: 190 |  Loss: (0.4443) |  Loss2: (0.0000) | Acc: (84.00%) (20688/24448)
Epoch: 55 | Batch_idx: 200 |  Loss: (0.4434) |  Loss2: (0.0000) | Acc: (84.00%) (21781/25728)
Epoch: 55 | Batch_idx: 210 |  Loss: (0.4414) |  Loss2: (0.0000) | Acc: (84.00%) (22895/27008)
Epoch: 55 | Batch_idx: 220 |  Loss: (0.4409) |  Loss2: (0.0000) | Acc: (84.00%) (23994/28288)
Epoch: 55 | Batch_idx: 230 |  Loss: (0.4402) |  Loss2: (0.0000) | Acc: (84.00%) (25086/29568)
Epoch: 55 | Batch_idx: 240 |  Loss: (0.4408) |  Loss2: (0.0000) | Acc: (84.00%) (26156/30848)
Epoch: 55 | Batch_idx: 250 |  Loss: (0.4404) |  Loss2: (0.0000) | Acc: (84.00%) (27244/32128)
Epoch: 55 | Batch_idx: 260 |  Loss: (0.4400) |  Loss2: (0.0000) | Acc: (84.00%) (28340/33408)
Epoch: 55 | Batch_idx: 270 |  Loss: (0.4386) |  Loss2: (0.0000) | Acc: (84.00%) (29435/34688)
Epoch: 55 | Batch_idx: 280 |  Loss: (0.4378) |  Loss2: (0.0000) | Acc: (84.00%) (30525/35968)
Epoch: 55 | Batch_idx: 290 |  Loss: (0.4372) |  Loss2: (0.0000) | Acc: (84.00%) (31608/37248)
Epoch: 55 | Batch_idx: 300 |  Loss: (0.4364) |  Loss2: (0.0000) | Acc: (84.00%) (32708/38528)
Epoch: 55 | Batch_idx: 310 |  Loss: (0.4356) |  Loss2: (0.0000) | Acc: (84.00%) (33797/39808)
Epoch: 55 | Batch_idx: 320 |  Loss: (0.4346) |  Loss2: (0.0000) | Acc: (84.00%) (34900/41088)
Epoch: 55 | Batch_idx: 330 |  Loss: (0.4345) |  Loss2: (0.0000) | Acc: (84.00%) (35991/42368)
Epoch: 55 | Batch_idx: 340 |  Loss: (0.4327) |  Loss2: (0.0000) | Acc: (84.00%) (37099/43648)
Epoch: 55 | Batch_idx: 350 |  Loss: (0.4313) |  Loss2: (0.0000) | Acc: (85.00%) (38205/44928)
Epoch: 55 | Batch_idx: 360 |  Loss: (0.4310) |  Loss2: (0.0000) | Acc: (85.00%) (39299/46208)
Epoch: 55 | Batch_idx: 370 |  Loss: (0.4315) |  Loss2: (0.0000) | Acc: (85.00%) (40380/47488)
Epoch: 55 | Batch_idx: 380 |  Loss: (0.4299) |  Loss2: (0.0000) | Acc: (85.00%) (41484/48768)
Epoch: 55 | Batch_idx: 390 |  Loss: (0.4278) |  Loss2: (0.0000) | Acc: (85.00%) (42569/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_055.pth.tar'
# TEST : Loss: (0.5126) | Acc: (82.00%) (8291/10000)
percent tensor([0.5063, 0.4937], device='cuda:0')
percent tensor([0.4835, 0.5165], device='cuda:0')
percent tensor([0.4818, 0.5182], device='cuda:0')
percent tensor([0.5644, 0.4356], device='cuda:0')
percent tensor([0.6251, 0.3749], device='cuda:0')
percent tensor([0.5806, 0.4194], device='cuda:0')
percent tensor([0.6443, 0.3557], device='cuda:0')
percent tensor([0.9950, 0.0050], device='cuda:0')
Epoch: 56 | Batch_idx: 0 |  Loss: (0.4379) |  Loss2: (0.0000) | Acc: (84.00%) (108/128)
Epoch: 56 | Batch_idx: 10 |  Loss: (0.4183) |  Loss2: (0.0000) | Acc: (85.00%) (1209/1408)
Epoch: 56 | Batch_idx: 20 |  Loss: (0.4195) |  Loss2: (0.0000) | Acc: (85.00%) (2295/2688)
Epoch: 56 | Batch_idx: 30 |  Loss: (0.4064) |  Loss2: (0.0000) | Acc: (85.00%) (3408/3968)
Epoch: 56 | Batch_idx: 40 |  Loss: (0.4076) |  Loss2: (0.0000) | Acc: (85.00%) (4512/5248)
Epoch: 56 | Batch_idx: 50 |  Loss: (0.4007) |  Loss2: (0.0000) | Acc: (86.00%) (5628/6528)
Epoch: 56 | Batch_idx: 60 |  Loss: (0.4043) |  Loss2: (0.0000) | Acc: (86.00%) (6731/7808)
Epoch: 56 | Batch_idx: 70 |  Loss: (0.4054) |  Loss2: (0.0000) | Acc: (86.00%) (7831/9088)
Epoch: 56 | Batch_idx: 80 |  Loss: (0.4060) |  Loss2: (0.0000) | Acc: (86.00%) (8937/10368)
Epoch: 56 | Batch_idx: 90 |  Loss: (0.4029) |  Loss2: (0.0000) | Acc: (86.00%) (10047/11648)
Epoch: 56 | Batch_idx: 100 |  Loss: (0.4027) |  Loss2: (0.0000) | Acc: (86.00%) (11150/12928)
Epoch: 56 | Batch_idx: 110 |  Loss: (0.4009) |  Loss2: (0.0000) | Acc: (86.00%) (12253/14208)
Epoch: 56 | Batch_idx: 120 |  Loss: (0.4030) |  Loss2: (0.0000) | Acc: (86.00%) (13340/15488)
Epoch: 56 | Batch_idx: 130 |  Loss: (0.4010) |  Loss2: (0.0000) | Acc: (86.00%) (14452/16768)
Epoch: 56 | Batch_idx: 140 |  Loss: (0.4012) |  Loss2: (0.0000) | Acc: (86.00%) (15544/18048)
Epoch: 56 | Batch_idx: 150 |  Loss: (0.4012) |  Loss2: (0.0000) | Acc: (86.00%) (16642/19328)
Epoch: 56 | Batch_idx: 160 |  Loss: (0.3999) |  Loss2: (0.0000) | Acc: (86.00%) (17767/20608)
Epoch: 56 | Batch_idx: 170 |  Loss: (0.3993) |  Loss2: (0.0000) | Acc: (86.00%) (18865/21888)
Epoch: 56 | Batch_idx: 180 |  Loss: (0.3973) |  Loss2: (0.0000) | Acc: (86.00%) (19981/23168)
Epoch: 56 | Batch_idx: 190 |  Loss: (0.3967) |  Loss2: (0.0000) | Acc: (86.00%) (21084/24448)
Epoch: 56 | Batch_idx: 200 |  Loss: (0.3953) |  Loss2: (0.0000) | Acc: (86.00%) (22197/25728)
Epoch: 56 | Batch_idx: 210 |  Loss: (0.3952) |  Loss2: (0.0000) | Acc: (86.00%) (23311/27008)
Epoch: 56 | Batch_idx: 220 |  Loss: (0.3957) |  Loss2: (0.0000) | Acc: (86.00%) (24408/28288)
Epoch: 56 | Batch_idx: 230 |  Loss: (0.3949) |  Loss2: (0.0000) | Acc: (86.00%) (25512/29568)
Epoch: 56 | Batch_idx: 240 |  Loss: (0.3945) |  Loss2: (0.0000) | Acc: (86.00%) (26622/30848)
Epoch: 56 | Batch_idx: 250 |  Loss: (0.3945) |  Loss2: (0.0000) | Acc: (86.00%) (27726/32128)
Epoch: 56 | Batch_idx: 260 |  Loss: (0.3949) |  Loss2: (0.0000) | Acc: (86.00%) (28832/33408)
Epoch: 56 | Batch_idx: 270 |  Loss: (0.3949) |  Loss2: (0.0000) | Acc: (86.00%) (29927/34688)
Epoch: 56 | Batch_idx: 280 |  Loss: (0.3947) |  Loss2: (0.0000) | Acc: (86.00%) (31033/35968)
Epoch: 56 | Batch_idx: 290 |  Loss: (0.3939) |  Loss2: (0.0000) | Acc: (86.00%) (32141/37248)
Epoch: 56 | Batch_idx: 300 |  Loss: (0.3953) |  Loss2: (0.0000) | Acc: (86.00%) (33232/38528)
Epoch: 56 | Batch_idx: 310 |  Loss: (0.3939) |  Loss2: (0.0000) | Acc: (86.00%) (34348/39808)
Epoch: 56 | Batch_idx: 320 |  Loss: (0.3937) |  Loss2: (0.0000) | Acc: (86.00%) (35452/41088)
Epoch: 56 | Batch_idx: 330 |  Loss: (0.3931) |  Loss2: (0.0000) | Acc: (86.00%) (36566/42368)
Epoch: 56 | Batch_idx: 340 |  Loss: (0.3924) |  Loss2: (0.0000) | Acc: (86.00%) (37682/43648)
Epoch: 56 | Batch_idx: 350 |  Loss: (0.3921) |  Loss2: (0.0000) | Acc: (86.00%) (38805/44928)
Epoch: 56 | Batch_idx: 360 |  Loss: (0.3920) |  Loss2: (0.0000) | Acc: (86.00%) (39902/46208)
Epoch: 56 | Batch_idx: 370 |  Loss: (0.3915) |  Loss2: (0.0000) | Acc: (86.00%) (41022/47488)
Epoch: 56 | Batch_idx: 380 |  Loss: (0.3908) |  Loss2: (0.0000) | Acc: (86.00%) (42144/48768)
Epoch: 56 | Batch_idx: 390 |  Loss: (0.3903) |  Loss2: (0.0000) | Acc: (86.00%) (43212/50000)
# TEST : Loss: (0.4854) | Acc: (83.00%) (8370/10000)
percent tensor([0.5068, 0.4932], device='cuda:0')
percent tensor([0.4827, 0.5173], device='cuda:0')
percent tensor([0.4824, 0.5176], device='cuda:0')
percent tensor([0.5649, 0.4351], device='cuda:0')
percent tensor([0.6215, 0.3785], device='cuda:0')
percent tensor([0.5904, 0.4096], device='cuda:0')
percent tensor([0.6476, 0.3524], device='cuda:0')
percent tensor([0.9956, 0.0044], device='cuda:0')
Epoch: 57 | Batch_idx: 0 |  Loss: (0.3751) |  Loss2: (0.0000) | Acc: (85.00%) (110/128)
Epoch: 57 | Batch_idx: 10 |  Loss: (0.3589) |  Loss2: (0.0000) | Acc: (87.00%) (1227/1408)
Epoch: 57 | Batch_idx: 20 |  Loss: (0.3790) |  Loss2: (0.0000) | Acc: (86.00%) (2320/2688)
Epoch: 57 | Batch_idx: 30 |  Loss: (0.3773) |  Loss2: (0.0000) | Acc: (86.00%) (3435/3968)
Epoch: 57 | Batch_idx: 40 |  Loss: (0.3805) |  Loss2: (0.0000) | Acc: (86.00%) (4546/5248)
Epoch: 57 | Batch_idx: 50 |  Loss: (0.3806) |  Loss2: (0.0000) | Acc: (86.00%) (5650/6528)
Epoch: 57 | Batch_idx: 60 |  Loss: (0.3843) |  Loss2: (0.0000) | Acc: (86.00%) (6758/7808)
Epoch: 57 | Batch_idx: 70 |  Loss: (0.3820) |  Loss2: (0.0000) | Acc: (86.00%) (7878/9088)
Epoch: 57 | Batch_idx: 80 |  Loss: (0.3795) |  Loss2: (0.0000) | Acc: (86.00%) (9003/10368)
Epoch: 57 | Batch_idx: 90 |  Loss: (0.3772) |  Loss2: (0.0000) | Acc: (86.00%) (10117/11648)
Epoch: 57 | Batch_idx: 100 |  Loss: (0.3735) |  Loss2: (0.0000) | Acc: (87.00%) (11253/12928)
Epoch: 57 | Batch_idx: 110 |  Loss: (0.3773) |  Loss2: (0.0000) | Acc: (86.00%) (12353/14208)
Epoch: 57 | Batch_idx: 120 |  Loss: (0.3770) |  Loss2: (0.0000) | Acc: (86.00%) (13473/15488)
Epoch: 57 | Batch_idx: 130 |  Loss: (0.3784) |  Loss2: (0.0000) | Acc: (86.00%) (14567/16768)
Epoch: 57 | Batch_idx: 140 |  Loss: (0.3808) |  Loss2: (0.0000) | Acc: (86.00%) (15668/18048)
Epoch: 57 | Batch_idx: 150 |  Loss: (0.3803) |  Loss2: (0.0000) | Acc: (86.00%) (16777/19328)
Epoch: 57 | Batch_idx: 160 |  Loss: (0.3813) |  Loss2: (0.0000) | Acc: (86.00%) (17881/20608)
Epoch: 57 | Batch_idx: 170 |  Loss: (0.3825) |  Loss2: (0.0000) | Acc: (86.00%) (18988/21888)
Epoch: 57 | Batch_idx: 180 |  Loss: (0.3812) |  Loss2: (0.0000) | Acc: (86.00%) (20116/23168)
Epoch: 57 | Batch_idx: 190 |  Loss: (0.3795) |  Loss2: (0.0000) | Acc: (86.00%) (21238/24448)
Epoch: 57 | Batch_idx: 200 |  Loss: (0.3782) |  Loss2: (0.0000) | Acc: (86.00%) (22355/25728)
Epoch: 57 | Batch_idx: 210 |  Loss: (0.3780) |  Loss2: (0.0000) | Acc: (86.00%) (23472/27008)
Epoch: 57 | Batch_idx: 220 |  Loss: (0.3775) |  Loss2: (0.0000) | Acc: (86.00%) (24590/28288)
Epoch: 57 | Batch_idx: 230 |  Loss: (0.3786) |  Loss2: (0.0000) | Acc: (86.00%) (25700/29568)
Epoch: 57 | Batch_idx: 240 |  Loss: (0.3768) |  Loss2: (0.0000) | Acc: (86.00%) (26827/30848)
Epoch: 57 | Batch_idx: 250 |  Loss: (0.3773) |  Loss2: (0.0000) | Acc: (86.00%) (27936/32128)
Epoch: 57 | Batch_idx: 260 |  Loss: (0.3767) |  Loss2: (0.0000) | Acc: (86.00%) (29056/33408)
Epoch: 57 | Batch_idx: 270 |  Loss: (0.3763) |  Loss2: (0.0000) | Acc: (86.00%) (30175/34688)
Epoch: 57 | Batch_idx: 280 |  Loss: (0.3761) |  Loss2: (0.0000) | Acc: (86.00%) (31292/35968)
Epoch: 57 | Batch_idx: 290 |  Loss: (0.3772) |  Loss2: (0.0000) | Acc: (86.00%) (32389/37248)
Epoch: 57 | Batch_idx: 300 |  Loss: (0.3771) |  Loss2: (0.0000) | Acc: (86.00%) (33499/38528)
Epoch: 57 | Batch_idx: 310 |  Loss: (0.3764) |  Loss2: (0.0000) | Acc: (86.00%) (34617/39808)
Epoch: 57 | Batch_idx: 320 |  Loss: (0.3763) |  Loss2: (0.0000) | Acc: (86.00%) (35746/41088)
Epoch: 57 | Batch_idx: 330 |  Loss: (0.3759) |  Loss2: (0.0000) | Acc: (86.00%) (36859/42368)
Epoch: 57 | Batch_idx: 340 |  Loss: (0.3763) |  Loss2: (0.0000) | Acc: (86.00%) (37968/43648)
Epoch: 57 | Batch_idx: 350 |  Loss: (0.3767) |  Loss2: (0.0000) | Acc: (86.00%) (39060/44928)
Epoch: 57 | Batch_idx: 360 |  Loss: (0.3764) |  Loss2: (0.0000) | Acc: (86.00%) (40168/46208)
Epoch: 57 | Batch_idx: 370 |  Loss: (0.3765) |  Loss2: (0.0000) | Acc: (86.00%) (41284/47488)
Epoch: 57 | Batch_idx: 380 |  Loss: (0.3758) |  Loss2: (0.0000) | Acc: (86.00%) (42416/48768)
Epoch: 57 | Batch_idx: 390 |  Loss: (0.3756) |  Loss2: (0.0000) | Acc: (86.00%) (43496/50000)
# TEST : Loss: (0.4735) | Acc: (84.00%) (8401/10000)
percent tensor([0.5074, 0.4926], device='cuda:0')
percent tensor([0.4839, 0.5161], device='cuda:0')
percent tensor([0.4843, 0.5157], device='cuda:0')
percent tensor([0.5659, 0.4341], device='cuda:0')
percent tensor([0.6174, 0.3826], device='cuda:0')
percent tensor([0.5992, 0.4008], device='cuda:0')
percent tensor([0.6587, 0.3413], device='cuda:0')
percent tensor([0.9961, 0.0039], device='cuda:0')
Epoch: 58 | Batch_idx: 0 |  Loss: (0.3419) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 58 | Batch_idx: 10 |  Loss: (0.3918) |  Loss2: (0.0000) | Acc: (87.00%) (1225/1408)
Epoch: 58 | Batch_idx: 20 |  Loss: (0.3712) |  Loss2: (0.0000) | Acc: (87.00%) (2347/2688)
Epoch: 58 | Batch_idx: 30 |  Loss: (0.3759) |  Loss2: (0.0000) | Acc: (86.00%) (3447/3968)
Epoch: 58 | Batch_idx: 40 |  Loss: (0.3787) |  Loss2: (0.0000) | Acc: (86.00%) (4555/5248)
Epoch: 58 | Batch_idx: 50 |  Loss: (0.3816) |  Loss2: (0.0000) | Acc: (86.00%) (5663/6528)
Epoch: 58 | Batch_idx: 60 |  Loss: (0.3789) |  Loss2: (0.0000) | Acc: (86.00%) (6788/7808)
Epoch: 58 | Batch_idx: 70 |  Loss: (0.3764) |  Loss2: (0.0000) | Acc: (87.00%) (7916/9088)
Epoch: 58 | Batch_idx: 80 |  Loss: (0.3770) |  Loss2: (0.0000) | Acc: (87.00%) (9025/10368)
Epoch: 58 | Batch_idx: 90 |  Loss: (0.3738) |  Loss2: (0.0000) | Acc: (87.00%) (10144/11648)
Epoch: 58 | Batch_idx: 100 |  Loss: (0.3745) |  Loss2: (0.0000) | Acc: (87.00%) (11275/12928)
Epoch: 58 | Batch_idx: 110 |  Loss: (0.3717) |  Loss2: (0.0000) | Acc: (87.00%) (12397/14208)
Epoch: 58 | Batch_idx: 120 |  Loss: (0.3728) |  Loss2: (0.0000) | Acc: (87.00%) (13512/15488)
Epoch: 58 | Batch_idx: 130 |  Loss: (0.3704) |  Loss2: (0.0000) | Acc: (87.00%) (14650/16768)
Epoch: 58 | Batch_idx: 140 |  Loss: (0.3686) |  Loss2: (0.0000) | Acc: (87.00%) (15781/18048)
Epoch: 58 | Batch_idx: 150 |  Loss: (0.3698) |  Loss2: (0.0000) | Acc: (87.00%) (16884/19328)
Epoch: 58 | Batch_idx: 160 |  Loss: (0.3717) |  Loss2: (0.0000) | Acc: (87.00%) (18004/20608)
Epoch: 58 | Batch_idx: 170 |  Loss: (0.3705) |  Loss2: (0.0000) | Acc: (87.00%) (19127/21888)
Epoch: 58 | Batch_idx: 180 |  Loss: (0.3701) |  Loss2: (0.0000) | Acc: (87.00%) (20247/23168)
Epoch: 58 | Batch_idx: 190 |  Loss: (0.3687) |  Loss2: (0.0000) | Acc: (87.00%) (21374/24448)
Epoch: 58 | Batch_idx: 200 |  Loss: (0.3688) |  Loss2: (0.0000) | Acc: (87.00%) (22485/25728)
Epoch: 58 | Batch_idx: 210 |  Loss: (0.3675) |  Loss2: (0.0000) | Acc: (87.00%) (23605/27008)
Epoch: 58 | Batch_idx: 220 |  Loss: (0.3690) |  Loss2: (0.0000) | Acc: (87.00%) (24715/28288)
Epoch: 58 | Batch_idx: 230 |  Loss: (0.3689) |  Loss2: (0.0000) | Acc: (87.00%) (25834/29568)
Epoch: 58 | Batch_idx: 240 |  Loss: (0.3680) |  Loss2: (0.0000) | Acc: (87.00%) (26948/30848)
Epoch: 58 | Batch_idx: 250 |  Loss: (0.3694) |  Loss2: (0.0000) | Acc: (87.00%) (28059/32128)
Epoch: 58 | Batch_idx: 260 |  Loss: (0.3683) |  Loss2: (0.0000) | Acc: (87.00%) (29196/33408)
Epoch: 58 | Batch_idx: 270 |  Loss: (0.3674) |  Loss2: (0.0000) | Acc: (87.00%) (30327/34688)
Epoch: 58 | Batch_idx: 280 |  Loss: (0.3669) |  Loss2: (0.0000) | Acc: (87.00%) (31451/35968)
Epoch: 58 | Batch_idx: 290 |  Loss: (0.3668) |  Loss2: (0.0000) | Acc: (87.00%) (32570/37248)
Epoch: 58 | Batch_idx: 300 |  Loss: (0.3658) |  Loss2: (0.0000) | Acc: (87.00%) (33704/38528)
Epoch: 58 | Batch_idx: 310 |  Loss: (0.3668) |  Loss2: (0.0000) | Acc: (87.00%) (34814/39808)
Epoch: 58 | Batch_idx: 320 |  Loss: (0.3669) |  Loss2: (0.0000) | Acc: (87.00%) (35923/41088)
Epoch: 58 | Batch_idx: 330 |  Loss: (0.3665) |  Loss2: (0.0000) | Acc: (87.00%) (37047/42368)
Epoch: 58 | Batch_idx: 340 |  Loss: (0.3663) |  Loss2: (0.0000) | Acc: (87.00%) (38178/43648)
Epoch: 58 | Batch_idx: 350 |  Loss: (0.3671) |  Loss2: (0.0000) | Acc: (87.00%) (39287/44928)
Epoch: 58 | Batch_idx: 360 |  Loss: (0.3676) |  Loss2: (0.0000) | Acc: (87.00%) (40401/46208)
Epoch: 58 | Batch_idx: 370 |  Loss: (0.3672) |  Loss2: (0.0000) | Acc: (87.00%) (41534/47488)
Epoch: 58 | Batch_idx: 380 |  Loss: (0.3666) |  Loss2: (0.0000) | Acc: (87.00%) (42659/48768)
Epoch: 58 | Batch_idx: 390 |  Loss: (0.3662) |  Loss2: (0.0000) | Acc: (87.00%) (43742/50000)
# TEST : Loss: (0.4650) | Acc: (84.00%) (8442/10000)
percent tensor([0.5076, 0.4924], device='cuda:0')
percent tensor([0.4856, 0.5144], device='cuda:0')
percent tensor([0.4868, 0.5132], device='cuda:0')
percent tensor([0.5676, 0.4324], device='cuda:0')
percent tensor([0.6143, 0.3857], device='cuda:0')
percent tensor([0.6076, 0.3924], device='cuda:0')
percent tensor([0.6618, 0.3382], device='cuda:0')
percent tensor([0.9966, 0.0034], device='cuda:0')
Epoch: 59 | Batch_idx: 0 |  Loss: (0.4850) |  Loss2: (0.0000) | Acc: (83.00%) (107/128)
Epoch: 59 | Batch_idx: 10 |  Loss: (0.3959) |  Loss2: (0.0000) | Acc: (86.00%) (1222/1408)
Epoch: 59 | Batch_idx: 20 |  Loss: (0.3784) |  Loss2: (0.0000) | Acc: (87.00%) (2359/2688)
Epoch: 59 | Batch_idx: 30 |  Loss: (0.3716) |  Loss2: (0.0000) | Acc: (88.00%) (3496/3968)
Epoch: 59 | Batch_idx: 40 |  Loss: (0.3718) |  Loss2: (0.0000) | Acc: (87.00%) (4612/5248)
Epoch: 59 | Batch_idx: 50 |  Loss: (0.3694) |  Loss2: (0.0000) | Acc: (87.00%) (5728/6528)
Epoch: 59 | Batch_idx: 60 |  Loss: (0.3675) |  Loss2: (0.0000) | Acc: (87.00%) (6857/7808)
Epoch: 59 | Batch_idx: 70 |  Loss: (0.3627) |  Loss2: (0.0000) | Acc: (88.00%) (7999/9088)
Epoch: 59 | Batch_idx: 80 |  Loss: (0.3596) |  Loss2: (0.0000) | Acc: (88.00%) (9139/10368)
Epoch: 59 | Batch_idx: 90 |  Loss: (0.3612) |  Loss2: (0.0000) | Acc: (88.00%) (10252/11648)
Epoch: 59 | Batch_idx: 100 |  Loss: (0.3581) |  Loss2: (0.0000) | Acc: (88.00%) (11386/12928)
Epoch: 59 | Batch_idx: 110 |  Loss: (0.3533) |  Loss2: (0.0000) | Acc: (88.00%) (12534/14208)
Epoch: 59 | Batch_idx: 120 |  Loss: (0.3546) |  Loss2: (0.0000) | Acc: (88.00%) (13661/15488)
Epoch: 59 | Batch_idx: 130 |  Loss: (0.3531) |  Loss2: (0.0000) | Acc: (88.00%) (14797/16768)
Epoch: 59 | Batch_idx: 140 |  Loss: (0.3580) |  Loss2: (0.0000) | Acc: (88.00%) (15896/18048)
Epoch: 59 | Batch_idx: 150 |  Loss: (0.3566) |  Loss2: (0.0000) | Acc: (88.00%) (17023/19328)
Epoch: 59 | Batch_idx: 160 |  Loss: (0.3570) |  Loss2: (0.0000) | Acc: (88.00%) (18143/20608)
Epoch: 59 | Batch_idx: 170 |  Loss: (0.3574) |  Loss2: (0.0000) | Acc: (88.00%) (19264/21888)
Epoch: 59 | Batch_idx: 180 |  Loss: (0.3562) |  Loss2: (0.0000) | Acc: (88.00%) (20397/23168)
Epoch: 59 | Batch_idx: 190 |  Loss: (0.3572) |  Loss2: (0.0000) | Acc: (87.00%) (21511/24448)
Epoch: 59 | Batch_idx: 200 |  Loss: (0.3576) |  Loss2: (0.0000) | Acc: (87.00%) (22629/25728)
Epoch: 59 | Batch_idx: 210 |  Loss: (0.3579) |  Loss2: (0.0000) | Acc: (87.00%) (23746/27008)
Epoch: 59 | Batch_idx: 220 |  Loss: (0.3568) |  Loss2: (0.0000) | Acc: (87.00%) (24877/28288)
Epoch: 59 | Batch_idx: 230 |  Loss: (0.3565) |  Loss2: (0.0000) | Acc: (87.00%) (26001/29568)
Epoch: 59 | Batch_idx: 240 |  Loss: (0.3566) |  Loss2: (0.0000) | Acc: (87.00%) (27130/30848)
Epoch: 59 | Batch_idx: 250 |  Loss: (0.3569) |  Loss2: (0.0000) | Acc: (87.00%) (28230/32128)
Epoch: 59 | Batch_idx: 260 |  Loss: (0.3591) |  Loss2: (0.0000) | Acc: (87.00%) (29324/33408)
Epoch: 59 | Batch_idx: 270 |  Loss: (0.3585) |  Loss2: (0.0000) | Acc: (87.00%) (30459/34688)
Epoch: 59 | Batch_idx: 280 |  Loss: (0.3580) |  Loss2: (0.0000) | Acc: (87.00%) (31590/35968)
Epoch: 59 | Batch_idx: 290 |  Loss: (0.3584) |  Loss2: (0.0000) | Acc: (87.00%) (32712/37248)
Epoch: 59 | Batch_idx: 300 |  Loss: (0.3581) |  Loss2: (0.0000) | Acc: (87.00%) (33845/38528)
Epoch: 59 | Batch_idx: 310 |  Loss: (0.3583) |  Loss2: (0.0000) | Acc: (87.00%) (34954/39808)
Epoch: 59 | Batch_idx: 320 |  Loss: (0.3582) |  Loss2: (0.0000) | Acc: (87.00%) (36075/41088)
Epoch: 59 | Batch_idx: 330 |  Loss: (0.3577) |  Loss2: (0.0000) | Acc: (87.00%) (37203/42368)
Epoch: 59 | Batch_idx: 340 |  Loss: (0.3568) |  Loss2: (0.0000) | Acc: (87.00%) (38342/43648)
Epoch: 59 | Batch_idx: 350 |  Loss: (0.3568) |  Loss2: (0.0000) | Acc: (87.00%) (39474/44928)
Epoch: 59 | Batch_idx: 360 |  Loss: (0.3570) |  Loss2: (0.0000) | Acc: (87.00%) (40586/46208)
Epoch: 59 | Batch_idx: 370 |  Loss: (0.3573) |  Loss2: (0.0000) | Acc: (87.00%) (41693/47488)
Epoch: 59 | Batch_idx: 380 |  Loss: (0.3584) |  Loss2: (0.0000) | Acc: (87.00%) (42800/48768)
Epoch: 59 | Batch_idx: 390 |  Loss: (0.3582) |  Loss2: (0.0000) | Acc: (87.00%) (43878/50000)
# TEST : Loss: (0.4585) | Acc: (84.00%) (8455/10000)
percent tensor([0.5079, 0.4921], device='cuda:0')
percent tensor([0.4868, 0.5132], device='cuda:0')
percent tensor([0.4887, 0.5113], device='cuda:0')
percent tensor([0.5710, 0.4290], device='cuda:0')
percent tensor([0.6146, 0.3854], device='cuda:0')
percent tensor([0.6122, 0.3878], device='cuda:0')
percent tensor([0.6634, 0.3366], device='cuda:0')
percent tensor([0.9968, 0.0032], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 60 | Batch_idx: 0 |  Loss: (0.3673) |  Loss2: (0.0000) | Acc: (86.00%) (111/128)
Epoch: 60 | Batch_idx: 10 |  Loss: (0.3338) |  Loss2: (0.0000) | Acc: (89.00%) (1254/1408)
Epoch: 60 | Batch_idx: 20 |  Loss: (0.3455) |  Loss2: (0.0000) | Acc: (88.00%) (2377/2688)
Epoch: 60 | Batch_idx: 30 |  Loss: (0.3462) |  Loss2: (0.0000) | Acc: (88.00%) (3504/3968)
Epoch: 60 | Batch_idx: 40 |  Loss: (0.3497) |  Loss2: (0.0000) | Acc: (88.00%) (4628/5248)
Epoch: 60 | Batch_idx: 50 |  Loss: (0.3639) |  Loss2: (0.0000) | Acc: (87.00%) (5702/6528)
Epoch: 60 | Batch_idx: 60 |  Loss: (0.3627) |  Loss2: (0.0000) | Acc: (87.00%) (6824/7808)
Epoch: 60 | Batch_idx: 70 |  Loss: (0.3625) |  Loss2: (0.0000) | Acc: (87.00%) (7949/9088)
Epoch: 60 | Batch_idx: 80 |  Loss: (0.3646) |  Loss2: (0.0000) | Acc: (87.00%) (9057/10368)
Epoch: 60 | Batch_idx: 90 |  Loss: (0.3570) |  Loss2: (0.0000) | Acc: (87.00%) (10211/11648)
Epoch: 60 | Batch_idx: 100 |  Loss: (0.3558) |  Loss2: (0.0000) | Acc: (87.00%) (11339/12928)
Epoch: 60 | Batch_idx: 110 |  Loss: (0.3574) |  Loss2: (0.0000) | Acc: (87.00%) (12458/14208)
Epoch: 60 | Batch_idx: 120 |  Loss: (0.3580) |  Loss2: (0.0000) | Acc: (87.00%) (13568/15488)
Epoch: 60 | Batch_idx: 130 |  Loss: (0.3586) |  Loss2: (0.0000) | Acc: (87.00%) (14676/16768)
Epoch: 60 | Batch_idx: 140 |  Loss: (0.3575) |  Loss2: (0.0000) | Acc: (87.00%) (15799/18048)
Epoch: 60 | Batch_idx: 150 |  Loss: (0.3576) |  Loss2: (0.0000) | Acc: (87.00%) (16926/19328)
Epoch: 60 | Batch_idx: 160 |  Loss: (0.3567) |  Loss2: (0.0000) | Acc: (87.00%) (18065/20608)
Epoch: 60 | Batch_idx: 170 |  Loss: (0.3570) |  Loss2: (0.0000) | Acc: (87.00%) (19191/21888)
Epoch: 60 | Batch_idx: 180 |  Loss: (0.3584) |  Loss2: (0.0000) | Acc: (87.00%) (20293/23168)
Epoch: 60 | Batch_idx: 190 |  Loss: (0.3589) |  Loss2: (0.0000) | Acc: (87.00%) (21406/24448)
Epoch: 60 | Batch_idx: 200 |  Loss: (0.3597) |  Loss2: (0.0000) | Acc: (87.00%) (22510/25728)
Epoch: 60 | Batch_idx: 210 |  Loss: (0.3604) |  Loss2: (0.0000) | Acc: (87.00%) (23623/27008)
Epoch: 60 | Batch_idx: 220 |  Loss: (0.3602) |  Loss2: (0.0000) | Acc: (87.00%) (24761/28288)
Epoch: 60 | Batch_idx: 230 |  Loss: (0.3587) |  Loss2: (0.0000) | Acc: (87.00%) (25901/29568)
Epoch: 60 | Batch_idx: 240 |  Loss: (0.3587) |  Loss2: (0.0000) | Acc: (87.00%) (27026/30848)
Epoch: 60 | Batch_idx: 250 |  Loss: (0.3586) |  Loss2: (0.0000) | Acc: (87.00%) (28153/32128)
Epoch: 60 | Batch_idx: 260 |  Loss: (0.3589) |  Loss2: (0.0000) | Acc: (87.00%) (29273/33408)
Epoch: 60 | Batch_idx: 270 |  Loss: (0.3599) |  Loss2: (0.0000) | Acc: (87.00%) (30387/34688)
Epoch: 60 | Batch_idx: 280 |  Loss: (0.3603) |  Loss2: (0.0000) | Acc: (87.00%) (31510/35968)
Epoch: 60 | Batch_idx: 290 |  Loss: (0.3618) |  Loss2: (0.0000) | Acc: (87.00%) (32619/37248)
Epoch: 60 | Batch_idx: 300 |  Loss: (0.3622) |  Loss2: (0.0000) | Acc: (87.00%) (33735/38528)
Epoch: 60 | Batch_idx: 310 |  Loss: (0.3625) |  Loss2: (0.0000) | Acc: (87.00%) (34854/39808)
Epoch: 60 | Batch_idx: 320 |  Loss: (0.3633) |  Loss2: (0.0000) | Acc: (87.00%) (35955/41088)
Epoch: 60 | Batch_idx: 330 |  Loss: (0.3645) |  Loss2: (0.0000) | Acc: (87.00%) (37072/42368)
Epoch: 60 | Batch_idx: 340 |  Loss: (0.3643) |  Loss2: (0.0000) | Acc: (87.00%) (38209/43648)
Epoch: 60 | Batch_idx: 350 |  Loss: (0.3638) |  Loss2: (0.0000) | Acc: (87.00%) (39337/44928)
Epoch: 60 | Batch_idx: 360 |  Loss: (0.3637) |  Loss2: (0.0000) | Acc: (87.00%) (40445/46208)
Epoch: 60 | Batch_idx: 370 |  Loss: (0.3629) |  Loss2: (0.0000) | Acc: (87.00%) (41572/47488)
Epoch: 60 | Batch_idx: 380 |  Loss: (0.3628) |  Loss2: (0.0000) | Acc: (87.00%) (42693/48768)
Epoch: 60 | Batch_idx: 390 |  Loss: (0.3628) |  Loss2: (0.0000) | Acc: (87.00%) (43774/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_060.pth.tar'
# TEST : Loss: (0.5515) | Acc: (82.00%) (8251/10000)
percent tensor([0.5076, 0.4924], device='cuda:0')
percent tensor([0.4869, 0.5131], device='cuda:0')
percent tensor([0.4882, 0.5118], device='cuda:0')
percent tensor([0.5726, 0.4274], device='cuda:0')
percent tensor([0.6137, 0.3863], device='cuda:0')
percent tensor([0.6144, 0.3856], device='cuda:0')
percent tensor([0.6599, 0.3401], device='cuda:0')
percent tensor([0.9967, 0.0033], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(176.9794, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(807.7946, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(798.6042, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1517.4009, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(510.3359, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2207.1226, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4283.6123, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1422.1923, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6112.9932, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12015.9512, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4001.2571, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16911.5840, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 61 | Batch_idx: 0 |  Loss: (0.3929) |  Loss2: (0.0000) | Acc: (85.00%) (110/128)
Epoch: 61 | Batch_idx: 10 |  Loss: (0.3479) |  Loss2: (0.0000) | Acc: (88.00%) (1243/1408)
Epoch: 61 | Batch_idx: 20 |  Loss: (0.3392) |  Loss2: (0.0000) | Acc: (88.00%) (2381/2688)
Epoch: 61 | Batch_idx: 30 |  Loss: (0.3419) |  Loss2: (0.0000) | Acc: (88.00%) (3493/3968)
Epoch: 61 | Batch_idx: 40 |  Loss: (0.3436) |  Loss2: (0.0000) | Acc: (88.00%) (4621/5248)
Epoch: 61 | Batch_idx: 50 |  Loss: (0.3361) |  Loss2: (0.0000) | Acc: (88.00%) (5765/6528)
Epoch: 61 | Batch_idx: 60 |  Loss: (0.3287) |  Loss2: (0.0000) | Acc: (88.00%) (6925/7808)
Epoch: 61 | Batch_idx: 70 |  Loss: (0.3312) |  Loss2: (0.0000) | Acc: (88.00%) (8059/9088)
Epoch: 61 | Batch_idx: 80 |  Loss: (0.3312) |  Loss2: (0.0000) | Acc: (88.00%) (9186/10368)
Epoch: 61 | Batch_idx: 90 |  Loss: (0.3340) |  Loss2: (0.0000) | Acc: (88.00%) (10302/11648)
Epoch: 61 | Batch_idx: 100 |  Loss: (0.3390) |  Loss2: (0.0000) | Acc: (88.00%) (11410/12928)
Epoch: 61 | Batch_idx: 110 |  Loss: (0.3399) |  Loss2: (0.0000) | Acc: (88.00%) (12547/14208)
Epoch: 61 | Batch_idx: 120 |  Loss: (0.3400) |  Loss2: (0.0000) | Acc: (88.00%) (13664/15488)
Epoch: 61 | Batch_idx: 130 |  Loss: (0.3408) |  Loss2: (0.0000) | Acc: (88.00%) (14789/16768)
Epoch: 61 | Batch_idx: 140 |  Loss: (0.3434) |  Loss2: (0.0000) | Acc: (88.00%) (15915/18048)
Epoch: 61 | Batch_idx: 150 |  Loss: (0.3422) |  Loss2: (0.0000) | Acc: (88.00%) (17047/19328)
Epoch: 61 | Batch_idx: 160 |  Loss: (0.3420) |  Loss2: (0.0000) | Acc: (88.00%) (18171/20608)
Epoch: 61 | Batch_idx: 170 |  Loss: (0.3418) |  Loss2: (0.0000) | Acc: (88.00%) (19304/21888)
Epoch: 61 | Batch_idx: 180 |  Loss: (0.3429) |  Loss2: (0.0000) | Acc: (88.00%) (20420/23168)
Epoch: 61 | Batch_idx: 190 |  Loss: (0.3465) |  Loss2: (0.0000) | Acc: (87.00%) (21504/24448)
Epoch: 61 | Batch_idx: 200 |  Loss: (0.3465) |  Loss2: (0.0000) | Acc: (87.00%) (22631/25728)
Epoch: 61 | Batch_idx: 210 |  Loss: (0.3456) |  Loss2: (0.0000) | Acc: (87.00%) (23763/27008)
Epoch: 61 | Batch_idx: 220 |  Loss: (0.3452) |  Loss2: (0.0000) | Acc: (88.00%) (24899/28288)
Epoch: 61 | Batch_idx: 230 |  Loss: (0.3450) |  Loss2: (0.0000) | Acc: (88.00%) (26023/29568)
Epoch: 61 | Batch_idx: 240 |  Loss: (0.3449) |  Loss2: (0.0000) | Acc: (88.00%) (27152/30848)
Epoch: 61 | Batch_idx: 250 |  Loss: (0.3462) |  Loss2: (0.0000) | Acc: (87.00%) (28271/32128)
Epoch: 61 | Batch_idx: 260 |  Loss: (0.3459) |  Loss2: (0.0000) | Acc: (88.00%) (29408/33408)
Epoch: 61 | Batch_idx: 270 |  Loss: (0.3452) |  Loss2: (0.0000) | Acc: (88.00%) (30530/34688)
Epoch: 61 | Batch_idx: 280 |  Loss: (0.3457) |  Loss2: (0.0000) | Acc: (87.00%) (31649/35968)
Epoch: 61 | Batch_idx: 290 |  Loss: (0.3462) |  Loss2: (0.0000) | Acc: (87.00%) (32775/37248)
Epoch: 61 | Batch_idx: 300 |  Loss: (0.3453) |  Loss2: (0.0000) | Acc: (88.00%) (33910/38528)
Epoch: 61 | Batch_idx: 310 |  Loss: (0.3448) |  Loss2: (0.0000) | Acc: (88.00%) (35049/39808)
Epoch: 61 | Batch_idx: 320 |  Loss: (0.3445) |  Loss2: (0.0000) | Acc: (88.00%) (36186/41088)
Epoch: 61 | Batch_idx: 330 |  Loss: (0.3457) |  Loss2: (0.0000) | Acc: (88.00%) (37300/42368)
Epoch: 61 | Batch_idx: 340 |  Loss: (0.3450) |  Loss2: (0.0000) | Acc: (88.00%) (38440/43648)
Epoch: 61 | Batch_idx: 350 |  Loss: (0.3460) |  Loss2: (0.0000) | Acc: (88.00%) (39552/44928)
Epoch: 61 | Batch_idx: 360 |  Loss: (0.3466) |  Loss2: (0.0000) | Acc: (87.00%) (40662/46208)
Epoch: 61 | Batch_idx: 370 |  Loss: (0.3473) |  Loss2: (0.0000) | Acc: (87.00%) (41772/47488)
Epoch: 61 | Batch_idx: 380 |  Loss: (0.3470) |  Loss2: (0.0000) | Acc: (87.00%) (42900/48768)
Epoch: 61 | Batch_idx: 390 |  Loss: (0.3468) |  Loss2: (0.0000) | Acc: (87.00%) (43978/50000)
# TEST : Loss: (0.5025) | Acc: (83.00%) (8399/10000)
percent tensor([0.5076, 0.4924], device='cuda:0')
percent tensor([0.4871, 0.5129], device='cuda:0')
percent tensor([0.4885, 0.5115], device='cuda:0')
percent tensor([0.5721, 0.4279], device='cuda:0')
percent tensor([0.6172, 0.3828], device='cuda:0')
percent tensor([0.6138, 0.3862], device='cuda:0')
percent tensor([0.6654, 0.3346], device='cuda:0')
percent tensor([0.9974, 0.0026], device='cuda:0')
Epoch: 62 | Batch_idx: 0 |  Loss: (0.3637) |  Loss2: (0.0000) | Acc: (85.00%) (110/128)
Epoch: 62 | Batch_idx: 10 |  Loss: (0.2850) |  Loss2: (0.0000) | Acc: (91.00%) (1283/1408)
Epoch: 62 | Batch_idx: 20 |  Loss: (0.2959) |  Loss2: (0.0000) | Acc: (89.00%) (2416/2688)
Epoch: 62 | Batch_idx: 30 |  Loss: (0.2875) |  Loss2: (0.0000) | Acc: (90.00%) (3577/3968)
Epoch: 62 | Batch_idx: 40 |  Loss: (0.3043) |  Loss2: (0.0000) | Acc: (89.00%) (4698/5248)
Epoch: 62 | Batch_idx: 50 |  Loss: (0.3011) |  Loss2: (0.0000) | Acc: (89.00%) (5856/6528)
Epoch: 62 | Batch_idx: 60 |  Loss: (0.3111) |  Loss2: (0.0000) | Acc: (89.00%) (6984/7808)
Epoch: 62 | Batch_idx: 70 |  Loss: (0.3209) |  Loss2: (0.0000) | Acc: (89.00%) (8091/9088)
Epoch: 62 | Batch_idx: 80 |  Loss: (0.3252) |  Loss2: (0.0000) | Acc: (88.00%) (9208/10368)
Epoch: 62 | Batch_idx: 90 |  Loss: (0.3245) |  Loss2: (0.0000) | Acc: (88.00%) (10351/11648)
Epoch: 62 | Batch_idx: 100 |  Loss: (0.3311) |  Loss2: (0.0000) | Acc: (88.00%) (11461/12928)
Epoch: 62 | Batch_idx: 110 |  Loss: (0.3329) |  Loss2: (0.0000) | Acc: (88.00%) (12582/14208)
Epoch: 62 | Batch_idx: 120 |  Loss: (0.3323) |  Loss2: (0.0000) | Acc: (88.00%) (13715/15488)
Epoch: 62 | Batch_idx: 130 |  Loss: (0.3313) |  Loss2: (0.0000) | Acc: (88.00%) (14855/16768)
Epoch: 62 | Batch_idx: 140 |  Loss: (0.3302) |  Loss2: (0.0000) | Acc: (88.00%) (15982/18048)
Epoch: 62 | Batch_idx: 150 |  Loss: (0.3301) |  Loss2: (0.0000) | Acc: (88.00%) (17109/19328)
Epoch: 62 | Batch_idx: 160 |  Loss: (0.3303) |  Loss2: (0.0000) | Acc: (88.00%) (18237/20608)
Epoch: 62 | Batch_idx: 170 |  Loss: (0.3316) |  Loss2: (0.0000) | Acc: (88.00%) (19364/21888)
Epoch: 62 | Batch_idx: 180 |  Loss: (0.3316) |  Loss2: (0.0000) | Acc: (88.00%) (20488/23168)
Epoch: 62 | Batch_idx: 190 |  Loss: (0.3331) |  Loss2: (0.0000) | Acc: (88.00%) (21601/24448)
Epoch: 62 | Batch_idx: 200 |  Loss: (0.3356) |  Loss2: (0.0000) | Acc: (88.00%) (22707/25728)
Epoch: 62 | Batch_idx: 210 |  Loss: (0.3377) |  Loss2: (0.0000) | Acc: (88.00%) (23816/27008)
Epoch: 62 | Batch_idx: 220 |  Loss: (0.3383) |  Loss2: (0.0000) | Acc: (88.00%) (24941/28288)
Epoch: 62 | Batch_idx: 230 |  Loss: (0.3382) |  Loss2: (0.0000) | Acc: (88.00%) (26076/29568)
Epoch: 62 | Batch_idx: 240 |  Loss: (0.3389) |  Loss2: (0.0000) | Acc: (88.00%) (27209/30848)
Epoch: 62 | Batch_idx: 250 |  Loss: (0.3381) |  Loss2: (0.0000) | Acc: (88.00%) (28343/32128)
Epoch: 62 | Batch_idx: 260 |  Loss: (0.3383) |  Loss2: (0.0000) | Acc: (88.00%) (29483/33408)
Epoch: 62 | Batch_idx: 270 |  Loss: (0.3385) |  Loss2: (0.0000) | Acc: (88.00%) (30613/34688)
Epoch: 62 | Batch_idx: 280 |  Loss: (0.3403) |  Loss2: (0.0000) | Acc: (88.00%) (31721/35968)
Epoch: 62 | Batch_idx: 290 |  Loss: (0.3410) |  Loss2: (0.0000) | Acc: (88.00%) (32840/37248)
Epoch: 62 | Batch_idx: 300 |  Loss: (0.3419) |  Loss2: (0.0000) | Acc: (88.00%) (33946/38528)
Epoch: 62 | Batch_idx: 310 |  Loss: (0.3415) |  Loss2: (0.0000) | Acc: (88.00%) (35073/39808)
Epoch: 62 | Batch_idx: 320 |  Loss: (0.3416) |  Loss2: (0.0000) | Acc: (88.00%) (36196/41088)
Epoch: 62 | Batch_idx: 330 |  Loss: (0.3417) |  Loss2: (0.0000) | Acc: (88.00%) (37327/42368)
Epoch: 62 | Batch_idx: 340 |  Loss: (0.3422) |  Loss2: (0.0000) | Acc: (88.00%) (38459/43648)
Epoch: 62 | Batch_idx: 350 |  Loss: (0.3418) |  Loss2: (0.0000) | Acc: (88.00%) (39592/44928)
Epoch: 62 | Batch_idx: 360 |  Loss: (0.3413) |  Loss2: (0.0000) | Acc: (88.00%) (40734/46208)
Epoch: 62 | Batch_idx: 370 |  Loss: (0.3405) |  Loss2: (0.0000) | Acc: (88.00%) (41885/47488)
Epoch: 62 | Batch_idx: 380 |  Loss: (0.3400) |  Loss2: (0.0000) | Acc: (88.00%) (43021/48768)
Epoch: 62 | Batch_idx: 390 |  Loss: (0.3399) |  Loss2: (0.0000) | Acc: (88.00%) (44106/50000)
# TEST : Loss: (0.4854) | Acc: (84.00%) (8451/10000)
percent tensor([0.5076, 0.4924], device='cuda:0')
percent tensor([0.4870, 0.5130], device='cuda:0')
percent tensor([0.4888, 0.5112], device='cuda:0')
percent tensor([0.5713, 0.4287], device='cuda:0')
percent tensor([0.6145, 0.3855], device='cuda:0')
percent tensor([0.6145, 0.3855], device='cuda:0')
percent tensor([0.6621, 0.3379], device='cuda:0')
percent tensor([0.9970, 0.0030], device='cuda:0')
Epoch: 63 | Batch_idx: 0 |  Loss: (0.3452) |  Loss2: (0.0000) | Acc: (87.00%) (112/128)
Epoch: 63 | Batch_idx: 10 |  Loss: (0.3282) |  Loss2: (0.0000) | Acc: (88.00%) (1249/1408)
Epoch: 63 | Batch_idx: 20 |  Loss: (0.3419) |  Loss2: (0.0000) | Acc: (88.00%) (2381/2688)
Epoch: 63 | Batch_idx: 30 |  Loss: (0.3246) |  Loss2: (0.0000) | Acc: (89.00%) (3539/3968)
Epoch: 63 | Batch_idx: 40 |  Loss: (0.3246) |  Loss2: (0.0000) | Acc: (89.00%) (4685/5248)
Epoch: 63 | Batch_idx: 50 |  Loss: (0.3232) |  Loss2: (0.0000) | Acc: (89.00%) (5826/6528)
Epoch: 63 | Batch_idx: 60 |  Loss: (0.3230) |  Loss2: (0.0000) | Acc: (89.00%) (6963/7808)
Epoch: 63 | Batch_idx: 70 |  Loss: (0.3253) |  Loss2: (0.0000) | Acc: (89.00%) (8092/9088)
Epoch: 63 | Batch_idx: 80 |  Loss: (0.3232) |  Loss2: (0.0000) | Acc: (89.00%) (9233/10368)
Epoch: 63 | Batch_idx: 90 |  Loss: (0.3242) |  Loss2: (0.0000) | Acc: (89.00%) (10368/11648)
Epoch: 63 | Batch_idx: 100 |  Loss: (0.3214) |  Loss2: (0.0000) | Acc: (89.00%) (11517/12928)
Epoch: 63 | Batch_idx: 110 |  Loss: (0.3215) |  Loss2: (0.0000) | Acc: (89.00%) (12660/14208)
Epoch: 63 | Batch_idx: 120 |  Loss: (0.3237) |  Loss2: (0.0000) | Acc: (89.00%) (13785/15488)
Epoch: 63 | Batch_idx: 130 |  Loss: (0.3220) |  Loss2: (0.0000) | Acc: (89.00%) (14924/16768)
Epoch: 63 | Batch_idx: 140 |  Loss: (0.3228) |  Loss2: (0.0000) | Acc: (89.00%) (16063/18048)
Epoch: 63 | Batch_idx: 150 |  Loss: (0.3250) |  Loss2: (0.0000) | Acc: (88.00%) (17178/19328)
Epoch: 63 | Batch_idx: 160 |  Loss: (0.3262) |  Loss2: (0.0000) | Acc: (88.00%) (18305/20608)
Epoch: 63 | Batch_idx: 170 |  Loss: (0.3270) |  Loss2: (0.0000) | Acc: (88.00%) (19440/21888)
Epoch: 63 | Batch_idx: 180 |  Loss: (0.3272) |  Loss2: (0.0000) | Acc: (88.00%) (20580/23168)
Epoch: 63 | Batch_idx: 190 |  Loss: (0.3262) |  Loss2: (0.0000) | Acc: (88.00%) (21724/24448)
Epoch: 63 | Batch_idx: 200 |  Loss: (0.3266) |  Loss2: (0.0000) | Acc: (88.00%) (22853/25728)
Epoch: 63 | Batch_idx: 210 |  Loss: (0.3256) |  Loss2: (0.0000) | Acc: (88.00%) (24001/27008)
Epoch: 63 | Batch_idx: 220 |  Loss: (0.3269) |  Loss2: (0.0000) | Acc: (88.00%) (25128/28288)
Epoch: 63 | Batch_idx: 230 |  Loss: (0.3273) |  Loss2: (0.0000) | Acc: (88.00%) (26264/29568)
Epoch: 63 | Batch_idx: 240 |  Loss: (0.3268) |  Loss2: (0.0000) | Acc: (88.00%) (27408/30848)
Epoch: 63 | Batch_idx: 250 |  Loss: (0.3275) |  Loss2: (0.0000) | Acc: (88.00%) (28539/32128)
Epoch: 63 | Batch_idx: 260 |  Loss: (0.3279) |  Loss2: (0.0000) | Acc: (88.00%) (29677/33408)
Epoch: 63 | Batch_idx: 270 |  Loss: (0.3275) |  Loss2: (0.0000) | Acc: (88.00%) (30833/34688)
Epoch: 63 | Batch_idx: 280 |  Loss: (0.3277) |  Loss2: (0.0000) | Acc: (88.00%) (31964/35968)
Epoch: 63 | Batch_idx: 290 |  Loss: (0.3281) |  Loss2: (0.0000) | Acc: (88.00%) (33099/37248)
Epoch: 63 | Batch_idx: 300 |  Loss: (0.3274) |  Loss2: (0.0000) | Acc: (88.00%) (34240/38528)
Epoch: 63 | Batch_idx: 310 |  Loss: (0.3282) |  Loss2: (0.0000) | Acc: (88.00%) (35372/39808)
Epoch: 63 | Batch_idx: 320 |  Loss: (0.3275) |  Loss2: (0.0000) | Acc: (88.00%) (36521/41088)
Epoch: 63 | Batch_idx: 330 |  Loss: (0.3273) |  Loss2: (0.0000) | Acc: (88.00%) (37657/42368)
Epoch: 63 | Batch_idx: 340 |  Loss: (0.3276) |  Loss2: (0.0000) | Acc: (88.00%) (38797/43648)
Epoch: 63 | Batch_idx: 350 |  Loss: (0.3278) |  Loss2: (0.0000) | Acc: (88.00%) (39931/44928)
Epoch: 63 | Batch_idx: 360 |  Loss: (0.3278) |  Loss2: (0.0000) | Acc: (88.00%) (41050/46208)
Epoch: 63 | Batch_idx: 370 |  Loss: (0.3286) |  Loss2: (0.0000) | Acc: (88.00%) (42165/47488)
Epoch: 63 | Batch_idx: 380 |  Loss: (0.3291) |  Loss2: (0.0000) | Acc: (88.00%) (43289/48768)
Epoch: 63 | Batch_idx: 390 |  Loss: (0.3303) |  Loss2: (0.0000) | Acc: (88.00%) (44372/50000)
# TEST : Loss: (0.4680) | Acc: (84.00%) (8479/10000)
percent tensor([0.5077, 0.4923], device='cuda:0')
percent tensor([0.4871, 0.5129], device='cuda:0')
percent tensor([0.4883, 0.5117], device='cuda:0')
percent tensor([0.5709, 0.4291], device='cuda:0')
percent tensor([0.6144, 0.3856], device='cuda:0')
percent tensor([0.6119, 0.3881], device='cuda:0')
percent tensor([0.6643, 0.3357], device='cuda:0')
percent tensor([0.9967, 0.0033], device='cuda:0')
Epoch: 64 | Batch_idx: 0 |  Loss: (0.4053) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 64 | Batch_idx: 10 |  Loss: (0.3080) |  Loss2: (0.0000) | Acc: (89.00%) (1263/1408)
Epoch: 64 | Batch_idx: 20 |  Loss: (0.3019) |  Loss2: (0.0000) | Acc: (90.00%) (2429/2688)
Epoch: 64 | Batch_idx: 30 |  Loss: (0.3086) |  Loss2: (0.0000) | Acc: (89.00%) (3568/3968)
Epoch: 64 | Batch_idx: 40 |  Loss: (0.3126) |  Loss2: (0.0000) | Acc: (89.00%) (4706/5248)
Epoch: 64 | Batch_idx: 50 |  Loss: (0.3059) |  Loss2: (0.0000) | Acc: (89.00%) (5870/6528)
Epoch: 64 | Batch_idx: 60 |  Loss: (0.3107) |  Loss2: (0.0000) | Acc: (89.00%) (7000/7808)
Epoch: 64 | Batch_idx: 70 |  Loss: (0.3179) |  Loss2: (0.0000) | Acc: (89.00%) (8106/9088)
Epoch: 64 | Batch_idx: 80 |  Loss: (0.3168) |  Loss2: (0.0000) | Acc: (89.00%) (9244/10368)
Epoch: 64 | Batch_idx: 90 |  Loss: (0.3163) |  Loss2: (0.0000) | Acc: (89.00%) (10396/11648)
Epoch: 64 | Batch_idx: 100 |  Loss: (0.3151) |  Loss2: (0.0000) | Acc: (89.00%) (11549/12928)
Epoch: 64 | Batch_idx: 110 |  Loss: (0.3136) |  Loss2: (0.0000) | Acc: (89.00%) (12706/14208)
Epoch: 64 | Batch_idx: 120 |  Loss: (0.3113) |  Loss2: (0.0000) | Acc: (89.00%) (13859/15488)
Epoch: 64 | Batch_idx: 130 |  Loss: (0.3104) |  Loss2: (0.0000) | Acc: (89.00%) (15000/16768)
Epoch: 64 | Batch_idx: 140 |  Loss: (0.3127) |  Loss2: (0.0000) | Acc: (89.00%) (16122/18048)
Epoch: 64 | Batch_idx: 150 |  Loss: (0.3119) |  Loss2: (0.0000) | Acc: (89.00%) (17270/19328)
Epoch: 64 | Batch_idx: 160 |  Loss: (0.3118) |  Loss2: (0.0000) | Acc: (89.00%) (18420/20608)
Epoch: 64 | Batch_idx: 170 |  Loss: (0.3118) |  Loss2: (0.0000) | Acc: (89.00%) (19568/21888)
Epoch: 64 | Batch_idx: 180 |  Loss: (0.3125) |  Loss2: (0.0000) | Acc: (89.00%) (20706/23168)
Epoch: 64 | Batch_idx: 190 |  Loss: (0.3135) |  Loss2: (0.0000) | Acc: (89.00%) (21841/24448)
Epoch: 64 | Batch_idx: 200 |  Loss: (0.3143) |  Loss2: (0.0000) | Acc: (89.00%) (22988/25728)
Epoch: 64 | Batch_idx: 210 |  Loss: (0.3152) |  Loss2: (0.0000) | Acc: (89.00%) (24129/27008)
Epoch: 64 | Batch_idx: 220 |  Loss: (0.3153) |  Loss2: (0.0000) | Acc: (89.00%) (25263/28288)
Epoch: 64 | Batch_idx: 230 |  Loss: (0.3157) |  Loss2: (0.0000) | Acc: (89.00%) (26402/29568)
Epoch: 64 | Batch_idx: 240 |  Loss: (0.3171) |  Loss2: (0.0000) | Acc: (89.00%) (27532/30848)
Epoch: 64 | Batch_idx: 250 |  Loss: (0.3187) |  Loss2: (0.0000) | Acc: (89.00%) (28637/32128)
Epoch: 64 | Batch_idx: 260 |  Loss: (0.3197) |  Loss2: (0.0000) | Acc: (89.00%) (29761/33408)
Epoch: 64 | Batch_idx: 270 |  Loss: (0.3194) |  Loss2: (0.0000) | Acc: (89.00%) (30908/34688)
Epoch: 64 | Batch_idx: 280 |  Loss: (0.3195) |  Loss2: (0.0000) | Acc: (89.00%) (32037/35968)
Epoch: 64 | Batch_idx: 290 |  Loss: (0.3195) |  Loss2: (0.0000) | Acc: (89.00%) (33178/37248)
Epoch: 64 | Batch_idx: 300 |  Loss: (0.3207) |  Loss2: (0.0000) | Acc: (89.00%) (34308/38528)
Epoch: 64 | Batch_idx: 310 |  Loss: (0.3210) |  Loss2: (0.0000) | Acc: (89.00%) (35437/39808)
Epoch: 64 | Batch_idx: 320 |  Loss: (0.3216) |  Loss2: (0.0000) | Acc: (88.00%) (36562/41088)
Epoch: 64 | Batch_idx: 330 |  Loss: (0.3226) |  Loss2: (0.0000) | Acc: (88.00%) (37678/42368)
Epoch: 64 | Batch_idx: 340 |  Loss: (0.3219) |  Loss2: (0.0000) | Acc: (88.00%) (38839/43648)
Epoch: 64 | Batch_idx: 350 |  Loss: (0.3219) |  Loss2: (0.0000) | Acc: (88.00%) (39970/44928)
Epoch: 64 | Batch_idx: 360 |  Loss: (0.3219) |  Loss2: (0.0000) | Acc: (88.00%) (41118/46208)
Epoch: 64 | Batch_idx: 370 |  Loss: (0.3219) |  Loss2: (0.0000) | Acc: (88.00%) (42261/47488)
Epoch: 64 | Batch_idx: 380 |  Loss: (0.3215) |  Loss2: (0.0000) | Acc: (88.00%) (43402/48768)
Epoch: 64 | Batch_idx: 390 |  Loss: (0.3225) |  Loss2: (0.0000) | Acc: (88.00%) (44478/50000)
# TEST : Loss: (0.4971) | Acc: (83.00%) (8359/10000)
percent tensor([0.5076, 0.4924], device='cuda:0')
percent tensor([0.4871, 0.5129], device='cuda:0')
percent tensor([0.4885, 0.5115], device='cuda:0')
percent tensor([0.5721, 0.4279], device='cuda:0')
percent tensor([0.6114, 0.3886], device='cuda:0')
percent tensor([0.6123, 0.3877], device='cuda:0')
percent tensor([0.6604, 0.3396], device='cuda:0')
percent tensor([0.9968, 0.0032], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 65 | Batch_idx: 0 |  Loss: (0.4126) |  Loss2: (0.0000) | Acc: (86.00%) (111/128)
Epoch: 65 | Batch_idx: 10 |  Loss: (0.3823) |  Loss2: (0.0000) | Acc: (87.00%) (1225/1408)
Epoch: 65 | Batch_idx: 20 |  Loss: (0.3721) |  Loss2: (0.0000) | Acc: (87.00%) (2347/2688)
Epoch: 65 | Batch_idx: 30 |  Loss: (0.3858) |  Loss2: (0.0000) | Acc: (86.00%) (3444/3968)
Epoch: 65 | Batch_idx: 40 |  Loss: (0.3897) |  Loss2: (0.0000) | Acc: (86.00%) (4538/5248)
Epoch: 65 | Batch_idx: 50 |  Loss: (0.3935) |  Loss2: (0.0000) | Acc: (86.00%) (5632/6528)
Epoch: 65 | Batch_idx: 60 |  Loss: (0.3910) |  Loss2: (0.0000) | Acc: (86.00%) (6756/7808)
Epoch: 65 | Batch_idx: 70 |  Loss: (0.3894) |  Loss2: (0.0000) | Acc: (86.00%) (7859/9088)
Epoch: 65 | Batch_idx: 80 |  Loss: (0.3911) |  Loss2: (0.0000) | Acc: (86.00%) (8949/10368)
Epoch: 65 | Batch_idx: 90 |  Loss: (0.3933) |  Loss2: (0.0000) | Acc: (86.00%) (10030/11648)
Epoch: 65 | Batch_idx: 100 |  Loss: (0.3962) |  Loss2: (0.0000) | Acc: (85.00%) (11112/12928)
Epoch: 65 | Batch_idx: 110 |  Loss: (0.3962) |  Loss2: (0.0000) | Acc: (86.00%) (12225/14208)
Epoch: 65 | Batch_idx: 120 |  Loss: (0.3968) |  Loss2: (0.0000) | Acc: (85.00%) (13311/15488)
Epoch: 65 | Batch_idx: 130 |  Loss: (0.3942) |  Loss2: (0.0000) | Acc: (85.00%) (14417/16768)
Epoch: 65 | Batch_idx: 140 |  Loss: (0.3947) |  Loss2: (0.0000) | Acc: (85.00%) (15508/18048)
Epoch: 65 | Batch_idx: 150 |  Loss: (0.3928) |  Loss2: (0.0000) | Acc: (86.00%) (16627/19328)
Epoch: 65 | Batch_idx: 160 |  Loss: (0.3928) |  Loss2: (0.0000) | Acc: (86.00%) (17728/20608)
Epoch: 65 | Batch_idx: 170 |  Loss: (0.3968) |  Loss2: (0.0000) | Acc: (85.00%) (18798/21888)
Epoch: 65 | Batch_idx: 180 |  Loss: (0.3957) |  Loss2: (0.0000) | Acc: (85.00%) (19902/23168)
Epoch: 65 | Batch_idx: 190 |  Loss: (0.3936) |  Loss2: (0.0000) | Acc: (86.00%) (21026/24448)
Epoch: 65 | Batch_idx: 200 |  Loss: (0.3948) |  Loss2: (0.0000) | Acc: (86.00%) (22127/25728)
Epoch: 65 | Batch_idx: 210 |  Loss: (0.3954) |  Loss2: (0.0000) | Acc: (86.00%) (23231/27008)
Epoch: 65 | Batch_idx: 220 |  Loss: (0.3952) |  Loss2: (0.0000) | Acc: (86.00%) (24330/28288)
Epoch: 65 | Batch_idx: 230 |  Loss: (0.3939) |  Loss2: (0.0000) | Acc: (86.00%) (25446/29568)
Epoch: 65 | Batch_idx: 240 |  Loss: (0.3924) |  Loss2: (0.0000) | Acc: (86.00%) (26558/30848)
Epoch: 65 | Batch_idx: 250 |  Loss: (0.3917) |  Loss2: (0.0000) | Acc: (86.00%) (27672/32128)
Epoch: 65 | Batch_idx: 260 |  Loss: (0.3920) |  Loss2: (0.0000) | Acc: (86.00%) (28768/33408)
Epoch: 65 | Batch_idx: 270 |  Loss: (0.3904) |  Loss2: (0.0000) | Acc: (86.00%) (29891/34688)
Epoch: 65 | Batch_idx: 280 |  Loss: (0.3916) |  Loss2: (0.0000) | Acc: (86.00%) (30979/35968)
Epoch: 65 | Batch_idx: 290 |  Loss: (0.3908) |  Loss2: (0.0000) | Acc: (86.00%) (32088/37248)
Epoch: 65 | Batch_idx: 300 |  Loss: (0.3901) |  Loss2: (0.0000) | Acc: (86.00%) (33183/38528)
Epoch: 65 | Batch_idx: 310 |  Loss: (0.3903) |  Loss2: (0.0000) | Acc: (86.00%) (34275/39808)
Epoch: 65 | Batch_idx: 320 |  Loss: (0.3890) |  Loss2: (0.0000) | Acc: (86.00%) (35403/41088)
Epoch: 65 | Batch_idx: 330 |  Loss: (0.3896) |  Loss2: (0.0000) | Acc: (86.00%) (36489/42368)
Epoch: 65 | Batch_idx: 340 |  Loss: (0.3887) |  Loss2: (0.0000) | Acc: (86.00%) (37617/43648)
Epoch: 65 | Batch_idx: 350 |  Loss: (0.3872) |  Loss2: (0.0000) | Acc: (86.00%) (38747/44928)
Epoch: 65 | Batch_idx: 360 |  Loss: (0.3865) |  Loss2: (0.0000) | Acc: (86.00%) (39855/46208)
Epoch: 65 | Batch_idx: 370 |  Loss: (0.3852) |  Loss2: (0.0000) | Acc: (86.00%) (40995/47488)
Epoch: 65 | Batch_idx: 380 |  Loss: (0.3841) |  Loss2: (0.0000) | Acc: (86.00%) (42120/48768)
Epoch: 65 | Batch_idx: 390 |  Loss: (0.3839) |  Loss2: (0.0000) | Acc: (86.00%) (43191/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_065.pth.tar'
# TEST : Loss: (0.4795) | Acc: (83.00%) (8392/10000)
percent tensor([0.5121, 0.4879], device='cuda:0')
percent tensor([0.4961, 0.5039], device='cuda:0')
percent tensor([0.4918, 0.5082], device='cuda:0')
percent tensor([0.5751, 0.4249], device='cuda:0')
percent tensor([0.5950, 0.4050], device='cuda:0')
percent tensor([0.6116, 0.3884], device='cuda:0')
percent tensor([0.5960, 0.4040], device='cuda:0')
percent tensor([0.9967, 0.0033], device='cuda:0')
Epoch: 66 | Batch_idx: 0 |  Loss: (0.2937) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 66 | Batch_idx: 10 |  Loss: (0.3235) |  Loss2: (0.0000) | Acc: (88.00%) (1244/1408)
Epoch: 66 | Batch_idx: 20 |  Loss: (0.3620) |  Loss2: (0.0000) | Acc: (86.00%) (2336/2688)
Epoch: 66 | Batch_idx: 30 |  Loss: (0.3666) |  Loss2: (0.0000) | Acc: (87.00%) (3454/3968)
Epoch: 66 | Batch_idx: 40 |  Loss: (0.3556) |  Loss2: (0.0000) | Acc: (87.00%) (4586/5248)
Epoch: 66 | Batch_idx: 50 |  Loss: (0.3502) |  Loss2: (0.0000) | Acc: (87.00%) (5710/6528)
Epoch: 66 | Batch_idx: 60 |  Loss: (0.3480) |  Loss2: (0.0000) | Acc: (87.00%) (6831/7808)
Epoch: 66 | Batch_idx: 70 |  Loss: (0.3503) |  Loss2: (0.0000) | Acc: (87.00%) (7962/9088)
Epoch: 66 | Batch_idx: 80 |  Loss: (0.3541) |  Loss2: (0.0000) | Acc: (87.00%) (9079/10368)
Epoch: 66 | Batch_idx: 90 |  Loss: (0.3516) |  Loss2: (0.0000) | Acc: (87.00%) (10200/11648)
Epoch: 66 | Batch_idx: 100 |  Loss: (0.3548) |  Loss2: (0.0000) | Acc: (87.00%) (11311/12928)
Epoch: 66 | Batch_idx: 110 |  Loss: (0.3529) |  Loss2: (0.0000) | Acc: (87.00%) (12436/14208)
Epoch: 66 | Batch_idx: 120 |  Loss: (0.3520) |  Loss2: (0.0000) | Acc: (87.00%) (13566/15488)
Epoch: 66 | Batch_idx: 130 |  Loss: (0.3510) |  Loss2: (0.0000) | Acc: (87.00%) (14697/16768)
Epoch: 66 | Batch_idx: 140 |  Loss: (0.3513) |  Loss2: (0.0000) | Acc: (87.00%) (15828/18048)
Epoch: 66 | Batch_idx: 150 |  Loss: (0.3501) |  Loss2: (0.0000) | Acc: (87.00%) (16950/19328)
Epoch: 66 | Batch_idx: 160 |  Loss: (0.3517) |  Loss2: (0.0000) | Acc: (87.00%) (18061/20608)
Epoch: 66 | Batch_idx: 170 |  Loss: (0.3495) |  Loss2: (0.0000) | Acc: (87.00%) (19194/21888)
Epoch: 66 | Batch_idx: 180 |  Loss: (0.3510) |  Loss2: (0.0000) | Acc: (87.00%) (20293/23168)
Epoch: 66 | Batch_idx: 190 |  Loss: (0.3498) |  Loss2: (0.0000) | Acc: (87.00%) (21422/24448)
Epoch: 66 | Batch_idx: 200 |  Loss: (0.3500) |  Loss2: (0.0000) | Acc: (87.00%) (22538/25728)
Epoch: 66 | Batch_idx: 210 |  Loss: (0.3514) |  Loss2: (0.0000) | Acc: (87.00%) (23653/27008)
Epoch: 66 | Batch_idx: 220 |  Loss: (0.3519) |  Loss2: (0.0000) | Acc: (87.00%) (24768/28288)
Epoch: 66 | Batch_idx: 230 |  Loss: (0.3533) |  Loss2: (0.0000) | Acc: (87.00%) (25878/29568)
Epoch: 66 | Batch_idx: 240 |  Loss: (0.3528) |  Loss2: (0.0000) | Acc: (87.00%) (27006/30848)
Epoch: 66 | Batch_idx: 250 |  Loss: (0.3530) |  Loss2: (0.0000) | Acc: (87.00%) (28132/32128)
Epoch: 66 | Batch_idx: 260 |  Loss: (0.3534) |  Loss2: (0.0000) | Acc: (87.00%) (29249/33408)
Epoch: 66 | Batch_idx: 270 |  Loss: (0.3526) |  Loss2: (0.0000) | Acc: (87.00%) (30381/34688)
Epoch: 66 | Batch_idx: 280 |  Loss: (0.3535) |  Loss2: (0.0000) | Acc: (87.00%) (31487/35968)
Epoch: 66 | Batch_idx: 290 |  Loss: (0.3543) |  Loss2: (0.0000) | Acc: (87.00%) (32605/37248)
Epoch: 66 | Batch_idx: 300 |  Loss: (0.3534) |  Loss2: (0.0000) | Acc: (87.00%) (33734/38528)
Epoch: 66 | Batch_idx: 310 |  Loss: (0.3535) |  Loss2: (0.0000) | Acc: (87.00%) (34849/39808)
Epoch: 66 | Batch_idx: 320 |  Loss: (0.3549) |  Loss2: (0.0000) | Acc: (87.00%) (35939/41088)
Epoch: 66 | Batch_idx: 330 |  Loss: (0.3538) |  Loss2: (0.0000) | Acc: (87.00%) (37078/42368)
Epoch: 66 | Batch_idx: 340 |  Loss: (0.3535) |  Loss2: (0.0000) | Acc: (87.00%) (38210/43648)
Epoch: 66 | Batch_idx: 350 |  Loss: (0.3516) |  Loss2: (0.0000) | Acc: (87.00%) (39364/44928)
Epoch: 66 | Batch_idx: 360 |  Loss: (0.3523) |  Loss2: (0.0000) | Acc: (87.00%) (40484/46208)
Epoch: 66 | Batch_idx: 370 |  Loss: (0.3515) |  Loss2: (0.0000) | Acc: (87.00%) (41620/47488)
Epoch: 66 | Batch_idx: 380 |  Loss: (0.3517) |  Loss2: (0.0000) | Acc: (87.00%) (42737/48768)
Epoch: 66 | Batch_idx: 390 |  Loss: (0.3522) |  Loss2: (0.0000) | Acc: (87.00%) (43815/50000)
# TEST : Loss: (0.4560) | Acc: (84.00%) (8477/10000)
percent tensor([0.5133, 0.4867], device='cuda:0')
percent tensor([0.4977, 0.5023], device='cuda:0')
percent tensor([0.4945, 0.5055], device='cuda:0')
percent tensor([0.5799, 0.4201], device='cuda:0')
percent tensor([0.5968, 0.4032], device='cuda:0')
percent tensor([0.6125, 0.3875], device='cuda:0')
percent tensor([0.6002, 0.3998], device='cuda:0')
percent tensor([0.9970, 0.0030], device='cuda:0')
Epoch: 67 | Batch_idx: 0 |  Loss: (0.2825) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 67 | Batch_idx: 10 |  Loss: (0.3257) |  Loss2: (0.0000) | Acc: (88.00%) (1244/1408)
Epoch: 67 | Batch_idx: 20 |  Loss: (0.3434) |  Loss2: (0.0000) | Acc: (88.00%) (2368/2688)
Epoch: 67 | Batch_idx: 30 |  Loss: (0.3311) |  Loss2: (0.0000) | Acc: (88.00%) (3512/3968)
Epoch: 67 | Batch_idx: 40 |  Loss: (0.3320) |  Loss2: (0.0000) | Acc: (88.00%) (4631/5248)
Epoch: 67 | Batch_idx: 50 |  Loss: (0.3282) |  Loss2: (0.0000) | Acc: (88.00%) (5780/6528)
Epoch: 67 | Batch_idx: 60 |  Loss: (0.3259) |  Loss2: (0.0000) | Acc: (88.00%) (6924/7808)
Epoch: 67 | Batch_idx: 70 |  Loss: (0.3259) |  Loss2: (0.0000) | Acc: (88.00%) (8050/9088)
Epoch: 67 | Batch_idx: 80 |  Loss: (0.3279) |  Loss2: (0.0000) | Acc: (88.00%) (9185/10368)
Epoch: 67 | Batch_idx: 90 |  Loss: (0.3310) |  Loss2: (0.0000) | Acc: (88.00%) (10302/11648)
Epoch: 67 | Batch_idx: 100 |  Loss: (0.3332) |  Loss2: (0.0000) | Acc: (88.00%) (11429/12928)
Epoch: 67 | Batch_idx: 110 |  Loss: (0.3348) |  Loss2: (0.0000) | Acc: (88.00%) (12547/14208)
Epoch: 67 | Batch_idx: 120 |  Loss: (0.3332) |  Loss2: (0.0000) | Acc: (88.00%) (13690/15488)
Epoch: 67 | Batch_idx: 130 |  Loss: (0.3343) |  Loss2: (0.0000) | Acc: (88.00%) (14815/16768)
Epoch: 67 | Batch_idx: 140 |  Loss: (0.3376) |  Loss2: (0.0000) | Acc: (88.00%) (15935/18048)
Epoch: 67 | Batch_idx: 150 |  Loss: (0.3368) |  Loss2: (0.0000) | Acc: (88.00%) (17073/19328)
Epoch: 67 | Batch_idx: 160 |  Loss: (0.3380) |  Loss2: (0.0000) | Acc: (88.00%) (18187/20608)
Epoch: 67 | Batch_idx: 170 |  Loss: (0.3378) |  Loss2: (0.0000) | Acc: (88.00%) (19316/21888)
Epoch: 67 | Batch_idx: 180 |  Loss: (0.3400) |  Loss2: (0.0000) | Acc: (88.00%) (20416/23168)
Epoch: 67 | Batch_idx: 190 |  Loss: (0.3401) |  Loss2: (0.0000) | Acc: (88.00%) (21536/24448)
Epoch: 67 | Batch_idx: 200 |  Loss: (0.3401) |  Loss2: (0.0000) | Acc: (88.00%) (22663/25728)
Epoch: 67 | Batch_idx: 210 |  Loss: (0.3382) |  Loss2: (0.0000) | Acc: (88.00%) (23805/27008)
Epoch: 67 | Batch_idx: 220 |  Loss: (0.3381) |  Loss2: (0.0000) | Acc: (88.00%) (24938/28288)
Epoch: 67 | Batch_idx: 230 |  Loss: (0.3378) |  Loss2: (0.0000) | Acc: (88.00%) (26076/29568)
Epoch: 67 | Batch_idx: 240 |  Loss: (0.3369) |  Loss2: (0.0000) | Acc: (88.00%) (27221/30848)
Epoch: 67 | Batch_idx: 250 |  Loss: (0.3374) |  Loss2: (0.0000) | Acc: (88.00%) (28350/32128)
Epoch: 67 | Batch_idx: 260 |  Loss: (0.3360) |  Loss2: (0.0000) | Acc: (88.00%) (29490/33408)
Epoch: 67 | Batch_idx: 270 |  Loss: (0.3364) |  Loss2: (0.0000) | Acc: (88.00%) (30607/34688)
Epoch: 67 | Batch_idx: 280 |  Loss: (0.3368) |  Loss2: (0.0000) | Acc: (88.00%) (31730/35968)
Epoch: 67 | Batch_idx: 290 |  Loss: (0.3371) |  Loss2: (0.0000) | Acc: (88.00%) (32862/37248)
Epoch: 67 | Batch_idx: 300 |  Loss: (0.3371) |  Loss2: (0.0000) | Acc: (88.00%) (33983/38528)
Epoch: 67 | Batch_idx: 310 |  Loss: (0.3377) |  Loss2: (0.0000) | Acc: (88.00%) (35107/39808)
Epoch: 67 | Batch_idx: 320 |  Loss: (0.3371) |  Loss2: (0.0000) | Acc: (88.00%) (36254/41088)
Epoch: 67 | Batch_idx: 330 |  Loss: (0.3370) |  Loss2: (0.0000) | Acc: (88.00%) (37395/42368)
Epoch: 67 | Batch_idx: 340 |  Loss: (0.3373) |  Loss2: (0.0000) | Acc: (88.00%) (38521/43648)
Epoch: 67 | Batch_idx: 350 |  Loss: (0.3370) |  Loss2: (0.0000) | Acc: (88.00%) (39646/44928)
Epoch: 67 | Batch_idx: 360 |  Loss: (0.3374) |  Loss2: (0.0000) | Acc: (88.00%) (40770/46208)
Epoch: 67 | Batch_idx: 370 |  Loss: (0.3370) |  Loss2: (0.0000) | Acc: (88.00%) (41905/47488)
Epoch: 67 | Batch_idx: 380 |  Loss: (0.3366) |  Loss2: (0.0000) | Acc: (88.00%) (43039/48768)
Epoch: 67 | Batch_idx: 390 |  Loss: (0.3364) |  Loss2: (0.0000) | Acc: (88.00%) (44133/50000)
# TEST : Loss: (0.4404) | Acc: (85.00%) (8537/10000)
percent tensor([0.5143, 0.4857], device='cuda:0')
percent tensor([0.4982, 0.5018], device='cuda:0')
percent tensor([0.4967, 0.5033], device='cuda:0')
percent tensor([0.5831, 0.4169], device='cuda:0')
percent tensor([0.5951, 0.4049], device='cuda:0')
percent tensor([0.6132, 0.3868], device='cuda:0')
percent tensor([0.6157, 0.3843], device='cuda:0')
percent tensor([0.9972, 0.0028], device='cuda:0')
Epoch: 68 | Batch_idx: 0 |  Loss: (0.3005) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 68 | Batch_idx: 10 |  Loss: (0.3399) |  Loss2: (0.0000) | Acc: (87.00%) (1238/1408)
Epoch: 68 | Batch_idx: 20 |  Loss: (0.3287) |  Loss2: (0.0000) | Acc: (88.00%) (2380/2688)
Epoch: 68 | Batch_idx: 30 |  Loss: (0.3418) |  Loss2: (0.0000) | Acc: (88.00%) (3501/3968)
Epoch: 68 | Batch_idx: 40 |  Loss: (0.3470) |  Loss2: (0.0000) | Acc: (87.00%) (4615/5248)
Epoch: 68 | Batch_idx: 50 |  Loss: (0.3508) |  Loss2: (0.0000) | Acc: (87.00%) (5741/6528)
Epoch: 68 | Batch_idx: 60 |  Loss: (0.3399) |  Loss2: (0.0000) | Acc: (88.00%) (6895/7808)
Epoch: 68 | Batch_idx: 70 |  Loss: (0.3402) |  Loss2: (0.0000) | Acc: (88.00%) (8034/9088)
Epoch: 68 | Batch_idx: 80 |  Loss: (0.3424) |  Loss2: (0.0000) | Acc: (88.00%) (9146/10368)
Epoch: 68 | Batch_idx: 90 |  Loss: (0.3390) |  Loss2: (0.0000) | Acc: (88.00%) (10291/11648)
Epoch: 68 | Batch_idx: 100 |  Loss: (0.3361) |  Loss2: (0.0000) | Acc: (88.00%) (11434/12928)
Epoch: 68 | Batch_idx: 110 |  Loss: (0.3339) |  Loss2: (0.0000) | Acc: (88.00%) (12574/14208)
Epoch: 68 | Batch_idx: 120 |  Loss: (0.3328) |  Loss2: (0.0000) | Acc: (88.00%) (13707/15488)
Epoch: 68 | Batch_idx: 130 |  Loss: (0.3351) |  Loss2: (0.0000) | Acc: (88.00%) (14822/16768)
Epoch: 68 | Batch_idx: 140 |  Loss: (0.3346) |  Loss2: (0.0000) | Acc: (88.00%) (15947/18048)
Epoch: 68 | Batch_idx: 150 |  Loss: (0.3360) |  Loss2: (0.0000) | Acc: (88.00%) (17066/19328)
Epoch: 68 | Batch_idx: 160 |  Loss: (0.3352) |  Loss2: (0.0000) | Acc: (88.00%) (18194/20608)
Epoch: 68 | Batch_idx: 170 |  Loss: (0.3345) |  Loss2: (0.0000) | Acc: (88.00%) (19342/21888)
Epoch: 68 | Batch_idx: 180 |  Loss: (0.3328) |  Loss2: (0.0000) | Acc: (88.00%) (20483/23168)
Epoch: 68 | Batch_idx: 190 |  Loss: (0.3342) |  Loss2: (0.0000) | Acc: (88.00%) (21597/24448)
Epoch: 68 | Batch_idx: 200 |  Loss: (0.3357) |  Loss2: (0.0000) | Acc: (88.00%) (22717/25728)
Epoch: 68 | Batch_idx: 210 |  Loss: (0.3355) |  Loss2: (0.0000) | Acc: (88.00%) (23844/27008)
Epoch: 68 | Batch_idx: 220 |  Loss: (0.3345) |  Loss2: (0.0000) | Acc: (88.00%) (24972/28288)
Epoch: 68 | Batch_idx: 230 |  Loss: (0.3338) |  Loss2: (0.0000) | Acc: (88.00%) (26096/29568)
Epoch: 68 | Batch_idx: 240 |  Loss: (0.3329) |  Loss2: (0.0000) | Acc: (88.00%) (27234/30848)
Epoch: 68 | Batch_idx: 250 |  Loss: (0.3320) |  Loss2: (0.0000) | Acc: (88.00%) (28368/32128)
Epoch: 68 | Batch_idx: 260 |  Loss: (0.3309) |  Loss2: (0.0000) | Acc: (88.00%) (29508/33408)
Epoch: 68 | Batch_idx: 270 |  Loss: (0.3308) |  Loss2: (0.0000) | Acc: (88.00%) (30634/34688)
Epoch: 68 | Batch_idx: 280 |  Loss: (0.3310) |  Loss2: (0.0000) | Acc: (88.00%) (31763/35968)
Epoch: 68 | Batch_idx: 290 |  Loss: (0.3325) |  Loss2: (0.0000) | Acc: (88.00%) (32878/37248)
Epoch: 68 | Batch_idx: 300 |  Loss: (0.3327) |  Loss2: (0.0000) | Acc: (88.00%) (34016/38528)
Epoch: 68 | Batch_idx: 310 |  Loss: (0.3328) |  Loss2: (0.0000) | Acc: (88.00%) (35141/39808)
Epoch: 68 | Batch_idx: 320 |  Loss: (0.3320) |  Loss2: (0.0000) | Acc: (88.00%) (36285/41088)
Epoch: 68 | Batch_idx: 330 |  Loss: (0.3312) |  Loss2: (0.0000) | Acc: (88.00%) (37427/42368)
Epoch: 68 | Batch_idx: 340 |  Loss: (0.3311) |  Loss2: (0.0000) | Acc: (88.00%) (38560/43648)
Epoch: 68 | Batch_idx: 350 |  Loss: (0.3304) |  Loss2: (0.0000) | Acc: (88.00%) (39699/44928)
Epoch: 68 | Batch_idx: 360 |  Loss: (0.3290) |  Loss2: (0.0000) | Acc: (88.00%) (40866/46208)
Epoch: 68 | Batch_idx: 370 |  Loss: (0.3288) |  Loss2: (0.0000) | Acc: (88.00%) (42003/47488)
Epoch: 68 | Batch_idx: 380 |  Loss: (0.3287) |  Loss2: (0.0000) | Acc: (88.00%) (43145/48768)
Epoch: 68 | Batch_idx: 390 |  Loss: (0.3291) |  Loss2: (0.0000) | Acc: (88.00%) (44234/50000)
# TEST : Loss: (0.4340) | Acc: (85.00%) (8564/10000)
percent tensor([0.5158, 0.4842], device='cuda:0')
percent tensor([0.4987, 0.5013], device='cuda:0')
percent tensor([0.4980, 0.5020], device='cuda:0')
percent tensor([0.5876, 0.4124], device='cuda:0')
percent tensor([0.6045, 0.3955], device='cuda:0')
percent tensor([0.6111, 0.3889], device='cuda:0')
percent tensor([0.6275, 0.3725], device='cuda:0')
percent tensor([0.9974, 0.0026], device='cuda:0')
Epoch: 69 | Batch_idx: 0 |  Loss: (0.3578) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 69 | Batch_idx: 10 |  Loss: (0.3290) |  Loss2: (0.0000) | Acc: (88.00%) (1252/1408)
Epoch: 69 | Batch_idx: 20 |  Loss: (0.3297) |  Loss2: (0.0000) | Acc: (88.00%) (2387/2688)
Epoch: 69 | Batch_idx: 30 |  Loss: (0.3270) |  Loss2: (0.0000) | Acc: (89.00%) (3550/3968)
Epoch: 69 | Batch_idx: 40 |  Loss: (0.3293) |  Loss2: (0.0000) | Acc: (89.00%) (4681/5248)
Epoch: 69 | Batch_idx: 50 |  Loss: (0.3328) |  Loss2: (0.0000) | Acc: (88.00%) (5806/6528)
Epoch: 69 | Batch_idx: 60 |  Loss: (0.3287) |  Loss2: (0.0000) | Acc: (89.00%) (6957/7808)
Epoch: 69 | Batch_idx: 70 |  Loss: (0.3268) |  Loss2: (0.0000) | Acc: (89.00%) (8099/9088)
Epoch: 69 | Batch_idx: 80 |  Loss: (0.3276) |  Loss2: (0.0000) | Acc: (89.00%) (9243/10368)
Epoch: 69 | Batch_idx: 90 |  Loss: (0.3293) |  Loss2: (0.0000) | Acc: (89.00%) (10373/11648)
Epoch: 69 | Batch_idx: 100 |  Loss: (0.3258) |  Loss2: (0.0000) | Acc: (89.00%) (11525/12928)
Epoch: 69 | Batch_idx: 110 |  Loss: (0.3261) |  Loss2: (0.0000) | Acc: (89.00%) (12657/14208)
Epoch: 69 | Batch_idx: 120 |  Loss: (0.3265) |  Loss2: (0.0000) | Acc: (89.00%) (13790/15488)
Epoch: 69 | Batch_idx: 130 |  Loss: (0.3240) |  Loss2: (0.0000) | Acc: (89.00%) (14944/16768)
Epoch: 69 | Batch_idx: 140 |  Loss: (0.3259) |  Loss2: (0.0000) | Acc: (89.00%) (16074/18048)
Epoch: 69 | Batch_idx: 150 |  Loss: (0.3244) |  Loss2: (0.0000) | Acc: (89.00%) (17226/19328)
Epoch: 69 | Batch_idx: 160 |  Loss: (0.3237) |  Loss2: (0.0000) | Acc: (89.00%) (18384/20608)
Epoch: 69 | Batch_idx: 170 |  Loss: (0.3246) |  Loss2: (0.0000) | Acc: (89.00%) (19517/21888)
Epoch: 69 | Batch_idx: 180 |  Loss: (0.3233) |  Loss2: (0.0000) | Acc: (89.00%) (20668/23168)
Epoch: 69 | Batch_idx: 190 |  Loss: (0.3235) |  Loss2: (0.0000) | Acc: (89.00%) (21796/24448)
Epoch: 69 | Batch_idx: 200 |  Loss: (0.3244) |  Loss2: (0.0000) | Acc: (89.00%) (22921/25728)
Epoch: 69 | Batch_idx: 210 |  Loss: (0.3231) |  Loss2: (0.0000) | Acc: (89.00%) (24071/27008)
Epoch: 69 | Batch_idx: 220 |  Loss: (0.3231) |  Loss2: (0.0000) | Acc: (89.00%) (25212/28288)
Epoch: 69 | Batch_idx: 230 |  Loss: (0.3220) |  Loss2: (0.0000) | Acc: (89.00%) (26367/29568)
Epoch: 69 | Batch_idx: 240 |  Loss: (0.3229) |  Loss2: (0.0000) | Acc: (89.00%) (27493/30848)
Epoch: 69 | Batch_idx: 250 |  Loss: (0.3242) |  Loss2: (0.0000) | Acc: (89.00%) (28617/32128)
Epoch: 69 | Batch_idx: 260 |  Loss: (0.3249) |  Loss2: (0.0000) | Acc: (89.00%) (29738/33408)
Epoch: 69 | Batch_idx: 270 |  Loss: (0.3243) |  Loss2: (0.0000) | Acc: (89.00%) (30875/34688)
Epoch: 69 | Batch_idx: 280 |  Loss: (0.3241) |  Loss2: (0.0000) | Acc: (88.00%) (32009/35968)
Epoch: 69 | Batch_idx: 290 |  Loss: (0.3227) |  Loss2: (0.0000) | Acc: (89.00%) (33173/37248)
Epoch: 69 | Batch_idx: 300 |  Loss: (0.3233) |  Loss2: (0.0000) | Acc: (89.00%) (34301/38528)
Epoch: 69 | Batch_idx: 310 |  Loss: (0.3227) |  Loss2: (0.0000) | Acc: (89.00%) (35442/39808)
Epoch: 69 | Batch_idx: 320 |  Loss: (0.3222) |  Loss2: (0.0000) | Acc: (89.00%) (36585/41088)
Epoch: 69 | Batch_idx: 330 |  Loss: (0.3213) |  Loss2: (0.0000) | Acc: (89.00%) (37735/42368)
Epoch: 69 | Batch_idx: 340 |  Loss: (0.3208) |  Loss2: (0.0000) | Acc: (89.00%) (38877/43648)
Epoch: 69 | Batch_idx: 350 |  Loss: (0.3208) |  Loss2: (0.0000) | Acc: (89.00%) (40020/44928)
Epoch: 69 | Batch_idx: 360 |  Loss: (0.3205) |  Loss2: (0.0000) | Acc: (89.00%) (41163/46208)
Epoch: 69 | Batch_idx: 370 |  Loss: (0.3209) |  Loss2: (0.0000) | Acc: (89.00%) (42301/47488)
Epoch: 69 | Batch_idx: 380 |  Loss: (0.3217) |  Loss2: (0.0000) | Acc: (89.00%) (43430/48768)
Epoch: 69 | Batch_idx: 390 |  Loss: (0.3216) |  Loss2: (0.0000) | Acc: (89.00%) (44536/50000)
# TEST : Loss: (0.4274) | Acc: (85.00%) (8579/10000)
percent tensor([0.5168, 0.4832], device='cuda:0')
percent tensor([0.4987, 0.5013], device='cuda:0')
percent tensor([0.4990, 0.5010], device='cuda:0')
percent tensor([0.5859, 0.4141], device='cuda:0')
percent tensor([0.6049, 0.3951], device='cuda:0')
percent tensor([0.6118, 0.3882], device='cuda:0')
percent tensor([0.6382, 0.3618], device='cuda:0')
percent tensor([0.9976, 0.0024], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 70 | Batch_idx: 0 |  Loss: (0.2653) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 70 | Batch_idx: 10 |  Loss: (0.3158) |  Loss2: (0.0000) | Acc: (89.00%) (1260/1408)
Epoch: 70 | Batch_idx: 20 |  Loss: (0.3293) |  Loss2: (0.0000) | Acc: (89.00%) (2394/2688)
Epoch: 70 | Batch_idx: 30 |  Loss: (0.3187) |  Loss2: (0.0000) | Acc: (89.00%) (3534/3968)
Epoch: 70 | Batch_idx: 40 |  Loss: (0.3222) |  Loss2: (0.0000) | Acc: (88.00%) (4664/5248)
Epoch: 70 | Batch_idx: 50 |  Loss: (0.3153) |  Loss2: (0.0000) | Acc: (89.00%) (5823/6528)
Epoch: 70 | Batch_idx: 60 |  Loss: (0.3164) |  Loss2: (0.0000) | Acc: (89.00%) (6956/7808)
Epoch: 70 | Batch_idx: 70 |  Loss: (0.3181) |  Loss2: (0.0000) | Acc: (88.00%) (8087/9088)
Epoch: 70 | Batch_idx: 80 |  Loss: (0.3229) |  Loss2: (0.0000) | Acc: (88.00%) (9194/10368)
Epoch: 70 | Batch_idx: 90 |  Loss: (0.3233) |  Loss2: (0.0000) | Acc: (88.00%) (10330/11648)
Epoch: 70 | Batch_idx: 100 |  Loss: (0.3212) |  Loss2: (0.0000) | Acc: (88.00%) (11481/12928)
Epoch: 70 | Batch_idx: 110 |  Loss: (0.3186) |  Loss2: (0.0000) | Acc: (88.00%) (12620/14208)
Epoch: 70 | Batch_idx: 120 |  Loss: (0.3160) |  Loss2: (0.0000) | Acc: (88.00%) (13766/15488)
Epoch: 70 | Batch_idx: 130 |  Loss: (0.3134) |  Loss2: (0.0000) | Acc: (89.00%) (14926/16768)
Epoch: 70 | Batch_idx: 140 |  Loss: (0.3129) |  Loss2: (0.0000) | Acc: (89.00%) (16072/18048)
Epoch: 70 | Batch_idx: 150 |  Loss: (0.3119) |  Loss2: (0.0000) | Acc: (89.00%) (17221/19328)
Epoch: 70 | Batch_idx: 160 |  Loss: (0.3130) |  Loss2: (0.0000) | Acc: (89.00%) (18353/20608)
Epoch: 70 | Batch_idx: 170 |  Loss: (0.3127) |  Loss2: (0.0000) | Acc: (89.00%) (19503/21888)
Epoch: 70 | Batch_idx: 180 |  Loss: (0.3124) |  Loss2: (0.0000) | Acc: (89.00%) (20656/23168)
Epoch: 70 | Batch_idx: 190 |  Loss: (0.3118) |  Loss2: (0.0000) | Acc: (89.00%) (21804/24448)
Epoch: 70 | Batch_idx: 200 |  Loss: (0.3126) |  Loss2: (0.0000) | Acc: (89.00%) (22944/25728)
Epoch: 70 | Batch_idx: 210 |  Loss: (0.3131) |  Loss2: (0.0000) | Acc: (89.00%) (24093/27008)
Epoch: 70 | Batch_idx: 220 |  Loss: (0.3136) |  Loss2: (0.0000) | Acc: (89.00%) (25241/28288)
Epoch: 70 | Batch_idx: 230 |  Loss: (0.3133) |  Loss2: (0.0000) | Acc: (89.00%) (26383/29568)
Epoch: 70 | Batch_idx: 240 |  Loss: (0.3138) |  Loss2: (0.0000) | Acc: (89.00%) (27521/30848)
Epoch: 70 | Batch_idx: 250 |  Loss: (0.3132) |  Loss2: (0.0000) | Acc: (89.00%) (28656/32128)
Epoch: 70 | Batch_idx: 260 |  Loss: (0.3123) |  Loss2: (0.0000) | Acc: (89.00%) (29816/33408)
Epoch: 70 | Batch_idx: 270 |  Loss: (0.3130) |  Loss2: (0.0000) | Acc: (89.00%) (30941/34688)
Epoch: 70 | Batch_idx: 280 |  Loss: (0.3132) |  Loss2: (0.0000) | Acc: (89.00%) (32078/35968)
Epoch: 70 | Batch_idx: 290 |  Loss: (0.3135) |  Loss2: (0.0000) | Acc: (89.00%) (33219/37248)
Epoch: 70 | Batch_idx: 300 |  Loss: (0.3141) |  Loss2: (0.0000) | Acc: (89.00%) (34359/38528)
Epoch: 70 | Batch_idx: 310 |  Loss: (0.3144) |  Loss2: (0.0000) | Acc: (89.00%) (35497/39808)
Epoch: 70 | Batch_idx: 320 |  Loss: (0.3151) |  Loss2: (0.0000) | Acc: (89.00%) (36625/41088)
Epoch: 70 | Batch_idx: 330 |  Loss: (0.3154) |  Loss2: (0.0000) | Acc: (89.00%) (37766/42368)
Epoch: 70 | Batch_idx: 340 |  Loss: (0.3151) |  Loss2: (0.0000) | Acc: (89.00%) (38904/43648)
Epoch: 70 | Batch_idx: 350 |  Loss: (0.3154) |  Loss2: (0.0000) | Acc: (89.00%) (40047/44928)
Epoch: 70 | Batch_idx: 360 |  Loss: (0.3160) |  Loss2: (0.0000) | Acc: (89.00%) (41179/46208)
Epoch: 70 | Batch_idx: 370 |  Loss: (0.3156) |  Loss2: (0.0000) | Acc: (89.00%) (42334/47488)
Epoch: 70 | Batch_idx: 380 |  Loss: (0.3155) |  Loss2: (0.0000) | Acc: (89.00%) (43484/48768)
Epoch: 70 | Batch_idx: 390 |  Loss: (0.3154) |  Loss2: (0.0000) | Acc: (89.00%) (44576/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_070.pth.tar'
# TEST : Loss: (0.4441) | Acc: (85.00%) (8567/10000)
percent tensor([0.5168, 0.4832], device='cuda:0')
percent tensor([0.4988, 0.5012], device='cuda:0')
percent tensor([0.4989, 0.5011], device='cuda:0')
percent tensor([0.5867, 0.4133], device='cuda:0')
percent tensor([0.6064, 0.3936], device='cuda:0')
percent tensor([0.6125, 0.3875], device='cuda:0')
percent tensor([0.6389, 0.3611], device='cuda:0')
percent tensor([0.9976, 0.0024], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(177.8922, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(810.7232, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(802.1703, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1515.5442, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(508.6483, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2215.1282, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4281.5200, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1417.1688, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6122.4697, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11978.6436, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3985.7937, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16842.7617, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 71 | Batch_idx: 0 |  Loss: (0.2743) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 71 | Batch_idx: 10 |  Loss: (0.3264) |  Loss2: (0.0000) | Acc: (88.00%) (1252/1408)
Epoch: 71 | Batch_idx: 20 |  Loss: (0.3174) |  Loss2: (0.0000) | Acc: (88.00%) (2389/2688)
Epoch: 71 | Batch_idx: 30 |  Loss: (0.3109) |  Loss2: (0.0000) | Acc: (89.00%) (3532/3968)
Epoch: 71 | Batch_idx: 40 |  Loss: (0.3065) |  Loss2: (0.0000) | Acc: (89.00%) (4685/5248)
Epoch: 71 | Batch_idx: 50 |  Loss: (0.3088) |  Loss2: (0.0000) | Acc: (89.00%) (5827/6528)
Epoch: 71 | Batch_idx: 60 |  Loss: (0.3026) |  Loss2: (0.0000) | Acc: (89.00%) (6998/7808)
Epoch: 71 | Batch_idx: 70 |  Loss: (0.3060) |  Loss2: (0.0000) | Acc: (89.00%) (8135/9088)
Epoch: 71 | Batch_idx: 80 |  Loss: (0.3067) |  Loss2: (0.0000) | Acc: (89.00%) (9277/10368)
Epoch: 71 | Batch_idx: 90 |  Loss: (0.3051) |  Loss2: (0.0000) | Acc: (89.00%) (10425/11648)
Epoch: 71 | Batch_idx: 100 |  Loss: (0.3016) |  Loss2: (0.0000) | Acc: (89.00%) (11585/12928)
Epoch: 71 | Batch_idx: 110 |  Loss: (0.3031) |  Loss2: (0.0000) | Acc: (89.00%) (12718/14208)
Epoch: 71 | Batch_idx: 120 |  Loss: (0.3026) |  Loss2: (0.0000) | Acc: (89.00%) (13861/15488)
Epoch: 71 | Batch_idx: 130 |  Loss: (0.3043) |  Loss2: (0.0000) | Acc: (89.00%) (14984/16768)
Epoch: 71 | Batch_idx: 140 |  Loss: (0.3047) |  Loss2: (0.0000) | Acc: (89.00%) (16124/18048)
Epoch: 71 | Batch_idx: 150 |  Loss: (0.3053) |  Loss2: (0.0000) | Acc: (89.00%) (17272/19328)
Epoch: 71 | Batch_idx: 160 |  Loss: (0.3059) |  Loss2: (0.0000) | Acc: (89.00%) (18412/20608)
Epoch: 71 | Batch_idx: 170 |  Loss: (0.3054) |  Loss2: (0.0000) | Acc: (89.00%) (19566/21888)
Epoch: 71 | Batch_idx: 180 |  Loss: (0.3066) |  Loss2: (0.0000) | Acc: (89.00%) (20702/23168)
Epoch: 71 | Batch_idx: 190 |  Loss: (0.3072) |  Loss2: (0.0000) | Acc: (89.00%) (21834/24448)
Epoch: 71 | Batch_idx: 200 |  Loss: (0.3087) |  Loss2: (0.0000) | Acc: (89.00%) (22964/25728)
Epoch: 71 | Batch_idx: 210 |  Loss: (0.3100) |  Loss2: (0.0000) | Acc: (89.00%) (24091/27008)
Epoch: 71 | Batch_idx: 220 |  Loss: (0.3096) |  Loss2: (0.0000) | Acc: (89.00%) (25243/28288)
Epoch: 71 | Batch_idx: 230 |  Loss: (0.3092) |  Loss2: (0.0000) | Acc: (89.00%) (26391/29568)
Epoch: 71 | Batch_idx: 240 |  Loss: (0.3100) |  Loss2: (0.0000) | Acc: (89.00%) (27529/30848)
Epoch: 71 | Batch_idx: 250 |  Loss: (0.3093) |  Loss2: (0.0000) | Acc: (89.00%) (28673/32128)
Epoch: 71 | Batch_idx: 260 |  Loss: (0.3094) |  Loss2: (0.0000) | Acc: (89.00%) (29812/33408)
Epoch: 71 | Batch_idx: 270 |  Loss: (0.3090) |  Loss2: (0.0000) | Acc: (89.00%) (30962/34688)
Epoch: 71 | Batch_idx: 280 |  Loss: (0.3086) |  Loss2: (0.0000) | Acc: (89.00%) (32105/35968)
Epoch: 71 | Batch_idx: 290 |  Loss: (0.3089) |  Loss2: (0.0000) | Acc: (89.00%) (33245/37248)
Epoch: 71 | Batch_idx: 300 |  Loss: (0.3099) |  Loss2: (0.0000) | Acc: (89.00%) (34380/38528)
Epoch: 71 | Batch_idx: 310 |  Loss: (0.3095) |  Loss2: (0.0000) | Acc: (89.00%) (35528/39808)
Epoch: 71 | Batch_idx: 320 |  Loss: (0.3099) |  Loss2: (0.0000) | Acc: (89.00%) (36675/41088)
Epoch: 71 | Batch_idx: 330 |  Loss: (0.3099) |  Loss2: (0.0000) | Acc: (89.00%) (37821/42368)
Epoch: 71 | Batch_idx: 340 |  Loss: (0.3095) |  Loss2: (0.0000) | Acc: (89.00%) (38957/43648)
Epoch: 71 | Batch_idx: 350 |  Loss: (0.3083) |  Loss2: (0.0000) | Acc: (89.00%) (40117/44928)
Epoch: 71 | Batch_idx: 360 |  Loss: (0.3081) |  Loss2: (0.0000) | Acc: (89.00%) (41257/46208)
Epoch: 71 | Batch_idx: 370 |  Loss: (0.3079) |  Loss2: (0.0000) | Acc: (89.00%) (42392/47488)
Epoch: 71 | Batch_idx: 380 |  Loss: (0.3077) |  Loss2: (0.0000) | Acc: (89.00%) (43545/48768)
Epoch: 71 | Batch_idx: 390 |  Loss: (0.3078) |  Loss2: (0.0000) | Acc: (89.00%) (44646/50000)
# TEST : Loss: (0.4756) | Acc: (84.00%) (8457/10000)
percent tensor([0.5168, 0.4832], device='cuda:0')
percent tensor([0.4987, 0.5013], device='cuda:0')
percent tensor([0.4990, 0.5010], device='cuda:0')
percent tensor([0.5865, 0.4135], device='cuda:0')
percent tensor([0.6041, 0.3959], device='cuda:0')
percent tensor([0.6087, 0.3913], device='cuda:0')
percent tensor([0.6377, 0.3623], device='cuda:0')
percent tensor([0.9975, 0.0025], device='cuda:0')
Epoch: 72 | Batch_idx: 0 |  Loss: (0.2943) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 72 | Batch_idx: 10 |  Loss: (0.2822) |  Loss2: (0.0000) | Acc: (90.00%) (1269/1408)
Epoch: 72 | Batch_idx: 20 |  Loss: (0.3020) |  Loss2: (0.0000) | Acc: (89.00%) (2406/2688)
Epoch: 72 | Batch_idx: 30 |  Loss: (0.2984) |  Loss2: (0.0000) | Acc: (89.00%) (3547/3968)
Epoch: 72 | Batch_idx: 40 |  Loss: (0.3012) |  Loss2: (0.0000) | Acc: (89.00%) (4695/5248)
Epoch: 72 | Batch_idx: 50 |  Loss: (0.2995) |  Loss2: (0.0000) | Acc: (89.00%) (5847/6528)
Epoch: 72 | Batch_idx: 60 |  Loss: (0.2995) |  Loss2: (0.0000) | Acc: (89.00%) (6986/7808)
Epoch: 72 | Batch_idx: 70 |  Loss: (0.2973) |  Loss2: (0.0000) | Acc: (89.00%) (8143/9088)
Epoch: 72 | Batch_idx: 80 |  Loss: (0.2967) |  Loss2: (0.0000) | Acc: (89.00%) (9302/10368)
Epoch: 72 | Batch_idx: 90 |  Loss: (0.2992) |  Loss2: (0.0000) | Acc: (89.00%) (10438/11648)
Epoch: 72 | Batch_idx: 100 |  Loss: (0.3013) |  Loss2: (0.0000) | Acc: (89.00%) (11580/12928)
Epoch: 72 | Batch_idx: 110 |  Loss: (0.3027) |  Loss2: (0.0000) | Acc: (89.00%) (12730/14208)
Epoch: 72 | Batch_idx: 120 |  Loss: (0.3026) |  Loss2: (0.0000) | Acc: (89.00%) (13878/15488)
Epoch: 72 | Batch_idx: 130 |  Loss: (0.3004) |  Loss2: (0.0000) | Acc: (89.00%) (15041/16768)
Epoch: 72 | Batch_idx: 140 |  Loss: (0.2999) |  Loss2: (0.0000) | Acc: (89.00%) (16185/18048)
Epoch: 72 | Batch_idx: 150 |  Loss: (0.2989) |  Loss2: (0.0000) | Acc: (89.00%) (17340/19328)
Epoch: 72 | Batch_idx: 160 |  Loss: (0.2995) |  Loss2: (0.0000) | Acc: (89.00%) (18483/20608)
Epoch: 72 | Batch_idx: 170 |  Loss: (0.2990) |  Loss2: (0.0000) | Acc: (89.00%) (19633/21888)
Epoch: 72 | Batch_idx: 180 |  Loss: (0.3005) |  Loss2: (0.0000) | Acc: (89.00%) (20781/23168)
Epoch: 72 | Batch_idx: 190 |  Loss: (0.3005) |  Loss2: (0.0000) | Acc: (89.00%) (21929/24448)
Epoch: 72 | Batch_idx: 200 |  Loss: (0.3000) |  Loss2: (0.0000) | Acc: (89.00%) (23089/25728)
Epoch: 72 | Batch_idx: 210 |  Loss: (0.2984) |  Loss2: (0.0000) | Acc: (89.00%) (24260/27008)
Epoch: 72 | Batch_idx: 220 |  Loss: (0.2996) |  Loss2: (0.0000) | Acc: (89.00%) (25399/28288)
Epoch: 72 | Batch_idx: 230 |  Loss: (0.2988) |  Loss2: (0.0000) | Acc: (89.00%) (26553/29568)
Epoch: 72 | Batch_idx: 240 |  Loss: (0.2994) |  Loss2: (0.0000) | Acc: (89.00%) (27696/30848)
Epoch: 72 | Batch_idx: 250 |  Loss: (0.3004) |  Loss2: (0.0000) | Acc: (89.00%) (28836/32128)
Epoch: 72 | Batch_idx: 260 |  Loss: (0.3010) |  Loss2: (0.0000) | Acc: (89.00%) (29979/33408)
Epoch: 72 | Batch_idx: 270 |  Loss: (0.3022) |  Loss2: (0.0000) | Acc: (89.00%) (31104/34688)
Epoch: 72 | Batch_idx: 280 |  Loss: (0.3025) |  Loss2: (0.0000) | Acc: (89.00%) (32252/35968)
Epoch: 72 | Batch_idx: 290 |  Loss: (0.3023) |  Loss2: (0.0000) | Acc: (89.00%) (33409/37248)
Epoch: 72 | Batch_idx: 300 |  Loss: (0.3023) |  Loss2: (0.0000) | Acc: (89.00%) (34563/38528)
Epoch: 72 | Batch_idx: 310 |  Loss: (0.3020) |  Loss2: (0.0000) | Acc: (89.00%) (35717/39808)
Epoch: 72 | Batch_idx: 320 |  Loss: (0.3022) |  Loss2: (0.0000) | Acc: (89.00%) (36858/41088)
Epoch: 72 | Batch_idx: 330 |  Loss: (0.3024) |  Loss2: (0.0000) | Acc: (89.00%) (38016/42368)
Epoch: 72 | Batch_idx: 340 |  Loss: (0.3017) |  Loss2: (0.0000) | Acc: (89.00%) (39170/43648)
Epoch: 72 | Batch_idx: 350 |  Loss: (0.3014) |  Loss2: (0.0000) | Acc: (89.00%) (40315/44928)
Epoch: 72 | Batch_idx: 360 |  Loss: (0.3004) |  Loss2: (0.0000) | Acc: (89.00%) (41483/46208)
Epoch: 72 | Batch_idx: 370 |  Loss: (0.3004) |  Loss2: (0.0000) | Acc: (89.00%) (42635/47488)
Epoch: 72 | Batch_idx: 380 |  Loss: (0.2996) |  Loss2: (0.0000) | Acc: (89.00%) (43798/48768)
Epoch: 72 | Batch_idx: 390 |  Loss: (0.2994) |  Loss2: (0.0000) | Acc: (89.00%) (44910/50000)
# TEST : Loss: (0.4207) | Acc: (85.00%) (8576/10000)
percent tensor([0.5165, 0.4835], device='cuda:0')
percent tensor([0.4988, 0.5012], device='cuda:0')
percent tensor([0.4990, 0.5010], device='cuda:0')
percent tensor([0.5855, 0.4145], device='cuda:0')
percent tensor([0.6044, 0.3956], device='cuda:0')
percent tensor([0.6092, 0.3908], device='cuda:0')
percent tensor([0.6361, 0.3639], device='cuda:0')
percent tensor([0.9975, 0.0025], device='cuda:0')
Epoch: 73 | Batch_idx: 0 |  Loss: (0.3277) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 73 | Batch_idx: 10 |  Loss: (0.2851) |  Loss2: (0.0000) | Acc: (90.00%) (1276/1408)
Epoch: 73 | Batch_idx: 20 |  Loss: (0.2833) |  Loss2: (0.0000) | Acc: (90.00%) (2423/2688)
Epoch: 73 | Batch_idx: 30 |  Loss: (0.2910) |  Loss2: (0.0000) | Acc: (89.00%) (3562/3968)
Epoch: 73 | Batch_idx: 40 |  Loss: (0.2904) |  Loss2: (0.0000) | Acc: (90.00%) (4726/5248)
Epoch: 73 | Batch_idx: 50 |  Loss: (0.2921) |  Loss2: (0.0000) | Acc: (90.00%) (5877/6528)
Epoch: 73 | Batch_idx: 60 |  Loss: (0.2924) |  Loss2: (0.0000) | Acc: (89.00%) (7021/7808)
Epoch: 73 | Batch_idx: 70 |  Loss: (0.2944) |  Loss2: (0.0000) | Acc: (89.00%) (8159/9088)
Epoch: 73 | Batch_idx: 80 |  Loss: (0.2942) |  Loss2: (0.0000) | Acc: (89.00%) (9304/10368)
Epoch: 73 | Batch_idx: 90 |  Loss: (0.2913) |  Loss2: (0.0000) | Acc: (89.00%) (10468/11648)
Epoch: 73 | Batch_idx: 100 |  Loss: (0.2906) |  Loss2: (0.0000) | Acc: (89.00%) (11627/12928)
Epoch: 73 | Batch_idx: 110 |  Loss: (0.2893) |  Loss2: (0.0000) | Acc: (90.00%) (12798/14208)
Epoch: 73 | Batch_idx: 120 |  Loss: (0.2902) |  Loss2: (0.0000) | Acc: (90.00%) (13948/15488)
Epoch: 73 | Batch_idx: 130 |  Loss: (0.2916) |  Loss2: (0.0000) | Acc: (89.00%) (15088/16768)
Epoch: 73 | Batch_idx: 140 |  Loss: (0.2919) |  Loss2: (0.0000) | Acc: (89.00%) (16233/18048)
Epoch: 73 | Batch_idx: 150 |  Loss: (0.2932) |  Loss2: (0.0000) | Acc: (89.00%) (17376/19328)
Epoch: 73 | Batch_idx: 160 |  Loss: (0.2936) |  Loss2: (0.0000) | Acc: (89.00%) (18521/20608)
Epoch: 73 | Batch_idx: 170 |  Loss: (0.2925) |  Loss2: (0.0000) | Acc: (89.00%) (19682/21888)
Epoch: 73 | Batch_idx: 180 |  Loss: (0.2916) |  Loss2: (0.0000) | Acc: (89.00%) (20849/23168)
Epoch: 73 | Batch_idx: 190 |  Loss: (0.2924) |  Loss2: (0.0000) | Acc: (89.00%) (21990/24448)
Epoch: 73 | Batch_idx: 200 |  Loss: (0.2902) |  Loss2: (0.0000) | Acc: (90.00%) (23161/25728)
Epoch: 73 | Batch_idx: 210 |  Loss: (0.2905) |  Loss2: (0.0000) | Acc: (89.00%) (24300/27008)
Epoch: 73 | Batch_idx: 220 |  Loss: (0.2900) |  Loss2: (0.0000) | Acc: (89.00%) (25444/28288)
Epoch: 73 | Batch_idx: 230 |  Loss: (0.2903) |  Loss2: (0.0000) | Acc: (89.00%) (26601/29568)
Epoch: 73 | Batch_idx: 240 |  Loss: (0.2910) |  Loss2: (0.0000) | Acc: (89.00%) (27742/30848)
Epoch: 73 | Batch_idx: 250 |  Loss: (0.2908) |  Loss2: (0.0000) | Acc: (89.00%) (28898/32128)
Epoch: 73 | Batch_idx: 260 |  Loss: (0.2912) |  Loss2: (0.0000) | Acc: (89.00%) (30049/33408)
Epoch: 73 | Batch_idx: 270 |  Loss: (0.2915) |  Loss2: (0.0000) | Acc: (89.00%) (31186/34688)
Epoch: 73 | Batch_idx: 280 |  Loss: (0.2917) |  Loss2: (0.0000) | Acc: (89.00%) (32329/35968)
Epoch: 73 | Batch_idx: 290 |  Loss: (0.2933) |  Loss2: (0.0000) | Acc: (89.00%) (33454/37248)
Epoch: 73 | Batch_idx: 300 |  Loss: (0.2926) |  Loss2: (0.0000) | Acc: (89.00%) (34608/38528)
Epoch: 73 | Batch_idx: 310 |  Loss: (0.2926) |  Loss2: (0.0000) | Acc: (89.00%) (35768/39808)
Epoch: 73 | Batch_idx: 320 |  Loss: (0.2932) |  Loss2: (0.0000) | Acc: (89.00%) (36912/41088)
Epoch: 73 | Batch_idx: 330 |  Loss: (0.2929) |  Loss2: (0.0000) | Acc: (89.00%) (38065/42368)
Epoch: 73 | Batch_idx: 340 |  Loss: (0.2934) |  Loss2: (0.0000) | Acc: (89.00%) (39220/43648)
Epoch: 73 | Batch_idx: 350 |  Loss: (0.2931) |  Loss2: (0.0000) | Acc: (89.00%) (40369/44928)
Epoch: 73 | Batch_idx: 360 |  Loss: (0.2934) |  Loss2: (0.0000) | Acc: (89.00%) (41516/46208)
Epoch: 73 | Batch_idx: 370 |  Loss: (0.2937) |  Loss2: (0.0000) | Acc: (89.00%) (42645/47488)
Epoch: 73 | Batch_idx: 380 |  Loss: (0.2938) |  Loss2: (0.0000) | Acc: (89.00%) (43790/48768)
Epoch: 73 | Batch_idx: 390 |  Loss: (0.2939) |  Loss2: (0.0000) | Acc: (89.00%) (44907/50000)
# TEST : Loss: (0.5268) | Acc: (83.00%) (8337/10000)
percent tensor([0.5167, 0.4833], device='cuda:0')
percent tensor([0.4988, 0.5012], device='cuda:0')
percent tensor([0.4991, 0.5009], device='cuda:0')
percent tensor([0.5848, 0.4152], device='cuda:0')
percent tensor([0.6052, 0.3948], device='cuda:0')
percent tensor([0.6121, 0.3879], device='cuda:0')
percent tensor([0.6420, 0.3580], device='cuda:0')
percent tensor([0.9978, 0.0022], device='cuda:0')
Epoch: 74 | Batch_idx: 0 |  Loss: (0.2383) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 74 | Batch_idx: 10 |  Loss: (0.2850) |  Loss2: (0.0000) | Acc: (91.00%) (1286/1408)
Epoch: 74 | Batch_idx: 20 |  Loss: (0.2660) |  Loss2: (0.0000) | Acc: (91.00%) (2463/2688)
Epoch: 74 | Batch_idx: 30 |  Loss: (0.2730) |  Loss2: (0.0000) | Acc: (91.00%) (3619/3968)
Epoch: 74 | Batch_idx: 40 |  Loss: (0.2752) |  Loss2: (0.0000) | Acc: (91.00%) (4784/5248)
Epoch: 74 | Batch_idx: 50 |  Loss: (0.2778) |  Loss2: (0.0000) | Acc: (90.00%) (5938/6528)
Epoch: 74 | Batch_idx: 60 |  Loss: (0.2766) |  Loss2: (0.0000) | Acc: (90.00%) (7090/7808)
Epoch: 74 | Batch_idx: 70 |  Loss: (0.2809) |  Loss2: (0.0000) | Acc: (90.00%) (8231/9088)
Epoch: 74 | Batch_idx: 80 |  Loss: (0.2777) |  Loss2: (0.0000) | Acc: (90.00%) (9389/10368)
Epoch: 74 | Batch_idx: 90 |  Loss: (0.2786) |  Loss2: (0.0000) | Acc: (90.00%) (10546/11648)
Epoch: 74 | Batch_idx: 100 |  Loss: (0.2817) |  Loss2: (0.0000) | Acc: (90.00%) (11683/12928)
Epoch: 74 | Batch_idx: 110 |  Loss: (0.2779) |  Loss2: (0.0000) | Acc: (90.00%) (12860/14208)
Epoch: 74 | Batch_idx: 120 |  Loss: (0.2753) |  Loss2: (0.0000) | Acc: (90.00%) (14035/15488)
Epoch: 74 | Batch_idx: 130 |  Loss: (0.2775) |  Loss2: (0.0000) | Acc: (90.00%) (15194/16768)
Epoch: 74 | Batch_idx: 140 |  Loss: (0.2800) |  Loss2: (0.0000) | Acc: (90.00%) (16345/18048)
Epoch: 74 | Batch_idx: 150 |  Loss: (0.2825) |  Loss2: (0.0000) | Acc: (90.00%) (17483/19328)
Epoch: 74 | Batch_idx: 160 |  Loss: (0.2841) |  Loss2: (0.0000) | Acc: (90.00%) (18631/20608)
Epoch: 74 | Batch_idx: 170 |  Loss: (0.2845) |  Loss2: (0.0000) | Acc: (90.00%) (19781/21888)
Epoch: 74 | Batch_idx: 180 |  Loss: (0.2832) |  Loss2: (0.0000) | Acc: (90.00%) (20941/23168)
Epoch: 74 | Batch_idx: 190 |  Loss: (0.2841) |  Loss2: (0.0000) | Acc: (90.00%) (22087/24448)
Epoch: 74 | Batch_idx: 200 |  Loss: (0.2834) |  Loss2: (0.0000) | Acc: (90.00%) (23246/25728)
Epoch: 74 | Batch_idx: 210 |  Loss: (0.2821) |  Loss2: (0.0000) | Acc: (90.00%) (24411/27008)
Epoch: 74 | Batch_idx: 220 |  Loss: (0.2809) |  Loss2: (0.0000) | Acc: (90.00%) (25574/28288)
Epoch: 74 | Batch_idx: 230 |  Loss: (0.2823) |  Loss2: (0.0000) | Acc: (90.00%) (26728/29568)
Epoch: 74 | Batch_idx: 240 |  Loss: (0.2829) |  Loss2: (0.0000) | Acc: (90.00%) (27873/30848)
Epoch: 74 | Batch_idx: 250 |  Loss: (0.2831) |  Loss2: (0.0000) | Acc: (90.00%) (29030/32128)
Epoch: 74 | Batch_idx: 260 |  Loss: (0.2827) |  Loss2: (0.0000) | Acc: (90.00%) (30189/33408)
Epoch: 74 | Batch_idx: 270 |  Loss: (0.2817) |  Loss2: (0.0000) | Acc: (90.00%) (31347/34688)
Epoch: 74 | Batch_idx: 280 |  Loss: (0.2825) |  Loss2: (0.0000) | Acc: (90.00%) (32488/35968)
Epoch: 74 | Batch_idx: 290 |  Loss: (0.2832) |  Loss2: (0.0000) | Acc: (90.00%) (33629/37248)
Epoch: 74 | Batch_idx: 300 |  Loss: (0.2833) |  Loss2: (0.0000) | Acc: (90.00%) (34778/38528)
Epoch: 74 | Batch_idx: 310 |  Loss: (0.2833) |  Loss2: (0.0000) | Acc: (90.00%) (35929/39808)
Epoch: 74 | Batch_idx: 320 |  Loss: (0.2839) |  Loss2: (0.0000) | Acc: (90.00%) (37084/41088)
Epoch: 74 | Batch_idx: 330 |  Loss: (0.2839) |  Loss2: (0.0000) | Acc: (90.00%) (38242/42368)
Epoch: 74 | Batch_idx: 340 |  Loss: (0.2833) |  Loss2: (0.0000) | Acc: (90.00%) (39409/43648)
Epoch: 74 | Batch_idx: 350 |  Loss: (0.2843) |  Loss2: (0.0000) | Acc: (90.00%) (40546/44928)
Epoch: 74 | Batch_idx: 360 |  Loss: (0.2836) |  Loss2: (0.0000) | Acc: (90.00%) (41711/46208)
Epoch: 74 | Batch_idx: 370 |  Loss: (0.2841) |  Loss2: (0.0000) | Acc: (90.00%) (42862/47488)
Epoch: 74 | Batch_idx: 380 |  Loss: (0.2845) |  Loss2: (0.0000) | Acc: (90.00%) (44023/48768)
Epoch: 74 | Batch_idx: 390 |  Loss: (0.2846) |  Loss2: (0.0000) | Acc: (90.00%) (45126/50000)
# TEST : Loss: (0.4170) | Acc: (86.00%) (8639/10000)
percent tensor([0.5165, 0.4835], device='cuda:0')
percent tensor([0.4988, 0.5012], device='cuda:0')
percent tensor([0.4988, 0.5012], device='cuda:0')
percent tensor([0.5864, 0.4136], device='cuda:0')
percent tensor([0.6059, 0.3941], device='cuda:0')
percent tensor([0.6105, 0.3895], device='cuda:0')
percent tensor([0.6424, 0.3576], device='cuda:0')
percent tensor([0.9980, 0.0020], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 75 | Batch_idx: 0 |  Loss: (0.2269) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 75 | Batch_idx: 10 |  Loss: (0.2656) |  Loss2: (0.0000) | Acc: (90.00%) (1274/1408)
Epoch: 75 | Batch_idx: 20 |  Loss: (0.3180) |  Loss2: (0.0000) | Acc: (88.00%) (2372/2688)
Epoch: 75 | Batch_idx: 30 |  Loss: (0.3301) |  Loss2: (0.0000) | Acc: (88.00%) (3498/3968)
Epoch: 75 | Batch_idx: 40 |  Loss: (0.3380) |  Loss2: (0.0000) | Acc: (87.00%) (4616/5248)
Epoch: 75 | Batch_idx: 50 |  Loss: (0.3437) |  Loss2: (0.0000) | Acc: (87.00%) (5735/6528)
Epoch: 75 | Batch_idx: 60 |  Loss: (0.3462) |  Loss2: (0.0000) | Acc: (87.00%) (6860/7808)
Epoch: 75 | Batch_idx: 70 |  Loss: (0.3459) |  Loss2: (0.0000) | Acc: (87.00%) (7986/9088)
Epoch: 75 | Batch_idx: 80 |  Loss: (0.3463) |  Loss2: (0.0000) | Acc: (87.00%) (9117/10368)
Epoch: 75 | Batch_idx: 90 |  Loss: (0.3411) |  Loss2: (0.0000) | Acc: (88.00%) (10255/11648)
Epoch: 75 | Batch_idx: 100 |  Loss: (0.3425) |  Loss2: (0.0000) | Acc: (87.00%) (11370/12928)
Epoch: 75 | Batch_idx: 110 |  Loss: (0.3402) |  Loss2: (0.0000) | Acc: (88.00%) (12511/14208)
Epoch: 75 | Batch_idx: 120 |  Loss: (0.3402) |  Loss2: (0.0000) | Acc: (88.00%) (13632/15488)
Epoch: 75 | Batch_idx: 130 |  Loss: (0.3381) |  Loss2: (0.0000) | Acc: (88.00%) (14774/16768)
Epoch: 75 | Batch_idx: 140 |  Loss: (0.3412) |  Loss2: (0.0000) | Acc: (87.00%) (15878/18048)
Epoch: 75 | Batch_idx: 150 |  Loss: (0.3396) |  Loss2: (0.0000) | Acc: (88.00%) (17014/19328)
Epoch: 75 | Batch_idx: 160 |  Loss: (0.3399) |  Loss2: (0.0000) | Acc: (87.00%) (18128/20608)
Epoch: 75 | Batch_idx: 170 |  Loss: (0.3409) |  Loss2: (0.0000) | Acc: (87.00%) (19257/21888)
Epoch: 75 | Batch_idx: 180 |  Loss: (0.3396) |  Loss2: (0.0000) | Acc: (88.00%) (20398/23168)
Epoch: 75 | Batch_idx: 190 |  Loss: (0.3392) |  Loss2: (0.0000) | Acc: (88.00%) (21541/24448)
Epoch: 75 | Batch_idx: 200 |  Loss: (0.3385) |  Loss2: (0.0000) | Acc: (88.00%) (22673/25728)
Epoch: 75 | Batch_idx: 210 |  Loss: (0.3379) |  Loss2: (0.0000) | Acc: (88.00%) (23810/27008)
Epoch: 75 | Batch_idx: 220 |  Loss: (0.3352) |  Loss2: (0.0000) | Acc: (88.00%) (24972/28288)
Epoch: 75 | Batch_idx: 230 |  Loss: (0.3333) |  Loss2: (0.0000) | Acc: (88.00%) (26125/29568)
Epoch: 75 | Batch_idx: 240 |  Loss: (0.3318) |  Loss2: (0.0000) | Acc: (88.00%) (27268/30848)
Epoch: 75 | Batch_idx: 250 |  Loss: (0.3300) |  Loss2: (0.0000) | Acc: (88.00%) (28414/32128)
Epoch: 75 | Batch_idx: 260 |  Loss: (0.3290) |  Loss2: (0.0000) | Acc: (88.00%) (29562/33408)
Epoch: 75 | Batch_idx: 270 |  Loss: (0.3296) |  Loss2: (0.0000) | Acc: (88.00%) (30700/34688)
Epoch: 75 | Batch_idx: 280 |  Loss: (0.3303) |  Loss2: (0.0000) | Acc: (88.00%) (31826/35968)
Epoch: 75 | Batch_idx: 290 |  Loss: (0.3303) |  Loss2: (0.0000) | Acc: (88.00%) (32958/37248)
Epoch: 75 | Batch_idx: 300 |  Loss: (0.3309) |  Loss2: (0.0000) | Acc: (88.00%) (34077/38528)
Epoch: 75 | Batch_idx: 310 |  Loss: (0.3309) |  Loss2: (0.0000) | Acc: (88.00%) (35204/39808)
Epoch: 75 | Batch_idx: 320 |  Loss: (0.3295) |  Loss2: (0.0000) | Acc: (88.00%) (36358/41088)
Epoch: 75 | Batch_idx: 330 |  Loss: (0.3299) |  Loss2: (0.0000) | Acc: (88.00%) (37477/42368)
Epoch: 75 | Batch_idx: 340 |  Loss: (0.3303) |  Loss2: (0.0000) | Acc: (88.00%) (38614/43648)
Epoch: 75 | Batch_idx: 350 |  Loss: (0.3297) |  Loss2: (0.0000) | Acc: (88.00%) (39756/44928)
Epoch: 75 | Batch_idx: 360 |  Loss: (0.3292) |  Loss2: (0.0000) | Acc: (88.00%) (40899/46208)
Epoch: 75 | Batch_idx: 370 |  Loss: (0.3285) |  Loss2: (0.0000) | Acc: (88.00%) (42037/47488)
Epoch: 75 | Batch_idx: 380 |  Loss: (0.3286) |  Loss2: (0.0000) | Acc: (88.00%) (43178/48768)
Epoch: 75 | Batch_idx: 390 |  Loss: (0.3268) |  Loss2: (0.0000) | Acc: (88.00%) (44297/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_075.pth.tar'
# TEST : Loss: (0.4617) | Acc: (84.00%) (8476/10000)
percent tensor([0.5227, 0.4773], device='cuda:0')
percent tensor([0.4989, 0.5011], device='cuda:0')
percent tensor([0.4996, 0.5004], device='cuda:0')
percent tensor([0.6022, 0.3978], device='cuda:0')
percent tensor([0.5875, 0.4125], device='cuda:0')
percent tensor([0.6559, 0.3441], device='cuda:0')
percent tensor([0.6731, 0.3269], device='cuda:0')
percent tensor([0.9980, 0.0020], device='cuda:0')
Epoch: 76 | Batch_idx: 0 |  Loss: (0.2718) |  Loss2: (0.0000) | Acc: (86.00%) (111/128)
Epoch: 76 | Batch_idx: 10 |  Loss: (0.3068) |  Loss2: (0.0000) | Acc: (89.00%) (1255/1408)
Epoch: 76 | Batch_idx: 20 |  Loss: (0.3000) |  Loss2: (0.0000) | Acc: (89.00%) (2410/2688)
Epoch: 76 | Batch_idx: 30 |  Loss: (0.3142) |  Loss2: (0.0000) | Acc: (89.00%) (3533/3968)
Epoch: 76 | Batch_idx: 40 |  Loss: (0.3152) |  Loss2: (0.0000) | Acc: (88.00%) (4665/5248)
Epoch: 76 | Batch_idx: 50 |  Loss: (0.3069) |  Loss2: (0.0000) | Acc: (89.00%) (5818/6528)
Epoch: 76 | Batch_idx: 60 |  Loss: (0.3074) |  Loss2: (0.0000) | Acc: (89.00%) (6959/7808)
Epoch: 76 | Batch_idx: 70 |  Loss: (0.3018) |  Loss2: (0.0000) | Acc: (89.00%) (8122/9088)
Epoch: 76 | Batch_idx: 80 |  Loss: (0.3007) |  Loss2: (0.0000) | Acc: (89.00%) (9275/10368)
Epoch: 76 | Batch_idx: 90 |  Loss: (0.3021) |  Loss2: (0.0000) | Acc: (89.00%) (10412/11648)
Epoch: 76 | Batch_idx: 100 |  Loss: (0.2989) |  Loss2: (0.0000) | Acc: (89.00%) (11583/12928)
Epoch: 76 | Batch_idx: 110 |  Loss: (0.2996) |  Loss2: (0.0000) | Acc: (89.00%) (12720/14208)
Epoch: 76 | Batch_idx: 120 |  Loss: (0.2993) |  Loss2: (0.0000) | Acc: (89.00%) (13871/15488)
Epoch: 76 | Batch_idx: 130 |  Loss: (0.2977) |  Loss2: (0.0000) | Acc: (89.00%) (15026/16768)
Epoch: 76 | Batch_idx: 140 |  Loss: (0.2974) |  Loss2: (0.0000) | Acc: (89.00%) (16182/18048)
Epoch: 76 | Batch_idx: 150 |  Loss: (0.2988) |  Loss2: (0.0000) | Acc: (89.00%) (17318/19328)
Epoch: 76 | Batch_idx: 160 |  Loss: (0.3003) |  Loss2: (0.0000) | Acc: (89.00%) (18453/20608)
Epoch: 76 | Batch_idx: 170 |  Loss: (0.3037) |  Loss2: (0.0000) | Acc: (89.00%) (19578/21888)
Epoch: 76 | Batch_idx: 180 |  Loss: (0.3039) |  Loss2: (0.0000) | Acc: (89.00%) (20718/23168)
Epoch: 76 | Batch_idx: 190 |  Loss: (0.3052) |  Loss2: (0.0000) | Acc: (89.00%) (21857/24448)
Epoch: 76 | Batch_idx: 200 |  Loss: (0.3047) |  Loss2: (0.0000) | Acc: (89.00%) (23011/25728)
Epoch: 76 | Batch_idx: 210 |  Loss: (0.3038) |  Loss2: (0.0000) | Acc: (89.00%) (24163/27008)
Epoch: 76 | Batch_idx: 220 |  Loss: (0.3028) |  Loss2: (0.0000) | Acc: (89.00%) (25313/28288)
Epoch: 76 | Batch_idx: 230 |  Loss: (0.3036) |  Loss2: (0.0000) | Acc: (89.00%) (26455/29568)
Epoch: 76 | Batch_idx: 240 |  Loss: (0.3040) |  Loss2: (0.0000) | Acc: (89.00%) (27605/30848)
Epoch: 76 | Batch_idx: 250 |  Loss: (0.3039) |  Loss2: (0.0000) | Acc: (89.00%) (28746/32128)
Epoch: 76 | Batch_idx: 260 |  Loss: (0.3033) |  Loss2: (0.0000) | Acc: (89.00%) (29894/33408)
Epoch: 76 | Batch_idx: 270 |  Loss: (0.3034) |  Loss2: (0.0000) | Acc: (89.00%) (31028/34688)
Epoch: 76 | Batch_idx: 280 |  Loss: (0.3026) |  Loss2: (0.0000) | Acc: (89.00%) (32188/35968)
Epoch: 76 | Batch_idx: 290 |  Loss: (0.3020) |  Loss2: (0.0000) | Acc: (89.00%) (33343/37248)
Epoch: 76 | Batch_idx: 300 |  Loss: (0.3010) |  Loss2: (0.0000) | Acc: (89.00%) (34502/38528)
Epoch: 76 | Batch_idx: 310 |  Loss: (0.3010) |  Loss2: (0.0000) | Acc: (89.00%) (35654/39808)
Epoch: 76 | Batch_idx: 320 |  Loss: (0.3004) |  Loss2: (0.0000) | Acc: (89.00%) (36801/41088)
Epoch: 76 | Batch_idx: 330 |  Loss: (0.3004) |  Loss2: (0.0000) | Acc: (89.00%) (37953/42368)
Epoch: 76 | Batch_idx: 340 |  Loss: (0.3000) |  Loss2: (0.0000) | Acc: (89.00%) (39116/43648)
Epoch: 76 | Batch_idx: 350 |  Loss: (0.2990) |  Loss2: (0.0000) | Acc: (89.00%) (40277/44928)
Epoch: 76 | Batch_idx: 360 |  Loss: (0.2988) |  Loss2: (0.0000) | Acc: (89.00%) (41422/46208)
Epoch: 76 | Batch_idx: 370 |  Loss: (0.2981) |  Loss2: (0.0000) | Acc: (89.00%) (42590/47488)
Epoch: 76 | Batch_idx: 380 |  Loss: (0.2977) |  Loss2: (0.0000) | Acc: (89.00%) (43749/48768)
Epoch: 76 | Batch_idx: 390 |  Loss: (0.2976) |  Loss2: (0.0000) | Acc: (89.00%) (44865/50000)
# TEST : Loss: (0.4411) | Acc: (85.00%) (8554/10000)
percent tensor([0.5264, 0.4736], device='cuda:0')
percent tensor([0.4998, 0.5002], device='cuda:0')
percent tensor([0.5001, 0.4999], device='cuda:0')
percent tensor([0.5959, 0.4041], device='cuda:0')
percent tensor([0.5915, 0.4085], device='cuda:0')
percent tensor([0.6550, 0.3450], device='cuda:0')
percent tensor([0.6871, 0.3129], device='cuda:0')
percent tensor([0.9981, 0.0019], device='cuda:0')
Epoch: 77 | Batch_idx: 0 |  Loss: (0.3168) |  Loss2: (0.0000) | Acc: (86.00%) (111/128)
Epoch: 77 | Batch_idx: 10 |  Loss: (0.2941) |  Loss2: (0.0000) | Acc: (89.00%) (1263/1408)
Epoch: 77 | Batch_idx: 20 |  Loss: (0.3107) |  Loss2: (0.0000) | Acc: (88.00%) (2386/2688)
Epoch: 77 | Batch_idx: 30 |  Loss: (0.3049) |  Loss2: (0.0000) | Acc: (89.00%) (3539/3968)
Epoch: 77 | Batch_idx: 40 |  Loss: (0.2993) |  Loss2: (0.0000) | Acc: (89.00%) (4696/5248)
Epoch: 77 | Batch_idx: 50 |  Loss: (0.2999) |  Loss2: (0.0000) | Acc: (89.00%) (5838/6528)
Epoch: 77 | Batch_idx: 60 |  Loss: (0.3019) |  Loss2: (0.0000) | Acc: (89.00%) (6984/7808)
Epoch: 77 | Batch_idx: 70 |  Loss: (0.2991) |  Loss2: (0.0000) | Acc: (89.00%) (8141/9088)
Epoch: 77 | Batch_idx: 80 |  Loss: (0.2969) |  Loss2: (0.0000) | Acc: (89.00%) (9295/10368)
Epoch: 77 | Batch_idx: 90 |  Loss: (0.2968) |  Loss2: (0.0000) | Acc: (89.00%) (10448/11648)
Epoch: 77 | Batch_idx: 100 |  Loss: (0.2944) |  Loss2: (0.0000) | Acc: (89.00%) (11601/12928)
Epoch: 77 | Batch_idx: 110 |  Loss: (0.2927) |  Loss2: (0.0000) | Acc: (89.00%) (12750/14208)
Epoch: 77 | Batch_idx: 120 |  Loss: (0.2921) |  Loss2: (0.0000) | Acc: (89.00%) (13891/15488)
Epoch: 77 | Batch_idx: 130 |  Loss: (0.2913) |  Loss2: (0.0000) | Acc: (89.00%) (15043/16768)
Epoch: 77 | Batch_idx: 140 |  Loss: (0.2887) |  Loss2: (0.0000) | Acc: (89.00%) (16223/18048)
Epoch: 77 | Batch_idx: 150 |  Loss: (0.2885) |  Loss2: (0.0000) | Acc: (89.00%) (17375/19328)
Epoch: 77 | Batch_idx: 160 |  Loss: (0.2905) |  Loss2: (0.0000) | Acc: (89.00%) (18513/20608)
Epoch: 77 | Batch_idx: 170 |  Loss: (0.2899) |  Loss2: (0.0000) | Acc: (89.00%) (19675/21888)
Epoch: 77 | Batch_idx: 180 |  Loss: (0.2897) |  Loss2: (0.0000) | Acc: (89.00%) (20834/23168)
Epoch: 77 | Batch_idx: 190 |  Loss: (0.2899) |  Loss2: (0.0000) | Acc: (89.00%) (21987/24448)
Epoch: 77 | Batch_idx: 200 |  Loss: (0.2904) |  Loss2: (0.0000) | Acc: (89.00%) (23132/25728)
Epoch: 77 | Batch_idx: 210 |  Loss: (0.2897) |  Loss2: (0.0000) | Acc: (89.00%) (24297/27008)
Epoch: 77 | Batch_idx: 220 |  Loss: (0.2887) |  Loss2: (0.0000) | Acc: (89.00%) (25459/28288)
Epoch: 77 | Batch_idx: 230 |  Loss: (0.2879) |  Loss2: (0.0000) | Acc: (90.00%) (26617/29568)
Epoch: 77 | Batch_idx: 240 |  Loss: (0.2870) |  Loss2: (0.0000) | Acc: (90.00%) (27791/30848)
Epoch: 77 | Batch_idx: 250 |  Loss: (0.2864) |  Loss2: (0.0000) | Acc: (90.00%) (28953/32128)
Epoch: 77 | Batch_idx: 260 |  Loss: (0.2858) |  Loss2: (0.0000) | Acc: (90.00%) (30121/33408)
Epoch: 77 | Batch_idx: 270 |  Loss: (0.2856) |  Loss2: (0.0000) | Acc: (90.00%) (31281/34688)
Epoch: 77 | Batch_idx: 280 |  Loss: (0.2858) |  Loss2: (0.0000) | Acc: (90.00%) (32439/35968)
Epoch: 77 | Batch_idx: 290 |  Loss: (0.2848) |  Loss2: (0.0000) | Acc: (90.00%) (33607/37248)
Epoch: 77 | Batch_idx: 300 |  Loss: (0.2848) |  Loss2: (0.0000) | Acc: (90.00%) (34757/38528)
Epoch: 77 | Batch_idx: 310 |  Loss: (0.2851) |  Loss2: (0.0000) | Acc: (90.00%) (35906/39808)
Epoch: 77 | Batch_idx: 320 |  Loss: (0.2850) |  Loss2: (0.0000) | Acc: (90.00%) (37066/41088)
Epoch: 77 | Batch_idx: 330 |  Loss: (0.2849) |  Loss2: (0.0000) | Acc: (90.00%) (38234/42368)
Epoch: 77 | Batch_idx: 340 |  Loss: (0.2851) |  Loss2: (0.0000) | Acc: (90.00%) (39387/43648)
Epoch: 77 | Batch_idx: 350 |  Loss: (0.2844) |  Loss2: (0.0000) | Acc: (90.00%) (40553/44928)
Epoch: 77 | Batch_idx: 360 |  Loss: (0.2844) |  Loss2: (0.0000) | Acc: (90.00%) (41709/46208)
Epoch: 77 | Batch_idx: 370 |  Loss: (0.2842) |  Loss2: (0.0000) | Acc: (90.00%) (42868/47488)
Epoch: 77 | Batch_idx: 380 |  Loss: (0.2841) |  Loss2: (0.0000) | Acc: (90.00%) (44020/48768)
Epoch: 77 | Batch_idx: 390 |  Loss: (0.2839) |  Loss2: (0.0000) | Acc: (90.00%) (45139/50000)
# TEST : Loss: (0.4282) | Acc: (85.00%) (8597/10000)
percent tensor([0.5287, 0.4713], device='cuda:0')
percent tensor([0.5002, 0.4998], device='cuda:0')
percent tensor([0.5006, 0.4994], device='cuda:0')
percent tensor([0.5971, 0.4029], device='cuda:0')
percent tensor([0.5918, 0.4082], device='cuda:0')
percent tensor([0.6572, 0.3428], device='cuda:0')
percent tensor([0.6888, 0.3112], device='cuda:0')
percent tensor([0.9981, 0.0019], device='cuda:0')
Epoch: 78 | Batch_idx: 0 |  Loss: (0.2412) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 78 | Batch_idx: 10 |  Loss: (0.2769) |  Loss2: (0.0000) | Acc: (90.00%) (1272/1408)
Epoch: 78 | Batch_idx: 20 |  Loss: (0.2819) |  Loss2: (0.0000) | Acc: (90.00%) (2428/2688)
Epoch: 78 | Batch_idx: 30 |  Loss: (0.2825) |  Loss2: (0.0000) | Acc: (90.00%) (3580/3968)
Epoch: 78 | Batch_idx: 40 |  Loss: (0.2710) |  Loss2: (0.0000) | Acc: (90.00%) (4760/5248)
Epoch: 78 | Batch_idx: 50 |  Loss: (0.2684) |  Loss2: (0.0000) | Acc: (90.00%) (5930/6528)
Epoch: 78 | Batch_idx: 60 |  Loss: (0.2766) |  Loss2: (0.0000) | Acc: (90.00%) (7071/7808)
Epoch: 78 | Batch_idx: 70 |  Loss: (0.2788) |  Loss2: (0.0000) | Acc: (90.00%) (8228/9088)
Epoch: 78 | Batch_idx: 80 |  Loss: (0.2759) |  Loss2: (0.0000) | Acc: (90.00%) (9407/10368)
Epoch: 78 | Batch_idx: 90 |  Loss: (0.2760) |  Loss2: (0.0000) | Acc: (90.00%) (10566/11648)
Epoch: 78 | Batch_idx: 100 |  Loss: (0.2763) |  Loss2: (0.0000) | Acc: (90.00%) (11717/12928)
Epoch: 78 | Batch_idx: 110 |  Loss: (0.2778) |  Loss2: (0.0000) | Acc: (90.00%) (12872/14208)
Epoch: 78 | Batch_idx: 120 |  Loss: (0.2761) |  Loss2: (0.0000) | Acc: (90.00%) (14042/15488)
Epoch: 78 | Batch_idx: 130 |  Loss: (0.2755) |  Loss2: (0.0000) | Acc: (90.00%) (15204/16768)
Epoch: 78 | Batch_idx: 140 |  Loss: (0.2742) |  Loss2: (0.0000) | Acc: (90.00%) (16372/18048)
Epoch: 78 | Batch_idx: 150 |  Loss: (0.2754) |  Loss2: (0.0000) | Acc: (90.00%) (17535/19328)
Epoch: 78 | Batch_idx: 160 |  Loss: (0.2769) |  Loss2: (0.0000) | Acc: (90.00%) (18677/20608)
Epoch: 78 | Batch_idx: 170 |  Loss: (0.2777) |  Loss2: (0.0000) | Acc: (90.00%) (19826/21888)
Epoch: 78 | Batch_idx: 180 |  Loss: (0.2780) |  Loss2: (0.0000) | Acc: (90.00%) (20981/23168)
Epoch: 78 | Batch_idx: 190 |  Loss: (0.2789) |  Loss2: (0.0000) | Acc: (90.00%) (22137/24448)
Epoch: 78 | Batch_idx: 200 |  Loss: (0.2780) |  Loss2: (0.0000) | Acc: (90.00%) (23298/25728)
Epoch: 78 | Batch_idx: 210 |  Loss: (0.2765) |  Loss2: (0.0000) | Acc: (90.00%) (24464/27008)
Epoch: 78 | Batch_idx: 220 |  Loss: (0.2758) |  Loss2: (0.0000) | Acc: (90.00%) (25624/28288)
Epoch: 78 | Batch_idx: 230 |  Loss: (0.2781) |  Loss2: (0.0000) | Acc: (90.00%) (26768/29568)
Epoch: 78 | Batch_idx: 240 |  Loss: (0.2766) |  Loss2: (0.0000) | Acc: (90.00%) (27939/30848)
Epoch: 78 | Batch_idx: 250 |  Loss: (0.2770) |  Loss2: (0.0000) | Acc: (90.00%) (29087/32128)
Epoch: 78 | Batch_idx: 260 |  Loss: (0.2769) |  Loss2: (0.0000) | Acc: (90.00%) (30241/33408)
Epoch: 78 | Batch_idx: 270 |  Loss: (0.2775) |  Loss2: (0.0000) | Acc: (90.00%) (31397/34688)
Epoch: 78 | Batch_idx: 280 |  Loss: (0.2787) |  Loss2: (0.0000) | Acc: (90.00%) (32533/35968)
Epoch: 78 | Batch_idx: 290 |  Loss: (0.2791) |  Loss2: (0.0000) | Acc: (90.00%) (33690/37248)
Epoch: 78 | Batch_idx: 300 |  Loss: (0.2806) |  Loss2: (0.0000) | Acc: (90.00%) (34826/38528)
Epoch: 78 | Batch_idx: 310 |  Loss: (0.2807) |  Loss2: (0.0000) | Acc: (90.00%) (35980/39808)
Epoch: 78 | Batch_idx: 320 |  Loss: (0.2807) |  Loss2: (0.0000) | Acc: (90.00%) (37150/41088)
Epoch: 78 | Batch_idx: 330 |  Loss: (0.2810) |  Loss2: (0.0000) | Acc: (90.00%) (38299/42368)
Epoch: 78 | Batch_idx: 340 |  Loss: (0.2814) |  Loss2: (0.0000) | Acc: (90.00%) (39435/43648)
Epoch: 78 | Batch_idx: 350 |  Loss: (0.2805) |  Loss2: (0.0000) | Acc: (90.00%) (40602/44928)
Epoch: 78 | Batch_idx: 360 |  Loss: (0.2799) |  Loss2: (0.0000) | Acc: (90.00%) (41769/46208)
Epoch: 78 | Batch_idx: 370 |  Loss: (0.2798) |  Loss2: (0.0000) | Acc: (90.00%) (42932/47488)
Epoch: 78 | Batch_idx: 380 |  Loss: (0.2795) |  Loss2: (0.0000) | Acc: (90.00%) (44084/48768)
Epoch: 78 | Batch_idx: 390 |  Loss: (0.2787) |  Loss2: (0.0000) | Acc: (90.00%) (45216/50000)
# TEST : Loss: (0.4196) | Acc: (86.00%) (8626/10000)
percent tensor([0.5303, 0.4697], device='cuda:0')
percent tensor([0.4996, 0.5004], device='cuda:0')
percent tensor([0.5011, 0.4989], device='cuda:0')
percent tensor([0.5972, 0.4028], device='cuda:0')
percent tensor([0.6015, 0.3985], device='cuda:0')
percent tensor([0.6592, 0.3408], device='cuda:0')
percent tensor([0.6930, 0.3070], device='cuda:0')
percent tensor([0.9982, 0.0018], device='cuda:0')
Epoch: 79 | Batch_idx: 0 |  Loss: (0.2525) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 79 | Batch_idx: 10 |  Loss: (0.2681) |  Loss2: (0.0000) | Acc: (90.00%) (1278/1408)
Epoch: 79 | Batch_idx: 20 |  Loss: (0.2819) |  Loss2: (0.0000) | Acc: (89.00%) (2417/2688)
Epoch: 79 | Batch_idx: 30 |  Loss: (0.2815) |  Loss2: (0.0000) | Acc: (89.00%) (3568/3968)
Epoch: 79 | Batch_idx: 40 |  Loss: (0.2794) |  Loss2: (0.0000) | Acc: (90.00%) (4732/5248)
Epoch: 79 | Batch_idx: 50 |  Loss: (0.2808) |  Loss2: (0.0000) | Acc: (90.00%) (5886/6528)
Epoch: 79 | Batch_idx: 60 |  Loss: (0.2795) |  Loss2: (0.0000) | Acc: (90.00%) (7037/7808)
Epoch: 79 | Batch_idx: 70 |  Loss: (0.2805) |  Loss2: (0.0000) | Acc: (90.00%) (8198/9088)
Epoch: 79 | Batch_idx: 80 |  Loss: (0.2813) |  Loss2: (0.0000) | Acc: (90.00%) (9361/10368)
Epoch: 79 | Batch_idx: 90 |  Loss: (0.2804) |  Loss2: (0.0000) | Acc: (90.00%) (10520/11648)
Epoch: 79 | Batch_idx: 100 |  Loss: (0.2778) |  Loss2: (0.0000) | Acc: (90.00%) (11689/12928)
Epoch: 79 | Batch_idx: 110 |  Loss: (0.2769) |  Loss2: (0.0000) | Acc: (90.00%) (12850/14208)
Epoch: 79 | Batch_idx: 120 |  Loss: (0.2801) |  Loss2: (0.0000) | Acc: (90.00%) (13997/15488)
Epoch: 79 | Batch_idx: 130 |  Loss: (0.2788) |  Loss2: (0.0000) | Acc: (90.00%) (15162/16768)
Epoch: 79 | Batch_idx: 140 |  Loss: (0.2805) |  Loss2: (0.0000) | Acc: (90.00%) (16301/18048)
Epoch: 79 | Batch_idx: 150 |  Loss: (0.2810) |  Loss2: (0.0000) | Acc: (90.00%) (17440/19328)
Epoch: 79 | Batch_idx: 160 |  Loss: (0.2800) |  Loss2: (0.0000) | Acc: (90.00%) (18604/20608)
Epoch: 79 | Batch_idx: 170 |  Loss: (0.2777) |  Loss2: (0.0000) | Acc: (90.00%) (19779/21888)
Epoch: 79 | Batch_idx: 180 |  Loss: (0.2774) |  Loss2: (0.0000) | Acc: (90.00%) (20934/23168)
Epoch: 79 | Batch_idx: 190 |  Loss: (0.2780) |  Loss2: (0.0000) | Acc: (90.00%) (22077/24448)
Epoch: 79 | Batch_idx: 200 |  Loss: (0.2785) |  Loss2: (0.0000) | Acc: (90.00%) (23223/25728)
Epoch: 79 | Batch_idx: 210 |  Loss: (0.2789) |  Loss2: (0.0000) | Acc: (90.00%) (24381/27008)
Epoch: 79 | Batch_idx: 220 |  Loss: (0.2774) |  Loss2: (0.0000) | Acc: (90.00%) (25547/28288)
Epoch: 79 | Batch_idx: 230 |  Loss: (0.2763) |  Loss2: (0.0000) | Acc: (90.00%) (26706/29568)
Epoch: 79 | Batch_idx: 240 |  Loss: (0.2761) |  Loss2: (0.0000) | Acc: (90.00%) (27861/30848)
Epoch: 79 | Batch_idx: 250 |  Loss: (0.2748) |  Loss2: (0.0000) | Acc: (90.00%) (29032/32128)
Epoch: 79 | Batch_idx: 260 |  Loss: (0.2738) |  Loss2: (0.0000) | Acc: (90.00%) (30201/33408)
Epoch: 79 | Batch_idx: 270 |  Loss: (0.2747) |  Loss2: (0.0000) | Acc: (90.00%) (31355/34688)
Epoch: 79 | Batch_idx: 280 |  Loss: (0.2744) |  Loss2: (0.0000) | Acc: (90.00%) (32514/35968)
Epoch: 79 | Batch_idx: 290 |  Loss: (0.2747) |  Loss2: (0.0000) | Acc: (90.00%) (33664/37248)
Epoch: 79 | Batch_idx: 300 |  Loss: (0.2750) |  Loss2: (0.0000) | Acc: (90.00%) (34837/38528)
Epoch: 79 | Batch_idx: 310 |  Loss: (0.2752) |  Loss2: (0.0000) | Acc: (90.00%) (35991/39808)
Epoch: 79 | Batch_idx: 320 |  Loss: (0.2748) |  Loss2: (0.0000) | Acc: (90.00%) (37150/41088)
Epoch: 79 | Batch_idx: 330 |  Loss: (0.2748) |  Loss2: (0.0000) | Acc: (90.00%) (38306/42368)
Epoch: 79 | Batch_idx: 340 |  Loss: (0.2754) |  Loss2: (0.0000) | Acc: (90.00%) (39453/43648)
Epoch: 79 | Batch_idx: 350 |  Loss: (0.2752) |  Loss2: (0.0000) | Acc: (90.00%) (40621/44928)
Epoch: 79 | Batch_idx: 360 |  Loss: (0.2748) |  Loss2: (0.0000) | Acc: (90.00%) (41785/46208)
Epoch: 79 | Batch_idx: 370 |  Loss: (0.2745) |  Loss2: (0.0000) | Acc: (90.00%) (42944/47488)
Epoch: 79 | Batch_idx: 380 |  Loss: (0.2745) |  Loss2: (0.0000) | Acc: (90.00%) (44105/48768)
Epoch: 79 | Batch_idx: 390 |  Loss: (0.2739) |  Loss2: (0.0000) | Acc: (90.00%) (45237/50000)
# TEST : Loss: (0.4184) | Acc: (86.00%) (8622/10000)
percent tensor([0.5299, 0.4701], device='cuda:0')
percent tensor([0.4995, 0.5005], device='cuda:0')
percent tensor([0.5016, 0.4984], device='cuda:0')
percent tensor([0.5934, 0.4066], device='cuda:0')
percent tensor([0.5975, 0.4025], device='cuda:0')
percent tensor([0.6494, 0.3506], device='cuda:0')
percent tensor([0.6950, 0.3050], device='cuda:0')
percent tensor([0.9983, 0.0017], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 80 | Batch_idx: 0 |  Loss: (0.2514) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 80 | Batch_idx: 10 |  Loss: (0.2667) |  Loss2: (0.0000) | Acc: (91.00%) (1283/1408)
Epoch: 80 | Batch_idx: 20 |  Loss: (0.2635) |  Loss2: (0.0000) | Acc: (91.00%) (2454/2688)
Epoch: 80 | Batch_idx: 30 |  Loss: (0.2645) |  Loss2: (0.0000) | Acc: (91.00%) (3613/3968)
Epoch: 80 | Batch_idx: 40 |  Loss: (0.2806) |  Loss2: (0.0000) | Acc: (90.00%) (4749/5248)
Epoch: 80 | Batch_idx: 50 |  Loss: (0.2775) |  Loss2: (0.0000) | Acc: (90.00%) (5916/6528)
Epoch: 80 | Batch_idx: 60 |  Loss: (0.2775) |  Loss2: (0.0000) | Acc: (90.00%) (7070/7808)
Epoch: 80 | Batch_idx: 70 |  Loss: (0.2757) |  Loss2: (0.0000) | Acc: (90.00%) (8237/9088)
Epoch: 80 | Batch_idx: 80 |  Loss: (0.2735) |  Loss2: (0.0000) | Acc: (90.00%) (9399/10368)
Epoch: 80 | Batch_idx: 90 |  Loss: (0.2685) |  Loss2: (0.0000) | Acc: (90.00%) (10572/11648)
Epoch: 80 | Batch_idx: 100 |  Loss: (0.2681) |  Loss2: (0.0000) | Acc: (90.00%) (11735/12928)
Epoch: 80 | Batch_idx: 110 |  Loss: (0.2691) |  Loss2: (0.0000) | Acc: (90.00%) (12897/14208)
Epoch: 80 | Batch_idx: 120 |  Loss: (0.2704) |  Loss2: (0.0000) | Acc: (90.00%) (14050/15488)
Epoch: 80 | Batch_idx: 130 |  Loss: (0.2710) |  Loss2: (0.0000) | Acc: (90.00%) (15202/16768)
Epoch: 80 | Batch_idx: 140 |  Loss: (0.2722) |  Loss2: (0.0000) | Acc: (90.00%) (16365/18048)
Epoch: 80 | Batch_idx: 150 |  Loss: (0.2735) |  Loss2: (0.0000) | Acc: (90.00%) (17514/19328)
Epoch: 80 | Batch_idx: 160 |  Loss: (0.2720) |  Loss2: (0.0000) | Acc: (90.00%) (18693/20608)
Epoch: 80 | Batch_idx: 170 |  Loss: (0.2726) |  Loss2: (0.0000) | Acc: (90.00%) (19850/21888)
Epoch: 80 | Batch_idx: 180 |  Loss: (0.2724) |  Loss2: (0.0000) | Acc: (90.00%) (21010/23168)
Epoch: 80 | Batch_idx: 190 |  Loss: (0.2738) |  Loss2: (0.0000) | Acc: (90.00%) (22154/24448)
Epoch: 80 | Batch_idx: 200 |  Loss: (0.2744) |  Loss2: (0.0000) | Acc: (90.00%) (23312/25728)
Epoch: 80 | Batch_idx: 210 |  Loss: (0.2768) |  Loss2: (0.0000) | Acc: (90.00%) (24436/27008)
Epoch: 80 | Batch_idx: 220 |  Loss: (0.2777) |  Loss2: (0.0000) | Acc: (90.00%) (25582/28288)
Epoch: 80 | Batch_idx: 230 |  Loss: (0.2775) |  Loss2: (0.0000) | Acc: (90.00%) (26754/29568)
Epoch: 80 | Batch_idx: 240 |  Loss: (0.2771) |  Loss2: (0.0000) | Acc: (90.00%) (27916/30848)
Epoch: 80 | Batch_idx: 250 |  Loss: (0.2772) |  Loss2: (0.0000) | Acc: (90.00%) (29074/32128)
Epoch: 80 | Batch_idx: 260 |  Loss: (0.2769) |  Loss2: (0.0000) | Acc: (90.00%) (30230/33408)
Epoch: 80 | Batch_idx: 270 |  Loss: (0.2768) |  Loss2: (0.0000) | Acc: (90.00%) (31396/34688)
Epoch: 80 | Batch_idx: 280 |  Loss: (0.2761) |  Loss2: (0.0000) | Acc: (90.00%) (32570/35968)
Epoch: 80 | Batch_idx: 290 |  Loss: (0.2770) |  Loss2: (0.0000) | Acc: (90.00%) (33712/37248)
Epoch: 80 | Batch_idx: 300 |  Loss: (0.2762) |  Loss2: (0.0000) | Acc: (90.00%) (34886/38528)
Epoch: 80 | Batch_idx: 310 |  Loss: (0.2762) |  Loss2: (0.0000) | Acc: (90.00%) (36040/39808)
Epoch: 80 | Batch_idx: 320 |  Loss: (0.2763) |  Loss2: (0.0000) | Acc: (90.00%) (37192/41088)
Epoch: 80 | Batch_idx: 330 |  Loss: (0.2770) |  Loss2: (0.0000) | Acc: (90.00%) (38331/42368)
Epoch: 80 | Batch_idx: 340 |  Loss: (0.2773) |  Loss2: (0.0000) | Acc: (90.00%) (39484/43648)
Epoch: 80 | Batch_idx: 350 |  Loss: (0.2772) |  Loss2: (0.0000) | Acc: (90.00%) (40637/44928)
Epoch: 80 | Batch_idx: 360 |  Loss: (0.2780) |  Loss2: (0.0000) | Acc: (90.00%) (41781/46208)
Epoch: 80 | Batch_idx: 370 |  Loss: (0.2795) |  Loss2: (0.0000) | Acc: (90.00%) (42912/47488)
Epoch: 80 | Batch_idx: 380 |  Loss: (0.2807) |  Loss2: (0.0000) | Acc: (90.00%) (44046/48768)
Epoch: 80 | Batch_idx: 390 |  Loss: (0.2818) |  Loss2: (0.0000) | Acc: (90.00%) (45152/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_080.pth.tar'
# TEST : Loss: (0.4622) | Acc: (85.00%) (8533/10000)
percent tensor([0.5299, 0.4701], device='cuda:0')
percent tensor([0.4995, 0.5005], device='cuda:0')
percent tensor([0.5012, 0.4988], device='cuda:0')
percent tensor([0.5930, 0.4070], device='cuda:0')
percent tensor([0.5949, 0.4051], device='cuda:0')
percent tensor([0.6471, 0.3529], device='cuda:0')
percent tensor([0.6862, 0.3138], device='cuda:0')
percent tensor([0.9981, 0.0019], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(178.7941, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(813.7054, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(805.0360, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1514.1942, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(507.0573, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2223.0957, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4278.9409, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1412.0759, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6134.6948, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11941.9648, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3970.3564, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16774.4355, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 81 | Batch_idx: 0 |  Loss: (0.2159) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 81 | Batch_idx: 10 |  Loss: (0.2630) |  Loss2: (0.0000) | Acc: (90.00%) (1274/1408)
Epoch: 81 | Batch_idx: 20 |  Loss: (0.2613) |  Loss2: (0.0000) | Acc: (90.00%) (2438/2688)
Epoch: 81 | Batch_idx: 30 |  Loss: (0.2630) |  Loss2: (0.0000) | Acc: (90.00%) (3591/3968)
Epoch: 81 | Batch_idx: 40 |  Loss: (0.2574) |  Loss2: (0.0000) | Acc: (90.00%) (4775/5248)
Epoch: 81 | Batch_idx: 50 |  Loss: (0.2592) |  Loss2: (0.0000) | Acc: (90.00%) (5936/6528)
Epoch: 81 | Batch_idx: 60 |  Loss: (0.2528) |  Loss2: (0.0000) | Acc: (91.00%) (7124/7808)
Epoch: 81 | Batch_idx: 70 |  Loss: (0.2558) |  Loss2: (0.0000) | Acc: (91.00%) (8288/9088)
Epoch: 81 | Batch_idx: 80 |  Loss: (0.2593) |  Loss2: (0.0000) | Acc: (91.00%) (9436/10368)
Epoch: 81 | Batch_idx: 90 |  Loss: (0.2593) |  Loss2: (0.0000) | Acc: (90.00%) (10598/11648)
Epoch: 81 | Batch_idx: 100 |  Loss: (0.2607) |  Loss2: (0.0000) | Acc: (90.00%) (11745/12928)
Epoch: 81 | Batch_idx: 110 |  Loss: (0.2657) |  Loss2: (0.0000) | Acc: (90.00%) (12889/14208)
Epoch: 81 | Batch_idx: 120 |  Loss: (0.2654) |  Loss2: (0.0000) | Acc: (90.00%) (14052/15488)
Epoch: 81 | Batch_idx: 130 |  Loss: (0.2643) |  Loss2: (0.0000) | Acc: (90.00%) (15216/16768)
Epoch: 81 | Batch_idx: 140 |  Loss: (0.2641) |  Loss2: (0.0000) | Acc: (90.00%) (16382/18048)
Epoch: 81 | Batch_idx: 150 |  Loss: (0.2643) |  Loss2: (0.0000) | Acc: (90.00%) (17549/19328)
Epoch: 81 | Batch_idx: 160 |  Loss: (0.2665) |  Loss2: (0.0000) | Acc: (90.00%) (18704/20608)
Epoch: 81 | Batch_idx: 170 |  Loss: (0.2660) |  Loss2: (0.0000) | Acc: (90.00%) (19870/21888)
Epoch: 81 | Batch_idx: 180 |  Loss: (0.2668) |  Loss2: (0.0000) | Acc: (90.00%) (21027/23168)
Epoch: 81 | Batch_idx: 190 |  Loss: (0.2662) |  Loss2: (0.0000) | Acc: (90.00%) (22194/24448)
Epoch: 81 | Batch_idx: 200 |  Loss: (0.2649) |  Loss2: (0.0000) | Acc: (90.00%) (23360/25728)
Epoch: 81 | Batch_idx: 210 |  Loss: (0.2644) |  Loss2: (0.0000) | Acc: (90.00%) (24534/27008)
Epoch: 81 | Batch_idx: 220 |  Loss: (0.2643) |  Loss2: (0.0000) | Acc: (90.00%) (25688/28288)
Epoch: 81 | Batch_idx: 230 |  Loss: (0.2647) |  Loss2: (0.0000) | Acc: (90.00%) (26851/29568)
Epoch: 81 | Batch_idx: 240 |  Loss: (0.2659) |  Loss2: (0.0000) | Acc: (90.00%) (27997/30848)
Epoch: 81 | Batch_idx: 250 |  Loss: (0.2661) |  Loss2: (0.0000) | Acc: (90.00%) (29158/32128)
Epoch: 81 | Batch_idx: 260 |  Loss: (0.2676) |  Loss2: (0.0000) | Acc: (90.00%) (30307/33408)
Epoch: 81 | Batch_idx: 270 |  Loss: (0.2681) |  Loss2: (0.0000) | Acc: (90.00%) (31467/34688)
Epoch: 81 | Batch_idx: 280 |  Loss: (0.2687) |  Loss2: (0.0000) | Acc: (90.00%) (32617/35968)
Epoch: 81 | Batch_idx: 290 |  Loss: (0.2687) |  Loss2: (0.0000) | Acc: (90.00%) (33783/37248)
Epoch: 81 | Batch_idx: 300 |  Loss: (0.2692) |  Loss2: (0.0000) | Acc: (90.00%) (34935/38528)
Epoch: 81 | Batch_idx: 310 |  Loss: (0.2708) |  Loss2: (0.0000) | Acc: (90.00%) (36071/39808)
Epoch: 81 | Batch_idx: 320 |  Loss: (0.2716) |  Loss2: (0.0000) | Acc: (90.00%) (37230/41088)
Epoch: 81 | Batch_idx: 330 |  Loss: (0.2723) |  Loss2: (0.0000) | Acc: (90.00%) (38371/42368)
Epoch: 81 | Batch_idx: 340 |  Loss: (0.2727) |  Loss2: (0.0000) | Acc: (90.00%) (39525/43648)
Epoch: 81 | Batch_idx: 350 |  Loss: (0.2727) |  Loss2: (0.0000) | Acc: (90.00%) (40681/44928)
Epoch: 81 | Batch_idx: 360 |  Loss: (0.2739) |  Loss2: (0.0000) | Acc: (90.00%) (41817/46208)
Epoch: 81 | Batch_idx: 370 |  Loss: (0.2738) |  Loss2: (0.0000) | Acc: (90.00%) (42973/47488)
Epoch: 81 | Batch_idx: 380 |  Loss: (0.2736) |  Loss2: (0.0000) | Acc: (90.00%) (44121/48768)
Epoch: 81 | Batch_idx: 390 |  Loss: (0.2736) |  Loss2: (0.0000) | Acc: (90.00%) (45228/50000)
# TEST : Loss: (0.5378) | Acc: (83.00%) (8391/10000)
percent tensor([0.5296, 0.4704], device='cuda:0')
percent tensor([0.4996, 0.5004], device='cuda:0')
percent tensor([0.5019, 0.4981], device='cuda:0')
percent tensor([0.5938, 0.4062], device='cuda:0')
percent tensor([0.5987, 0.4013], device='cuda:0')
percent tensor([0.6492, 0.3508], device='cuda:0')
percent tensor([0.6963, 0.3037], device='cuda:0')
percent tensor([0.9983, 0.0017], device='cuda:0')
Epoch: 82 | Batch_idx: 0 |  Loss: (0.2123) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 82 | Batch_idx: 10 |  Loss: (0.2683) |  Loss2: (0.0000) | Acc: (90.00%) (1273/1408)
Epoch: 82 | Batch_idx: 20 |  Loss: (0.2592) |  Loss2: (0.0000) | Acc: (90.00%) (2444/2688)
Epoch: 82 | Batch_idx: 30 |  Loss: (0.2682) |  Loss2: (0.0000) | Acc: (90.00%) (3587/3968)
Epoch: 82 | Batch_idx: 40 |  Loss: (0.2583) |  Loss2: (0.0000) | Acc: (90.00%) (4762/5248)
Epoch: 82 | Batch_idx: 50 |  Loss: (0.2624) |  Loss2: (0.0000) | Acc: (90.00%) (5911/6528)
Epoch: 82 | Batch_idx: 60 |  Loss: (0.2642) |  Loss2: (0.0000) | Acc: (90.00%) (7071/7808)
Epoch: 82 | Batch_idx: 70 |  Loss: (0.2645) |  Loss2: (0.0000) | Acc: (90.00%) (8237/9088)
Epoch: 82 | Batch_idx: 80 |  Loss: (0.2676) |  Loss2: (0.0000) | Acc: (90.00%) (9375/10368)
Epoch: 82 | Batch_idx: 90 |  Loss: (0.2686) |  Loss2: (0.0000) | Acc: (90.00%) (10537/11648)
Epoch: 82 | Batch_idx: 100 |  Loss: (0.2704) |  Loss2: (0.0000) | Acc: (90.00%) (11685/12928)
Epoch: 82 | Batch_idx: 110 |  Loss: (0.2691) |  Loss2: (0.0000) | Acc: (90.00%) (12861/14208)
Epoch: 82 | Batch_idx: 120 |  Loss: (0.2689) |  Loss2: (0.0000) | Acc: (90.00%) (14020/15488)
Epoch: 82 | Batch_idx: 130 |  Loss: (0.2687) |  Loss2: (0.0000) | Acc: (90.00%) (15189/16768)
Epoch: 82 | Batch_idx: 140 |  Loss: (0.2686) |  Loss2: (0.0000) | Acc: (90.00%) (16358/18048)
Epoch: 82 | Batch_idx: 150 |  Loss: (0.2683) |  Loss2: (0.0000) | Acc: (90.00%) (17535/19328)
Epoch: 82 | Batch_idx: 160 |  Loss: (0.2685) |  Loss2: (0.0000) | Acc: (90.00%) (18693/20608)
Epoch: 82 | Batch_idx: 170 |  Loss: (0.2674) |  Loss2: (0.0000) | Acc: (90.00%) (19862/21888)
Epoch: 82 | Batch_idx: 180 |  Loss: (0.2669) |  Loss2: (0.0000) | Acc: (90.00%) (21037/23168)
Epoch: 82 | Batch_idx: 190 |  Loss: (0.2661) |  Loss2: (0.0000) | Acc: (90.00%) (22210/24448)
Epoch: 82 | Batch_idx: 200 |  Loss: (0.2667) |  Loss2: (0.0000) | Acc: (90.00%) (23362/25728)
Epoch: 82 | Batch_idx: 210 |  Loss: (0.2663) |  Loss2: (0.0000) | Acc: (90.00%) (24529/27008)
Epoch: 82 | Batch_idx: 220 |  Loss: (0.2658) |  Loss2: (0.0000) | Acc: (90.00%) (25702/28288)
Epoch: 82 | Batch_idx: 230 |  Loss: (0.2664) |  Loss2: (0.0000) | Acc: (90.00%) (26856/29568)
Epoch: 82 | Batch_idx: 240 |  Loss: (0.2665) |  Loss2: (0.0000) | Acc: (90.00%) (28020/30848)
Epoch: 82 | Batch_idx: 250 |  Loss: (0.2663) |  Loss2: (0.0000) | Acc: (90.00%) (29178/32128)
Epoch: 82 | Batch_idx: 260 |  Loss: (0.2670) |  Loss2: (0.0000) | Acc: (90.00%) (30339/33408)
Epoch: 82 | Batch_idx: 270 |  Loss: (0.2669) |  Loss2: (0.0000) | Acc: (90.00%) (31511/34688)
Epoch: 82 | Batch_idx: 280 |  Loss: (0.2665) |  Loss2: (0.0000) | Acc: (90.00%) (32672/35968)
Epoch: 82 | Batch_idx: 290 |  Loss: (0.2657) |  Loss2: (0.0000) | Acc: (90.00%) (33845/37248)
Epoch: 82 | Batch_idx: 300 |  Loss: (0.2666) |  Loss2: (0.0000) | Acc: (90.00%) (35001/38528)
Epoch: 82 | Batch_idx: 310 |  Loss: (0.2673) |  Loss2: (0.0000) | Acc: (90.00%) (36162/39808)
Epoch: 82 | Batch_idx: 320 |  Loss: (0.2677) |  Loss2: (0.0000) | Acc: (90.00%) (37315/41088)
Epoch: 82 | Batch_idx: 330 |  Loss: (0.2689) |  Loss2: (0.0000) | Acc: (90.00%) (38451/42368)
Epoch: 82 | Batch_idx: 340 |  Loss: (0.2687) |  Loss2: (0.0000) | Acc: (90.00%) (39614/43648)
Epoch: 82 | Batch_idx: 350 |  Loss: (0.2683) |  Loss2: (0.0000) | Acc: (90.00%) (40782/44928)
Epoch: 82 | Batch_idx: 360 |  Loss: (0.2675) |  Loss2: (0.0000) | Acc: (90.00%) (41970/46208)
Epoch: 82 | Batch_idx: 370 |  Loss: (0.2666) |  Loss2: (0.0000) | Acc: (90.00%) (43143/47488)
Epoch: 82 | Batch_idx: 380 |  Loss: (0.2665) |  Loss2: (0.0000) | Acc: (90.00%) (44299/48768)
Epoch: 82 | Batch_idx: 390 |  Loss: (0.2664) |  Loss2: (0.0000) | Acc: (90.00%) (45421/50000)
# TEST : Loss: (0.4606) | Acc: (85.00%) (8535/10000)
percent tensor([0.5300, 0.4700], device='cuda:0')
percent tensor([0.4996, 0.5004], device='cuda:0')
percent tensor([0.5017, 0.4983], device='cuda:0')
percent tensor([0.5941, 0.4059], device='cuda:0')
percent tensor([0.5983, 0.4017], device='cuda:0')
percent tensor([0.6497, 0.3503], device='cuda:0')
percent tensor([0.6921, 0.3079], device='cuda:0')
percent tensor([0.9982, 0.0018], device='cuda:0')
Epoch: 83 | Batch_idx: 0 |  Loss: (0.2458) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 83 | Batch_idx: 10 |  Loss: (0.2668) |  Loss2: (0.0000) | Acc: (90.00%) (1281/1408)
Epoch: 83 | Batch_idx: 20 |  Loss: (0.2600) |  Loss2: (0.0000) | Acc: (91.00%) (2453/2688)
Epoch: 83 | Batch_idx: 30 |  Loss: (0.2665) |  Loss2: (0.0000) | Acc: (91.00%) (3615/3968)
Epoch: 83 | Batch_idx: 40 |  Loss: (0.2694) |  Loss2: (0.0000) | Acc: (90.00%) (4771/5248)
Epoch: 83 | Batch_idx: 50 |  Loss: (0.2641) |  Loss2: (0.0000) | Acc: (90.00%) (5938/6528)
Epoch: 83 | Batch_idx: 60 |  Loss: (0.2626) |  Loss2: (0.0000) | Acc: (90.00%) (7099/7808)
Epoch: 83 | Batch_idx: 70 |  Loss: (0.2621) |  Loss2: (0.0000) | Acc: (90.00%) (8261/9088)
Epoch: 83 | Batch_idx: 80 |  Loss: (0.2589) |  Loss2: (0.0000) | Acc: (91.00%) (9439/10368)
Epoch: 83 | Batch_idx: 90 |  Loss: (0.2580) |  Loss2: (0.0000) | Acc: (90.00%) (10598/11648)
Epoch: 83 | Batch_idx: 100 |  Loss: (0.2575) |  Loss2: (0.0000) | Acc: (90.00%) (11763/12928)
Epoch: 83 | Batch_idx: 110 |  Loss: (0.2568) |  Loss2: (0.0000) | Acc: (91.00%) (12934/14208)
Epoch: 83 | Batch_idx: 120 |  Loss: (0.2550) |  Loss2: (0.0000) | Acc: (91.00%) (14109/15488)
Epoch: 83 | Batch_idx: 130 |  Loss: (0.2572) |  Loss2: (0.0000) | Acc: (91.00%) (15263/16768)
Epoch: 83 | Batch_idx: 140 |  Loss: (0.2582) |  Loss2: (0.0000) | Acc: (91.00%) (16428/18048)
Epoch: 83 | Batch_idx: 150 |  Loss: (0.2590) |  Loss2: (0.0000) | Acc: (91.00%) (17589/19328)
Epoch: 83 | Batch_idx: 160 |  Loss: (0.2609) |  Loss2: (0.0000) | Acc: (90.00%) (18742/20608)
Epoch: 83 | Batch_idx: 170 |  Loss: (0.2616) |  Loss2: (0.0000) | Acc: (90.00%) (19886/21888)
Epoch: 83 | Batch_idx: 180 |  Loss: (0.2616) |  Loss2: (0.0000) | Acc: (90.00%) (21054/23168)
Epoch: 83 | Batch_idx: 190 |  Loss: (0.2611) |  Loss2: (0.0000) | Acc: (90.00%) (22221/24448)
Epoch: 83 | Batch_idx: 200 |  Loss: (0.2613) |  Loss2: (0.0000) | Acc: (90.00%) (23389/25728)
Epoch: 83 | Batch_idx: 210 |  Loss: (0.2605) |  Loss2: (0.0000) | Acc: (90.00%) (24569/27008)
Epoch: 83 | Batch_idx: 220 |  Loss: (0.2613) |  Loss2: (0.0000) | Acc: (90.00%) (25729/28288)
Epoch: 83 | Batch_idx: 230 |  Loss: (0.2612) |  Loss2: (0.0000) | Acc: (90.00%) (26886/29568)
Epoch: 83 | Batch_idx: 240 |  Loss: (0.2601) |  Loss2: (0.0000) | Acc: (90.00%) (28070/30848)
Epoch: 83 | Batch_idx: 250 |  Loss: (0.2596) |  Loss2: (0.0000) | Acc: (91.00%) (29245/32128)
Epoch: 83 | Batch_idx: 260 |  Loss: (0.2597) |  Loss2: (0.0000) | Acc: (91.00%) (30408/33408)
Epoch: 83 | Batch_idx: 270 |  Loss: (0.2588) |  Loss2: (0.0000) | Acc: (91.00%) (31585/34688)
Epoch: 83 | Batch_idx: 280 |  Loss: (0.2591) |  Loss2: (0.0000) | Acc: (91.00%) (32746/35968)
Epoch: 83 | Batch_idx: 290 |  Loss: (0.2592) |  Loss2: (0.0000) | Acc: (91.00%) (33896/37248)
Epoch: 83 | Batch_idx: 300 |  Loss: (0.2591) |  Loss2: (0.0000) | Acc: (90.00%) (35055/38528)
Epoch: 83 | Batch_idx: 310 |  Loss: (0.2604) |  Loss2: (0.0000) | Acc: (90.00%) (36203/39808)
Epoch: 83 | Batch_idx: 320 |  Loss: (0.2608) |  Loss2: (0.0000) | Acc: (90.00%) (37355/41088)
Epoch: 83 | Batch_idx: 330 |  Loss: (0.2615) |  Loss2: (0.0000) | Acc: (90.00%) (38511/42368)
Epoch: 83 | Batch_idx: 340 |  Loss: (0.2615) |  Loss2: (0.0000) | Acc: (90.00%) (39678/43648)
Epoch: 83 | Batch_idx: 350 |  Loss: (0.2622) |  Loss2: (0.0000) | Acc: (90.00%) (40827/44928)
Epoch: 83 | Batch_idx: 360 |  Loss: (0.2621) |  Loss2: (0.0000) | Acc: (90.00%) (41981/46208)
Epoch: 83 | Batch_idx: 370 |  Loss: (0.2633) |  Loss2: (0.0000) | Acc: (90.00%) (43130/47488)
Epoch: 83 | Batch_idx: 380 |  Loss: (0.2637) |  Loss2: (0.0000) | Acc: (90.00%) (44277/48768)
Epoch: 83 | Batch_idx: 390 |  Loss: (0.2639) |  Loss2: (0.0000) | Acc: (90.00%) (45388/50000)
# TEST : Loss: (0.4373) | Acc: (86.00%) (8637/10000)
percent tensor([0.5304, 0.4696], device='cuda:0')
percent tensor([0.4995, 0.5005], device='cuda:0')
percent tensor([0.5016, 0.4984], device='cuda:0')
percent tensor([0.5939, 0.4061], device='cuda:0')
percent tensor([0.5963, 0.4037], device='cuda:0')
percent tensor([0.6481, 0.3519], device='cuda:0')
percent tensor([0.6906, 0.3094], device='cuda:0')
percent tensor([0.9982, 0.0018], device='cuda:0')
Epoch: 84 | Batch_idx: 0 |  Loss: (0.2121) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 84 | Batch_idx: 10 |  Loss: (0.2449) |  Loss2: (0.0000) | Acc: (91.00%) (1293/1408)
Epoch: 84 | Batch_idx: 20 |  Loss: (0.2412) |  Loss2: (0.0000) | Acc: (91.00%) (2463/2688)
Epoch: 84 | Batch_idx: 30 |  Loss: (0.2516) |  Loss2: (0.0000) | Acc: (91.00%) (3625/3968)
Epoch: 84 | Batch_idx: 40 |  Loss: (0.2445) |  Loss2: (0.0000) | Acc: (91.00%) (4808/5248)
Epoch: 84 | Batch_idx: 50 |  Loss: (0.2441) |  Loss2: (0.0000) | Acc: (91.00%) (5976/6528)
Epoch: 84 | Batch_idx: 60 |  Loss: (0.2452) |  Loss2: (0.0000) | Acc: (91.00%) (7144/7808)
Epoch: 84 | Batch_idx: 70 |  Loss: (0.2473) |  Loss2: (0.0000) | Acc: (91.00%) (8312/9088)
Epoch: 84 | Batch_idx: 80 |  Loss: (0.2458) |  Loss2: (0.0000) | Acc: (91.00%) (9494/10368)
Epoch: 84 | Batch_idx: 90 |  Loss: (0.2444) |  Loss2: (0.0000) | Acc: (91.00%) (10665/11648)
Epoch: 84 | Batch_idx: 100 |  Loss: (0.2487) |  Loss2: (0.0000) | Acc: (91.00%) (11825/12928)
Epoch: 84 | Batch_idx: 110 |  Loss: (0.2494) |  Loss2: (0.0000) | Acc: (91.00%) (12981/14208)
Epoch: 84 | Batch_idx: 120 |  Loss: (0.2496) |  Loss2: (0.0000) | Acc: (91.00%) (14147/15488)
Epoch: 84 | Batch_idx: 130 |  Loss: (0.2507) |  Loss2: (0.0000) | Acc: (91.00%) (15312/16768)
Epoch: 84 | Batch_idx: 140 |  Loss: (0.2486) |  Loss2: (0.0000) | Acc: (91.00%) (16495/18048)
Epoch: 84 | Batch_idx: 150 |  Loss: (0.2486) |  Loss2: (0.0000) | Acc: (91.00%) (17670/19328)
Epoch: 84 | Batch_idx: 160 |  Loss: (0.2517) |  Loss2: (0.0000) | Acc: (91.00%) (18811/20608)
Epoch: 84 | Batch_idx: 170 |  Loss: (0.2523) |  Loss2: (0.0000) | Acc: (91.00%) (19976/21888)
Epoch: 84 | Batch_idx: 180 |  Loss: (0.2543) |  Loss2: (0.0000) | Acc: (91.00%) (21127/23168)
Epoch: 84 | Batch_idx: 190 |  Loss: (0.2543) |  Loss2: (0.0000) | Acc: (91.00%) (22291/24448)
Epoch: 84 | Batch_idx: 200 |  Loss: (0.2536) |  Loss2: (0.0000) | Acc: (91.00%) (23474/25728)
Epoch: 84 | Batch_idx: 210 |  Loss: (0.2522) |  Loss2: (0.0000) | Acc: (91.00%) (24655/27008)
Epoch: 84 | Batch_idx: 220 |  Loss: (0.2534) |  Loss2: (0.0000) | Acc: (91.00%) (25805/28288)
Epoch: 84 | Batch_idx: 230 |  Loss: (0.2529) |  Loss2: (0.0000) | Acc: (91.00%) (26987/29568)
Epoch: 84 | Batch_idx: 240 |  Loss: (0.2525) |  Loss2: (0.0000) | Acc: (91.00%) (28162/30848)
Epoch: 84 | Batch_idx: 250 |  Loss: (0.2517) |  Loss2: (0.0000) | Acc: (91.00%) (29338/32128)
Epoch: 84 | Batch_idx: 260 |  Loss: (0.2522) |  Loss2: (0.0000) | Acc: (91.00%) (30504/33408)
Epoch: 84 | Batch_idx: 270 |  Loss: (0.2523) |  Loss2: (0.0000) | Acc: (91.00%) (31663/34688)
Epoch: 84 | Batch_idx: 280 |  Loss: (0.2531) |  Loss2: (0.0000) | Acc: (91.00%) (32823/35968)
Epoch: 84 | Batch_idx: 290 |  Loss: (0.2532) |  Loss2: (0.0000) | Acc: (91.00%) (33985/37248)
Epoch: 84 | Batch_idx: 300 |  Loss: (0.2528) |  Loss2: (0.0000) | Acc: (91.00%) (35166/38528)
Epoch: 84 | Batch_idx: 310 |  Loss: (0.2523) |  Loss2: (0.0000) | Acc: (91.00%) (36330/39808)
Epoch: 84 | Batch_idx: 320 |  Loss: (0.2521) |  Loss2: (0.0000) | Acc: (91.00%) (37502/41088)
Epoch: 84 | Batch_idx: 330 |  Loss: (0.2522) |  Loss2: (0.0000) | Acc: (91.00%) (38674/42368)
Epoch: 84 | Batch_idx: 340 |  Loss: (0.2523) |  Loss2: (0.0000) | Acc: (91.00%) (39842/43648)
Epoch: 84 | Batch_idx: 350 |  Loss: (0.2522) |  Loss2: (0.0000) | Acc: (91.00%) (41012/44928)
Epoch: 84 | Batch_idx: 360 |  Loss: (0.2533) |  Loss2: (0.0000) | Acc: (91.00%) (42167/46208)
Epoch: 84 | Batch_idx: 370 |  Loss: (0.2531) |  Loss2: (0.0000) | Acc: (91.00%) (43338/47488)
Epoch: 84 | Batch_idx: 380 |  Loss: (0.2527) |  Loss2: (0.0000) | Acc: (91.00%) (44528/48768)
Epoch: 84 | Batch_idx: 390 |  Loss: (0.2528) |  Loss2: (0.0000) | Acc: (91.00%) (45648/50000)
# TEST : Loss: (0.4373) | Acc: (86.00%) (8622/10000)
percent tensor([0.5298, 0.4702], device='cuda:0')
percent tensor([0.4996, 0.5004], device='cuda:0')
percent tensor([0.5012, 0.4988], device='cuda:0')
percent tensor([0.5930, 0.4070], device='cuda:0')
percent tensor([0.5967, 0.4033], device='cuda:0')
percent tensor([0.6498, 0.3502], device='cuda:0')
percent tensor([0.6905, 0.3095], device='cuda:0')
percent tensor([0.9981, 0.0019], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 85 | Batch_idx: 0 |  Loss: (0.1606) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 85 | Batch_idx: 10 |  Loss: (0.2475) |  Loss2: (0.0000) | Acc: (91.00%) (1288/1408)
Epoch: 85 | Batch_idx: 20 |  Loss: (0.2598) |  Loss2: (0.0000) | Acc: (90.00%) (2444/2688)
Epoch: 85 | Batch_idx: 30 |  Loss: (0.2713) |  Loss2: (0.0000) | Acc: (90.00%) (3598/3968)
Epoch: 85 | Batch_idx: 40 |  Loss: (0.2735) |  Loss2: (0.0000) | Acc: (90.00%) (4755/5248)
Epoch: 85 | Batch_idx: 50 |  Loss: (0.2768) |  Loss2: (0.0000) | Acc: (90.00%) (5906/6528)
Epoch: 85 | Batch_idx: 60 |  Loss: (0.2691) |  Loss2: (0.0000) | Acc: (90.00%) (7081/7808)
Epoch: 85 | Batch_idx: 70 |  Loss: (0.2711) |  Loss2: (0.0000) | Acc: (90.00%) (8232/9088)
Epoch: 85 | Batch_idx: 80 |  Loss: (0.2705) |  Loss2: (0.0000) | Acc: (90.00%) (9393/10368)
Epoch: 85 | Batch_idx: 90 |  Loss: (0.2688) |  Loss2: (0.0000) | Acc: (90.00%) (10567/11648)
Epoch: 85 | Batch_idx: 100 |  Loss: (0.2705) |  Loss2: (0.0000) | Acc: (90.00%) (11714/12928)
Epoch: 85 | Batch_idx: 110 |  Loss: (0.2681) |  Loss2: (0.0000) | Acc: (90.00%) (12882/14208)
Epoch: 85 | Batch_idx: 120 |  Loss: (0.2704) |  Loss2: (0.0000) | Acc: (90.00%) (14035/15488)
Epoch: 85 | Batch_idx: 130 |  Loss: (0.2685) |  Loss2: (0.0000) | Acc: (90.00%) (15208/16768)
Epoch: 85 | Batch_idx: 140 |  Loss: (0.2702) |  Loss2: (0.0000) | Acc: (90.00%) (16351/18048)
Epoch: 85 | Batch_idx: 150 |  Loss: (0.2702) |  Loss2: (0.0000) | Acc: (90.00%) (17498/19328)
Epoch: 85 | Batch_idx: 160 |  Loss: (0.2725) |  Loss2: (0.0000) | Acc: (90.00%) (18638/20608)
Epoch: 85 | Batch_idx: 170 |  Loss: (0.2743) |  Loss2: (0.0000) | Acc: (90.00%) (19783/21888)
Epoch: 85 | Batch_idx: 180 |  Loss: (0.2737) |  Loss2: (0.0000) | Acc: (90.00%) (20955/23168)
Epoch: 85 | Batch_idx: 190 |  Loss: (0.2715) |  Loss2: (0.0000) | Acc: (90.00%) (22135/24448)
Epoch: 85 | Batch_idx: 200 |  Loss: (0.2730) |  Loss2: (0.0000) | Acc: (90.00%) (23282/25728)
Epoch: 85 | Batch_idx: 210 |  Loss: (0.2716) |  Loss2: (0.0000) | Acc: (90.00%) (24463/27008)
Epoch: 85 | Batch_idx: 220 |  Loss: (0.2701) |  Loss2: (0.0000) | Acc: (90.00%) (25641/28288)
Epoch: 85 | Batch_idx: 230 |  Loss: (0.2705) |  Loss2: (0.0000) | Acc: (90.00%) (26791/29568)
Epoch: 85 | Batch_idx: 240 |  Loss: (0.2705) |  Loss2: (0.0000) | Acc: (90.00%) (27948/30848)
Epoch: 85 | Batch_idx: 250 |  Loss: (0.2711) |  Loss2: (0.0000) | Acc: (90.00%) (29107/32128)
Epoch: 85 | Batch_idx: 260 |  Loss: (0.2709) |  Loss2: (0.0000) | Acc: (90.00%) (30270/33408)
Epoch: 85 | Batch_idx: 270 |  Loss: (0.2700) |  Loss2: (0.0000) | Acc: (90.00%) (31445/34688)
Epoch: 85 | Batch_idx: 280 |  Loss: (0.2691) |  Loss2: (0.0000) | Acc: (90.00%) (32621/35968)
Epoch: 85 | Batch_idx: 290 |  Loss: (0.2682) |  Loss2: (0.0000) | Acc: (90.00%) (33782/37248)
Epoch: 85 | Batch_idx: 300 |  Loss: (0.2674) |  Loss2: (0.0000) | Acc: (90.00%) (34953/38528)
Epoch: 85 | Batch_idx: 310 |  Loss: (0.2673) |  Loss2: (0.0000) | Acc: (90.00%) (36121/39808)
Epoch: 85 | Batch_idx: 320 |  Loss: (0.2663) |  Loss2: (0.0000) | Acc: (90.00%) (37291/41088)
Epoch: 85 | Batch_idx: 330 |  Loss: (0.2661) |  Loss2: (0.0000) | Acc: (90.00%) (38448/42368)
Epoch: 85 | Batch_idx: 340 |  Loss: (0.2653) |  Loss2: (0.0000) | Acc: (90.00%) (39620/43648)
Epoch: 85 | Batch_idx: 350 |  Loss: (0.2650) |  Loss2: (0.0000) | Acc: (90.00%) (40789/44928)
Epoch: 85 | Batch_idx: 360 |  Loss: (0.2645) |  Loss2: (0.0000) | Acc: (90.00%) (41976/46208)
Epoch: 85 | Batch_idx: 370 |  Loss: (0.2635) |  Loss2: (0.0000) | Acc: (90.00%) (43156/47488)
Epoch: 85 | Batch_idx: 380 |  Loss: (0.2638) |  Loss2: (0.0000) | Acc: (90.00%) (44316/48768)
Epoch: 85 | Batch_idx: 390 |  Loss: (0.2634) |  Loss2: (0.0000) | Acc: (90.00%) (45443/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_085.pth.tar'
# TEST : Loss: (0.4279) | Acc: (86.00%) (8608/10000)
percent tensor([0.5331, 0.4669], device='cuda:0')
percent tensor([0.4952, 0.5048], device='cuda:0')
percent tensor([0.5056, 0.4944], device='cuda:0')
percent tensor([0.5975, 0.4025], device='cuda:0')
percent tensor([0.6161, 0.3839], device='cuda:0')
percent tensor([0.6416, 0.3584], device='cuda:0')
percent tensor([0.6543, 0.3457], device='cuda:0')
percent tensor([0.9981, 0.0019], device='cuda:0')
Epoch: 86 | Batch_idx: 0 |  Loss: (0.2507) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 86 | Batch_idx: 10 |  Loss: (0.2741) |  Loss2: (0.0000) | Acc: (90.00%) (1269/1408)
Epoch: 86 | Batch_idx: 20 |  Loss: (0.2610) |  Loss2: (0.0000) | Acc: (90.00%) (2445/2688)
Epoch: 86 | Batch_idx: 30 |  Loss: (0.2649) |  Loss2: (0.0000) | Acc: (90.00%) (3607/3968)
Epoch: 86 | Batch_idx: 40 |  Loss: (0.2614) |  Loss2: (0.0000) | Acc: (90.00%) (4773/5248)
Epoch: 86 | Batch_idx: 50 |  Loss: (0.2590) |  Loss2: (0.0000) | Acc: (90.00%) (5938/6528)
Epoch: 86 | Batch_idx: 60 |  Loss: (0.2616) |  Loss2: (0.0000) | Acc: (90.00%) (7102/7808)
Epoch: 86 | Batch_idx: 70 |  Loss: (0.2625) |  Loss2: (0.0000) | Acc: (90.00%) (8257/9088)
Epoch: 86 | Batch_idx: 80 |  Loss: (0.2595) |  Loss2: (0.0000) | Acc: (91.00%) (9439/10368)
Epoch: 86 | Batch_idx: 90 |  Loss: (0.2567) |  Loss2: (0.0000) | Acc: (91.00%) (10623/11648)
Epoch: 86 | Batch_idx: 100 |  Loss: (0.2590) |  Loss2: (0.0000) | Acc: (91.00%) (11774/12928)
Epoch: 86 | Batch_idx: 110 |  Loss: (0.2582) |  Loss2: (0.0000) | Acc: (91.00%) (12942/14208)
Epoch: 86 | Batch_idx: 120 |  Loss: (0.2567) |  Loss2: (0.0000) | Acc: (91.00%) (14119/15488)
Epoch: 86 | Batch_idx: 130 |  Loss: (0.2557) |  Loss2: (0.0000) | Acc: (91.00%) (15286/16768)
Epoch: 86 | Batch_idx: 140 |  Loss: (0.2554) |  Loss2: (0.0000) | Acc: (91.00%) (16453/18048)
Epoch: 86 | Batch_idx: 150 |  Loss: (0.2569) |  Loss2: (0.0000) | Acc: (91.00%) (17598/19328)
Epoch: 86 | Batch_idx: 160 |  Loss: (0.2554) |  Loss2: (0.0000) | Acc: (91.00%) (18782/20608)
Epoch: 86 | Batch_idx: 170 |  Loss: (0.2550) |  Loss2: (0.0000) | Acc: (91.00%) (19951/21888)
Epoch: 86 | Batch_idx: 180 |  Loss: (0.2564) |  Loss2: (0.0000) | Acc: (91.00%) (21112/23168)
Epoch: 86 | Batch_idx: 190 |  Loss: (0.2575) |  Loss2: (0.0000) | Acc: (91.00%) (22269/24448)
Epoch: 86 | Batch_idx: 200 |  Loss: (0.2577) |  Loss2: (0.0000) | Acc: (91.00%) (23420/25728)
Epoch: 86 | Batch_idx: 210 |  Loss: (0.2578) |  Loss2: (0.0000) | Acc: (91.00%) (24585/27008)
Epoch: 86 | Batch_idx: 220 |  Loss: (0.2568) |  Loss2: (0.0000) | Acc: (91.00%) (25758/28288)
Epoch: 86 | Batch_idx: 230 |  Loss: (0.2561) |  Loss2: (0.0000) | Acc: (91.00%) (26930/29568)
Epoch: 86 | Batch_idx: 240 |  Loss: (0.2569) |  Loss2: (0.0000) | Acc: (91.00%) (28097/30848)
Epoch: 86 | Batch_idx: 250 |  Loss: (0.2567) |  Loss2: (0.0000) | Acc: (91.00%) (29257/32128)
Epoch: 86 | Batch_idx: 260 |  Loss: (0.2569) |  Loss2: (0.0000) | Acc: (91.00%) (30423/33408)
Epoch: 86 | Batch_idx: 270 |  Loss: (0.2561) |  Loss2: (0.0000) | Acc: (91.00%) (31599/34688)
Epoch: 86 | Batch_idx: 280 |  Loss: (0.2560) |  Loss2: (0.0000) | Acc: (91.00%) (32762/35968)
Epoch: 86 | Batch_idx: 290 |  Loss: (0.2553) |  Loss2: (0.0000) | Acc: (91.00%) (33937/37248)
Epoch: 86 | Batch_idx: 300 |  Loss: (0.2544) |  Loss2: (0.0000) | Acc: (91.00%) (35113/38528)
Epoch: 86 | Batch_idx: 310 |  Loss: (0.2544) |  Loss2: (0.0000) | Acc: (91.00%) (36279/39808)
Epoch: 86 | Batch_idx: 320 |  Loss: (0.2531) |  Loss2: (0.0000) | Acc: (91.00%) (37471/41088)
Epoch: 86 | Batch_idx: 330 |  Loss: (0.2531) |  Loss2: (0.0000) | Acc: (91.00%) (38631/42368)
Epoch: 86 | Batch_idx: 340 |  Loss: (0.2538) |  Loss2: (0.0000) | Acc: (91.00%) (39790/43648)
Epoch: 86 | Batch_idx: 350 |  Loss: (0.2531) |  Loss2: (0.0000) | Acc: (91.00%) (40970/44928)
Epoch: 86 | Batch_idx: 360 |  Loss: (0.2533) |  Loss2: (0.0000) | Acc: (91.00%) (42141/46208)
Epoch: 86 | Batch_idx: 370 |  Loss: (0.2533) |  Loss2: (0.0000) | Acc: (91.00%) (43314/47488)
Epoch: 86 | Batch_idx: 380 |  Loss: (0.2529) |  Loss2: (0.0000) | Acc: (91.00%) (44490/48768)
Epoch: 86 | Batch_idx: 390 |  Loss: (0.2521) |  Loss2: (0.0000) | Acc: (91.00%) (45630/50000)
# TEST : Loss: (0.4159) | Acc: (86.00%) (8640/10000)
percent tensor([0.5338, 0.4662], device='cuda:0')
percent tensor([0.4935, 0.5065], device='cuda:0')
percent tensor([0.5071, 0.4929], device='cuda:0')
percent tensor([0.6026, 0.3974], device='cuda:0')
percent tensor([0.6118, 0.3882], device='cuda:0')
percent tensor([0.6461, 0.3539], device='cuda:0')
percent tensor([0.6542, 0.3458], device='cuda:0')
percent tensor([0.9982, 0.0018], device='cuda:0')
Epoch: 87 | Batch_idx: 0 |  Loss: (0.2467) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 87 | Batch_idx: 10 |  Loss: (0.2233) |  Loss2: (0.0000) | Acc: (92.00%) (1298/1408)
Epoch: 87 | Batch_idx: 20 |  Loss: (0.2445) |  Loss2: (0.0000) | Acc: (91.00%) (2461/2688)
Epoch: 87 | Batch_idx: 30 |  Loss: (0.2441) |  Loss2: (0.0000) | Acc: (91.00%) (3638/3968)
Epoch: 87 | Batch_idx: 40 |  Loss: (0.2380) |  Loss2: (0.0000) | Acc: (91.00%) (4819/5248)
Epoch: 87 | Batch_idx: 50 |  Loss: (0.2366) |  Loss2: (0.0000) | Acc: (91.00%) (5999/6528)
Epoch: 87 | Batch_idx: 60 |  Loss: (0.2357) |  Loss2: (0.0000) | Acc: (92.00%) (7185/7808)
Epoch: 87 | Batch_idx: 70 |  Loss: (0.2361) |  Loss2: (0.0000) | Acc: (91.00%) (8356/9088)
Epoch: 87 | Batch_idx: 80 |  Loss: (0.2351) |  Loss2: (0.0000) | Acc: (91.00%) (9532/10368)
Epoch: 87 | Batch_idx: 90 |  Loss: (0.2390) |  Loss2: (0.0000) | Acc: (91.00%) (10695/11648)
Epoch: 87 | Batch_idx: 100 |  Loss: (0.2397) |  Loss2: (0.0000) | Acc: (91.00%) (11871/12928)
Epoch: 87 | Batch_idx: 110 |  Loss: (0.2416) |  Loss2: (0.0000) | Acc: (91.00%) (13040/14208)
Epoch: 87 | Batch_idx: 120 |  Loss: (0.2425) |  Loss2: (0.0000) | Acc: (91.00%) (14219/15488)
Epoch: 87 | Batch_idx: 130 |  Loss: (0.2405) |  Loss2: (0.0000) | Acc: (91.00%) (15405/16768)
Epoch: 87 | Batch_idx: 140 |  Loss: (0.2401) |  Loss2: (0.0000) | Acc: (91.00%) (16591/18048)
Epoch: 87 | Batch_idx: 150 |  Loss: (0.2409) |  Loss2: (0.0000) | Acc: (91.00%) (17756/19328)
Epoch: 87 | Batch_idx: 160 |  Loss: (0.2420) |  Loss2: (0.0000) | Acc: (91.00%) (18924/20608)
Epoch: 87 | Batch_idx: 170 |  Loss: (0.2409) |  Loss2: (0.0000) | Acc: (91.00%) (20106/21888)
Epoch: 87 | Batch_idx: 180 |  Loss: (0.2423) |  Loss2: (0.0000) | Acc: (91.00%) (21269/23168)
Epoch: 87 | Batch_idx: 190 |  Loss: (0.2424) |  Loss2: (0.0000) | Acc: (91.00%) (22447/24448)
Epoch: 87 | Batch_idx: 200 |  Loss: (0.2434) |  Loss2: (0.0000) | Acc: (91.00%) (23605/25728)
Epoch: 87 | Batch_idx: 210 |  Loss: (0.2432) |  Loss2: (0.0000) | Acc: (91.00%) (24779/27008)
Epoch: 87 | Batch_idx: 220 |  Loss: (0.2427) |  Loss2: (0.0000) | Acc: (91.00%) (25959/28288)
Epoch: 87 | Batch_idx: 230 |  Loss: (0.2433) |  Loss2: (0.0000) | Acc: (91.00%) (27135/29568)
Epoch: 87 | Batch_idx: 240 |  Loss: (0.2423) |  Loss2: (0.0000) | Acc: (91.00%) (28331/30848)
Epoch: 87 | Batch_idx: 250 |  Loss: (0.2430) |  Loss2: (0.0000) | Acc: (91.00%) (29504/32128)
Epoch: 87 | Batch_idx: 260 |  Loss: (0.2428) |  Loss2: (0.0000) | Acc: (91.00%) (30687/33408)
Epoch: 87 | Batch_idx: 270 |  Loss: (0.2438) |  Loss2: (0.0000) | Acc: (91.00%) (31848/34688)
Epoch: 87 | Batch_idx: 280 |  Loss: (0.2437) |  Loss2: (0.0000) | Acc: (91.00%) (33020/35968)
Epoch: 87 | Batch_idx: 290 |  Loss: (0.2429) |  Loss2: (0.0000) | Acc: (91.00%) (34206/37248)
Epoch: 87 | Batch_idx: 300 |  Loss: (0.2423) |  Loss2: (0.0000) | Acc: (91.00%) (35392/38528)
Epoch: 87 | Batch_idx: 310 |  Loss: (0.2431) |  Loss2: (0.0000) | Acc: (91.00%) (36555/39808)
Epoch: 87 | Batch_idx: 320 |  Loss: (0.2427) |  Loss2: (0.0000) | Acc: (91.00%) (37734/41088)
Epoch: 87 | Batch_idx: 330 |  Loss: (0.2425) |  Loss2: (0.0000) | Acc: (91.00%) (38910/42368)
Epoch: 87 | Batch_idx: 340 |  Loss: (0.2422) |  Loss2: (0.0000) | Acc: (91.00%) (40089/43648)
Epoch: 87 | Batch_idx: 350 |  Loss: (0.2413) |  Loss2: (0.0000) | Acc: (91.00%) (41276/44928)
Epoch: 87 | Batch_idx: 360 |  Loss: (0.2404) |  Loss2: (0.0000) | Acc: (91.00%) (42467/46208)
Epoch: 87 | Batch_idx: 370 |  Loss: (0.2404) |  Loss2: (0.0000) | Acc: (91.00%) (43648/47488)
Epoch: 87 | Batch_idx: 380 |  Loss: (0.2416) |  Loss2: (0.0000) | Acc: (91.00%) (44807/48768)
Epoch: 87 | Batch_idx: 390 |  Loss: (0.2420) |  Loss2: (0.0000) | Acc: (91.00%) (45936/50000)
# TEST : Loss: (0.4064) | Acc: (86.00%) (8692/10000)
percent tensor([0.5333, 0.4667], device='cuda:0')
percent tensor([0.4928, 0.5072], device='cuda:0')
percent tensor([0.5072, 0.4928], device='cuda:0')
percent tensor([0.5998, 0.4002], device='cuda:0')
percent tensor([0.6157, 0.3843], device='cuda:0')
percent tensor([0.6485, 0.3515], device='cuda:0')
percent tensor([0.6529, 0.3471], device='cuda:0')
percent tensor([0.9984, 0.0016], device='cuda:0')
Epoch: 88 | Batch_idx: 0 |  Loss: (0.2367) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 88 | Batch_idx: 10 |  Loss: (0.2506) |  Loss2: (0.0000) | Acc: (92.00%) (1300/1408)
Epoch: 88 | Batch_idx: 20 |  Loss: (0.2623) |  Loss2: (0.0000) | Acc: (91.00%) (2452/2688)
Epoch: 88 | Batch_idx: 30 |  Loss: (0.2560) |  Loss2: (0.0000) | Acc: (91.00%) (3623/3968)
Epoch: 88 | Batch_idx: 40 |  Loss: (0.2530) |  Loss2: (0.0000) | Acc: (91.00%) (4810/5248)
Epoch: 88 | Batch_idx: 50 |  Loss: (0.2455) |  Loss2: (0.0000) | Acc: (91.00%) (6002/6528)
Epoch: 88 | Batch_idx: 60 |  Loss: (0.2434) |  Loss2: (0.0000) | Acc: (91.00%) (7176/7808)
Epoch: 88 | Batch_idx: 70 |  Loss: (0.2428) |  Loss2: (0.0000) | Acc: (91.00%) (8353/9088)
Epoch: 88 | Batch_idx: 80 |  Loss: (0.2418) |  Loss2: (0.0000) | Acc: (91.00%) (9529/10368)
Epoch: 88 | Batch_idx: 90 |  Loss: (0.2421) |  Loss2: (0.0000) | Acc: (91.00%) (10702/11648)
Epoch: 88 | Batch_idx: 100 |  Loss: (0.2417) |  Loss2: (0.0000) | Acc: (91.00%) (11884/12928)
Epoch: 88 | Batch_idx: 110 |  Loss: (0.2440) |  Loss2: (0.0000) | Acc: (91.00%) (13049/14208)
Epoch: 88 | Batch_idx: 120 |  Loss: (0.2446) |  Loss2: (0.0000) | Acc: (91.00%) (14226/15488)
Epoch: 88 | Batch_idx: 130 |  Loss: (0.2482) |  Loss2: (0.0000) | Acc: (91.00%) (15376/16768)
Epoch: 88 | Batch_idx: 140 |  Loss: (0.2447) |  Loss2: (0.0000) | Acc: (91.00%) (16577/18048)
Epoch: 88 | Batch_idx: 150 |  Loss: (0.2432) |  Loss2: (0.0000) | Acc: (91.00%) (17761/19328)
Epoch: 88 | Batch_idx: 160 |  Loss: (0.2426) |  Loss2: (0.0000) | Acc: (91.00%) (18948/20608)
Epoch: 88 | Batch_idx: 170 |  Loss: (0.2418) |  Loss2: (0.0000) | Acc: (91.00%) (20127/21888)
Epoch: 88 | Batch_idx: 180 |  Loss: (0.2409) |  Loss2: (0.0000) | Acc: (91.00%) (21304/23168)
Epoch: 88 | Batch_idx: 190 |  Loss: (0.2409) |  Loss2: (0.0000) | Acc: (91.00%) (22483/24448)
Epoch: 88 | Batch_idx: 200 |  Loss: (0.2418) |  Loss2: (0.0000) | Acc: (91.00%) (23653/25728)
Epoch: 88 | Batch_idx: 210 |  Loss: (0.2427) |  Loss2: (0.0000) | Acc: (91.00%) (24811/27008)
Epoch: 88 | Batch_idx: 220 |  Loss: (0.2423) |  Loss2: (0.0000) | Acc: (91.00%) (25995/28288)
Epoch: 88 | Batch_idx: 230 |  Loss: (0.2414) |  Loss2: (0.0000) | Acc: (91.00%) (27178/29568)
Epoch: 88 | Batch_idx: 240 |  Loss: (0.2402) |  Loss2: (0.0000) | Acc: (91.00%) (28364/30848)
Epoch: 88 | Batch_idx: 250 |  Loss: (0.2400) |  Loss2: (0.0000) | Acc: (91.00%) (29538/32128)
Epoch: 88 | Batch_idx: 260 |  Loss: (0.2405) |  Loss2: (0.0000) | Acc: (91.00%) (30707/33408)
Epoch: 88 | Batch_idx: 270 |  Loss: (0.2404) |  Loss2: (0.0000) | Acc: (91.00%) (31891/34688)
Epoch: 88 | Batch_idx: 280 |  Loss: (0.2413) |  Loss2: (0.0000) | Acc: (91.00%) (33054/35968)
Epoch: 88 | Batch_idx: 290 |  Loss: (0.2405) |  Loss2: (0.0000) | Acc: (91.00%) (34249/37248)
Epoch: 88 | Batch_idx: 300 |  Loss: (0.2409) |  Loss2: (0.0000) | Acc: (91.00%) (35420/38528)
Epoch: 88 | Batch_idx: 310 |  Loss: (0.2410) |  Loss2: (0.0000) | Acc: (91.00%) (36597/39808)
Epoch: 88 | Batch_idx: 320 |  Loss: (0.2414) |  Loss2: (0.0000) | Acc: (91.00%) (37768/41088)
Epoch: 88 | Batch_idx: 330 |  Loss: (0.2410) |  Loss2: (0.0000) | Acc: (91.00%) (38936/42368)
Epoch: 88 | Batch_idx: 340 |  Loss: (0.2412) |  Loss2: (0.0000) | Acc: (91.00%) (40112/43648)
Epoch: 88 | Batch_idx: 350 |  Loss: (0.2402) |  Loss2: (0.0000) | Acc: (91.00%) (41303/44928)
Epoch: 88 | Batch_idx: 360 |  Loss: (0.2396) |  Loss2: (0.0000) | Acc: (91.00%) (42483/46208)
Epoch: 88 | Batch_idx: 370 |  Loss: (0.2395) |  Loss2: (0.0000) | Acc: (91.00%) (43656/47488)
Epoch: 88 | Batch_idx: 380 |  Loss: (0.2394) |  Loss2: (0.0000) | Acc: (91.00%) (44828/48768)
Epoch: 88 | Batch_idx: 390 |  Loss: (0.2392) |  Loss2: (0.0000) | Acc: (91.00%) (45963/50000)
# TEST : Loss: (0.3988) | Acc: (87.00%) (8706/10000)
percent tensor([0.5317, 0.4683], device='cuda:0')
percent tensor([0.4924, 0.5076], device='cuda:0')
percent tensor([0.5064, 0.4936], device='cuda:0')
percent tensor([0.6047, 0.3953], device='cuda:0')
percent tensor([0.6143, 0.3857], device='cuda:0')
percent tensor([0.6535, 0.3465], device='cuda:0')
percent tensor([0.6626, 0.3374], device='cuda:0')
percent tensor([0.9984, 0.0016], device='cuda:0')
Epoch: 89 | Batch_idx: 0 |  Loss: (0.1685) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 89 | Batch_idx: 10 |  Loss: (0.2131) |  Loss2: (0.0000) | Acc: (93.00%) (1314/1408)
Epoch: 89 | Batch_idx: 20 |  Loss: (0.2321) |  Loss2: (0.0000) | Acc: (92.00%) (2483/2688)
Epoch: 89 | Batch_idx: 30 |  Loss: (0.2285) |  Loss2: (0.0000) | Acc: (92.00%) (3663/3968)
Epoch: 89 | Batch_idx: 40 |  Loss: (0.2271) |  Loss2: (0.0000) | Acc: (92.00%) (4839/5248)
Epoch: 89 | Batch_idx: 50 |  Loss: (0.2268) |  Loss2: (0.0000) | Acc: (92.00%) (6028/6528)
Epoch: 89 | Batch_idx: 60 |  Loss: (0.2264) |  Loss2: (0.0000) | Acc: (92.00%) (7210/7808)
Epoch: 89 | Batch_idx: 70 |  Loss: (0.2274) |  Loss2: (0.0000) | Acc: (92.00%) (8394/9088)
Epoch: 89 | Batch_idx: 80 |  Loss: (0.2290) |  Loss2: (0.0000) | Acc: (92.00%) (9564/10368)
Epoch: 89 | Batch_idx: 90 |  Loss: (0.2267) |  Loss2: (0.0000) | Acc: (92.00%) (10760/11648)
Epoch: 89 | Batch_idx: 100 |  Loss: (0.2255) |  Loss2: (0.0000) | Acc: (92.00%) (11958/12928)
Epoch: 89 | Batch_idx: 110 |  Loss: (0.2273) |  Loss2: (0.0000) | Acc: (92.00%) (13127/14208)
Epoch: 89 | Batch_idx: 120 |  Loss: (0.2270) |  Loss2: (0.0000) | Acc: (92.00%) (14307/15488)
Epoch: 89 | Batch_idx: 130 |  Loss: (0.2302) |  Loss2: (0.0000) | Acc: (92.00%) (15450/16768)
Epoch: 89 | Batch_idx: 140 |  Loss: (0.2303) |  Loss2: (0.0000) | Acc: (92.00%) (16615/18048)
Epoch: 89 | Batch_idx: 150 |  Loss: (0.2296) |  Loss2: (0.0000) | Acc: (92.00%) (17795/19328)
Epoch: 89 | Batch_idx: 160 |  Loss: (0.2311) |  Loss2: (0.0000) | Acc: (91.00%) (18956/20608)
Epoch: 89 | Batch_idx: 170 |  Loss: (0.2314) |  Loss2: (0.0000) | Acc: (91.00%) (20134/21888)
Epoch: 89 | Batch_idx: 180 |  Loss: (0.2321) |  Loss2: (0.0000) | Acc: (91.00%) (21300/23168)
Epoch: 89 | Batch_idx: 190 |  Loss: (0.2338) |  Loss2: (0.0000) | Acc: (91.00%) (22467/24448)
Epoch: 89 | Batch_idx: 200 |  Loss: (0.2349) |  Loss2: (0.0000) | Acc: (91.00%) (23623/25728)
Epoch: 89 | Batch_idx: 210 |  Loss: (0.2341) |  Loss2: (0.0000) | Acc: (91.00%) (24803/27008)
Epoch: 89 | Batch_idx: 220 |  Loss: (0.2351) |  Loss2: (0.0000) | Acc: (91.00%) (25972/28288)
Epoch: 89 | Batch_idx: 230 |  Loss: (0.2339) |  Loss2: (0.0000) | Acc: (91.00%) (27164/29568)
Epoch: 89 | Batch_idx: 240 |  Loss: (0.2339) |  Loss2: (0.0000) | Acc: (91.00%) (28336/30848)
Epoch: 89 | Batch_idx: 250 |  Loss: (0.2338) |  Loss2: (0.0000) | Acc: (91.00%) (29513/32128)
Epoch: 89 | Batch_idx: 260 |  Loss: (0.2346) |  Loss2: (0.0000) | Acc: (91.00%) (30676/33408)
Epoch: 89 | Batch_idx: 270 |  Loss: (0.2338) |  Loss2: (0.0000) | Acc: (91.00%) (31870/34688)
Epoch: 89 | Batch_idx: 280 |  Loss: (0.2326) |  Loss2: (0.0000) | Acc: (91.00%) (33065/35968)
Epoch: 89 | Batch_idx: 290 |  Loss: (0.2322) |  Loss2: (0.0000) | Acc: (91.00%) (34239/37248)
Epoch: 89 | Batch_idx: 300 |  Loss: (0.2318) |  Loss2: (0.0000) | Acc: (91.00%) (35425/38528)
Epoch: 89 | Batch_idx: 310 |  Loss: (0.2322) |  Loss2: (0.0000) | Acc: (91.00%) (36601/39808)
Epoch: 89 | Batch_idx: 320 |  Loss: (0.2331) |  Loss2: (0.0000) | Acc: (91.00%) (37766/41088)
Epoch: 89 | Batch_idx: 330 |  Loss: (0.2334) |  Loss2: (0.0000) | Acc: (91.00%) (38944/42368)
Epoch: 89 | Batch_idx: 340 |  Loss: (0.2336) |  Loss2: (0.0000) | Acc: (91.00%) (40118/43648)
Epoch: 89 | Batch_idx: 350 |  Loss: (0.2329) |  Loss2: (0.0000) | Acc: (91.00%) (41303/44928)
Epoch: 89 | Batch_idx: 360 |  Loss: (0.2333) |  Loss2: (0.0000) | Acc: (91.00%) (42476/46208)
Epoch: 89 | Batch_idx: 370 |  Loss: (0.2326) |  Loss2: (0.0000) | Acc: (91.00%) (43682/47488)
Epoch: 89 | Batch_idx: 380 |  Loss: (0.2333) |  Loss2: (0.0000) | Acc: (91.00%) (44842/48768)
Epoch: 89 | Batch_idx: 390 |  Loss: (0.2341) |  Loss2: (0.0000) | Acc: (91.00%) (45969/50000)
# TEST : Loss: (0.3974) | Acc: (87.00%) (8729/10000)
percent tensor([0.5325, 0.4675], device='cuda:0')
percent tensor([0.4920, 0.5080], device='cuda:0')
percent tensor([0.5056, 0.4944], device='cuda:0')
percent tensor([0.6059, 0.3941], device='cuda:0')
percent tensor([0.6165, 0.3835], device='cuda:0')
percent tensor([0.6553, 0.3447], device='cuda:0')
percent tensor([0.6637, 0.3363], device='cuda:0')
percent tensor([0.9985, 0.0015], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 90 | Batch_idx: 0 |  Loss: (0.1740) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 90 | Batch_idx: 10 |  Loss: (0.2280) |  Loss2: (0.0000) | Acc: (92.00%) (1298/1408)
Epoch: 90 | Batch_idx: 20 |  Loss: (0.2438) |  Loss2: (0.0000) | Acc: (91.00%) (2460/2688)
Epoch: 90 | Batch_idx: 30 |  Loss: (0.2405) |  Loss2: (0.0000) | Acc: (91.00%) (3639/3968)
Epoch: 90 | Batch_idx: 40 |  Loss: (0.2333) |  Loss2: (0.0000) | Acc: (92.00%) (4837/5248)
Epoch: 90 | Batch_idx: 50 |  Loss: (0.2305) |  Loss2: (0.0000) | Acc: (92.00%) (6027/6528)
Epoch: 90 | Batch_idx: 60 |  Loss: (0.2350) |  Loss2: (0.0000) | Acc: (92.00%) (7190/7808)
Epoch: 90 | Batch_idx: 70 |  Loss: (0.2367) |  Loss2: (0.0000) | Acc: (92.00%) (8375/9088)
Epoch: 90 | Batch_idx: 80 |  Loss: (0.2395) |  Loss2: (0.0000) | Acc: (92.00%) (9547/10368)
Epoch: 90 | Batch_idx: 90 |  Loss: (0.2396) |  Loss2: (0.0000) | Acc: (92.00%) (10733/11648)
Epoch: 90 | Batch_idx: 100 |  Loss: (0.2399) |  Loss2: (0.0000) | Acc: (92.00%) (11896/12928)
Epoch: 90 | Batch_idx: 110 |  Loss: (0.2385) |  Loss2: (0.0000) | Acc: (92.00%) (13075/14208)
Epoch: 90 | Batch_idx: 120 |  Loss: (0.2368) |  Loss2: (0.0000) | Acc: (92.00%) (14262/15488)
Epoch: 90 | Batch_idx: 130 |  Loss: (0.2376) |  Loss2: (0.0000) | Acc: (92.00%) (15432/16768)
Epoch: 90 | Batch_idx: 140 |  Loss: (0.2361) |  Loss2: (0.0000) | Acc: (92.00%) (16624/18048)
Epoch: 90 | Batch_idx: 150 |  Loss: (0.2388) |  Loss2: (0.0000) | Acc: (92.00%) (17788/19328)
Epoch: 90 | Batch_idx: 160 |  Loss: (0.2409) |  Loss2: (0.0000) | Acc: (91.00%) (18948/20608)
Epoch: 90 | Batch_idx: 170 |  Loss: (0.2425) |  Loss2: (0.0000) | Acc: (91.00%) (20104/21888)
Epoch: 90 | Batch_idx: 180 |  Loss: (0.2423) |  Loss2: (0.0000) | Acc: (91.00%) (21275/23168)
Epoch: 90 | Batch_idx: 190 |  Loss: (0.2420) |  Loss2: (0.0000) | Acc: (91.00%) (22456/24448)
Epoch: 90 | Batch_idx: 200 |  Loss: (0.2439) |  Loss2: (0.0000) | Acc: (91.00%) (23614/25728)
Epoch: 90 | Batch_idx: 210 |  Loss: (0.2428) |  Loss2: (0.0000) | Acc: (91.00%) (24786/27008)
Epoch: 90 | Batch_idx: 220 |  Loss: (0.2428) |  Loss2: (0.0000) | Acc: (91.00%) (25954/28288)
Epoch: 90 | Batch_idx: 230 |  Loss: (0.2440) |  Loss2: (0.0000) | Acc: (91.00%) (27108/29568)
Epoch: 90 | Batch_idx: 240 |  Loss: (0.2449) |  Loss2: (0.0000) | Acc: (91.00%) (28269/30848)
Epoch: 90 | Batch_idx: 250 |  Loss: (0.2450) |  Loss2: (0.0000) | Acc: (91.00%) (29450/32128)
Epoch: 90 | Batch_idx: 260 |  Loss: (0.2450) |  Loss2: (0.0000) | Acc: (91.00%) (30618/33408)
Epoch: 90 | Batch_idx: 270 |  Loss: (0.2448) |  Loss2: (0.0000) | Acc: (91.00%) (31773/34688)
Epoch: 90 | Batch_idx: 280 |  Loss: (0.2450) |  Loss2: (0.0000) | Acc: (91.00%) (32949/35968)
Epoch: 90 | Batch_idx: 290 |  Loss: (0.2442) |  Loss2: (0.0000) | Acc: (91.00%) (34120/37248)
Epoch: 90 | Batch_idx: 300 |  Loss: (0.2446) |  Loss2: (0.0000) | Acc: (91.00%) (35295/38528)
Epoch: 90 | Batch_idx: 310 |  Loss: (0.2443) |  Loss2: (0.0000) | Acc: (91.00%) (36458/39808)
Epoch: 90 | Batch_idx: 320 |  Loss: (0.2442) |  Loss2: (0.0000) | Acc: (91.00%) (37629/41088)
Epoch: 90 | Batch_idx: 330 |  Loss: (0.2453) |  Loss2: (0.0000) | Acc: (91.00%) (38781/42368)
Epoch: 90 | Batch_idx: 340 |  Loss: (0.2453) |  Loss2: (0.0000) | Acc: (91.00%) (39944/43648)
Epoch: 90 | Batch_idx: 350 |  Loss: (0.2455) |  Loss2: (0.0000) | Acc: (91.00%) (41114/44928)
Epoch: 90 | Batch_idx: 360 |  Loss: (0.2453) |  Loss2: (0.0000) | Acc: (91.00%) (42292/46208)
Epoch: 90 | Batch_idx: 370 |  Loss: (0.2464) |  Loss2: (0.0000) | Acc: (91.00%) (43451/47488)
Epoch: 90 | Batch_idx: 380 |  Loss: (0.2471) |  Loss2: (0.0000) | Acc: (91.00%) (44614/48768)
Epoch: 90 | Batch_idx: 390 |  Loss: (0.2481) |  Loss2: (0.0000) | Acc: (91.00%) (45723/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_090.pth.tar'
# TEST : Loss: (0.4266) | Acc: (86.00%) (8639/10000)
percent tensor([0.5320, 0.4680], device='cuda:0')
percent tensor([0.4920, 0.5080], device='cuda:0')
percent tensor([0.5051, 0.4949], device='cuda:0')
percent tensor([0.6053, 0.3947], device='cuda:0')
percent tensor([0.6173, 0.3827], device='cuda:0')
percent tensor([0.6528, 0.3472], device='cuda:0')
percent tensor([0.6655, 0.3345], device='cuda:0')
percent tensor([0.9987, 0.0013], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(179.5372, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(816.2371, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(808.1080, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1512.6659, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(505.3882, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2230.3831, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4276.8203, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1406.9672, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6148.8403, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11906.6514, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3954.8784, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16707.0371, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 91 | Batch_idx: 0 |  Loss: (0.2553) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 91 | Batch_idx: 10 |  Loss: (0.2127) |  Loss2: (0.0000) | Acc: (92.00%) (1307/1408)
Epoch: 91 | Batch_idx: 20 |  Loss: (0.2168) |  Loss2: (0.0000) | Acc: (92.00%) (2487/2688)
Epoch: 91 | Batch_idx: 30 |  Loss: (0.2146) |  Loss2: (0.0000) | Acc: (92.00%) (3670/3968)
Epoch: 91 | Batch_idx: 40 |  Loss: (0.2207) |  Loss2: (0.0000) | Acc: (92.00%) (4845/5248)
Epoch: 91 | Batch_idx: 50 |  Loss: (0.2233) |  Loss2: (0.0000) | Acc: (92.00%) (6030/6528)
Epoch: 91 | Batch_idx: 60 |  Loss: (0.2241) |  Loss2: (0.0000) | Acc: (92.00%) (7211/7808)
Epoch: 91 | Batch_idx: 70 |  Loss: (0.2273) |  Loss2: (0.0000) | Acc: (92.00%) (8380/9088)
Epoch: 91 | Batch_idx: 80 |  Loss: (0.2289) |  Loss2: (0.0000) | Acc: (92.00%) (9545/10368)
Epoch: 91 | Batch_idx: 90 |  Loss: (0.2311) |  Loss2: (0.0000) | Acc: (91.00%) (10712/11648)
Epoch: 91 | Batch_idx: 100 |  Loss: (0.2319) |  Loss2: (0.0000) | Acc: (91.00%) (11889/12928)
Epoch: 91 | Batch_idx: 110 |  Loss: (0.2330) |  Loss2: (0.0000) | Acc: (91.00%) (13064/14208)
Epoch: 91 | Batch_idx: 120 |  Loss: (0.2343) |  Loss2: (0.0000) | Acc: (91.00%) (14224/15488)
Epoch: 91 | Batch_idx: 130 |  Loss: (0.2372) |  Loss2: (0.0000) | Acc: (91.00%) (15379/16768)
Epoch: 91 | Batch_idx: 140 |  Loss: (0.2386) |  Loss2: (0.0000) | Acc: (91.00%) (16557/18048)
Epoch: 91 | Batch_idx: 150 |  Loss: (0.2384) |  Loss2: (0.0000) | Acc: (91.00%) (17739/19328)
Epoch: 91 | Batch_idx: 160 |  Loss: (0.2369) |  Loss2: (0.0000) | Acc: (91.00%) (18932/20608)
Epoch: 91 | Batch_idx: 170 |  Loss: (0.2368) |  Loss2: (0.0000) | Acc: (91.00%) (20109/21888)
Epoch: 91 | Batch_idx: 180 |  Loss: (0.2358) |  Loss2: (0.0000) | Acc: (91.00%) (21296/23168)
Epoch: 91 | Batch_idx: 190 |  Loss: (0.2351) |  Loss2: (0.0000) | Acc: (91.00%) (22472/24448)
Epoch: 91 | Batch_idx: 200 |  Loss: (0.2358) |  Loss2: (0.0000) | Acc: (91.00%) (23645/25728)
Epoch: 91 | Batch_idx: 210 |  Loss: (0.2355) |  Loss2: (0.0000) | Acc: (91.00%) (24821/27008)
Epoch: 91 | Batch_idx: 220 |  Loss: (0.2356) |  Loss2: (0.0000) | Acc: (91.00%) (26001/28288)
Epoch: 91 | Batch_idx: 230 |  Loss: (0.2373) |  Loss2: (0.0000) | Acc: (91.00%) (27167/29568)
Epoch: 91 | Batch_idx: 240 |  Loss: (0.2384) |  Loss2: (0.0000) | Acc: (91.00%) (28329/30848)
Epoch: 91 | Batch_idx: 250 |  Loss: (0.2386) |  Loss2: (0.0000) | Acc: (91.00%) (29500/32128)
Epoch: 91 | Batch_idx: 260 |  Loss: (0.2383) |  Loss2: (0.0000) | Acc: (91.00%) (30675/33408)
Epoch: 91 | Batch_idx: 270 |  Loss: (0.2390) |  Loss2: (0.0000) | Acc: (91.00%) (31843/34688)
Epoch: 91 | Batch_idx: 280 |  Loss: (0.2385) |  Loss2: (0.0000) | Acc: (91.00%) (33033/35968)
Epoch: 91 | Batch_idx: 290 |  Loss: (0.2385) |  Loss2: (0.0000) | Acc: (91.00%) (34212/37248)
Epoch: 91 | Batch_idx: 300 |  Loss: (0.2377) |  Loss2: (0.0000) | Acc: (91.00%) (35399/38528)
Epoch: 91 | Batch_idx: 310 |  Loss: (0.2387) |  Loss2: (0.0000) | Acc: (91.00%) (36565/39808)
Epoch: 91 | Batch_idx: 320 |  Loss: (0.2397) |  Loss2: (0.0000) | Acc: (91.00%) (37721/41088)
Epoch: 91 | Batch_idx: 330 |  Loss: (0.2396) |  Loss2: (0.0000) | Acc: (91.00%) (38902/42368)
Epoch: 91 | Batch_idx: 340 |  Loss: (0.2404) |  Loss2: (0.0000) | Acc: (91.00%) (40062/43648)
Epoch: 91 | Batch_idx: 350 |  Loss: (0.2405) |  Loss2: (0.0000) | Acc: (91.00%) (41225/44928)
Epoch: 91 | Batch_idx: 360 |  Loss: (0.2411) |  Loss2: (0.0000) | Acc: (91.00%) (42390/46208)
Epoch: 91 | Batch_idx: 370 |  Loss: (0.2418) |  Loss2: (0.0000) | Acc: (91.00%) (43548/47488)
Epoch: 91 | Batch_idx: 380 |  Loss: (0.2421) |  Loss2: (0.0000) | Acc: (91.00%) (44727/48768)
Epoch: 91 | Batch_idx: 390 |  Loss: (0.2424) |  Loss2: (0.0000) | Acc: (91.00%) (45841/50000)
# TEST : Loss: (0.4379) | Acc: (86.00%) (8618/10000)
percent tensor([0.5325, 0.4675], device='cuda:0')
percent tensor([0.4920, 0.5080], device='cuda:0')
percent tensor([0.5055, 0.4945], device='cuda:0')
percent tensor([0.6054, 0.3946], device='cuda:0')
percent tensor([0.6159, 0.3841], device='cuda:0')
percent tensor([0.6544, 0.3456], device='cuda:0')
percent tensor([0.6615, 0.3385], device='cuda:0')
percent tensor([0.9986, 0.0014], device='cuda:0')
Epoch: 92 | Batch_idx: 0 |  Loss: (0.2322) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 92 | Batch_idx: 10 |  Loss: (0.2314) |  Loss2: (0.0000) | Acc: (91.00%) (1295/1408)
Epoch: 92 | Batch_idx: 20 |  Loss: (0.2379) |  Loss2: (0.0000) | Acc: (91.00%) (2460/2688)
Epoch: 92 | Batch_idx: 30 |  Loss: (0.2253) |  Loss2: (0.0000) | Acc: (91.00%) (3641/3968)
Epoch: 92 | Batch_idx: 40 |  Loss: (0.2253) |  Loss2: (0.0000) | Acc: (91.00%) (4822/5248)
Epoch: 92 | Batch_idx: 50 |  Loss: (0.2337) |  Loss2: (0.0000) | Acc: (91.00%) (5980/6528)
Epoch: 92 | Batch_idx: 60 |  Loss: (0.2320) |  Loss2: (0.0000) | Acc: (91.00%) (7164/7808)
Epoch: 92 | Batch_idx: 70 |  Loss: (0.2315) |  Loss2: (0.0000) | Acc: (91.00%) (8337/9088)
Epoch: 92 | Batch_idx: 80 |  Loss: (0.2281) |  Loss2: (0.0000) | Acc: (91.00%) (9527/10368)
Epoch: 92 | Batch_idx: 90 |  Loss: (0.2266) |  Loss2: (0.0000) | Acc: (91.00%) (10715/11648)
Epoch: 92 | Batch_idx: 100 |  Loss: (0.2250) |  Loss2: (0.0000) | Acc: (92.00%) (11910/12928)
Epoch: 92 | Batch_idx: 110 |  Loss: (0.2239) |  Loss2: (0.0000) | Acc: (92.00%) (13101/14208)
Epoch: 92 | Batch_idx: 120 |  Loss: (0.2239) |  Loss2: (0.0000) | Acc: (92.00%) (14291/15488)
Epoch: 92 | Batch_idx: 130 |  Loss: (0.2254) |  Loss2: (0.0000) | Acc: (92.00%) (15468/16768)
Epoch: 92 | Batch_idx: 140 |  Loss: (0.2252) |  Loss2: (0.0000) | Acc: (92.00%) (16658/18048)
Epoch: 92 | Batch_idx: 150 |  Loss: (0.2290) |  Loss2: (0.0000) | Acc: (92.00%) (17816/19328)
Epoch: 92 | Batch_idx: 160 |  Loss: (0.2285) |  Loss2: (0.0000) | Acc: (92.00%) (18999/20608)
Epoch: 92 | Batch_idx: 170 |  Loss: (0.2289) |  Loss2: (0.0000) | Acc: (92.00%) (20179/21888)
Epoch: 92 | Batch_idx: 180 |  Loss: (0.2286) |  Loss2: (0.0000) | Acc: (92.00%) (21362/23168)
Epoch: 92 | Batch_idx: 190 |  Loss: (0.2287) |  Loss2: (0.0000) | Acc: (92.00%) (22534/24448)
Epoch: 92 | Batch_idx: 200 |  Loss: (0.2302) |  Loss2: (0.0000) | Acc: (92.00%) (23698/25728)
Epoch: 92 | Batch_idx: 210 |  Loss: (0.2300) |  Loss2: (0.0000) | Acc: (92.00%) (24884/27008)
Epoch: 92 | Batch_idx: 220 |  Loss: (0.2284) |  Loss2: (0.0000) | Acc: (92.00%) (26074/28288)
Epoch: 92 | Batch_idx: 230 |  Loss: (0.2284) |  Loss2: (0.0000) | Acc: (92.00%) (27268/29568)
Epoch: 92 | Batch_idx: 240 |  Loss: (0.2301) |  Loss2: (0.0000) | Acc: (92.00%) (28421/30848)
Epoch: 92 | Batch_idx: 250 |  Loss: (0.2300) |  Loss2: (0.0000) | Acc: (92.00%) (29602/32128)
Epoch: 92 | Batch_idx: 260 |  Loss: (0.2305) |  Loss2: (0.0000) | Acc: (92.00%) (30775/33408)
Epoch: 92 | Batch_idx: 270 |  Loss: (0.2314) |  Loss2: (0.0000) | Acc: (92.00%) (31938/34688)
Epoch: 92 | Batch_idx: 280 |  Loss: (0.2312) |  Loss2: (0.0000) | Acc: (92.00%) (33129/35968)
Epoch: 92 | Batch_idx: 290 |  Loss: (0.2320) |  Loss2: (0.0000) | Acc: (92.00%) (34297/37248)
Epoch: 92 | Batch_idx: 300 |  Loss: (0.2321) |  Loss2: (0.0000) | Acc: (92.00%) (35482/38528)
Epoch: 92 | Batch_idx: 310 |  Loss: (0.2315) |  Loss2: (0.0000) | Acc: (92.00%) (36662/39808)
Epoch: 92 | Batch_idx: 320 |  Loss: (0.2305) |  Loss2: (0.0000) | Acc: (92.00%) (37852/41088)
Epoch: 92 | Batch_idx: 330 |  Loss: (0.2313) |  Loss2: (0.0000) | Acc: (92.00%) (39022/42368)
Epoch: 92 | Batch_idx: 340 |  Loss: (0.2311) |  Loss2: (0.0000) | Acc: (92.00%) (40196/43648)
Epoch: 92 | Batch_idx: 350 |  Loss: (0.2322) |  Loss2: (0.0000) | Acc: (92.00%) (41351/44928)
Epoch: 92 | Batch_idx: 360 |  Loss: (0.2332) |  Loss2: (0.0000) | Acc: (92.00%) (42519/46208)
Epoch: 92 | Batch_idx: 370 |  Loss: (0.2345) |  Loss2: (0.0000) | Acc: (91.00%) (43679/47488)
Epoch: 92 | Batch_idx: 380 |  Loss: (0.2346) |  Loss2: (0.0000) | Acc: (91.00%) (44843/48768)
Epoch: 92 | Batch_idx: 390 |  Loss: (0.2355) |  Loss2: (0.0000) | Acc: (91.00%) (45960/50000)
# TEST : Loss: (0.4432) | Acc: (86.00%) (8628/10000)
percent tensor([0.5323, 0.4677], device='cuda:0')
percent tensor([0.4920, 0.5080], device='cuda:0')
percent tensor([0.5050, 0.4950], device='cuda:0')
percent tensor([0.6053, 0.3947], device='cuda:0')
percent tensor([0.6155, 0.3845], device='cuda:0')
percent tensor([0.6494, 0.3506], device='cuda:0')
percent tensor([0.6620, 0.3380], device='cuda:0')
percent tensor([0.9986, 0.0014], device='cuda:0')
Epoch: 93 | Batch_idx: 0 |  Loss: (0.2497) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 93 | Batch_idx: 10 |  Loss: (0.2237) |  Loss2: (0.0000) | Acc: (92.00%) (1305/1408)
Epoch: 93 | Batch_idx: 20 |  Loss: (0.2428) |  Loss2: (0.0000) | Acc: (92.00%) (2476/2688)
Epoch: 93 | Batch_idx: 30 |  Loss: (0.2296) |  Loss2: (0.0000) | Acc: (92.00%) (3672/3968)
Epoch: 93 | Batch_idx: 40 |  Loss: (0.2298) |  Loss2: (0.0000) | Acc: (92.00%) (4847/5248)
Epoch: 93 | Batch_idx: 50 |  Loss: (0.2284) |  Loss2: (0.0000) | Acc: (92.00%) (6031/6528)
Epoch: 93 | Batch_idx: 60 |  Loss: (0.2281) |  Loss2: (0.0000) | Acc: (92.00%) (7211/7808)
Epoch: 93 | Batch_idx: 70 |  Loss: (0.2296) |  Loss2: (0.0000) | Acc: (92.00%) (8378/9088)
Epoch: 93 | Batch_idx: 80 |  Loss: (0.2300) |  Loss2: (0.0000) | Acc: (92.00%) (9558/10368)
Epoch: 93 | Batch_idx: 90 |  Loss: (0.2304) |  Loss2: (0.0000) | Acc: (92.00%) (10728/11648)
Epoch: 93 | Batch_idx: 100 |  Loss: (0.2298) |  Loss2: (0.0000) | Acc: (92.00%) (11902/12928)
Epoch: 93 | Batch_idx: 110 |  Loss: (0.2293) |  Loss2: (0.0000) | Acc: (92.00%) (13080/14208)
Epoch: 93 | Batch_idx: 120 |  Loss: (0.2306) |  Loss2: (0.0000) | Acc: (91.00%) (14246/15488)
Epoch: 93 | Batch_idx: 130 |  Loss: (0.2292) |  Loss2: (0.0000) | Acc: (92.00%) (15433/16768)
Epoch: 93 | Batch_idx: 140 |  Loss: (0.2299) |  Loss2: (0.0000) | Acc: (92.00%) (16610/18048)
Epoch: 93 | Batch_idx: 150 |  Loss: (0.2302) |  Loss2: (0.0000) | Acc: (92.00%) (17788/19328)
Epoch: 93 | Batch_idx: 160 |  Loss: (0.2281) |  Loss2: (0.0000) | Acc: (92.00%) (18985/20608)
Epoch: 93 | Batch_idx: 170 |  Loss: (0.2287) |  Loss2: (0.0000) | Acc: (92.00%) (20151/21888)
Epoch: 93 | Batch_idx: 180 |  Loss: (0.2285) |  Loss2: (0.0000) | Acc: (92.00%) (21335/23168)
Epoch: 93 | Batch_idx: 190 |  Loss: (0.2278) |  Loss2: (0.0000) | Acc: (92.00%) (22527/24448)
Epoch: 93 | Batch_idx: 200 |  Loss: (0.2290) |  Loss2: (0.0000) | Acc: (92.00%) (23697/25728)
Epoch: 93 | Batch_idx: 210 |  Loss: (0.2291) |  Loss2: (0.0000) | Acc: (92.00%) (24883/27008)
Epoch: 93 | Batch_idx: 220 |  Loss: (0.2282) |  Loss2: (0.0000) | Acc: (92.00%) (26079/28288)
Epoch: 93 | Batch_idx: 230 |  Loss: (0.2284) |  Loss2: (0.0000) | Acc: (92.00%) (27256/29568)
Epoch: 93 | Batch_idx: 240 |  Loss: (0.2285) |  Loss2: (0.0000) | Acc: (92.00%) (28442/30848)
Epoch: 93 | Batch_idx: 250 |  Loss: (0.2292) |  Loss2: (0.0000) | Acc: (92.00%) (29601/32128)
Epoch: 93 | Batch_idx: 260 |  Loss: (0.2293) |  Loss2: (0.0000) | Acc: (92.00%) (30777/33408)
Epoch: 93 | Batch_idx: 270 |  Loss: (0.2293) |  Loss2: (0.0000) | Acc: (92.00%) (31946/34688)
Epoch: 93 | Batch_idx: 280 |  Loss: (0.2288) |  Loss2: (0.0000) | Acc: (92.00%) (33130/35968)
Epoch: 93 | Batch_idx: 290 |  Loss: (0.2289) |  Loss2: (0.0000) | Acc: (92.00%) (34310/37248)
Epoch: 93 | Batch_idx: 300 |  Loss: (0.2292) |  Loss2: (0.0000) | Acc: (92.00%) (35478/38528)
Epoch: 93 | Batch_idx: 310 |  Loss: (0.2295) |  Loss2: (0.0000) | Acc: (92.00%) (36649/39808)
Epoch: 93 | Batch_idx: 320 |  Loss: (0.2297) |  Loss2: (0.0000) | Acc: (92.00%) (37824/41088)
Epoch: 93 | Batch_idx: 330 |  Loss: (0.2296) |  Loss2: (0.0000) | Acc: (92.00%) (39013/42368)
Epoch: 93 | Batch_idx: 340 |  Loss: (0.2299) |  Loss2: (0.0000) | Acc: (92.00%) (40185/43648)
Epoch: 93 | Batch_idx: 350 |  Loss: (0.2305) |  Loss2: (0.0000) | Acc: (92.00%) (41366/44928)
Epoch: 93 | Batch_idx: 360 |  Loss: (0.2305) |  Loss2: (0.0000) | Acc: (92.00%) (42548/46208)
Epoch: 93 | Batch_idx: 370 |  Loss: (0.2303) |  Loss2: (0.0000) | Acc: (92.00%) (43732/47488)
Epoch: 93 | Batch_idx: 380 |  Loss: (0.2303) |  Loss2: (0.0000) | Acc: (92.00%) (44916/48768)
Epoch: 93 | Batch_idx: 390 |  Loss: (0.2304) |  Loss2: (0.0000) | Acc: (92.00%) (46053/50000)
# TEST : Loss: (0.4718) | Acc: (85.00%) (8534/10000)
percent tensor([0.5320, 0.4680], device='cuda:0')
percent tensor([0.4921, 0.5079], device='cuda:0')
percent tensor([0.5053, 0.4947], device='cuda:0')
percent tensor([0.6055, 0.3945], device='cuda:0')
percent tensor([0.6179, 0.3821], device='cuda:0')
percent tensor([0.6544, 0.3456], device='cuda:0')
percent tensor([0.6648, 0.3352], device='cuda:0')
percent tensor([0.9986, 0.0014], device='cuda:0')
Epoch: 94 | Batch_idx: 0 |  Loss: (0.1595) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 94 | Batch_idx: 10 |  Loss: (0.2468) |  Loss2: (0.0000) | Acc: (91.00%) (1286/1408)
Epoch: 94 | Batch_idx: 20 |  Loss: (0.2419) |  Loss2: (0.0000) | Acc: (91.00%) (2459/2688)
Epoch: 94 | Batch_idx: 30 |  Loss: (0.2262) |  Loss2: (0.0000) | Acc: (92.00%) (3654/3968)
Epoch: 94 | Batch_idx: 40 |  Loss: (0.2294) |  Loss2: (0.0000) | Acc: (91.00%) (4826/5248)
Epoch: 94 | Batch_idx: 50 |  Loss: (0.2295) |  Loss2: (0.0000) | Acc: (91.00%) (6004/6528)
Epoch: 94 | Batch_idx: 60 |  Loss: (0.2287) |  Loss2: (0.0000) | Acc: (91.00%) (7183/7808)
Epoch: 94 | Batch_idx: 70 |  Loss: (0.2306) |  Loss2: (0.0000) | Acc: (92.00%) (8365/9088)
Epoch: 94 | Batch_idx: 80 |  Loss: (0.2288) |  Loss2: (0.0000) | Acc: (92.00%) (9565/10368)
Epoch: 94 | Batch_idx: 90 |  Loss: (0.2296) |  Loss2: (0.0000) | Acc: (92.00%) (10742/11648)
Epoch: 94 | Batch_idx: 100 |  Loss: (0.2277) |  Loss2: (0.0000) | Acc: (92.00%) (11936/12928)
Epoch: 94 | Batch_idx: 110 |  Loss: (0.2268) |  Loss2: (0.0000) | Acc: (92.00%) (13112/14208)
Epoch: 94 | Batch_idx: 120 |  Loss: (0.2259) |  Loss2: (0.0000) | Acc: (92.00%) (14291/15488)
Epoch: 94 | Batch_idx: 130 |  Loss: (0.2290) |  Loss2: (0.0000) | Acc: (92.00%) (15458/16768)
Epoch: 94 | Batch_idx: 140 |  Loss: (0.2287) |  Loss2: (0.0000) | Acc: (92.00%) (16639/18048)
Epoch: 94 | Batch_idx: 150 |  Loss: (0.2263) |  Loss2: (0.0000) | Acc: (92.00%) (17839/19328)
Epoch: 94 | Batch_idx: 160 |  Loss: (0.2259) |  Loss2: (0.0000) | Acc: (92.00%) (19015/20608)
Epoch: 94 | Batch_idx: 170 |  Loss: (0.2240) |  Loss2: (0.0000) | Acc: (92.00%) (20210/21888)
Epoch: 94 | Batch_idx: 180 |  Loss: (0.2229) |  Loss2: (0.0000) | Acc: (92.00%) (21396/23168)
Epoch: 94 | Batch_idx: 190 |  Loss: (0.2232) |  Loss2: (0.0000) | Acc: (92.00%) (22578/24448)
Epoch: 94 | Batch_idx: 200 |  Loss: (0.2221) |  Loss2: (0.0000) | Acc: (92.00%) (23762/25728)
Epoch: 94 | Batch_idx: 210 |  Loss: (0.2223) |  Loss2: (0.0000) | Acc: (92.00%) (24938/27008)
Epoch: 94 | Batch_idx: 220 |  Loss: (0.2222) |  Loss2: (0.0000) | Acc: (92.00%) (26129/28288)
Epoch: 94 | Batch_idx: 230 |  Loss: (0.2217) |  Loss2: (0.0000) | Acc: (92.00%) (27322/29568)
Epoch: 94 | Batch_idx: 240 |  Loss: (0.2221) |  Loss2: (0.0000) | Acc: (92.00%) (28495/30848)
Epoch: 94 | Batch_idx: 250 |  Loss: (0.2219) |  Loss2: (0.0000) | Acc: (92.00%) (29681/32128)
Epoch: 94 | Batch_idx: 260 |  Loss: (0.2222) |  Loss2: (0.0000) | Acc: (92.00%) (30853/33408)
Epoch: 94 | Batch_idx: 270 |  Loss: (0.2214) |  Loss2: (0.0000) | Acc: (92.00%) (32038/34688)
Epoch: 94 | Batch_idx: 280 |  Loss: (0.2219) |  Loss2: (0.0000) | Acc: (92.00%) (33225/35968)
Epoch: 94 | Batch_idx: 290 |  Loss: (0.2218) |  Loss2: (0.0000) | Acc: (92.00%) (34402/37248)
Epoch: 94 | Batch_idx: 300 |  Loss: (0.2220) |  Loss2: (0.0000) | Acc: (92.00%) (35586/38528)
Epoch: 94 | Batch_idx: 310 |  Loss: (0.2220) |  Loss2: (0.0000) | Acc: (92.00%) (36769/39808)
Epoch: 94 | Batch_idx: 320 |  Loss: (0.2217) |  Loss2: (0.0000) | Acc: (92.00%) (37941/41088)
Epoch: 94 | Batch_idx: 330 |  Loss: (0.2224) |  Loss2: (0.0000) | Acc: (92.00%) (39121/42368)
Epoch: 94 | Batch_idx: 340 |  Loss: (0.2217) |  Loss2: (0.0000) | Acc: (92.00%) (40313/43648)
Epoch: 94 | Batch_idx: 350 |  Loss: (0.2223) |  Loss2: (0.0000) | Acc: (92.00%) (41484/44928)
Epoch: 94 | Batch_idx: 360 |  Loss: (0.2217) |  Loss2: (0.0000) | Acc: (92.00%) (42682/46208)
Epoch: 94 | Batch_idx: 370 |  Loss: (0.2225) |  Loss2: (0.0000) | Acc: (92.00%) (43844/47488)
Epoch: 94 | Batch_idx: 380 |  Loss: (0.2231) |  Loss2: (0.0000) | Acc: (92.00%) (45014/48768)
Epoch: 94 | Batch_idx: 390 |  Loss: (0.2240) |  Loss2: (0.0000) | Acc: (92.00%) (46146/50000)
# TEST : Loss: (0.4376) | Acc: (86.00%) (8675/10000)
percent tensor([0.5323, 0.4677], device='cuda:0')
percent tensor([0.4921, 0.5079], device='cuda:0')
percent tensor([0.5055, 0.4945], device='cuda:0')
percent tensor([0.6051, 0.3949], device='cuda:0')
percent tensor([0.6167, 0.3833], device='cuda:0')
percent tensor([0.6542, 0.3458], device='cuda:0')
percent tensor([0.6652, 0.3348], device='cuda:0')
percent tensor([0.9986, 0.0014], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 95 | Batch_idx: 0 |  Loss: (0.1853) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 95 | Batch_idx: 10 |  Loss: (0.2496) |  Loss2: (0.0000) | Acc: (91.00%) (1282/1408)
Epoch: 95 | Batch_idx: 20 |  Loss: (0.2479) |  Loss2: (0.0000) | Acc: (91.00%) (2455/2688)
Epoch: 95 | Batch_idx: 30 |  Loss: (0.2746) |  Loss2: (0.0000) | Acc: (90.00%) (3584/3968)
Epoch: 95 | Batch_idx: 40 |  Loss: (0.2930) |  Loss2: (0.0000) | Acc: (89.00%) (4706/5248)
Epoch: 95 | Batch_idx: 50 |  Loss: (0.2974) |  Loss2: (0.0000) | Acc: (89.00%) (5840/6528)
Epoch: 95 | Batch_idx: 60 |  Loss: (0.3066) |  Loss2: (0.0000) | Acc: (89.00%) (6953/7808)
Epoch: 95 | Batch_idx: 70 |  Loss: (0.3079) |  Loss2: (0.0000) | Acc: (88.00%) (8079/9088)
Epoch: 95 | Batch_idx: 80 |  Loss: (0.3118) |  Loss2: (0.0000) | Acc: (88.00%) (9204/10368)
Epoch: 95 | Batch_idx: 90 |  Loss: (0.3129) |  Loss2: (0.0000) | Acc: (88.00%) (10342/11648)
Epoch: 95 | Batch_idx: 100 |  Loss: (0.3144) |  Loss2: (0.0000) | Acc: (88.00%) (11477/12928)
Epoch: 95 | Batch_idx: 110 |  Loss: (0.3132) |  Loss2: (0.0000) | Acc: (88.00%) (12608/14208)
Epoch: 95 | Batch_idx: 120 |  Loss: (0.3119) |  Loss2: (0.0000) | Acc: (88.00%) (13748/15488)
Epoch: 95 | Batch_idx: 130 |  Loss: (0.3117) |  Loss2: (0.0000) | Acc: (88.00%) (14889/16768)
Epoch: 95 | Batch_idx: 140 |  Loss: (0.3107) |  Loss2: (0.0000) | Acc: (88.00%) (16030/18048)
Epoch: 95 | Batch_idx: 150 |  Loss: (0.3104) |  Loss2: (0.0000) | Acc: (88.00%) (17171/19328)
Epoch: 95 | Batch_idx: 160 |  Loss: (0.3096) |  Loss2: (0.0000) | Acc: (88.00%) (18320/20608)
Epoch: 95 | Batch_idx: 170 |  Loss: (0.3101) |  Loss2: (0.0000) | Acc: (88.00%) (19455/21888)
Epoch: 95 | Batch_idx: 180 |  Loss: (0.3104) |  Loss2: (0.0000) | Acc: (88.00%) (20585/23168)
Epoch: 95 | Batch_idx: 190 |  Loss: (0.3088) |  Loss2: (0.0000) | Acc: (88.00%) (21746/24448)
Epoch: 95 | Batch_idx: 200 |  Loss: (0.3097) |  Loss2: (0.0000) | Acc: (88.00%) (22877/25728)
Epoch: 95 | Batch_idx: 210 |  Loss: (0.3080) |  Loss2: (0.0000) | Acc: (88.00%) (24028/27008)
Epoch: 95 | Batch_idx: 220 |  Loss: (0.3058) |  Loss2: (0.0000) | Acc: (89.00%) (25192/28288)
Epoch: 95 | Batch_idx: 230 |  Loss: (0.3051) |  Loss2: (0.0000) | Acc: (89.00%) (26333/29568)
Epoch: 95 | Batch_idx: 240 |  Loss: (0.3044) |  Loss2: (0.0000) | Acc: (89.00%) (27474/30848)
Epoch: 95 | Batch_idx: 250 |  Loss: (0.3055) |  Loss2: (0.0000) | Acc: (89.00%) (28602/32128)
Epoch: 95 | Batch_idx: 260 |  Loss: (0.3043) |  Loss2: (0.0000) | Acc: (89.00%) (29759/33408)
Epoch: 95 | Batch_idx: 270 |  Loss: (0.3032) |  Loss2: (0.0000) | Acc: (89.00%) (30916/34688)
Epoch: 95 | Batch_idx: 280 |  Loss: (0.3029) |  Loss2: (0.0000) | Acc: (89.00%) (32061/35968)
Epoch: 95 | Batch_idx: 290 |  Loss: (0.3019) |  Loss2: (0.0000) | Acc: (89.00%) (33221/37248)
Epoch: 95 | Batch_idx: 300 |  Loss: (0.3005) |  Loss2: (0.0000) | Acc: (89.00%) (34370/38528)
Epoch: 95 | Batch_idx: 310 |  Loss: (0.3001) |  Loss2: (0.0000) | Acc: (89.00%) (35519/39808)
Epoch: 95 | Batch_idx: 320 |  Loss: (0.2989) |  Loss2: (0.0000) | Acc: (89.00%) (36672/41088)
Epoch: 95 | Batch_idx: 330 |  Loss: (0.2974) |  Loss2: (0.0000) | Acc: (89.00%) (37842/42368)
Epoch: 95 | Batch_idx: 340 |  Loss: (0.2982) |  Loss2: (0.0000) | Acc: (89.00%) (38987/43648)
Epoch: 95 | Batch_idx: 350 |  Loss: (0.2987) |  Loss2: (0.0000) | Acc: (89.00%) (40125/44928)
Epoch: 95 | Batch_idx: 360 |  Loss: (0.2983) |  Loss2: (0.0000) | Acc: (89.00%) (41278/46208)
Epoch: 95 | Batch_idx: 370 |  Loss: (0.2975) |  Loss2: (0.0000) | Acc: (89.00%) (42428/47488)
Epoch: 95 | Batch_idx: 380 |  Loss: (0.2975) |  Loss2: (0.0000) | Acc: (89.00%) (43575/48768)
Epoch: 95 | Batch_idx: 390 |  Loss: (0.2968) |  Loss2: (0.0000) | Acc: (89.00%) (44689/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_095.pth.tar'
# TEST : Loss: (0.4799) | Acc: (85.00%) (8543/10000)
percent tensor([0.5352, 0.4648], device='cuda:0')
percent tensor([0.4954, 0.5046], device='cuda:0')
percent tensor([0.5102, 0.4898], device='cuda:0')
percent tensor([0.6073, 0.3927], device='cuda:0')
percent tensor([0.6269, 0.3731], device='cuda:0')
percent tensor([0.5955, 0.4045], device='cuda:0')
percent tensor([0.6630, 0.3370], device='cuda:0')
percent tensor([0.9986, 0.0014], device='cuda:0')
Epoch: 96 | Batch_idx: 0 |  Loss: (0.2755) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 96 | Batch_idx: 10 |  Loss: (0.3209) |  Loss2: (0.0000) | Acc: (89.00%) (1254/1408)
Epoch: 96 | Batch_idx: 20 |  Loss: (0.2889) |  Loss2: (0.0000) | Acc: (89.00%) (2397/2688)
Epoch: 96 | Batch_idx: 30 |  Loss: (0.2785) |  Loss2: (0.0000) | Acc: (89.00%) (3568/3968)
Epoch: 96 | Batch_idx: 40 |  Loss: (0.2742) |  Loss2: (0.0000) | Acc: (90.00%) (4738/5248)
Epoch: 96 | Batch_idx: 50 |  Loss: (0.2687) |  Loss2: (0.0000) | Acc: (90.00%) (5900/6528)
Epoch: 96 | Batch_idx: 60 |  Loss: (0.2744) |  Loss2: (0.0000) | Acc: (90.00%) (7047/7808)
Epoch: 96 | Batch_idx: 70 |  Loss: (0.2794) |  Loss2: (0.0000) | Acc: (90.00%) (8188/9088)
Epoch: 96 | Batch_idx: 80 |  Loss: (0.2751) |  Loss2: (0.0000) | Acc: (90.00%) (9357/10368)
Epoch: 96 | Batch_idx: 90 |  Loss: (0.2748) |  Loss2: (0.0000) | Acc: (90.00%) (10515/11648)
Epoch: 96 | Batch_idx: 100 |  Loss: (0.2726) |  Loss2: (0.0000) | Acc: (90.00%) (11687/12928)
Epoch: 96 | Batch_idx: 110 |  Loss: (0.2711) |  Loss2: (0.0000) | Acc: (90.00%) (12855/14208)
Epoch: 96 | Batch_idx: 120 |  Loss: (0.2700) |  Loss2: (0.0000) | Acc: (90.00%) (14017/15488)
Epoch: 96 | Batch_idx: 130 |  Loss: (0.2735) |  Loss2: (0.0000) | Acc: (90.00%) (15158/16768)
Epoch: 96 | Batch_idx: 140 |  Loss: (0.2719) |  Loss2: (0.0000) | Acc: (90.00%) (16316/18048)
Epoch: 96 | Batch_idx: 150 |  Loss: (0.2697) |  Loss2: (0.0000) | Acc: (90.00%) (17483/19328)
Epoch: 96 | Batch_idx: 160 |  Loss: (0.2698) |  Loss2: (0.0000) | Acc: (90.00%) (18644/20608)
Epoch: 96 | Batch_idx: 170 |  Loss: (0.2719) |  Loss2: (0.0000) | Acc: (90.00%) (19785/21888)
Epoch: 96 | Batch_idx: 180 |  Loss: (0.2697) |  Loss2: (0.0000) | Acc: (90.00%) (20969/23168)
Epoch: 96 | Batch_idx: 190 |  Loss: (0.2682) |  Loss2: (0.0000) | Acc: (90.00%) (22143/24448)
Epoch: 96 | Batch_idx: 200 |  Loss: (0.2686) |  Loss2: (0.0000) | Acc: (90.00%) (23298/25728)
Epoch: 96 | Batch_idx: 210 |  Loss: (0.2695) |  Loss2: (0.0000) | Acc: (90.00%) (24451/27008)
Epoch: 96 | Batch_idx: 220 |  Loss: (0.2680) |  Loss2: (0.0000) | Acc: (90.00%) (25631/28288)
Epoch: 96 | Batch_idx: 230 |  Loss: (0.2670) |  Loss2: (0.0000) | Acc: (90.00%) (26801/29568)
Epoch: 96 | Batch_idx: 240 |  Loss: (0.2668) |  Loss2: (0.0000) | Acc: (90.00%) (27959/30848)
Epoch: 96 | Batch_idx: 250 |  Loss: (0.2671) |  Loss2: (0.0000) | Acc: (90.00%) (29118/32128)
Epoch: 96 | Batch_idx: 260 |  Loss: (0.2679) |  Loss2: (0.0000) | Acc: (90.00%) (30261/33408)
Epoch: 96 | Batch_idx: 270 |  Loss: (0.2678) |  Loss2: (0.0000) | Acc: (90.00%) (31420/34688)
Epoch: 96 | Batch_idx: 280 |  Loss: (0.2666) |  Loss2: (0.0000) | Acc: (90.00%) (32596/35968)
Epoch: 96 | Batch_idx: 290 |  Loss: (0.2655) |  Loss2: (0.0000) | Acc: (90.00%) (33766/37248)
Epoch: 96 | Batch_idx: 300 |  Loss: (0.2650) |  Loss2: (0.0000) | Acc: (90.00%) (34939/38528)
Epoch: 96 | Batch_idx: 310 |  Loss: (0.2633) |  Loss2: (0.0000) | Acc: (90.00%) (36121/39808)
Epoch: 96 | Batch_idx: 320 |  Loss: (0.2633) |  Loss2: (0.0000) | Acc: (90.00%) (37289/41088)
Epoch: 96 | Batch_idx: 330 |  Loss: (0.2644) |  Loss2: (0.0000) | Acc: (90.00%) (38433/42368)
Epoch: 96 | Batch_idx: 340 |  Loss: (0.2648) |  Loss2: (0.0000) | Acc: (90.00%) (39590/43648)
Epoch: 96 | Batch_idx: 350 |  Loss: (0.2645) |  Loss2: (0.0000) | Acc: (90.00%) (40755/44928)
Epoch: 96 | Batch_idx: 360 |  Loss: (0.2641) |  Loss2: (0.0000) | Acc: (90.00%) (41910/46208)
Epoch: 96 | Batch_idx: 370 |  Loss: (0.2638) |  Loss2: (0.0000) | Acc: (90.00%) (43086/47488)
Epoch: 96 | Batch_idx: 380 |  Loss: (0.2632) |  Loss2: (0.0000) | Acc: (90.00%) (44263/48768)
Epoch: 96 | Batch_idx: 390 |  Loss: (0.2628) |  Loss2: (0.0000) | Acc: (90.00%) (45394/50000)
# TEST : Loss: (0.4537) | Acc: (86.00%) (8605/10000)
percent tensor([0.5349, 0.4651], device='cuda:0')
percent tensor([0.4973, 0.5027], device='cuda:0')
percent tensor([0.5136, 0.4864], device='cuda:0')
percent tensor([0.6097, 0.3903], device='cuda:0')
percent tensor([0.6298, 0.3702], device='cuda:0')
percent tensor([0.5984, 0.4016], device='cuda:0')
percent tensor([0.6768, 0.3232], device='cuda:0')
percent tensor([0.9986, 0.0014], device='cuda:0')
Epoch: 97 | Batch_idx: 0 |  Loss: (0.3596) |  Loss2: (0.0000) | Acc: (85.00%) (109/128)
Epoch: 97 | Batch_idx: 10 |  Loss: (0.2389) |  Loss2: (0.0000) | Acc: (91.00%) (1285/1408)
Epoch: 97 | Batch_idx: 20 |  Loss: (0.2639) |  Loss2: (0.0000) | Acc: (90.00%) (2435/2688)
Epoch: 97 | Batch_idx: 30 |  Loss: (0.2523) |  Loss2: (0.0000) | Acc: (90.00%) (3610/3968)
Epoch: 97 | Batch_idx: 40 |  Loss: (0.2504) |  Loss2: (0.0000) | Acc: (91.00%) (4782/5248)
Epoch: 97 | Batch_idx: 50 |  Loss: (0.2443) |  Loss2: (0.0000) | Acc: (91.00%) (5952/6528)
Epoch: 97 | Batch_idx: 60 |  Loss: (0.2371) |  Loss2: (0.0000) | Acc: (91.00%) (7146/7808)
Epoch: 97 | Batch_idx: 70 |  Loss: (0.2362) |  Loss2: (0.0000) | Acc: (91.00%) (8321/9088)
Epoch: 97 | Batch_idx: 80 |  Loss: (0.2383) |  Loss2: (0.0000) | Acc: (91.00%) (9474/10368)
Epoch: 97 | Batch_idx: 90 |  Loss: (0.2408) |  Loss2: (0.0000) | Acc: (91.00%) (10636/11648)
Epoch: 97 | Batch_idx: 100 |  Loss: (0.2413) |  Loss2: (0.0000) | Acc: (91.00%) (11810/12928)
Epoch: 97 | Batch_idx: 110 |  Loss: (0.2437) |  Loss2: (0.0000) | Acc: (91.00%) (12960/14208)
Epoch: 97 | Batch_idx: 120 |  Loss: (0.2453) |  Loss2: (0.0000) | Acc: (91.00%) (14132/15488)
Epoch: 97 | Batch_idx: 130 |  Loss: (0.2445) |  Loss2: (0.0000) | Acc: (91.00%) (15306/16768)
Epoch: 97 | Batch_idx: 140 |  Loss: (0.2441) |  Loss2: (0.0000) | Acc: (91.00%) (16480/18048)
Epoch: 97 | Batch_idx: 150 |  Loss: (0.2432) |  Loss2: (0.0000) | Acc: (91.00%) (17655/19328)
Epoch: 97 | Batch_idx: 160 |  Loss: (0.2449) |  Loss2: (0.0000) | Acc: (91.00%) (18815/20608)
Epoch: 97 | Batch_idx: 170 |  Loss: (0.2475) |  Loss2: (0.0000) | Acc: (91.00%) (19968/21888)
Epoch: 97 | Batch_idx: 180 |  Loss: (0.2468) |  Loss2: (0.0000) | Acc: (91.00%) (21137/23168)
Epoch: 97 | Batch_idx: 190 |  Loss: (0.2460) |  Loss2: (0.0000) | Acc: (91.00%) (22323/24448)
Epoch: 97 | Batch_idx: 200 |  Loss: (0.2454) |  Loss2: (0.0000) | Acc: (91.00%) (23497/25728)
Epoch: 97 | Batch_idx: 210 |  Loss: (0.2457) |  Loss2: (0.0000) | Acc: (91.00%) (24667/27008)
Epoch: 97 | Batch_idx: 220 |  Loss: (0.2462) |  Loss2: (0.0000) | Acc: (91.00%) (25832/28288)
Epoch: 97 | Batch_idx: 230 |  Loss: (0.2457) |  Loss2: (0.0000) | Acc: (91.00%) (27011/29568)
Epoch: 97 | Batch_idx: 240 |  Loss: (0.2446) |  Loss2: (0.0000) | Acc: (91.00%) (28190/30848)
Epoch: 97 | Batch_idx: 250 |  Loss: (0.2438) |  Loss2: (0.0000) | Acc: (91.00%) (29379/32128)
Epoch: 97 | Batch_idx: 260 |  Loss: (0.2437) |  Loss2: (0.0000) | Acc: (91.00%) (30557/33408)
Epoch: 97 | Batch_idx: 270 |  Loss: (0.2438) |  Loss2: (0.0000) | Acc: (91.00%) (31723/34688)
Epoch: 97 | Batch_idx: 280 |  Loss: (0.2437) |  Loss2: (0.0000) | Acc: (91.00%) (32888/35968)
Epoch: 97 | Batch_idx: 290 |  Loss: (0.2437) |  Loss2: (0.0000) | Acc: (91.00%) (34055/37248)
Epoch: 97 | Batch_idx: 300 |  Loss: (0.2433) |  Loss2: (0.0000) | Acc: (91.00%) (35237/38528)
Epoch: 97 | Batch_idx: 310 |  Loss: (0.2427) |  Loss2: (0.0000) | Acc: (91.00%) (36419/39808)
Epoch: 97 | Batch_idx: 320 |  Loss: (0.2429) |  Loss2: (0.0000) | Acc: (91.00%) (37582/41088)
Epoch: 97 | Batch_idx: 330 |  Loss: (0.2430) |  Loss2: (0.0000) | Acc: (91.00%) (38755/42368)
Epoch: 97 | Batch_idx: 340 |  Loss: (0.2427) |  Loss2: (0.0000) | Acc: (91.00%) (39932/43648)
Epoch: 97 | Batch_idx: 350 |  Loss: (0.2420) |  Loss2: (0.0000) | Acc: (91.00%) (41113/44928)
Epoch: 97 | Batch_idx: 360 |  Loss: (0.2422) |  Loss2: (0.0000) | Acc: (91.00%) (42293/46208)
Epoch: 97 | Batch_idx: 370 |  Loss: (0.2420) |  Loss2: (0.0000) | Acc: (91.00%) (43476/47488)
Epoch: 97 | Batch_idx: 380 |  Loss: (0.2426) |  Loss2: (0.0000) | Acc: (91.00%) (44641/48768)
Epoch: 97 | Batch_idx: 390 |  Loss: (0.2429) |  Loss2: (0.0000) | Acc: (91.00%) (45762/50000)
# TEST : Loss: (0.4382) | Acc: (86.00%) (8662/10000)
percent tensor([0.5331, 0.4669], device='cuda:0')
percent tensor([0.4990, 0.5010], device='cuda:0')
percent tensor([0.5160, 0.4840], device='cuda:0')
percent tensor([0.6120, 0.3880], device='cuda:0')
percent tensor([0.6237, 0.3763], device='cuda:0')
percent tensor([0.6029, 0.3971], device='cuda:0')
percent tensor([0.6849, 0.3151], device='cuda:0')
percent tensor([0.9987, 0.0013], device='cuda:0')
Epoch: 98 | Batch_idx: 0 |  Loss: (0.1634) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 98 | Batch_idx: 10 |  Loss: (0.2228) |  Loss2: (0.0000) | Acc: (92.00%) (1297/1408)
Epoch: 98 | Batch_idx: 20 |  Loss: (0.2365) |  Loss2: (0.0000) | Acc: (91.00%) (2453/2688)
Epoch: 98 | Batch_idx: 30 |  Loss: (0.2404) |  Loss2: (0.0000) | Acc: (91.00%) (3616/3968)
Epoch: 98 | Batch_idx: 40 |  Loss: (0.2370) |  Loss2: (0.0000) | Acc: (91.00%) (4797/5248)
Epoch: 98 | Batch_idx: 50 |  Loss: (0.2370) |  Loss2: (0.0000) | Acc: (91.00%) (5968/6528)
Epoch: 98 | Batch_idx: 60 |  Loss: (0.2375) |  Loss2: (0.0000) | Acc: (91.00%) (7138/7808)
Epoch: 98 | Batch_idx: 70 |  Loss: (0.2399) |  Loss2: (0.0000) | Acc: (91.00%) (8308/9088)
Epoch: 98 | Batch_idx: 80 |  Loss: (0.2385) |  Loss2: (0.0000) | Acc: (91.00%) (9486/10368)
Epoch: 98 | Batch_idx: 90 |  Loss: (0.2379) |  Loss2: (0.0000) | Acc: (91.00%) (10675/11648)
Epoch: 98 | Batch_idx: 100 |  Loss: (0.2372) |  Loss2: (0.0000) | Acc: (91.00%) (11848/12928)
Epoch: 98 | Batch_idx: 110 |  Loss: (0.2363) |  Loss2: (0.0000) | Acc: (91.00%) (13032/14208)
Epoch: 98 | Batch_idx: 120 |  Loss: (0.2360) |  Loss2: (0.0000) | Acc: (91.00%) (14223/15488)
Epoch: 98 | Batch_idx: 130 |  Loss: (0.2361) |  Loss2: (0.0000) | Acc: (91.00%) (15393/16768)
Epoch: 98 | Batch_idx: 140 |  Loss: (0.2360) |  Loss2: (0.0000) | Acc: (91.00%) (16576/18048)
Epoch: 98 | Batch_idx: 150 |  Loss: (0.2369) |  Loss2: (0.0000) | Acc: (91.00%) (17749/19328)
Epoch: 98 | Batch_idx: 160 |  Loss: (0.2381) |  Loss2: (0.0000) | Acc: (91.00%) (18915/20608)
Epoch: 98 | Batch_idx: 170 |  Loss: (0.2390) |  Loss2: (0.0000) | Acc: (91.00%) (20087/21888)
Epoch: 98 | Batch_idx: 180 |  Loss: (0.2396) |  Loss2: (0.0000) | Acc: (91.00%) (21254/23168)
Epoch: 98 | Batch_idx: 190 |  Loss: (0.2395) |  Loss2: (0.0000) | Acc: (91.00%) (22431/24448)
Epoch: 98 | Batch_idx: 200 |  Loss: (0.2405) |  Loss2: (0.0000) | Acc: (91.00%) (23585/25728)
Epoch: 98 | Batch_idx: 210 |  Loss: (0.2409) |  Loss2: (0.0000) | Acc: (91.00%) (24753/27008)
Epoch: 98 | Batch_idx: 220 |  Loss: (0.2405) |  Loss2: (0.0000) | Acc: (91.00%) (25921/28288)
Epoch: 98 | Batch_idx: 230 |  Loss: (0.2403) |  Loss2: (0.0000) | Acc: (91.00%) (27105/29568)
Epoch: 98 | Batch_idx: 240 |  Loss: (0.2396) |  Loss2: (0.0000) | Acc: (91.00%) (28280/30848)
Epoch: 98 | Batch_idx: 250 |  Loss: (0.2391) |  Loss2: (0.0000) | Acc: (91.00%) (29460/32128)
Epoch: 98 | Batch_idx: 260 |  Loss: (0.2386) |  Loss2: (0.0000) | Acc: (91.00%) (30635/33408)
Epoch: 98 | Batch_idx: 270 |  Loss: (0.2388) |  Loss2: (0.0000) | Acc: (91.00%) (31808/34688)
Epoch: 98 | Batch_idx: 280 |  Loss: (0.2386) |  Loss2: (0.0000) | Acc: (91.00%) (32987/35968)
Epoch: 98 | Batch_idx: 290 |  Loss: (0.2386) |  Loss2: (0.0000) | Acc: (91.00%) (34154/37248)
Epoch: 98 | Batch_idx: 300 |  Loss: (0.2383) |  Loss2: (0.0000) | Acc: (91.00%) (35327/38528)
Epoch: 98 | Batch_idx: 310 |  Loss: (0.2380) |  Loss2: (0.0000) | Acc: (91.00%) (36508/39808)
Epoch: 98 | Batch_idx: 320 |  Loss: (0.2370) |  Loss2: (0.0000) | Acc: (91.00%) (37702/41088)
Epoch: 98 | Batch_idx: 330 |  Loss: (0.2369) |  Loss2: (0.0000) | Acc: (91.00%) (38874/42368)
Epoch: 98 | Batch_idx: 340 |  Loss: (0.2369) |  Loss2: (0.0000) | Acc: (91.00%) (40052/43648)
Epoch: 98 | Batch_idx: 350 |  Loss: (0.2361) |  Loss2: (0.0000) | Acc: (91.00%) (41237/44928)
Epoch: 98 | Batch_idx: 360 |  Loss: (0.2360) |  Loss2: (0.0000) | Acc: (91.00%) (42410/46208)
Epoch: 98 | Batch_idx: 370 |  Loss: (0.2362) |  Loss2: (0.0000) | Acc: (91.00%) (43581/47488)
Epoch: 98 | Batch_idx: 380 |  Loss: (0.2364) |  Loss2: (0.0000) | Acc: (91.00%) (44755/48768)
Epoch: 98 | Batch_idx: 390 |  Loss: (0.2365) |  Loss2: (0.0000) | Acc: (91.00%) (45885/50000)
# TEST : Loss: (0.4252) | Acc: (86.00%) (8675/10000)
percent tensor([0.5321, 0.4679], device='cuda:0')
percent tensor([0.4996, 0.5004], device='cuda:0')
percent tensor([0.5162, 0.4838], device='cuda:0')
percent tensor([0.6143, 0.3857], device='cuda:0')
percent tensor([0.6248, 0.3752], device='cuda:0')
percent tensor([0.6144, 0.3856], device='cuda:0')
percent tensor([0.6869, 0.3131], device='cuda:0')
percent tensor([0.9987, 0.0013], device='cuda:0')
Epoch: 99 | Batch_idx: 0 |  Loss: (0.1817) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 99 | Batch_idx: 10 |  Loss: (0.2250) |  Loss2: (0.0000) | Acc: (92.00%) (1304/1408)
Epoch: 99 | Batch_idx: 20 |  Loss: (0.2357) |  Loss2: (0.0000) | Acc: (92.00%) (2475/2688)
Epoch: 99 | Batch_idx: 30 |  Loss: (0.2290) |  Loss2: (0.0000) | Acc: (92.00%) (3665/3968)
Epoch: 99 | Batch_idx: 40 |  Loss: (0.2301) |  Loss2: (0.0000) | Acc: (92.00%) (4848/5248)
Epoch: 99 | Batch_idx: 50 |  Loss: (0.2350) |  Loss2: (0.0000) | Acc: (92.00%) (6007/6528)
Epoch: 99 | Batch_idx: 60 |  Loss: (0.2353) |  Loss2: (0.0000) | Acc: (91.00%) (7183/7808)
Epoch: 99 | Batch_idx: 70 |  Loss: (0.2320) |  Loss2: (0.0000) | Acc: (92.00%) (8371/9088)
Epoch: 99 | Batch_idx: 80 |  Loss: (0.2325) |  Loss2: (0.0000) | Acc: (92.00%) (9549/10368)
Epoch: 99 | Batch_idx: 90 |  Loss: (0.2303) |  Loss2: (0.0000) | Acc: (92.00%) (10734/11648)
Epoch: 99 | Batch_idx: 100 |  Loss: (0.2299) |  Loss2: (0.0000) | Acc: (92.00%) (11918/12928)
Epoch: 99 | Batch_idx: 110 |  Loss: (0.2281) |  Loss2: (0.0000) | Acc: (92.00%) (13106/14208)
Epoch: 99 | Batch_idx: 120 |  Loss: (0.2292) |  Loss2: (0.0000) | Acc: (92.00%) (14270/15488)
Epoch: 99 | Batch_idx: 130 |  Loss: (0.2282) |  Loss2: (0.0000) | Acc: (92.00%) (15451/16768)
Epoch: 99 | Batch_idx: 140 |  Loss: (0.2259) |  Loss2: (0.0000) | Acc: (92.00%) (16640/18048)
Epoch: 99 | Batch_idx: 150 |  Loss: (0.2269) |  Loss2: (0.0000) | Acc: (92.00%) (17815/19328)
Epoch: 99 | Batch_idx: 160 |  Loss: (0.2287) |  Loss2: (0.0000) | Acc: (92.00%) (18975/20608)
Epoch: 99 | Batch_idx: 170 |  Loss: (0.2291) |  Loss2: (0.0000) | Acc: (92.00%) (20146/21888)
Epoch: 99 | Batch_idx: 180 |  Loss: (0.2279) |  Loss2: (0.0000) | Acc: (92.00%) (21341/23168)
Epoch: 99 | Batch_idx: 190 |  Loss: (0.2279) |  Loss2: (0.0000) | Acc: (92.00%) (22519/24448)
Epoch: 99 | Batch_idx: 200 |  Loss: (0.2272) |  Loss2: (0.0000) | Acc: (92.00%) (23705/25728)
Epoch: 99 | Batch_idx: 210 |  Loss: (0.2271) |  Loss2: (0.0000) | Acc: (92.00%) (24887/27008)
Epoch: 99 | Batch_idx: 220 |  Loss: (0.2274) |  Loss2: (0.0000) | Acc: (92.00%) (26054/28288)
Epoch: 99 | Batch_idx: 230 |  Loss: (0.2280) |  Loss2: (0.0000) | Acc: (92.00%) (27226/29568)
Epoch: 99 | Batch_idx: 240 |  Loss: (0.2269) |  Loss2: (0.0000) | Acc: (92.00%) (28412/30848)
Epoch: 99 | Batch_idx: 250 |  Loss: (0.2261) |  Loss2: (0.0000) | Acc: (92.00%) (29593/32128)
Epoch: 99 | Batch_idx: 260 |  Loss: (0.2269) |  Loss2: (0.0000) | Acc: (92.00%) (30757/33408)
Epoch: 99 | Batch_idx: 270 |  Loss: (0.2278) |  Loss2: (0.0000) | Acc: (92.00%) (31933/34688)
Epoch: 99 | Batch_idx: 280 |  Loss: (0.2284) |  Loss2: (0.0000) | Acc: (92.00%) (33108/35968)
Epoch: 99 | Batch_idx: 290 |  Loss: (0.2296) |  Loss2: (0.0000) | Acc: (92.00%) (34269/37248)
Epoch: 99 | Batch_idx: 300 |  Loss: (0.2290) |  Loss2: (0.0000) | Acc: (92.00%) (35459/38528)
Epoch: 99 | Batch_idx: 310 |  Loss: (0.2296) |  Loss2: (0.0000) | Acc: (91.00%) (36622/39808)
Epoch: 99 | Batch_idx: 320 |  Loss: (0.2294) |  Loss2: (0.0000) | Acc: (92.00%) (37801/41088)
Epoch: 99 | Batch_idx: 330 |  Loss: (0.2297) |  Loss2: (0.0000) | Acc: (91.00%) (38973/42368)
Epoch: 99 | Batch_idx: 340 |  Loss: (0.2293) |  Loss2: (0.0000) | Acc: (91.00%) (40152/43648)
Epoch: 99 | Batch_idx: 350 |  Loss: (0.2286) |  Loss2: (0.0000) | Acc: (92.00%) (41340/44928)
Epoch: 99 | Batch_idx: 360 |  Loss: (0.2291) |  Loss2: (0.0000) | Acc: (92.00%) (42512/46208)
Epoch: 99 | Batch_idx: 370 |  Loss: (0.2296) |  Loss2: (0.0000) | Acc: (91.00%) (43683/47488)
Epoch: 99 | Batch_idx: 380 |  Loss: (0.2294) |  Loss2: (0.0000) | Acc: (91.00%) (44860/48768)
Epoch: 99 | Batch_idx: 390 |  Loss: (0.2300) |  Loss2: (0.0000) | Acc: (91.00%) (45973/50000)
# TEST : Loss: (0.4196) | Acc: (86.00%) (8684/10000)
percent tensor([0.5320, 0.4680], device='cuda:0')
percent tensor([0.5002, 0.4998], device='cuda:0')
percent tensor([0.5164, 0.4836], device='cuda:0')
percent tensor([0.6144, 0.3856], device='cuda:0')
percent tensor([0.6239, 0.3761], device='cuda:0')
percent tensor([0.6160, 0.3840], device='cuda:0')
percent tensor([0.6864, 0.3136], device='cuda:0')
percent tensor([0.9987, 0.0013], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 100 | Batch_idx: 0 |  Loss: (0.1924) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 100 | Batch_idx: 10 |  Loss: (0.1896) |  Loss2: (0.0000) | Acc: (93.00%) (1313/1408)
Epoch: 100 | Batch_idx: 20 |  Loss: (0.1964) |  Loss2: (0.0000) | Acc: (92.00%) (2499/2688)
Epoch: 100 | Batch_idx: 30 |  Loss: (0.2097) |  Loss2: (0.0000) | Acc: (92.00%) (3680/3968)
Epoch: 100 | Batch_idx: 40 |  Loss: (0.2137) |  Loss2: (0.0000) | Acc: (92.00%) (4855/5248)
Epoch: 100 | Batch_idx: 50 |  Loss: (0.2121) |  Loss2: (0.0000) | Acc: (92.00%) (6045/6528)
Epoch: 100 | Batch_idx: 60 |  Loss: (0.2104) |  Loss2: (0.0000) | Acc: (92.00%) (7224/7808)
Epoch: 100 | Batch_idx: 70 |  Loss: (0.2139) |  Loss2: (0.0000) | Acc: (92.00%) (8410/9088)
Epoch: 100 | Batch_idx: 80 |  Loss: (0.2157) |  Loss2: (0.0000) | Acc: (92.00%) (9579/10368)
Epoch: 100 | Batch_idx: 90 |  Loss: (0.2164) |  Loss2: (0.0000) | Acc: (92.00%) (10762/11648)
Epoch: 100 | Batch_idx: 100 |  Loss: (0.2146) |  Loss2: (0.0000) | Acc: (92.00%) (11953/12928)
Epoch: 100 | Batch_idx: 110 |  Loss: (0.2179) |  Loss2: (0.0000) | Acc: (92.00%) (13118/14208)
Epoch: 100 | Batch_idx: 120 |  Loss: (0.2157) |  Loss2: (0.0000) | Acc: (92.00%) (14322/15488)
Epoch: 100 | Batch_idx: 130 |  Loss: (0.2159) |  Loss2: (0.0000) | Acc: (92.00%) (15509/16768)
Epoch: 100 | Batch_idx: 140 |  Loss: (0.2179) |  Loss2: (0.0000) | Acc: (92.00%) (16676/18048)
Epoch: 100 | Batch_idx: 150 |  Loss: (0.2174) |  Loss2: (0.0000) | Acc: (92.00%) (17860/19328)
Epoch: 100 | Batch_idx: 160 |  Loss: (0.2198) |  Loss2: (0.0000) | Acc: (92.00%) (19028/20608)
Epoch: 100 | Batch_idx: 170 |  Loss: (0.2189) |  Loss2: (0.0000) | Acc: (92.00%) (20218/21888)
Epoch: 100 | Batch_idx: 180 |  Loss: (0.2187) |  Loss2: (0.0000) | Acc: (92.00%) (21407/23168)
Epoch: 100 | Batch_idx: 190 |  Loss: (0.2189) |  Loss2: (0.0000) | Acc: (92.00%) (22582/24448)
Epoch: 100 | Batch_idx: 200 |  Loss: (0.2195) |  Loss2: (0.0000) | Acc: (92.00%) (23770/25728)
Epoch: 100 | Batch_idx: 210 |  Loss: (0.2196) |  Loss2: (0.0000) | Acc: (92.00%) (24955/27008)
Epoch: 100 | Batch_idx: 220 |  Loss: (0.2196) |  Loss2: (0.0000) | Acc: (92.00%) (26134/28288)
Epoch: 100 | Batch_idx: 230 |  Loss: (0.2201) |  Loss2: (0.0000) | Acc: (92.00%) (27319/29568)
Epoch: 100 | Batch_idx: 240 |  Loss: (0.2190) |  Loss2: (0.0000) | Acc: (92.00%) (28506/30848)
Epoch: 100 | Batch_idx: 250 |  Loss: (0.2185) |  Loss2: (0.0000) | Acc: (92.00%) (29693/32128)
Epoch: 100 | Batch_idx: 260 |  Loss: (0.2191) |  Loss2: (0.0000) | Acc: (92.00%) (30874/33408)
Epoch: 100 | Batch_idx: 270 |  Loss: (0.2203) |  Loss2: (0.0000) | Acc: (92.00%) (32040/34688)
Epoch: 100 | Batch_idx: 280 |  Loss: (0.2211) |  Loss2: (0.0000) | Acc: (92.00%) (33206/35968)
Epoch: 100 | Batch_idx: 290 |  Loss: (0.2210) |  Loss2: (0.0000) | Acc: (92.00%) (34384/37248)
Epoch: 100 | Batch_idx: 300 |  Loss: (0.2217) |  Loss2: (0.0000) | Acc: (92.00%) (35552/38528)
Epoch: 100 | Batch_idx: 310 |  Loss: (0.2218) |  Loss2: (0.0000) | Acc: (92.00%) (36727/39808)
Epoch: 100 | Batch_idx: 320 |  Loss: (0.2220) |  Loss2: (0.0000) | Acc: (92.00%) (37900/41088)
Epoch: 100 | Batch_idx: 330 |  Loss: (0.2213) |  Loss2: (0.0000) | Acc: (92.00%) (39096/42368)
Epoch: 100 | Batch_idx: 340 |  Loss: (0.2212) |  Loss2: (0.0000) | Acc: (92.00%) (40290/43648)
Epoch: 100 | Batch_idx: 350 |  Loss: (0.2221) |  Loss2: (0.0000) | Acc: (92.00%) (41452/44928)
Epoch: 100 | Batch_idx: 360 |  Loss: (0.2223) |  Loss2: (0.0000) | Acc: (92.00%) (42624/46208)
Epoch: 100 | Batch_idx: 370 |  Loss: (0.2230) |  Loss2: (0.0000) | Acc: (92.00%) (43797/47488)
Epoch: 100 | Batch_idx: 380 |  Loss: (0.2239) |  Loss2: (0.0000) | Acc: (92.00%) (44951/48768)
Epoch: 100 | Batch_idx: 390 |  Loss: (0.2249) |  Loss2: (0.0000) | Acc: (92.00%) (46067/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_100.pth.tar'
# TEST : Loss: (0.4634) | Acc: (86.00%) (8606/10000)
percent tensor([0.5320, 0.4680], device='cuda:0')
percent tensor([0.5001, 0.4999], device='cuda:0')
percent tensor([0.5160, 0.4840], device='cuda:0')
percent tensor([0.6148, 0.3852], device='cuda:0')
percent tensor([0.6248, 0.3752], device='cuda:0')
percent tensor([0.6172, 0.3828], device='cuda:0')
percent tensor([0.6866, 0.3134], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(180.2119, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(818.5329, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(810.5601, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1511.0315, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(503.7317, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2237.6687, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4274.8271, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1401.9159, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6161.6938, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11871.4756, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3939.6179, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16639.9512, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 101 | Batch_idx: 0 |  Loss: (0.1851) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 101 | Batch_idx: 10 |  Loss: (0.2118) |  Loss2: (0.0000) | Acc: (92.00%) (1304/1408)
Epoch: 101 | Batch_idx: 20 |  Loss: (0.2136) |  Loss2: (0.0000) | Acc: (92.00%) (2490/2688)
Epoch: 101 | Batch_idx: 30 |  Loss: (0.2205) |  Loss2: (0.0000) | Acc: (92.00%) (3664/3968)
Epoch: 101 | Batch_idx: 40 |  Loss: (0.2173) |  Loss2: (0.0000) | Acc: (92.00%) (4854/5248)
Epoch: 101 | Batch_idx: 50 |  Loss: (0.2147) |  Loss2: (0.0000) | Acc: (92.00%) (6050/6528)
Epoch: 101 | Batch_idx: 60 |  Loss: (0.2155) |  Loss2: (0.0000) | Acc: (92.00%) (7230/7808)
Epoch: 101 | Batch_idx: 70 |  Loss: (0.2124) |  Loss2: (0.0000) | Acc: (92.00%) (8430/9088)
Epoch: 101 | Batch_idx: 80 |  Loss: (0.2092) |  Loss2: (0.0000) | Acc: (92.00%) (9628/10368)
Epoch: 101 | Batch_idx: 90 |  Loss: (0.2091) |  Loss2: (0.0000) | Acc: (92.00%) (10809/11648)
Epoch: 101 | Batch_idx: 100 |  Loss: (0.2114) |  Loss2: (0.0000) | Acc: (92.00%) (11993/12928)
Epoch: 101 | Batch_idx: 110 |  Loss: (0.2109) |  Loss2: (0.0000) | Acc: (92.00%) (13193/14208)
Epoch: 101 | Batch_idx: 120 |  Loss: (0.2097) |  Loss2: (0.0000) | Acc: (92.00%) (14389/15488)
Epoch: 101 | Batch_idx: 130 |  Loss: (0.2112) |  Loss2: (0.0000) | Acc: (92.00%) (15576/16768)
Epoch: 101 | Batch_idx: 140 |  Loss: (0.2118) |  Loss2: (0.0000) | Acc: (92.00%) (16774/18048)
Epoch: 101 | Batch_idx: 150 |  Loss: (0.2124) |  Loss2: (0.0000) | Acc: (92.00%) (17952/19328)
Epoch: 101 | Batch_idx: 160 |  Loss: (0.2128) |  Loss2: (0.0000) | Acc: (92.00%) (19131/20608)
Epoch: 101 | Batch_idx: 170 |  Loss: (0.2137) |  Loss2: (0.0000) | Acc: (92.00%) (20314/21888)
Epoch: 101 | Batch_idx: 180 |  Loss: (0.2138) |  Loss2: (0.0000) | Acc: (92.00%) (21499/23168)
Epoch: 101 | Batch_idx: 190 |  Loss: (0.2145) |  Loss2: (0.0000) | Acc: (92.00%) (22679/24448)
Epoch: 101 | Batch_idx: 200 |  Loss: (0.2144) |  Loss2: (0.0000) | Acc: (92.00%) (23868/25728)
Epoch: 101 | Batch_idx: 210 |  Loss: (0.2149) |  Loss2: (0.0000) | Acc: (92.00%) (25053/27008)
Epoch: 101 | Batch_idx: 220 |  Loss: (0.2152) |  Loss2: (0.0000) | Acc: (92.00%) (26233/28288)
Epoch: 101 | Batch_idx: 230 |  Loss: (0.2150) |  Loss2: (0.0000) | Acc: (92.00%) (27415/29568)
Epoch: 101 | Batch_idx: 240 |  Loss: (0.2155) |  Loss2: (0.0000) | Acc: (92.00%) (28596/30848)
Epoch: 101 | Batch_idx: 250 |  Loss: (0.2160) |  Loss2: (0.0000) | Acc: (92.00%) (29766/32128)
Epoch: 101 | Batch_idx: 260 |  Loss: (0.2161) |  Loss2: (0.0000) | Acc: (92.00%) (30958/33408)
Epoch: 101 | Batch_idx: 270 |  Loss: (0.2170) |  Loss2: (0.0000) | Acc: (92.00%) (32126/34688)
Epoch: 101 | Batch_idx: 280 |  Loss: (0.2166) |  Loss2: (0.0000) | Acc: (92.00%) (33326/35968)
Epoch: 101 | Batch_idx: 290 |  Loss: (0.2166) |  Loss2: (0.0000) | Acc: (92.00%) (34512/37248)
Epoch: 101 | Batch_idx: 300 |  Loss: (0.2167) |  Loss2: (0.0000) | Acc: (92.00%) (35700/38528)
Epoch: 101 | Batch_idx: 310 |  Loss: (0.2168) |  Loss2: (0.0000) | Acc: (92.00%) (36885/39808)
Epoch: 101 | Batch_idx: 320 |  Loss: (0.2175) |  Loss2: (0.0000) | Acc: (92.00%) (38051/41088)
Epoch: 101 | Batch_idx: 330 |  Loss: (0.2172) |  Loss2: (0.0000) | Acc: (92.00%) (39232/42368)
Epoch: 101 | Batch_idx: 340 |  Loss: (0.2177) |  Loss2: (0.0000) | Acc: (92.00%) (40403/43648)
Epoch: 101 | Batch_idx: 350 |  Loss: (0.2182) |  Loss2: (0.0000) | Acc: (92.00%) (41576/44928)
Epoch: 101 | Batch_idx: 360 |  Loss: (0.2175) |  Loss2: (0.0000) | Acc: (92.00%) (42773/46208)
Epoch: 101 | Batch_idx: 370 |  Loss: (0.2168) |  Loss2: (0.0000) | Acc: (92.00%) (43968/47488)
Epoch: 101 | Batch_idx: 380 |  Loss: (0.2177) |  Loss2: (0.0000) | Acc: (92.00%) (45137/48768)
Epoch: 101 | Batch_idx: 390 |  Loss: (0.2181) |  Loss2: (0.0000) | Acc: (92.00%) (46263/50000)
# TEST : Loss: (0.5036) | Acc: (84.00%) (8498/10000)
percent tensor([0.5319, 0.4681], device='cuda:0')
percent tensor([0.5001, 0.4999], device='cuda:0')
percent tensor([0.5159, 0.4841], device='cuda:0')
percent tensor([0.6132, 0.3868], device='cuda:0')
percent tensor([0.6209, 0.3791], device='cuda:0')
percent tensor([0.6172, 0.3828], device='cuda:0')
percent tensor([0.6847, 0.3153], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
Epoch: 102 | Batch_idx: 0 |  Loss: (0.2116) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 102 | Batch_idx: 10 |  Loss: (0.2066) |  Loss2: (0.0000) | Acc: (92.00%) (1304/1408)
Epoch: 102 | Batch_idx: 20 |  Loss: (0.2065) |  Loss2: (0.0000) | Acc: (92.00%) (2495/2688)
Epoch: 102 | Batch_idx: 30 |  Loss: (0.2066) |  Loss2: (0.0000) | Acc: (93.00%) (3693/3968)
Epoch: 102 | Batch_idx: 40 |  Loss: (0.2103) |  Loss2: (0.0000) | Acc: (93.00%) (4881/5248)
Epoch: 102 | Batch_idx: 50 |  Loss: (0.2070) |  Loss2: (0.0000) | Acc: (93.00%) (6077/6528)
Epoch: 102 | Batch_idx: 60 |  Loss: (0.2053) |  Loss2: (0.0000) | Acc: (93.00%) (7276/7808)
Epoch: 102 | Batch_idx: 70 |  Loss: (0.2071) |  Loss2: (0.0000) | Acc: (93.00%) (8461/9088)
Epoch: 102 | Batch_idx: 80 |  Loss: (0.2061) |  Loss2: (0.0000) | Acc: (93.00%) (9648/10368)
Epoch: 102 | Batch_idx: 90 |  Loss: (0.2059) |  Loss2: (0.0000) | Acc: (93.00%) (10841/11648)
Epoch: 102 | Batch_idx: 100 |  Loss: (0.2084) |  Loss2: (0.0000) | Acc: (92.00%) (12014/12928)
Epoch: 102 | Batch_idx: 110 |  Loss: (0.2067) |  Loss2: (0.0000) | Acc: (92.00%) (13213/14208)
Epoch: 102 | Batch_idx: 120 |  Loss: (0.2058) |  Loss2: (0.0000) | Acc: (92.00%) (14402/15488)
Epoch: 102 | Batch_idx: 130 |  Loss: (0.2074) |  Loss2: (0.0000) | Acc: (92.00%) (15585/16768)
Epoch: 102 | Batch_idx: 140 |  Loss: (0.2077) |  Loss2: (0.0000) | Acc: (92.00%) (16768/18048)
Epoch: 102 | Batch_idx: 150 |  Loss: (0.2081) |  Loss2: (0.0000) | Acc: (92.00%) (17953/19328)
Epoch: 102 | Batch_idx: 160 |  Loss: (0.2076) |  Loss2: (0.0000) | Acc: (92.00%) (19145/20608)
Epoch: 102 | Batch_idx: 170 |  Loss: (0.2078) |  Loss2: (0.0000) | Acc: (92.00%) (20328/21888)
Epoch: 102 | Batch_idx: 180 |  Loss: (0.2079) |  Loss2: (0.0000) | Acc: (92.00%) (21515/23168)
Epoch: 102 | Batch_idx: 190 |  Loss: (0.2069) |  Loss2: (0.0000) | Acc: (92.00%) (22712/24448)
Epoch: 102 | Batch_idx: 200 |  Loss: (0.2063) |  Loss2: (0.0000) | Acc: (92.00%) (23903/25728)
Epoch: 102 | Batch_idx: 210 |  Loss: (0.2069) |  Loss2: (0.0000) | Acc: (92.00%) (25083/27008)
Epoch: 102 | Batch_idx: 220 |  Loss: (0.2082) |  Loss2: (0.0000) | Acc: (92.00%) (26269/28288)
Epoch: 102 | Batch_idx: 230 |  Loss: (0.2087) |  Loss2: (0.0000) | Acc: (92.00%) (27455/29568)
Epoch: 102 | Batch_idx: 240 |  Loss: (0.2097) |  Loss2: (0.0000) | Acc: (92.00%) (28631/30848)
Epoch: 102 | Batch_idx: 250 |  Loss: (0.2101) |  Loss2: (0.0000) | Acc: (92.00%) (29810/32128)
Epoch: 102 | Batch_idx: 260 |  Loss: (0.2101) |  Loss2: (0.0000) | Acc: (92.00%) (31003/33408)
Epoch: 102 | Batch_idx: 270 |  Loss: (0.2104) |  Loss2: (0.0000) | Acc: (92.00%) (32184/34688)
Epoch: 102 | Batch_idx: 280 |  Loss: (0.2109) |  Loss2: (0.0000) | Acc: (92.00%) (33360/35968)
Epoch: 102 | Batch_idx: 290 |  Loss: (0.2113) |  Loss2: (0.0000) | Acc: (92.00%) (34543/37248)
Epoch: 102 | Batch_idx: 300 |  Loss: (0.2103) |  Loss2: (0.0000) | Acc: (92.00%) (35746/38528)
Epoch: 102 | Batch_idx: 310 |  Loss: (0.2114) |  Loss2: (0.0000) | Acc: (92.00%) (36910/39808)
Epoch: 102 | Batch_idx: 320 |  Loss: (0.2117) |  Loss2: (0.0000) | Acc: (92.00%) (38087/41088)
Epoch: 102 | Batch_idx: 330 |  Loss: (0.2124) |  Loss2: (0.0000) | Acc: (92.00%) (39264/42368)
Epoch: 102 | Batch_idx: 340 |  Loss: (0.2125) |  Loss2: (0.0000) | Acc: (92.00%) (40448/43648)
Epoch: 102 | Batch_idx: 350 |  Loss: (0.2130) |  Loss2: (0.0000) | Acc: (92.00%) (41632/44928)
Epoch: 102 | Batch_idx: 360 |  Loss: (0.2131) |  Loss2: (0.0000) | Acc: (92.00%) (42822/46208)
Epoch: 102 | Batch_idx: 370 |  Loss: (0.2137) |  Loss2: (0.0000) | Acc: (92.00%) (43999/47488)
Epoch: 102 | Batch_idx: 380 |  Loss: (0.2140) |  Loss2: (0.0000) | Acc: (92.00%) (45178/48768)
Epoch: 102 | Batch_idx: 390 |  Loss: (0.2137) |  Loss2: (0.0000) | Acc: (92.00%) (46322/50000)
# TEST : Loss: (0.4139) | Acc: (87.00%) (8735/10000)
percent tensor([0.5320, 0.4680], device='cuda:0')
percent tensor([0.5001, 0.4999], device='cuda:0')
percent tensor([0.5159, 0.4841], device='cuda:0')
percent tensor([0.6127, 0.3873], device='cuda:0')
percent tensor([0.6231, 0.3769], device='cuda:0')
percent tensor([0.6176, 0.3824], device='cuda:0')
percent tensor([0.6852, 0.3148], device='cuda:0')
percent tensor([0.9988, 0.0012], device='cuda:0')
Epoch: 103 | Batch_idx: 0 |  Loss: (0.1916) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 103 | Batch_idx: 10 |  Loss: (0.1974) |  Loss2: (0.0000) | Acc: (93.00%) (1315/1408)
Epoch: 103 | Batch_idx: 20 |  Loss: (0.1883) |  Loss2: (0.0000) | Acc: (93.00%) (2513/2688)
Epoch: 103 | Batch_idx: 30 |  Loss: (0.2059) |  Loss2: (0.0000) | Acc: (92.00%) (3684/3968)
Epoch: 103 | Batch_idx: 40 |  Loss: (0.2039) |  Loss2: (0.0000) | Acc: (92.00%) (4875/5248)
Epoch: 103 | Batch_idx: 50 |  Loss: (0.2025) |  Loss2: (0.0000) | Acc: (93.00%) (6074/6528)
Epoch: 103 | Batch_idx: 60 |  Loss: (0.2013) |  Loss2: (0.0000) | Acc: (93.00%) (7270/7808)
Epoch: 103 | Batch_idx: 70 |  Loss: (0.2003) |  Loss2: (0.0000) | Acc: (93.00%) (8466/9088)
Epoch: 103 | Batch_idx: 80 |  Loss: (0.1993) |  Loss2: (0.0000) | Acc: (93.00%) (9671/10368)
Epoch: 103 | Batch_idx: 90 |  Loss: (0.1998) |  Loss2: (0.0000) | Acc: (93.00%) (10862/11648)
Epoch: 103 | Batch_idx: 100 |  Loss: (0.1983) |  Loss2: (0.0000) | Acc: (93.00%) (12060/12928)
Epoch: 103 | Batch_idx: 110 |  Loss: (0.1972) |  Loss2: (0.0000) | Acc: (93.00%) (13258/14208)
Epoch: 103 | Batch_idx: 120 |  Loss: (0.1990) |  Loss2: (0.0000) | Acc: (93.00%) (14440/15488)
Epoch: 103 | Batch_idx: 130 |  Loss: (0.1996) |  Loss2: (0.0000) | Acc: (93.00%) (15624/16768)
Epoch: 103 | Batch_idx: 140 |  Loss: (0.2012) |  Loss2: (0.0000) | Acc: (93.00%) (16805/18048)
Epoch: 103 | Batch_idx: 150 |  Loss: (0.2007) |  Loss2: (0.0000) | Acc: (93.00%) (18000/19328)
Epoch: 103 | Batch_idx: 160 |  Loss: (0.2012) |  Loss2: (0.0000) | Acc: (93.00%) (19187/20608)
Epoch: 103 | Batch_idx: 170 |  Loss: (0.2005) |  Loss2: (0.0000) | Acc: (93.00%) (20381/21888)
Epoch: 103 | Batch_idx: 180 |  Loss: (0.1996) |  Loss2: (0.0000) | Acc: (93.00%) (21567/23168)
Epoch: 103 | Batch_idx: 190 |  Loss: (0.1999) |  Loss2: (0.0000) | Acc: (93.00%) (22752/24448)
Epoch: 103 | Batch_idx: 200 |  Loss: (0.1993) |  Loss2: (0.0000) | Acc: (93.00%) (23948/25728)
Epoch: 103 | Batch_idx: 210 |  Loss: (0.2016) |  Loss2: (0.0000) | Acc: (93.00%) (25124/27008)
Epoch: 103 | Batch_idx: 220 |  Loss: (0.2036) |  Loss2: (0.0000) | Acc: (92.00%) (26305/28288)
Epoch: 103 | Batch_idx: 230 |  Loss: (0.2037) |  Loss2: (0.0000) | Acc: (92.00%) (27494/29568)
Epoch: 103 | Batch_idx: 240 |  Loss: (0.2022) |  Loss2: (0.0000) | Acc: (93.00%) (28699/30848)
Epoch: 103 | Batch_idx: 250 |  Loss: (0.2029) |  Loss2: (0.0000) | Acc: (93.00%) (29883/32128)
Epoch: 103 | Batch_idx: 260 |  Loss: (0.2040) |  Loss2: (0.0000) | Acc: (92.00%) (31060/33408)
Epoch: 103 | Batch_idx: 270 |  Loss: (0.2045) |  Loss2: (0.0000) | Acc: (92.00%) (32237/34688)
Epoch: 103 | Batch_idx: 280 |  Loss: (0.2043) |  Loss2: (0.0000) | Acc: (92.00%) (33426/35968)
Epoch: 103 | Batch_idx: 290 |  Loss: (0.2038) |  Loss2: (0.0000) | Acc: (92.00%) (34624/37248)
Epoch: 103 | Batch_idx: 300 |  Loss: (0.2035) |  Loss2: (0.0000) | Acc: (92.00%) (35826/38528)
Epoch: 103 | Batch_idx: 310 |  Loss: (0.2034) |  Loss2: (0.0000) | Acc: (92.00%) (37015/39808)
Epoch: 103 | Batch_idx: 320 |  Loss: (0.2043) |  Loss2: (0.0000) | Acc: (92.00%) (38202/41088)
Epoch: 103 | Batch_idx: 330 |  Loss: (0.2041) |  Loss2: (0.0000) | Acc: (92.00%) (39385/42368)
Epoch: 103 | Batch_idx: 340 |  Loss: (0.2044) |  Loss2: (0.0000) | Acc: (92.00%) (40579/43648)
Epoch: 103 | Batch_idx: 350 |  Loss: (0.2056) |  Loss2: (0.0000) | Acc: (92.00%) (41756/44928)
Epoch: 103 | Batch_idx: 360 |  Loss: (0.2056) |  Loss2: (0.0000) | Acc: (92.00%) (42953/46208)
Epoch: 103 | Batch_idx: 370 |  Loss: (0.2064) |  Loss2: (0.0000) | Acc: (92.00%) (44131/47488)
Epoch: 103 | Batch_idx: 380 |  Loss: (0.2073) |  Loss2: (0.0000) | Acc: (92.00%) (45296/48768)
Epoch: 103 | Batch_idx: 390 |  Loss: (0.2077) |  Loss2: (0.0000) | Acc: (92.00%) (46439/50000)
# TEST : Loss: (0.4728) | Acc: (85.00%) (8529/10000)
percent tensor([0.5319, 0.4681], device='cuda:0')
percent tensor([0.5001, 0.4999], device='cuda:0')
percent tensor([0.5162, 0.4838], device='cuda:0')
percent tensor([0.6124, 0.3876], device='cuda:0')
percent tensor([0.6238, 0.3762], device='cuda:0')
percent tensor([0.6187, 0.3813], device='cuda:0')
percent tensor([0.6875, 0.3125], device='cuda:0')
percent tensor([0.9988, 0.0012], device='cuda:0')
Epoch: 104 | Batch_idx: 0 |  Loss: (0.2059) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 104 | Batch_idx: 10 |  Loss: (0.2109) |  Loss2: (0.0000) | Acc: (92.00%) (1308/1408)
Epoch: 104 | Batch_idx: 20 |  Loss: (0.2238) |  Loss2: (0.0000) | Acc: (92.00%) (2485/2688)
Epoch: 104 | Batch_idx: 30 |  Loss: (0.2206) |  Loss2: (0.0000) | Acc: (92.00%) (3662/3968)
Epoch: 104 | Batch_idx: 40 |  Loss: (0.2164) |  Loss2: (0.0000) | Acc: (92.00%) (4847/5248)
Epoch: 104 | Batch_idx: 50 |  Loss: (0.2181) |  Loss2: (0.0000) | Acc: (92.00%) (6028/6528)
Epoch: 104 | Batch_idx: 60 |  Loss: (0.2191) |  Loss2: (0.0000) | Acc: (92.00%) (7200/7808)
Epoch: 104 | Batch_idx: 70 |  Loss: (0.2208) |  Loss2: (0.0000) | Acc: (92.00%) (8378/9088)
Epoch: 104 | Batch_idx: 80 |  Loss: (0.2187) |  Loss2: (0.0000) | Acc: (92.00%) (9567/10368)
Epoch: 104 | Batch_idx: 90 |  Loss: (0.2156) |  Loss2: (0.0000) | Acc: (92.00%) (10762/11648)
Epoch: 104 | Batch_idx: 100 |  Loss: (0.2154) |  Loss2: (0.0000) | Acc: (92.00%) (11939/12928)
Epoch: 104 | Batch_idx: 110 |  Loss: (0.2135) |  Loss2: (0.0000) | Acc: (92.00%) (13132/14208)
Epoch: 104 | Batch_idx: 120 |  Loss: (0.2124) |  Loss2: (0.0000) | Acc: (92.00%) (14326/15488)
Epoch: 104 | Batch_idx: 130 |  Loss: (0.2106) |  Loss2: (0.0000) | Acc: (92.00%) (15525/16768)
Epoch: 104 | Batch_idx: 140 |  Loss: (0.2112) |  Loss2: (0.0000) | Acc: (92.00%) (16706/18048)
Epoch: 104 | Batch_idx: 150 |  Loss: (0.2108) |  Loss2: (0.0000) | Acc: (92.00%) (17895/19328)
Epoch: 104 | Batch_idx: 160 |  Loss: (0.2086) |  Loss2: (0.0000) | Acc: (92.00%) (19098/20608)
Epoch: 104 | Batch_idx: 170 |  Loss: (0.2093) |  Loss2: (0.0000) | Acc: (92.00%) (20284/21888)
Epoch: 104 | Batch_idx: 180 |  Loss: (0.2086) |  Loss2: (0.0000) | Acc: (92.00%) (21475/23168)
Epoch: 104 | Batch_idx: 190 |  Loss: (0.2080) |  Loss2: (0.0000) | Acc: (92.00%) (22678/24448)
Epoch: 104 | Batch_idx: 200 |  Loss: (0.2074) |  Loss2: (0.0000) | Acc: (92.00%) (23874/25728)
Epoch: 104 | Batch_idx: 210 |  Loss: (0.2071) |  Loss2: (0.0000) | Acc: (92.00%) (25071/27008)
Epoch: 104 | Batch_idx: 220 |  Loss: (0.2060) |  Loss2: (0.0000) | Acc: (92.00%) (26269/28288)
Epoch: 104 | Batch_idx: 230 |  Loss: (0.2057) |  Loss2: (0.0000) | Acc: (92.00%) (27469/29568)
Epoch: 104 | Batch_idx: 240 |  Loss: (0.2055) |  Loss2: (0.0000) | Acc: (92.00%) (28668/30848)
Epoch: 104 | Batch_idx: 250 |  Loss: (0.2059) |  Loss2: (0.0000) | Acc: (92.00%) (29842/32128)
Epoch: 104 | Batch_idx: 260 |  Loss: (0.2045) |  Loss2: (0.0000) | Acc: (92.00%) (31051/33408)
Epoch: 104 | Batch_idx: 270 |  Loss: (0.2053) |  Loss2: (0.0000) | Acc: (92.00%) (32236/34688)
Epoch: 104 | Batch_idx: 280 |  Loss: (0.2042) |  Loss2: (0.0000) | Acc: (92.00%) (33438/35968)
Epoch: 104 | Batch_idx: 290 |  Loss: (0.2045) |  Loss2: (0.0000) | Acc: (92.00%) (34630/37248)
Epoch: 104 | Batch_idx: 300 |  Loss: (0.2051) |  Loss2: (0.0000) | Acc: (92.00%) (35815/38528)
Epoch: 104 | Batch_idx: 310 |  Loss: (0.2041) |  Loss2: (0.0000) | Acc: (93.00%) (37027/39808)
Epoch: 104 | Batch_idx: 320 |  Loss: (0.2048) |  Loss2: (0.0000) | Acc: (92.00%) (38207/41088)
Epoch: 104 | Batch_idx: 330 |  Loss: (0.2041) |  Loss2: (0.0000) | Acc: (92.00%) (39397/42368)
Epoch: 104 | Batch_idx: 340 |  Loss: (0.2036) |  Loss2: (0.0000) | Acc: (92.00%) (40591/43648)
Epoch: 104 | Batch_idx: 350 |  Loss: (0.2043) |  Loss2: (0.0000) | Acc: (92.00%) (41769/44928)
Epoch: 104 | Batch_idx: 360 |  Loss: (0.2043) |  Loss2: (0.0000) | Acc: (92.00%) (42958/46208)
Epoch: 104 | Batch_idx: 370 |  Loss: (0.2046) |  Loss2: (0.0000) | Acc: (92.00%) (44137/47488)
Epoch: 104 | Batch_idx: 380 |  Loss: (0.2053) |  Loss2: (0.0000) | Acc: (92.00%) (45321/48768)
Epoch: 104 | Batch_idx: 390 |  Loss: (0.2066) |  Loss2: (0.0000) | Acc: (92.00%) (46451/50000)
# TEST : Loss: (0.4587) | Acc: (86.00%) (8637/10000)
percent tensor([0.5322, 0.4678], device='cuda:0')
percent tensor([0.5001, 0.4999], device='cuda:0')
percent tensor([0.5163, 0.4837], device='cuda:0')
percent tensor([0.6131, 0.3869], device='cuda:0')
percent tensor([0.6241, 0.3759], device='cuda:0')
percent tensor([0.6178, 0.3822], device='cuda:0')
percent tensor([0.6878, 0.3122], device='cuda:0')
percent tensor([0.9988, 0.0012], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 105 | Batch_idx: 0 |  Loss: (0.2497) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 105 | Batch_idx: 10 |  Loss: (0.2477) |  Loss2: (0.0000) | Acc: (91.00%) (1293/1408)
Epoch: 105 | Batch_idx: 20 |  Loss: (0.2626) |  Loss2: (0.0000) | Acc: (91.00%) (2457/2688)
Epoch: 105 | Batch_idx: 30 |  Loss: (0.2869) |  Loss2: (0.0000) | Acc: (90.00%) (3592/3968)
Epoch: 105 | Batch_idx: 40 |  Loss: (0.2840) |  Loss2: (0.0000) | Acc: (90.00%) (4744/5248)
Epoch: 105 | Batch_idx: 50 |  Loss: (0.2898) |  Loss2: (0.0000) | Acc: (90.00%) (5883/6528)
Epoch: 105 | Batch_idx: 60 |  Loss: (0.2901) |  Loss2: (0.0000) | Acc: (90.00%) (7033/7808)
Epoch: 105 | Batch_idx: 70 |  Loss: (0.2870) |  Loss2: (0.0000) | Acc: (90.00%) (8194/9088)
Epoch: 105 | Batch_idx: 80 |  Loss: (0.2878) |  Loss2: (0.0000) | Acc: (90.00%) (9347/10368)
Epoch: 105 | Batch_idx: 90 |  Loss: (0.2887) |  Loss2: (0.0000) | Acc: (90.00%) (10495/11648)
Epoch: 105 | Batch_idx: 100 |  Loss: (0.2863) |  Loss2: (0.0000) | Acc: (90.00%) (11653/12928)
Epoch: 105 | Batch_idx: 110 |  Loss: (0.2851) |  Loss2: (0.0000) | Acc: (90.00%) (12810/14208)
Epoch: 105 | Batch_idx: 120 |  Loss: (0.2838) |  Loss2: (0.0000) | Acc: (90.00%) (13974/15488)
Epoch: 105 | Batch_idx: 130 |  Loss: (0.2839) |  Loss2: (0.0000) | Acc: (90.00%) (15127/16768)
Epoch: 105 | Batch_idx: 140 |  Loss: (0.2847) |  Loss2: (0.0000) | Acc: (90.00%) (16277/18048)
Epoch: 105 | Batch_idx: 150 |  Loss: (0.2842) |  Loss2: (0.0000) | Acc: (90.00%) (17427/19328)
Epoch: 105 | Batch_idx: 160 |  Loss: (0.2867) |  Loss2: (0.0000) | Acc: (90.00%) (18562/20608)
Epoch: 105 | Batch_idx: 170 |  Loss: (0.2881) |  Loss2: (0.0000) | Acc: (90.00%) (19703/21888)
Epoch: 105 | Batch_idx: 180 |  Loss: (0.2872) |  Loss2: (0.0000) | Acc: (90.00%) (20875/23168)
Epoch: 105 | Batch_idx: 190 |  Loss: (0.2869) |  Loss2: (0.0000) | Acc: (90.00%) (22042/24448)
Epoch: 105 | Batch_idx: 200 |  Loss: (0.2876) |  Loss2: (0.0000) | Acc: (90.00%) (23183/25728)
Epoch: 105 | Batch_idx: 210 |  Loss: (0.2872) |  Loss2: (0.0000) | Acc: (90.00%) (24332/27008)
Epoch: 105 | Batch_idx: 220 |  Loss: (0.2847) |  Loss2: (0.0000) | Acc: (90.00%) (25507/28288)
Epoch: 105 | Batch_idx: 230 |  Loss: (0.2838) |  Loss2: (0.0000) | Acc: (90.00%) (26671/29568)
Epoch: 105 | Batch_idx: 240 |  Loss: (0.2832) |  Loss2: (0.0000) | Acc: (90.00%) (27828/30848)
Epoch: 105 | Batch_idx: 250 |  Loss: (0.2822) |  Loss2: (0.0000) | Acc: (90.00%) (28997/32128)
Epoch: 105 | Batch_idx: 260 |  Loss: (0.2815) |  Loss2: (0.0000) | Acc: (90.00%) (30153/33408)
Epoch: 105 | Batch_idx: 270 |  Loss: (0.2799) |  Loss2: (0.0000) | Acc: (90.00%) (31333/34688)
Epoch: 105 | Batch_idx: 280 |  Loss: (0.2782) |  Loss2: (0.0000) | Acc: (90.00%) (32507/35968)
Epoch: 105 | Batch_idx: 290 |  Loss: (0.2761) |  Loss2: (0.0000) | Acc: (90.00%) (33686/37248)
Epoch: 105 | Batch_idx: 300 |  Loss: (0.2756) |  Loss2: (0.0000) | Acc: (90.00%) (34842/38528)
Epoch: 105 | Batch_idx: 310 |  Loss: (0.2748) |  Loss2: (0.0000) | Acc: (90.00%) (36000/39808)
Epoch: 105 | Batch_idx: 320 |  Loss: (0.2748) |  Loss2: (0.0000) | Acc: (90.00%) (37160/41088)
Epoch: 105 | Batch_idx: 330 |  Loss: (0.2744) |  Loss2: (0.0000) | Acc: (90.00%) (38321/42368)
Epoch: 105 | Batch_idx: 340 |  Loss: (0.2741) |  Loss2: (0.0000) | Acc: (90.00%) (39478/43648)
Epoch: 105 | Batch_idx: 350 |  Loss: (0.2746) |  Loss2: (0.0000) | Acc: (90.00%) (40625/44928)
Epoch: 105 | Batch_idx: 360 |  Loss: (0.2736) |  Loss2: (0.0000) | Acc: (90.00%) (41800/46208)
Epoch: 105 | Batch_idx: 370 |  Loss: (0.2732) |  Loss2: (0.0000) | Acc: (90.00%) (42959/47488)
Epoch: 105 | Batch_idx: 380 |  Loss: (0.2723) |  Loss2: (0.0000) | Acc: (90.00%) (44124/48768)
Epoch: 105 | Batch_idx: 390 |  Loss: (0.2716) |  Loss2: (0.0000) | Acc: (90.00%) (45254/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_105.pth.tar'
# TEST : Loss: (0.4563) | Acc: (86.00%) (8621/10000)
percent tensor([0.5290, 0.4710], device='cuda:0')
percent tensor([0.4994, 0.5006], device='cuda:0')
percent tensor([0.5157, 0.4843], device='cuda:0')
percent tensor([0.6168, 0.3832], device='cuda:0')
percent tensor([0.6156, 0.3844], device='cuda:0')
percent tensor([0.6072, 0.3928], device='cuda:0')
percent tensor([0.6987, 0.3013], device='cuda:0')
percent tensor([0.9987, 0.0013], device='cuda:0')
Epoch: 106 | Batch_idx: 0 |  Loss: (0.1944) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 106 | Batch_idx: 10 |  Loss: (0.2134) |  Loss2: (0.0000) | Acc: (92.00%) (1303/1408)
Epoch: 106 | Batch_idx: 20 |  Loss: (0.2408) |  Loss2: (0.0000) | Acc: (91.00%) (2457/2688)
Epoch: 106 | Batch_idx: 30 |  Loss: (0.2470) |  Loss2: (0.0000) | Acc: (91.00%) (3620/3968)
Epoch: 106 | Batch_idx: 40 |  Loss: (0.2494) |  Loss2: (0.0000) | Acc: (91.00%) (4787/5248)
Epoch: 106 | Batch_idx: 50 |  Loss: (0.2485) |  Loss2: (0.0000) | Acc: (91.00%) (5967/6528)
Epoch: 106 | Batch_idx: 60 |  Loss: (0.2471) |  Loss2: (0.0000) | Acc: (91.00%) (7133/7808)
Epoch: 106 | Batch_idx: 70 |  Loss: (0.2470) |  Loss2: (0.0000) | Acc: (91.00%) (8297/9088)
Epoch: 106 | Batch_idx: 80 |  Loss: (0.2435) |  Loss2: (0.0000) | Acc: (91.00%) (9478/10368)
Epoch: 106 | Batch_idx: 90 |  Loss: (0.2417) |  Loss2: (0.0000) | Acc: (91.00%) (10653/11648)
Epoch: 106 | Batch_idx: 100 |  Loss: (0.2422) |  Loss2: (0.0000) | Acc: (91.00%) (11825/12928)
Epoch: 106 | Batch_idx: 110 |  Loss: (0.2402) |  Loss2: (0.0000) | Acc: (91.00%) (13008/14208)
Epoch: 106 | Batch_idx: 120 |  Loss: (0.2393) |  Loss2: (0.0000) | Acc: (91.00%) (14189/15488)
Epoch: 106 | Batch_idx: 130 |  Loss: (0.2376) |  Loss2: (0.0000) | Acc: (91.00%) (15374/16768)
Epoch: 106 | Batch_idx: 140 |  Loss: (0.2382) |  Loss2: (0.0000) | Acc: (91.00%) (16539/18048)
Epoch: 106 | Batch_idx: 150 |  Loss: (0.2363) |  Loss2: (0.0000) | Acc: (91.00%) (17731/19328)
Epoch: 106 | Batch_idx: 160 |  Loss: (0.2338) |  Loss2: (0.0000) | Acc: (91.00%) (18921/20608)
Epoch: 106 | Batch_idx: 170 |  Loss: (0.2344) |  Loss2: (0.0000) | Acc: (91.00%) (20102/21888)
Epoch: 106 | Batch_idx: 180 |  Loss: (0.2327) |  Loss2: (0.0000) | Acc: (91.00%) (21290/23168)
Epoch: 106 | Batch_idx: 190 |  Loss: (0.2311) |  Loss2: (0.0000) | Acc: (91.00%) (22492/24448)
Epoch: 106 | Batch_idx: 200 |  Loss: (0.2303) |  Loss2: (0.0000) | Acc: (92.00%) (23678/25728)
Epoch: 106 | Batch_idx: 210 |  Loss: (0.2296) |  Loss2: (0.0000) | Acc: (92.00%) (24863/27008)
Epoch: 106 | Batch_idx: 220 |  Loss: (0.2287) |  Loss2: (0.0000) | Acc: (92.00%) (26044/28288)
Epoch: 106 | Batch_idx: 230 |  Loss: (0.2290) |  Loss2: (0.0000) | Acc: (92.00%) (27215/29568)
Epoch: 106 | Batch_idx: 240 |  Loss: (0.2299) |  Loss2: (0.0000) | Acc: (92.00%) (28385/30848)
Epoch: 106 | Batch_idx: 250 |  Loss: (0.2281) |  Loss2: (0.0000) | Acc: (92.00%) (29578/32128)
Epoch: 106 | Batch_idx: 260 |  Loss: (0.2275) |  Loss2: (0.0000) | Acc: (92.00%) (30763/33408)
Epoch: 106 | Batch_idx: 270 |  Loss: (0.2279) |  Loss2: (0.0000) | Acc: (92.00%) (31930/34688)
Epoch: 106 | Batch_idx: 280 |  Loss: (0.2282) |  Loss2: (0.0000) | Acc: (92.00%) (33103/35968)
Epoch: 106 | Batch_idx: 290 |  Loss: (0.2274) |  Loss2: (0.0000) | Acc: (92.00%) (34282/37248)
Epoch: 106 | Batch_idx: 300 |  Loss: (0.2272) |  Loss2: (0.0000) | Acc: (92.00%) (35458/38528)
Epoch: 106 | Batch_idx: 310 |  Loss: (0.2278) |  Loss2: (0.0000) | Acc: (92.00%) (36636/39808)
Epoch: 106 | Batch_idx: 320 |  Loss: (0.2275) |  Loss2: (0.0000) | Acc: (92.00%) (37838/41088)
Epoch: 106 | Batch_idx: 330 |  Loss: (0.2265) |  Loss2: (0.0000) | Acc: (92.00%) (39024/42368)
Epoch: 106 | Batch_idx: 340 |  Loss: (0.2260) |  Loss2: (0.0000) | Acc: (92.00%) (40213/43648)
Epoch: 106 | Batch_idx: 350 |  Loss: (0.2267) |  Loss2: (0.0000) | Acc: (92.00%) (41380/44928)
Epoch: 106 | Batch_idx: 360 |  Loss: (0.2266) |  Loss2: (0.0000) | Acc: (92.00%) (42544/46208)
Epoch: 106 | Batch_idx: 370 |  Loss: (0.2269) |  Loss2: (0.0000) | Acc: (92.00%) (43720/47488)
Epoch: 106 | Batch_idx: 380 |  Loss: (0.2272) |  Loss2: (0.0000) | Acc: (92.00%) (44891/48768)
Epoch: 106 | Batch_idx: 390 |  Loss: (0.2274) |  Loss2: (0.0000) | Acc: (92.00%) (46023/50000)
# TEST : Loss: (0.4347) | Acc: (86.00%) (8670/10000)
percent tensor([0.5294, 0.4706], device='cuda:0')
percent tensor([0.4992, 0.5008], device='cuda:0')
percent tensor([0.5191, 0.4809], device='cuda:0')
percent tensor([0.6153, 0.3847], device='cuda:0')
percent tensor([0.6201, 0.3799], device='cuda:0')
percent tensor([0.6130, 0.3870], device='cuda:0')
percent tensor([0.7074, 0.2926], device='cuda:0')
percent tensor([0.9987, 0.0013], device='cuda:0')
Epoch: 107 | Batch_idx: 0 |  Loss: (0.1391) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 107 | Batch_idx: 10 |  Loss: (0.2063) |  Loss2: (0.0000) | Acc: (92.00%) (1305/1408)
Epoch: 107 | Batch_idx: 20 |  Loss: (0.2266) |  Loss2: (0.0000) | Acc: (92.00%) (2481/2688)
Epoch: 107 | Batch_idx: 30 |  Loss: (0.2203) |  Loss2: (0.0000) | Acc: (92.00%) (3669/3968)
Epoch: 107 | Batch_idx: 40 |  Loss: (0.2256) |  Loss2: (0.0000) | Acc: (92.00%) (4837/5248)
Epoch: 107 | Batch_idx: 50 |  Loss: (0.2263) |  Loss2: (0.0000) | Acc: (92.00%) (6014/6528)
Epoch: 107 | Batch_idx: 60 |  Loss: (0.2294) |  Loss2: (0.0000) | Acc: (91.00%) (7179/7808)
Epoch: 107 | Batch_idx: 70 |  Loss: (0.2265) |  Loss2: (0.0000) | Acc: (92.00%) (8366/9088)
Epoch: 107 | Batch_idx: 80 |  Loss: (0.2239) |  Loss2: (0.0000) | Acc: (92.00%) (9560/10368)
Epoch: 107 | Batch_idx: 90 |  Loss: (0.2226) |  Loss2: (0.0000) | Acc: (92.00%) (10749/11648)
Epoch: 107 | Batch_idx: 100 |  Loss: (0.2216) |  Loss2: (0.0000) | Acc: (92.00%) (11942/12928)
Epoch: 107 | Batch_idx: 110 |  Loss: (0.2183) |  Loss2: (0.0000) | Acc: (92.00%) (13143/14208)
Epoch: 107 | Batch_idx: 120 |  Loss: (0.2210) |  Loss2: (0.0000) | Acc: (92.00%) (14315/15488)
Epoch: 107 | Batch_idx: 130 |  Loss: (0.2196) |  Loss2: (0.0000) | Acc: (92.00%) (15503/16768)
Epoch: 107 | Batch_idx: 140 |  Loss: (0.2207) |  Loss2: (0.0000) | Acc: (92.00%) (16681/18048)
Epoch: 107 | Batch_idx: 150 |  Loss: (0.2216) |  Loss2: (0.0000) | Acc: (92.00%) (17859/19328)
Epoch: 107 | Batch_idx: 160 |  Loss: (0.2211) |  Loss2: (0.0000) | Acc: (92.00%) (19042/20608)
Epoch: 107 | Batch_idx: 170 |  Loss: (0.2214) |  Loss2: (0.0000) | Acc: (92.00%) (20228/21888)
Epoch: 107 | Batch_idx: 180 |  Loss: (0.2207) |  Loss2: (0.0000) | Acc: (92.00%) (21407/23168)
Epoch: 107 | Batch_idx: 190 |  Loss: (0.2204) |  Loss2: (0.0000) | Acc: (92.00%) (22599/24448)
Epoch: 107 | Batch_idx: 200 |  Loss: (0.2214) |  Loss2: (0.0000) | Acc: (92.00%) (23779/25728)
Epoch: 107 | Batch_idx: 210 |  Loss: (0.2204) |  Loss2: (0.0000) | Acc: (92.00%) (24967/27008)
Epoch: 107 | Batch_idx: 220 |  Loss: (0.2191) |  Loss2: (0.0000) | Acc: (92.00%) (26170/28288)
Epoch: 107 | Batch_idx: 230 |  Loss: (0.2190) |  Loss2: (0.0000) | Acc: (92.00%) (27357/29568)
Epoch: 107 | Batch_idx: 240 |  Loss: (0.2176) |  Loss2: (0.0000) | Acc: (92.00%) (28563/30848)
Epoch: 107 | Batch_idx: 250 |  Loss: (0.2171) |  Loss2: (0.0000) | Acc: (92.00%) (29753/32128)
Epoch: 107 | Batch_idx: 260 |  Loss: (0.2164) |  Loss2: (0.0000) | Acc: (92.00%) (30952/33408)
Epoch: 107 | Batch_idx: 270 |  Loss: (0.2167) |  Loss2: (0.0000) | Acc: (92.00%) (32135/34688)
Epoch: 107 | Batch_idx: 280 |  Loss: (0.2170) |  Loss2: (0.0000) | Acc: (92.00%) (33318/35968)
Epoch: 107 | Batch_idx: 290 |  Loss: (0.2173) |  Loss2: (0.0000) | Acc: (92.00%) (34498/37248)
Epoch: 107 | Batch_idx: 300 |  Loss: (0.2169) |  Loss2: (0.0000) | Acc: (92.00%) (35683/38528)
Epoch: 107 | Batch_idx: 310 |  Loss: (0.2172) |  Loss2: (0.0000) | Acc: (92.00%) (36869/39808)
Epoch: 107 | Batch_idx: 320 |  Loss: (0.2175) |  Loss2: (0.0000) | Acc: (92.00%) (38046/41088)
Epoch: 107 | Batch_idx: 330 |  Loss: (0.2174) |  Loss2: (0.0000) | Acc: (92.00%) (39235/42368)
Epoch: 107 | Batch_idx: 340 |  Loss: (0.2176) |  Loss2: (0.0000) | Acc: (92.00%) (40415/43648)
Epoch: 107 | Batch_idx: 350 |  Loss: (0.2177) |  Loss2: (0.0000) | Acc: (92.00%) (41599/44928)
Epoch: 107 | Batch_idx: 360 |  Loss: (0.2179) |  Loss2: (0.0000) | Acc: (92.00%) (42786/46208)
Epoch: 107 | Batch_idx: 370 |  Loss: (0.2170) |  Loss2: (0.0000) | Acc: (92.00%) (43986/47488)
Epoch: 107 | Batch_idx: 380 |  Loss: (0.2171) |  Loss2: (0.0000) | Acc: (92.00%) (45163/48768)
Epoch: 107 | Batch_idx: 390 |  Loss: (0.2176) |  Loss2: (0.0000) | Acc: (92.00%) (46295/50000)
# TEST : Loss: (0.4230) | Acc: (86.00%) (8687/10000)
percent tensor([0.5306, 0.4694], device='cuda:0')
percent tensor([0.4995, 0.5005], device='cuda:0')
percent tensor([0.5206, 0.4794], device='cuda:0')
percent tensor([0.6124, 0.3876], device='cuda:0')
percent tensor([0.6197, 0.3803], device='cuda:0')
percent tensor([0.6191, 0.3809], device='cuda:0')
percent tensor([0.7183, 0.2817], device='cuda:0')
percent tensor([0.9987, 0.0013], device='cuda:0')
Epoch: 108 | Batch_idx: 0 |  Loss: (0.1595) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 108 | Batch_idx: 10 |  Loss: (0.2113) |  Loss2: (0.0000) | Acc: (92.00%) (1303/1408)
Epoch: 108 | Batch_idx: 20 |  Loss: (0.2050) |  Loss2: (0.0000) | Acc: (92.00%) (2494/2688)
Epoch: 108 | Batch_idx: 30 |  Loss: (0.2077) |  Loss2: (0.0000) | Acc: (92.00%) (3678/3968)
Epoch: 108 | Batch_idx: 40 |  Loss: (0.2021) |  Loss2: (0.0000) | Acc: (92.00%) (4877/5248)
Epoch: 108 | Batch_idx: 50 |  Loss: (0.2085) |  Loss2: (0.0000) | Acc: (92.00%) (6051/6528)
Epoch: 108 | Batch_idx: 60 |  Loss: (0.2070) |  Loss2: (0.0000) | Acc: (92.00%) (7242/7808)
Epoch: 108 | Batch_idx: 70 |  Loss: (0.2073) |  Loss2: (0.0000) | Acc: (92.00%) (8444/9088)
Epoch: 108 | Batch_idx: 80 |  Loss: (0.2087) |  Loss2: (0.0000) | Acc: (92.00%) (9629/10368)
Epoch: 108 | Batch_idx: 90 |  Loss: (0.2089) |  Loss2: (0.0000) | Acc: (92.00%) (10812/11648)
Epoch: 108 | Batch_idx: 100 |  Loss: (0.2065) |  Loss2: (0.0000) | Acc: (92.00%) (12013/12928)
Epoch: 108 | Batch_idx: 110 |  Loss: (0.2056) |  Loss2: (0.0000) | Acc: (92.00%) (13202/14208)
Epoch: 108 | Batch_idx: 120 |  Loss: (0.2058) |  Loss2: (0.0000) | Acc: (92.00%) (14393/15488)
Epoch: 108 | Batch_idx: 130 |  Loss: (0.2064) |  Loss2: (0.0000) | Acc: (92.00%) (15574/16768)
Epoch: 108 | Batch_idx: 140 |  Loss: (0.2067) |  Loss2: (0.0000) | Acc: (92.00%) (16762/18048)
Epoch: 108 | Batch_idx: 150 |  Loss: (0.2067) |  Loss2: (0.0000) | Acc: (92.00%) (17950/19328)
Epoch: 108 | Batch_idx: 160 |  Loss: (0.2073) |  Loss2: (0.0000) | Acc: (92.00%) (19135/20608)
Epoch: 108 | Batch_idx: 170 |  Loss: (0.2074) |  Loss2: (0.0000) | Acc: (92.00%) (20319/21888)
Epoch: 108 | Batch_idx: 180 |  Loss: (0.2087) |  Loss2: (0.0000) | Acc: (92.00%) (21504/23168)
Epoch: 108 | Batch_idx: 190 |  Loss: (0.2071) |  Loss2: (0.0000) | Acc: (92.00%) (22706/24448)
Epoch: 108 | Batch_idx: 200 |  Loss: (0.2074) |  Loss2: (0.0000) | Acc: (92.00%) (23888/25728)
Epoch: 108 | Batch_idx: 210 |  Loss: (0.2071) |  Loss2: (0.0000) | Acc: (92.00%) (25077/27008)
Epoch: 108 | Batch_idx: 220 |  Loss: (0.2080) |  Loss2: (0.0000) | Acc: (92.00%) (26248/28288)
Epoch: 108 | Batch_idx: 230 |  Loss: (0.2086) |  Loss2: (0.0000) | Acc: (92.00%) (27427/29568)
Epoch: 108 | Batch_idx: 240 |  Loss: (0.2085) |  Loss2: (0.0000) | Acc: (92.00%) (28617/30848)
Epoch: 108 | Batch_idx: 250 |  Loss: (0.2087) |  Loss2: (0.0000) | Acc: (92.00%) (29804/32128)
Epoch: 108 | Batch_idx: 260 |  Loss: (0.2087) |  Loss2: (0.0000) | Acc: (92.00%) (30991/33408)
Epoch: 108 | Batch_idx: 270 |  Loss: (0.2087) |  Loss2: (0.0000) | Acc: (92.00%) (32182/34688)
Epoch: 108 | Batch_idx: 280 |  Loss: (0.2089) |  Loss2: (0.0000) | Acc: (92.00%) (33370/35968)
Epoch: 108 | Batch_idx: 290 |  Loss: (0.2085) |  Loss2: (0.0000) | Acc: (92.00%) (34563/37248)
Epoch: 108 | Batch_idx: 300 |  Loss: (0.2085) |  Loss2: (0.0000) | Acc: (92.00%) (35747/38528)
Epoch: 108 | Batch_idx: 310 |  Loss: (0.2091) |  Loss2: (0.0000) | Acc: (92.00%) (36928/39808)
Epoch: 108 | Batch_idx: 320 |  Loss: (0.2095) |  Loss2: (0.0000) | Acc: (92.00%) (38109/41088)
Epoch: 108 | Batch_idx: 330 |  Loss: (0.2088) |  Loss2: (0.0000) | Acc: (92.00%) (39312/42368)
Epoch: 108 | Batch_idx: 340 |  Loss: (0.2091) |  Loss2: (0.0000) | Acc: (92.00%) (40498/43648)
Epoch: 108 | Batch_idx: 350 |  Loss: (0.2086) |  Loss2: (0.0000) | Acc: (92.00%) (41696/44928)
Epoch: 108 | Batch_idx: 360 |  Loss: (0.2085) |  Loss2: (0.0000) | Acc: (92.00%) (42895/46208)
Epoch: 108 | Batch_idx: 370 |  Loss: (0.2083) |  Loss2: (0.0000) | Acc: (92.00%) (44083/47488)
Epoch: 108 | Batch_idx: 380 |  Loss: (0.2095) |  Loss2: (0.0000) | Acc: (92.00%) (45253/48768)
Epoch: 108 | Batch_idx: 390 |  Loss: (0.2094) |  Loss2: (0.0000) | Acc: (92.00%) (46398/50000)
# TEST : Loss: (0.4182) | Acc: (87.00%) (8705/10000)
percent tensor([0.5305, 0.4695], device='cuda:0')
percent tensor([0.4991, 0.5009], device='cuda:0')
percent tensor([0.5223, 0.4777], device='cuda:0')
percent tensor([0.6149, 0.3851], device='cuda:0')
percent tensor([0.6191, 0.3809], device='cuda:0')
percent tensor([0.6244, 0.3756], device='cuda:0')
percent tensor([0.7177, 0.2823], device='cuda:0')
percent tensor([0.9988, 0.0012], device='cuda:0')
Epoch: 109 | Batch_idx: 0 |  Loss: (0.2846) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 109 | Batch_idx: 10 |  Loss: (0.2079) |  Loss2: (0.0000) | Acc: (92.00%) (1304/1408)
Epoch: 109 | Batch_idx: 20 |  Loss: (0.2019) |  Loss2: (0.0000) | Acc: (92.00%) (2490/2688)
Epoch: 109 | Batch_idx: 30 |  Loss: (0.2018) |  Loss2: (0.0000) | Acc: (92.00%) (3682/3968)
Epoch: 109 | Batch_idx: 40 |  Loss: (0.1995) |  Loss2: (0.0000) | Acc: (92.00%) (4873/5248)
Epoch: 109 | Batch_idx: 50 |  Loss: (0.1981) |  Loss2: (0.0000) | Acc: (92.00%) (6064/6528)
Epoch: 109 | Batch_idx: 60 |  Loss: (0.1994) |  Loss2: (0.0000) | Acc: (92.00%) (7254/7808)
Epoch: 109 | Batch_idx: 70 |  Loss: (0.1971) |  Loss2: (0.0000) | Acc: (93.00%) (8458/9088)
Epoch: 109 | Batch_idx: 80 |  Loss: (0.1997) |  Loss2: (0.0000) | Acc: (92.00%) (9627/10368)
Epoch: 109 | Batch_idx: 90 |  Loss: (0.1973) |  Loss2: (0.0000) | Acc: (92.00%) (10829/11648)
Epoch: 109 | Batch_idx: 100 |  Loss: (0.1971) |  Loss2: (0.0000) | Acc: (93.00%) (12031/12928)
Epoch: 109 | Batch_idx: 110 |  Loss: (0.2007) |  Loss2: (0.0000) | Acc: (92.00%) (13207/14208)
Epoch: 109 | Batch_idx: 120 |  Loss: (0.2002) |  Loss2: (0.0000) | Acc: (92.00%) (14399/15488)
Epoch: 109 | Batch_idx: 130 |  Loss: (0.1996) |  Loss2: (0.0000) | Acc: (92.00%) (15592/16768)
Epoch: 109 | Batch_idx: 140 |  Loss: (0.1989) |  Loss2: (0.0000) | Acc: (92.00%) (16780/18048)
Epoch: 109 | Batch_idx: 150 |  Loss: (0.1999) |  Loss2: (0.0000) | Acc: (92.00%) (17963/19328)
Epoch: 109 | Batch_idx: 160 |  Loss: (0.1997) |  Loss2: (0.0000) | Acc: (92.00%) (19152/20608)
Epoch: 109 | Batch_idx: 170 |  Loss: (0.1994) |  Loss2: (0.0000) | Acc: (92.00%) (20349/21888)
Epoch: 109 | Batch_idx: 180 |  Loss: (0.1983) |  Loss2: (0.0000) | Acc: (93.00%) (21553/23168)
Epoch: 109 | Batch_idx: 190 |  Loss: (0.1990) |  Loss2: (0.0000) | Acc: (93.00%) (22744/24448)
Epoch: 109 | Batch_idx: 200 |  Loss: (0.1993) |  Loss2: (0.0000) | Acc: (93.00%) (23930/25728)
Epoch: 109 | Batch_idx: 210 |  Loss: (0.2008) |  Loss2: (0.0000) | Acc: (92.00%) (25103/27008)
Epoch: 109 | Batch_idx: 220 |  Loss: (0.2010) |  Loss2: (0.0000) | Acc: (92.00%) (26292/28288)
Epoch: 109 | Batch_idx: 230 |  Loss: (0.2015) |  Loss2: (0.0000) | Acc: (92.00%) (27480/29568)
Epoch: 109 | Batch_idx: 240 |  Loss: (0.2017) |  Loss2: (0.0000) | Acc: (92.00%) (28669/30848)
Epoch: 109 | Batch_idx: 250 |  Loss: (0.2019) |  Loss2: (0.0000) | Acc: (92.00%) (29859/32128)
Epoch: 109 | Batch_idx: 260 |  Loss: (0.2023) |  Loss2: (0.0000) | Acc: (92.00%) (31050/33408)
Epoch: 109 | Batch_idx: 270 |  Loss: (0.2025) |  Loss2: (0.0000) | Acc: (92.00%) (32246/34688)
Epoch: 109 | Batch_idx: 280 |  Loss: (0.2030) |  Loss2: (0.0000) | Acc: (92.00%) (33437/35968)
Epoch: 109 | Batch_idx: 290 |  Loss: (0.2023) |  Loss2: (0.0000) | Acc: (92.00%) (34636/37248)
Epoch: 109 | Batch_idx: 300 |  Loss: (0.2018) |  Loss2: (0.0000) | Acc: (92.00%) (35829/38528)
Epoch: 109 | Batch_idx: 310 |  Loss: (0.2019) |  Loss2: (0.0000) | Acc: (93.00%) (37022/39808)
Epoch: 109 | Batch_idx: 320 |  Loss: (0.2020) |  Loss2: (0.0000) | Acc: (92.00%) (38206/41088)
Epoch: 109 | Batch_idx: 330 |  Loss: (0.2021) |  Loss2: (0.0000) | Acc: (92.00%) (39397/42368)
Epoch: 109 | Batch_idx: 340 |  Loss: (0.2013) |  Loss2: (0.0000) | Acc: (93.00%) (40606/43648)
Epoch: 109 | Batch_idx: 350 |  Loss: (0.2014) |  Loss2: (0.0000) | Acc: (93.00%) (41787/44928)
Epoch: 109 | Batch_idx: 360 |  Loss: (0.2008) |  Loss2: (0.0000) | Acc: (93.00%) (42993/46208)
Epoch: 109 | Batch_idx: 370 |  Loss: (0.2012) |  Loss2: (0.0000) | Acc: (93.00%) (44172/47488)
Epoch: 109 | Batch_idx: 380 |  Loss: (0.2016) |  Loss2: (0.0000) | Acc: (93.00%) (45356/48768)
Epoch: 109 | Batch_idx: 390 |  Loss: (0.2013) |  Loss2: (0.0000) | Acc: (93.00%) (46505/50000)
# TEST : Loss: (0.4104) | Acc: (87.00%) (8722/10000)
percent tensor([0.5308, 0.4692], device='cuda:0')
percent tensor([0.4993, 0.5007], device='cuda:0')
percent tensor([0.5244, 0.4756], device='cuda:0')
percent tensor([0.6200, 0.3800], device='cuda:0')
percent tensor([0.6282, 0.3718], device='cuda:0')
percent tensor([0.6256, 0.3744], device='cuda:0')
percent tensor([0.7240, 0.2760], device='cuda:0')
percent tensor([0.9988, 0.0012], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 110 | Batch_idx: 0 |  Loss: (0.2010) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 110 | Batch_idx: 10 |  Loss: (0.1969) |  Loss2: (0.0000) | Acc: (93.00%) (1312/1408)
Epoch: 110 | Batch_idx: 20 |  Loss: (0.2019) |  Loss2: (0.0000) | Acc: (93.00%) (2500/2688)
Epoch: 110 | Batch_idx: 30 |  Loss: (0.2045) |  Loss2: (0.0000) | Acc: (92.00%) (3677/3968)
Epoch: 110 | Batch_idx: 40 |  Loss: (0.2024) |  Loss2: (0.0000) | Acc: (92.00%) (4874/5248)
Epoch: 110 | Batch_idx: 50 |  Loss: (0.2088) |  Loss2: (0.0000) | Acc: (92.00%) (6056/6528)
Epoch: 110 | Batch_idx: 60 |  Loss: (0.2060) |  Loss2: (0.0000) | Acc: (92.00%) (7256/7808)
Epoch: 110 | Batch_idx: 70 |  Loss: (0.2032) |  Loss2: (0.0000) | Acc: (93.00%) (8459/9088)
Epoch: 110 | Batch_idx: 80 |  Loss: (0.2019) |  Loss2: (0.0000) | Acc: (93.00%) (9655/10368)
Epoch: 110 | Batch_idx: 90 |  Loss: (0.2015) |  Loss2: (0.0000) | Acc: (93.00%) (10854/11648)
Epoch: 110 | Batch_idx: 100 |  Loss: (0.2018) |  Loss2: (0.0000) | Acc: (93.00%) (12035/12928)
Epoch: 110 | Batch_idx: 110 |  Loss: (0.2006) |  Loss2: (0.0000) | Acc: (93.00%) (13239/14208)
Epoch: 110 | Batch_idx: 120 |  Loss: (0.2027) |  Loss2: (0.0000) | Acc: (93.00%) (14408/15488)
Epoch: 110 | Batch_idx: 130 |  Loss: (0.2014) |  Loss2: (0.0000) | Acc: (93.00%) (15606/16768)
Epoch: 110 | Batch_idx: 140 |  Loss: (0.1991) |  Loss2: (0.0000) | Acc: (93.00%) (16817/18048)
Epoch: 110 | Batch_idx: 150 |  Loss: (0.2014) |  Loss2: (0.0000) | Acc: (93.00%) (17996/19328)
Epoch: 110 | Batch_idx: 160 |  Loss: (0.2020) |  Loss2: (0.0000) | Acc: (93.00%) (19190/20608)
Epoch: 110 | Batch_idx: 170 |  Loss: (0.2013) |  Loss2: (0.0000) | Acc: (93.00%) (20396/21888)
Epoch: 110 | Batch_idx: 180 |  Loss: (0.2024) |  Loss2: (0.0000) | Acc: (93.00%) (21589/23168)
Epoch: 110 | Batch_idx: 190 |  Loss: (0.2024) |  Loss2: (0.0000) | Acc: (93.00%) (22785/24448)
Epoch: 110 | Batch_idx: 200 |  Loss: (0.2022) |  Loss2: (0.0000) | Acc: (93.00%) (23980/25728)
Epoch: 110 | Batch_idx: 210 |  Loss: (0.2023) |  Loss2: (0.0000) | Acc: (93.00%) (25164/27008)
Epoch: 110 | Batch_idx: 220 |  Loss: (0.2021) |  Loss2: (0.0000) | Acc: (93.00%) (26367/28288)
Epoch: 110 | Batch_idx: 230 |  Loss: (0.2024) |  Loss2: (0.0000) | Acc: (93.00%) (27554/29568)
Epoch: 110 | Batch_idx: 240 |  Loss: (0.2021) |  Loss2: (0.0000) | Acc: (93.00%) (28759/30848)
Epoch: 110 | Batch_idx: 250 |  Loss: (0.2030) |  Loss2: (0.0000) | Acc: (93.00%) (29937/32128)
Epoch: 110 | Batch_idx: 260 |  Loss: (0.2026) |  Loss2: (0.0000) | Acc: (93.00%) (31133/33408)
Epoch: 110 | Batch_idx: 270 |  Loss: (0.2026) |  Loss2: (0.0000) | Acc: (93.00%) (32317/34688)
Epoch: 110 | Batch_idx: 280 |  Loss: (0.2019) |  Loss2: (0.0000) | Acc: (93.00%) (33514/35968)
Epoch: 110 | Batch_idx: 290 |  Loss: (0.2028) |  Loss2: (0.0000) | Acc: (93.00%) (34692/37248)
Epoch: 110 | Batch_idx: 300 |  Loss: (0.2039) |  Loss2: (0.0000) | Acc: (93.00%) (35873/38528)
Epoch: 110 | Batch_idx: 310 |  Loss: (0.2036) |  Loss2: (0.0000) | Acc: (93.00%) (37073/39808)
Epoch: 110 | Batch_idx: 320 |  Loss: (0.2029) |  Loss2: (0.0000) | Acc: (93.00%) (38280/41088)
Epoch: 110 | Batch_idx: 330 |  Loss: (0.2022) |  Loss2: (0.0000) | Acc: (93.00%) (39488/42368)
Epoch: 110 | Batch_idx: 340 |  Loss: (0.2028) |  Loss2: (0.0000) | Acc: (93.00%) (40657/43648)
Epoch: 110 | Batch_idx: 350 |  Loss: (0.2032) |  Loss2: (0.0000) | Acc: (93.00%) (41847/44928)
Epoch: 110 | Batch_idx: 360 |  Loss: (0.2029) |  Loss2: (0.0000) | Acc: (93.00%) (43040/46208)
Epoch: 110 | Batch_idx: 370 |  Loss: (0.2036) |  Loss2: (0.0000) | Acc: (93.00%) (44214/47488)
Epoch: 110 | Batch_idx: 380 |  Loss: (0.2034) |  Loss2: (0.0000) | Acc: (93.00%) (45409/48768)
Epoch: 110 | Batch_idx: 390 |  Loss: (0.2029) |  Loss2: (0.0000) | Acc: (93.00%) (46569/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_110.pth.tar'
# TEST : Loss: (0.4707) | Acc: (85.00%) (8571/10000)
percent tensor([0.5307, 0.4693], device='cuda:0')
percent tensor([0.4993, 0.5007], device='cuda:0')
percent tensor([0.5240, 0.4760], device='cuda:0')
percent tensor([0.6203, 0.3797], device='cuda:0')
percent tensor([0.6271, 0.3729], device='cuda:0')
percent tensor([0.6262, 0.3738], device='cuda:0')
percent tensor([0.7196, 0.2804], device='cuda:0')
percent tensor([0.9987, 0.0013], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(180.8369, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(820.6205, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(812.8804, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1510.0594, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(502.1016, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2244.9351, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4272.7163, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1396.8903, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6175.6221, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11837.7373, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3924.3916, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16573.3262, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 111 | Batch_idx: 0 |  Loss: (0.2161) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 111 | Batch_idx: 10 |  Loss: (0.1833) |  Loss2: (0.0000) | Acc: (93.00%) (1314/1408)
Epoch: 111 | Batch_idx: 20 |  Loss: (0.1932) |  Loss2: (0.0000) | Acc: (93.00%) (2505/2688)
Epoch: 111 | Batch_idx: 30 |  Loss: (0.1946) |  Loss2: (0.0000) | Acc: (93.00%) (3704/3968)
Epoch: 111 | Batch_idx: 40 |  Loss: (0.1925) |  Loss2: (0.0000) | Acc: (93.00%) (4904/5248)
Epoch: 111 | Batch_idx: 50 |  Loss: (0.1875) |  Loss2: (0.0000) | Acc: (93.00%) (6112/6528)
Epoch: 111 | Batch_idx: 60 |  Loss: (0.1848) |  Loss2: (0.0000) | Acc: (93.00%) (7316/7808)
Epoch: 111 | Batch_idx: 70 |  Loss: (0.1850) |  Loss2: (0.0000) | Acc: (93.00%) (8515/9088)
Epoch: 111 | Batch_idx: 80 |  Loss: (0.1844) |  Loss2: (0.0000) | Acc: (93.00%) (9719/10368)
Epoch: 111 | Batch_idx: 90 |  Loss: (0.1847) |  Loss2: (0.0000) | Acc: (93.00%) (10916/11648)
Epoch: 111 | Batch_idx: 100 |  Loss: (0.1870) |  Loss2: (0.0000) | Acc: (93.00%) (12101/12928)
Epoch: 111 | Batch_idx: 110 |  Loss: (0.1900) |  Loss2: (0.0000) | Acc: (93.00%) (13289/14208)
Epoch: 111 | Batch_idx: 120 |  Loss: (0.1902) |  Loss2: (0.0000) | Acc: (93.00%) (14492/15488)
Epoch: 111 | Batch_idx: 130 |  Loss: (0.1905) |  Loss2: (0.0000) | Acc: (93.00%) (15678/16768)
Epoch: 111 | Batch_idx: 140 |  Loss: (0.1897) |  Loss2: (0.0000) | Acc: (93.00%) (16879/18048)
Epoch: 111 | Batch_idx: 150 |  Loss: (0.1887) |  Loss2: (0.0000) | Acc: (93.00%) (18083/19328)
Epoch: 111 | Batch_idx: 160 |  Loss: (0.1879) |  Loss2: (0.0000) | Acc: (93.00%) (19286/20608)
Epoch: 111 | Batch_idx: 170 |  Loss: (0.1884) |  Loss2: (0.0000) | Acc: (93.00%) (20484/21888)
Epoch: 111 | Batch_idx: 180 |  Loss: (0.1894) |  Loss2: (0.0000) | Acc: (93.00%) (21671/23168)
Epoch: 111 | Batch_idx: 190 |  Loss: (0.1888) |  Loss2: (0.0000) | Acc: (93.00%) (22873/24448)
Epoch: 111 | Batch_idx: 200 |  Loss: (0.1892) |  Loss2: (0.0000) | Acc: (93.00%) (24069/25728)
Epoch: 111 | Batch_idx: 210 |  Loss: (0.1890) |  Loss2: (0.0000) | Acc: (93.00%) (25263/27008)
Epoch: 111 | Batch_idx: 220 |  Loss: (0.1902) |  Loss2: (0.0000) | Acc: (93.00%) (26453/28288)
Epoch: 111 | Batch_idx: 230 |  Loss: (0.1913) |  Loss2: (0.0000) | Acc: (93.00%) (27631/29568)
Epoch: 111 | Batch_idx: 240 |  Loss: (0.1915) |  Loss2: (0.0000) | Acc: (93.00%) (28824/30848)
Epoch: 111 | Batch_idx: 250 |  Loss: (0.1912) |  Loss2: (0.0000) | Acc: (93.00%) (30014/32128)
Epoch: 111 | Batch_idx: 260 |  Loss: (0.1916) |  Loss2: (0.0000) | Acc: (93.00%) (31210/33408)
Epoch: 111 | Batch_idx: 270 |  Loss: (0.1914) |  Loss2: (0.0000) | Acc: (93.00%) (32405/34688)
Epoch: 111 | Batch_idx: 280 |  Loss: (0.1918) |  Loss2: (0.0000) | Acc: (93.00%) (33594/35968)
Epoch: 111 | Batch_idx: 290 |  Loss: (0.1913) |  Loss2: (0.0000) | Acc: (93.00%) (34802/37248)
Epoch: 111 | Batch_idx: 300 |  Loss: (0.1914) |  Loss2: (0.0000) | Acc: (93.00%) (35991/38528)
Epoch: 111 | Batch_idx: 310 |  Loss: (0.1925) |  Loss2: (0.0000) | Acc: (93.00%) (37170/39808)
Epoch: 111 | Batch_idx: 320 |  Loss: (0.1926) |  Loss2: (0.0000) | Acc: (93.00%) (38377/41088)
Epoch: 111 | Batch_idx: 330 |  Loss: (0.1922) |  Loss2: (0.0000) | Acc: (93.00%) (39578/42368)
Epoch: 111 | Batch_idx: 340 |  Loss: (0.1920) |  Loss2: (0.0000) | Acc: (93.00%) (40774/43648)
Epoch: 111 | Batch_idx: 350 |  Loss: (0.1928) |  Loss2: (0.0000) | Acc: (93.00%) (41957/44928)
Epoch: 111 | Batch_idx: 360 |  Loss: (0.1936) |  Loss2: (0.0000) | Acc: (93.00%) (43142/46208)
Epoch: 111 | Batch_idx: 370 |  Loss: (0.1931) |  Loss2: (0.0000) | Acc: (93.00%) (44347/47488)
Epoch: 111 | Batch_idx: 380 |  Loss: (0.1931) |  Loss2: (0.0000) | Acc: (93.00%) (45537/48768)
Epoch: 111 | Batch_idx: 390 |  Loss: (0.1939) |  Loss2: (0.0000) | Acc: (93.00%) (46676/50000)
# TEST : Loss: (0.5432) | Acc: (84.00%) (8433/10000)
percent tensor([0.5303, 0.4697], device='cuda:0')
percent tensor([0.4993, 0.5007], device='cuda:0')
percent tensor([0.5241, 0.4759], device='cuda:0')
percent tensor([0.6200, 0.3800], device='cuda:0')
percent tensor([0.6297, 0.3703], device='cuda:0')
percent tensor([0.6243, 0.3757], device='cuda:0')
percent tensor([0.7286, 0.2714], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
Epoch: 112 | Batch_idx: 0 |  Loss: (0.1700) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 112 | Batch_idx: 10 |  Loss: (0.1882) |  Loss2: (0.0000) | Acc: (93.00%) (1318/1408)
Epoch: 112 | Batch_idx: 20 |  Loss: (0.1819) |  Loss2: (0.0000) | Acc: (93.00%) (2517/2688)
Epoch: 112 | Batch_idx: 30 |  Loss: (0.1794) |  Loss2: (0.0000) | Acc: (93.00%) (3724/3968)
Epoch: 112 | Batch_idx: 40 |  Loss: (0.1904) |  Loss2: (0.0000) | Acc: (93.00%) (4900/5248)
Epoch: 112 | Batch_idx: 50 |  Loss: (0.1889) |  Loss2: (0.0000) | Acc: (93.00%) (6100/6528)
Epoch: 112 | Batch_idx: 60 |  Loss: (0.1873) |  Loss2: (0.0000) | Acc: (93.00%) (7292/7808)
Epoch: 112 | Batch_idx: 70 |  Loss: (0.1891) |  Loss2: (0.0000) | Acc: (93.00%) (8475/9088)
Epoch: 112 | Batch_idx: 80 |  Loss: (0.1852) |  Loss2: (0.0000) | Acc: (93.00%) (9692/10368)
Epoch: 112 | Batch_idx: 90 |  Loss: (0.1851) |  Loss2: (0.0000) | Acc: (93.00%) (10896/11648)
Epoch: 112 | Batch_idx: 100 |  Loss: (0.1842) |  Loss2: (0.0000) | Acc: (93.00%) (12095/12928)
Epoch: 112 | Batch_idx: 110 |  Loss: (0.1860) |  Loss2: (0.0000) | Acc: (93.00%) (13286/14208)
Epoch: 112 | Batch_idx: 120 |  Loss: (0.1868) |  Loss2: (0.0000) | Acc: (93.00%) (14488/15488)
Epoch: 112 | Batch_idx: 130 |  Loss: (0.1876) |  Loss2: (0.0000) | Acc: (93.00%) (15683/16768)
Epoch: 112 | Batch_idx: 140 |  Loss: (0.1875) |  Loss2: (0.0000) | Acc: (93.00%) (16879/18048)
Epoch: 112 | Batch_idx: 150 |  Loss: (0.1884) |  Loss2: (0.0000) | Acc: (93.00%) (18073/19328)
Epoch: 112 | Batch_idx: 160 |  Loss: (0.1873) |  Loss2: (0.0000) | Acc: (93.00%) (19285/20608)
Epoch: 112 | Batch_idx: 170 |  Loss: (0.1877) |  Loss2: (0.0000) | Acc: (93.00%) (20476/21888)
Epoch: 112 | Batch_idx: 180 |  Loss: (0.1884) |  Loss2: (0.0000) | Acc: (93.00%) (21666/23168)
Epoch: 112 | Batch_idx: 190 |  Loss: (0.1893) |  Loss2: (0.0000) | Acc: (93.00%) (22858/24448)
Epoch: 112 | Batch_idx: 200 |  Loss: (0.1899) |  Loss2: (0.0000) | Acc: (93.00%) (24044/25728)
Epoch: 112 | Batch_idx: 210 |  Loss: (0.1909) |  Loss2: (0.0000) | Acc: (93.00%) (25227/27008)
Epoch: 112 | Batch_idx: 220 |  Loss: (0.1914) |  Loss2: (0.0000) | Acc: (93.00%) (26401/28288)
Epoch: 112 | Batch_idx: 230 |  Loss: (0.1927) |  Loss2: (0.0000) | Acc: (93.00%) (27592/29568)
Epoch: 112 | Batch_idx: 240 |  Loss: (0.1936) |  Loss2: (0.0000) | Acc: (93.00%) (28771/30848)
Epoch: 112 | Batch_idx: 250 |  Loss: (0.1947) |  Loss2: (0.0000) | Acc: (93.00%) (29947/32128)
Epoch: 112 | Batch_idx: 260 |  Loss: (0.1948) |  Loss2: (0.0000) | Acc: (93.00%) (31134/33408)
Epoch: 112 | Batch_idx: 270 |  Loss: (0.1949) |  Loss2: (0.0000) | Acc: (93.00%) (32333/34688)
Epoch: 112 | Batch_idx: 280 |  Loss: (0.1950) |  Loss2: (0.0000) | Acc: (93.00%) (33531/35968)
Epoch: 112 | Batch_idx: 290 |  Loss: (0.1953) |  Loss2: (0.0000) | Acc: (93.00%) (34716/37248)
Epoch: 112 | Batch_idx: 300 |  Loss: (0.1948) |  Loss2: (0.0000) | Acc: (93.00%) (35920/38528)
Epoch: 112 | Batch_idx: 310 |  Loss: (0.1947) |  Loss2: (0.0000) | Acc: (93.00%) (37115/39808)
Epoch: 112 | Batch_idx: 320 |  Loss: (0.1963) |  Loss2: (0.0000) | Acc: (93.00%) (38280/41088)
Epoch: 112 | Batch_idx: 330 |  Loss: (0.1959) |  Loss2: (0.0000) | Acc: (93.00%) (39478/42368)
Epoch: 112 | Batch_idx: 340 |  Loss: (0.1962) |  Loss2: (0.0000) | Acc: (93.00%) (40667/43648)
Epoch: 112 | Batch_idx: 350 |  Loss: (0.1962) |  Loss2: (0.0000) | Acc: (93.00%) (41865/44928)
Epoch: 112 | Batch_idx: 360 |  Loss: (0.1963) |  Loss2: (0.0000) | Acc: (93.00%) (43056/46208)
Epoch: 112 | Batch_idx: 370 |  Loss: (0.1964) |  Loss2: (0.0000) | Acc: (93.00%) (44248/47488)
Epoch: 112 | Batch_idx: 380 |  Loss: (0.1962) |  Loss2: (0.0000) | Acc: (93.00%) (45438/48768)
Epoch: 112 | Batch_idx: 390 |  Loss: (0.1964) |  Loss2: (0.0000) | Acc: (93.00%) (46576/50000)
# TEST : Loss: (0.4023) | Acc: (87.00%) (8729/10000)
percent tensor([0.5307, 0.4693], device='cuda:0')
percent tensor([0.4993, 0.5007], device='cuda:0')
percent tensor([0.5246, 0.4754], device='cuda:0')
percent tensor([0.6210, 0.3790], device='cuda:0')
percent tensor([0.6271, 0.3729], device='cuda:0')
percent tensor([0.6249, 0.3751], device='cuda:0')
percent tensor([0.7190, 0.2810], device='cuda:0')
percent tensor([0.9987, 0.0013], device='cuda:0')
Epoch: 113 | Batch_idx: 0 |  Loss: (0.1504) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 113 | Batch_idx: 10 |  Loss: (0.1659) |  Loss2: (0.0000) | Acc: (94.00%) (1329/1408)
Epoch: 113 | Batch_idx: 20 |  Loss: (0.1613) |  Loss2: (0.0000) | Acc: (94.00%) (2549/2688)
Epoch: 113 | Batch_idx: 30 |  Loss: (0.1657) |  Loss2: (0.0000) | Acc: (94.00%) (3756/3968)
Epoch: 113 | Batch_idx: 40 |  Loss: (0.1563) |  Loss2: (0.0000) | Acc: (95.00%) (4991/5248)
Epoch: 113 | Batch_idx: 50 |  Loss: (0.1609) |  Loss2: (0.0000) | Acc: (94.00%) (6187/6528)
Epoch: 113 | Batch_idx: 60 |  Loss: (0.1659) |  Loss2: (0.0000) | Acc: (94.00%) (7383/7808)
Epoch: 113 | Batch_idx: 70 |  Loss: (0.1668) |  Loss2: (0.0000) | Acc: (94.00%) (8596/9088)
Epoch: 113 | Batch_idx: 80 |  Loss: (0.1664) |  Loss2: (0.0000) | Acc: (94.00%) (9804/10368)
Epoch: 113 | Batch_idx: 90 |  Loss: (0.1652) |  Loss2: (0.0000) | Acc: (94.00%) (11018/11648)
Epoch: 113 | Batch_idx: 100 |  Loss: (0.1648) |  Loss2: (0.0000) | Acc: (94.00%) (12227/12928)
Epoch: 113 | Batch_idx: 110 |  Loss: (0.1668) |  Loss2: (0.0000) | Acc: (94.00%) (13420/14208)
Epoch: 113 | Batch_idx: 120 |  Loss: (0.1665) |  Loss2: (0.0000) | Acc: (94.00%) (14629/15488)
Epoch: 113 | Batch_idx: 130 |  Loss: (0.1680) |  Loss2: (0.0000) | Acc: (94.00%) (15833/16768)
Epoch: 113 | Batch_idx: 140 |  Loss: (0.1697) |  Loss2: (0.0000) | Acc: (94.00%) (17019/18048)
Epoch: 113 | Batch_idx: 150 |  Loss: (0.1718) |  Loss2: (0.0000) | Acc: (94.00%) (18216/19328)
Epoch: 113 | Batch_idx: 160 |  Loss: (0.1713) |  Loss2: (0.0000) | Acc: (94.00%) (19425/20608)
Epoch: 113 | Batch_idx: 170 |  Loss: (0.1742) |  Loss2: (0.0000) | Acc: (94.00%) (20611/21888)
Epoch: 113 | Batch_idx: 180 |  Loss: (0.1768) |  Loss2: (0.0000) | Acc: (94.00%) (21797/23168)
Epoch: 113 | Batch_idx: 190 |  Loss: (0.1788) |  Loss2: (0.0000) | Acc: (93.00%) (22980/24448)
Epoch: 113 | Batch_idx: 200 |  Loss: (0.1786) |  Loss2: (0.0000) | Acc: (94.00%) (24192/25728)
Epoch: 113 | Batch_idx: 210 |  Loss: (0.1780) |  Loss2: (0.0000) | Acc: (94.00%) (25392/27008)
Epoch: 113 | Batch_idx: 220 |  Loss: (0.1775) |  Loss2: (0.0000) | Acc: (94.00%) (26602/28288)
Epoch: 113 | Batch_idx: 230 |  Loss: (0.1785) |  Loss2: (0.0000) | Acc: (93.00%) (27793/29568)
Epoch: 113 | Batch_idx: 240 |  Loss: (0.1798) |  Loss2: (0.0000) | Acc: (93.00%) (28980/30848)
Epoch: 113 | Batch_idx: 250 |  Loss: (0.1797) |  Loss2: (0.0000) | Acc: (93.00%) (30184/32128)
Epoch: 113 | Batch_idx: 260 |  Loss: (0.1797) |  Loss2: (0.0000) | Acc: (93.00%) (31377/33408)
Epoch: 113 | Batch_idx: 270 |  Loss: (0.1802) |  Loss2: (0.0000) | Acc: (93.00%) (32573/34688)
Epoch: 113 | Batch_idx: 280 |  Loss: (0.1808) |  Loss2: (0.0000) | Acc: (93.00%) (33757/35968)
Epoch: 113 | Batch_idx: 290 |  Loss: (0.1807) |  Loss2: (0.0000) | Acc: (93.00%) (34958/37248)
Epoch: 113 | Batch_idx: 300 |  Loss: (0.1812) |  Loss2: (0.0000) | Acc: (93.00%) (36137/38528)
Epoch: 113 | Batch_idx: 310 |  Loss: (0.1824) |  Loss2: (0.0000) | Acc: (93.00%) (37312/39808)
Epoch: 113 | Batch_idx: 320 |  Loss: (0.1828) |  Loss2: (0.0000) | Acc: (93.00%) (38508/41088)
Epoch: 113 | Batch_idx: 330 |  Loss: (0.1830) |  Loss2: (0.0000) | Acc: (93.00%) (39702/42368)
Epoch: 113 | Batch_idx: 340 |  Loss: (0.1837) |  Loss2: (0.0000) | Acc: (93.00%) (40882/43648)
Epoch: 113 | Batch_idx: 350 |  Loss: (0.1844) |  Loss2: (0.0000) | Acc: (93.00%) (42060/44928)
Epoch: 113 | Batch_idx: 360 |  Loss: (0.1850) |  Loss2: (0.0000) | Acc: (93.00%) (43242/46208)
Epoch: 113 | Batch_idx: 370 |  Loss: (0.1843) |  Loss2: (0.0000) | Acc: (93.00%) (44456/47488)
Epoch: 113 | Batch_idx: 380 |  Loss: (0.1845) |  Loss2: (0.0000) | Acc: (93.00%) (45659/48768)
Epoch: 113 | Batch_idx: 390 |  Loss: (0.1847) |  Loss2: (0.0000) | Acc: (93.00%) (46809/50000)
# TEST : Loss: (0.4462) | Acc: (86.00%) (8637/10000)
percent tensor([0.5306, 0.4694], device='cuda:0')
percent tensor([0.4993, 0.5007], device='cuda:0')
percent tensor([0.5247, 0.4753], device='cuda:0')
percent tensor([0.6209, 0.3791], device='cuda:0')
percent tensor([0.6316, 0.3684], device='cuda:0')
percent tensor([0.6271, 0.3729], device='cuda:0')
percent tensor([0.7292, 0.2708], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
Epoch: 114 | Batch_idx: 0 |  Loss: (0.1290) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 114 | Batch_idx: 10 |  Loss: (0.1708) |  Loss2: (0.0000) | Acc: (94.00%) (1324/1408)
Epoch: 114 | Batch_idx: 20 |  Loss: (0.1761) |  Loss2: (0.0000) | Acc: (93.00%) (2523/2688)
Epoch: 114 | Batch_idx: 30 |  Loss: (0.1761) |  Loss2: (0.0000) | Acc: (93.00%) (3726/3968)
Epoch: 114 | Batch_idx: 40 |  Loss: (0.1717) |  Loss2: (0.0000) | Acc: (94.00%) (4940/5248)
Epoch: 114 | Batch_idx: 50 |  Loss: (0.1699) |  Loss2: (0.0000) | Acc: (94.00%) (6150/6528)
Epoch: 114 | Batch_idx: 60 |  Loss: (0.1696) |  Loss2: (0.0000) | Acc: (94.00%) (7360/7808)
Epoch: 114 | Batch_idx: 70 |  Loss: (0.1673) |  Loss2: (0.0000) | Acc: (94.00%) (8582/9088)
Epoch: 114 | Batch_idx: 80 |  Loss: (0.1706) |  Loss2: (0.0000) | Acc: (94.00%) (9785/10368)
Epoch: 114 | Batch_idx: 90 |  Loss: (0.1723) |  Loss2: (0.0000) | Acc: (94.00%) (10977/11648)
Epoch: 114 | Batch_idx: 100 |  Loss: (0.1735) |  Loss2: (0.0000) | Acc: (94.00%) (12174/12928)
Epoch: 114 | Batch_idx: 110 |  Loss: (0.1750) |  Loss2: (0.0000) | Acc: (94.00%) (13371/14208)
Epoch: 114 | Batch_idx: 120 |  Loss: (0.1734) |  Loss2: (0.0000) | Acc: (94.00%) (14580/15488)
Epoch: 114 | Batch_idx: 130 |  Loss: (0.1721) |  Loss2: (0.0000) | Acc: (94.00%) (15796/16768)
Epoch: 114 | Batch_idx: 140 |  Loss: (0.1740) |  Loss2: (0.0000) | Acc: (94.00%) (16988/18048)
Epoch: 114 | Batch_idx: 150 |  Loss: (0.1736) |  Loss2: (0.0000) | Acc: (94.00%) (18195/19328)
Epoch: 114 | Batch_idx: 160 |  Loss: (0.1743) |  Loss2: (0.0000) | Acc: (94.00%) (19393/20608)
Epoch: 114 | Batch_idx: 170 |  Loss: (0.1731) |  Loss2: (0.0000) | Acc: (94.00%) (20605/21888)
Epoch: 114 | Batch_idx: 180 |  Loss: (0.1733) |  Loss2: (0.0000) | Acc: (94.00%) (21808/23168)
Epoch: 114 | Batch_idx: 190 |  Loss: (0.1733) |  Loss2: (0.0000) | Acc: (94.00%) (23006/24448)
Epoch: 114 | Batch_idx: 200 |  Loss: (0.1739) |  Loss2: (0.0000) | Acc: (94.00%) (24208/25728)
Epoch: 114 | Batch_idx: 210 |  Loss: (0.1742) |  Loss2: (0.0000) | Acc: (94.00%) (25413/27008)
Epoch: 114 | Batch_idx: 220 |  Loss: (0.1750) |  Loss2: (0.0000) | Acc: (94.00%) (26604/28288)
Epoch: 114 | Batch_idx: 230 |  Loss: (0.1749) |  Loss2: (0.0000) | Acc: (94.00%) (27809/29568)
Epoch: 114 | Batch_idx: 240 |  Loss: (0.1748) |  Loss2: (0.0000) | Acc: (94.00%) (29020/30848)
Epoch: 114 | Batch_idx: 250 |  Loss: (0.1752) |  Loss2: (0.0000) | Acc: (94.00%) (30215/32128)
Epoch: 114 | Batch_idx: 260 |  Loss: (0.1779) |  Loss2: (0.0000) | Acc: (93.00%) (31380/33408)
Epoch: 114 | Batch_idx: 270 |  Loss: (0.1785) |  Loss2: (0.0000) | Acc: (93.00%) (32569/34688)
Epoch: 114 | Batch_idx: 280 |  Loss: (0.1779) |  Loss2: (0.0000) | Acc: (93.00%) (33784/35968)
Epoch: 114 | Batch_idx: 290 |  Loss: (0.1788) |  Loss2: (0.0000) | Acc: (93.00%) (34973/37248)
Epoch: 114 | Batch_idx: 300 |  Loss: (0.1783) |  Loss2: (0.0000) | Acc: (93.00%) (36177/38528)
Epoch: 114 | Batch_idx: 310 |  Loss: (0.1792) |  Loss2: (0.0000) | Acc: (93.00%) (37357/39808)
Epoch: 114 | Batch_idx: 320 |  Loss: (0.1793) |  Loss2: (0.0000) | Acc: (93.00%) (38558/41088)
Epoch: 114 | Batch_idx: 330 |  Loss: (0.1791) |  Loss2: (0.0000) | Acc: (93.00%) (39759/42368)
Epoch: 114 | Batch_idx: 340 |  Loss: (0.1796) |  Loss2: (0.0000) | Acc: (93.00%) (40950/43648)
Epoch: 114 | Batch_idx: 350 |  Loss: (0.1804) |  Loss2: (0.0000) | Acc: (93.00%) (42141/44928)
Epoch: 114 | Batch_idx: 360 |  Loss: (0.1808) |  Loss2: (0.0000) | Acc: (93.00%) (43338/46208)
Epoch: 114 | Batch_idx: 370 |  Loss: (0.1807) |  Loss2: (0.0000) | Acc: (93.00%) (44547/47488)
Epoch: 114 | Batch_idx: 380 |  Loss: (0.1807) |  Loss2: (0.0000) | Acc: (93.00%) (45748/48768)
Epoch: 114 | Batch_idx: 390 |  Loss: (0.1809) |  Loss2: (0.0000) | Acc: (93.00%) (46899/50000)
# TEST : Loss: (0.4194) | Acc: (87.00%) (8734/10000)
percent tensor([0.5308, 0.4692], device='cuda:0')
percent tensor([0.4993, 0.5007], device='cuda:0')
percent tensor([0.5241, 0.4759], device='cuda:0')
percent tensor([0.6205, 0.3795], device='cuda:0')
percent tensor([0.6268, 0.3732], device='cuda:0')
percent tensor([0.6270, 0.3730], device='cuda:0')
percent tensor([0.7239, 0.2761], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 115 | Batch_idx: 0 |  Loss: (0.1815) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 115 | Batch_idx: 10 |  Loss: (0.1744) |  Loss2: (0.0000) | Acc: (94.00%) (1325/1408)
Epoch: 115 | Batch_idx: 20 |  Loss: (0.1939) |  Loss2: (0.0000) | Acc: (93.00%) (2511/2688)
Epoch: 115 | Batch_idx: 30 |  Loss: (0.1950) |  Loss2: (0.0000) | Acc: (93.00%) (3705/3968)
Epoch: 115 | Batch_idx: 40 |  Loss: (0.1992) |  Loss2: (0.0000) | Acc: (93.00%) (4887/5248)
Epoch: 115 | Batch_idx: 50 |  Loss: (0.2015) |  Loss2: (0.0000) | Acc: (92.00%) (6068/6528)
Epoch: 115 | Batch_idx: 60 |  Loss: (0.2045) |  Loss2: (0.0000) | Acc: (92.00%) (7251/7808)
Epoch: 115 | Batch_idx: 70 |  Loss: (0.2093) |  Loss2: (0.0000) | Acc: (92.00%) (8429/9088)
Epoch: 115 | Batch_idx: 80 |  Loss: (0.2104) |  Loss2: (0.0000) | Acc: (92.00%) (9609/10368)
Epoch: 115 | Batch_idx: 90 |  Loss: (0.2159) |  Loss2: (0.0000) | Acc: (92.00%) (10768/11648)
Epoch: 115 | Batch_idx: 100 |  Loss: (0.2143) |  Loss2: (0.0000) | Acc: (92.00%) (11963/12928)
Epoch: 115 | Batch_idx: 110 |  Loss: (0.2179) |  Loss2: (0.0000) | Acc: (92.00%) (13132/14208)
Epoch: 115 | Batch_idx: 120 |  Loss: (0.2156) |  Loss2: (0.0000) | Acc: (92.00%) (14330/15488)
Epoch: 115 | Batch_idx: 130 |  Loss: (0.2124) |  Loss2: (0.0000) | Acc: (92.00%) (15526/16768)
Epoch: 115 | Batch_idx: 140 |  Loss: (0.2132) |  Loss2: (0.0000) | Acc: (92.00%) (16693/18048)
Epoch: 115 | Batch_idx: 150 |  Loss: (0.2130) |  Loss2: (0.0000) | Acc: (92.00%) (17876/19328)
Epoch: 115 | Batch_idx: 160 |  Loss: (0.2128) |  Loss2: (0.0000) | Acc: (92.00%) (19053/20608)
Epoch: 115 | Batch_idx: 170 |  Loss: (0.2130) |  Loss2: (0.0000) | Acc: (92.00%) (20227/21888)
Epoch: 115 | Batch_idx: 180 |  Loss: (0.2127) |  Loss2: (0.0000) | Acc: (92.00%) (21408/23168)
Epoch: 115 | Batch_idx: 190 |  Loss: (0.2137) |  Loss2: (0.0000) | Acc: (92.00%) (22574/24448)
Epoch: 115 | Batch_idx: 200 |  Loss: (0.2131) |  Loss2: (0.0000) | Acc: (92.00%) (23768/25728)
Epoch: 115 | Batch_idx: 210 |  Loss: (0.2131) |  Loss2: (0.0000) | Acc: (92.00%) (24941/27008)
Epoch: 115 | Batch_idx: 220 |  Loss: (0.2122) |  Loss2: (0.0000) | Acc: (92.00%) (26128/28288)
Epoch: 115 | Batch_idx: 230 |  Loss: (0.2125) |  Loss2: (0.0000) | Acc: (92.00%) (27298/29568)
Epoch: 115 | Batch_idx: 240 |  Loss: (0.2128) |  Loss2: (0.0000) | Acc: (92.00%) (28479/30848)
Epoch: 115 | Batch_idx: 250 |  Loss: (0.2116) |  Loss2: (0.0000) | Acc: (92.00%) (29678/32128)
Epoch: 115 | Batch_idx: 260 |  Loss: (0.2116) |  Loss2: (0.0000) | Acc: (92.00%) (30862/33408)
Epoch: 115 | Batch_idx: 270 |  Loss: (0.2118) |  Loss2: (0.0000) | Acc: (92.00%) (32050/34688)
Epoch: 115 | Batch_idx: 280 |  Loss: (0.2105) |  Loss2: (0.0000) | Acc: (92.00%) (33248/35968)
Epoch: 115 | Batch_idx: 290 |  Loss: (0.2103) |  Loss2: (0.0000) | Acc: (92.00%) (34434/37248)
Epoch: 115 | Batch_idx: 300 |  Loss: (0.2098) |  Loss2: (0.0000) | Acc: (92.00%) (35631/38528)
Epoch: 115 | Batch_idx: 310 |  Loss: (0.2090) |  Loss2: (0.0000) | Acc: (92.00%) (36831/39808)
Epoch: 115 | Batch_idx: 320 |  Loss: (0.2088) |  Loss2: (0.0000) | Acc: (92.00%) (38016/41088)
Epoch: 115 | Batch_idx: 330 |  Loss: (0.2091) |  Loss2: (0.0000) | Acc: (92.00%) (39206/42368)
Epoch: 115 | Batch_idx: 340 |  Loss: (0.2090) |  Loss2: (0.0000) | Acc: (92.00%) (40400/43648)
Epoch: 115 | Batch_idx: 350 |  Loss: (0.2081) |  Loss2: (0.0000) | Acc: (92.00%) (41608/44928)
Epoch: 115 | Batch_idx: 360 |  Loss: (0.2081) |  Loss2: (0.0000) | Acc: (92.00%) (42788/46208)
Epoch: 115 | Batch_idx: 370 |  Loss: (0.2078) |  Loss2: (0.0000) | Acc: (92.00%) (43989/47488)
Epoch: 115 | Batch_idx: 380 |  Loss: (0.2075) |  Loss2: (0.0000) | Acc: (92.00%) (45191/48768)
Epoch: 115 | Batch_idx: 390 |  Loss: (0.2069) |  Loss2: (0.0000) | Acc: (92.00%) (46346/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_115.pth.tar'
# TEST : Loss: (0.4259) | Acc: (86.00%) (8697/10000)
percent tensor([0.5275, 0.4725], device='cuda:0')
percent tensor([0.4979, 0.5021], device='cuda:0')
percent tensor([0.5144, 0.4856], device='cuda:0')
percent tensor([0.5946, 0.4054], device='cuda:0')
percent tensor([0.6150, 0.3850], device='cuda:0')
percent tensor([0.6196, 0.3804], device='cuda:0')
percent tensor([0.7350, 0.2650], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
Epoch: 116 | Batch_idx: 0 |  Loss: (0.1372) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 116 | Batch_idx: 10 |  Loss: (0.1952) |  Loss2: (0.0000) | Acc: (93.00%) (1310/1408)
Epoch: 116 | Batch_idx: 20 |  Loss: (0.1899) |  Loss2: (0.0000) | Acc: (93.00%) (2508/2688)
Epoch: 116 | Batch_idx: 30 |  Loss: (0.2019) |  Loss2: (0.0000) | Acc: (92.00%) (3684/3968)
Epoch: 116 | Batch_idx: 40 |  Loss: (0.2013) |  Loss2: (0.0000) | Acc: (92.00%) (4866/5248)
Epoch: 116 | Batch_idx: 50 |  Loss: (0.2013) |  Loss2: (0.0000) | Acc: (92.00%) (6053/6528)
Epoch: 116 | Batch_idx: 60 |  Loss: (0.1962) |  Loss2: (0.0000) | Acc: (92.00%) (7248/7808)
Epoch: 116 | Batch_idx: 70 |  Loss: (0.1994) |  Loss2: (0.0000) | Acc: (92.00%) (8435/9088)
Epoch: 116 | Batch_idx: 80 |  Loss: (0.1987) |  Loss2: (0.0000) | Acc: (92.00%) (9639/10368)
Epoch: 116 | Batch_idx: 90 |  Loss: (0.1978) |  Loss2: (0.0000) | Acc: (93.00%) (10833/11648)
Epoch: 116 | Batch_idx: 100 |  Loss: (0.1974) |  Loss2: (0.0000) | Acc: (93.00%) (12028/12928)
Epoch: 116 | Batch_idx: 110 |  Loss: (0.1960) |  Loss2: (0.0000) | Acc: (93.00%) (13222/14208)
Epoch: 116 | Batch_idx: 120 |  Loss: (0.1973) |  Loss2: (0.0000) | Acc: (93.00%) (14408/15488)
Epoch: 116 | Batch_idx: 130 |  Loss: (0.1948) |  Loss2: (0.0000) | Acc: (93.00%) (15623/16768)
Epoch: 116 | Batch_idx: 140 |  Loss: (0.1931) |  Loss2: (0.0000) | Acc: (93.00%) (16821/18048)
Epoch: 116 | Batch_idx: 150 |  Loss: (0.1937) |  Loss2: (0.0000) | Acc: (93.00%) (18008/19328)
Epoch: 116 | Batch_idx: 160 |  Loss: (0.1932) |  Loss2: (0.0000) | Acc: (93.00%) (19205/20608)
Epoch: 116 | Batch_idx: 170 |  Loss: (0.1916) |  Loss2: (0.0000) | Acc: (93.00%) (20411/21888)
Epoch: 116 | Batch_idx: 180 |  Loss: (0.1916) |  Loss2: (0.0000) | Acc: (93.00%) (21616/23168)
Epoch: 116 | Batch_idx: 190 |  Loss: (0.1928) |  Loss2: (0.0000) | Acc: (93.00%) (22808/24448)
Epoch: 116 | Batch_idx: 200 |  Loss: (0.1940) |  Loss2: (0.0000) | Acc: (93.00%) (23991/25728)
Epoch: 116 | Batch_idx: 210 |  Loss: (0.1944) |  Loss2: (0.0000) | Acc: (93.00%) (25176/27008)
Epoch: 116 | Batch_idx: 220 |  Loss: (0.1938) |  Loss2: (0.0000) | Acc: (93.00%) (26381/28288)
Epoch: 116 | Batch_idx: 230 |  Loss: (0.1951) |  Loss2: (0.0000) | Acc: (93.00%) (27558/29568)
Epoch: 116 | Batch_idx: 240 |  Loss: (0.1951) |  Loss2: (0.0000) | Acc: (93.00%) (28745/30848)
Epoch: 116 | Batch_idx: 250 |  Loss: (0.1949) |  Loss2: (0.0000) | Acc: (93.00%) (29947/32128)
Epoch: 116 | Batch_idx: 260 |  Loss: (0.1946) |  Loss2: (0.0000) | Acc: (93.00%) (31149/33408)
Epoch: 116 | Batch_idx: 270 |  Loss: (0.1942) |  Loss2: (0.0000) | Acc: (93.00%) (32355/34688)
Epoch: 116 | Batch_idx: 280 |  Loss: (0.1945) |  Loss2: (0.0000) | Acc: (93.00%) (33544/35968)
Epoch: 116 | Batch_idx: 290 |  Loss: (0.1939) |  Loss2: (0.0000) | Acc: (93.00%) (34749/37248)
Epoch: 116 | Batch_idx: 300 |  Loss: (0.1932) |  Loss2: (0.0000) | Acc: (93.00%) (35954/38528)
Epoch: 116 | Batch_idx: 310 |  Loss: (0.1929) |  Loss2: (0.0000) | Acc: (93.00%) (37154/39808)
Epoch: 116 | Batch_idx: 320 |  Loss: (0.1931) |  Loss2: (0.0000) | Acc: (93.00%) (38347/41088)
Epoch: 116 | Batch_idx: 330 |  Loss: (0.1927) |  Loss2: (0.0000) | Acc: (93.00%) (39546/42368)
Epoch: 116 | Batch_idx: 340 |  Loss: (0.1922) |  Loss2: (0.0000) | Acc: (93.00%) (40749/43648)
Epoch: 116 | Batch_idx: 350 |  Loss: (0.1920) |  Loss2: (0.0000) | Acc: (93.00%) (41955/44928)
Epoch: 116 | Batch_idx: 360 |  Loss: (0.1916) |  Loss2: (0.0000) | Acc: (93.00%) (43153/46208)
Epoch: 116 | Batch_idx: 370 |  Loss: (0.1912) |  Loss2: (0.0000) | Acc: (93.00%) (44356/47488)
Epoch: 116 | Batch_idx: 380 |  Loss: (0.1912) |  Loss2: (0.0000) | Acc: (93.00%) (45555/48768)
Epoch: 116 | Batch_idx: 390 |  Loss: (0.1908) |  Loss2: (0.0000) | Acc: (93.00%) (46709/50000)
# TEST : Loss: (0.4156) | Acc: (87.00%) (8734/10000)
percent tensor([0.5270, 0.4730], device='cuda:0')
percent tensor([0.4982, 0.5018], device='cuda:0')
percent tensor([0.5127, 0.4873], device='cuda:0')
percent tensor([0.5981, 0.4019], device='cuda:0')
percent tensor([0.6115, 0.3885], device='cuda:0')
percent tensor([0.6231, 0.3769], device='cuda:0')
percent tensor([0.7423, 0.2577], device='cuda:0')
percent tensor([0.9990, 0.0010], device='cuda:0')
Epoch: 117 | Batch_idx: 0 |  Loss: (0.2278) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 117 | Batch_idx: 10 |  Loss: (0.1936) |  Loss2: (0.0000) | Acc: (93.00%) (1310/1408)
Epoch: 117 | Batch_idx: 20 |  Loss: (0.1757) |  Loss2: (0.0000) | Acc: (93.00%) (2526/2688)
Epoch: 117 | Batch_idx: 30 |  Loss: (0.1801) |  Loss2: (0.0000) | Acc: (93.00%) (3721/3968)
Epoch: 117 | Batch_idx: 40 |  Loss: (0.1809) |  Loss2: (0.0000) | Acc: (93.00%) (4918/5248)
Epoch: 117 | Batch_idx: 50 |  Loss: (0.1805) |  Loss2: (0.0000) | Acc: (93.00%) (6124/6528)
Epoch: 117 | Batch_idx: 60 |  Loss: (0.1815) |  Loss2: (0.0000) | Acc: (93.00%) (7315/7808)
Epoch: 117 | Batch_idx: 70 |  Loss: (0.1838) |  Loss2: (0.0000) | Acc: (93.00%) (8499/9088)
Epoch: 117 | Batch_idx: 80 |  Loss: (0.1813) |  Loss2: (0.0000) | Acc: (93.00%) (9704/10368)
Epoch: 117 | Batch_idx: 90 |  Loss: (0.1804) |  Loss2: (0.0000) | Acc: (93.00%) (10907/11648)
Epoch: 117 | Batch_idx: 100 |  Loss: (0.1802) |  Loss2: (0.0000) | Acc: (93.00%) (12112/12928)
Epoch: 117 | Batch_idx: 110 |  Loss: (0.1795) |  Loss2: (0.0000) | Acc: (93.00%) (13310/14208)
Epoch: 117 | Batch_idx: 120 |  Loss: (0.1807) |  Loss2: (0.0000) | Acc: (93.00%) (14506/15488)
Epoch: 117 | Batch_idx: 130 |  Loss: (0.1802) |  Loss2: (0.0000) | Acc: (93.00%) (15706/16768)
Epoch: 117 | Batch_idx: 140 |  Loss: (0.1812) |  Loss2: (0.0000) | Acc: (93.00%) (16901/18048)
Epoch: 117 | Batch_idx: 150 |  Loss: (0.1795) |  Loss2: (0.0000) | Acc: (93.00%) (18115/19328)
Epoch: 117 | Batch_idx: 160 |  Loss: (0.1798) |  Loss2: (0.0000) | Acc: (93.00%) (19315/20608)
Epoch: 117 | Batch_idx: 170 |  Loss: (0.1796) |  Loss2: (0.0000) | Acc: (93.00%) (20517/21888)
Epoch: 117 | Batch_idx: 180 |  Loss: (0.1798) |  Loss2: (0.0000) | Acc: (93.00%) (21723/23168)
Epoch: 117 | Batch_idx: 190 |  Loss: (0.1791) |  Loss2: (0.0000) | Acc: (93.00%) (22932/24448)
Epoch: 117 | Batch_idx: 200 |  Loss: (0.1808) |  Loss2: (0.0000) | Acc: (93.00%) (24116/25728)
Epoch: 117 | Batch_idx: 210 |  Loss: (0.1791) |  Loss2: (0.0000) | Acc: (93.00%) (25340/27008)
Epoch: 117 | Batch_idx: 220 |  Loss: (0.1796) |  Loss2: (0.0000) | Acc: (93.00%) (26536/28288)
Epoch: 117 | Batch_idx: 230 |  Loss: (0.1798) |  Loss2: (0.0000) | Acc: (93.00%) (27732/29568)
Epoch: 117 | Batch_idx: 240 |  Loss: (0.1789) |  Loss2: (0.0000) | Acc: (93.00%) (28941/30848)
Epoch: 117 | Batch_idx: 250 |  Loss: (0.1790) |  Loss2: (0.0000) | Acc: (93.00%) (30133/32128)
Epoch: 117 | Batch_idx: 260 |  Loss: (0.1794) |  Loss2: (0.0000) | Acc: (93.00%) (31326/33408)
Epoch: 117 | Batch_idx: 270 |  Loss: (0.1786) |  Loss2: (0.0000) | Acc: (93.00%) (32537/34688)
Epoch: 117 | Batch_idx: 280 |  Loss: (0.1791) |  Loss2: (0.0000) | Acc: (93.00%) (33737/35968)
Epoch: 117 | Batch_idx: 290 |  Loss: (0.1793) |  Loss2: (0.0000) | Acc: (93.00%) (34936/37248)
Epoch: 117 | Batch_idx: 300 |  Loss: (0.1790) |  Loss2: (0.0000) | Acc: (93.00%) (36140/38528)
Epoch: 117 | Batch_idx: 310 |  Loss: (0.1785) |  Loss2: (0.0000) | Acc: (93.00%) (37350/39808)
Epoch: 117 | Batch_idx: 320 |  Loss: (0.1778) |  Loss2: (0.0000) | Acc: (93.00%) (38569/41088)
Epoch: 117 | Batch_idx: 330 |  Loss: (0.1778) |  Loss2: (0.0000) | Acc: (93.00%) (39778/42368)
Epoch: 117 | Batch_idx: 340 |  Loss: (0.1781) |  Loss2: (0.0000) | Acc: (93.00%) (40973/43648)
Epoch: 117 | Batch_idx: 350 |  Loss: (0.1784) |  Loss2: (0.0000) | Acc: (93.00%) (42166/44928)
Epoch: 117 | Batch_idx: 360 |  Loss: (0.1783) |  Loss2: (0.0000) | Acc: (93.00%) (43364/46208)
Epoch: 117 | Batch_idx: 370 |  Loss: (0.1785) |  Loss2: (0.0000) | Acc: (93.00%) (44571/47488)
Epoch: 117 | Batch_idx: 380 |  Loss: (0.1790) |  Loss2: (0.0000) | Acc: (93.00%) (45766/48768)
Epoch: 117 | Batch_idx: 390 |  Loss: (0.1787) |  Loss2: (0.0000) | Acc: (93.00%) (46927/50000)
# TEST : Loss: (0.4039) | Acc: (87.00%) (8762/10000)
percent tensor([0.5290, 0.4710], device='cuda:0')
percent tensor([0.4980, 0.5020], device='cuda:0')
percent tensor([0.5113, 0.4887], device='cuda:0')
percent tensor([0.6009, 0.3991], device='cuda:0')
percent tensor([0.6210, 0.3790], device='cuda:0')
percent tensor([0.6316, 0.3684], device='cuda:0')
percent tensor([0.7404, 0.2596], device='cuda:0')
percent tensor([0.9990, 0.0010], device='cuda:0')
Epoch: 118 | Batch_idx: 0 |  Loss: (0.1577) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 118 | Batch_idx: 10 |  Loss: (0.1707) |  Loss2: (0.0000) | Acc: (94.00%) (1326/1408)
Epoch: 118 | Batch_idx: 20 |  Loss: (0.1715) |  Loss2: (0.0000) | Acc: (93.00%) (2526/2688)
Epoch: 118 | Batch_idx: 30 |  Loss: (0.1814) |  Loss2: (0.0000) | Acc: (93.00%) (3717/3968)
Epoch: 118 | Batch_idx: 40 |  Loss: (0.1870) |  Loss2: (0.0000) | Acc: (93.00%) (4912/5248)
Epoch: 118 | Batch_idx: 50 |  Loss: (0.1846) |  Loss2: (0.0000) | Acc: (93.00%) (6116/6528)
Epoch: 118 | Batch_idx: 60 |  Loss: (0.1848) |  Loss2: (0.0000) | Acc: (93.00%) (7309/7808)
Epoch: 118 | Batch_idx: 70 |  Loss: (0.1807) |  Loss2: (0.0000) | Acc: (93.00%) (8524/9088)
Epoch: 118 | Batch_idx: 80 |  Loss: (0.1793) |  Loss2: (0.0000) | Acc: (93.00%) (9721/10368)
Epoch: 118 | Batch_idx: 90 |  Loss: (0.1781) |  Loss2: (0.0000) | Acc: (93.00%) (10934/11648)
Epoch: 118 | Batch_idx: 100 |  Loss: (0.1766) |  Loss2: (0.0000) | Acc: (93.00%) (12136/12928)
Epoch: 118 | Batch_idx: 110 |  Loss: (0.1772) |  Loss2: (0.0000) | Acc: (93.00%) (13337/14208)
Epoch: 118 | Batch_idx: 120 |  Loss: (0.1754) |  Loss2: (0.0000) | Acc: (93.00%) (14549/15488)
Epoch: 118 | Batch_idx: 130 |  Loss: (0.1760) |  Loss2: (0.0000) | Acc: (93.00%) (15754/16768)
Epoch: 118 | Batch_idx: 140 |  Loss: (0.1751) |  Loss2: (0.0000) | Acc: (93.00%) (16964/18048)
Epoch: 118 | Batch_idx: 150 |  Loss: (0.1758) |  Loss2: (0.0000) | Acc: (93.00%) (18156/19328)
Epoch: 118 | Batch_idx: 160 |  Loss: (0.1772) |  Loss2: (0.0000) | Acc: (93.00%) (19356/20608)
Epoch: 118 | Batch_idx: 170 |  Loss: (0.1763) |  Loss2: (0.0000) | Acc: (93.00%) (20568/21888)
Epoch: 118 | Batch_idx: 180 |  Loss: (0.1750) |  Loss2: (0.0000) | Acc: (94.00%) (21783/23168)
Epoch: 118 | Batch_idx: 190 |  Loss: (0.1760) |  Loss2: (0.0000) | Acc: (93.00%) (22969/24448)
Epoch: 118 | Batch_idx: 200 |  Loss: (0.1756) |  Loss2: (0.0000) | Acc: (93.00%) (24177/25728)
Epoch: 118 | Batch_idx: 210 |  Loss: (0.1748) |  Loss2: (0.0000) | Acc: (93.00%) (25381/27008)
Epoch: 118 | Batch_idx: 220 |  Loss: (0.1763) |  Loss2: (0.0000) | Acc: (93.00%) (26563/28288)
Epoch: 118 | Batch_idx: 230 |  Loss: (0.1779) |  Loss2: (0.0000) | Acc: (93.00%) (27743/29568)
Epoch: 118 | Batch_idx: 240 |  Loss: (0.1777) |  Loss2: (0.0000) | Acc: (93.00%) (28939/30848)
Epoch: 118 | Batch_idx: 250 |  Loss: (0.1780) |  Loss2: (0.0000) | Acc: (93.00%) (30131/32128)
Epoch: 118 | Batch_idx: 260 |  Loss: (0.1774) |  Loss2: (0.0000) | Acc: (93.00%) (31333/33408)
Epoch: 118 | Batch_idx: 270 |  Loss: (0.1770) |  Loss2: (0.0000) | Acc: (93.00%) (32540/34688)
Epoch: 118 | Batch_idx: 280 |  Loss: (0.1769) |  Loss2: (0.0000) | Acc: (93.00%) (33738/35968)
Epoch: 118 | Batch_idx: 290 |  Loss: (0.1770) |  Loss2: (0.0000) | Acc: (93.00%) (34938/37248)
Epoch: 118 | Batch_idx: 300 |  Loss: (0.1774) |  Loss2: (0.0000) | Acc: (93.00%) (36130/38528)
Epoch: 118 | Batch_idx: 310 |  Loss: (0.1780) |  Loss2: (0.0000) | Acc: (93.00%) (37331/39808)
Epoch: 118 | Batch_idx: 320 |  Loss: (0.1776) |  Loss2: (0.0000) | Acc: (93.00%) (38540/41088)
Epoch: 118 | Batch_idx: 330 |  Loss: (0.1771) |  Loss2: (0.0000) | Acc: (93.00%) (39752/42368)
Epoch: 118 | Batch_idx: 340 |  Loss: (0.1772) |  Loss2: (0.0000) | Acc: (93.00%) (40951/43648)
Epoch: 118 | Batch_idx: 350 |  Loss: (0.1774) |  Loss2: (0.0000) | Acc: (93.00%) (42150/44928)
Epoch: 118 | Batch_idx: 360 |  Loss: (0.1779) |  Loss2: (0.0000) | Acc: (93.00%) (43335/46208)
Epoch: 118 | Batch_idx: 370 |  Loss: (0.1779) |  Loss2: (0.0000) | Acc: (93.00%) (44534/47488)
Epoch: 118 | Batch_idx: 380 |  Loss: (0.1777) |  Loss2: (0.0000) | Acc: (93.00%) (45740/48768)
Epoch: 118 | Batch_idx: 390 |  Loss: (0.1773) |  Loss2: (0.0000) | Acc: (93.00%) (46905/50000)
# TEST : Loss: (0.3996) | Acc: (87.00%) (8778/10000)
percent tensor([0.5296, 0.4704], device='cuda:0')
percent tensor([0.4984, 0.5016], device='cuda:0')
percent tensor([0.5109, 0.4891], device='cuda:0')
percent tensor([0.6068, 0.3932], device='cuda:0')
percent tensor([0.6223, 0.3777], device='cuda:0')
percent tensor([0.6334, 0.3666], device='cuda:0')
percent tensor([0.7411, 0.2589], device='cuda:0')
percent tensor([0.9990, 0.0010], device='cuda:0')
Epoch: 119 | Batch_idx: 0 |  Loss: (0.1992) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 119 | Batch_idx: 10 |  Loss: (0.1864) |  Loss2: (0.0000) | Acc: (93.00%) (1323/1408)
Epoch: 119 | Batch_idx: 20 |  Loss: (0.1822) |  Loss2: (0.0000) | Acc: (94.00%) (2535/2688)
Epoch: 119 | Batch_idx: 30 |  Loss: (0.1737) |  Loss2: (0.0000) | Acc: (94.00%) (3752/3968)
Epoch: 119 | Batch_idx: 40 |  Loss: (0.1743) |  Loss2: (0.0000) | Acc: (94.00%) (4959/5248)
Epoch: 119 | Batch_idx: 50 |  Loss: (0.1751) |  Loss2: (0.0000) | Acc: (94.00%) (6164/6528)
Epoch: 119 | Batch_idx: 60 |  Loss: (0.1697) |  Loss2: (0.0000) | Acc: (94.00%) (7385/7808)
Epoch: 119 | Batch_idx: 70 |  Loss: (0.1682) |  Loss2: (0.0000) | Acc: (94.00%) (8592/9088)
Epoch: 119 | Batch_idx: 80 |  Loss: (0.1675) |  Loss2: (0.0000) | Acc: (94.00%) (9802/10368)
Epoch: 119 | Batch_idx: 90 |  Loss: (0.1670) |  Loss2: (0.0000) | Acc: (94.00%) (11013/11648)
Epoch: 119 | Batch_idx: 100 |  Loss: (0.1682) |  Loss2: (0.0000) | Acc: (94.00%) (12215/12928)
Epoch: 119 | Batch_idx: 110 |  Loss: (0.1671) |  Loss2: (0.0000) | Acc: (94.00%) (13428/14208)
Epoch: 119 | Batch_idx: 120 |  Loss: (0.1669) |  Loss2: (0.0000) | Acc: (94.00%) (14630/15488)
Epoch: 119 | Batch_idx: 130 |  Loss: (0.1684) |  Loss2: (0.0000) | Acc: (94.00%) (15819/16768)
Epoch: 119 | Batch_idx: 140 |  Loss: (0.1697) |  Loss2: (0.0000) | Acc: (94.00%) (17013/18048)
Epoch: 119 | Batch_idx: 150 |  Loss: (0.1698) |  Loss2: (0.0000) | Acc: (94.00%) (18219/19328)
Epoch: 119 | Batch_idx: 160 |  Loss: (0.1688) |  Loss2: (0.0000) | Acc: (94.00%) (19436/20608)
Epoch: 119 | Batch_idx: 170 |  Loss: (0.1678) |  Loss2: (0.0000) | Acc: (94.00%) (20653/21888)
Epoch: 119 | Batch_idx: 180 |  Loss: (0.1677) |  Loss2: (0.0000) | Acc: (94.00%) (21862/23168)
Epoch: 119 | Batch_idx: 190 |  Loss: (0.1680) |  Loss2: (0.0000) | Acc: (94.00%) (23065/24448)
Epoch: 119 | Batch_idx: 200 |  Loss: (0.1692) |  Loss2: (0.0000) | Acc: (94.00%) (24261/25728)
Epoch: 119 | Batch_idx: 210 |  Loss: (0.1694) |  Loss2: (0.0000) | Acc: (94.00%) (25471/27008)
Epoch: 119 | Batch_idx: 220 |  Loss: (0.1694) |  Loss2: (0.0000) | Acc: (94.00%) (26678/28288)
Epoch: 119 | Batch_idx: 230 |  Loss: (0.1699) |  Loss2: (0.0000) | Acc: (94.00%) (27893/29568)
Epoch: 119 | Batch_idx: 240 |  Loss: (0.1700) |  Loss2: (0.0000) | Acc: (94.00%) (29095/30848)
Epoch: 119 | Batch_idx: 250 |  Loss: (0.1692) |  Loss2: (0.0000) | Acc: (94.00%) (30311/32128)
Epoch: 119 | Batch_idx: 260 |  Loss: (0.1689) |  Loss2: (0.0000) | Acc: (94.00%) (31520/33408)
Epoch: 119 | Batch_idx: 270 |  Loss: (0.1686) |  Loss2: (0.0000) | Acc: (94.00%) (32728/34688)
Epoch: 119 | Batch_idx: 280 |  Loss: (0.1687) |  Loss2: (0.0000) | Acc: (94.00%) (33939/35968)
Epoch: 119 | Batch_idx: 290 |  Loss: (0.1693) |  Loss2: (0.0000) | Acc: (94.00%) (35135/37248)
Epoch: 119 | Batch_idx: 300 |  Loss: (0.1689) |  Loss2: (0.0000) | Acc: (94.00%) (36343/38528)
Epoch: 119 | Batch_idx: 310 |  Loss: (0.1691) |  Loss2: (0.0000) | Acc: (94.00%) (37549/39808)
Epoch: 119 | Batch_idx: 320 |  Loss: (0.1695) |  Loss2: (0.0000) | Acc: (94.00%) (38752/41088)
Epoch: 119 | Batch_idx: 330 |  Loss: (0.1697) |  Loss2: (0.0000) | Acc: (94.00%) (39961/42368)
Epoch: 119 | Batch_idx: 340 |  Loss: (0.1697) |  Loss2: (0.0000) | Acc: (94.00%) (41173/43648)
Epoch: 119 | Batch_idx: 350 |  Loss: (0.1698) |  Loss2: (0.0000) | Acc: (94.00%) (42382/44928)
Epoch: 119 | Batch_idx: 360 |  Loss: (0.1698) |  Loss2: (0.0000) | Acc: (94.00%) (43582/46208)
Epoch: 119 | Batch_idx: 370 |  Loss: (0.1700) |  Loss2: (0.0000) | Acc: (94.00%) (44781/47488)
Epoch: 119 | Batch_idx: 380 |  Loss: (0.1699) |  Loss2: (0.0000) | Acc: (94.00%) (45989/48768)
Epoch: 119 | Batch_idx: 390 |  Loss: (0.1701) |  Loss2: (0.0000) | Acc: (94.00%) (47149/50000)
# TEST : Loss: (0.3943) | Acc: (87.00%) (8788/10000)
percent tensor([0.5292, 0.4708], device='cuda:0')
percent tensor([0.4991, 0.5009], device='cuda:0')
percent tensor([0.5119, 0.4881], device='cuda:0')
percent tensor([0.6061, 0.3939], device='cuda:0')
percent tensor([0.6232, 0.3768], device='cuda:0')
percent tensor([0.6379, 0.3621], device='cuda:0')
percent tensor([0.7387, 0.2613], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 120 | Batch_idx: 0 |  Loss: (0.1127) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 120 | Batch_idx: 10 |  Loss: (0.1762) |  Loss2: (0.0000) | Acc: (94.00%) (1327/1408)
Epoch: 120 | Batch_idx: 20 |  Loss: (0.1751) |  Loss2: (0.0000) | Acc: (94.00%) (2532/2688)
Epoch: 120 | Batch_idx: 30 |  Loss: (0.1768) |  Loss2: (0.0000) | Acc: (94.00%) (3736/3968)
Epoch: 120 | Batch_idx: 40 |  Loss: (0.1752) |  Loss2: (0.0000) | Acc: (94.00%) (4944/5248)
Epoch: 120 | Batch_idx: 50 |  Loss: (0.1763) |  Loss2: (0.0000) | Acc: (94.00%) (6143/6528)
Epoch: 120 | Batch_idx: 60 |  Loss: (0.1787) |  Loss2: (0.0000) | Acc: (93.00%) (7324/7808)
Epoch: 120 | Batch_idx: 70 |  Loss: (0.1776) |  Loss2: (0.0000) | Acc: (93.00%) (8526/9088)
Epoch: 120 | Batch_idx: 80 |  Loss: (0.1761) |  Loss2: (0.0000) | Acc: (93.00%) (9735/10368)
Epoch: 120 | Batch_idx: 90 |  Loss: (0.1750) |  Loss2: (0.0000) | Acc: (93.00%) (10939/11648)
Epoch: 120 | Batch_idx: 100 |  Loss: (0.1755) |  Loss2: (0.0000) | Acc: (93.00%) (12133/12928)
Epoch: 120 | Batch_idx: 110 |  Loss: (0.1754) |  Loss2: (0.0000) | Acc: (93.00%) (13331/14208)
Epoch: 120 | Batch_idx: 120 |  Loss: (0.1738) |  Loss2: (0.0000) | Acc: (93.00%) (14535/15488)
Epoch: 120 | Batch_idx: 130 |  Loss: (0.1721) |  Loss2: (0.0000) | Acc: (93.00%) (15747/16768)
Epoch: 120 | Batch_idx: 140 |  Loss: (0.1722) |  Loss2: (0.0000) | Acc: (93.00%) (16948/18048)
Epoch: 120 | Batch_idx: 150 |  Loss: (0.1733) |  Loss2: (0.0000) | Acc: (93.00%) (18148/19328)
Epoch: 120 | Batch_idx: 160 |  Loss: (0.1749) |  Loss2: (0.0000) | Acc: (93.00%) (19340/20608)
Epoch: 120 | Batch_idx: 170 |  Loss: (0.1752) |  Loss2: (0.0000) | Acc: (93.00%) (20546/21888)
Epoch: 120 | Batch_idx: 180 |  Loss: (0.1765) |  Loss2: (0.0000) | Acc: (93.00%) (21745/23168)
Epoch: 120 | Batch_idx: 190 |  Loss: (0.1756) |  Loss2: (0.0000) | Acc: (93.00%) (22961/24448)
Epoch: 120 | Batch_idx: 200 |  Loss: (0.1763) |  Loss2: (0.0000) | Acc: (93.00%) (24158/25728)
Epoch: 120 | Batch_idx: 210 |  Loss: (0.1755) |  Loss2: (0.0000) | Acc: (93.00%) (25371/27008)
Epoch: 120 | Batch_idx: 220 |  Loss: (0.1762) |  Loss2: (0.0000) | Acc: (93.00%) (26567/28288)
Epoch: 120 | Batch_idx: 230 |  Loss: (0.1769) |  Loss2: (0.0000) | Acc: (93.00%) (27755/29568)
Epoch: 120 | Batch_idx: 240 |  Loss: (0.1760) |  Loss2: (0.0000) | Acc: (93.00%) (28972/30848)
Epoch: 120 | Batch_idx: 250 |  Loss: (0.1771) |  Loss2: (0.0000) | Acc: (93.00%) (30161/32128)
Epoch: 120 | Batch_idx: 260 |  Loss: (0.1775) |  Loss2: (0.0000) | Acc: (93.00%) (31360/33408)
Epoch: 120 | Batch_idx: 270 |  Loss: (0.1782) |  Loss2: (0.0000) | Acc: (93.00%) (32553/34688)
Epoch: 120 | Batch_idx: 280 |  Loss: (0.1779) |  Loss2: (0.0000) | Acc: (93.00%) (33759/35968)
Epoch: 120 | Batch_idx: 290 |  Loss: (0.1787) |  Loss2: (0.0000) | Acc: (93.00%) (34949/37248)
Epoch: 120 | Batch_idx: 300 |  Loss: (0.1798) |  Loss2: (0.0000) | Acc: (93.00%) (36131/38528)
Epoch: 120 | Batch_idx: 310 |  Loss: (0.1809) |  Loss2: (0.0000) | Acc: (93.00%) (37316/39808)
Epoch: 120 | Batch_idx: 320 |  Loss: (0.1802) |  Loss2: (0.0000) | Acc: (93.00%) (38531/41088)
Epoch: 120 | Batch_idx: 330 |  Loss: (0.1803) |  Loss2: (0.0000) | Acc: (93.00%) (39729/42368)
Epoch: 120 | Batch_idx: 340 |  Loss: (0.1801) |  Loss2: (0.0000) | Acc: (93.00%) (40933/43648)
Epoch: 120 | Batch_idx: 350 |  Loss: (0.1804) |  Loss2: (0.0000) | Acc: (93.00%) (42132/44928)
Epoch: 120 | Batch_idx: 360 |  Loss: (0.1809) |  Loss2: (0.0000) | Acc: (93.00%) (43317/46208)
Epoch: 120 | Batch_idx: 370 |  Loss: (0.1811) |  Loss2: (0.0000) | Acc: (93.00%) (44507/47488)
Epoch: 120 | Batch_idx: 380 |  Loss: (0.1821) |  Loss2: (0.0000) | Acc: (93.00%) (45689/48768)
Epoch: 120 | Batch_idx: 390 |  Loss: (0.1826) |  Loss2: (0.0000) | Acc: (93.00%) (46837/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_120.pth.tar'
# TEST : Loss: (0.4833) | Acc: (85.00%) (8528/10000)
percent tensor([0.5292, 0.4708], device='cuda:0')
percent tensor([0.4991, 0.5009], device='cuda:0')
percent tensor([0.5123, 0.4877], device='cuda:0')
percent tensor([0.6073, 0.3927], device='cuda:0')
percent tensor([0.6238, 0.3762], device='cuda:0')
percent tensor([0.6385, 0.3615], device='cuda:0')
percent tensor([0.7443, 0.2557], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(181.4363, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(822.6320, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(814.9678, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1508.9069, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(500.4231, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2251.5298, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4271.4790, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1391.7866, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6190.3657, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11804.3945, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3909.2258, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16507.0039, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 121 | Batch_idx: 0 |  Loss: (0.1504) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 121 | Batch_idx: 10 |  Loss: (0.1530) |  Loss2: (0.0000) | Acc: (94.00%) (1337/1408)
Epoch: 121 | Batch_idx: 20 |  Loss: (0.1648) |  Loss2: (0.0000) | Acc: (94.00%) (2542/2688)
Epoch: 121 | Batch_idx: 30 |  Loss: (0.1725) |  Loss2: (0.0000) | Acc: (94.00%) (3738/3968)
Epoch: 121 | Batch_idx: 40 |  Loss: (0.1765) |  Loss2: (0.0000) | Acc: (93.00%) (4927/5248)
Epoch: 121 | Batch_idx: 50 |  Loss: (0.1729) |  Loss2: (0.0000) | Acc: (94.00%) (6147/6528)
Epoch: 121 | Batch_idx: 60 |  Loss: (0.1723) |  Loss2: (0.0000) | Acc: (94.00%) (7341/7808)
Epoch: 121 | Batch_idx: 70 |  Loss: (0.1687) |  Loss2: (0.0000) | Acc: (94.00%) (8558/9088)
Epoch: 121 | Batch_idx: 80 |  Loss: (0.1711) |  Loss2: (0.0000) | Acc: (94.00%) (9748/10368)
Epoch: 121 | Batch_idx: 90 |  Loss: (0.1711) |  Loss2: (0.0000) | Acc: (94.00%) (10951/11648)
Epoch: 121 | Batch_idx: 100 |  Loss: (0.1674) |  Loss2: (0.0000) | Acc: (94.00%) (12178/12928)
Epoch: 121 | Batch_idx: 110 |  Loss: (0.1671) |  Loss2: (0.0000) | Acc: (94.00%) (13389/14208)
Epoch: 121 | Batch_idx: 120 |  Loss: (0.1697) |  Loss2: (0.0000) | Acc: (94.00%) (14576/15488)
Epoch: 121 | Batch_idx: 130 |  Loss: (0.1718) |  Loss2: (0.0000) | Acc: (94.00%) (15765/16768)
Epoch: 121 | Batch_idx: 140 |  Loss: (0.1735) |  Loss2: (0.0000) | Acc: (94.00%) (16967/18048)
Epoch: 121 | Batch_idx: 150 |  Loss: (0.1742) |  Loss2: (0.0000) | Acc: (93.00%) (18164/19328)
Epoch: 121 | Batch_idx: 160 |  Loss: (0.1742) |  Loss2: (0.0000) | Acc: (93.00%) (19370/20608)
Epoch: 121 | Batch_idx: 170 |  Loss: (0.1739) |  Loss2: (0.0000) | Acc: (94.00%) (20586/21888)
Epoch: 121 | Batch_idx: 180 |  Loss: (0.1728) |  Loss2: (0.0000) | Acc: (94.00%) (21794/23168)
Epoch: 121 | Batch_idx: 190 |  Loss: (0.1731) |  Loss2: (0.0000) | Acc: (94.00%) (23003/24448)
Epoch: 121 | Batch_idx: 200 |  Loss: (0.1751) |  Loss2: (0.0000) | Acc: (93.00%) (24177/25728)
Epoch: 121 | Batch_idx: 210 |  Loss: (0.1746) |  Loss2: (0.0000) | Acc: (93.00%) (25385/27008)
Epoch: 121 | Batch_idx: 220 |  Loss: (0.1744) |  Loss2: (0.0000) | Acc: (94.00%) (26594/28288)
Epoch: 121 | Batch_idx: 230 |  Loss: (0.1756) |  Loss2: (0.0000) | Acc: (93.00%) (27788/29568)
Epoch: 121 | Batch_idx: 240 |  Loss: (0.1753) |  Loss2: (0.0000) | Acc: (93.00%) (28996/30848)
Epoch: 121 | Batch_idx: 250 |  Loss: (0.1743) |  Loss2: (0.0000) | Acc: (94.00%) (30209/32128)
Epoch: 121 | Batch_idx: 260 |  Loss: (0.1747) |  Loss2: (0.0000) | Acc: (94.00%) (31411/33408)
Epoch: 121 | Batch_idx: 270 |  Loss: (0.1759) |  Loss2: (0.0000) | Acc: (93.00%) (32595/34688)
Epoch: 121 | Batch_idx: 280 |  Loss: (0.1765) |  Loss2: (0.0000) | Acc: (93.00%) (33782/35968)
Epoch: 121 | Batch_idx: 290 |  Loss: (0.1767) |  Loss2: (0.0000) | Acc: (93.00%) (34973/37248)
Epoch: 121 | Batch_idx: 300 |  Loss: (0.1764) |  Loss2: (0.0000) | Acc: (93.00%) (36183/38528)
Epoch: 121 | Batch_idx: 310 |  Loss: (0.1762) |  Loss2: (0.0000) | Acc: (93.00%) (37385/39808)
Epoch: 121 | Batch_idx: 320 |  Loss: (0.1764) |  Loss2: (0.0000) | Acc: (93.00%) (38580/41088)
Epoch: 121 | Batch_idx: 330 |  Loss: (0.1774) |  Loss2: (0.0000) | Acc: (93.00%) (39769/42368)
Epoch: 121 | Batch_idx: 340 |  Loss: (0.1780) |  Loss2: (0.0000) | Acc: (93.00%) (40957/43648)
Epoch: 121 | Batch_idx: 350 |  Loss: (0.1781) |  Loss2: (0.0000) | Acc: (93.00%) (42158/44928)
Epoch: 121 | Batch_idx: 360 |  Loss: (0.1781) |  Loss2: (0.0000) | Acc: (93.00%) (43361/46208)
Epoch: 121 | Batch_idx: 370 |  Loss: (0.1778) |  Loss2: (0.0000) | Acc: (93.00%) (44564/47488)
Epoch: 121 | Batch_idx: 380 |  Loss: (0.1774) |  Loss2: (0.0000) | Acc: (93.00%) (45771/48768)
Epoch: 121 | Batch_idx: 390 |  Loss: (0.1776) |  Loss2: (0.0000) | Acc: (93.00%) (46922/50000)
# TEST : Loss: (0.4709) | Acc: (85.00%) (8583/10000)
percent tensor([0.5295, 0.4705], device='cuda:0')
percent tensor([0.4991, 0.5009], device='cuda:0')
percent tensor([0.5122, 0.4878], device='cuda:0')
percent tensor([0.6060, 0.3940], device='cuda:0')
percent tensor([0.6226, 0.3774], device='cuda:0')
percent tensor([0.6400, 0.3600], device='cuda:0')
percent tensor([0.7353, 0.2647], device='cuda:0')
percent tensor([0.9990, 0.0010], device='cuda:0')
Epoch: 122 | Batch_idx: 0 |  Loss: (0.1439) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 122 | Batch_idx: 10 |  Loss: (0.1651) |  Loss2: (0.0000) | Acc: (94.00%) (1333/1408)
Epoch: 122 | Batch_idx: 20 |  Loss: (0.1541) |  Loss2: (0.0000) | Acc: (95.00%) (2557/2688)
Epoch: 122 | Batch_idx: 30 |  Loss: (0.1530) |  Loss2: (0.0000) | Acc: (94.00%) (3769/3968)
Epoch: 122 | Batch_idx: 40 |  Loss: (0.1546) |  Loss2: (0.0000) | Acc: (94.00%) (4974/5248)
Epoch: 122 | Batch_idx: 50 |  Loss: (0.1568) |  Loss2: (0.0000) | Acc: (94.00%) (6181/6528)
Epoch: 122 | Batch_idx: 60 |  Loss: (0.1597) |  Loss2: (0.0000) | Acc: (94.00%) (7381/7808)
Epoch: 122 | Batch_idx: 70 |  Loss: (0.1603) |  Loss2: (0.0000) | Acc: (94.00%) (8593/9088)
Epoch: 122 | Batch_idx: 80 |  Loss: (0.1609) |  Loss2: (0.0000) | Acc: (94.00%) (9807/10368)
Epoch: 122 | Batch_idx: 90 |  Loss: (0.1612) |  Loss2: (0.0000) | Acc: (94.00%) (11009/11648)
Epoch: 122 | Batch_idx: 100 |  Loss: (0.1605) |  Loss2: (0.0000) | Acc: (94.00%) (12219/12928)
Epoch: 122 | Batch_idx: 110 |  Loss: (0.1617) |  Loss2: (0.0000) | Acc: (94.00%) (13420/14208)
Epoch: 122 | Batch_idx: 120 |  Loss: (0.1624) |  Loss2: (0.0000) | Acc: (94.00%) (14624/15488)
Epoch: 122 | Batch_idx: 130 |  Loss: (0.1611) |  Loss2: (0.0000) | Acc: (94.00%) (15838/16768)
Epoch: 122 | Batch_idx: 140 |  Loss: (0.1613) |  Loss2: (0.0000) | Acc: (94.00%) (17042/18048)
Epoch: 122 | Batch_idx: 150 |  Loss: (0.1608) |  Loss2: (0.0000) | Acc: (94.00%) (18262/19328)
Epoch: 122 | Batch_idx: 160 |  Loss: (0.1622) |  Loss2: (0.0000) | Acc: (94.00%) (19449/20608)
Epoch: 122 | Batch_idx: 170 |  Loss: (0.1635) |  Loss2: (0.0000) | Acc: (94.00%) (20641/21888)
Epoch: 122 | Batch_idx: 180 |  Loss: (0.1622) |  Loss2: (0.0000) | Acc: (94.00%) (21858/23168)
Epoch: 122 | Batch_idx: 190 |  Loss: (0.1635) |  Loss2: (0.0000) | Acc: (94.00%) (23053/24448)
Epoch: 122 | Batch_idx: 200 |  Loss: (0.1627) |  Loss2: (0.0000) | Acc: (94.00%) (24273/25728)
Epoch: 122 | Batch_idx: 210 |  Loss: (0.1637) |  Loss2: (0.0000) | Acc: (94.00%) (25460/27008)
Epoch: 122 | Batch_idx: 220 |  Loss: (0.1645) |  Loss2: (0.0000) | Acc: (94.00%) (26652/28288)
Epoch: 122 | Batch_idx: 230 |  Loss: (0.1659) |  Loss2: (0.0000) | Acc: (94.00%) (27852/29568)
Epoch: 122 | Batch_idx: 240 |  Loss: (0.1656) |  Loss2: (0.0000) | Acc: (94.00%) (29063/30848)
Epoch: 122 | Batch_idx: 250 |  Loss: (0.1657) |  Loss2: (0.0000) | Acc: (94.00%) (30272/32128)
Epoch: 122 | Batch_idx: 260 |  Loss: (0.1655) |  Loss2: (0.0000) | Acc: (94.00%) (31479/33408)
Epoch: 122 | Batch_idx: 270 |  Loss: (0.1668) |  Loss2: (0.0000) | Acc: (94.00%) (32673/34688)
Epoch: 122 | Batch_idx: 280 |  Loss: (0.1674) |  Loss2: (0.0000) | Acc: (94.00%) (33872/35968)
Epoch: 122 | Batch_idx: 290 |  Loss: (0.1681) |  Loss2: (0.0000) | Acc: (94.00%) (35069/37248)
Epoch: 122 | Batch_idx: 300 |  Loss: (0.1687) |  Loss2: (0.0000) | Acc: (94.00%) (36260/38528)
Epoch: 122 | Batch_idx: 310 |  Loss: (0.1685) |  Loss2: (0.0000) | Acc: (94.00%) (37462/39808)
Epoch: 122 | Batch_idx: 320 |  Loss: (0.1685) |  Loss2: (0.0000) | Acc: (94.00%) (38661/41088)
Epoch: 122 | Batch_idx: 330 |  Loss: (0.1687) |  Loss2: (0.0000) | Acc: (94.00%) (39869/42368)
Epoch: 122 | Batch_idx: 340 |  Loss: (0.1692) |  Loss2: (0.0000) | Acc: (94.00%) (41062/43648)
Epoch: 122 | Batch_idx: 350 |  Loss: (0.1698) |  Loss2: (0.0000) | Acc: (94.00%) (42254/44928)
Epoch: 122 | Batch_idx: 360 |  Loss: (0.1703) |  Loss2: (0.0000) | Acc: (94.00%) (43461/46208)
Epoch: 122 | Batch_idx: 370 |  Loss: (0.1711) |  Loss2: (0.0000) | Acc: (94.00%) (44653/47488)
Epoch: 122 | Batch_idx: 380 |  Loss: (0.1717) |  Loss2: (0.0000) | Acc: (94.00%) (45846/48768)
Epoch: 122 | Batch_idx: 390 |  Loss: (0.1717) |  Loss2: (0.0000) | Acc: (94.00%) (47007/50000)
# TEST : Loss: (0.4134) | Acc: (87.00%) (8727/10000)
percent tensor([0.5292, 0.4708], device='cuda:0')
percent tensor([0.4991, 0.5009], device='cuda:0')
percent tensor([0.5118, 0.4882], device='cuda:0')
percent tensor([0.6062, 0.3938], device='cuda:0')
percent tensor([0.6262, 0.3738], device='cuda:0')
percent tensor([0.6386, 0.3614], device='cuda:0')
percent tensor([0.7388, 0.2612], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 123 | Batch_idx: 0 |  Loss: (0.1986) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 123 | Batch_idx: 10 |  Loss: (0.1433) |  Loss2: (0.0000) | Acc: (95.00%) (1346/1408)
Epoch: 123 | Batch_idx: 20 |  Loss: (0.1437) |  Loss2: (0.0000) | Acc: (95.00%) (2575/2688)
Epoch: 123 | Batch_idx: 30 |  Loss: (0.1458) |  Loss2: (0.0000) | Acc: (95.00%) (3789/3968)
Epoch: 123 | Batch_idx: 40 |  Loss: (0.1532) |  Loss2: (0.0000) | Acc: (95.00%) (4988/5248)
Epoch: 123 | Batch_idx: 50 |  Loss: (0.1533) |  Loss2: (0.0000) | Acc: (94.00%) (6199/6528)
Epoch: 123 | Batch_idx: 60 |  Loss: (0.1579) |  Loss2: (0.0000) | Acc: (94.00%) (7397/7808)
Epoch: 123 | Batch_idx: 70 |  Loss: (0.1635) |  Loss2: (0.0000) | Acc: (94.00%) (8594/9088)
Epoch: 123 | Batch_idx: 80 |  Loss: (0.1671) |  Loss2: (0.0000) | Acc: (94.00%) (9789/10368)
Epoch: 123 | Batch_idx: 90 |  Loss: (0.1676) |  Loss2: (0.0000) | Acc: (94.00%) (10994/11648)
Epoch: 123 | Batch_idx: 100 |  Loss: (0.1679) |  Loss2: (0.0000) | Acc: (94.00%) (12197/12928)
Epoch: 123 | Batch_idx: 110 |  Loss: (0.1659) |  Loss2: (0.0000) | Acc: (94.00%) (13414/14208)
Epoch: 123 | Batch_idx: 120 |  Loss: (0.1658) |  Loss2: (0.0000) | Acc: (94.00%) (14621/15488)
Epoch: 123 | Batch_idx: 130 |  Loss: (0.1663) |  Loss2: (0.0000) | Acc: (94.00%) (15812/16768)
Epoch: 123 | Batch_idx: 140 |  Loss: (0.1649) |  Loss2: (0.0000) | Acc: (94.00%) (17034/18048)
Epoch: 123 | Batch_idx: 150 |  Loss: (0.1649) |  Loss2: (0.0000) | Acc: (94.00%) (18243/19328)
Epoch: 123 | Batch_idx: 160 |  Loss: (0.1629) |  Loss2: (0.0000) | Acc: (94.00%) (19459/20608)
Epoch: 123 | Batch_idx: 170 |  Loss: (0.1629) |  Loss2: (0.0000) | Acc: (94.00%) (20666/21888)
Epoch: 123 | Batch_idx: 180 |  Loss: (0.1637) |  Loss2: (0.0000) | Acc: (94.00%) (21864/23168)
Epoch: 123 | Batch_idx: 190 |  Loss: (0.1634) |  Loss2: (0.0000) | Acc: (94.00%) (23075/24448)
Epoch: 123 | Batch_idx: 200 |  Loss: (0.1649) |  Loss2: (0.0000) | Acc: (94.00%) (24274/25728)
Epoch: 123 | Batch_idx: 210 |  Loss: (0.1657) |  Loss2: (0.0000) | Acc: (94.00%) (25470/27008)
Epoch: 123 | Batch_idx: 220 |  Loss: (0.1656) |  Loss2: (0.0000) | Acc: (94.00%) (26679/28288)
Epoch: 123 | Batch_idx: 230 |  Loss: (0.1651) |  Loss2: (0.0000) | Acc: (94.00%) (27895/29568)
Epoch: 123 | Batch_idx: 240 |  Loss: (0.1664) |  Loss2: (0.0000) | Acc: (94.00%) (29088/30848)
Epoch: 123 | Batch_idx: 250 |  Loss: (0.1669) |  Loss2: (0.0000) | Acc: (94.00%) (30286/32128)
Epoch: 123 | Batch_idx: 260 |  Loss: (0.1666) |  Loss2: (0.0000) | Acc: (94.00%) (31490/33408)
Epoch: 123 | Batch_idx: 270 |  Loss: (0.1662) |  Loss2: (0.0000) | Acc: (94.00%) (32702/34688)
Epoch: 123 | Batch_idx: 280 |  Loss: (0.1661) |  Loss2: (0.0000) | Acc: (94.00%) (33904/35968)
Epoch: 123 | Batch_idx: 290 |  Loss: (0.1666) |  Loss2: (0.0000) | Acc: (94.00%) (35102/37248)
Epoch: 123 | Batch_idx: 300 |  Loss: (0.1667) |  Loss2: (0.0000) | Acc: (94.00%) (36303/38528)
Epoch: 123 | Batch_idx: 310 |  Loss: (0.1661) |  Loss2: (0.0000) | Acc: (94.00%) (37516/39808)
Epoch: 123 | Batch_idx: 320 |  Loss: (0.1669) |  Loss2: (0.0000) | Acc: (94.00%) (38713/41088)
Epoch: 123 | Batch_idx: 330 |  Loss: (0.1669) |  Loss2: (0.0000) | Acc: (94.00%) (39922/42368)
Epoch: 123 | Batch_idx: 340 |  Loss: (0.1670) |  Loss2: (0.0000) | Acc: (94.00%) (41122/43648)
Epoch: 123 | Batch_idx: 350 |  Loss: (0.1672) |  Loss2: (0.0000) | Acc: (94.00%) (42324/44928)
Epoch: 123 | Batch_idx: 360 |  Loss: (0.1671) |  Loss2: (0.0000) | Acc: (94.00%) (43533/46208)
Epoch: 123 | Batch_idx: 370 |  Loss: (0.1664) |  Loss2: (0.0000) | Acc: (94.00%) (44748/47488)
Epoch: 123 | Batch_idx: 380 |  Loss: (0.1671) |  Loss2: (0.0000) | Acc: (94.00%) (45938/48768)
Epoch: 123 | Batch_idx: 390 |  Loss: (0.1672) |  Loss2: (0.0000) | Acc: (94.00%) (47102/50000)
# TEST : Loss: (0.4207) | Acc: (87.00%) (8765/10000)
percent tensor([0.5292, 0.4708], device='cuda:0')
percent tensor([0.4991, 0.5009], device='cuda:0')
percent tensor([0.5115, 0.4885], device='cuda:0')
percent tensor([0.6070, 0.3930], device='cuda:0')
percent tensor([0.6251, 0.3749], device='cuda:0')
percent tensor([0.6371, 0.3629], device='cuda:0')
percent tensor([0.7427, 0.2573], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 124 | Batch_idx: 0 |  Loss: (0.1399) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 124 | Batch_idx: 10 |  Loss: (0.1588) |  Loss2: (0.0000) | Acc: (94.00%) (1335/1408)
Epoch: 124 | Batch_idx: 20 |  Loss: (0.1616) |  Loss2: (0.0000) | Acc: (94.00%) (2548/2688)
Epoch: 124 | Batch_idx: 30 |  Loss: (0.1646) |  Loss2: (0.0000) | Acc: (94.00%) (3758/3968)
Epoch: 124 | Batch_idx: 40 |  Loss: (0.1651) |  Loss2: (0.0000) | Acc: (94.00%) (4966/5248)
Epoch: 124 | Batch_idx: 50 |  Loss: (0.1619) |  Loss2: (0.0000) | Acc: (94.00%) (6175/6528)
Epoch: 124 | Batch_idx: 60 |  Loss: (0.1597) |  Loss2: (0.0000) | Acc: (94.00%) (7389/7808)
Epoch: 124 | Batch_idx: 70 |  Loss: (0.1582) |  Loss2: (0.0000) | Acc: (94.00%) (8604/9088)
Epoch: 124 | Batch_idx: 80 |  Loss: (0.1571) |  Loss2: (0.0000) | Acc: (94.00%) (9817/10368)
Epoch: 124 | Batch_idx: 90 |  Loss: (0.1578) |  Loss2: (0.0000) | Acc: (94.00%) (11025/11648)
Epoch: 124 | Batch_idx: 100 |  Loss: (0.1564) |  Loss2: (0.0000) | Acc: (94.00%) (12240/12928)
Epoch: 124 | Batch_idx: 110 |  Loss: (0.1578) |  Loss2: (0.0000) | Acc: (94.00%) (13447/14208)
Epoch: 124 | Batch_idx: 120 |  Loss: (0.1563) |  Loss2: (0.0000) | Acc: (94.00%) (14671/15488)
Epoch: 124 | Batch_idx: 130 |  Loss: (0.1544) |  Loss2: (0.0000) | Acc: (94.00%) (15891/16768)
Epoch: 124 | Batch_idx: 140 |  Loss: (0.1555) |  Loss2: (0.0000) | Acc: (94.00%) (17101/18048)
Epoch: 124 | Batch_idx: 150 |  Loss: (0.1566) |  Loss2: (0.0000) | Acc: (94.00%) (18307/19328)
Epoch: 124 | Batch_idx: 160 |  Loss: (0.1575) |  Loss2: (0.0000) | Acc: (94.00%) (19518/20608)
Epoch: 124 | Batch_idx: 170 |  Loss: (0.1578) |  Loss2: (0.0000) | Acc: (94.00%) (20725/21888)
Epoch: 124 | Batch_idx: 180 |  Loss: (0.1583) |  Loss2: (0.0000) | Acc: (94.00%) (21925/23168)
Epoch: 124 | Batch_idx: 190 |  Loss: (0.1590) |  Loss2: (0.0000) | Acc: (94.00%) (23125/24448)
Epoch: 124 | Batch_idx: 200 |  Loss: (0.1591) |  Loss2: (0.0000) | Acc: (94.00%) (24326/25728)
Epoch: 124 | Batch_idx: 210 |  Loss: (0.1588) |  Loss2: (0.0000) | Acc: (94.00%) (25537/27008)
Epoch: 124 | Batch_idx: 220 |  Loss: (0.1599) |  Loss2: (0.0000) | Acc: (94.00%) (26734/28288)
Epoch: 124 | Batch_idx: 230 |  Loss: (0.1603) |  Loss2: (0.0000) | Acc: (94.00%) (27932/29568)
Epoch: 124 | Batch_idx: 240 |  Loss: (0.1600) |  Loss2: (0.0000) | Acc: (94.00%) (29154/30848)
Epoch: 124 | Batch_idx: 250 |  Loss: (0.1607) |  Loss2: (0.0000) | Acc: (94.00%) (30353/32128)
Epoch: 124 | Batch_idx: 260 |  Loss: (0.1614) |  Loss2: (0.0000) | Acc: (94.00%) (31547/33408)
Epoch: 124 | Batch_idx: 270 |  Loss: (0.1615) |  Loss2: (0.0000) | Acc: (94.00%) (32750/34688)
Epoch: 124 | Batch_idx: 280 |  Loss: (0.1615) |  Loss2: (0.0000) | Acc: (94.00%) (33961/35968)
Epoch: 124 | Batch_idx: 290 |  Loss: (0.1618) |  Loss2: (0.0000) | Acc: (94.00%) (35167/37248)
Epoch: 124 | Batch_idx: 300 |  Loss: (0.1619) |  Loss2: (0.0000) | Acc: (94.00%) (36371/38528)
Epoch: 124 | Batch_idx: 310 |  Loss: (0.1633) |  Loss2: (0.0000) | Acc: (94.00%) (37562/39808)
Epoch: 124 | Batch_idx: 320 |  Loss: (0.1635) |  Loss2: (0.0000) | Acc: (94.00%) (38766/41088)
Epoch: 124 | Batch_idx: 330 |  Loss: (0.1634) |  Loss2: (0.0000) | Acc: (94.00%) (39973/42368)
Epoch: 124 | Batch_idx: 340 |  Loss: (0.1636) |  Loss2: (0.0000) | Acc: (94.00%) (41180/43648)
Epoch: 124 | Batch_idx: 350 |  Loss: (0.1635) |  Loss2: (0.0000) | Acc: (94.00%) (42394/44928)
Epoch: 124 | Batch_idx: 360 |  Loss: (0.1636) |  Loss2: (0.0000) | Acc: (94.00%) (43590/46208)
Epoch: 124 | Batch_idx: 370 |  Loss: (0.1633) |  Loss2: (0.0000) | Acc: (94.00%) (44803/47488)
Epoch: 124 | Batch_idx: 380 |  Loss: (0.1635) |  Loss2: (0.0000) | Acc: (94.00%) (46007/48768)
Epoch: 124 | Batch_idx: 390 |  Loss: (0.1643) |  Loss2: (0.0000) | Acc: (94.00%) (47157/50000)
# TEST : Loss: (0.4403) | Acc: (87.00%) (8702/10000)
percent tensor([0.5296, 0.4704], device='cuda:0')
percent tensor([0.4990, 0.5010], device='cuda:0')
percent tensor([0.5117, 0.4883], device='cuda:0')
percent tensor([0.6064, 0.3936], device='cuda:0')
percent tensor([0.6232, 0.3768], device='cuda:0')
percent tensor([0.6361, 0.3639], device='cuda:0')
percent tensor([0.7384, 0.2616], device='cuda:0')
percent tensor([0.9990, 0.0010], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 125 | Batch_idx: 0 |  Loss: (0.2090) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 125 | Batch_idx: 10 |  Loss: (0.1685) |  Loss2: (0.0000) | Acc: (94.00%) (1324/1408)
Epoch: 125 | Batch_idx: 20 |  Loss: (0.1737) |  Loss2: (0.0000) | Acc: (93.00%) (2514/2688)
Epoch: 125 | Batch_idx: 30 |  Loss: (0.1754) |  Loss2: (0.0000) | Acc: (93.00%) (3724/3968)
Epoch: 125 | Batch_idx: 40 |  Loss: (0.1776) |  Loss2: (0.0000) | Acc: (93.00%) (4919/5248)
Epoch: 125 | Batch_idx: 50 |  Loss: (0.1771) |  Loss2: (0.0000) | Acc: (93.00%) (6114/6528)
Epoch: 125 | Batch_idx: 60 |  Loss: (0.1771) |  Loss2: (0.0000) | Acc: (93.00%) (7320/7808)
Epoch: 125 | Batch_idx: 70 |  Loss: (0.1776) |  Loss2: (0.0000) | Acc: (93.00%) (8524/9088)
Epoch: 125 | Batch_idx: 80 |  Loss: (0.1783) |  Loss2: (0.0000) | Acc: (93.00%) (9725/10368)
Epoch: 125 | Batch_idx: 90 |  Loss: (0.1776) |  Loss2: (0.0000) | Acc: (93.00%) (10935/11648)
Epoch: 125 | Batch_idx: 100 |  Loss: (0.1783) |  Loss2: (0.0000) | Acc: (93.00%) (12128/12928)
Epoch: 125 | Batch_idx: 110 |  Loss: (0.1794) |  Loss2: (0.0000) | Acc: (93.00%) (13327/14208)
Epoch: 125 | Batch_idx: 120 |  Loss: (0.1795) |  Loss2: (0.0000) | Acc: (93.00%) (14520/15488)
Epoch: 125 | Batch_idx: 130 |  Loss: (0.1809) |  Loss2: (0.0000) | Acc: (93.00%) (15711/16768)
Epoch: 125 | Batch_idx: 140 |  Loss: (0.1807) |  Loss2: (0.0000) | Acc: (93.00%) (16915/18048)
Epoch: 125 | Batch_idx: 150 |  Loss: (0.1807) |  Loss2: (0.0000) | Acc: (93.00%) (18115/19328)
Epoch: 125 | Batch_idx: 160 |  Loss: (0.1796) |  Loss2: (0.0000) | Acc: (93.00%) (19317/20608)
Epoch: 125 | Batch_idx: 170 |  Loss: (0.1791) |  Loss2: (0.0000) | Acc: (93.00%) (20518/21888)
Epoch: 125 | Batch_idx: 180 |  Loss: (0.1793) |  Loss2: (0.0000) | Acc: (93.00%) (21714/23168)
Epoch: 125 | Batch_idx: 190 |  Loss: (0.1793) |  Loss2: (0.0000) | Acc: (93.00%) (22914/24448)
Epoch: 125 | Batch_idx: 200 |  Loss: (0.1791) |  Loss2: (0.0000) | Acc: (93.00%) (24118/25728)
Epoch: 125 | Batch_idx: 210 |  Loss: (0.1804) |  Loss2: (0.0000) | Acc: (93.00%) (25305/27008)
Epoch: 125 | Batch_idx: 220 |  Loss: (0.1793) |  Loss2: (0.0000) | Acc: (93.00%) (26513/28288)
Epoch: 125 | Batch_idx: 230 |  Loss: (0.1796) |  Loss2: (0.0000) | Acc: (93.00%) (27708/29568)
Epoch: 125 | Batch_idx: 240 |  Loss: (0.1794) |  Loss2: (0.0000) | Acc: (93.00%) (28902/30848)
Epoch: 125 | Batch_idx: 250 |  Loss: (0.1786) |  Loss2: (0.0000) | Acc: (93.00%) (30112/32128)
Epoch: 125 | Batch_idx: 260 |  Loss: (0.1799) |  Loss2: (0.0000) | Acc: (93.00%) (31293/33408)
Epoch: 125 | Batch_idx: 270 |  Loss: (0.1808) |  Loss2: (0.0000) | Acc: (93.00%) (32483/34688)
Epoch: 125 | Batch_idx: 280 |  Loss: (0.1806) |  Loss2: (0.0000) | Acc: (93.00%) (33680/35968)
Epoch: 125 | Batch_idx: 290 |  Loss: (0.1814) |  Loss2: (0.0000) | Acc: (93.00%) (34870/37248)
Epoch: 125 | Batch_idx: 300 |  Loss: (0.1812) |  Loss2: (0.0000) | Acc: (93.00%) (36069/38528)
Epoch: 125 | Batch_idx: 310 |  Loss: (0.1811) |  Loss2: (0.0000) | Acc: (93.00%) (37263/39808)
Epoch: 125 | Batch_idx: 320 |  Loss: (0.1810) |  Loss2: (0.0000) | Acc: (93.00%) (38462/41088)
Epoch: 125 | Batch_idx: 330 |  Loss: (0.1801) |  Loss2: (0.0000) | Acc: (93.00%) (39681/42368)
Epoch: 125 | Batch_idx: 340 |  Loss: (0.1799) |  Loss2: (0.0000) | Acc: (93.00%) (40880/43648)
Epoch: 125 | Batch_idx: 350 |  Loss: (0.1791) |  Loss2: (0.0000) | Acc: (93.00%) (42098/44928)
Epoch: 125 | Batch_idx: 360 |  Loss: (0.1788) |  Loss2: (0.0000) | Acc: (93.00%) (43296/46208)
Epoch: 125 | Batch_idx: 370 |  Loss: (0.1791) |  Loss2: (0.0000) | Acc: (93.00%) (44494/47488)
Epoch: 125 | Batch_idx: 380 |  Loss: (0.1791) |  Loss2: (0.0000) | Acc: (93.00%) (45694/48768)
Epoch: 125 | Batch_idx: 390 |  Loss: (0.1785) |  Loss2: (0.0000) | Acc: (93.00%) (46865/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_125.pth.tar'
# TEST : Loss: (0.4090) | Acc: (87.00%) (8797/10000)
percent tensor([0.5268, 0.4732], device='cuda:0')
percent tensor([0.5011, 0.4989], device='cuda:0')
percent tensor([0.5168, 0.4832], device='cuda:0')
percent tensor([0.6158, 0.3842], device='cuda:0')
percent tensor([0.6242, 0.3758], device='cuda:0')
percent tensor([0.6363, 0.3637], device='cuda:0')
percent tensor([0.6875, 0.3125], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
Epoch: 126 | Batch_idx: 0 |  Loss: (0.2657) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 126 | Batch_idx: 10 |  Loss: (0.1577) |  Loss2: (0.0000) | Acc: (94.00%) (1334/1408)
Epoch: 126 | Batch_idx: 20 |  Loss: (0.1596) |  Loss2: (0.0000) | Acc: (94.00%) (2540/2688)
Epoch: 126 | Batch_idx: 30 |  Loss: (0.1578) |  Loss2: (0.0000) | Acc: (94.00%) (3752/3968)
Epoch: 126 | Batch_idx: 40 |  Loss: (0.1581) |  Loss2: (0.0000) | Acc: (94.00%) (4957/5248)
Epoch: 126 | Batch_idx: 50 |  Loss: (0.1598) |  Loss2: (0.0000) | Acc: (94.00%) (6160/6528)
Epoch: 126 | Batch_idx: 60 |  Loss: (0.1597) |  Loss2: (0.0000) | Acc: (94.00%) (7365/7808)
Epoch: 126 | Batch_idx: 70 |  Loss: (0.1605) |  Loss2: (0.0000) | Acc: (94.00%) (8580/9088)
Epoch: 126 | Batch_idx: 80 |  Loss: (0.1648) |  Loss2: (0.0000) | Acc: (94.00%) (9772/10368)
Epoch: 126 | Batch_idx: 90 |  Loss: (0.1679) |  Loss2: (0.0000) | Acc: (94.00%) (10971/11648)
Epoch: 126 | Batch_idx: 100 |  Loss: (0.1659) |  Loss2: (0.0000) | Acc: (94.00%) (12188/12928)
Epoch: 126 | Batch_idx: 110 |  Loss: (0.1656) |  Loss2: (0.0000) | Acc: (94.00%) (13394/14208)
Epoch: 126 | Batch_idx: 120 |  Loss: (0.1654) |  Loss2: (0.0000) | Acc: (94.00%) (14606/15488)
Epoch: 126 | Batch_idx: 130 |  Loss: (0.1653) |  Loss2: (0.0000) | Acc: (94.00%) (15817/16768)
Epoch: 126 | Batch_idx: 140 |  Loss: (0.1636) |  Loss2: (0.0000) | Acc: (94.00%) (17034/18048)
Epoch: 126 | Batch_idx: 150 |  Loss: (0.1637) |  Loss2: (0.0000) | Acc: (94.00%) (18248/19328)
Epoch: 126 | Batch_idx: 160 |  Loss: (0.1624) |  Loss2: (0.0000) | Acc: (94.00%) (19466/20608)
Epoch: 126 | Batch_idx: 170 |  Loss: (0.1629) |  Loss2: (0.0000) | Acc: (94.00%) (20669/21888)
Epoch: 126 | Batch_idx: 180 |  Loss: (0.1642) |  Loss2: (0.0000) | Acc: (94.00%) (21869/23168)
Epoch: 126 | Batch_idx: 190 |  Loss: (0.1646) |  Loss2: (0.0000) | Acc: (94.00%) (23073/24448)
Epoch: 126 | Batch_idx: 200 |  Loss: (0.1643) |  Loss2: (0.0000) | Acc: (94.00%) (24281/25728)
Epoch: 126 | Batch_idx: 210 |  Loss: (0.1656) |  Loss2: (0.0000) | Acc: (94.00%) (25478/27008)
Epoch: 126 | Batch_idx: 220 |  Loss: (0.1653) |  Loss2: (0.0000) | Acc: (94.00%) (26689/28288)
Epoch: 126 | Batch_idx: 230 |  Loss: (0.1661) |  Loss2: (0.0000) | Acc: (94.00%) (27882/29568)
Epoch: 126 | Batch_idx: 240 |  Loss: (0.1663) |  Loss2: (0.0000) | Acc: (94.00%) (29086/30848)
Epoch: 126 | Batch_idx: 250 |  Loss: (0.1657) |  Loss2: (0.0000) | Acc: (94.00%) (30306/32128)
Epoch: 126 | Batch_idx: 260 |  Loss: (0.1663) |  Loss2: (0.0000) | Acc: (94.00%) (31504/33408)
Epoch: 126 | Batch_idx: 270 |  Loss: (0.1663) |  Loss2: (0.0000) | Acc: (94.00%) (32713/34688)
Epoch: 126 | Batch_idx: 280 |  Loss: (0.1654) |  Loss2: (0.0000) | Acc: (94.00%) (33932/35968)
Epoch: 126 | Batch_idx: 290 |  Loss: (0.1649) |  Loss2: (0.0000) | Acc: (94.00%) (35144/37248)
Epoch: 126 | Batch_idx: 300 |  Loss: (0.1651) |  Loss2: (0.0000) | Acc: (94.00%) (36342/38528)
Epoch: 126 | Batch_idx: 310 |  Loss: (0.1655) |  Loss2: (0.0000) | Acc: (94.00%) (37553/39808)
Epoch: 126 | Batch_idx: 320 |  Loss: (0.1657) |  Loss2: (0.0000) | Acc: (94.00%) (38757/41088)
Epoch: 126 | Batch_idx: 330 |  Loss: (0.1654) |  Loss2: (0.0000) | Acc: (94.00%) (39970/42368)
Epoch: 126 | Batch_idx: 340 |  Loss: (0.1652) |  Loss2: (0.0000) | Acc: (94.00%) (41183/43648)
Epoch: 126 | Batch_idx: 350 |  Loss: (0.1646) |  Loss2: (0.0000) | Acc: (94.00%) (42401/44928)
Epoch: 126 | Batch_idx: 360 |  Loss: (0.1646) |  Loss2: (0.0000) | Acc: (94.00%) (43607/46208)
Epoch: 126 | Batch_idx: 370 |  Loss: (0.1646) |  Loss2: (0.0000) | Acc: (94.00%) (44819/47488)
Epoch: 126 | Batch_idx: 380 |  Loss: (0.1643) |  Loss2: (0.0000) | Acc: (94.00%) (46024/48768)
Epoch: 126 | Batch_idx: 390 |  Loss: (0.1636) |  Loss2: (0.0000) | Acc: (94.00%) (47191/50000)
# TEST : Loss: (0.3933) | Acc: (88.00%) (8842/10000)
percent tensor([0.5252, 0.4748], device='cuda:0')
percent tensor([0.5024, 0.4976], device='cuda:0')
percent tensor([0.5197, 0.4803], device='cuda:0')
percent tensor([0.6123, 0.3877], device='cuda:0')
percent tensor([0.6226, 0.3774], device='cuda:0')
percent tensor([0.6383, 0.3617], device='cuda:0')
percent tensor([0.6904, 0.3096], device='cuda:0')
percent tensor([0.9990, 0.0010], device='cuda:0')
Epoch: 127 | Batch_idx: 0 |  Loss: (0.2044) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 127 | Batch_idx: 10 |  Loss: (0.1749) |  Loss2: (0.0000) | Acc: (93.00%) (1320/1408)
Epoch: 127 | Batch_idx: 20 |  Loss: (0.1636) |  Loss2: (0.0000) | Acc: (94.00%) (2528/2688)
Epoch: 127 | Batch_idx: 30 |  Loss: (0.1592) |  Loss2: (0.0000) | Acc: (94.00%) (3745/3968)
Epoch: 127 | Batch_idx: 40 |  Loss: (0.1581) |  Loss2: (0.0000) | Acc: (94.00%) (4952/5248)
Epoch: 127 | Batch_idx: 50 |  Loss: (0.1579) |  Loss2: (0.0000) | Acc: (94.00%) (6159/6528)
Epoch: 127 | Batch_idx: 60 |  Loss: (0.1546) |  Loss2: (0.0000) | Acc: (94.00%) (7376/7808)
Epoch: 127 | Batch_idx: 70 |  Loss: (0.1540) |  Loss2: (0.0000) | Acc: (94.00%) (8590/9088)
Epoch: 127 | Batch_idx: 80 |  Loss: (0.1527) |  Loss2: (0.0000) | Acc: (94.00%) (9808/10368)
Epoch: 127 | Batch_idx: 90 |  Loss: (0.1530) |  Loss2: (0.0000) | Acc: (94.00%) (11018/11648)
Epoch: 127 | Batch_idx: 100 |  Loss: (0.1540) |  Loss2: (0.0000) | Acc: (94.00%) (12221/12928)
Epoch: 127 | Batch_idx: 110 |  Loss: (0.1551) |  Loss2: (0.0000) | Acc: (94.00%) (13429/14208)
Epoch: 127 | Batch_idx: 120 |  Loss: (0.1552) |  Loss2: (0.0000) | Acc: (94.00%) (14640/15488)
Epoch: 127 | Batch_idx: 130 |  Loss: (0.1533) |  Loss2: (0.0000) | Acc: (94.00%) (15869/16768)
Epoch: 127 | Batch_idx: 140 |  Loss: (0.1541) |  Loss2: (0.0000) | Acc: (94.00%) (17075/18048)
Epoch: 127 | Batch_idx: 150 |  Loss: (0.1557) |  Loss2: (0.0000) | Acc: (94.00%) (18275/19328)
Epoch: 127 | Batch_idx: 160 |  Loss: (0.1581) |  Loss2: (0.0000) | Acc: (94.00%) (19474/20608)
Epoch: 127 | Batch_idx: 170 |  Loss: (0.1593) |  Loss2: (0.0000) | Acc: (94.00%) (20673/21888)
Epoch: 127 | Batch_idx: 180 |  Loss: (0.1597) |  Loss2: (0.0000) | Acc: (94.00%) (21884/23168)
Epoch: 127 | Batch_idx: 190 |  Loss: (0.1588) |  Loss2: (0.0000) | Acc: (94.00%) (23106/24448)
Epoch: 127 | Batch_idx: 200 |  Loss: (0.1585) |  Loss2: (0.0000) | Acc: (94.00%) (24322/25728)
Epoch: 127 | Batch_idx: 210 |  Loss: (0.1577) |  Loss2: (0.0000) | Acc: (94.00%) (25546/27008)
Epoch: 127 | Batch_idx: 220 |  Loss: (0.1574) |  Loss2: (0.0000) | Acc: (94.00%) (26761/28288)
Epoch: 127 | Batch_idx: 230 |  Loss: (0.1574) |  Loss2: (0.0000) | Acc: (94.00%) (27973/29568)
Epoch: 127 | Batch_idx: 240 |  Loss: (0.1563) |  Loss2: (0.0000) | Acc: (94.00%) (29192/30848)
Epoch: 127 | Batch_idx: 250 |  Loss: (0.1565) |  Loss2: (0.0000) | Acc: (94.00%) (30400/32128)
Epoch: 127 | Batch_idx: 260 |  Loss: (0.1566) |  Loss2: (0.0000) | Acc: (94.00%) (31612/33408)
Epoch: 127 | Batch_idx: 270 |  Loss: (0.1558) |  Loss2: (0.0000) | Acc: (94.00%) (32829/34688)
Epoch: 127 | Batch_idx: 280 |  Loss: (0.1553) |  Loss2: (0.0000) | Acc: (94.00%) (34054/35968)
Epoch: 127 | Batch_idx: 290 |  Loss: (0.1559) |  Loss2: (0.0000) | Acc: (94.00%) (35255/37248)
Epoch: 127 | Batch_idx: 300 |  Loss: (0.1558) |  Loss2: (0.0000) | Acc: (94.00%) (36467/38528)
Epoch: 127 | Batch_idx: 310 |  Loss: (0.1565) |  Loss2: (0.0000) | Acc: (94.00%) (37664/39808)
Epoch: 127 | Batch_idx: 320 |  Loss: (0.1563) |  Loss2: (0.0000) | Acc: (94.00%) (38883/41088)
Epoch: 127 | Batch_idx: 330 |  Loss: (0.1574) |  Loss2: (0.0000) | Acc: (94.00%) (40079/42368)
Epoch: 127 | Batch_idx: 340 |  Loss: (0.1576) |  Loss2: (0.0000) | Acc: (94.00%) (41286/43648)
Epoch: 127 | Batch_idx: 350 |  Loss: (0.1571) |  Loss2: (0.0000) | Acc: (94.00%) (42506/44928)
Epoch: 127 | Batch_idx: 360 |  Loss: (0.1565) |  Loss2: (0.0000) | Acc: (94.00%) (43729/46208)
Epoch: 127 | Batch_idx: 370 |  Loss: (0.1567) |  Loss2: (0.0000) | Acc: (94.00%) (44936/47488)
Epoch: 127 | Batch_idx: 380 |  Loss: (0.1570) |  Loss2: (0.0000) | Acc: (94.00%) (46142/48768)
Epoch: 127 | Batch_idx: 390 |  Loss: (0.1568) |  Loss2: (0.0000) | Acc: (94.00%) (47322/50000)
# TEST : Loss: (0.3863) | Acc: (88.00%) (8846/10000)
percent tensor([0.5248, 0.4752], device='cuda:0')
percent tensor([0.5031, 0.4969], device='cuda:0')
percent tensor([0.5210, 0.4790], device='cuda:0')
percent tensor([0.6126, 0.3874], device='cuda:0')
percent tensor([0.6216, 0.3784], device='cuda:0')
percent tensor([0.6362, 0.3638], device='cuda:0')
percent tensor([0.6843, 0.3157], device='cuda:0')
percent tensor([0.9990, 0.0010], device='cuda:0')
Epoch: 128 | Batch_idx: 0 |  Loss: (0.1805) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 128 | Batch_idx: 10 |  Loss: (0.1588) |  Loss2: (0.0000) | Acc: (94.00%) (1328/1408)
Epoch: 128 | Batch_idx: 20 |  Loss: (0.1525) |  Loss2: (0.0000) | Acc: (94.00%) (2545/2688)
Epoch: 128 | Batch_idx: 30 |  Loss: (0.1492) |  Loss2: (0.0000) | Acc: (94.00%) (3759/3968)
Epoch: 128 | Batch_idx: 40 |  Loss: (0.1557) |  Loss2: (0.0000) | Acc: (94.00%) (4965/5248)
Epoch: 128 | Batch_idx: 50 |  Loss: (0.1537) |  Loss2: (0.0000) | Acc: (94.00%) (6181/6528)
Epoch: 128 | Batch_idx: 60 |  Loss: (0.1565) |  Loss2: (0.0000) | Acc: (94.00%) (7381/7808)
Epoch: 128 | Batch_idx: 70 |  Loss: (0.1530) |  Loss2: (0.0000) | Acc: (94.00%) (8605/9088)
Epoch: 128 | Batch_idx: 80 |  Loss: (0.1539) |  Loss2: (0.0000) | Acc: (94.00%) (9812/10368)
Epoch: 128 | Batch_idx: 90 |  Loss: (0.1541) |  Loss2: (0.0000) | Acc: (94.00%) (11026/11648)
Epoch: 128 | Batch_idx: 100 |  Loss: (0.1542) |  Loss2: (0.0000) | Acc: (94.00%) (12238/12928)
Epoch: 128 | Batch_idx: 110 |  Loss: (0.1546) |  Loss2: (0.0000) | Acc: (94.00%) (13448/14208)
Epoch: 128 | Batch_idx: 120 |  Loss: (0.1546) |  Loss2: (0.0000) | Acc: (94.00%) (14659/15488)
Epoch: 128 | Batch_idx: 130 |  Loss: (0.1553) |  Loss2: (0.0000) | Acc: (94.00%) (15869/16768)
Epoch: 128 | Batch_idx: 140 |  Loss: (0.1554) |  Loss2: (0.0000) | Acc: (94.00%) (17087/18048)
Epoch: 128 | Batch_idx: 150 |  Loss: (0.1547) |  Loss2: (0.0000) | Acc: (94.00%) (18306/19328)
Epoch: 128 | Batch_idx: 160 |  Loss: (0.1546) |  Loss2: (0.0000) | Acc: (94.00%) (19521/20608)
Epoch: 128 | Batch_idx: 170 |  Loss: (0.1543) |  Loss2: (0.0000) | Acc: (94.00%) (20741/21888)
Epoch: 128 | Batch_idx: 180 |  Loss: (0.1536) |  Loss2: (0.0000) | Acc: (94.00%) (21951/23168)
Epoch: 128 | Batch_idx: 190 |  Loss: (0.1535) |  Loss2: (0.0000) | Acc: (94.00%) (23162/24448)
Epoch: 128 | Batch_idx: 200 |  Loss: (0.1541) |  Loss2: (0.0000) | Acc: (94.00%) (24371/25728)
Epoch: 128 | Batch_idx: 210 |  Loss: (0.1538) |  Loss2: (0.0000) | Acc: (94.00%) (25581/27008)
Epoch: 128 | Batch_idx: 220 |  Loss: (0.1531) |  Loss2: (0.0000) | Acc: (94.00%) (26799/28288)
Epoch: 128 | Batch_idx: 230 |  Loss: (0.1533) |  Loss2: (0.0000) | Acc: (94.00%) (28018/29568)
Epoch: 128 | Batch_idx: 240 |  Loss: (0.1532) |  Loss2: (0.0000) | Acc: (94.00%) (29238/30848)
Epoch: 128 | Batch_idx: 250 |  Loss: (0.1534) |  Loss2: (0.0000) | Acc: (94.00%) (30445/32128)
Epoch: 128 | Batch_idx: 260 |  Loss: (0.1533) |  Loss2: (0.0000) | Acc: (94.00%) (31658/33408)
Epoch: 128 | Batch_idx: 270 |  Loss: (0.1534) |  Loss2: (0.0000) | Acc: (94.00%) (32870/34688)
Epoch: 128 | Batch_idx: 280 |  Loss: (0.1537) |  Loss2: (0.0000) | Acc: (94.00%) (34082/35968)
Epoch: 128 | Batch_idx: 290 |  Loss: (0.1536) |  Loss2: (0.0000) | Acc: (94.00%) (35304/37248)
Epoch: 128 | Batch_idx: 300 |  Loss: (0.1536) |  Loss2: (0.0000) | Acc: (94.00%) (36511/38528)
Epoch: 128 | Batch_idx: 310 |  Loss: (0.1544) |  Loss2: (0.0000) | Acc: (94.00%) (37708/39808)
Epoch: 128 | Batch_idx: 320 |  Loss: (0.1549) |  Loss2: (0.0000) | Acc: (94.00%) (38914/41088)
Epoch: 128 | Batch_idx: 330 |  Loss: (0.1552) |  Loss2: (0.0000) | Acc: (94.00%) (40115/42368)
Epoch: 128 | Batch_idx: 340 |  Loss: (0.1556) |  Loss2: (0.0000) | Acc: (94.00%) (41318/43648)
Epoch: 128 | Batch_idx: 350 |  Loss: (0.1559) |  Loss2: (0.0000) | Acc: (94.00%) (42527/44928)
Epoch: 128 | Batch_idx: 360 |  Loss: (0.1557) |  Loss2: (0.0000) | Acc: (94.00%) (43744/46208)
Epoch: 128 | Batch_idx: 370 |  Loss: (0.1557) |  Loss2: (0.0000) | Acc: (94.00%) (44954/47488)
Epoch: 128 | Batch_idx: 380 |  Loss: (0.1562) |  Loss2: (0.0000) | Acc: (94.00%) (46166/48768)
Epoch: 128 | Batch_idx: 390 |  Loss: (0.1562) |  Loss2: (0.0000) | Acc: (94.00%) (47325/50000)
# TEST : Loss: (0.3836) | Acc: (88.00%) (8854/10000)
percent tensor([0.5243, 0.4757], device='cuda:0')
percent tensor([0.5045, 0.4955], device='cuda:0')
percent tensor([0.5236, 0.4764], device='cuda:0')
percent tensor([0.6158, 0.3842], device='cuda:0')
percent tensor([0.6249, 0.3751], device='cuda:0')
percent tensor([0.6371, 0.3629], device='cuda:0')
percent tensor([0.6934, 0.3066], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 129 | Batch_idx: 0 |  Loss: (0.1196) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 129 | Batch_idx: 10 |  Loss: (0.1554) |  Loss2: (0.0000) | Acc: (94.00%) (1334/1408)
Epoch: 129 | Batch_idx: 20 |  Loss: (0.1657) |  Loss2: (0.0000) | Acc: (94.00%) (2530/2688)
Epoch: 129 | Batch_idx: 30 |  Loss: (0.1593) |  Loss2: (0.0000) | Acc: (94.00%) (3747/3968)
Epoch: 129 | Batch_idx: 40 |  Loss: (0.1603) |  Loss2: (0.0000) | Acc: (94.00%) (4954/5248)
Epoch: 129 | Batch_idx: 50 |  Loss: (0.1592) |  Loss2: (0.0000) | Acc: (94.00%) (6161/6528)
Epoch: 129 | Batch_idx: 60 |  Loss: (0.1557) |  Loss2: (0.0000) | Acc: (94.00%) (7381/7808)
Epoch: 129 | Batch_idx: 70 |  Loss: (0.1539) |  Loss2: (0.0000) | Acc: (94.00%) (8599/9088)
Epoch: 129 | Batch_idx: 80 |  Loss: (0.1522) |  Loss2: (0.0000) | Acc: (94.00%) (9817/10368)
Epoch: 129 | Batch_idx: 90 |  Loss: (0.1530) |  Loss2: (0.0000) | Acc: (94.00%) (11032/11648)
Epoch: 129 | Batch_idx: 100 |  Loss: (0.1530) |  Loss2: (0.0000) | Acc: (94.00%) (12246/12928)
Epoch: 129 | Batch_idx: 110 |  Loss: (0.1507) |  Loss2: (0.0000) | Acc: (94.00%) (13465/14208)
Epoch: 129 | Batch_idx: 120 |  Loss: (0.1497) |  Loss2: (0.0000) | Acc: (94.00%) (14692/15488)
Epoch: 129 | Batch_idx: 130 |  Loss: (0.1509) |  Loss2: (0.0000) | Acc: (94.00%) (15905/16768)
Epoch: 129 | Batch_idx: 140 |  Loss: (0.1521) |  Loss2: (0.0000) | Acc: (94.00%) (17114/18048)
Epoch: 129 | Batch_idx: 150 |  Loss: (0.1522) |  Loss2: (0.0000) | Acc: (94.00%) (18335/19328)
Epoch: 129 | Batch_idx: 160 |  Loss: (0.1524) |  Loss2: (0.0000) | Acc: (94.00%) (19544/20608)
Epoch: 129 | Batch_idx: 170 |  Loss: (0.1522) |  Loss2: (0.0000) | Acc: (94.00%) (20753/21888)
Epoch: 129 | Batch_idx: 180 |  Loss: (0.1513) |  Loss2: (0.0000) | Acc: (94.00%) (21976/23168)
Epoch: 129 | Batch_idx: 190 |  Loss: (0.1516) |  Loss2: (0.0000) | Acc: (94.00%) (23187/24448)
Epoch: 129 | Batch_idx: 200 |  Loss: (0.1520) |  Loss2: (0.0000) | Acc: (94.00%) (24392/25728)
Epoch: 129 | Batch_idx: 210 |  Loss: (0.1525) |  Loss2: (0.0000) | Acc: (94.00%) (25597/27008)
Epoch: 129 | Batch_idx: 220 |  Loss: (0.1516) |  Loss2: (0.0000) | Acc: (94.00%) (26823/28288)
Epoch: 129 | Batch_idx: 230 |  Loss: (0.1514) |  Loss2: (0.0000) | Acc: (94.00%) (28039/29568)
Epoch: 129 | Batch_idx: 240 |  Loss: (0.1505) |  Loss2: (0.0000) | Acc: (94.00%) (29263/30848)
Epoch: 129 | Batch_idx: 250 |  Loss: (0.1505) |  Loss2: (0.0000) | Acc: (94.00%) (30478/32128)
Epoch: 129 | Batch_idx: 260 |  Loss: (0.1498) |  Loss2: (0.0000) | Acc: (94.00%) (31703/33408)
Epoch: 129 | Batch_idx: 270 |  Loss: (0.1504) |  Loss2: (0.0000) | Acc: (94.00%) (32901/34688)
Epoch: 129 | Batch_idx: 280 |  Loss: (0.1507) |  Loss2: (0.0000) | Acc: (94.00%) (34110/35968)
Epoch: 129 | Batch_idx: 290 |  Loss: (0.1514) |  Loss2: (0.0000) | Acc: (94.00%) (35318/37248)
Epoch: 129 | Batch_idx: 300 |  Loss: (0.1514) |  Loss2: (0.0000) | Acc: (94.00%) (36526/38528)
Epoch: 129 | Batch_idx: 310 |  Loss: (0.1510) |  Loss2: (0.0000) | Acc: (94.00%) (37735/39808)
Epoch: 129 | Batch_idx: 320 |  Loss: (0.1510) |  Loss2: (0.0000) | Acc: (94.00%) (38948/41088)
Epoch: 129 | Batch_idx: 330 |  Loss: (0.1510) |  Loss2: (0.0000) | Acc: (94.00%) (40172/42368)
Epoch: 129 | Batch_idx: 340 |  Loss: (0.1503) |  Loss2: (0.0000) | Acc: (94.00%) (41406/43648)
Epoch: 129 | Batch_idx: 350 |  Loss: (0.1506) |  Loss2: (0.0000) | Acc: (94.00%) (42614/44928)
Epoch: 129 | Batch_idx: 360 |  Loss: (0.1505) |  Loss2: (0.0000) | Acc: (94.00%) (43827/46208)
Epoch: 129 | Batch_idx: 370 |  Loss: (0.1501) |  Loss2: (0.0000) | Acc: (94.00%) (45050/47488)
Epoch: 129 | Batch_idx: 380 |  Loss: (0.1499) |  Loss2: (0.0000) | Acc: (94.00%) (46265/48768)
Epoch: 129 | Batch_idx: 390 |  Loss: (0.1500) |  Loss2: (0.0000) | Acc: (94.00%) (47428/50000)
# TEST : Loss: (0.3832) | Acc: (88.00%) (8859/10000)
percent tensor([0.5244, 0.4756], device='cuda:0')
percent tensor([0.5047, 0.4953], device='cuda:0')
percent tensor([0.5230, 0.4770], device='cuda:0')
percent tensor([0.6096, 0.3904], device='cuda:0')
percent tensor([0.6266, 0.3734], device='cuda:0')
percent tensor([0.6372, 0.3628], device='cuda:0')
percent tensor([0.7023, 0.2977], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 130 | Batch_idx: 0 |  Loss: (0.1303) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 130 | Batch_idx: 10 |  Loss: (0.1460) |  Loss2: (0.0000) | Acc: (94.00%) (1331/1408)
Epoch: 130 | Batch_idx: 20 |  Loss: (0.1547) |  Loss2: (0.0000) | Acc: (94.00%) (2537/2688)
Epoch: 130 | Batch_idx: 30 |  Loss: (0.1582) |  Loss2: (0.0000) | Acc: (94.00%) (3743/3968)
Epoch: 130 | Batch_idx: 40 |  Loss: (0.1579) |  Loss2: (0.0000) | Acc: (94.00%) (4946/5248)
Epoch: 130 | Batch_idx: 50 |  Loss: (0.1600) |  Loss2: (0.0000) | Acc: (94.00%) (6166/6528)
Epoch: 130 | Batch_idx: 60 |  Loss: (0.1596) |  Loss2: (0.0000) | Acc: (94.00%) (7373/7808)
Epoch: 130 | Batch_idx: 70 |  Loss: (0.1609) |  Loss2: (0.0000) | Acc: (94.00%) (8581/9088)
Epoch: 130 | Batch_idx: 80 |  Loss: (0.1602) |  Loss2: (0.0000) | Acc: (94.00%) (9795/10368)
Epoch: 130 | Batch_idx: 90 |  Loss: (0.1593) |  Loss2: (0.0000) | Acc: (94.00%) (11011/11648)
Epoch: 130 | Batch_idx: 100 |  Loss: (0.1593) |  Loss2: (0.0000) | Acc: (94.00%) (12225/12928)
Epoch: 130 | Batch_idx: 110 |  Loss: (0.1593) |  Loss2: (0.0000) | Acc: (94.00%) (13444/14208)
Epoch: 130 | Batch_idx: 120 |  Loss: (0.1582) |  Loss2: (0.0000) | Acc: (94.00%) (14657/15488)
Epoch: 130 | Batch_idx: 130 |  Loss: (0.1573) |  Loss2: (0.0000) | Acc: (94.00%) (15872/16768)
Epoch: 130 | Batch_idx: 140 |  Loss: (0.1592) |  Loss2: (0.0000) | Acc: (94.00%) (17070/18048)
Epoch: 130 | Batch_idx: 150 |  Loss: (0.1610) |  Loss2: (0.0000) | Acc: (94.00%) (18265/19328)
Epoch: 130 | Batch_idx: 160 |  Loss: (0.1610) |  Loss2: (0.0000) | Acc: (94.00%) (19476/20608)
Epoch: 130 | Batch_idx: 170 |  Loss: (0.1605) |  Loss2: (0.0000) | Acc: (94.00%) (20689/21888)
Epoch: 130 | Batch_idx: 180 |  Loss: (0.1602) |  Loss2: (0.0000) | Acc: (94.00%) (21899/23168)
Epoch: 130 | Batch_idx: 190 |  Loss: (0.1593) |  Loss2: (0.0000) | Acc: (94.00%) (23114/24448)
Epoch: 130 | Batch_idx: 200 |  Loss: (0.1585) |  Loss2: (0.0000) | Acc: (94.00%) (24331/25728)
Epoch: 130 | Batch_idx: 210 |  Loss: (0.1596) |  Loss2: (0.0000) | Acc: (94.00%) (25525/27008)
Epoch: 130 | Batch_idx: 220 |  Loss: (0.1589) |  Loss2: (0.0000) | Acc: (94.00%) (26739/28288)
Epoch: 130 | Batch_idx: 230 |  Loss: (0.1584) |  Loss2: (0.0000) | Acc: (94.00%) (27951/29568)
Epoch: 130 | Batch_idx: 240 |  Loss: (0.1586) |  Loss2: (0.0000) | Acc: (94.00%) (29160/30848)
Epoch: 130 | Batch_idx: 250 |  Loss: (0.1592) |  Loss2: (0.0000) | Acc: (94.00%) (30362/32128)
Epoch: 130 | Batch_idx: 260 |  Loss: (0.1588) |  Loss2: (0.0000) | Acc: (94.00%) (31569/33408)
Epoch: 130 | Batch_idx: 270 |  Loss: (0.1590) |  Loss2: (0.0000) | Acc: (94.00%) (32774/34688)
Epoch: 130 | Batch_idx: 280 |  Loss: (0.1592) |  Loss2: (0.0000) | Acc: (94.00%) (33977/35968)
Epoch: 130 | Batch_idx: 290 |  Loss: (0.1593) |  Loss2: (0.0000) | Acc: (94.00%) (35181/37248)
Epoch: 130 | Batch_idx: 300 |  Loss: (0.1597) |  Loss2: (0.0000) | Acc: (94.00%) (36386/38528)
Epoch: 130 | Batch_idx: 310 |  Loss: (0.1600) |  Loss2: (0.0000) | Acc: (94.00%) (37589/39808)
Epoch: 130 | Batch_idx: 320 |  Loss: (0.1608) |  Loss2: (0.0000) | Acc: (94.00%) (38786/41088)
Epoch: 130 | Batch_idx: 330 |  Loss: (0.1613) |  Loss2: (0.0000) | Acc: (94.00%) (39983/42368)
Epoch: 130 | Batch_idx: 340 |  Loss: (0.1616) |  Loss2: (0.0000) | Acc: (94.00%) (41188/43648)
Epoch: 130 | Batch_idx: 350 |  Loss: (0.1621) |  Loss2: (0.0000) | Acc: (94.00%) (42392/44928)
Epoch: 130 | Batch_idx: 360 |  Loss: (0.1620) |  Loss2: (0.0000) | Acc: (94.00%) (43602/46208)
Epoch: 130 | Batch_idx: 370 |  Loss: (0.1619) |  Loss2: (0.0000) | Acc: (94.00%) (44828/47488)
Epoch: 130 | Batch_idx: 380 |  Loss: (0.1617) |  Loss2: (0.0000) | Acc: (94.00%) (46043/48768)
Epoch: 130 | Batch_idx: 390 |  Loss: (0.1614) |  Loss2: (0.0000) | Acc: (94.00%) (47216/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_130.pth.tar'
# TEST : Loss: (0.4151) | Acc: (87.00%) (8727/10000)
percent tensor([0.5243, 0.4757], device='cuda:0')
percent tensor([0.5046, 0.4954], device='cuda:0')
percent tensor([0.5229, 0.4771], device='cuda:0')
percent tensor([0.6079, 0.3921], device='cuda:0')
percent tensor([0.6253, 0.3747], device='cuda:0')
percent tensor([0.6367, 0.3633], device='cuda:0')
percent tensor([0.7018, 0.2982], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(181.9299, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(824.2408, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(816.7097, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1507.3893, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(498.9632, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2257.5488, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4269.5879, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1386.8275, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6204.1255, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11771.6562, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3894.0447, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16440.9160, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 131 | Batch_idx: 0 |  Loss: (0.1682) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 131 | Batch_idx: 10 |  Loss: (0.1421) |  Loss2: (0.0000) | Acc: (95.00%) (1346/1408)
Epoch: 131 | Batch_idx: 20 |  Loss: (0.1500) |  Loss2: (0.0000) | Acc: (94.00%) (2553/2688)
Epoch: 131 | Batch_idx: 30 |  Loss: (0.1555) |  Loss2: (0.0000) | Acc: (94.00%) (3757/3968)
Epoch: 131 | Batch_idx: 40 |  Loss: (0.1573) |  Loss2: (0.0000) | Acc: (94.00%) (4959/5248)
Epoch: 131 | Batch_idx: 50 |  Loss: (0.1587) |  Loss2: (0.0000) | Acc: (94.00%) (6165/6528)
Epoch: 131 | Batch_idx: 60 |  Loss: (0.1604) |  Loss2: (0.0000) | Acc: (94.00%) (7367/7808)
Epoch: 131 | Batch_idx: 70 |  Loss: (0.1584) |  Loss2: (0.0000) | Acc: (94.00%) (8596/9088)
Epoch: 131 | Batch_idx: 80 |  Loss: (0.1548) |  Loss2: (0.0000) | Acc: (94.00%) (9819/10368)
Epoch: 131 | Batch_idx: 90 |  Loss: (0.1553) |  Loss2: (0.0000) | Acc: (94.00%) (11028/11648)
Epoch: 131 | Batch_idx: 100 |  Loss: (0.1539) |  Loss2: (0.0000) | Acc: (94.00%) (12245/12928)
Epoch: 131 | Batch_idx: 110 |  Loss: (0.1536) |  Loss2: (0.0000) | Acc: (94.00%) (13460/14208)
Epoch: 131 | Batch_idx: 120 |  Loss: (0.1538) |  Loss2: (0.0000) | Acc: (94.00%) (14667/15488)
Epoch: 131 | Batch_idx: 130 |  Loss: (0.1533) |  Loss2: (0.0000) | Acc: (94.00%) (15873/16768)
Epoch: 131 | Batch_idx: 140 |  Loss: (0.1528) |  Loss2: (0.0000) | Acc: (94.00%) (17087/18048)
Epoch: 131 | Batch_idx: 150 |  Loss: (0.1538) |  Loss2: (0.0000) | Acc: (94.00%) (18297/19328)
Epoch: 131 | Batch_idx: 160 |  Loss: (0.1551) |  Loss2: (0.0000) | Acc: (94.00%) (19496/20608)
Epoch: 131 | Batch_idx: 170 |  Loss: (0.1548) |  Loss2: (0.0000) | Acc: (94.00%) (20702/21888)
Epoch: 131 | Batch_idx: 180 |  Loss: (0.1552) |  Loss2: (0.0000) | Acc: (94.00%) (21909/23168)
Epoch: 131 | Batch_idx: 190 |  Loss: (0.1560) |  Loss2: (0.0000) | Acc: (94.00%) (23129/24448)
Epoch: 131 | Batch_idx: 200 |  Loss: (0.1573) |  Loss2: (0.0000) | Acc: (94.00%) (24321/25728)
Epoch: 131 | Batch_idx: 210 |  Loss: (0.1583) |  Loss2: (0.0000) | Acc: (94.00%) (25512/27008)
Epoch: 131 | Batch_idx: 220 |  Loss: (0.1595) |  Loss2: (0.0000) | Acc: (94.00%) (26712/28288)
Epoch: 131 | Batch_idx: 230 |  Loss: (0.1592) |  Loss2: (0.0000) | Acc: (94.00%) (27928/29568)
Epoch: 131 | Batch_idx: 240 |  Loss: (0.1591) |  Loss2: (0.0000) | Acc: (94.00%) (29138/30848)
Epoch: 131 | Batch_idx: 250 |  Loss: (0.1580) |  Loss2: (0.0000) | Acc: (94.00%) (30363/32128)
Epoch: 131 | Batch_idx: 260 |  Loss: (0.1571) |  Loss2: (0.0000) | Acc: (94.00%) (31589/33408)
Epoch: 131 | Batch_idx: 270 |  Loss: (0.1583) |  Loss2: (0.0000) | Acc: (94.00%) (32783/34688)
Epoch: 131 | Batch_idx: 280 |  Loss: (0.1586) |  Loss2: (0.0000) | Acc: (94.00%) (33990/35968)
Epoch: 131 | Batch_idx: 290 |  Loss: (0.1584) |  Loss2: (0.0000) | Acc: (94.00%) (35202/37248)
Epoch: 131 | Batch_idx: 300 |  Loss: (0.1590) |  Loss2: (0.0000) | Acc: (94.00%) (36403/38528)
Epoch: 131 | Batch_idx: 310 |  Loss: (0.1595) |  Loss2: (0.0000) | Acc: (94.00%) (37607/39808)
Epoch: 131 | Batch_idx: 320 |  Loss: (0.1592) |  Loss2: (0.0000) | Acc: (94.00%) (38817/41088)
Epoch: 131 | Batch_idx: 330 |  Loss: (0.1603) |  Loss2: (0.0000) | Acc: (94.00%) (40012/42368)
Epoch: 131 | Batch_idx: 340 |  Loss: (0.1602) |  Loss2: (0.0000) | Acc: (94.00%) (41225/43648)
Epoch: 131 | Batch_idx: 350 |  Loss: (0.1600) |  Loss2: (0.0000) | Acc: (94.00%) (42438/44928)
Epoch: 131 | Batch_idx: 360 |  Loss: (0.1603) |  Loss2: (0.0000) | Acc: (94.00%) (43637/46208)
Epoch: 131 | Batch_idx: 370 |  Loss: (0.1596) |  Loss2: (0.0000) | Acc: (94.00%) (44858/47488)
Epoch: 131 | Batch_idx: 380 |  Loss: (0.1595) |  Loss2: (0.0000) | Acc: (94.00%) (46066/48768)
Epoch: 131 | Batch_idx: 390 |  Loss: (0.1597) |  Loss2: (0.0000) | Acc: (94.00%) (47230/50000)
# TEST : Loss: (0.4438) | Acc: (87.00%) (8710/10000)
percent tensor([0.5243, 0.4757], device='cuda:0')
percent tensor([0.5046, 0.4954], device='cuda:0')
percent tensor([0.5227, 0.4773], device='cuda:0')
percent tensor([0.6096, 0.3904], device='cuda:0')
percent tensor([0.6266, 0.3734], device='cuda:0')
percent tensor([0.6384, 0.3616], device='cuda:0')
percent tensor([0.7036, 0.2964], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 132 | Batch_idx: 0 |  Loss: (0.1096) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 132 | Batch_idx: 10 |  Loss: (0.1344) |  Loss2: (0.0000) | Acc: (95.00%) (1347/1408)
Epoch: 132 | Batch_idx: 20 |  Loss: (0.1325) |  Loss2: (0.0000) | Acc: (95.00%) (2570/2688)
Epoch: 132 | Batch_idx: 30 |  Loss: (0.1379) |  Loss2: (0.0000) | Acc: (95.00%) (3792/3968)
Epoch: 132 | Batch_idx: 40 |  Loss: (0.1358) |  Loss2: (0.0000) | Acc: (95.00%) (5017/5248)
Epoch: 132 | Batch_idx: 50 |  Loss: (0.1409) |  Loss2: (0.0000) | Acc: (95.00%) (6220/6528)
Epoch: 132 | Batch_idx: 60 |  Loss: (0.1446) |  Loss2: (0.0000) | Acc: (95.00%) (7429/7808)
Epoch: 132 | Batch_idx: 70 |  Loss: (0.1438) |  Loss2: (0.0000) | Acc: (95.00%) (8653/9088)
Epoch: 132 | Batch_idx: 80 |  Loss: (0.1443) |  Loss2: (0.0000) | Acc: (95.00%) (9867/10368)
Epoch: 132 | Batch_idx: 90 |  Loss: (0.1428) |  Loss2: (0.0000) | Acc: (95.00%) (11087/11648)
Epoch: 132 | Batch_idx: 100 |  Loss: (0.1440) |  Loss2: (0.0000) | Acc: (95.00%) (12297/12928)
Epoch: 132 | Batch_idx: 110 |  Loss: (0.1463) |  Loss2: (0.0000) | Acc: (94.00%) (13496/14208)
Epoch: 132 | Batch_idx: 120 |  Loss: (0.1476) |  Loss2: (0.0000) | Acc: (94.00%) (14700/15488)
Epoch: 132 | Batch_idx: 130 |  Loss: (0.1479) |  Loss2: (0.0000) | Acc: (94.00%) (15916/16768)
Epoch: 132 | Batch_idx: 140 |  Loss: (0.1488) |  Loss2: (0.0000) | Acc: (94.00%) (17128/18048)
Epoch: 132 | Batch_idx: 150 |  Loss: (0.1500) |  Loss2: (0.0000) | Acc: (94.00%) (18319/19328)
Epoch: 132 | Batch_idx: 160 |  Loss: (0.1496) |  Loss2: (0.0000) | Acc: (94.00%) (19534/20608)
Epoch: 132 | Batch_idx: 170 |  Loss: (0.1507) |  Loss2: (0.0000) | Acc: (94.00%) (20735/21888)
Epoch: 132 | Batch_idx: 180 |  Loss: (0.1511) |  Loss2: (0.0000) | Acc: (94.00%) (21943/23168)
Epoch: 132 | Batch_idx: 190 |  Loss: (0.1514) |  Loss2: (0.0000) | Acc: (94.00%) (23154/24448)
Epoch: 132 | Batch_idx: 200 |  Loss: (0.1518) |  Loss2: (0.0000) | Acc: (94.00%) (24364/25728)
Epoch: 132 | Batch_idx: 210 |  Loss: (0.1524) |  Loss2: (0.0000) | Acc: (94.00%) (25568/27008)
Epoch: 132 | Batch_idx: 220 |  Loss: (0.1522) |  Loss2: (0.0000) | Acc: (94.00%) (26782/28288)
Epoch: 132 | Batch_idx: 230 |  Loss: (0.1523) |  Loss2: (0.0000) | Acc: (94.00%) (28000/29568)
Epoch: 132 | Batch_idx: 240 |  Loss: (0.1516) |  Loss2: (0.0000) | Acc: (94.00%) (29219/30848)
Epoch: 132 | Batch_idx: 250 |  Loss: (0.1517) |  Loss2: (0.0000) | Acc: (94.00%) (30430/32128)
Epoch: 132 | Batch_idx: 260 |  Loss: (0.1511) |  Loss2: (0.0000) | Acc: (94.00%) (31653/33408)
Epoch: 132 | Batch_idx: 270 |  Loss: (0.1516) |  Loss2: (0.0000) | Acc: (94.00%) (32867/34688)
Epoch: 132 | Batch_idx: 280 |  Loss: (0.1511) |  Loss2: (0.0000) | Acc: (94.00%) (34099/35968)
Epoch: 132 | Batch_idx: 290 |  Loss: (0.1510) |  Loss2: (0.0000) | Acc: (94.00%) (35303/37248)
Epoch: 132 | Batch_idx: 300 |  Loss: (0.1519) |  Loss2: (0.0000) | Acc: (94.00%) (36500/38528)
Epoch: 132 | Batch_idx: 310 |  Loss: (0.1519) |  Loss2: (0.0000) | Acc: (94.00%) (37718/39808)
Epoch: 132 | Batch_idx: 320 |  Loss: (0.1525) |  Loss2: (0.0000) | Acc: (94.00%) (38921/41088)
Epoch: 132 | Batch_idx: 330 |  Loss: (0.1530) |  Loss2: (0.0000) | Acc: (94.00%) (40123/42368)
Epoch: 132 | Batch_idx: 340 |  Loss: (0.1529) |  Loss2: (0.0000) | Acc: (94.00%) (41344/43648)
Epoch: 132 | Batch_idx: 350 |  Loss: (0.1534) |  Loss2: (0.0000) | Acc: (94.00%) (42553/44928)
Epoch: 132 | Batch_idx: 360 |  Loss: (0.1541) |  Loss2: (0.0000) | Acc: (94.00%) (43760/46208)
Epoch: 132 | Batch_idx: 370 |  Loss: (0.1539) |  Loss2: (0.0000) | Acc: (94.00%) (44973/47488)
Epoch: 132 | Batch_idx: 380 |  Loss: (0.1540) |  Loss2: (0.0000) | Acc: (94.00%) (46190/48768)
Epoch: 132 | Batch_idx: 390 |  Loss: (0.1544) |  Loss2: (0.0000) | Acc: (94.00%) (47357/50000)
# TEST : Loss: (0.4431) | Acc: (86.00%) (8694/10000)
percent tensor([0.5245, 0.4755], device='cuda:0')
percent tensor([0.5046, 0.4954], device='cuda:0')
percent tensor([0.5232, 0.4768], device='cuda:0')
percent tensor([0.6109, 0.3891], device='cuda:0')
percent tensor([0.6277, 0.3723], device='cuda:0')
percent tensor([0.6371, 0.3629], device='cuda:0')
percent tensor([0.6988, 0.3012], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 133 | Batch_idx: 0 |  Loss: (0.1304) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 133 | Batch_idx: 10 |  Loss: (0.1439) |  Loss2: (0.0000) | Acc: (94.00%) (1334/1408)
Epoch: 133 | Batch_idx: 20 |  Loss: (0.1345) |  Loss2: (0.0000) | Acc: (95.00%) (2566/2688)
Epoch: 133 | Batch_idx: 30 |  Loss: (0.1326) |  Loss2: (0.0000) | Acc: (95.00%) (3790/3968)
Epoch: 133 | Batch_idx: 40 |  Loss: (0.1325) |  Loss2: (0.0000) | Acc: (95.00%) (5017/5248)
Epoch: 133 | Batch_idx: 50 |  Loss: (0.1401) |  Loss2: (0.0000) | Acc: (95.00%) (6213/6528)
Epoch: 133 | Batch_idx: 60 |  Loss: (0.1397) |  Loss2: (0.0000) | Acc: (95.00%) (7436/7808)
Epoch: 133 | Batch_idx: 70 |  Loss: (0.1390) |  Loss2: (0.0000) | Acc: (95.00%) (8662/9088)
Epoch: 133 | Batch_idx: 80 |  Loss: (0.1385) |  Loss2: (0.0000) | Acc: (95.00%) (9882/10368)
Epoch: 133 | Batch_idx: 90 |  Loss: (0.1391) |  Loss2: (0.0000) | Acc: (95.00%) (11103/11648)
Epoch: 133 | Batch_idx: 100 |  Loss: (0.1400) |  Loss2: (0.0000) | Acc: (95.00%) (12317/12928)
Epoch: 133 | Batch_idx: 110 |  Loss: (0.1421) |  Loss2: (0.0000) | Acc: (95.00%) (13524/14208)
Epoch: 133 | Batch_idx: 120 |  Loss: (0.1417) |  Loss2: (0.0000) | Acc: (95.00%) (14745/15488)
Epoch: 133 | Batch_idx: 130 |  Loss: (0.1429) |  Loss2: (0.0000) | Acc: (95.00%) (15951/16768)
Epoch: 133 | Batch_idx: 140 |  Loss: (0.1418) |  Loss2: (0.0000) | Acc: (95.00%) (17172/18048)
Epoch: 133 | Batch_idx: 150 |  Loss: (0.1417) |  Loss2: (0.0000) | Acc: (95.00%) (18395/19328)
Epoch: 133 | Batch_idx: 160 |  Loss: (0.1415) |  Loss2: (0.0000) | Acc: (95.00%) (19615/20608)
Epoch: 133 | Batch_idx: 170 |  Loss: (0.1413) |  Loss2: (0.0000) | Acc: (95.00%) (20834/21888)
Epoch: 133 | Batch_idx: 180 |  Loss: (0.1414) |  Loss2: (0.0000) | Acc: (95.00%) (22053/23168)
Epoch: 133 | Batch_idx: 190 |  Loss: (0.1417) |  Loss2: (0.0000) | Acc: (95.00%) (23263/24448)
Epoch: 133 | Batch_idx: 200 |  Loss: (0.1417) |  Loss2: (0.0000) | Acc: (95.00%) (24482/25728)
Epoch: 133 | Batch_idx: 210 |  Loss: (0.1422) |  Loss2: (0.0000) | Acc: (95.00%) (25694/27008)
Epoch: 133 | Batch_idx: 220 |  Loss: (0.1434) |  Loss2: (0.0000) | Acc: (95.00%) (26900/28288)
Epoch: 133 | Batch_idx: 230 |  Loss: (0.1433) |  Loss2: (0.0000) | Acc: (95.00%) (28118/29568)
Epoch: 133 | Batch_idx: 240 |  Loss: (0.1438) |  Loss2: (0.0000) | Acc: (95.00%) (29336/30848)
Epoch: 133 | Batch_idx: 250 |  Loss: (0.1443) |  Loss2: (0.0000) | Acc: (95.00%) (30545/32128)
Epoch: 133 | Batch_idx: 260 |  Loss: (0.1442) |  Loss2: (0.0000) | Acc: (95.00%) (31760/33408)
Epoch: 133 | Batch_idx: 270 |  Loss: (0.1444) |  Loss2: (0.0000) | Acc: (95.00%) (32972/34688)
Epoch: 133 | Batch_idx: 280 |  Loss: (0.1451) |  Loss2: (0.0000) | Acc: (95.00%) (34181/35968)
Epoch: 133 | Batch_idx: 290 |  Loss: (0.1452) |  Loss2: (0.0000) | Acc: (95.00%) (35399/37248)
Epoch: 133 | Batch_idx: 300 |  Loss: (0.1454) |  Loss2: (0.0000) | Acc: (95.00%) (36602/38528)
Epoch: 133 | Batch_idx: 310 |  Loss: (0.1453) |  Loss2: (0.0000) | Acc: (94.00%) (37812/39808)
Epoch: 133 | Batch_idx: 320 |  Loss: (0.1453) |  Loss2: (0.0000) | Acc: (94.00%) (39028/41088)
Epoch: 133 | Batch_idx: 330 |  Loss: (0.1450) |  Loss2: (0.0000) | Acc: (94.00%) (40247/42368)
Epoch: 133 | Batch_idx: 340 |  Loss: (0.1454) |  Loss2: (0.0000) | Acc: (94.00%) (41458/43648)
Epoch: 133 | Batch_idx: 350 |  Loss: (0.1463) |  Loss2: (0.0000) | Acc: (94.00%) (42656/44928)
Epoch: 133 | Batch_idx: 360 |  Loss: (0.1468) |  Loss2: (0.0000) | Acc: (94.00%) (43863/46208)
Epoch: 133 | Batch_idx: 370 |  Loss: (0.1472) |  Loss2: (0.0000) | Acc: (94.00%) (45074/47488)
Epoch: 133 | Batch_idx: 380 |  Loss: (0.1472) |  Loss2: (0.0000) | Acc: (94.00%) (46291/48768)
Epoch: 133 | Batch_idx: 390 |  Loss: (0.1478) |  Loss2: (0.0000) | Acc: (94.00%) (47443/50000)
# TEST : Loss: (0.4805) | Acc: (86.00%) (8625/10000)
percent tensor([0.5244, 0.4756], device='cuda:0')
percent tensor([0.5046, 0.4954], device='cuda:0')
percent tensor([0.5220, 0.4780], device='cuda:0')
percent tensor([0.6090, 0.3910], device='cuda:0')
percent tensor([0.6264, 0.3736], device='cuda:0')
percent tensor([0.6354, 0.3646], device='cuda:0')
percent tensor([0.6993, 0.3007], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 134 | Batch_idx: 0 |  Loss: (0.1519) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 134 | Batch_idx: 10 |  Loss: (0.1332) |  Loss2: (0.0000) | Acc: (95.00%) (1350/1408)
Epoch: 134 | Batch_idx: 20 |  Loss: (0.1504) |  Loss2: (0.0000) | Acc: (95.00%) (2555/2688)
Epoch: 134 | Batch_idx: 30 |  Loss: (0.1491) |  Loss2: (0.0000) | Acc: (94.00%) (3769/3968)
Epoch: 134 | Batch_idx: 40 |  Loss: (0.1513) |  Loss2: (0.0000) | Acc: (94.00%) (4981/5248)
Epoch: 134 | Batch_idx: 50 |  Loss: (0.1461) |  Loss2: (0.0000) | Acc: (95.00%) (6209/6528)
Epoch: 134 | Batch_idx: 60 |  Loss: (0.1467) |  Loss2: (0.0000) | Acc: (95.00%) (7421/7808)
Epoch: 134 | Batch_idx: 70 |  Loss: (0.1447) |  Loss2: (0.0000) | Acc: (95.00%) (8636/9088)
Epoch: 134 | Batch_idx: 80 |  Loss: (0.1461) |  Loss2: (0.0000) | Acc: (95.00%) (9850/10368)
Epoch: 134 | Batch_idx: 90 |  Loss: (0.1469) |  Loss2: (0.0000) | Acc: (95.00%) (11066/11648)
Epoch: 134 | Batch_idx: 100 |  Loss: (0.1480) |  Loss2: (0.0000) | Acc: (94.00%) (12263/12928)
Epoch: 134 | Batch_idx: 110 |  Loss: (0.1479) |  Loss2: (0.0000) | Acc: (94.00%) (13487/14208)
Epoch: 134 | Batch_idx: 120 |  Loss: (0.1476) |  Loss2: (0.0000) | Acc: (94.00%) (14706/15488)
Epoch: 134 | Batch_idx: 130 |  Loss: (0.1487) |  Loss2: (0.0000) | Acc: (94.00%) (15904/16768)
Epoch: 134 | Batch_idx: 140 |  Loss: (0.1484) |  Loss2: (0.0000) | Acc: (94.00%) (17115/18048)
Epoch: 134 | Batch_idx: 150 |  Loss: (0.1488) |  Loss2: (0.0000) | Acc: (94.00%) (18327/19328)
Epoch: 134 | Batch_idx: 160 |  Loss: (0.1497) |  Loss2: (0.0000) | Acc: (94.00%) (19535/20608)
Epoch: 134 | Batch_idx: 170 |  Loss: (0.1496) |  Loss2: (0.0000) | Acc: (94.00%) (20758/21888)
Epoch: 134 | Batch_idx: 180 |  Loss: (0.1495) |  Loss2: (0.0000) | Acc: (94.00%) (21970/23168)
Epoch: 134 | Batch_idx: 190 |  Loss: (0.1484) |  Loss2: (0.0000) | Acc: (94.00%) (23186/24448)
Epoch: 134 | Batch_idx: 200 |  Loss: (0.1493) |  Loss2: (0.0000) | Acc: (94.00%) (24383/25728)
Epoch: 134 | Batch_idx: 210 |  Loss: (0.1490) |  Loss2: (0.0000) | Acc: (94.00%) (25595/27008)
Epoch: 134 | Batch_idx: 220 |  Loss: (0.1492) |  Loss2: (0.0000) | Acc: (94.00%) (26805/28288)
Epoch: 134 | Batch_idx: 230 |  Loss: (0.1492) |  Loss2: (0.0000) | Acc: (94.00%) (28023/29568)
Epoch: 134 | Batch_idx: 240 |  Loss: (0.1477) |  Loss2: (0.0000) | Acc: (94.00%) (29253/30848)
Epoch: 134 | Batch_idx: 250 |  Loss: (0.1484) |  Loss2: (0.0000) | Acc: (94.00%) (30460/32128)
Epoch: 134 | Batch_idx: 260 |  Loss: (0.1483) |  Loss2: (0.0000) | Acc: (94.00%) (31679/33408)
Epoch: 134 | Batch_idx: 270 |  Loss: (0.1482) |  Loss2: (0.0000) | Acc: (94.00%) (32883/34688)
Epoch: 134 | Batch_idx: 280 |  Loss: (0.1476) |  Loss2: (0.0000) | Acc: (94.00%) (34107/35968)
Epoch: 134 | Batch_idx: 290 |  Loss: (0.1477) |  Loss2: (0.0000) | Acc: (94.00%) (35316/37248)
Epoch: 134 | Batch_idx: 300 |  Loss: (0.1484) |  Loss2: (0.0000) | Acc: (94.00%) (36513/38528)
Epoch: 134 | Batch_idx: 310 |  Loss: (0.1490) |  Loss2: (0.0000) | Acc: (94.00%) (37720/39808)
Epoch: 134 | Batch_idx: 320 |  Loss: (0.1490) |  Loss2: (0.0000) | Acc: (94.00%) (38933/41088)
Epoch: 134 | Batch_idx: 330 |  Loss: (0.1485) |  Loss2: (0.0000) | Acc: (94.00%) (40160/42368)
Epoch: 134 | Batch_idx: 340 |  Loss: (0.1485) |  Loss2: (0.0000) | Acc: (94.00%) (41374/43648)
Epoch: 134 | Batch_idx: 350 |  Loss: (0.1490) |  Loss2: (0.0000) | Acc: (94.00%) (42578/44928)
Epoch: 134 | Batch_idx: 360 |  Loss: (0.1491) |  Loss2: (0.0000) | Acc: (94.00%) (43790/46208)
Epoch: 134 | Batch_idx: 370 |  Loss: (0.1493) |  Loss2: (0.0000) | Acc: (94.00%) (45004/47488)
Epoch: 134 | Batch_idx: 380 |  Loss: (0.1496) |  Loss2: (0.0000) | Acc: (94.00%) (46210/48768)
Epoch: 134 | Batch_idx: 390 |  Loss: (0.1492) |  Loss2: (0.0000) | Acc: (94.00%) (47382/50000)
# TEST : Loss: (0.4174) | Acc: (87.00%) (8779/10000)
percent tensor([0.5243, 0.4757], device='cuda:0')
percent tensor([0.5045, 0.4955], device='cuda:0')
percent tensor([0.5231, 0.4769], device='cuda:0')
percent tensor([0.6092, 0.3908], device='cuda:0')
percent tensor([0.6261, 0.3739], device='cuda:0')
percent tensor([0.6347, 0.3653], device='cuda:0')
percent tensor([0.7009, 0.2991], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 135 | Batch_idx: 0 |  Loss: (0.1041) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 135 | Batch_idx: 10 |  Loss: (0.1723) |  Loss2: (0.0000) | Acc: (93.00%) (1315/1408)
Epoch: 135 | Batch_idx: 20 |  Loss: (0.1640) |  Loss2: (0.0000) | Acc: (93.00%) (2524/2688)
Epoch: 135 | Batch_idx: 30 |  Loss: (0.1682) |  Loss2: (0.0000) | Acc: (93.00%) (3719/3968)
Epoch: 135 | Batch_idx: 40 |  Loss: (0.1716) |  Loss2: (0.0000) | Acc: (93.00%) (4914/5248)
Epoch: 135 | Batch_idx: 50 |  Loss: (0.1741) |  Loss2: (0.0000) | Acc: (93.00%) (6113/6528)
Epoch: 135 | Batch_idx: 60 |  Loss: (0.1744) |  Loss2: (0.0000) | Acc: (93.00%) (7315/7808)
Epoch: 135 | Batch_idx: 70 |  Loss: (0.1774) |  Loss2: (0.0000) | Acc: (93.00%) (8515/9088)
Epoch: 135 | Batch_idx: 80 |  Loss: (0.1738) |  Loss2: (0.0000) | Acc: (93.00%) (9732/10368)
Epoch: 135 | Batch_idx: 90 |  Loss: (0.1769) |  Loss2: (0.0000) | Acc: (93.00%) (10916/11648)
Epoch: 135 | Batch_idx: 100 |  Loss: (0.1771) |  Loss2: (0.0000) | Acc: (93.00%) (12120/12928)
Epoch: 135 | Batch_idx: 110 |  Loss: (0.1757) |  Loss2: (0.0000) | Acc: (93.00%) (13325/14208)
Epoch: 135 | Batch_idx: 120 |  Loss: (0.1745) |  Loss2: (0.0000) | Acc: (93.00%) (14530/15488)
Epoch: 135 | Batch_idx: 130 |  Loss: (0.1735) |  Loss2: (0.0000) | Acc: (93.00%) (15736/16768)
Epoch: 135 | Batch_idx: 140 |  Loss: (0.1743) |  Loss2: (0.0000) | Acc: (93.00%) (16927/18048)
Epoch: 135 | Batch_idx: 150 |  Loss: (0.1738) |  Loss2: (0.0000) | Acc: (93.00%) (18138/19328)
Epoch: 135 | Batch_idx: 160 |  Loss: (0.1733) |  Loss2: (0.0000) | Acc: (93.00%) (19343/20608)
Epoch: 135 | Batch_idx: 170 |  Loss: (0.1716) |  Loss2: (0.0000) | Acc: (93.00%) (20561/21888)
Epoch: 135 | Batch_idx: 180 |  Loss: (0.1729) |  Loss2: (0.0000) | Acc: (93.00%) (21756/23168)
Epoch: 135 | Batch_idx: 190 |  Loss: (0.1725) |  Loss2: (0.0000) | Acc: (93.00%) (22962/24448)
Epoch: 135 | Batch_idx: 200 |  Loss: (0.1713) |  Loss2: (0.0000) | Acc: (93.00%) (24175/25728)
Epoch: 135 | Batch_idx: 210 |  Loss: (0.1716) |  Loss2: (0.0000) | Acc: (93.00%) (25372/27008)
Epoch: 135 | Batch_idx: 220 |  Loss: (0.1710) |  Loss2: (0.0000) | Acc: (93.00%) (26590/28288)
Epoch: 135 | Batch_idx: 230 |  Loss: (0.1702) |  Loss2: (0.0000) | Acc: (94.00%) (27805/29568)
Epoch: 135 | Batch_idx: 240 |  Loss: (0.1707) |  Loss2: (0.0000) | Acc: (94.00%) (29005/30848)
Epoch: 135 | Batch_idx: 250 |  Loss: (0.1703) |  Loss2: (0.0000) | Acc: (94.00%) (30208/32128)
Epoch: 135 | Batch_idx: 260 |  Loss: (0.1700) |  Loss2: (0.0000) | Acc: (94.00%) (31416/33408)
Epoch: 135 | Batch_idx: 270 |  Loss: (0.1701) |  Loss2: (0.0000) | Acc: (94.00%) (32614/34688)
Epoch: 135 | Batch_idx: 280 |  Loss: (0.1692) |  Loss2: (0.0000) | Acc: (94.00%) (33826/35968)
Epoch: 135 | Batch_idx: 290 |  Loss: (0.1689) |  Loss2: (0.0000) | Acc: (94.00%) (35031/37248)
Epoch: 135 | Batch_idx: 300 |  Loss: (0.1682) |  Loss2: (0.0000) | Acc: (94.00%) (36256/38528)
Epoch: 135 | Batch_idx: 310 |  Loss: (0.1687) |  Loss2: (0.0000) | Acc: (94.00%) (37455/39808)
Epoch: 135 | Batch_idx: 320 |  Loss: (0.1684) |  Loss2: (0.0000) | Acc: (94.00%) (38669/41088)
Epoch: 135 | Batch_idx: 330 |  Loss: (0.1678) |  Loss2: (0.0000) | Acc: (94.00%) (39884/42368)
Epoch: 135 | Batch_idx: 340 |  Loss: (0.1675) |  Loss2: (0.0000) | Acc: (94.00%) (41089/43648)
Epoch: 135 | Batch_idx: 350 |  Loss: (0.1674) |  Loss2: (0.0000) | Acc: (94.00%) (42301/44928)
Epoch: 135 | Batch_idx: 360 |  Loss: (0.1669) |  Loss2: (0.0000) | Acc: (94.00%) (43507/46208)
Epoch: 135 | Batch_idx: 370 |  Loss: (0.1661) |  Loss2: (0.0000) | Acc: (94.00%) (44727/47488)
Epoch: 135 | Batch_idx: 380 |  Loss: (0.1658) |  Loss2: (0.0000) | Acc: (94.00%) (45941/48768)
Epoch: 135 | Batch_idx: 390 |  Loss: (0.1649) |  Loss2: (0.0000) | Acc: (94.00%) (47120/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_135.pth.tar'
# TEST : Loss: (0.4193) | Acc: (87.00%) (8773/10000)
percent tensor([0.5281, 0.4719], device='cuda:0')
percent tensor([0.5047, 0.4953], device='cuda:0')
percent tensor([0.5258, 0.4742], device='cuda:0')
percent tensor([0.6142, 0.3858], device='cuda:0')
percent tensor([0.6174, 0.3826], device='cuda:0')
percent tensor([0.6459, 0.3541], device='cuda:0')
percent tensor([0.6849, 0.3151], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 136 | Batch_idx: 0 |  Loss: (0.1591) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 136 | Batch_idx: 10 |  Loss: (0.1526) |  Loss2: (0.0000) | Acc: (95.00%) (1340/1408)
Epoch: 136 | Batch_idx: 20 |  Loss: (0.1450) |  Loss2: (0.0000) | Acc: (95.00%) (2558/2688)
Epoch: 136 | Batch_idx: 30 |  Loss: (0.1522) |  Loss2: (0.0000) | Acc: (94.00%) (3764/3968)
Epoch: 136 | Batch_idx: 40 |  Loss: (0.1521) |  Loss2: (0.0000) | Acc: (94.00%) (4980/5248)
Epoch: 136 | Batch_idx: 50 |  Loss: (0.1541) |  Loss2: (0.0000) | Acc: (94.00%) (6194/6528)
Epoch: 136 | Batch_idx: 60 |  Loss: (0.1498) |  Loss2: (0.0000) | Acc: (94.00%) (7413/7808)
Epoch: 136 | Batch_idx: 70 |  Loss: (0.1519) |  Loss2: (0.0000) | Acc: (94.00%) (8615/9088)
Epoch: 136 | Batch_idx: 80 |  Loss: (0.1514) |  Loss2: (0.0000) | Acc: (94.00%) (9824/10368)
Epoch: 136 | Batch_idx: 90 |  Loss: (0.1534) |  Loss2: (0.0000) | Acc: (94.00%) (11019/11648)
Epoch: 136 | Batch_idx: 100 |  Loss: (0.1545) |  Loss2: (0.0000) | Acc: (94.00%) (12227/12928)
Epoch: 136 | Batch_idx: 110 |  Loss: (0.1536) |  Loss2: (0.0000) | Acc: (94.00%) (13444/14208)
Epoch: 136 | Batch_idx: 120 |  Loss: (0.1527) |  Loss2: (0.0000) | Acc: (94.00%) (14671/15488)
Epoch: 136 | Batch_idx: 130 |  Loss: (0.1529) |  Loss2: (0.0000) | Acc: (94.00%) (15875/16768)
Epoch: 136 | Batch_idx: 140 |  Loss: (0.1534) |  Loss2: (0.0000) | Acc: (94.00%) (17082/18048)
Epoch: 136 | Batch_idx: 150 |  Loss: (0.1535) |  Loss2: (0.0000) | Acc: (94.00%) (18290/19328)
Epoch: 136 | Batch_idx: 160 |  Loss: (0.1532) |  Loss2: (0.0000) | Acc: (94.00%) (19510/20608)
Epoch: 136 | Batch_idx: 170 |  Loss: (0.1526) |  Loss2: (0.0000) | Acc: (94.00%) (20725/21888)
Epoch: 136 | Batch_idx: 180 |  Loss: (0.1522) |  Loss2: (0.0000) | Acc: (94.00%) (21941/23168)
Epoch: 136 | Batch_idx: 190 |  Loss: (0.1518) |  Loss2: (0.0000) | Acc: (94.00%) (23157/24448)
Epoch: 136 | Batch_idx: 200 |  Loss: (0.1519) |  Loss2: (0.0000) | Acc: (94.00%) (24366/25728)
Epoch: 136 | Batch_idx: 210 |  Loss: (0.1517) |  Loss2: (0.0000) | Acc: (94.00%) (25586/27008)
Epoch: 136 | Batch_idx: 220 |  Loss: (0.1508) |  Loss2: (0.0000) | Acc: (94.00%) (26810/28288)
Epoch: 136 | Batch_idx: 230 |  Loss: (0.1508) |  Loss2: (0.0000) | Acc: (94.00%) (28013/29568)
Epoch: 136 | Batch_idx: 240 |  Loss: (0.1508) |  Loss2: (0.0000) | Acc: (94.00%) (29229/30848)
Epoch: 136 | Batch_idx: 250 |  Loss: (0.1509) |  Loss2: (0.0000) | Acc: (94.00%) (30438/32128)
Epoch: 136 | Batch_idx: 260 |  Loss: (0.1497) |  Loss2: (0.0000) | Acc: (94.00%) (31669/33408)
Epoch: 136 | Batch_idx: 270 |  Loss: (0.1499) |  Loss2: (0.0000) | Acc: (94.00%) (32875/34688)
Epoch: 136 | Batch_idx: 280 |  Loss: (0.1494) |  Loss2: (0.0000) | Acc: (94.00%) (34093/35968)
Epoch: 136 | Batch_idx: 290 |  Loss: (0.1494) |  Loss2: (0.0000) | Acc: (94.00%) (35306/37248)
Epoch: 136 | Batch_idx: 300 |  Loss: (0.1495) |  Loss2: (0.0000) | Acc: (94.00%) (36526/38528)
Epoch: 136 | Batch_idx: 310 |  Loss: (0.1487) |  Loss2: (0.0000) | Acc: (94.00%) (37747/39808)
Epoch: 136 | Batch_idx: 320 |  Loss: (0.1487) |  Loss2: (0.0000) | Acc: (94.00%) (38960/41088)
Epoch: 136 | Batch_idx: 330 |  Loss: (0.1489) |  Loss2: (0.0000) | Acc: (94.00%) (40177/42368)
Epoch: 136 | Batch_idx: 340 |  Loss: (0.1491) |  Loss2: (0.0000) | Acc: (94.00%) (41386/43648)
Epoch: 136 | Batch_idx: 350 |  Loss: (0.1494) |  Loss2: (0.0000) | Acc: (94.00%) (42595/44928)
Epoch: 136 | Batch_idx: 360 |  Loss: (0.1487) |  Loss2: (0.0000) | Acc: (94.00%) (43822/46208)
Epoch: 136 | Batch_idx: 370 |  Loss: (0.1489) |  Loss2: (0.0000) | Acc: (94.00%) (45033/47488)
Epoch: 136 | Batch_idx: 380 |  Loss: (0.1487) |  Loss2: (0.0000) | Acc: (94.00%) (46250/48768)
Epoch: 136 | Batch_idx: 390 |  Loss: (0.1487) |  Loss2: (0.0000) | Acc: (94.00%) (47412/50000)
# TEST : Loss: (0.4058) | Acc: (87.00%) (8796/10000)
percent tensor([0.5291, 0.4709], device='cuda:0')
percent tensor([0.5058, 0.4942], device='cuda:0')
percent tensor([0.5257, 0.4743], device='cuda:0')
percent tensor([0.6161, 0.3839], device='cuda:0')
percent tensor([0.6238, 0.3762], device='cuda:0')
percent tensor([0.6521, 0.3479], device='cuda:0')
percent tensor([0.6902, 0.3098], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 137 | Batch_idx: 0 |  Loss: (0.0957) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 137 | Batch_idx: 10 |  Loss: (0.1348) |  Loss2: (0.0000) | Acc: (94.00%) (1330/1408)
Epoch: 137 | Batch_idx: 20 |  Loss: (0.1354) |  Loss2: (0.0000) | Acc: (95.00%) (2556/2688)
Epoch: 137 | Batch_idx: 30 |  Loss: (0.1387) |  Loss2: (0.0000) | Acc: (95.00%) (3773/3968)
Epoch: 137 | Batch_idx: 40 |  Loss: (0.1370) |  Loss2: (0.0000) | Acc: (95.00%) (4996/5248)
Epoch: 137 | Batch_idx: 50 |  Loss: (0.1402) |  Loss2: (0.0000) | Acc: (95.00%) (6206/6528)
Epoch: 137 | Batch_idx: 60 |  Loss: (0.1418) |  Loss2: (0.0000) | Acc: (95.00%) (7420/7808)
Epoch: 137 | Batch_idx: 70 |  Loss: (0.1459) |  Loss2: (0.0000) | Acc: (94.00%) (8628/9088)
Epoch: 137 | Batch_idx: 80 |  Loss: (0.1481) |  Loss2: (0.0000) | Acc: (94.00%) (9835/10368)
Epoch: 137 | Batch_idx: 90 |  Loss: (0.1484) |  Loss2: (0.0000) | Acc: (94.00%) (11052/11648)
Epoch: 137 | Batch_idx: 100 |  Loss: (0.1471) |  Loss2: (0.0000) | Acc: (94.00%) (12272/12928)
Epoch: 137 | Batch_idx: 110 |  Loss: (0.1465) |  Loss2: (0.0000) | Acc: (94.00%) (13496/14208)
Epoch: 137 | Batch_idx: 120 |  Loss: (0.1454) |  Loss2: (0.0000) | Acc: (95.00%) (14725/15488)
Epoch: 137 | Batch_idx: 130 |  Loss: (0.1452) |  Loss2: (0.0000) | Acc: (95.00%) (15935/16768)
Epoch: 137 | Batch_idx: 140 |  Loss: (0.1440) |  Loss2: (0.0000) | Acc: (95.00%) (17163/18048)
Epoch: 137 | Batch_idx: 150 |  Loss: (0.1447) |  Loss2: (0.0000) | Acc: (95.00%) (18378/19328)
Epoch: 137 | Batch_idx: 160 |  Loss: (0.1441) |  Loss2: (0.0000) | Acc: (95.00%) (19597/20608)
Epoch: 137 | Batch_idx: 170 |  Loss: (0.1448) |  Loss2: (0.0000) | Acc: (95.00%) (20812/21888)
Epoch: 137 | Batch_idx: 180 |  Loss: (0.1462) |  Loss2: (0.0000) | Acc: (95.00%) (22027/23168)
Epoch: 137 | Batch_idx: 190 |  Loss: (0.1452) |  Loss2: (0.0000) | Acc: (95.00%) (23249/24448)
Epoch: 137 | Batch_idx: 200 |  Loss: (0.1453) |  Loss2: (0.0000) | Acc: (95.00%) (24462/25728)
Epoch: 137 | Batch_idx: 210 |  Loss: (0.1458) |  Loss2: (0.0000) | Acc: (95.00%) (25669/27008)
Epoch: 137 | Batch_idx: 220 |  Loss: (0.1462) |  Loss2: (0.0000) | Acc: (95.00%) (26875/28288)
Epoch: 137 | Batch_idx: 230 |  Loss: (0.1460) |  Loss2: (0.0000) | Acc: (95.00%) (28097/29568)
Epoch: 137 | Batch_idx: 240 |  Loss: (0.1458) |  Loss2: (0.0000) | Acc: (95.00%) (29312/30848)
Epoch: 137 | Batch_idx: 250 |  Loss: (0.1453) |  Loss2: (0.0000) | Acc: (95.00%) (30529/32128)
Epoch: 137 | Batch_idx: 260 |  Loss: (0.1446) |  Loss2: (0.0000) | Acc: (95.00%) (31753/33408)
Epoch: 137 | Batch_idx: 270 |  Loss: (0.1440) |  Loss2: (0.0000) | Acc: (95.00%) (32981/34688)
Epoch: 137 | Batch_idx: 280 |  Loss: (0.1453) |  Loss2: (0.0000) | Acc: (95.00%) (34183/35968)
Epoch: 137 | Batch_idx: 290 |  Loss: (0.1454) |  Loss2: (0.0000) | Acc: (95.00%) (35398/37248)
Epoch: 137 | Batch_idx: 300 |  Loss: (0.1450) |  Loss2: (0.0000) | Acc: (95.00%) (36623/38528)
Epoch: 137 | Batch_idx: 310 |  Loss: (0.1448) |  Loss2: (0.0000) | Acc: (95.00%) (37845/39808)
Epoch: 137 | Batch_idx: 320 |  Loss: (0.1449) |  Loss2: (0.0000) | Acc: (95.00%) (39066/41088)
Epoch: 137 | Batch_idx: 330 |  Loss: (0.1452) |  Loss2: (0.0000) | Acc: (95.00%) (40287/42368)
Epoch: 137 | Batch_idx: 340 |  Loss: (0.1453) |  Loss2: (0.0000) | Acc: (95.00%) (41505/43648)
Epoch: 137 | Batch_idx: 350 |  Loss: (0.1447) |  Loss2: (0.0000) | Acc: (95.00%) (42726/44928)
Epoch: 137 | Batch_idx: 360 |  Loss: (0.1445) |  Loss2: (0.0000) | Acc: (95.00%) (43939/46208)
Epoch: 137 | Batch_idx: 370 |  Loss: (0.1448) |  Loss2: (0.0000) | Acc: (95.00%) (45155/47488)
Epoch: 137 | Batch_idx: 380 |  Loss: (0.1446) |  Loss2: (0.0000) | Acc: (95.00%) (46373/48768)
Epoch: 137 | Batch_idx: 390 |  Loss: (0.1443) |  Loss2: (0.0000) | Acc: (95.00%) (47553/50000)
# TEST : Loss: (0.4023) | Acc: (88.00%) (8821/10000)
percent tensor([0.5307, 0.4693], device='cuda:0')
percent tensor([0.5067, 0.4933], device='cuda:0')
percent tensor([0.5255, 0.4745], device='cuda:0')
percent tensor([0.6175, 0.3825], device='cuda:0')
percent tensor([0.6261, 0.3739], device='cuda:0')
percent tensor([0.6492, 0.3508], device='cuda:0')
percent tensor([0.6890, 0.3110], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 138 | Batch_idx: 0 |  Loss: (0.1431) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 138 | Batch_idx: 10 |  Loss: (0.1428) |  Loss2: (0.0000) | Acc: (94.00%) (1335/1408)
Epoch: 138 | Batch_idx: 20 |  Loss: (0.1434) |  Loss2: (0.0000) | Acc: (95.00%) (2557/2688)
Epoch: 138 | Batch_idx: 30 |  Loss: (0.1424) |  Loss2: (0.0000) | Acc: (95.00%) (3784/3968)
Epoch: 138 | Batch_idx: 40 |  Loss: (0.1418) |  Loss2: (0.0000) | Acc: (95.00%) (5001/5248)
Epoch: 138 | Batch_idx: 50 |  Loss: (0.1436) |  Loss2: (0.0000) | Acc: (95.00%) (6207/6528)
Epoch: 138 | Batch_idx: 60 |  Loss: (0.1434) |  Loss2: (0.0000) | Acc: (95.00%) (7429/7808)
Epoch: 138 | Batch_idx: 70 |  Loss: (0.1438) |  Loss2: (0.0000) | Acc: (95.00%) (8648/9088)
Epoch: 138 | Batch_idx: 80 |  Loss: (0.1424) |  Loss2: (0.0000) | Acc: (95.00%) (9872/10368)
Epoch: 138 | Batch_idx: 90 |  Loss: (0.1427) |  Loss2: (0.0000) | Acc: (95.00%) (11086/11648)
Epoch: 138 | Batch_idx: 100 |  Loss: (0.1414) |  Loss2: (0.0000) | Acc: (95.00%) (12318/12928)
Epoch: 138 | Batch_idx: 110 |  Loss: (0.1413) |  Loss2: (0.0000) | Acc: (95.00%) (13532/14208)
Epoch: 138 | Batch_idx: 120 |  Loss: (0.1410) |  Loss2: (0.0000) | Acc: (95.00%) (14749/15488)
Epoch: 138 | Batch_idx: 130 |  Loss: (0.1430) |  Loss2: (0.0000) | Acc: (95.00%) (15961/16768)
Epoch: 138 | Batch_idx: 140 |  Loss: (0.1420) |  Loss2: (0.0000) | Acc: (95.00%) (17187/18048)
Epoch: 138 | Batch_idx: 150 |  Loss: (0.1421) |  Loss2: (0.0000) | Acc: (95.00%) (18402/19328)
Epoch: 138 | Batch_idx: 160 |  Loss: (0.1414) |  Loss2: (0.0000) | Acc: (95.00%) (19627/20608)
Epoch: 138 | Batch_idx: 170 |  Loss: (0.1414) |  Loss2: (0.0000) | Acc: (95.00%) (20843/21888)
Epoch: 138 | Batch_idx: 180 |  Loss: (0.1417) |  Loss2: (0.0000) | Acc: (95.00%) (22065/23168)
Epoch: 138 | Batch_idx: 190 |  Loss: (0.1415) |  Loss2: (0.0000) | Acc: (95.00%) (23288/24448)
Epoch: 138 | Batch_idx: 200 |  Loss: (0.1419) |  Loss2: (0.0000) | Acc: (95.00%) (24500/25728)
Epoch: 138 | Batch_idx: 210 |  Loss: (0.1423) |  Loss2: (0.0000) | Acc: (95.00%) (25706/27008)
Epoch: 138 | Batch_idx: 220 |  Loss: (0.1418) |  Loss2: (0.0000) | Acc: (95.00%) (26924/28288)
Epoch: 138 | Batch_idx: 230 |  Loss: (0.1409) |  Loss2: (0.0000) | Acc: (95.00%) (28149/29568)
Epoch: 138 | Batch_idx: 240 |  Loss: (0.1413) |  Loss2: (0.0000) | Acc: (95.00%) (29359/30848)
Epoch: 138 | Batch_idx: 250 |  Loss: (0.1413) |  Loss2: (0.0000) | Acc: (95.00%) (30579/32128)
Epoch: 138 | Batch_idx: 260 |  Loss: (0.1410) |  Loss2: (0.0000) | Acc: (95.00%) (31803/33408)
Epoch: 138 | Batch_idx: 270 |  Loss: (0.1417) |  Loss2: (0.0000) | Acc: (95.00%) (33001/34688)
Epoch: 138 | Batch_idx: 280 |  Loss: (0.1424) |  Loss2: (0.0000) | Acc: (95.00%) (34216/35968)
Epoch: 138 | Batch_idx: 290 |  Loss: (0.1421) |  Loss2: (0.0000) | Acc: (95.00%) (35443/37248)
Epoch: 138 | Batch_idx: 300 |  Loss: (0.1423) |  Loss2: (0.0000) | Acc: (95.00%) (36668/38528)
Epoch: 138 | Batch_idx: 310 |  Loss: (0.1424) |  Loss2: (0.0000) | Acc: (95.00%) (37883/39808)
Epoch: 138 | Batch_idx: 320 |  Loss: (0.1420) |  Loss2: (0.0000) | Acc: (95.00%) (39111/41088)
Epoch: 138 | Batch_idx: 330 |  Loss: (0.1414) |  Loss2: (0.0000) | Acc: (95.00%) (40346/42368)
Epoch: 138 | Batch_idx: 340 |  Loss: (0.1410) |  Loss2: (0.0000) | Acc: (95.00%) (41571/43648)
Epoch: 138 | Batch_idx: 350 |  Loss: (0.1412) |  Loss2: (0.0000) | Acc: (95.00%) (42786/44928)
Epoch: 138 | Batch_idx: 360 |  Loss: (0.1412) |  Loss2: (0.0000) | Acc: (95.00%) (44002/46208)
Epoch: 138 | Batch_idx: 370 |  Loss: (0.1411) |  Loss2: (0.0000) | Acc: (95.00%) (45229/47488)
Epoch: 138 | Batch_idx: 380 |  Loss: (0.1408) |  Loss2: (0.0000) | Acc: (95.00%) (46452/48768)
Epoch: 138 | Batch_idx: 390 |  Loss: (0.1408) |  Loss2: (0.0000) | Acc: (95.00%) (47624/50000)
# TEST : Loss: (0.3934) | Acc: (88.00%) (8839/10000)
percent tensor([0.5331, 0.4669], device='cuda:0')
percent tensor([0.5074, 0.4926], device='cuda:0')
percent tensor([0.5260, 0.4740], device='cuda:0')
percent tensor([0.6172, 0.3828], device='cuda:0')
percent tensor([0.6303, 0.3697], device='cuda:0')
percent tensor([0.6544, 0.3456], device='cuda:0')
percent tensor([0.6861, 0.3139], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 139 | Batch_idx: 0 |  Loss: (0.1167) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 139 | Batch_idx: 10 |  Loss: (0.1340) |  Loss2: (0.0000) | Acc: (95.00%) (1340/1408)
Epoch: 139 | Batch_idx: 20 |  Loss: (0.1417) |  Loss2: (0.0000) | Acc: (94.00%) (2547/2688)
Epoch: 139 | Batch_idx: 30 |  Loss: (0.1364) |  Loss2: (0.0000) | Acc: (95.00%) (3772/3968)
Epoch: 139 | Batch_idx: 40 |  Loss: (0.1396) |  Loss2: (0.0000) | Acc: (95.00%) (4990/5248)
Epoch: 139 | Batch_idx: 50 |  Loss: (0.1333) |  Loss2: (0.0000) | Acc: (95.00%) (6224/6528)
Epoch: 139 | Batch_idx: 60 |  Loss: (0.1351) |  Loss2: (0.0000) | Acc: (95.00%) (7446/7808)
Epoch: 139 | Batch_idx: 70 |  Loss: (0.1350) |  Loss2: (0.0000) | Acc: (95.00%) (8667/9088)
Epoch: 139 | Batch_idx: 80 |  Loss: (0.1343) |  Loss2: (0.0000) | Acc: (95.00%) (9886/10368)
Epoch: 139 | Batch_idx: 90 |  Loss: (0.1360) |  Loss2: (0.0000) | Acc: (95.00%) (11101/11648)
Epoch: 139 | Batch_idx: 100 |  Loss: (0.1379) |  Loss2: (0.0000) | Acc: (95.00%) (12306/12928)
Epoch: 139 | Batch_idx: 110 |  Loss: (0.1370) |  Loss2: (0.0000) | Acc: (95.00%) (13526/14208)
Epoch: 139 | Batch_idx: 120 |  Loss: (0.1353) |  Loss2: (0.0000) | Acc: (95.00%) (14753/15488)
Epoch: 139 | Batch_idx: 130 |  Loss: (0.1361) |  Loss2: (0.0000) | Acc: (95.00%) (15971/16768)
Epoch: 139 | Batch_idx: 140 |  Loss: (0.1362) |  Loss2: (0.0000) | Acc: (95.00%) (17191/18048)
Epoch: 139 | Batch_idx: 150 |  Loss: (0.1354) |  Loss2: (0.0000) | Acc: (95.00%) (18420/19328)
Epoch: 139 | Batch_idx: 160 |  Loss: (0.1358) |  Loss2: (0.0000) | Acc: (95.00%) (19635/20608)
Epoch: 139 | Batch_idx: 170 |  Loss: (0.1358) |  Loss2: (0.0000) | Acc: (95.00%) (20858/21888)
Epoch: 139 | Batch_idx: 180 |  Loss: (0.1351) |  Loss2: (0.0000) | Acc: (95.00%) (22083/23168)
Epoch: 139 | Batch_idx: 190 |  Loss: (0.1351) |  Loss2: (0.0000) | Acc: (95.00%) (23309/24448)
Epoch: 139 | Batch_idx: 200 |  Loss: (0.1352) |  Loss2: (0.0000) | Acc: (95.00%) (24529/25728)
Epoch: 139 | Batch_idx: 210 |  Loss: (0.1357) |  Loss2: (0.0000) | Acc: (95.00%) (25748/27008)
Epoch: 139 | Batch_idx: 220 |  Loss: (0.1353) |  Loss2: (0.0000) | Acc: (95.00%) (26976/28288)
Epoch: 139 | Batch_idx: 230 |  Loss: (0.1351) |  Loss2: (0.0000) | Acc: (95.00%) (28197/29568)
Epoch: 139 | Batch_idx: 240 |  Loss: (0.1351) |  Loss2: (0.0000) | Acc: (95.00%) (29421/30848)
Epoch: 139 | Batch_idx: 250 |  Loss: (0.1344) |  Loss2: (0.0000) | Acc: (95.00%) (30651/32128)
Epoch: 139 | Batch_idx: 260 |  Loss: (0.1346) |  Loss2: (0.0000) | Acc: (95.00%) (31872/33408)
Epoch: 139 | Batch_idx: 270 |  Loss: (0.1349) |  Loss2: (0.0000) | Acc: (95.00%) (33089/34688)
Epoch: 139 | Batch_idx: 280 |  Loss: (0.1351) |  Loss2: (0.0000) | Acc: (95.00%) (34310/35968)
Epoch: 139 | Batch_idx: 290 |  Loss: (0.1350) |  Loss2: (0.0000) | Acc: (95.00%) (35541/37248)
Epoch: 139 | Batch_idx: 300 |  Loss: (0.1351) |  Loss2: (0.0000) | Acc: (95.00%) (36768/38528)
Epoch: 139 | Batch_idx: 310 |  Loss: (0.1353) |  Loss2: (0.0000) | Acc: (95.00%) (37982/39808)
Epoch: 139 | Batch_idx: 320 |  Loss: (0.1351) |  Loss2: (0.0000) | Acc: (95.00%) (39208/41088)
Epoch: 139 | Batch_idx: 330 |  Loss: (0.1355) |  Loss2: (0.0000) | Acc: (95.00%) (40423/42368)
Epoch: 139 | Batch_idx: 340 |  Loss: (0.1357) |  Loss2: (0.0000) | Acc: (95.00%) (41637/43648)
Epoch: 139 | Batch_idx: 350 |  Loss: (0.1356) |  Loss2: (0.0000) | Acc: (95.00%) (42863/44928)
Epoch: 139 | Batch_idx: 360 |  Loss: (0.1358) |  Loss2: (0.0000) | Acc: (95.00%) (44077/46208)
Epoch: 139 | Batch_idx: 370 |  Loss: (0.1356) |  Loss2: (0.0000) | Acc: (95.00%) (45304/47488)
Epoch: 139 | Batch_idx: 380 |  Loss: (0.1361) |  Loss2: (0.0000) | Acc: (95.00%) (46521/48768)
Epoch: 139 | Batch_idx: 390 |  Loss: (0.1364) |  Loss2: (0.0000) | Acc: (95.00%) (47688/50000)
# TEST : Loss: (0.3933) | Acc: (88.00%) (8817/10000)
percent tensor([0.5336, 0.4664], device='cuda:0')
percent tensor([0.5072, 0.4928], device='cuda:0')
percent tensor([0.5256, 0.4744], device='cuda:0')
percent tensor([0.6219, 0.3781], device='cuda:0')
percent tensor([0.6308, 0.3692], device='cuda:0')
percent tensor([0.6574, 0.3426], device='cuda:0')
percent tensor([0.6971, 0.3029], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 140 | Batch_idx: 0 |  Loss: (0.1178) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 140 | Batch_idx: 10 |  Loss: (0.1583) |  Loss2: (0.0000) | Acc: (94.00%) (1332/1408)
Epoch: 140 | Batch_idx: 20 |  Loss: (0.1428) |  Loss2: (0.0000) | Acc: (95.00%) (2558/2688)
Epoch: 140 | Batch_idx: 30 |  Loss: (0.1360) |  Loss2: (0.0000) | Acc: (95.00%) (3793/3968)
Epoch: 140 | Batch_idx: 40 |  Loss: (0.1382) |  Loss2: (0.0000) | Acc: (95.00%) (5008/5248)
Epoch: 140 | Batch_idx: 50 |  Loss: (0.1405) |  Loss2: (0.0000) | Acc: (95.00%) (6223/6528)
Epoch: 140 | Batch_idx: 60 |  Loss: (0.1369) |  Loss2: (0.0000) | Acc: (95.00%) (7449/7808)
Epoch: 140 | Batch_idx: 70 |  Loss: (0.1377) |  Loss2: (0.0000) | Acc: (95.00%) (8670/9088)
Epoch: 140 | Batch_idx: 80 |  Loss: (0.1362) |  Loss2: (0.0000) | Acc: (95.00%) (9892/10368)
Epoch: 140 | Batch_idx: 90 |  Loss: (0.1389) |  Loss2: (0.0000) | Acc: (95.00%) (11098/11648)
Epoch: 140 | Batch_idx: 100 |  Loss: (0.1386) |  Loss2: (0.0000) | Acc: (95.00%) (12316/12928)
Epoch: 140 | Batch_idx: 110 |  Loss: (0.1388) |  Loss2: (0.0000) | Acc: (95.00%) (13541/14208)
Epoch: 140 | Batch_idx: 120 |  Loss: (0.1373) |  Loss2: (0.0000) | Acc: (95.00%) (14769/15488)
Epoch: 140 | Batch_idx: 130 |  Loss: (0.1387) |  Loss2: (0.0000) | Acc: (95.00%) (15980/16768)
Epoch: 140 | Batch_idx: 140 |  Loss: (0.1391) |  Loss2: (0.0000) | Acc: (95.00%) (17191/18048)
Epoch: 140 | Batch_idx: 150 |  Loss: (0.1379) |  Loss2: (0.0000) | Acc: (95.00%) (18418/19328)
Epoch: 140 | Batch_idx: 160 |  Loss: (0.1374) |  Loss2: (0.0000) | Acc: (95.00%) (19642/20608)
Epoch: 140 | Batch_idx: 170 |  Loss: (0.1386) |  Loss2: (0.0000) | Acc: (95.00%) (20849/21888)
Epoch: 140 | Batch_idx: 180 |  Loss: (0.1383) |  Loss2: (0.0000) | Acc: (95.00%) (22074/23168)
Epoch: 140 | Batch_idx: 190 |  Loss: (0.1406) |  Loss2: (0.0000) | Acc: (95.00%) (23267/24448)
Epoch: 140 | Batch_idx: 200 |  Loss: (0.1409) |  Loss2: (0.0000) | Acc: (95.00%) (24486/25728)
Epoch: 140 | Batch_idx: 210 |  Loss: (0.1401) |  Loss2: (0.0000) | Acc: (95.00%) (25704/27008)
Epoch: 140 | Batch_idx: 220 |  Loss: (0.1397) |  Loss2: (0.0000) | Acc: (95.00%) (26923/28288)
Epoch: 140 | Batch_idx: 230 |  Loss: (0.1394) |  Loss2: (0.0000) | Acc: (95.00%) (28141/29568)
Epoch: 140 | Batch_idx: 240 |  Loss: (0.1402) |  Loss2: (0.0000) | Acc: (95.00%) (29356/30848)
Epoch: 140 | Batch_idx: 250 |  Loss: (0.1406) |  Loss2: (0.0000) | Acc: (95.00%) (30571/32128)
Epoch: 140 | Batch_idx: 260 |  Loss: (0.1406) |  Loss2: (0.0000) | Acc: (95.00%) (31790/33408)
Epoch: 140 | Batch_idx: 270 |  Loss: (0.1414) |  Loss2: (0.0000) | Acc: (95.00%) (32992/34688)
Epoch: 140 | Batch_idx: 280 |  Loss: (0.1416) |  Loss2: (0.0000) | Acc: (95.00%) (34202/35968)
Epoch: 140 | Batch_idx: 290 |  Loss: (0.1424) |  Loss2: (0.0000) | Acc: (95.00%) (35409/37248)
Epoch: 140 | Batch_idx: 300 |  Loss: (0.1428) |  Loss2: (0.0000) | Acc: (95.00%) (36617/38528)
Epoch: 140 | Batch_idx: 310 |  Loss: (0.1434) |  Loss2: (0.0000) | Acc: (95.00%) (37829/39808)
Epoch: 140 | Batch_idx: 320 |  Loss: (0.1441) |  Loss2: (0.0000) | Acc: (95.00%) (39035/41088)
Epoch: 140 | Batch_idx: 330 |  Loss: (0.1445) |  Loss2: (0.0000) | Acc: (94.00%) (40242/42368)
Epoch: 140 | Batch_idx: 340 |  Loss: (0.1443) |  Loss2: (0.0000) | Acc: (94.00%) (41462/43648)
Epoch: 140 | Batch_idx: 350 |  Loss: (0.1447) |  Loss2: (0.0000) | Acc: (94.00%) (42668/44928)
Epoch: 140 | Batch_idx: 360 |  Loss: (0.1444) |  Loss2: (0.0000) | Acc: (94.00%) (43894/46208)
Epoch: 140 | Batch_idx: 370 |  Loss: (0.1443) |  Loss2: (0.0000) | Acc: (95.00%) (45116/47488)
Epoch: 140 | Batch_idx: 380 |  Loss: (0.1441) |  Loss2: (0.0000) | Acc: (95.00%) (46330/48768)
Epoch: 140 | Batch_idx: 390 |  Loss: (0.1451) |  Loss2: (0.0000) | Acc: (94.00%) (47471/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_140.pth.tar'
# TEST : Loss: (0.4256) | Acc: (87.00%) (8770/10000)
percent tensor([0.5336, 0.4664], device='cuda:0')
percent tensor([0.5072, 0.4928], device='cuda:0')
percent tensor([0.5254, 0.4746], device='cuda:0')
percent tensor([0.6225, 0.3775], device='cuda:0')
percent tensor([0.6345, 0.3655], device='cuda:0')
percent tensor([0.6600, 0.3400], device='cuda:0')
percent tensor([0.7031, 0.2969], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(182.3289, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(825.6276, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(818.6632, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1506.1842, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(497.4200, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2263.5176, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4267.4556, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1381.8256, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6217.7856, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11737.7314, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3879.0850, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16375.1260, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 141 | Batch_idx: 0 |  Loss: (0.1396) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 141 | Batch_idx: 10 |  Loss: (0.1196) |  Loss2: (0.0000) | Acc: (95.00%) (1345/1408)
Epoch: 141 | Batch_idx: 20 |  Loss: (0.1303) |  Loss2: (0.0000) | Acc: (95.00%) (2559/2688)
Epoch: 141 | Batch_idx: 30 |  Loss: (0.1334) |  Loss2: (0.0000) | Acc: (95.00%) (3784/3968)
Epoch: 141 | Batch_idx: 40 |  Loss: (0.1346) |  Loss2: (0.0000) | Acc: (95.00%) (5007/5248)
Epoch: 141 | Batch_idx: 50 |  Loss: (0.1350) |  Loss2: (0.0000) | Acc: (95.00%) (6227/6528)
Epoch: 141 | Batch_idx: 60 |  Loss: (0.1347) |  Loss2: (0.0000) | Acc: (95.00%) (7451/7808)
Epoch: 141 | Batch_idx: 70 |  Loss: (0.1379) |  Loss2: (0.0000) | Acc: (95.00%) (8648/9088)
Epoch: 141 | Batch_idx: 80 |  Loss: (0.1384) |  Loss2: (0.0000) | Acc: (95.00%) (9862/10368)
Epoch: 141 | Batch_idx: 90 |  Loss: (0.1390) |  Loss2: (0.0000) | Acc: (95.00%) (11079/11648)
Epoch: 141 | Batch_idx: 100 |  Loss: (0.1411) |  Loss2: (0.0000) | Acc: (94.00%) (12280/12928)
Epoch: 141 | Batch_idx: 110 |  Loss: (0.1404) |  Loss2: (0.0000) | Acc: (95.00%) (13504/14208)
Epoch: 141 | Batch_idx: 120 |  Loss: (0.1414) |  Loss2: (0.0000) | Acc: (95.00%) (14720/15488)
Epoch: 141 | Batch_idx: 130 |  Loss: (0.1427) |  Loss2: (0.0000) | Acc: (94.00%) (15929/16768)
Epoch: 141 | Batch_idx: 140 |  Loss: (0.1421) |  Loss2: (0.0000) | Acc: (95.00%) (17152/18048)
Epoch: 141 | Batch_idx: 150 |  Loss: (0.1415) |  Loss2: (0.0000) | Acc: (95.00%) (18370/19328)
Epoch: 141 | Batch_idx: 160 |  Loss: (0.1416) |  Loss2: (0.0000) | Acc: (95.00%) (19590/20608)
Epoch: 141 | Batch_idx: 170 |  Loss: (0.1411) |  Loss2: (0.0000) | Acc: (95.00%) (20808/21888)
Epoch: 141 | Batch_idx: 180 |  Loss: (0.1406) |  Loss2: (0.0000) | Acc: (95.00%) (22019/23168)
Epoch: 141 | Batch_idx: 190 |  Loss: (0.1390) |  Loss2: (0.0000) | Acc: (95.00%) (23259/24448)
Epoch: 141 | Batch_idx: 200 |  Loss: (0.1397) |  Loss2: (0.0000) | Acc: (95.00%) (24477/25728)
Epoch: 141 | Batch_idx: 210 |  Loss: (0.1395) |  Loss2: (0.0000) | Acc: (95.00%) (25694/27008)
Epoch: 141 | Batch_idx: 220 |  Loss: (0.1394) |  Loss2: (0.0000) | Acc: (95.00%) (26906/28288)
Epoch: 141 | Batch_idx: 230 |  Loss: (0.1403) |  Loss2: (0.0000) | Acc: (95.00%) (28115/29568)
Epoch: 141 | Batch_idx: 240 |  Loss: (0.1398) |  Loss2: (0.0000) | Acc: (95.00%) (29342/30848)
Epoch: 141 | Batch_idx: 250 |  Loss: (0.1405) |  Loss2: (0.0000) | Acc: (95.00%) (30551/32128)
Epoch: 141 | Batch_idx: 260 |  Loss: (0.1406) |  Loss2: (0.0000) | Acc: (95.00%) (31760/33408)
Epoch: 141 | Batch_idx: 270 |  Loss: (0.1399) |  Loss2: (0.0000) | Acc: (95.00%) (32991/34688)
Epoch: 141 | Batch_idx: 280 |  Loss: (0.1399) |  Loss2: (0.0000) | Acc: (95.00%) (34204/35968)
Epoch: 141 | Batch_idx: 290 |  Loss: (0.1389) |  Loss2: (0.0000) | Acc: (95.00%) (35439/37248)
Epoch: 141 | Batch_idx: 300 |  Loss: (0.1392) |  Loss2: (0.0000) | Acc: (95.00%) (36659/38528)
Epoch: 141 | Batch_idx: 310 |  Loss: (0.1399) |  Loss2: (0.0000) | Acc: (95.00%) (37873/39808)
Epoch: 141 | Batch_idx: 320 |  Loss: (0.1401) |  Loss2: (0.0000) | Acc: (95.00%) (39090/41088)
Epoch: 141 | Batch_idx: 330 |  Loss: (0.1402) |  Loss2: (0.0000) | Acc: (95.00%) (40305/42368)
Epoch: 141 | Batch_idx: 340 |  Loss: (0.1403) |  Loss2: (0.0000) | Acc: (95.00%) (41524/43648)
Epoch: 141 | Batch_idx: 350 |  Loss: (0.1400) |  Loss2: (0.0000) | Acc: (95.00%) (42742/44928)
Epoch: 141 | Batch_idx: 360 |  Loss: (0.1397) |  Loss2: (0.0000) | Acc: (95.00%) (43964/46208)
Epoch: 141 | Batch_idx: 370 |  Loss: (0.1397) |  Loss2: (0.0000) | Acc: (95.00%) (45187/47488)
Epoch: 141 | Batch_idx: 380 |  Loss: (0.1398) |  Loss2: (0.0000) | Acc: (95.00%) (46405/48768)
Epoch: 141 | Batch_idx: 390 |  Loss: (0.1401) |  Loss2: (0.0000) | Acc: (95.00%) (47575/50000)
# TEST : Loss: (0.4088) | Acc: (87.00%) (8796/10000)
percent tensor([0.5334, 0.4666], device='cuda:0')
percent tensor([0.5073, 0.4927], device='cuda:0')
percent tensor([0.5257, 0.4743], device='cuda:0')
percent tensor([0.6224, 0.3776], device='cuda:0')
percent tensor([0.6312, 0.3688], device='cuda:0')
percent tensor([0.6577, 0.3423], device='cuda:0')
percent tensor([0.6987, 0.3013], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 142 | Batch_idx: 0 |  Loss: (0.1055) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 142 | Batch_idx: 10 |  Loss: (0.1287) |  Loss2: (0.0000) | Acc: (95.00%) (1351/1408)
Epoch: 142 | Batch_idx: 20 |  Loss: (0.1310) |  Loss2: (0.0000) | Acc: (95.00%) (2572/2688)
Epoch: 142 | Batch_idx: 30 |  Loss: (0.1261) |  Loss2: (0.0000) | Acc: (95.00%) (3800/3968)
Epoch: 142 | Batch_idx: 40 |  Loss: (0.1293) |  Loss2: (0.0000) | Acc: (95.00%) (5012/5248)
Epoch: 142 | Batch_idx: 50 |  Loss: (0.1355) |  Loss2: (0.0000) | Acc: (95.00%) (6221/6528)
Epoch: 142 | Batch_idx: 60 |  Loss: (0.1369) |  Loss2: (0.0000) | Acc: (95.00%) (7445/7808)
Epoch: 142 | Batch_idx: 70 |  Loss: (0.1365) |  Loss2: (0.0000) | Acc: (95.00%) (8673/9088)
Epoch: 142 | Batch_idx: 80 |  Loss: (0.1388) |  Loss2: (0.0000) | Acc: (95.00%) (9882/10368)
Epoch: 142 | Batch_idx: 90 |  Loss: (0.1394) |  Loss2: (0.0000) | Acc: (95.00%) (11096/11648)
Epoch: 142 | Batch_idx: 100 |  Loss: (0.1402) |  Loss2: (0.0000) | Acc: (95.00%) (12312/12928)
Epoch: 142 | Batch_idx: 110 |  Loss: (0.1407) |  Loss2: (0.0000) | Acc: (95.00%) (13528/14208)
Epoch: 142 | Batch_idx: 120 |  Loss: (0.1408) |  Loss2: (0.0000) | Acc: (95.00%) (14744/15488)
Epoch: 142 | Batch_idx: 130 |  Loss: (0.1395) |  Loss2: (0.0000) | Acc: (95.00%) (15973/16768)
Epoch: 142 | Batch_idx: 140 |  Loss: (0.1387) |  Loss2: (0.0000) | Acc: (95.00%) (17197/18048)
Epoch: 142 | Batch_idx: 150 |  Loss: (0.1399) |  Loss2: (0.0000) | Acc: (95.00%) (18407/19328)
Epoch: 142 | Batch_idx: 160 |  Loss: (0.1399) |  Loss2: (0.0000) | Acc: (95.00%) (19625/20608)
Epoch: 142 | Batch_idx: 170 |  Loss: (0.1402) |  Loss2: (0.0000) | Acc: (95.00%) (20834/21888)
Epoch: 142 | Batch_idx: 180 |  Loss: (0.1401) |  Loss2: (0.0000) | Acc: (95.00%) (22056/23168)
Epoch: 142 | Batch_idx: 190 |  Loss: (0.1418) |  Loss2: (0.0000) | Acc: (95.00%) (23261/24448)
Epoch: 142 | Batch_idx: 200 |  Loss: (0.1419) |  Loss2: (0.0000) | Acc: (95.00%) (24479/25728)
Epoch: 142 | Batch_idx: 210 |  Loss: (0.1416) |  Loss2: (0.0000) | Acc: (95.00%) (25700/27008)
Epoch: 142 | Batch_idx: 220 |  Loss: (0.1411) |  Loss2: (0.0000) | Acc: (95.00%) (26926/28288)
Epoch: 142 | Batch_idx: 230 |  Loss: (0.1409) |  Loss2: (0.0000) | Acc: (95.00%) (28149/29568)
Epoch: 142 | Batch_idx: 240 |  Loss: (0.1411) |  Loss2: (0.0000) | Acc: (95.00%) (29364/30848)
Epoch: 142 | Batch_idx: 250 |  Loss: (0.1412) |  Loss2: (0.0000) | Acc: (95.00%) (30578/32128)
Epoch: 142 | Batch_idx: 260 |  Loss: (0.1409) |  Loss2: (0.0000) | Acc: (95.00%) (31798/33408)
Epoch: 142 | Batch_idx: 270 |  Loss: (0.1400) |  Loss2: (0.0000) | Acc: (95.00%) (33033/34688)
Epoch: 142 | Batch_idx: 280 |  Loss: (0.1399) |  Loss2: (0.0000) | Acc: (95.00%) (34253/35968)
Epoch: 142 | Batch_idx: 290 |  Loss: (0.1398) |  Loss2: (0.0000) | Acc: (95.00%) (35472/37248)
Epoch: 142 | Batch_idx: 300 |  Loss: (0.1404) |  Loss2: (0.0000) | Acc: (95.00%) (36671/38528)
Epoch: 142 | Batch_idx: 310 |  Loss: (0.1402) |  Loss2: (0.0000) | Acc: (95.00%) (37896/39808)
Epoch: 142 | Batch_idx: 320 |  Loss: (0.1404) |  Loss2: (0.0000) | Acc: (95.00%) (39117/41088)
Epoch: 142 | Batch_idx: 330 |  Loss: (0.1403) |  Loss2: (0.0000) | Acc: (95.00%) (40342/42368)
Epoch: 142 | Batch_idx: 340 |  Loss: (0.1400) |  Loss2: (0.0000) | Acc: (95.00%) (41568/43648)
Epoch: 142 | Batch_idx: 350 |  Loss: (0.1399) |  Loss2: (0.0000) | Acc: (95.00%) (42795/44928)
Epoch: 142 | Batch_idx: 360 |  Loss: (0.1395) |  Loss2: (0.0000) | Acc: (95.00%) (44014/46208)
Epoch: 142 | Batch_idx: 370 |  Loss: (0.1402) |  Loss2: (0.0000) | Acc: (95.00%) (45225/47488)
Epoch: 142 | Batch_idx: 380 |  Loss: (0.1404) |  Loss2: (0.0000) | Acc: (95.00%) (46435/48768)
Epoch: 142 | Batch_idx: 390 |  Loss: (0.1409) |  Loss2: (0.0000) | Acc: (95.00%) (47594/50000)
# TEST : Loss: (0.4815) | Acc: (86.00%) (8660/10000)
percent tensor([0.5336, 0.4664], device='cuda:0')
percent tensor([0.5071, 0.4929], device='cuda:0')
percent tensor([0.5254, 0.4746], device='cuda:0')
percent tensor([0.6217, 0.3783], device='cuda:0')
percent tensor([0.6294, 0.3706], device='cuda:0')
percent tensor([0.6574, 0.3426], device='cuda:0')
percent tensor([0.6979, 0.3021], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 143 | Batch_idx: 0 |  Loss: (0.0951) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 143 | Batch_idx: 10 |  Loss: (0.1258) |  Loss2: (0.0000) | Acc: (95.00%) (1349/1408)
Epoch: 143 | Batch_idx: 20 |  Loss: (0.1306) |  Loss2: (0.0000) | Acc: (95.00%) (2556/2688)
Epoch: 143 | Batch_idx: 30 |  Loss: (0.1279) |  Loss2: (0.0000) | Acc: (95.00%) (3783/3968)
Epoch: 143 | Batch_idx: 40 |  Loss: (0.1277) |  Loss2: (0.0000) | Acc: (95.00%) (5007/5248)
Epoch: 143 | Batch_idx: 50 |  Loss: (0.1270) |  Loss2: (0.0000) | Acc: (95.00%) (6231/6528)
Epoch: 143 | Batch_idx: 60 |  Loss: (0.1242) |  Loss2: (0.0000) | Acc: (95.00%) (7461/7808)
Epoch: 143 | Batch_idx: 70 |  Loss: (0.1274) |  Loss2: (0.0000) | Acc: (95.00%) (8673/9088)
Epoch: 143 | Batch_idx: 80 |  Loss: (0.1302) |  Loss2: (0.0000) | Acc: (95.00%) (9888/10368)
Epoch: 143 | Batch_idx: 90 |  Loss: (0.1304) |  Loss2: (0.0000) | Acc: (95.00%) (11114/11648)
Epoch: 143 | Batch_idx: 100 |  Loss: (0.1327) |  Loss2: (0.0000) | Acc: (95.00%) (12335/12928)
Epoch: 143 | Batch_idx: 110 |  Loss: (0.1327) |  Loss2: (0.0000) | Acc: (95.00%) (13551/14208)
Epoch: 143 | Batch_idx: 120 |  Loss: (0.1324) |  Loss2: (0.0000) | Acc: (95.00%) (14782/15488)
Epoch: 143 | Batch_idx: 130 |  Loss: (0.1314) |  Loss2: (0.0000) | Acc: (95.00%) (16006/16768)
Epoch: 143 | Batch_idx: 140 |  Loss: (0.1312) |  Loss2: (0.0000) | Acc: (95.00%) (17219/18048)
Epoch: 143 | Batch_idx: 150 |  Loss: (0.1297) |  Loss2: (0.0000) | Acc: (95.00%) (18450/19328)
Epoch: 143 | Batch_idx: 160 |  Loss: (0.1293) |  Loss2: (0.0000) | Acc: (95.00%) (19674/20608)
Epoch: 143 | Batch_idx: 170 |  Loss: (0.1289) |  Loss2: (0.0000) | Acc: (95.00%) (20899/21888)
Epoch: 143 | Batch_idx: 180 |  Loss: (0.1289) |  Loss2: (0.0000) | Acc: (95.00%) (22127/23168)
Epoch: 143 | Batch_idx: 190 |  Loss: (0.1294) |  Loss2: (0.0000) | Acc: (95.00%) (23343/24448)
Epoch: 143 | Batch_idx: 200 |  Loss: (0.1301) |  Loss2: (0.0000) | Acc: (95.00%) (24563/25728)
Epoch: 143 | Batch_idx: 210 |  Loss: (0.1297) |  Loss2: (0.0000) | Acc: (95.00%) (25794/27008)
Epoch: 143 | Batch_idx: 220 |  Loss: (0.1300) |  Loss2: (0.0000) | Acc: (95.00%) (27010/28288)
Epoch: 143 | Batch_idx: 230 |  Loss: (0.1298) |  Loss2: (0.0000) | Acc: (95.00%) (28233/29568)
Epoch: 143 | Batch_idx: 240 |  Loss: (0.1299) |  Loss2: (0.0000) | Acc: (95.00%) (29455/30848)
Epoch: 143 | Batch_idx: 250 |  Loss: (0.1297) |  Loss2: (0.0000) | Acc: (95.00%) (30678/32128)
Epoch: 143 | Batch_idx: 260 |  Loss: (0.1303) |  Loss2: (0.0000) | Acc: (95.00%) (31897/33408)
Epoch: 143 | Batch_idx: 270 |  Loss: (0.1305) |  Loss2: (0.0000) | Acc: (95.00%) (33109/34688)
Epoch: 143 | Batch_idx: 280 |  Loss: (0.1306) |  Loss2: (0.0000) | Acc: (95.00%) (34326/35968)
Epoch: 143 | Batch_idx: 290 |  Loss: (0.1314) |  Loss2: (0.0000) | Acc: (95.00%) (35541/37248)
Epoch: 143 | Batch_idx: 300 |  Loss: (0.1313) |  Loss2: (0.0000) | Acc: (95.00%) (36766/38528)
Epoch: 143 | Batch_idx: 310 |  Loss: (0.1327) |  Loss2: (0.0000) | Acc: (95.00%) (37965/39808)
Epoch: 143 | Batch_idx: 320 |  Loss: (0.1328) |  Loss2: (0.0000) | Acc: (95.00%) (39188/41088)
Epoch: 143 | Batch_idx: 330 |  Loss: (0.1332) |  Loss2: (0.0000) | Acc: (95.00%) (40401/42368)
Epoch: 143 | Batch_idx: 340 |  Loss: (0.1332) |  Loss2: (0.0000) | Acc: (95.00%) (41626/43648)
Epoch: 143 | Batch_idx: 350 |  Loss: (0.1332) |  Loss2: (0.0000) | Acc: (95.00%) (42846/44928)
Epoch: 143 | Batch_idx: 360 |  Loss: (0.1332) |  Loss2: (0.0000) | Acc: (95.00%) (44070/46208)
Epoch: 143 | Batch_idx: 370 |  Loss: (0.1330) |  Loss2: (0.0000) | Acc: (95.00%) (45292/47488)
Epoch: 143 | Batch_idx: 380 |  Loss: (0.1329) |  Loss2: (0.0000) | Acc: (95.00%) (46513/48768)
Epoch: 143 | Batch_idx: 390 |  Loss: (0.1335) |  Loss2: (0.0000) | Acc: (95.00%) (47676/50000)
# TEST : Loss: (0.4331) | Acc: (87.00%) (8773/10000)
percent tensor([0.5334, 0.4666], device='cuda:0')
percent tensor([0.5071, 0.4929], device='cuda:0')
percent tensor([0.5246, 0.4754], device='cuda:0')
percent tensor([0.6199, 0.3801], device='cuda:0')
percent tensor([0.6321, 0.3679], device='cuda:0')
percent tensor([0.6588, 0.3412], device='cuda:0')
percent tensor([0.7043, 0.2957], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Epoch: 144 | Batch_idx: 0 |  Loss: (0.0684) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 144 | Batch_idx: 10 |  Loss: (0.1170) |  Loss2: (0.0000) | Acc: (96.00%) (1352/1408)
Epoch: 144 | Batch_idx: 20 |  Loss: (0.1182) |  Loss2: (0.0000) | Acc: (96.00%) (2583/2688)
Epoch: 144 | Batch_idx: 30 |  Loss: (0.1203) |  Loss2: (0.0000) | Acc: (95.00%) (3805/3968)
Epoch: 144 | Batch_idx: 40 |  Loss: (0.1220) |  Loss2: (0.0000) | Acc: (95.00%) (5026/5248)
Epoch: 144 | Batch_idx: 50 |  Loss: (0.1264) |  Loss2: (0.0000) | Acc: (95.00%) (6232/6528)
Epoch: 144 | Batch_idx: 60 |  Loss: (0.1297) |  Loss2: (0.0000) | Acc: (95.00%) (7444/7808)
Epoch: 144 | Batch_idx: 70 |  Loss: (0.1298) |  Loss2: (0.0000) | Acc: (95.00%) (8663/9088)
Epoch: 144 | Batch_idx: 80 |  Loss: (0.1297) |  Loss2: (0.0000) | Acc: (95.00%) (9883/10368)
Epoch: 144 | Batch_idx: 90 |  Loss: (0.1281) |  Loss2: (0.0000) | Acc: (95.00%) (11119/11648)
Epoch: 144 | Batch_idx: 100 |  Loss: (0.1288) |  Loss2: (0.0000) | Acc: (95.00%) (12339/12928)
Epoch: 144 | Batch_idx: 110 |  Loss: (0.1276) |  Loss2: (0.0000) | Acc: (95.00%) (13570/14208)
Epoch: 144 | Batch_idx: 120 |  Loss: (0.1261) |  Loss2: (0.0000) | Acc: (95.00%) (14800/15488)
Epoch: 144 | Batch_idx: 130 |  Loss: (0.1276) |  Loss2: (0.0000) | Acc: (95.00%) (16019/16768)
Epoch: 144 | Batch_idx: 140 |  Loss: (0.1283) |  Loss2: (0.0000) | Acc: (95.00%) (17242/18048)
Epoch: 144 | Batch_idx: 150 |  Loss: (0.1272) |  Loss2: (0.0000) | Acc: (95.00%) (18472/19328)
Epoch: 144 | Batch_idx: 160 |  Loss: (0.1286) |  Loss2: (0.0000) | Acc: (95.00%) (19690/20608)
Epoch: 144 | Batch_idx: 170 |  Loss: (0.1283) |  Loss2: (0.0000) | Acc: (95.00%) (20917/21888)
Epoch: 144 | Batch_idx: 180 |  Loss: (0.1294) |  Loss2: (0.0000) | Acc: (95.00%) (22131/23168)
Epoch: 144 | Batch_idx: 190 |  Loss: (0.1304) |  Loss2: (0.0000) | Acc: (95.00%) (23337/24448)
Epoch: 144 | Batch_idx: 200 |  Loss: (0.1300) |  Loss2: (0.0000) | Acc: (95.00%) (24567/25728)
Epoch: 144 | Batch_idx: 210 |  Loss: (0.1296) |  Loss2: (0.0000) | Acc: (95.00%) (25796/27008)
Epoch: 144 | Batch_idx: 220 |  Loss: (0.1287) |  Loss2: (0.0000) | Acc: (95.00%) (27034/28288)
Epoch: 144 | Batch_idx: 230 |  Loss: (0.1291) |  Loss2: (0.0000) | Acc: (95.00%) (28250/29568)
Epoch: 144 | Batch_idx: 240 |  Loss: (0.1297) |  Loss2: (0.0000) | Acc: (95.00%) (29472/30848)
Epoch: 144 | Batch_idx: 250 |  Loss: (0.1304) |  Loss2: (0.0000) | Acc: (95.00%) (30686/32128)
Epoch: 144 | Batch_idx: 260 |  Loss: (0.1303) |  Loss2: (0.0000) | Acc: (95.00%) (31906/33408)
Epoch: 144 | Batch_idx: 270 |  Loss: (0.1304) |  Loss2: (0.0000) | Acc: (95.00%) (33121/34688)
Epoch: 144 | Batch_idx: 280 |  Loss: (0.1308) |  Loss2: (0.0000) | Acc: (95.00%) (34332/35968)
Epoch: 144 | Batch_idx: 290 |  Loss: (0.1312) |  Loss2: (0.0000) | Acc: (95.00%) (35543/37248)
Epoch: 144 | Batch_idx: 300 |  Loss: (0.1310) |  Loss2: (0.0000) | Acc: (95.00%) (36767/38528)
Epoch: 144 | Batch_idx: 310 |  Loss: (0.1305) |  Loss2: (0.0000) | Acc: (95.00%) (38000/39808)
Epoch: 144 | Batch_idx: 320 |  Loss: (0.1308) |  Loss2: (0.0000) | Acc: (95.00%) (39214/41088)
Epoch: 144 | Batch_idx: 330 |  Loss: (0.1315) |  Loss2: (0.0000) | Acc: (95.00%) (40433/42368)
Epoch: 144 | Batch_idx: 340 |  Loss: (0.1315) |  Loss2: (0.0000) | Acc: (95.00%) (41651/43648)
Epoch: 144 | Batch_idx: 350 |  Loss: (0.1312) |  Loss2: (0.0000) | Acc: (95.00%) (42881/44928)
Epoch: 144 | Batch_idx: 360 |  Loss: (0.1321) |  Loss2: (0.0000) | Acc: (95.00%) (44085/46208)
Epoch: 144 | Batch_idx: 370 |  Loss: (0.1320) |  Loss2: (0.0000) | Acc: (95.00%) (45302/47488)
Epoch: 144 | Batch_idx: 380 |  Loss: (0.1331) |  Loss2: (0.0000) | Acc: (95.00%) (46509/48768)
Epoch: 144 | Batch_idx: 390 |  Loss: (0.1326) |  Loss2: (0.0000) | Acc: (95.00%) (47694/50000)
# TEST : Loss: (0.5002) | Acc: (86.00%) (8622/10000)
percent tensor([0.5335, 0.4665], device='cuda:0')
percent tensor([0.5072, 0.4928], device='cuda:0')
percent tensor([0.5257, 0.4743], device='cuda:0')
percent tensor([0.6220, 0.3780], device='cuda:0')
percent tensor([0.6317, 0.3683], device='cuda:0')
percent tensor([0.6568, 0.3432], device='cuda:0')
percent tensor([0.7033, 0.2967], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 145 | Batch_idx: 0 |  Loss: (0.1236) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 145 | Batch_idx: 10 |  Loss: (0.1306) |  Loss2: (0.0000) | Acc: (95.00%) (1345/1408)
Epoch: 145 | Batch_idx: 20 |  Loss: (0.1352) |  Loss2: (0.0000) | Acc: (95.00%) (2576/2688)
Epoch: 145 | Batch_idx: 30 |  Loss: (0.1370) |  Loss2: (0.0000) | Acc: (95.00%) (3785/3968)
Epoch: 145 | Batch_idx: 40 |  Loss: (0.1406) |  Loss2: (0.0000) | Acc: (95.00%) (5002/5248)
Epoch: 145 | Batch_idx: 50 |  Loss: (0.1410) |  Loss2: (0.0000) | Acc: (95.00%) (6214/6528)
Epoch: 145 | Batch_idx: 60 |  Loss: (0.1429) |  Loss2: (0.0000) | Acc: (95.00%) (7429/7808)
Epoch: 145 | Batch_idx: 70 |  Loss: (0.1454) |  Loss2: (0.0000) | Acc: (95.00%) (8634/9088)
Epoch: 145 | Batch_idx: 80 |  Loss: (0.1483) |  Loss2: (0.0000) | Acc: (94.00%) (9833/10368)
Epoch: 145 | Batch_idx: 90 |  Loss: (0.1473) |  Loss2: (0.0000) | Acc: (94.00%) (11056/11648)
Epoch: 145 | Batch_idx: 100 |  Loss: (0.1464) |  Loss2: (0.0000) | Acc: (94.00%) (12278/12928)
Epoch: 145 | Batch_idx: 110 |  Loss: (0.1456) |  Loss2: (0.0000) | Acc: (95.00%) (13500/14208)
Epoch: 145 | Batch_idx: 120 |  Loss: (0.1452) |  Loss2: (0.0000) | Acc: (94.00%) (14712/15488)
Epoch: 145 | Batch_idx: 130 |  Loss: (0.1453) |  Loss2: (0.0000) | Acc: (95.00%) (15932/16768)
Epoch: 145 | Batch_idx: 140 |  Loss: (0.1465) |  Loss2: (0.0000) | Acc: (94.00%) (17143/18048)
Epoch: 145 | Batch_idx: 150 |  Loss: (0.1466) |  Loss2: (0.0000) | Acc: (94.00%) (18357/19328)
Epoch: 145 | Batch_idx: 160 |  Loss: (0.1457) |  Loss2: (0.0000) | Acc: (95.00%) (19580/20608)
Epoch: 145 | Batch_idx: 170 |  Loss: (0.1454) |  Loss2: (0.0000) | Acc: (95.00%) (20799/21888)
Epoch: 145 | Batch_idx: 180 |  Loss: (0.1455) |  Loss2: (0.0000) | Acc: (94.00%) (22007/23168)
Epoch: 145 | Batch_idx: 190 |  Loss: (0.1459) |  Loss2: (0.0000) | Acc: (94.00%) (23217/24448)
Epoch: 145 | Batch_idx: 200 |  Loss: (0.1462) |  Loss2: (0.0000) | Acc: (94.00%) (24431/25728)
Epoch: 145 | Batch_idx: 210 |  Loss: (0.1455) |  Loss2: (0.0000) | Acc: (94.00%) (25649/27008)
Epoch: 145 | Batch_idx: 220 |  Loss: (0.1462) |  Loss2: (0.0000) | Acc: (94.00%) (26854/28288)
Epoch: 145 | Batch_idx: 230 |  Loss: (0.1466) |  Loss2: (0.0000) | Acc: (94.00%) (28066/29568)
Epoch: 145 | Batch_idx: 240 |  Loss: (0.1452) |  Loss2: (0.0000) | Acc: (94.00%) (29301/30848)
Epoch: 145 | Batch_idx: 250 |  Loss: (0.1445) |  Loss2: (0.0000) | Acc: (95.00%) (30527/32128)
Epoch: 145 | Batch_idx: 260 |  Loss: (0.1442) |  Loss2: (0.0000) | Acc: (95.00%) (31747/33408)
Epoch: 145 | Batch_idx: 270 |  Loss: (0.1437) |  Loss2: (0.0000) | Acc: (95.00%) (32966/34688)
Epoch: 145 | Batch_idx: 280 |  Loss: (0.1434) |  Loss2: (0.0000) | Acc: (95.00%) (34185/35968)
Epoch: 145 | Batch_idx: 290 |  Loss: (0.1438) |  Loss2: (0.0000) | Acc: (95.00%) (35404/37248)
Epoch: 145 | Batch_idx: 300 |  Loss: (0.1433) |  Loss2: (0.0000) | Acc: (95.00%) (36626/38528)
Epoch: 145 | Batch_idx: 310 |  Loss: (0.1429) |  Loss2: (0.0000) | Acc: (95.00%) (37837/39808)
Epoch: 145 | Batch_idx: 320 |  Loss: (0.1429) |  Loss2: (0.0000) | Acc: (95.00%) (39054/41088)
Epoch: 145 | Batch_idx: 330 |  Loss: (0.1423) |  Loss2: (0.0000) | Acc: (95.00%) (40277/42368)
Epoch: 145 | Batch_idx: 340 |  Loss: (0.1424) |  Loss2: (0.0000) | Acc: (95.00%) (41497/43648)
Epoch: 145 | Batch_idx: 350 |  Loss: (0.1416) |  Loss2: (0.0000) | Acc: (95.00%) (42726/44928)
Epoch: 145 | Batch_idx: 360 |  Loss: (0.1419) |  Loss2: (0.0000) | Acc: (95.00%) (43947/46208)
Epoch: 145 | Batch_idx: 370 |  Loss: (0.1418) |  Loss2: (0.0000) | Acc: (95.00%) (45168/47488)
Epoch: 145 | Batch_idx: 380 |  Loss: (0.1418) |  Loss2: (0.0000) | Acc: (95.00%) (46386/48768)
Epoch: 145 | Batch_idx: 390 |  Loss: (0.1411) |  Loss2: (0.0000) | Acc: (95.00%) (47565/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_145.pth.tar'
# TEST : Loss: (0.4086) | Acc: (88.00%) (8837/10000)
percent tensor([0.5336, 0.4664], device='cuda:0')
percent tensor([0.5081, 0.4919], device='cuda:0')
percent tensor([0.5294, 0.4706], device='cuda:0')
percent tensor([0.6170, 0.3830], device='cuda:0')
percent tensor([0.6443, 0.3557], device='cuda:0')
percent tensor([0.6660, 0.3340], device='cuda:0')
percent tensor([0.6735, 0.3265], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
