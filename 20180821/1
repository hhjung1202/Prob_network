Files already downloaded and verified
USE 1 GPUs!
Epoch: 0 | Batch_idx: 0 |  Loss: (2.3144) |  Loss2: (0.0000) | Acc: (10.00%) (14/128)
Epoch: 0 | Batch_idx: 10 |  Loss: (2.3042) |  Loss2: (0.0000) | Acc: (10.00%) (154/1408)
Epoch: 0 | Batch_idx: 20 |  Loss: (2.3009) |  Loss2: (0.0000) | Acc: (11.00%) (300/2688)
Epoch: 0 | Batch_idx: 30 |  Loss: (2.2941) |  Loss2: (0.0000) | Acc: (12.00%) (492/3968)
Epoch: 0 | Batch_idx: 40 |  Loss: (2.2883) |  Loss2: (0.0000) | Acc: (13.00%) (730/5248)
Epoch: 0 | Batch_idx: 50 |  Loss: (2.2817) |  Loss2: (0.0000) | Acc: (15.00%) (986/6528)
Epoch: 0 | Batch_idx: 60 |  Loss: (2.2753) |  Loss2: (0.0000) | Acc: (15.00%) (1243/7808)
Epoch: 0 | Batch_idx: 70 |  Loss: (2.2679) |  Loss2: (0.0000) | Acc: (16.00%) (1522/9088)
Epoch: 0 | Batch_idx: 80 |  Loss: (2.2604) |  Loss2: (0.0000) | Acc: (17.00%) (1794/10368)
Epoch: 0 | Batch_idx: 90 |  Loss: (2.2521) |  Loss2: (0.0000) | Acc: (17.00%) (2065/11648)
Epoch: 0 | Batch_idx: 100 |  Loss: (2.2425) |  Loss2: (0.0000) | Acc: (18.00%) (2361/12928)
Epoch: 0 | Batch_idx: 110 |  Loss: (2.2324) |  Loss2: (0.0000) | Acc: (18.00%) (2652/14208)
Epoch: 0 | Batch_idx: 120 |  Loss: (2.2217) |  Loss2: (0.0000) | Acc: (19.00%) (2967/15488)
Epoch: 0 | Batch_idx: 130 |  Loss: (2.2113) |  Loss2: (0.0000) | Acc: (19.00%) (3254/16768)
Epoch: 0 | Batch_idx: 140 |  Loss: (2.2029) |  Loss2: (0.0000) | Acc: (19.00%) (3562/18048)
Epoch: 0 | Batch_idx: 150 |  Loss: (2.1940) |  Loss2: (0.0000) | Acc: (19.00%) (3864/19328)
Epoch: 0 | Batch_idx: 160 |  Loss: (2.1850) |  Loss2: (0.0000) | Acc: (20.00%) (4200/20608)
Epoch: 0 | Batch_idx: 170 |  Loss: (2.1774) |  Loss2: (0.0000) | Acc: (20.00%) (4527/21888)
Epoch: 0 | Batch_idx: 180 |  Loss: (2.1685) |  Loss2: (0.0000) | Acc: (21.00%) (4893/23168)
Epoch: 0 | Batch_idx: 190 |  Loss: (2.1606) |  Loss2: (0.0000) | Acc: (21.00%) (5233/24448)
Epoch: 0 | Batch_idx: 200 |  Loss: (2.1526) |  Loss2: (0.0000) | Acc: (21.00%) (5568/25728)
Epoch: 0 | Batch_idx: 210 |  Loss: (2.1451) |  Loss2: (0.0000) | Acc: (21.00%) (5888/27008)
Epoch: 0 | Batch_idx: 220 |  Loss: (2.1367) |  Loss2: (0.0000) | Acc: (22.00%) (6237/28288)
Epoch: 0 | Batch_idx: 230 |  Loss: (2.1289) |  Loss2: (0.0000) | Acc: (22.00%) (6605/29568)
Epoch: 0 | Batch_idx: 240 |  Loss: (2.1227) |  Loss2: (0.0000) | Acc: (22.00%) (6961/30848)
Epoch: 0 | Batch_idx: 250 |  Loss: (2.1150) |  Loss2: (0.0000) | Acc: (22.00%) (7326/32128)
Epoch: 0 | Batch_idx: 260 |  Loss: (2.1089) |  Loss2: (0.0000) | Acc: (23.00%) (7688/33408)
Epoch: 0 | Batch_idx: 270 |  Loss: (2.1029) |  Loss2: (0.0000) | Acc: (23.00%) (8033/34688)
Epoch: 0 | Batch_idx: 280 |  Loss: (2.0965) |  Loss2: (0.0000) | Acc: (23.00%) (8420/35968)
Epoch: 0 | Batch_idx: 290 |  Loss: (2.0893) |  Loss2: (0.0000) | Acc: (23.00%) (8814/37248)
Epoch: 0 | Batch_idx: 300 |  Loss: (2.0826) |  Loss2: (0.0000) | Acc: (23.00%) (9234/38528)
Epoch: 0 | Batch_idx: 310 |  Loss: (2.0768) |  Loss2: (0.0000) | Acc: (24.00%) (9592/39808)
Epoch: 0 | Batch_idx: 320 |  Loss: (2.0717) |  Loss2: (0.0000) | Acc: (24.00%) (9965/41088)
Epoch: 0 | Batch_idx: 330 |  Loss: (2.0657) |  Loss2: (0.0000) | Acc: (24.00%) (10366/42368)
Epoch: 0 | Batch_idx: 340 |  Loss: (2.0600) |  Loss2: (0.0000) | Acc: (24.00%) (10766/43648)
Epoch: 0 | Batch_idx: 350 |  Loss: (2.0546) |  Loss2: (0.0000) | Acc: (24.00%) (11170/44928)
Epoch: 0 | Batch_idx: 360 |  Loss: (2.0493) |  Loss2: (0.0000) | Acc: (25.00%) (11562/46208)
Epoch: 0 | Batch_idx: 370 |  Loss: (2.0444) |  Loss2: (0.0000) | Acc: (25.00%) (11971/47488)
Epoch: 0 | Batch_idx: 380 |  Loss: (2.0388) |  Loss2: (0.0000) | Acc: (25.00%) (12411/48768)
Epoch: 0 | Batch_idx: 390 |  Loss: (2.0342) |  Loss2: (0.0000) | Acc: (25.00%) (12790/50000)
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type ResNet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type BasicBlock. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type _Gate. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_000.pth.tar'
# TEST : Loss: (1.8129) | Acc: (32.00%) (3253/10000)
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(166.9456, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(770.1288, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(766.8491, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1528.5629, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(520.1837, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2168.2239, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4339.7964, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1445.2157, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6142.4233, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12287.7715, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4086.3015, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(17370.7070, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 1 | Batch_idx: 0 |  Loss: (1.7972) |  Loss2: (0.0000) | Acc: (33.00%) (43/128)
Epoch: 1 | Batch_idx: 10 |  Loss: (1.8168) |  Loss2: (0.0000) | Acc: (32.00%) (462/1408)
Epoch: 1 | Batch_idx: 20 |  Loss: (1.8185) |  Loss2: (0.0000) | Acc: (32.00%) (882/2688)
Epoch: 1 | Batch_idx: 30 |  Loss: (1.8196) |  Loss2: (0.0000) | Acc: (33.00%) (1318/3968)
Epoch: 1 | Batch_idx: 40 |  Loss: (1.8134) |  Loss2: (0.0000) | Acc: (33.00%) (1738/5248)
Epoch: 1 | Batch_idx: 50 |  Loss: (1.8094) |  Loss2: (0.0000) | Acc: (33.00%) (2195/6528)
Epoch: 1 | Batch_idx: 60 |  Loss: (1.8085) |  Loss2: (0.0000) | Acc: (33.00%) (2624/7808)
Epoch: 1 | Batch_idx: 70 |  Loss: (1.8035) |  Loss2: (0.0000) | Acc: (33.00%) (3064/9088)
Epoch: 1 | Batch_idx: 80 |  Loss: (1.8002) |  Loss2: (0.0000) | Acc: (33.00%) (3522/10368)
Epoch: 1 | Batch_idx: 90 |  Loss: (1.7962) |  Loss2: (0.0000) | Acc: (34.00%) (4002/11648)
Epoch: 1 | Batch_idx: 100 |  Loss: (1.7951) |  Loss2: (0.0000) | Acc: (34.00%) (4449/12928)
Epoch: 1 | Batch_idx: 110 |  Loss: (1.7916) |  Loss2: (0.0000) | Acc: (34.00%) (4878/14208)
Epoch: 1 | Batch_idx: 120 |  Loss: (1.7908) |  Loss2: (0.0000) | Acc: (34.00%) (5303/15488)
Epoch: 1 | Batch_idx: 130 |  Loss: (1.7870) |  Loss2: (0.0000) | Acc: (34.00%) (5787/16768)
Epoch: 1 | Batch_idx: 140 |  Loss: (1.7839) |  Loss2: (0.0000) | Acc: (34.00%) (6241/18048)
Epoch: 1 | Batch_idx: 150 |  Loss: (1.7805) |  Loss2: (0.0000) | Acc: (34.00%) (6692/19328)
Epoch: 1 | Batch_idx: 160 |  Loss: (1.7780) |  Loss2: (0.0000) | Acc: (34.00%) (7151/20608)
Epoch: 1 | Batch_idx: 170 |  Loss: (1.7763) |  Loss2: (0.0000) | Acc: (34.00%) (7610/21888)
Epoch: 1 | Batch_idx: 180 |  Loss: (1.7727) |  Loss2: (0.0000) | Acc: (34.00%) (8091/23168)
Epoch: 1 | Batch_idx: 190 |  Loss: (1.7700) |  Loss2: (0.0000) | Acc: (35.00%) (8562/24448)
Epoch: 1 | Batch_idx: 200 |  Loss: (1.7663) |  Loss2: (0.0000) | Acc: (35.00%) (9028/25728)
Epoch: 1 | Batch_idx: 210 |  Loss: (1.7632) |  Loss2: (0.0000) | Acc: (35.00%) (9501/27008)
Epoch: 1 | Batch_idx: 220 |  Loss: (1.7611) |  Loss2: (0.0000) | Acc: (35.00%) (9982/28288)
Epoch: 1 | Batch_idx: 230 |  Loss: (1.7576) |  Loss2: (0.0000) | Acc: (35.00%) (10456/29568)
Epoch: 1 | Batch_idx: 240 |  Loss: (1.7549) |  Loss2: (0.0000) | Acc: (35.00%) (10905/30848)
Epoch: 1 | Batch_idx: 250 |  Loss: (1.7539) |  Loss2: (0.0000) | Acc: (35.00%) (11375/32128)
Epoch: 1 | Batch_idx: 260 |  Loss: (1.7528) |  Loss2: (0.0000) | Acc: (35.00%) (11804/33408)
Epoch: 1 | Batch_idx: 270 |  Loss: (1.7509) |  Loss2: (0.0000) | Acc: (35.00%) (12276/34688)
Epoch: 1 | Batch_idx: 280 |  Loss: (1.7478) |  Loss2: (0.0000) | Acc: (35.00%) (12748/35968)
Epoch: 1 | Batch_idx: 290 |  Loss: (1.7451) |  Loss2: (0.0000) | Acc: (35.00%) (13228/37248)
Epoch: 1 | Batch_idx: 300 |  Loss: (1.7423) |  Loss2: (0.0000) | Acc: (35.00%) (13706/38528)
Epoch: 1 | Batch_idx: 310 |  Loss: (1.7390) |  Loss2: (0.0000) | Acc: (35.00%) (14210/39808)
Epoch: 1 | Batch_idx: 320 |  Loss: (1.7365) |  Loss2: (0.0000) | Acc: (35.00%) (14715/41088)
Epoch: 1 | Batch_idx: 330 |  Loss: (1.7339) |  Loss2: (0.0000) | Acc: (35.00%) (15198/42368)
Epoch: 1 | Batch_idx: 340 |  Loss: (1.7319) |  Loss2: (0.0000) | Acc: (35.00%) (15668/43648)
Epoch: 1 | Batch_idx: 350 |  Loss: (1.7296) |  Loss2: (0.0000) | Acc: (35.00%) (16163/44928)
Epoch: 1 | Batch_idx: 360 |  Loss: (1.7259) |  Loss2: (0.0000) | Acc: (36.00%) (16696/46208)
Epoch: 1 | Batch_idx: 370 |  Loss: (1.7238) |  Loss2: (0.0000) | Acc: (36.00%) (17189/47488)
Epoch: 1 | Batch_idx: 380 |  Loss: (1.7213) |  Loss2: (0.0000) | Acc: (36.00%) (17687/48768)
Epoch: 1 | Batch_idx: 390 |  Loss: (1.7193) |  Loss2: (0.0000) | Acc: (36.00%) (18168/50000)
# TEST : Loss: (1.6587) | Acc: (37.00%) (3741/10000)
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
Epoch: 2 | Batch_idx: 0 |  Loss: (1.5636) |  Loss2: (0.0000) | Acc: (40.00%) (52/128)
Epoch: 2 | Batch_idx: 10 |  Loss: (1.6122) |  Loss2: (0.0000) | Acc: (39.00%) (557/1408)
Epoch: 2 | Batch_idx: 20 |  Loss: (1.6212) |  Loss2: (0.0000) | Acc: (38.00%) (1026/2688)
Epoch: 2 | Batch_idx: 30 |  Loss: (1.6207) |  Loss2: (0.0000) | Acc: (38.00%) (1511/3968)
Epoch: 2 | Batch_idx: 40 |  Loss: (1.6151) |  Loss2: (0.0000) | Acc: (38.00%) (2038/5248)
Epoch: 2 | Batch_idx: 50 |  Loss: (1.6171) |  Loss2: (0.0000) | Acc: (38.00%) (2544/6528)
Epoch: 2 | Batch_idx: 60 |  Loss: (1.6164) |  Loss2: (0.0000) | Acc: (39.00%) (3050/7808)
Epoch: 2 | Batch_idx: 70 |  Loss: (1.6105) |  Loss2: (0.0000) | Acc: (39.00%) (3588/9088)
Epoch: 2 | Batch_idx: 80 |  Loss: (1.6065) |  Loss2: (0.0000) | Acc: (39.00%) (4111/10368)
Epoch: 2 | Batch_idx: 90 |  Loss: (1.6057) |  Loss2: (0.0000) | Acc: (39.00%) (4629/11648)
Epoch: 2 | Batch_idx: 100 |  Loss: (1.6062) |  Loss2: (0.0000) | Acc: (39.00%) (5138/12928)
Epoch: 2 | Batch_idx: 110 |  Loss: (1.6027) |  Loss2: (0.0000) | Acc: (40.00%) (5693/14208)
Epoch: 2 | Batch_idx: 120 |  Loss: (1.6004) |  Loss2: (0.0000) | Acc: (40.00%) (6251/15488)
Epoch: 2 | Batch_idx: 130 |  Loss: (1.5995) |  Loss2: (0.0000) | Acc: (40.00%) (6781/16768)
Epoch: 2 | Batch_idx: 140 |  Loss: (1.5982) |  Loss2: (0.0000) | Acc: (40.00%) (7296/18048)
Epoch: 2 | Batch_idx: 150 |  Loss: (1.5961) |  Loss2: (0.0000) | Acc: (40.00%) (7829/19328)
Epoch: 2 | Batch_idx: 160 |  Loss: (1.5944) |  Loss2: (0.0000) | Acc: (40.00%) (8389/20608)
Epoch: 2 | Batch_idx: 170 |  Loss: (1.5930) |  Loss2: (0.0000) | Acc: (40.00%) (8934/21888)
Epoch: 2 | Batch_idx: 180 |  Loss: (1.5888) |  Loss2: (0.0000) | Acc: (40.00%) (9496/23168)
Epoch: 2 | Batch_idx: 190 |  Loss: (1.5856) |  Loss2: (0.0000) | Acc: (41.00%) (10051/24448)
Epoch: 2 | Batch_idx: 200 |  Loss: (1.5826) |  Loss2: (0.0000) | Acc: (41.00%) (10629/25728)
Epoch: 2 | Batch_idx: 210 |  Loss: (1.5814) |  Loss2: (0.0000) | Acc: (41.00%) (11187/27008)
Epoch: 2 | Batch_idx: 220 |  Loss: (1.5802) |  Loss2: (0.0000) | Acc: (41.00%) (11720/28288)
Epoch: 2 | Batch_idx: 230 |  Loss: (1.5785) |  Loss2: (0.0000) | Acc: (41.00%) (12289/29568)
Epoch: 2 | Batch_idx: 240 |  Loss: (1.5770) |  Loss2: (0.0000) | Acc: (41.00%) (12815/30848)
Epoch: 2 | Batch_idx: 250 |  Loss: (1.5741) |  Loss2: (0.0000) | Acc: (41.00%) (13393/32128)
Epoch: 2 | Batch_idx: 260 |  Loss: (1.5706) |  Loss2: (0.0000) | Acc: (41.00%) (14003/33408)
Epoch: 2 | Batch_idx: 270 |  Loss: (1.5671) |  Loss2: (0.0000) | Acc: (42.00%) (14583/34688)
Epoch: 2 | Batch_idx: 280 |  Loss: (1.5645) |  Loss2: (0.0000) | Acc: (42.00%) (15161/35968)
Epoch: 2 | Batch_idx: 290 |  Loss: (1.5626) |  Loss2: (0.0000) | Acc: (42.00%) (15723/37248)
Epoch: 2 | Batch_idx: 300 |  Loss: (1.5622) |  Loss2: (0.0000) | Acc: (42.00%) (16267/38528)
Epoch: 2 | Batch_idx: 310 |  Loss: (1.5604) |  Loss2: (0.0000) | Acc: (42.00%) (16833/39808)
Epoch: 2 | Batch_idx: 320 |  Loss: (1.5567) |  Loss2: (0.0000) | Acc: (42.00%) (17455/41088)
Epoch: 2 | Batch_idx: 330 |  Loss: (1.5545) |  Loss2: (0.0000) | Acc: (42.00%) (18055/42368)
Epoch: 2 | Batch_idx: 340 |  Loss: (1.5518) |  Loss2: (0.0000) | Acc: (42.00%) (18646/43648)
Epoch: 2 | Batch_idx: 350 |  Loss: (1.5489) |  Loss2: (0.0000) | Acc: (42.00%) (19246/44928)
Epoch: 2 | Batch_idx: 360 |  Loss: (1.5465) |  Loss2: (0.0000) | Acc: (42.00%) (19848/46208)
Epoch: 2 | Batch_idx: 370 |  Loss: (1.5441) |  Loss2: (0.0000) | Acc: (43.00%) (20465/47488)
Epoch: 2 | Batch_idx: 380 |  Loss: (1.5416) |  Loss2: (0.0000) | Acc: (43.00%) (21044/48768)
Epoch: 2 | Batch_idx: 390 |  Loss: (1.5393) |  Loss2: (0.0000) | Acc: (43.00%) (21613/50000)
# TEST : Loss: (1.5044) | Acc: (43.00%) (4324/10000)
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
Epoch: 3 | Batch_idx: 0 |  Loss: (1.4334) |  Loss2: (0.0000) | Acc: (43.00%) (56/128)
Epoch: 3 | Batch_idx: 10 |  Loss: (1.4589) |  Loss2: (0.0000) | Acc: (48.00%) (680/1408)
Epoch: 3 | Batch_idx: 20 |  Loss: (1.4486) |  Loss2: (0.0000) | Acc: (48.00%) (1302/2688)
Epoch: 3 | Batch_idx: 30 |  Loss: (1.4337) |  Loss2: (0.0000) | Acc: (48.00%) (1915/3968)
Epoch: 3 | Batch_idx: 40 |  Loss: (1.4303) |  Loss2: (0.0000) | Acc: (48.00%) (2521/5248)
Epoch: 3 | Batch_idx: 50 |  Loss: (1.4347) |  Loss2: (0.0000) | Acc: (47.00%) (3123/6528)
Epoch: 3 | Batch_idx: 60 |  Loss: (1.4372) |  Loss2: (0.0000) | Acc: (47.00%) (3738/7808)
Epoch: 3 | Batch_idx: 70 |  Loss: (1.4429) |  Loss2: (0.0000) | Acc: (47.00%) (4334/9088)
Epoch: 3 | Batch_idx: 80 |  Loss: (1.4400) |  Loss2: (0.0000) | Acc: (47.00%) (4951/10368)
Epoch: 3 | Batch_idx: 90 |  Loss: (1.4354) |  Loss2: (0.0000) | Acc: (47.00%) (5565/11648)
Epoch: 3 | Batch_idx: 100 |  Loss: (1.4323) |  Loss2: (0.0000) | Acc: (47.00%) (6195/12928)
Epoch: 3 | Batch_idx: 110 |  Loss: (1.4324) |  Loss2: (0.0000) | Acc: (47.00%) (6808/14208)
Epoch: 3 | Batch_idx: 120 |  Loss: (1.4302) |  Loss2: (0.0000) | Acc: (47.00%) (7414/15488)
Epoch: 3 | Batch_idx: 130 |  Loss: (1.4278) |  Loss2: (0.0000) | Acc: (47.00%) (8045/16768)
Epoch: 3 | Batch_idx: 140 |  Loss: (1.4261) |  Loss2: (0.0000) | Acc: (48.00%) (8687/18048)
Epoch: 3 | Batch_idx: 150 |  Loss: (1.4197) |  Loss2: (0.0000) | Acc: (48.00%) (9330/19328)
Epoch: 3 | Batch_idx: 160 |  Loss: (1.4181) |  Loss2: (0.0000) | Acc: (48.00%) (9957/20608)
Epoch: 3 | Batch_idx: 170 |  Loss: (1.4170) |  Loss2: (0.0000) | Acc: (48.00%) (10572/21888)
Epoch: 3 | Batch_idx: 180 |  Loss: (1.4156) |  Loss2: (0.0000) | Acc: (48.00%) (11229/23168)
Epoch: 3 | Batch_idx: 190 |  Loss: (1.4156) |  Loss2: (0.0000) | Acc: (48.00%) (11850/24448)
Epoch: 3 | Batch_idx: 200 |  Loss: (1.4130) |  Loss2: (0.0000) | Acc: (48.00%) (12474/25728)
Epoch: 3 | Batch_idx: 210 |  Loss: (1.4105) |  Loss2: (0.0000) | Acc: (48.00%) (13111/27008)
Epoch: 3 | Batch_idx: 220 |  Loss: (1.4066) |  Loss2: (0.0000) | Acc: (48.00%) (13810/28288)
Epoch: 3 | Batch_idx: 230 |  Loss: (1.4045) |  Loss2: (0.0000) | Acc: (48.00%) (14471/29568)
Epoch: 3 | Batch_idx: 240 |  Loss: (1.4047) |  Loss2: (0.0000) | Acc: (48.00%) (15091/30848)
Epoch: 3 | Batch_idx: 250 |  Loss: (1.4025) |  Loss2: (0.0000) | Acc: (48.00%) (15736/32128)
Epoch: 3 | Batch_idx: 260 |  Loss: (1.3998) |  Loss2: (0.0000) | Acc: (49.00%) (16423/33408)
Epoch: 3 | Batch_idx: 270 |  Loss: (1.3978) |  Loss2: (0.0000) | Acc: (49.00%) (17060/34688)
Epoch: 3 | Batch_idx: 280 |  Loss: (1.3973) |  Loss2: (0.0000) | Acc: (49.00%) (17697/35968)
Epoch: 3 | Batch_idx: 290 |  Loss: (1.3953) |  Loss2: (0.0000) | Acc: (49.00%) (18348/37248)
Epoch: 3 | Batch_idx: 300 |  Loss: (1.3948) |  Loss2: (0.0000) | Acc: (49.00%) (19000/38528)
Epoch: 3 | Batch_idx: 310 |  Loss: (1.3934) |  Loss2: (0.0000) | Acc: (49.00%) (19625/39808)
Epoch: 3 | Batch_idx: 320 |  Loss: (1.3919) |  Loss2: (0.0000) | Acc: (49.00%) (20284/41088)
Epoch: 3 | Batch_idx: 330 |  Loss: (1.3916) |  Loss2: (0.0000) | Acc: (49.00%) (20926/42368)
Epoch: 3 | Batch_idx: 340 |  Loss: (1.3900) |  Loss2: (0.0000) | Acc: (49.00%) (21576/43648)
Epoch: 3 | Batch_idx: 350 |  Loss: (1.3892) |  Loss2: (0.0000) | Acc: (49.00%) (22214/44928)
Epoch: 3 | Batch_idx: 360 |  Loss: (1.3876) |  Loss2: (0.0000) | Acc: (49.00%) (22875/46208)
Epoch: 3 | Batch_idx: 370 |  Loss: (1.3876) |  Loss2: (0.0000) | Acc: (49.00%) (23516/47488)
Epoch: 3 | Batch_idx: 380 |  Loss: (1.3853) |  Loss2: (0.0000) | Acc: (49.00%) (24184/48768)
Epoch: 3 | Batch_idx: 390 |  Loss: (1.3828) |  Loss2: (0.0000) | Acc: (49.00%) (24837/50000)
# TEST : Loss: (1.3552) | Acc: (50.00%) (5016/10000)
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
Epoch: 4 | Batch_idx: 0 |  Loss: (1.4460) |  Loss2: (0.0000) | Acc: (49.00%) (63/128)
Epoch: 4 | Batch_idx: 10 |  Loss: (1.3107) |  Loss2: (0.0000) | Acc: (52.00%) (736/1408)
Epoch: 4 | Batch_idx: 20 |  Loss: (1.3122) |  Loss2: (0.0000) | Acc: (52.00%) (1404/2688)
Epoch: 4 | Batch_idx: 30 |  Loss: (1.3083) |  Loss2: (0.0000) | Acc: (52.00%) (2086/3968)
Epoch: 4 | Batch_idx: 40 |  Loss: (1.3015) |  Loss2: (0.0000) | Acc: (53.00%) (2814/5248)
Epoch: 4 | Batch_idx: 50 |  Loss: (1.3038) |  Loss2: (0.0000) | Acc: (53.00%) (3481/6528)
Epoch: 4 | Batch_idx: 60 |  Loss: (1.2993) |  Loss2: (0.0000) | Acc: (53.00%) (4164/7808)
Epoch: 4 | Batch_idx: 70 |  Loss: (1.2957) |  Loss2: (0.0000) | Acc: (53.00%) (4866/9088)
Epoch: 4 | Batch_idx: 80 |  Loss: (1.2953) |  Loss2: (0.0000) | Acc: (53.00%) (5560/10368)
Epoch: 4 | Batch_idx: 90 |  Loss: (1.3014) |  Loss2: (0.0000) | Acc: (53.00%) (6218/11648)
Epoch: 4 | Batch_idx: 100 |  Loss: (1.2974) |  Loss2: (0.0000) | Acc: (53.00%) (6913/12928)
Epoch: 4 | Batch_idx: 110 |  Loss: (1.2973) |  Loss2: (0.0000) | Acc: (53.00%) (7583/14208)
Epoch: 4 | Batch_idx: 120 |  Loss: (1.2970) |  Loss2: (0.0000) | Acc: (53.00%) (8255/15488)
Epoch: 4 | Batch_idx: 130 |  Loss: (1.2961) |  Loss2: (0.0000) | Acc: (53.00%) (8949/16768)
Epoch: 4 | Batch_idx: 140 |  Loss: (1.2929) |  Loss2: (0.0000) | Acc: (53.00%) (9647/18048)
Epoch: 4 | Batch_idx: 150 |  Loss: (1.2929) |  Loss2: (0.0000) | Acc: (53.00%) (10352/19328)
Epoch: 4 | Batch_idx: 160 |  Loss: (1.2902) |  Loss2: (0.0000) | Acc: (53.00%) (11051/20608)
Epoch: 4 | Batch_idx: 170 |  Loss: (1.2885) |  Loss2: (0.0000) | Acc: (53.00%) (11753/21888)
Epoch: 4 | Batch_idx: 180 |  Loss: (1.2865) |  Loss2: (0.0000) | Acc: (53.00%) (12460/23168)
Epoch: 4 | Batch_idx: 190 |  Loss: (1.2845) |  Loss2: (0.0000) | Acc: (53.00%) (13171/24448)
Epoch: 4 | Batch_idx: 200 |  Loss: (1.2861) |  Loss2: (0.0000) | Acc: (53.00%) (13832/25728)
Epoch: 4 | Batch_idx: 210 |  Loss: (1.2847) |  Loss2: (0.0000) | Acc: (53.00%) (14536/27008)
Epoch: 4 | Batch_idx: 220 |  Loss: (1.2859) |  Loss2: (0.0000) | Acc: (53.00%) (15196/28288)
Epoch: 4 | Batch_idx: 230 |  Loss: (1.2872) |  Loss2: (0.0000) | Acc: (53.00%) (15860/29568)
Epoch: 4 | Batch_idx: 240 |  Loss: (1.2835) |  Loss2: (0.0000) | Acc: (53.00%) (16603/30848)
Epoch: 4 | Batch_idx: 250 |  Loss: (1.2820) |  Loss2: (0.0000) | Acc: (53.00%) (17302/32128)
Epoch: 4 | Batch_idx: 260 |  Loss: (1.2796) |  Loss2: (0.0000) | Acc: (53.00%) (18029/33408)
Epoch: 4 | Batch_idx: 270 |  Loss: (1.2773) |  Loss2: (0.0000) | Acc: (53.00%) (18731/34688)
Epoch: 4 | Batch_idx: 280 |  Loss: (1.2763) |  Loss2: (0.0000) | Acc: (54.00%) (19444/35968)
Epoch: 4 | Batch_idx: 290 |  Loss: (1.2740) |  Loss2: (0.0000) | Acc: (54.00%) (20158/37248)
Epoch: 4 | Batch_idx: 300 |  Loss: (1.2738) |  Loss2: (0.0000) | Acc: (54.00%) (20865/38528)
Epoch: 4 | Batch_idx: 310 |  Loss: (1.2722) |  Loss2: (0.0000) | Acc: (54.00%) (21587/39808)
Epoch: 4 | Batch_idx: 320 |  Loss: (1.2720) |  Loss2: (0.0000) | Acc: (54.00%) (22304/41088)
Epoch: 4 | Batch_idx: 330 |  Loss: (1.2717) |  Loss2: (0.0000) | Acc: (54.00%) (23019/42368)
Epoch: 4 | Batch_idx: 340 |  Loss: (1.2712) |  Loss2: (0.0000) | Acc: (54.00%) (23704/43648)
Epoch: 4 | Batch_idx: 350 |  Loss: (1.2696) |  Loss2: (0.0000) | Acc: (54.00%) (24422/44928)
Epoch: 4 | Batch_idx: 360 |  Loss: (1.2691) |  Loss2: (0.0000) | Acc: (54.00%) (25110/46208)
Epoch: 4 | Batch_idx: 370 |  Loss: (1.2690) |  Loss2: (0.0000) | Acc: (54.00%) (25817/47488)
Epoch: 4 | Batch_idx: 380 |  Loss: (1.2680) |  Loss2: (0.0000) | Acc: (54.00%) (26522/48768)
Epoch: 4 | Batch_idx: 390 |  Loss: (1.2664) |  Loss2: (0.0000) | Acc: (54.00%) (27226/50000)
# TEST : Loss: (1.2478) | Acc: (54.00%) (5444/10000)
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 5 | Batch_idx: 0 |  Loss: (1.1674) |  Loss2: (0.0000) | Acc: (58.00%) (75/128)
Epoch: 5 | Batch_idx: 10 |  Loss: (1.3283) |  Loss2: (0.0000) | Acc: (51.00%) (728/1408)
Epoch: 5 | Batch_idx: 20 |  Loss: (1.3692) |  Loss2: (0.0000) | Acc: (50.00%) (1362/2688)
Epoch: 5 | Batch_idx: 30 |  Loss: (1.4170) |  Loss2: (0.0000) | Acc: (48.00%) (1933/3968)
Epoch: 5 | Batch_idx: 40 |  Loss: (1.4353) |  Loss2: (0.0000) | Acc: (47.00%) (2506/5248)
Epoch: 5 | Batch_idx: 50 |  Loss: (1.4355) |  Loss2: (0.0000) | Acc: (47.00%) (3119/6528)
Epoch: 5 | Batch_idx: 60 |  Loss: (1.4271) |  Loss2: (0.0000) | Acc: (48.00%) (3752/7808)
Epoch: 5 | Batch_idx: 70 |  Loss: (1.4215) |  Loss2: (0.0000) | Acc: (48.00%) (4395/9088)
Epoch: 5 | Batch_idx: 80 |  Loss: (1.4151) |  Loss2: (0.0000) | Acc: (48.00%) (5041/10368)
Epoch: 5 | Batch_idx: 90 |  Loss: (1.4089) |  Loss2: (0.0000) | Acc: (48.00%) (5665/11648)
Epoch: 5 | Batch_idx: 100 |  Loss: (1.4052) |  Loss2: (0.0000) | Acc: (48.00%) (6312/12928)
Epoch: 5 | Batch_idx: 110 |  Loss: (1.3984) |  Loss2: (0.0000) | Acc: (49.00%) (6981/14208)
Epoch: 5 | Batch_idx: 120 |  Loss: (1.3956) |  Loss2: (0.0000) | Acc: (49.00%) (7630/15488)
Epoch: 5 | Batch_idx: 130 |  Loss: (1.3884) |  Loss2: (0.0000) | Acc: (49.00%) (8319/16768)
Epoch: 5 | Batch_idx: 140 |  Loss: (1.3802) |  Loss2: (0.0000) | Acc: (49.00%) (9009/18048)
Epoch: 5 | Batch_idx: 150 |  Loss: (1.3741) |  Loss2: (0.0000) | Acc: (50.00%) (9703/19328)
Epoch: 5 | Batch_idx: 160 |  Loss: (1.3669) |  Loss2: (0.0000) | Acc: (50.00%) (10418/20608)
Epoch: 5 | Batch_idx: 170 |  Loss: (1.3613) |  Loss2: (0.0000) | Acc: (50.00%) (11112/21888)
Epoch: 5 | Batch_idx: 180 |  Loss: (1.3556) |  Loss2: (0.0000) | Acc: (51.00%) (11848/23168)
Epoch: 5 | Batch_idx: 190 |  Loss: (1.3529) |  Loss2: (0.0000) | Acc: (51.00%) (12537/24448)
Epoch: 5 | Batch_idx: 200 |  Loss: (1.3494) |  Loss2: (0.0000) | Acc: (51.00%) (13210/25728)
Epoch: 5 | Batch_idx: 210 |  Loss: (1.3429) |  Loss2: (0.0000) | Acc: (51.00%) (13912/27008)
Epoch: 5 | Batch_idx: 220 |  Loss: (1.3388) |  Loss2: (0.0000) | Acc: (51.00%) (14619/28288)
Epoch: 5 | Batch_idx: 230 |  Loss: (1.3350) |  Loss2: (0.0000) | Acc: (51.00%) (15309/29568)
Epoch: 5 | Batch_idx: 240 |  Loss: (1.3311) |  Loss2: (0.0000) | Acc: (51.00%) (16005/30848)
Epoch: 5 | Batch_idx: 250 |  Loss: (1.3289) |  Loss2: (0.0000) | Acc: (51.00%) (16685/32128)
Epoch: 5 | Batch_idx: 260 |  Loss: (1.3247) |  Loss2: (0.0000) | Acc: (52.00%) (17393/33408)
Epoch: 5 | Batch_idx: 270 |  Loss: (1.3211) |  Loss2: (0.0000) | Acc: (52.00%) (18104/34688)
Epoch: 5 | Batch_idx: 280 |  Loss: (1.3187) |  Loss2: (0.0000) | Acc: (52.00%) (18793/35968)
Epoch: 5 | Batch_idx: 290 |  Loss: (1.3155) |  Loss2: (0.0000) | Acc: (52.00%) (19488/37248)
Epoch: 5 | Batch_idx: 300 |  Loss: (1.3137) |  Loss2: (0.0000) | Acc: (52.00%) (20171/38528)
Epoch: 5 | Batch_idx: 310 |  Loss: (1.3127) |  Loss2: (0.0000) | Acc: (52.00%) (20859/39808)
Epoch: 5 | Batch_idx: 320 |  Loss: (1.3111) |  Loss2: (0.0000) | Acc: (52.00%) (21558/41088)
Epoch: 5 | Batch_idx: 330 |  Loss: (1.3092) |  Loss2: (0.0000) | Acc: (52.00%) (22251/42368)
Epoch: 5 | Batch_idx: 340 |  Loss: (1.3062) |  Loss2: (0.0000) | Acc: (52.00%) (22973/43648)
Epoch: 5 | Batch_idx: 350 |  Loss: (1.3044) |  Loss2: (0.0000) | Acc: (52.00%) (23667/44928)
Epoch: 5 | Batch_idx: 360 |  Loss: (1.3035) |  Loss2: (0.0000) | Acc: (52.00%) (24359/46208)
Epoch: 5 | Batch_idx: 370 |  Loss: (1.3019) |  Loss2: (0.0000) | Acc: (52.00%) (25048/47488)
Epoch: 5 | Batch_idx: 380 |  Loss: (1.3004) |  Loss2: (0.0000) | Acc: (52.00%) (25752/48768)
Epoch: 5 | Batch_idx: 390 |  Loss: (1.2987) |  Loss2: (0.0000) | Acc: (52.00%) (26436/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_005.pth.tar'
# TEST : Loss: (1.2225) | Acc: (55.00%) (5542/10000)
percent tensor([0.4825, 0.5175], device='cuda:0')
percent tensor([0.4308, 0.5692], device='cuda:0')
percent tensor([0.4876, 0.5124], device='cuda:0')
percent tensor([0.4941, 0.5059], device='cuda:0')
percent tensor([0.5108, 0.4892], device='cuda:0')
percent tensor([0.5177, 0.4823], device='cuda:0')
percent tensor([0.5241, 0.4759], device='cuda:0')
percent tensor([0.6260, 0.3740], device='cuda:0')
Epoch: 6 | Batch_idx: 0 |  Loss: (1.1494) |  Loss2: (0.0000) | Acc: (60.00%) (78/128)
Epoch: 6 | Batch_idx: 10 |  Loss: (1.2698) |  Loss2: (0.0000) | Acc: (54.00%) (761/1408)
Epoch: 6 | Batch_idx: 20 |  Loss: (1.2382) |  Loss2: (0.0000) | Acc: (56.00%) (1508/2688)
Epoch: 6 | Batch_idx: 30 |  Loss: (1.2266) |  Loss2: (0.0000) | Acc: (56.00%) (2231/3968)
Epoch: 6 | Batch_idx: 40 |  Loss: (1.2321) |  Loss2: (0.0000) | Acc: (55.00%) (2926/5248)
Epoch: 6 | Batch_idx: 50 |  Loss: (1.2318) |  Loss2: (0.0000) | Acc: (56.00%) (3657/6528)
Epoch: 6 | Batch_idx: 60 |  Loss: (1.2357) |  Loss2: (0.0000) | Acc: (55.00%) (4372/7808)
Epoch: 6 | Batch_idx: 70 |  Loss: (1.2326) |  Loss2: (0.0000) | Acc: (55.00%) (5086/9088)
Epoch: 6 | Batch_idx: 80 |  Loss: (1.2353) |  Loss2: (0.0000) | Acc: (55.00%) (5781/10368)
Epoch: 6 | Batch_idx: 90 |  Loss: (1.2367) |  Loss2: (0.0000) | Acc: (55.00%) (6463/11648)
Epoch: 6 | Batch_idx: 100 |  Loss: (1.2355) |  Loss2: (0.0000) | Acc: (55.00%) (7157/12928)
Epoch: 6 | Batch_idx: 110 |  Loss: (1.2361) |  Loss2: (0.0000) | Acc: (55.00%) (7841/14208)
Epoch: 6 | Batch_idx: 120 |  Loss: (1.2317) |  Loss2: (0.0000) | Acc: (55.00%) (8574/15488)
Epoch: 6 | Batch_idx: 130 |  Loss: (1.2298) |  Loss2: (0.0000) | Acc: (55.00%) (9289/16768)
Epoch: 6 | Batch_idx: 140 |  Loss: (1.2304) |  Loss2: (0.0000) | Acc: (55.00%) (9986/18048)
Epoch: 6 | Batch_idx: 150 |  Loss: (1.2259) |  Loss2: (0.0000) | Acc: (55.00%) (10719/19328)
Epoch: 6 | Batch_idx: 160 |  Loss: (1.2256) |  Loss2: (0.0000) | Acc: (55.00%) (11402/20608)
Epoch: 6 | Batch_idx: 170 |  Loss: (1.2241) |  Loss2: (0.0000) | Acc: (55.00%) (12109/21888)
Epoch: 6 | Batch_idx: 180 |  Loss: (1.2225) |  Loss2: (0.0000) | Acc: (55.00%) (12831/23168)
Epoch: 6 | Batch_idx: 190 |  Loss: (1.2212) |  Loss2: (0.0000) | Acc: (55.00%) (13543/24448)
Epoch: 6 | Batch_idx: 200 |  Loss: (1.2198) |  Loss2: (0.0000) | Acc: (55.00%) (14280/25728)
Epoch: 6 | Batch_idx: 210 |  Loss: (1.2179) |  Loss2: (0.0000) | Acc: (55.00%) (15021/27008)
Epoch: 6 | Batch_idx: 220 |  Loss: (1.2167) |  Loss2: (0.0000) | Acc: (55.00%) (15741/28288)
Epoch: 6 | Batch_idx: 230 |  Loss: (1.2170) |  Loss2: (0.0000) | Acc: (55.00%) (16461/29568)
Epoch: 6 | Batch_idx: 240 |  Loss: (1.2167) |  Loss2: (0.0000) | Acc: (55.00%) (17170/30848)
Epoch: 6 | Batch_idx: 250 |  Loss: (1.2149) |  Loss2: (0.0000) | Acc: (55.00%) (17894/32128)
Epoch: 6 | Batch_idx: 260 |  Loss: (1.2148) |  Loss2: (0.0000) | Acc: (55.00%) (18612/33408)
Epoch: 6 | Batch_idx: 270 |  Loss: (1.2131) |  Loss2: (0.0000) | Acc: (55.00%) (19343/34688)
Epoch: 6 | Batch_idx: 280 |  Loss: (1.2142) |  Loss2: (0.0000) | Acc: (55.00%) (20072/35968)
Epoch: 6 | Batch_idx: 290 |  Loss: (1.2151) |  Loss2: (0.0000) | Acc: (55.00%) (20769/37248)
Epoch: 6 | Batch_idx: 300 |  Loss: (1.2153) |  Loss2: (0.0000) | Acc: (55.00%) (21483/38528)
Epoch: 6 | Batch_idx: 310 |  Loss: (1.2154) |  Loss2: (0.0000) | Acc: (55.00%) (22202/39808)
Epoch: 6 | Batch_idx: 320 |  Loss: (1.2150) |  Loss2: (0.0000) | Acc: (55.00%) (22923/41088)
Epoch: 6 | Batch_idx: 330 |  Loss: (1.2137) |  Loss2: (0.0000) | Acc: (55.00%) (23655/42368)
Epoch: 6 | Batch_idx: 340 |  Loss: (1.2125) |  Loss2: (0.0000) | Acc: (55.00%) (24385/43648)
Epoch: 6 | Batch_idx: 350 |  Loss: (1.2110) |  Loss2: (0.0000) | Acc: (55.00%) (25113/44928)
Epoch: 6 | Batch_idx: 360 |  Loss: (1.2098) |  Loss2: (0.0000) | Acc: (55.00%) (25835/46208)
Epoch: 6 | Batch_idx: 370 |  Loss: (1.2097) |  Loss2: (0.0000) | Acc: (55.00%) (26551/47488)
Epoch: 6 | Batch_idx: 380 |  Loss: (1.2102) |  Loss2: (0.0000) | Acc: (55.00%) (27273/48768)
Epoch: 6 | Batch_idx: 390 |  Loss: (1.2115) |  Loss2: (0.0000) | Acc: (55.00%) (27959/50000)
# TEST : Loss: (1.2104) | Acc: (56.00%) (5663/10000)
percent tensor([0.4842, 0.5158], device='cuda:0')
percent tensor([0.4345, 0.5655], device='cuda:0')
percent tensor([0.4818, 0.5182], device='cuda:0')
percent tensor([0.4927, 0.5073], device='cuda:0')
percent tensor([0.5190, 0.4810], device='cuda:0')
percent tensor([0.5263, 0.4737], device='cuda:0')
percent tensor([0.5378, 0.4622], device='cuda:0')
percent tensor([0.7548, 0.2452], device='cuda:0')
Epoch: 7 | Batch_idx: 0 |  Loss: (1.1469) |  Loss2: (0.0000) | Acc: (55.00%) (71/128)
Epoch: 7 | Batch_idx: 10 |  Loss: (1.2092) |  Loss2: (0.0000) | Acc: (55.00%) (777/1408)
Epoch: 7 | Batch_idx: 20 |  Loss: (1.2277) |  Loss2: (0.0000) | Acc: (54.00%) (1469/2688)
Epoch: 7 | Batch_idx: 30 |  Loss: (1.2087) |  Loss2: (0.0000) | Acc: (55.00%) (2194/3968)
Epoch: 7 | Batch_idx: 40 |  Loss: (1.2085) |  Loss2: (0.0000) | Acc: (55.00%) (2907/5248)
Epoch: 7 | Batch_idx: 50 |  Loss: (1.1998) |  Loss2: (0.0000) | Acc: (55.00%) (3641/6528)
Epoch: 7 | Batch_idx: 60 |  Loss: (1.1970) |  Loss2: (0.0000) | Acc: (55.00%) (4367/7808)
Epoch: 7 | Batch_idx: 70 |  Loss: (1.2013) |  Loss2: (0.0000) | Acc: (55.00%) (5063/9088)
Epoch: 7 | Batch_idx: 80 |  Loss: (1.1989) |  Loss2: (0.0000) | Acc: (55.00%) (5778/10368)
Epoch: 7 | Batch_idx: 90 |  Loss: (1.1967) |  Loss2: (0.0000) | Acc: (55.00%) (6516/11648)
Epoch: 7 | Batch_idx: 100 |  Loss: (1.1943) |  Loss2: (0.0000) | Acc: (56.00%) (7268/12928)
Epoch: 7 | Batch_idx: 110 |  Loss: (1.1943) |  Loss2: (0.0000) | Acc: (56.00%) (7978/14208)
Epoch: 7 | Batch_idx: 120 |  Loss: (1.1951) |  Loss2: (0.0000) | Acc: (56.00%) (8681/15488)
Epoch: 7 | Batch_idx: 130 |  Loss: (1.1967) |  Loss2: (0.0000) | Acc: (55.00%) (9381/16768)
Epoch: 7 | Batch_idx: 140 |  Loss: (1.1977) |  Loss2: (0.0000) | Acc: (55.00%) (10099/18048)
Epoch: 7 | Batch_idx: 150 |  Loss: (1.1949) |  Loss2: (0.0000) | Acc: (56.00%) (10850/19328)
Epoch: 7 | Batch_idx: 160 |  Loss: (1.1952) |  Loss2: (0.0000) | Acc: (56.00%) (11569/20608)
Epoch: 7 | Batch_idx: 170 |  Loss: (1.1954) |  Loss2: (0.0000) | Acc: (56.00%) (12302/21888)
Epoch: 7 | Batch_idx: 180 |  Loss: (1.1956) |  Loss2: (0.0000) | Acc: (56.00%) (13020/23168)
Epoch: 7 | Batch_idx: 190 |  Loss: (1.1960) |  Loss2: (0.0000) | Acc: (56.00%) (13752/24448)
Epoch: 7 | Batch_idx: 200 |  Loss: (1.1962) |  Loss2: (0.0000) | Acc: (56.00%) (14463/25728)
Epoch: 7 | Batch_idx: 210 |  Loss: (1.1942) |  Loss2: (0.0000) | Acc: (56.00%) (15196/27008)
Epoch: 7 | Batch_idx: 220 |  Loss: (1.1934) |  Loss2: (0.0000) | Acc: (56.00%) (15946/28288)
Epoch: 7 | Batch_idx: 230 |  Loss: (1.1938) |  Loss2: (0.0000) | Acc: (56.00%) (16655/29568)
Epoch: 7 | Batch_idx: 240 |  Loss: (1.1933) |  Loss2: (0.0000) | Acc: (56.00%) (17387/30848)
Epoch: 7 | Batch_idx: 250 |  Loss: (1.1932) |  Loss2: (0.0000) | Acc: (56.00%) (18125/32128)
Epoch: 7 | Batch_idx: 260 |  Loss: (1.1949) |  Loss2: (0.0000) | Acc: (56.00%) (18844/33408)
Epoch: 7 | Batch_idx: 270 |  Loss: (1.1945) |  Loss2: (0.0000) | Acc: (56.00%) (19577/34688)
Epoch: 7 | Batch_idx: 280 |  Loss: (1.1938) |  Loss2: (0.0000) | Acc: (56.00%) (20302/35968)
Epoch: 7 | Batch_idx: 290 |  Loss: (1.1939) |  Loss2: (0.0000) | Acc: (56.00%) (21025/37248)
Epoch: 7 | Batch_idx: 300 |  Loss: (1.1952) |  Loss2: (0.0000) | Acc: (56.00%) (21739/38528)
Epoch: 7 | Batch_idx: 310 |  Loss: (1.1953) |  Loss2: (0.0000) | Acc: (56.00%) (22453/39808)
Epoch: 7 | Batch_idx: 320 |  Loss: (1.1939) |  Loss2: (0.0000) | Acc: (56.00%) (23198/41088)
Epoch: 7 | Batch_idx: 330 |  Loss: (1.1942) |  Loss2: (0.0000) | Acc: (56.00%) (23901/42368)
Epoch: 7 | Batch_idx: 340 |  Loss: (1.1946) |  Loss2: (0.0000) | Acc: (56.00%) (24593/43648)
Epoch: 7 | Batch_idx: 350 |  Loss: (1.1946) |  Loss2: (0.0000) | Acc: (56.00%) (25307/44928)
Epoch: 7 | Batch_idx: 360 |  Loss: (1.1939) |  Loss2: (0.0000) | Acc: (56.00%) (26035/46208)
Epoch: 7 | Batch_idx: 370 |  Loss: (1.1936) |  Loss2: (0.0000) | Acc: (56.00%) (26768/47488)
Epoch: 7 | Batch_idx: 380 |  Loss: (1.1930) |  Loss2: (0.0000) | Acc: (56.00%) (27517/48768)
Epoch: 7 | Batch_idx: 390 |  Loss: (1.1925) |  Loss2: (0.0000) | Acc: (56.00%) (28210/50000)
# TEST : Loss: (1.1995) | Acc: (56.00%) (5669/10000)
percent tensor([0.4857, 0.5143], device='cuda:0')
percent tensor([0.4407, 0.5593], device='cuda:0')
percent tensor([0.4807, 0.5193], device='cuda:0')
percent tensor([0.4934, 0.5066], device='cuda:0')
percent tensor([0.5248, 0.4752], device='cuda:0')
percent tensor([0.5292, 0.4708], device='cuda:0')
percent tensor([0.5439, 0.4561], device='cuda:0')
percent tensor([0.7981, 0.2019], device='cuda:0')
Epoch: 8 | Batch_idx: 0 |  Loss: (1.1600) |  Loss2: (0.0000) | Acc: (60.00%) (78/128)
Epoch: 8 | Batch_idx: 10 |  Loss: (1.2240) |  Loss2: (0.0000) | Acc: (55.00%) (786/1408)
Epoch: 8 | Batch_idx: 20 |  Loss: (1.2177) |  Loss2: (0.0000) | Acc: (55.00%) (1503/2688)
Epoch: 8 | Batch_idx: 30 |  Loss: (1.2221) |  Loss2: (0.0000) | Acc: (55.00%) (2218/3968)
Epoch: 8 | Batch_idx: 40 |  Loss: (1.2172) |  Loss2: (0.0000) | Acc: (56.00%) (2944/5248)
Epoch: 8 | Batch_idx: 50 |  Loss: (1.2184) |  Loss2: (0.0000) | Acc: (56.00%) (3663/6528)
Epoch: 8 | Batch_idx: 60 |  Loss: (1.2113) |  Loss2: (0.0000) | Acc: (56.00%) (4387/7808)
Epoch: 8 | Batch_idx: 70 |  Loss: (1.2048) |  Loss2: (0.0000) | Acc: (56.00%) (5118/9088)
Epoch: 8 | Batch_idx: 80 |  Loss: (1.2015) |  Loss2: (0.0000) | Acc: (56.00%) (5839/10368)
Epoch: 8 | Batch_idx: 90 |  Loss: (1.1988) |  Loss2: (0.0000) | Acc: (56.00%) (6579/11648)
Epoch: 8 | Batch_idx: 100 |  Loss: (1.2016) |  Loss2: (0.0000) | Acc: (56.00%) (7281/12928)
Epoch: 8 | Batch_idx: 110 |  Loss: (1.2002) |  Loss2: (0.0000) | Acc: (56.00%) (8030/14208)
Epoch: 8 | Batch_idx: 120 |  Loss: (1.2001) |  Loss2: (0.0000) | Acc: (56.00%) (8756/15488)
Epoch: 8 | Batch_idx: 130 |  Loss: (1.1984) |  Loss2: (0.0000) | Acc: (56.00%) (9495/16768)
Epoch: 8 | Batch_idx: 140 |  Loss: (1.1960) |  Loss2: (0.0000) | Acc: (56.00%) (10239/18048)
Epoch: 8 | Batch_idx: 150 |  Loss: (1.1913) |  Loss2: (0.0000) | Acc: (56.00%) (10988/19328)
Epoch: 8 | Batch_idx: 160 |  Loss: (1.1910) |  Loss2: (0.0000) | Acc: (56.00%) (11688/20608)
Epoch: 8 | Batch_idx: 170 |  Loss: (1.1902) |  Loss2: (0.0000) | Acc: (56.00%) (12427/21888)
Epoch: 8 | Batch_idx: 180 |  Loss: (1.1908) |  Loss2: (0.0000) | Acc: (56.00%) (13139/23168)
Epoch: 8 | Batch_idx: 190 |  Loss: (1.1902) |  Loss2: (0.0000) | Acc: (56.00%) (13879/24448)
Epoch: 8 | Batch_idx: 200 |  Loss: (1.1908) |  Loss2: (0.0000) | Acc: (56.00%) (14613/25728)
Epoch: 8 | Batch_idx: 210 |  Loss: (1.1929) |  Loss2: (0.0000) | Acc: (56.00%) (15309/27008)
Epoch: 8 | Batch_idx: 220 |  Loss: (1.1909) |  Loss2: (0.0000) | Acc: (56.00%) (16052/28288)
Epoch: 8 | Batch_idx: 230 |  Loss: (1.1883) |  Loss2: (0.0000) | Acc: (56.00%) (16803/29568)
Epoch: 8 | Batch_idx: 240 |  Loss: (1.1903) |  Loss2: (0.0000) | Acc: (56.00%) (17496/30848)
Epoch: 8 | Batch_idx: 250 |  Loss: (1.1901) |  Loss2: (0.0000) | Acc: (56.00%) (18241/32128)
Epoch: 8 | Batch_idx: 260 |  Loss: (1.1894) |  Loss2: (0.0000) | Acc: (56.00%) (18966/33408)
Epoch: 8 | Batch_idx: 270 |  Loss: (1.1901) |  Loss2: (0.0000) | Acc: (56.00%) (19674/34688)
Epoch: 8 | Batch_idx: 280 |  Loss: (1.1886) |  Loss2: (0.0000) | Acc: (56.00%) (20424/35968)
Epoch: 8 | Batch_idx: 290 |  Loss: (1.1876) |  Loss2: (0.0000) | Acc: (56.00%) (21154/37248)
Epoch: 8 | Batch_idx: 300 |  Loss: (1.1877) |  Loss2: (0.0000) | Acc: (56.00%) (21857/38528)
Epoch: 8 | Batch_idx: 310 |  Loss: (1.1889) |  Loss2: (0.0000) | Acc: (56.00%) (22561/39808)
Epoch: 8 | Batch_idx: 320 |  Loss: (1.1885) |  Loss2: (0.0000) | Acc: (56.00%) (23290/41088)
Epoch: 8 | Batch_idx: 330 |  Loss: (1.1868) |  Loss2: (0.0000) | Acc: (56.00%) (24049/42368)
Epoch: 8 | Batch_idx: 340 |  Loss: (1.1867) |  Loss2: (0.0000) | Acc: (56.00%) (24788/43648)
Epoch: 8 | Batch_idx: 350 |  Loss: (1.1859) |  Loss2: (0.0000) | Acc: (56.00%) (25524/44928)
Epoch: 8 | Batch_idx: 360 |  Loss: (1.1863) |  Loss2: (0.0000) | Acc: (56.00%) (26241/46208)
Epoch: 8 | Batch_idx: 370 |  Loss: (1.1855) |  Loss2: (0.0000) | Acc: (56.00%) (26976/47488)
Epoch: 8 | Batch_idx: 380 |  Loss: (1.1857) |  Loss2: (0.0000) | Acc: (56.00%) (27684/48768)
Epoch: 8 | Batch_idx: 390 |  Loss: (1.1846) |  Loss2: (0.0000) | Acc: (56.00%) (28397/50000)
# TEST : Loss: (1.1968) | Acc: (56.00%) (5685/10000)
percent tensor([0.4872, 0.5128], device='cuda:0')
percent tensor([0.4431, 0.5569], device='cuda:0')
percent tensor([0.4815, 0.5185], device='cuda:0')
percent tensor([0.4951, 0.5049], device='cuda:0')
percent tensor([0.5303, 0.4697], device='cuda:0')
percent tensor([0.5304, 0.4696], device='cuda:0')
percent tensor([0.5496, 0.4504], device='cuda:0')
percent tensor([0.8216, 0.1784], device='cuda:0')
Epoch: 9 | Batch_idx: 0 |  Loss: (1.1472) |  Loss2: (0.0000) | Acc: (55.00%) (71/128)
Epoch: 9 | Batch_idx: 10 |  Loss: (1.2019) |  Loss2: (0.0000) | Acc: (55.00%) (783/1408)
Epoch: 9 | Batch_idx: 20 |  Loss: (1.1835) |  Loss2: (0.0000) | Acc: (56.00%) (1524/2688)
Epoch: 9 | Batch_idx: 30 |  Loss: (1.1833) |  Loss2: (0.0000) | Acc: (57.00%) (2263/3968)
Epoch: 9 | Batch_idx: 40 |  Loss: (1.1861) |  Loss2: (0.0000) | Acc: (56.00%) (2984/5248)
Epoch: 9 | Batch_idx: 50 |  Loss: (1.1930) |  Loss2: (0.0000) | Acc: (56.00%) (3691/6528)
Epoch: 9 | Batch_idx: 60 |  Loss: (1.1945) |  Loss2: (0.0000) | Acc: (56.00%) (4402/7808)
Epoch: 9 | Batch_idx: 70 |  Loss: (1.1949) |  Loss2: (0.0000) | Acc: (56.00%) (5111/9088)
Epoch: 9 | Batch_idx: 80 |  Loss: (1.1911) |  Loss2: (0.0000) | Acc: (56.00%) (5850/10368)
Epoch: 9 | Batch_idx: 90 |  Loss: (1.1910) |  Loss2: (0.0000) | Acc: (56.00%) (6548/11648)
Epoch: 9 | Batch_idx: 100 |  Loss: (1.1919) |  Loss2: (0.0000) | Acc: (56.00%) (7265/12928)
Epoch: 9 | Batch_idx: 110 |  Loss: (1.1937) |  Loss2: (0.0000) | Acc: (56.00%) (7965/14208)
Epoch: 9 | Batch_idx: 120 |  Loss: (1.1899) |  Loss2: (0.0000) | Acc: (56.00%) (8715/15488)
Epoch: 9 | Batch_idx: 130 |  Loss: (1.1911) |  Loss2: (0.0000) | Acc: (56.00%) (9435/16768)
Epoch: 9 | Batch_idx: 140 |  Loss: (1.1876) |  Loss2: (0.0000) | Acc: (56.00%) (10173/18048)
Epoch: 9 | Batch_idx: 150 |  Loss: (1.1906) |  Loss2: (0.0000) | Acc: (56.00%) (10880/19328)
Epoch: 9 | Batch_idx: 160 |  Loss: (1.1894) |  Loss2: (0.0000) | Acc: (56.00%) (11614/20608)
Epoch: 9 | Batch_idx: 170 |  Loss: (1.1885) |  Loss2: (0.0000) | Acc: (56.00%) (12337/21888)
Epoch: 9 | Batch_idx: 180 |  Loss: (1.1863) |  Loss2: (0.0000) | Acc: (56.00%) (13090/23168)
Epoch: 9 | Batch_idx: 190 |  Loss: (1.1872) |  Loss2: (0.0000) | Acc: (56.00%) (13797/24448)
Epoch: 9 | Batch_idx: 200 |  Loss: (1.1876) |  Loss2: (0.0000) | Acc: (56.00%) (14505/25728)
Epoch: 9 | Batch_idx: 210 |  Loss: (1.1869) |  Loss2: (0.0000) | Acc: (56.00%) (15231/27008)
Epoch: 9 | Batch_idx: 220 |  Loss: (1.1876) |  Loss2: (0.0000) | Acc: (56.00%) (15960/28288)
Epoch: 9 | Batch_idx: 230 |  Loss: (1.1873) |  Loss2: (0.0000) | Acc: (56.00%) (16692/29568)
Epoch: 9 | Batch_idx: 240 |  Loss: (1.1881) |  Loss2: (0.0000) | Acc: (56.00%) (17405/30848)
Epoch: 9 | Batch_idx: 250 |  Loss: (1.1873) |  Loss2: (0.0000) | Acc: (56.00%) (18155/32128)
Epoch: 9 | Batch_idx: 260 |  Loss: (1.1878) |  Loss2: (0.0000) | Acc: (56.00%) (18862/33408)
Epoch: 9 | Batch_idx: 270 |  Loss: (1.1866) |  Loss2: (0.0000) | Acc: (56.00%) (19618/34688)
Epoch: 9 | Batch_idx: 280 |  Loss: (1.1877) |  Loss2: (0.0000) | Acc: (56.00%) (20327/35968)
Epoch: 9 | Batch_idx: 290 |  Loss: (1.1883) |  Loss2: (0.0000) | Acc: (56.00%) (21039/37248)
Epoch: 9 | Batch_idx: 300 |  Loss: (1.1867) |  Loss2: (0.0000) | Acc: (56.00%) (21778/38528)
Epoch: 9 | Batch_idx: 310 |  Loss: (1.1857) |  Loss2: (0.0000) | Acc: (56.00%) (22525/39808)
Epoch: 9 | Batch_idx: 320 |  Loss: (1.1845) |  Loss2: (0.0000) | Acc: (56.00%) (23251/41088)
Epoch: 9 | Batch_idx: 330 |  Loss: (1.1838) |  Loss2: (0.0000) | Acc: (56.00%) (24005/42368)
Epoch: 9 | Batch_idx: 340 |  Loss: (1.1821) |  Loss2: (0.0000) | Acc: (56.00%) (24755/43648)
Epoch: 9 | Batch_idx: 350 |  Loss: (1.1817) |  Loss2: (0.0000) | Acc: (56.00%) (25499/44928)
Epoch: 9 | Batch_idx: 360 |  Loss: (1.1807) |  Loss2: (0.0000) | Acc: (56.00%) (26227/46208)
Epoch: 9 | Batch_idx: 370 |  Loss: (1.1814) |  Loss2: (0.0000) | Acc: (56.00%) (26945/47488)
Epoch: 9 | Batch_idx: 380 |  Loss: (1.1803) |  Loss2: (0.0000) | Acc: (56.00%) (27685/48768)
Epoch: 9 | Batch_idx: 390 |  Loss: (1.1797) |  Loss2: (0.0000) | Acc: (56.00%) (28392/50000)
# TEST : Loss: (1.1907) | Acc: (57.00%) (5726/10000)
percent tensor([0.4881, 0.5119], device='cuda:0')
percent tensor([0.4475, 0.5525], device='cuda:0')
percent tensor([0.4826, 0.5174], device='cuda:0')
percent tensor([0.4974, 0.5026], device='cuda:0')
percent tensor([0.5330, 0.4670], device='cuda:0')
percent tensor([0.5315, 0.4685], device='cuda:0')
percent tensor([0.5515, 0.4485], device='cuda:0')
percent tensor([0.8368, 0.1632], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 10 | Batch_idx: 0 |  Loss: (1.1070) |  Loss2: (0.0000) | Acc: (59.00%) (76/128)
Epoch: 10 | Batch_idx: 10 |  Loss: (1.2061) |  Loss2: (0.0000) | Acc: (56.00%) (790/1408)
Epoch: 10 | Batch_idx: 20 |  Loss: (1.1900) |  Loss2: (0.0000) | Acc: (55.00%) (1505/2688)
Epoch: 10 | Batch_idx: 30 |  Loss: (1.1843) |  Loss2: (0.0000) | Acc: (56.00%) (2250/3968)
Epoch: 10 | Batch_idx: 40 |  Loss: (1.1819) |  Loss2: (0.0000) | Acc: (56.00%) (2984/5248)
Epoch: 10 | Batch_idx: 50 |  Loss: (1.1915) |  Loss2: (0.0000) | Acc: (56.00%) (3688/6528)
Epoch: 10 | Batch_idx: 60 |  Loss: (1.1828) |  Loss2: (0.0000) | Acc: (56.00%) (4433/7808)
Epoch: 10 | Batch_idx: 70 |  Loss: (1.1841) |  Loss2: (0.0000) | Acc: (56.00%) (5130/9088)
Epoch: 10 | Batch_idx: 80 |  Loss: (1.1860) |  Loss2: (0.0000) | Acc: (56.00%) (5858/10368)
Epoch: 10 | Batch_idx: 90 |  Loss: (1.1817) |  Loss2: (0.0000) | Acc: (56.00%) (6583/11648)
Epoch: 10 | Batch_idx: 100 |  Loss: (1.1802) |  Loss2: (0.0000) | Acc: (56.00%) (7284/12928)
Epoch: 10 | Batch_idx: 110 |  Loss: (1.1796) |  Loss2: (0.0000) | Acc: (56.00%) (8030/14208)
Epoch: 10 | Batch_idx: 120 |  Loss: (1.1779) |  Loss2: (0.0000) | Acc: (56.00%) (8754/15488)
Epoch: 10 | Batch_idx: 130 |  Loss: (1.1750) |  Loss2: (0.0000) | Acc: (56.00%) (9493/16768)
Epoch: 10 | Batch_idx: 140 |  Loss: (1.1770) |  Loss2: (0.0000) | Acc: (56.00%) (10227/18048)
Epoch: 10 | Batch_idx: 150 |  Loss: (1.1766) |  Loss2: (0.0000) | Acc: (56.00%) (10971/19328)
Epoch: 10 | Batch_idx: 160 |  Loss: (1.1755) |  Loss2: (0.0000) | Acc: (56.00%) (11714/20608)
Epoch: 10 | Batch_idx: 170 |  Loss: (1.1738) |  Loss2: (0.0000) | Acc: (56.00%) (12459/21888)
Epoch: 10 | Batch_idx: 180 |  Loss: (1.1727) |  Loss2: (0.0000) | Acc: (56.00%) (13195/23168)
Epoch: 10 | Batch_idx: 190 |  Loss: (1.1740) |  Loss2: (0.0000) | Acc: (56.00%) (13927/24448)
Epoch: 10 | Batch_idx: 200 |  Loss: (1.1740) |  Loss2: (0.0000) | Acc: (57.00%) (14672/25728)
Epoch: 10 | Batch_idx: 210 |  Loss: (1.1730) |  Loss2: (0.0000) | Acc: (57.00%) (15415/27008)
Epoch: 10 | Batch_idx: 220 |  Loss: (1.1718) |  Loss2: (0.0000) | Acc: (57.00%) (16162/28288)
Epoch: 10 | Batch_idx: 230 |  Loss: (1.1686) |  Loss2: (0.0000) | Acc: (57.00%) (16937/29568)
Epoch: 10 | Batch_idx: 240 |  Loss: (1.1667) |  Loss2: (0.0000) | Acc: (57.00%) (17693/30848)
Epoch: 10 | Batch_idx: 250 |  Loss: (1.1639) |  Loss2: (0.0000) | Acc: (57.00%) (18466/32128)
Epoch: 10 | Batch_idx: 260 |  Loss: (1.1599) |  Loss2: (0.0000) | Acc: (57.00%) (19258/33408)
Epoch: 10 | Batch_idx: 270 |  Loss: (1.1584) |  Loss2: (0.0000) | Acc: (57.00%) (20021/34688)
Epoch: 10 | Batch_idx: 280 |  Loss: (1.1575) |  Loss2: (0.0000) | Acc: (57.00%) (20775/35968)
Epoch: 10 | Batch_idx: 290 |  Loss: (1.1553) |  Loss2: (0.0000) | Acc: (57.00%) (21553/37248)
Epoch: 10 | Batch_idx: 300 |  Loss: (1.1528) |  Loss2: (0.0000) | Acc: (57.00%) (22322/38528)
Epoch: 10 | Batch_idx: 310 |  Loss: (1.1511) |  Loss2: (0.0000) | Acc: (57.00%) (23083/39808)
Epoch: 10 | Batch_idx: 320 |  Loss: (1.1499) |  Loss2: (0.0000) | Acc: (58.00%) (23852/41088)
Epoch: 10 | Batch_idx: 330 |  Loss: (1.1493) |  Loss2: (0.0000) | Acc: (58.00%) (24619/42368)
Epoch: 10 | Batch_idx: 340 |  Loss: (1.1464) |  Loss2: (0.0000) | Acc: (58.00%) (25401/43648)
Epoch: 10 | Batch_idx: 350 |  Loss: (1.1442) |  Loss2: (0.0000) | Acc: (58.00%) (26174/44928)
Epoch: 10 | Batch_idx: 360 |  Loss: (1.1436) |  Loss2: (0.0000) | Acc: (58.00%) (26950/46208)
Epoch: 10 | Batch_idx: 370 |  Loss: (1.1423) |  Loss2: (0.0000) | Acc: (58.00%) (27733/47488)
Epoch: 10 | Batch_idx: 380 |  Loss: (1.1406) |  Loss2: (0.0000) | Acc: (58.00%) (28525/48768)
Epoch: 10 | Batch_idx: 390 |  Loss: (1.1376) |  Loss2: (0.0000) | Acc: (58.00%) (29303/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_010.pth.tar'
# TEST : Loss: (1.1644) | Acc: (58.00%) (5803/10000)
percent tensor([0.4882, 0.5118], device='cuda:0')
percent tensor([0.4477, 0.5523], device='cuda:0')
percent tensor([0.4812, 0.5188], device='cuda:0')
percent tensor([0.4974, 0.5026], device='cuda:0')
percent tensor([0.5339, 0.4661], device='cuda:0')
percent tensor([0.5312, 0.4688], device='cuda:0')
percent tensor([0.5531, 0.4469], device='cuda:0')
percent tensor([0.8431, 0.1569], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(167.5112, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(776.6439, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(770.4869, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1525.2926, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(518.1730, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2162.7375, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4322.4346, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1439.3588, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6113.6055, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12233.5244, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4069.5781, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(17284.2344, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 11 | Batch_idx: 0 |  Loss: (1.1319) |  Loss2: (0.0000) | Acc: (60.00%) (77/128)
Epoch: 11 | Batch_idx: 10 |  Loss: (1.1100) |  Loss2: (0.0000) | Acc: (59.00%) (843/1408)
Epoch: 11 | Batch_idx: 20 |  Loss: (1.0824) |  Loss2: (0.0000) | Acc: (60.00%) (1630/2688)
Epoch: 11 | Batch_idx: 30 |  Loss: (1.0926) |  Loss2: (0.0000) | Acc: (59.00%) (2377/3968)
Epoch: 11 | Batch_idx: 40 |  Loss: (1.0860) |  Loss2: (0.0000) | Acc: (60.00%) (3198/5248)
Epoch: 11 | Batch_idx: 50 |  Loss: (1.0821) |  Loss2: (0.0000) | Acc: (60.00%) (3981/6528)
Epoch: 11 | Batch_idx: 60 |  Loss: (1.0838) |  Loss2: (0.0000) | Acc: (60.00%) (4753/7808)
Epoch: 11 | Batch_idx: 70 |  Loss: (1.0750) |  Loss2: (0.0000) | Acc: (61.00%) (5573/9088)
Epoch: 11 | Batch_idx: 80 |  Loss: (1.0740) |  Loss2: (0.0000) | Acc: (61.00%) (6366/10368)
Epoch: 11 | Batch_idx: 90 |  Loss: (1.0737) |  Loss2: (0.0000) | Acc: (61.00%) (7162/11648)
Epoch: 11 | Batch_idx: 100 |  Loss: (1.0690) |  Loss2: (0.0000) | Acc: (61.00%) (7973/12928)
Epoch: 11 | Batch_idx: 110 |  Loss: (1.0676) |  Loss2: (0.0000) | Acc: (61.00%) (8771/14208)
Epoch: 11 | Batch_idx: 120 |  Loss: (1.0705) |  Loss2: (0.0000) | Acc: (61.00%) (9549/15488)
Epoch: 11 | Batch_idx: 130 |  Loss: (1.0657) |  Loss2: (0.0000) | Acc: (61.00%) (10374/16768)
Epoch: 11 | Batch_idx: 140 |  Loss: (1.0620) |  Loss2: (0.0000) | Acc: (62.00%) (11195/18048)
Epoch: 11 | Batch_idx: 150 |  Loss: (1.0581) |  Loss2: (0.0000) | Acc: (62.00%) (12026/19328)
Epoch: 11 | Batch_idx: 160 |  Loss: (1.0545) |  Loss2: (0.0000) | Acc: (62.00%) (12849/20608)
Epoch: 11 | Batch_idx: 170 |  Loss: (1.0519) |  Loss2: (0.0000) | Acc: (62.00%) (13679/21888)
Epoch: 11 | Batch_idx: 180 |  Loss: (1.0506) |  Loss2: (0.0000) | Acc: (62.00%) (14477/23168)
Epoch: 11 | Batch_idx: 190 |  Loss: (1.0501) |  Loss2: (0.0000) | Acc: (62.00%) (15272/24448)
Epoch: 11 | Batch_idx: 200 |  Loss: (1.0493) |  Loss2: (0.0000) | Acc: (62.00%) (16057/25728)
Epoch: 11 | Batch_idx: 210 |  Loss: (1.0498) |  Loss2: (0.0000) | Acc: (62.00%) (16844/27008)
Epoch: 11 | Batch_idx: 220 |  Loss: (1.0472) |  Loss2: (0.0000) | Acc: (62.00%) (17674/28288)
Epoch: 11 | Batch_idx: 230 |  Loss: (1.0454) |  Loss2: (0.0000) | Acc: (62.00%) (18504/29568)
Epoch: 11 | Batch_idx: 240 |  Loss: (1.0454) |  Loss2: (0.0000) | Acc: (62.00%) (19326/30848)
Epoch: 11 | Batch_idx: 250 |  Loss: (1.0460) |  Loss2: (0.0000) | Acc: (62.00%) (20146/32128)
Epoch: 11 | Batch_idx: 260 |  Loss: (1.0460) |  Loss2: (0.0000) | Acc: (62.00%) (20956/33408)
Epoch: 11 | Batch_idx: 270 |  Loss: (1.0467) |  Loss2: (0.0000) | Acc: (62.00%) (21736/34688)
Epoch: 11 | Batch_idx: 280 |  Loss: (1.0442) |  Loss2: (0.0000) | Acc: (62.00%) (22576/35968)
Epoch: 11 | Batch_idx: 290 |  Loss: (1.0433) |  Loss2: (0.0000) | Acc: (62.00%) (23390/37248)
Epoch: 11 | Batch_idx: 300 |  Loss: (1.0427) |  Loss2: (0.0000) | Acc: (62.00%) (24203/38528)
Epoch: 11 | Batch_idx: 310 |  Loss: (1.0423) |  Loss2: (0.0000) | Acc: (62.00%) (25014/39808)
Epoch: 11 | Batch_idx: 320 |  Loss: (1.0400) |  Loss2: (0.0000) | Acc: (62.00%) (25859/41088)
Epoch: 11 | Batch_idx: 330 |  Loss: (1.0387) |  Loss2: (0.0000) | Acc: (62.00%) (26678/42368)
Epoch: 11 | Batch_idx: 340 |  Loss: (1.0389) |  Loss2: (0.0000) | Acc: (62.00%) (27463/43648)
Epoch: 11 | Batch_idx: 350 |  Loss: (1.0383) |  Loss2: (0.0000) | Acc: (62.00%) (28291/44928)
Epoch: 11 | Batch_idx: 360 |  Loss: (1.0375) |  Loss2: (0.0000) | Acc: (62.00%) (29109/46208)
Epoch: 11 | Batch_idx: 370 |  Loss: (1.0374) |  Loss2: (0.0000) | Acc: (62.00%) (29916/47488)
Epoch: 11 | Batch_idx: 380 |  Loss: (1.0358) |  Loss2: (0.0000) | Acc: (63.00%) (30747/48768)
Epoch: 11 | Batch_idx: 390 |  Loss: (1.0345) |  Loss2: (0.0000) | Acc: (63.00%) (31535/50000)
# TEST : Loss: (1.1825) | Acc: (58.00%) (5826/10000)
percent tensor([0.4887, 0.5113], device='cuda:0')
percent tensor([0.4468, 0.5532], device='cuda:0')
percent tensor([0.4835, 0.5165], device='cuda:0')
percent tensor([0.4973, 0.5027], device='cuda:0')
percent tensor([0.5332, 0.4668], device='cuda:0')
percent tensor([0.5326, 0.4674], device='cuda:0')
percent tensor([0.5507, 0.4493], device='cuda:0')
percent tensor([0.8504, 0.1496], device='cuda:0')
Epoch: 12 | Batch_idx: 0 |  Loss: (0.8855) |  Loss2: (0.0000) | Acc: (64.00%) (83/128)
Epoch: 12 | Batch_idx: 10 |  Loss: (0.9495) |  Loss2: (0.0000) | Acc: (65.00%) (924/1408)
Epoch: 12 | Batch_idx: 20 |  Loss: (0.9560) |  Loss2: (0.0000) | Acc: (65.00%) (1769/2688)
Epoch: 12 | Batch_idx: 30 |  Loss: (0.9564) |  Loss2: (0.0000) | Acc: (65.00%) (2611/3968)
Epoch: 12 | Batch_idx: 40 |  Loss: (0.9603) |  Loss2: (0.0000) | Acc: (65.00%) (3447/5248)
Epoch: 12 | Batch_idx: 50 |  Loss: (0.9610) |  Loss2: (0.0000) | Acc: (65.00%) (4286/6528)
Epoch: 12 | Batch_idx: 60 |  Loss: (0.9614) |  Loss2: (0.0000) | Acc: (65.00%) (5122/7808)
Epoch: 12 | Batch_idx: 70 |  Loss: (0.9612) |  Loss2: (0.0000) | Acc: (65.00%) (5958/9088)
Epoch: 12 | Batch_idx: 80 |  Loss: (0.9668) |  Loss2: (0.0000) | Acc: (65.00%) (6781/10368)
Epoch: 12 | Batch_idx: 90 |  Loss: (0.9679) |  Loss2: (0.0000) | Acc: (65.00%) (7617/11648)
Epoch: 12 | Batch_idx: 100 |  Loss: (0.9682) |  Loss2: (0.0000) | Acc: (65.00%) (8468/12928)
Epoch: 12 | Batch_idx: 110 |  Loss: (0.9699) |  Loss2: (0.0000) | Acc: (65.00%) (9308/14208)
Epoch: 12 | Batch_idx: 120 |  Loss: (0.9738) |  Loss2: (0.0000) | Acc: (65.00%) (10097/15488)
Epoch: 12 | Batch_idx: 130 |  Loss: (0.9767) |  Loss2: (0.0000) | Acc: (65.00%) (10901/16768)
Epoch: 12 | Batch_idx: 140 |  Loss: (0.9735) |  Loss2: (0.0000) | Acc: (65.00%) (11754/18048)
Epoch: 12 | Batch_idx: 150 |  Loss: (0.9712) |  Loss2: (0.0000) | Acc: (65.00%) (12622/19328)
Epoch: 12 | Batch_idx: 160 |  Loss: (0.9710) |  Loss2: (0.0000) | Acc: (65.00%) (13466/20608)
Epoch: 12 | Batch_idx: 170 |  Loss: (0.9733) |  Loss2: (0.0000) | Acc: (65.00%) (14284/21888)
Epoch: 12 | Batch_idx: 180 |  Loss: (0.9733) |  Loss2: (0.0000) | Acc: (65.00%) (15130/23168)
Epoch: 12 | Batch_idx: 190 |  Loss: (0.9707) |  Loss2: (0.0000) | Acc: (65.00%) (15997/24448)
Epoch: 12 | Batch_idx: 200 |  Loss: (0.9683) |  Loss2: (0.0000) | Acc: (65.00%) (16840/25728)
Epoch: 12 | Batch_idx: 210 |  Loss: (0.9675) |  Loss2: (0.0000) | Acc: (65.00%) (17682/27008)
Epoch: 12 | Batch_idx: 220 |  Loss: (0.9671) |  Loss2: (0.0000) | Acc: (65.00%) (18522/28288)
Epoch: 12 | Batch_idx: 230 |  Loss: (0.9680) |  Loss2: (0.0000) | Acc: (65.00%) (19355/29568)
Epoch: 12 | Batch_idx: 240 |  Loss: (0.9674) |  Loss2: (0.0000) | Acc: (65.00%) (20198/30848)
Epoch: 12 | Batch_idx: 250 |  Loss: (0.9680) |  Loss2: (0.0000) | Acc: (65.00%) (21021/32128)
Epoch: 12 | Batch_idx: 260 |  Loss: (0.9659) |  Loss2: (0.0000) | Acc: (65.00%) (21898/33408)
Epoch: 12 | Batch_idx: 270 |  Loss: (0.9662) |  Loss2: (0.0000) | Acc: (65.00%) (22729/34688)
Epoch: 12 | Batch_idx: 280 |  Loss: (0.9666) |  Loss2: (0.0000) | Acc: (65.00%) (23560/35968)
Epoch: 12 | Batch_idx: 290 |  Loss: (0.9650) |  Loss2: (0.0000) | Acc: (65.00%) (24437/37248)
Epoch: 12 | Batch_idx: 300 |  Loss: (0.9640) |  Loss2: (0.0000) | Acc: (65.00%) (25329/38528)
Epoch: 12 | Batch_idx: 310 |  Loss: (0.9649) |  Loss2: (0.0000) | Acc: (65.00%) (26160/39808)
Epoch: 12 | Batch_idx: 320 |  Loss: (0.9643) |  Loss2: (0.0000) | Acc: (65.00%) (27000/41088)
Epoch: 12 | Batch_idx: 330 |  Loss: (0.9632) |  Loss2: (0.0000) | Acc: (65.00%) (27873/42368)
Epoch: 12 | Batch_idx: 340 |  Loss: (0.9614) |  Loss2: (0.0000) | Acc: (65.00%) (28753/43648)
Epoch: 12 | Batch_idx: 350 |  Loss: (0.9598) |  Loss2: (0.0000) | Acc: (65.00%) (29604/44928)
Epoch: 12 | Batch_idx: 360 |  Loss: (0.9584) |  Loss2: (0.0000) | Acc: (65.00%) (30473/46208)
Epoch: 12 | Batch_idx: 370 |  Loss: (0.9597) |  Loss2: (0.0000) | Acc: (65.00%) (31281/47488)
Epoch: 12 | Batch_idx: 380 |  Loss: (0.9588) |  Loss2: (0.0000) | Acc: (65.00%) (32146/48768)
Epoch: 12 | Batch_idx: 390 |  Loss: (0.9591) |  Loss2: (0.0000) | Acc: (65.00%) (32940/50000)
# TEST : Loss: (0.9929) | Acc: (64.00%) (6458/10000)
percent tensor([0.4884, 0.5116], device='cuda:0')
percent tensor([0.4495, 0.5505], device='cuda:0')
percent tensor([0.4811, 0.5189], device='cuda:0')
percent tensor([0.4973, 0.5027], device='cuda:0')
percent tensor([0.5330, 0.4670], device='cuda:0')
percent tensor([0.5314, 0.4686], device='cuda:0')
percent tensor([0.5492, 0.4508], device='cuda:0')
percent tensor([0.8409, 0.1591], device='cuda:0')
Epoch: 13 | Batch_idx: 0 |  Loss: (1.1924) |  Loss2: (0.0000) | Acc: (51.00%) (66/128)
Epoch: 13 | Batch_idx: 10 |  Loss: (0.9291) |  Loss2: (0.0000) | Acc: (65.00%) (928/1408)
Epoch: 13 | Batch_idx: 20 |  Loss: (0.9043) |  Loss2: (0.0000) | Acc: (66.00%) (1788/2688)
Epoch: 13 | Batch_idx: 30 |  Loss: (0.9203) |  Loss2: (0.0000) | Acc: (66.00%) (2627/3968)
Epoch: 13 | Batch_idx: 40 |  Loss: (0.9100) |  Loss2: (0.0000) | Acc: (66.00%) (3514/5248)
Epoch: 13 | Batch_idx: 50 |  Loss: (0.9082) |  Loss2: (0.0000) | Acc: (67.00%) (4386/6528)
Epoch: 13 | Batch_idx: 60 |  Loss: (0.9131) |  Loss2: (0.0000) | Acc: (67.00%) (5247/7808)
Epoch: 13 | Batch_idx: 70 |  Loss: (0.9076) |  Loss2: (0.0000) | Acc: (67.00%) (6136/9088)
Epoch: 13 | Batch_idx: 80 |  Loss: (0.9117) |  Loss2: (0.0000) | Acc: (67.00%) (6974/10368)
Epoch: 13 | Batch_idx: 90 |  Loss: (0.9112) |  Loss2: (0.0000) | Acc: (67.00%) (7843/11648)
Epoch: 13 | Batch_idx: 100 |  Loss: (0.9135) |  Loss2: (0.0000) | Acc: (67.00%) (8681/12928)
Epoch: 13 | Batch_idx: 110 |  Loss: (0.9110) |  Loss2: (0.0000) | Acc: (67.00%) (9585/14208)
Epoch: 13 | Batch_idx: 120 |  Loss: (0.9123) |  Loss2: (0.0000) | Acc: (67.00%) (10424/15488)
Epoch: 13 | Batch_idx: 130 |  Loss: (0.9129) |  Loss2: (0.0000) | Acc: (67.00%) (11286/16768)
Epoch: 13 | Batch_idx: 140 |  Loss: (0.9116) |  Loss2: (0.0000) | Acc: (67.00%) (12168/18048)
Epoch: 13 | Batch_idx: 150 |  Loss: (0.9133) |  Loss2: (0.0000) | Acc: (67.00%) (13015/19328)
Epoch: 13 | Batch_idx: 160 |  Loss: (0.9125) |  Loss2: (0.0000) | Acc: (67.00%) (13880/20608)
Epoch: 13 | Batch_idx: 170 |  Loss: (0.9139) |  Loss2: (0.0000) | Acc: (67.00%) (14725/21888)
Epoch: 13 | Batch_idx: 180 |  Loss: (0.9161) |  Loss2: (0.0000) | Acc: (67.00%) (15597/23168)
Epoch: 13 | Batch_idx: 190 |  Loss: (0.9168) |  Loss2: (0.0000) | Acc: (67.00%) (16460/24448)
Epoch: 13 | Batch_idx: 200 |  Loss: (0.9153) |  Loss2: (0.0000) | Acc: (67.00%) (17315/25728)
Epoch: 13 | Batch_idx: 210 |  Loss: (0.9141) |  Loss2: (0.0000) | Acc: (67.00%) (18184/27008)
Epoch: 13 | Batch_idx: 220 |  Loss: (0.9121) |  Loss2: (0.0000) | Acc: (67.00%) (19074/28288)
Epoch: 13 | Batch_idx: 230 |  Loss: (0.9093) |  Loss2: (0.0000) | Acc: (67.00%) (19987/29568)
Epoch: 13 | Batch_idx: 240 |  Loss: (0.9096) |  Loss2: (0.0000) | Acc: (67.00%) (20839/30848)
Epoch: 13 | Batch_idx: 250 |  Loss: (0.9081) |  Loss2: (0.0000) | Acc: (67.00%) (21712/32128)
Epoch: 13 | Batch_idx: 260 |  Loss: (0.9069) |  Loss2: (0.0000) | Acc: (67.00%) (22604/33408)
Epoch: 13 | Batch_idx: 270 |  Loss: (0.9057) |  Loss2: (0.0000) | Acc: (67.00%) (23487/34688)
Epoch: 13 | Batch_idx: 280 |  Loss: (0.9038) |  Loss2: (0.0000) | Acc: (67.00%) (24397/35968)
Epoch: 13 | Batch_idx: 290 |  Loss: (0.9053) |  Loss2: (0.0000) | Acc: (67.00%) (25235/37248)
Epoch: 13 | Batch_idx: 300 |  Loss: (0.9049) |  Loss2: (0.0000) | Acc: (67.00%) (26112/38528)
Epoch: 13 | Batch_idx: 310 |  Loss: (0.9041) |  Loss2: (0.0000) | Acc: (67.00%) (26989/39808)
Epoch: 13 | Batch_idx: 320 |  Loss: (0.9032) |  Loss2: (0.0000) | Acc: (67.00%) (27898/41088)
Epoch: 13 | Batch_idx: 330 |  Loss: (0.9030) |  Loss2: (0.0000) | Acc: (67.00%) (28770/42368)
Epoch: 13 | Batch_idx: 340 |  Loss: (0.9026) |  Loss2: (0.0000) | Acc: (67.00%) (29640/43648)
Epoch: 13 | Batch_idx: 350 |  Loss: (0.9023) |  Loss2: (0.0000) | Acc: (67.00%) (30515/44928)
Epoch: 13 | Batch_idx: 360 |  Loss: (0.9017) |  Loss2: (0.0000) | Acc: (67.00%) (31402/46208)
Epoch: 13 | Batch_idx: 370 |  Loss: (0.9025) |  Loss2: (0.0000) | Acc: (67.00%) (32246/47488)
Epoch: 13 | Batch_idx: 380 |  Loss: (0.9031) |  Loss2: (0.0000) | Acc: (67.00%) (33112/48768)
Epoch: 13 | Batch_idx: 390 |  Loss: (0.9022) |  Loss2: (0.0000) | Acc: (67.00%) (33952/50000)
# TEST : Loss: (0.9505) | Acc: (66.00%) (6620/10000)
percent tensor([0.4884, 0.5116], device='cuda:0')
percent tensor([0.4487, 0.5513], device='cuda:0')
percent tensor([0.4821, 0.5179], device='cuda:0')
percent tensor([0.4972, 0.5028], device='cuda:0')
percent tensor([0.5337, 0.4663], device='cuda:0')
percent tensor([0.5327, 0.4673], device='cuda:0')
percent tensor([0.5525, 0.4475], device='cuda:0')
percent tensor([0.8540, 0.1460], device='cuda:0')
Epoch: 14 | Batch_idx: 0 |  Loss: (0.7321) |  Loss2: (0.0000) | Acc: (76.00%) (98/128)
Epoch: 14 | Batch_idx: 10 |  Loss: (0.8448) |  Loss2: (0.0000) | Acc: (71.00%) (1002/1408)
Epoch: 14 | Batch_idx: 20 |  Loss: (0.8823) |  Loss2: (0.0000) | Acc: (69.00%) (1856/2688)
Epoch: 14 | Batch_idx: 30 |  Loss: (0.8687) |  Loss2: (0.0000) | Acc: (69.00%) (2754/3968)
Epoch: 14 | Batch_idx: 40 |  Loss: (0.8793) |  Loss2: (0.0000) | Acc: (68.00%) (3603/5248)
Epoch: 14 | Batch_idx: 50 |  Loss: (0.8729) |  Loss2: (0.0000) | Acc: (68.00%) (4501/6528)
Epoch: 14 | Batch_idx: 60 |  Loss: (0.8701) |  Loss2: (0.0000) | Acc: (68.00%) (5365/7808)
Epoch: 14 | Batch_idx: 70 |  Loss: (0.8684) |  Loss2: (0.0000) | Acc: (68.00%) (6257/9088)
Epoch: 14 | Batch_idx: 80 |  Loss: (0.8661) |  Loss2: (0.0000) | Acc: (69.00%) (7154/10368)
Epoch: 14 | Batch_idx: 90 |  Loss: (0.8633) |  Loss2: (0.0000) | Acc: (69.00%) (8059/11648)
Epoch: 14 | Batch_idx: 100 |  Loss: (0.8637) |  Loss2: (0.0000) | Acc: (69.00%) (8939/12928)
Epoch: 14 | Batch_idx: 110 |  Loss: (0.8611) |  Loss2: (0.0000) | Acc: (69.00%) (9852/14208)
Epoch: 14 | Batch_idx: 120 |  Loss: (0.8614) |  Loss2: (0.0000) | Acc: (69.00%) (10745/15488)
Epoch: 14 | Batch_idx: 130 |  Loss: (0.8585) |  Loss2: (0.0000) | Acc: (69.00%) (11653/16768)
Epoch: 14 | Batch_idx: 140 |  Loss: (0.8596) |  Loss2: (0.0000) | Acc: (69.00%) (12535/18048)
Epoch: 14 | Batch_idx: 150 |  Loss: (0.8617) |  Loss2: (0.0000) | Acc: (69.00%) (13419/19328)
Epoch: 14 | Batch_idx: 160 |  Loss: (0.8624) |  Loss2: (0.0000) | Acc: (69.00%) (14298/20608)
Epoch: 14 | Batch_idx: 170 |  Loss: (0.8620) |  Loss2: (0.0000) | Acc: (69.00%) (15186/21888)
Epoch: 14 | Batch_idx: 180 |  Loss: (0.8605) |  Loss2: (0.0000) | Acc: (69.00%) (16093/23168)
Epoch: 14 | Batch_idx: 190 |  Loss: (0.8610) |  Loss2: (0.0000) | Acc: (69.00%) (16990/24448)
Epoch: 14 | Batch_idx: 200 |  Loss: (0.8593) |  Loss2: (0.0000) | Acc: (69.00%) (17899/25728)
Epoch: 14 | Batch_idx: 210 |  Loss: (0.8592) |  Loss2: (0.0000) | Acc: (69.00%) (18791/27008)
Epoch: 14 | Batch_idx: 220 |  Loss: (0.8601) |  Loss2: (0.0000) | Acc: (69.00%) (19670/28288)
Epoch: 14 | Batch_idx: 230 |  Loss: (0.8591) |  Loss2: (0.0000) | Acc: (69.00%) (20591/29568)
Epoch: 14 | Batch_idx: 240 |  Loss: (0.8582) |  Loss2: (0.0000) | Acc: (69.00%) (21491/30848)
Epoch: 14 | Batch_idx: 250 |  Loss: (0.8577) |  Loss2: (0.0000) | Acc: (69.00%) (22392/32128)
Epoch: 14 | Batch_idx: 260 |  Loss: (0.8574) |  Loss2: (0.0000) | Acc: (69.00%) (23286/33408)
Epoch: 14 | Batch_idx: 270 |  Loss: (0.8575) |  Loss2: (0.0000) | Acc: (69.00%) (24163/34688)
Epoch: 14 | Batch_idx: 280 |  Loss: (0.8571) |  Loss2: (0.0000) | Acc: (69.00%) (25048/35968)
Epoch: 14 | Batch_idx: 290 |  Loss: (0.8567) |  Loss2: (0.0000) | Acc: (69.00%) (25949/37248)
Epoch: 14 | Batch_idx: 300 |  Loss: (0.8568) |  Loss2: (0.0000) | Acc: (69.00%) (26838/38528)
Epoch: 14 | Batch_idx: 310 |  Loss: (0.8569) |  Loss2: (0.0000) | Acc: (69.00%) (27734/39808)
Epoch: 14 | Batch_idx: 320 |  Loss: (0.8568) |  Loss2: (0.0000) | Acc: (69.00%) (28629/41088)
Epoch: 14 | Batch_idx: 330 |  Loss: (0.8553) |  Loss2: (0.0000) | Acc: (69.00%) (29538/42368)
Epoch: 14 | Batch_idx: 340 |  Loss: (0.8557) |  Loss2: (0.0000) | Acc: (69.00%) (30445/43648)
Epoch: 14 | Batch_idx: 350 |  Loss: (0.8554) |  Loss2: (0.0000) | Acc: (69.00%) (31343/44928)
Epoch: 14 | Batch_idx: 360 |  Loss: (0.8553) |  Loss2: (0.0000) | Acc: (69.00%) (32235/46208)
Epoch: 14 | Batch_idx: 370 |  Loss: (0.8538) |  Loss2: (0.0000) | Acc: (69.00%) (33148/47488)
Epoch: 14 | Batch_idx: 380 |  Loss: (0.8532) |  Loss2: (0.0000) | Acc: (69.00%) (34044/48768)
Epoch: 14 | Batch_idx: 390 |  Loss: (0.8522) |  Loss2: (0.0000) | Acc: (69.00%) (34912/50000)
# TEST : Loss: (0.8629) | Acc: (69.00%) (6914/10000)
percent tensor([0.4885, 0.5115], device='cuda:0')
percent tensor([0.4491, 0.5509], device='cuda:0')
percent tensor([0.4813, 0.5187], device='cuda:0')
percent tensor([0.4973, 0.5027], device='cuda:0')
percent tensor([0.5342, 0.4658], device='cuda:0')
percent tensor([0.5319, 0.4681], device='cuda:0')
percent tensor([0.5518, 0.4482], device='cuda:0')
percent tensor([0.8412, 0.1588], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 15 | Batch_idx: 0 |  Loss: (0.7550) |  Loss2: (0.0000) | Acc: (82.00%) (105/128)
Epoch: 15 | Batch_idx: 10 |  Loss: (0.8240) |  Loss2: (0.0000) | Acc: (70.00%) (996/1408)
Epoch: 15 | Batch_idx: 20 |  Loss: (0.8816) |  Loss2: (0.0000) | Acc: (68.00%) (1837/2688)
Epoch: 15 | Batch_idx: 30 |  Loss: (0.9215) |  Loss2: (0.0000) | Acc: (67.00%) (2661/3968)
Epoch: 15 | Batch_idx: 40 |  Loss: (0.9410) |  Loss2: (0.0000) | Acc: (66.00%) (3470/5248)
Epoch: 15 | Batch_idx: 50 |  Loss: (0.9525) |  Loss2: (0.0000) | Acc: (65.00%) (4308/6528)
Epoch: 15 | Batch_idx: 60 |  Loss: (0.9539) |  Loss2: (0.0000) | Acc: (66.00%) (5155/7808)
Epoch: 15 | Batch_idx: 70 |  Loss: (0.9548) |  Loss2: (0.0000) | Acc: (65.00%) (5985/9088)
Epoch: 15 | Batch_idx: 80 |  Loss: (0.9572) |  Loss2: (0.0000) | Acc: (65.00%) (6821/10368)
Epoch: 15 | Batch_idx: 90 |  Loss: (0.9563) |  Loss2: (0.0000) | Acc: (65.00%) (7674/11648)
Epoch: 15 | Batch_idx: 100 |  Loss: (0.9602) |  Loss2: (0.0000) | Acc: (65.00%) (8519/12928)
Epoch: 15 | Batch_idx: 110 |  Loss: (0.9583) |  Loss2: (0.0000) | Acc: (65.00%) (9360/14208)
Epoch: 15 | Batch_idx: 120 |  Loss: (0.9555) |  Loss2: (0.0000) | Acc: (65.00%) (10215/15488)
Epoch: 15 | Batch_idx: 130 |  Loss: (0.9519) |  Loss2: (0.0000) | Acc: (66.00%) (11074/16768)
Epoch: 15 | Batch_idx: 140 |  Loss: (0.9528) |  Loss2: (0.0000) | Acc: (65.00%) (11902/18048)
Epoch: 15 | Batch_idx: 150 |  Loss: (0.9500) |  Loss2: (0.0000) | Acc: (66.00%) (12764/19328)
Epoch: 15 | Batch_idx: 160 |  Loss: (0.9504) |  Loss2: (0.0000) | Acc: (65.00%) (13590/20608)
Epoch: 15 | Batch_idx: 170 |  Loss: (0.9467) |  Loss2: (0.0000) | Acc: (66.00%) (14472/21888)
Epoch: 15 | Batch_idx: 180 |  Loss: (0.9441) |  Loss2: (0.0000) | Acc: (66.00%) (15347/23168)
Epoch: 15 | Batch_idx: 190 |  Loss: (0.9451) |  Loss2: (0.0000) | Acc: (66.00%) (16196/24448)
Epoch: 15 | Batch_idx: 200 |  Loss: (0.9416) |  Loss2: (0.0000) | Acc: (66.00%) (17084/25728)
Epoch: 15 | Batch_idx: 210 |  Loss: (0.9389) |  Loss2: (0.0000) | Acc: (66.00%) (17968/27008)
Epoch: 15 | Batch_idx: 220 |  Loss: (0.9377) |  Loss2: (0.0000) | Acc: (66.00%) (18811/28288)
Epoch: 15 | Batch_idx: 230 |  Loss: (0.9361) |  Loss2: (0.0000) | Acc: (66.00%) (19677/29568)
Epoch: 15 | Batch_idx: 240 |  Loss: (0.9349) |  Loss2: (0.0000) | Acc: (66.00%) (20547/30848)
Epoch: 15 | Batch_idx: 250 |  Loss: (0.9346) |  Loss2: (0.0000) | Acc: (66.00%) (21393/32128)
Epoch: 15 | Batch_idx: 260 |  Loss: (0.9337) |  Loss2: (0.0000) | Acc: (66.00%) (22258/33408)
Epoch: 15 | Batch_idx: 270 |  Loss: (0.9353) |  Loss2: (0.0000) | Acc: (66.00%) (23086/34688)
Epoch: 15 | Batch_idx: 280 |  Loss: (0.9338) |  Loss2: (0.0000) | Acc: (66.00%) (23957/35968)
Epoch: 15 | Batch_idx: 290 |  Loss: (0.9313) |  Loss2: (0.0000) | Acc: (66.00%) (24856/37248)
Epoch: 15 | Batch_idx: 300 |  Loss: (0.9304) |  Loss2: (0.0000) | Acc: (66.00%) (25714/38528)
Epoch: 15 | Batch_idx: 310 |  Loss: (0.9287) |  Loss2: (0.0000) | Acc: (66.00%) (26592/39808)
Epoch: 15 | Batch_idx: 320 |  Loss: (0.9267) |  Loss2: (0.0000) | Acc: (66.00%) (27478/41088)
Epoch: 15 | Batch_idx: 330 |  Loss: (0.9252) |  Loss2: (0.0000) | Acc: (66.00%) (28354/42368)
Epoch: 15 | Batch_idx: 340 |  Loss: (0.9234) |  Loss2: (0.0000) | Acc: (67.00%) (29256/43648)
Epoch: 15 | Batch_idx: 350 |  Loss: (0.9226) |  Loss2: (0.0000) | Acc: (67.00%) (30128/44928)
Epoch: 15 | Batch_idx: 360 |  Loss: (0.9194) |  Loss2: (0.0000) | Acc: (67.00%) (31057/46208)
Epoch: 15 | Batch_idx: 370 |  Loss: (0.9185) |  Loss2: (0.0000) | Acc: (67.00%) (31934/47488)
Epoch: 15 | Batch_idx: 380 |  Loss: (0.9180) |  Loss2: (0.0000) | Acc: (67.00%) (32812/48768)
Epoch: 15 | Batch_idx: 390 |  Loss: (0.9178) |  Loss2: (0.0000) | Acc: (67.00%) (33641/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_015.pth.tar'
# TEST : Loss: (0.9096) | Acc: (67.00%) (6792/10000)
percent tensor([0.4960, 0.5040], device='cuda:0')
percent tensor([0.4375, 0.5625], device='cuda:0')
percent tensor([0.4828, 0.5172], device='cuda:0')
percent tensor([0.4930, 0.5070], device='cuda:0')
percent tensor([0.4970, 0.5030], device='cuda:0')
percent tensor([0.5742, 0.4258], device='cuda:0')
percent tensor([0.5653, 0.4347], device='cuda:0')
percent tensor([0.9031, 0.0969], device='cuda:0')
Epoch: 16 | Batch_idx: 0 |  Loss: (1.0610) |  Loss2: (0.0000) | Acc: (61.00%) (79/128)
Epoch: 16 | Batch_idx: 10 |  Loss: (0.9016) |  Loss2: (0.0000) | Acc: (68.00%) (960/1408)
Epoch: 16 | Batch_idx: 20 |  Loss: (0.8713) |  Loss2: (0.0000) | Acc: (69.00%) (1873/2688)
Epoch: 16 | Batch_idx: 30 |  Loss: (0.8832) |  Loss2: (0.0000) | Acc: (68.00%) (2732/3968)
Epoch: 16 | Batch_idx: 40 |  Loss: (0.8733) |  Loss2: (0.0000) | Acc: (69.00%) (3627/5248)
Epoch: 16 | Batch_idx: 50 |  Loss: (0.8666) |  Loss2: (0.0000) | Acc: (69.00%) (4533/6528)
Epoch: 16 | Batch_idx: 60 |  Loss: (0.8604) |  Loss2: (0.0000) | Acc: (69.00%) (5433/7808)
Epoch: 16 | Batch_idx: 70 |  Loss: (0.8558) |  Loss2: (0.0000) | Acc: (69.00%) (6338/9088)
Epoch: 16 | Batch_idx: 80 |  Loss: (0.8626) |  Loss2: (0.0000) | Acc: (69.00%) (7200/10368)
Epoch: 16 | Batch_idx: 90 |  Loss: (0.8616) |  Loss2: (0.0000) | Acc: (69.00%) (8094/11648)
Epoch: 16 | Batch_idx: 100 |  Loss: (0.8620) |  Loss2: (0.0000) | Acc: (69.00%) (8978/12928)
Epoch: 16 | Batch_idx: 110 |  Loss: (0.8588) |  Loss2: (0.0000) | Acc: (69.00%) (9882/14208)
Epoch: 16 | Batch_idx: 120 |  Loss: (0.8574) |  Loss2: (0.0000) | Acc: (69.00%) (10762/15488)
Epoch: 16 | Batch_idx: 130 |  Loss: (0.8589) |  Loss2: (0.0000) | Acc: (69.00%) (11644/16768)
Epoch: 16 | Batch_idx: 140 |  Loss: (0.8556) |  Loss2: (0.0000) | Acc: (69.00%) (12535/18048)
Epoch: 16 | Batch_idx: 150 |  Loss: (0.8574) |  Loss2: (0.0000) | Acc: (69.00%) (13414/19328)
Epoch: 16 | Batch_idx: 160 |  Loss: (0.8572) |  Loss2: (0.0000) | Acc: (69.00%) (14290/20608)
Epoch: 16 | Batch_idx: 170 |  Loss: (0.8575) |  Loss2: (0.0000) | Acc: (69.00%) (15180/21888)
Epoch: 16 | Batch_idx: 180 |  Loss: (0.8569) |  Loss2: (0.0000) | Acc: (69.00%) (16088/23168)
Epoch: 16 | Batch_idx: 190 |  Loss: (0.8554) |  Loss2: (0.0000) | Acc: (69.00%) (16999/24448)
Epoch: 16 | Batch_idx: 200 |  Loss: (0.8545) |  Loss2: (0.0000) | Acc: (69.00%) (17906/25728)
Epoch: 16 | Batch_idx: 210 |  Loss: (0.8559) |  Loss2: (0.0000) | Acc: (69.00%) (18769/27008)
Epoch: 16 | Batch_idx: 220 |  Loss: (0.8533) |  Loss2: (0.0000) | Acc: (69.00%) (19695/28288)
Epoch: 16 | Batch_idx: 230 |  Loss: (0.8517) |  Loss2: (0.0000) | Acc: (69.00%) (20592/29568)
Epoch: 16 | Batch_idx: 240 |  Loss: (0.8515) |  Loss2: (0.0000) | Acc: (69.00%) (21488/30848)
Epoch: 16 | Batch_idx: 250 |  Loss: (0.8498) |  Loss2: (0.0000) | Acc: (69.00%) (22409/32128)
Epoch: 16 | Batch_idx: 260 |  Loss: (0.8508) |  Loss2: (0.0000) | Acc: (69.00%) (23300/33408)
Epoch: 16 | Batch_idx: 270 |  Loss: (0.8499) |  Loss2: (0.0000) | Acc: (69.00%) (24192/34688)
Epoch: 16 | Batch_idx: 280 |  Loss: (0.8495) |  Loss2: (0.0000) | Acc: (69.00%) (25112/35968)
Epoch: 16 | Batch_idx: 290 |  Loss: (0.8497) |  Loss2: (0.0000) | Acc: (69.00%) (25988/37248)
Epoch: 16 | Batch_idx: 300 |  Loss: (0.8492) |  Loss2: (0.0000) | Acc: (69.00%) (26877/38528)
Epoch: 16 | Batch_idx: 310 |  Loss: (0.8479) |  Loss2: (0.0000) | Acc: (69.00%) (27781/39808)
Epoch: 16 | Batch_idx: 320 |  Loss: (0.8487) |  Loss2: (0.0000) | Acc: (69.00%) (28665/41088)
Epoch: 16 | Batch_idx: 330 |  Loss: (0.8474) |  Loss2: (0.0000) | Acc: (69.00%) (29584/42368)
Epoch: 16 | Batch_idx: 340 |  Loss: (0.8470) |  Loss2: (0.0000) | Acc: (69.00%) (30482/43648)
Epoch: 16 | Batch_idx: 350 |  Loss: (0.8484) |  Loss2: (0.0000) | Acc: (69.00%) (31343/44928)
Epoch: 16 | Batch_idx: 360 |  Loss: (0.8489) |  Loss2: (0.0000) | Acc: (69.00%) (32245/46208)
Epoch: 16 | Batch_idx: 370 |  Loss: (0.8495) |  Loss2: (0.0000) | Acc: (69.00%) (33136/47488)
Epoch: 16 | Batch_idx: 380 |  Loss: (0.8492) |  Loss2: (0.0000) | Acc: (69.00%) (34030/48768)
Epoch: 16 | Batch_idx: 390 |  Loss: (0.8499) |  Loss2: (0.0000) | Acc: (69.00%) (34893/50000)
# TEST : Loss: (0.8649) | Acc: (69.00%) (6956/10000)
percent tensor([0.4962, 0.5038], device='cuda:0')
percent tensor([0.4433, 0.5567], device='cuda:0')
percent tensor([0.4839, 0.5161], device='cuda:0')
percent tensor([0.4964, 0.5036], device='cuda:0')
percent tensor([0.4877, 0.5123], device='cuda:0')
percent tensor([0.5954, 0.4046], device='cuda:0')
percent tensor([0.5768, 0.4232], device='cuda:0')
percent tensor([0.9413, 0.0587], device='cuda:0')
Epoch: 17 | Batch_idx: 0 |  Loss: (0.7388) |  Loss2: (0.0000) | Acc: (75.00%) (96/128)
Epoch: 17 | Batch_idx: 10 |  Loss: (0.8506) |  Loss2: (0.0000) | Acc: (70.00%) (990/1408)
Epoch: 17 | Batch_idx: 20 |  Loss: (0.8161) |  Loss2: (0.0000) | Acc: (72.00%) (1936/2688)
Epoch: 17 | Batch_idx: 30 |  Loss: (0.8210) |  Loss2: (0.0000) | Acc: (71.00%) (2830/3968)
Epoch: 17 | Batch_idx: 40 |  Loss: (0.8211) |  Loss2: (0.0000) | Acc: (71.00%) (3735/5248)
Epoch: 17 | Batch_idx: 50 |  Loss: (0.8220) |  Loss2: (0.0000) | Acc: (71.00%) (4645/6528)
Epoch: 17 | Batch_idx: 60 |  Loss: (0.8235) |  Loss2: (0.0000) | Acc: (71.00%) (5547/7808)
Epoch: 17 | Batch_idx: 70 |  Loss: (0.8286) |  Loss2: (0.0000) | Acc: (70.00%) (6447/9088)
Epoch: 17 | Batch_idx: 80 |  Loss: (0.8308) |  Loss2: (0.0000) | Acc: (70.00%) (7355/10368)
Epoch: 17 | Batch_idx: 90 |  Loss: (0.8297) |  Loss2: (0.0000) | Acc: (71.00%) (8277/11648)
Epoch: 17 | Batch_idx: 100 |  Loss: (0.8316) |  Loss2: (0.0000) | Acc: (70.00%) (9159/12928)
Epoch: 17 | Batch_idx: 110 |  Loss: (0.8356) |  Loss2: (0.0000) | Acc: (70.00%) (10049/14208)
Epoch: 17 | Batch_idx: 120 |  Loss: (0.8334) |  Loss2: (0.0000) | Acc: (70.00%) (10942/15488)
Epoch: 17 | Batch_idx: 130 |  Loss: (0.8369) |  Loss2: (0.0000) | Acc: (70.00%) (11818/16768)
Epoch: 17 | Batch_idx: 140 |  Loss: (0.8406) |  Loss2: (0.0000) | Acc: (70.00%) (12708/18048)
Epoch: 17 | Batch_idx: 150 |  Loss: (0.8375) |  Loss2: (0.0000) | Acc: (70.00%) (13636/19328)
Epoch: 17 | Batch_idx: 160 |  Loss: (0.8375) |  Loss2: (0.0000) | Acc: (70.00%) (14548/20608)
Epoch: 17 | Batch_idx: 170 |  Loss: (0.8376) |  Loss2: (0.0000) | Acc: (70.00%) (15446/21888)
Epoch: 17 | Batch_idx: 180 |  Loss: (0.8370) |  Loss2: (0.0000) | Acc: (70.00%) (16345/23168)
Epoch: 17 | Batch_idx: 190 |  Loss: (0.8378) |  Loss2: (0.0000) | Acc: (70.00%) (17251/24448)
Epoch: 17 | Batch_idx: 200 |  Loss: (0.8394) |  Loss2: (0.0000) | Acc: (70.00%) (18146/25728)
Epoch: 17 | Batch_idx: 210 |  Loss: (0.8379) |  Loss2: (0.0000) | Acc: (70.00%) (19072/27008)
Epoch: 17 | Batch_idx: 220 |  Loss: (0.8390) |  Loss2: (0.0000) | Acc: (70.00%) (19972/28288)
Epoch: 17 | Batch_idx: 230 |  Loss: (0.8370) |  Loss2: (0.0000) | Acc: (70.00%) (20900/29568)
Epoch: 17 | Batch_idx: 240 |  Loss: (0.8341) |  Loss2: (0.0000) | Acc: (70.00%) (21828/30848)
Epoch: 17 | Batch_idx: 250 |  Loss: (0.8321) |  Loss2: (0.0000) | Acc: (70.00%) (22753/32128)
Epoch: 17 | Batch_idx: 260 |  Loss: (0.8313) |  Loss2: (0.0000) | Acc: (70.00%) (23666/33408)
Epoch: 17 | Batch_idx: 270 |  Loss: (0.8325) |  Loss2: (0.0000) | Acc: (70.00%) (24544/34688)
Epoch: 17 | Batch_idx: 280 |  Loss: (0.8310) |  Loss2: (0.0000) | Acc: (70.00%) (25472/35968)
Epoch: 17 | Batch_idx: 290 |  Loss: (0.8306) |  Loss2: (0.0000) | Acc: (70.00%) (26354/37248)
Epoch: 17 | Batch_idx: 300 |  Loss: (0.8309) |  Loss2: (0.0000) | Acc: (70.00%) (27257/38528)
Epoch: 17 | Batch_idx: 310 |  Loss: (0.8308) |  Loss2: (0.0000) | Acc: (70.00%) (28158/39808)
Epoch: 17 | Batch_idx: 320 |  Loss: (0.8305) |  Loss2: (0.0000) | Acc: (70.00%) (29061/41088)
Epoch: 17 | Batch_idx: 330 |  Loss: (0.8318) |  Loss2: (0.0000) | Acc: (70.00%) (29930/42368)
Epoch: 17 | Batch_idx: 340 |  Loss: (0.8304) |  Loss2: (0.0000) | Acc: (70.00%) (30856/43648)
Epoch: 17 | Batch_idx: 350 |  Loss: (0.8306) |  Loss2: (0.0000) | Acc: (70.00%) (31762/44928)
Epoch: 17 | Batch_idx: 360 |  Loss: (0.8317) |  Loss2: (0.0000) | Acc: (70.00%) (32640/46208)
Epoch: 17 | Batch_idx: 370 |  Loss: (0.8315) |  Loss2: (0.0000) | Acc: (70.00%) (33553/47488)
Epoch: 17 | Batch_idx: 380 |  Loss: (0.8296) |  Loss2: (0.0000) | Acc: (70.00%) (34488/48768)
Epoch: 17 | Batch_idx: 390 |  Loss: (0.8292) |  Loss2: (0.0000) | Acc: (70.00%) (35375/50000)
# TEST : Loss: (0.8510) | Acc: (70.00%) (7021/10000)
percent tensor([0.4954, 0.5046], device='cuda:0')
percent tensor([0.4428, 0.5572], device='cuda:0')
percent tensor([0.4851, 0.5149], device='cuda:0')
percent tensor([0.5004, 0.4996], device='cuda:0')
percent tensor([0.4885, 0.5115], device='cuda:0')
percent tensor([0.6042, 0.3958], device='cuda:0')
percent tensor([0.5838, 0.4162], device='cuda:0')
percent tensor([0.9582, 0.0418], device='cuda:0')
Epoch: 18 | Batch_idx: 0 |  Loss: (0.7718) |  Loss2: (0.0000) | Acc: (71.00%) (92/128)
Epoch: 18 | Batch_idx: 10 |  Loss: (0.8083) |  Loss2: (0.0000) | Acc: (70.00%) (995/1408)
Epoch: 18 | Batch_idx: 20 |  Loss: (0.8180) |  Loss2: (0.0000) | Acc: (70.00%) (1898/2688)
Epoch: 18 | Batch_idx: 30 |  Loss: (0.8259) |  Loss2: (0.0000) | Acc: (70.00%) (2787/3968)
Epoch: 18 | Batch_idx: 40 |  Loss: (0.8312) |  Loss2: (0.0000) | Acc: (70.00%) (3677/5248)
Epoch: 18 | Batch_idx: 50 |  Loss: (0.8329) |  Loss2: (0.0000) | Acc: (69.00%) (4562/6528)
Epoch: 18 | Batch_idx: 60 |  Loss: (0.8348) |  Loss2: (0.0000) | Acc: (69.00%) (5451/7808)
Epoch: 18 | Batch_idx: 70 |  Loss: (0.8322) |  Loss2: (0.0000) | Acc: (70.00%) (6367/9088)
Epoch: 18 | Batch_idx: 80 |  Loss: (0.8334) |  Loss2: (0.0000) | Acc: (70.00%) (7270/10368)
Epoch: 18 | Batch_idx: 90 |  Loss: (0.8301) |  Loss2: (0.0000) | Acc: (70.00%) (8176/11648)
Epoch: 18 | Batch_idx: 100 |  Loss: (0.8296) |  Loss2: (0.0000) | Acc: (70.00%) (9063/12928)
Epoch: 18 | Batch_idx: 110 |  Loss: (0.8283) |  Loss2: (0.0000) | Acc: (70.00%) (9966/14208)
Epoch: 18 | Batch_idx: 120 |  Loss: (0.8283) |  Loss2: (0.0000) | Acc: (70.00%) (10856/15488)
Epoch: 18 | Batch_idx: 130 |  Loss: (0.8261) |  Loss2: (0.0000) | Acc: (70.00%) (11781/16768)
Epoch: 18 | Batch_idx: 140 |  Loss: (0.8255) |  Loss2: (0.0000) | Acc: (70.00%) (12708/18048)
Epoch: 18 | Batch_idx: 150 |  Loss: (0.8213) |  Loss2: (0.0000) | Acc: (70.00%) (13662/19328)
Epoch: 18 | Batch_idx: 160 |  Loss: (0.8219) |  Loss2: (0.0000) | Acc: (70.00%) (14594/20608)
Epoch: 18 | Batch_idx: 170 |  Loss: (0.8212) |  Loss2: (0.0000) | Acc: (70.00%) (15490/21888)
Epoch: 18 | Batch_idx: 180 |  Loss: (0.8193) |  Loss2: (0.0000) | Acc: (70.00%) (16418/23168)
Epoch: 18 | Batch_idx: 190 |  Loss: (0.8176) |  Loss2: (0.0000) | Acc: (70.00%) (17322/24448)
Epoch: 18 | Batch_idx: 200 |  Loss: (0.8177) |  Loss2: (0.0000) | Acc: (70.00%) (18212/25728)
Epoch: 18 | Batch_idx: 210 |  Loss: (0.8169) |  Loss2: (0.0000) | Acc: (70.00%) (19119/27008)
Epoch: 18 | Batch_idx: 220 |  Loss: (0.8161) |  Loss2: (0.0000) | Acc: (70.00%) (20025/28288)
Epoch: 18 | Batch_idx: 230 |  Loss: (0.8157) |  Loss2: (0.0000) | Acc: (70.00%) (20930/29568)
Epoch: 18 | Batch_idx: 240 |  Loss: (0.8166) |  Loss2: (0.0000) | Acc: (70.00%) (21812/30848)
Epoch: 18 | Batch_idx: 250 |  Loss: (0.8165) |  Loss2: (0.0000) | Acc: (70.00%) (22712/32128)
Epoch: 18 | Batch_idx: 260 |  Loss: (0.8169) |  Loss2: (0.0000) | Acc: (70.00%) (23613/33408)
Epoch: 18 | Batch_idx: 270 |  Loss: (0.8177) |  Loss2: (0.0000) | Acc: (70.00%) (24513/34688)
Epoch: 18 | Batch_idx: 280 |  Loss: (0.8181) |  Loss2: (0.0000) | Acc: (70.00%) (25413/35968)
Epoch: 18 | Batch_idx: 290 |  Loss: (0.8182) |  Loss2: (0.0000) | Acc: (70.00%) (26314/37248)
Epoch: 18 | Batch_idx: 300 |  Loss: (0.8187) |  Loss2: (0.0000) | Acc: (70.00%) (27219/38528)
Epoch: 18 | Batch_idx: 310 |  Loss: (0.8182) |  Loss2: (0.0000) | Acc: (70.00%) (28133/39808)
Epoch: 18 | Batch_idx: 320 |  Loss: (0.8176) |  Loss2: (0.0000) | Acc: (70.00%) (29057/41088)
Epoch: 18 | Batch_idx: 330 |  Loss: (0.8184) |  Loss2: (0.0000) | Acc: (70.00%) (29960/42368)
Epoch: 18 | Batch_idx: 340 |  Loss: (0.8182) |  Loss2: (0.0000) | Acc: (70.00%) (30868/43648)
Epoch: 18 | Batch_idx: 350 |  Loss: (0.8179) |  Loss2: (0.0000) | Acc: (70.00%) (31797/44928)
Epoch: 18 | Batch_idx: 360 |  Loss: (0.8182) |  Loss2: (0.0000) | Acc: (70.00%) (32690/46208)
Epoch: 18 | Batch_idx: 370 |  Loss: (0.8183) |  Loss2: (0.0000) | Acc: (70.00%) (33605/47488)
Epoch: 18 | Batch_idx: 380 |  Loss: (0.8173) |  Loss2: (0.0000) | Acc: (70.00%) (34516/48768)
Epoch: 18 | Batch_idx: 390 |  Loss: (0.8173) |  Loss2: (0.0000) | Acc: (70.00%) (35391/50000)
# TEST : Loss: (0.8387) | Acc: (70.00%) (7037/10000)
percent tensor([0.4941, 0.5059], device='cuda:0')
percent tensor([0.4454, 0.5546], device='cuda:0')
percent tensor([0.4871, 0.5129], device='cuda:0')
percent tensor([0.5058, 0.4942], device='cuda:0')
percent tensor([0.4930, 0.5070], device='cuda:0')
percent tensor([0.6052, 0.3948], device='cuda:0')
percent tensor([0.5877, 0.4123], device='cuda:0')
percent tensor([0.9677, 0.0323], device='cuda:0')
Epoch: 19 | Batch_idx: 0 |  Loss: (0.8161) |  Loss2: (0.0000) | Acc: (67.00%) (87/128)
Epoch: 19 | Batch_idx: 10 |  Loss: (0.7855) |  Loss2: (0.0000) | Acc: (71.00%) (1012/1408)
Epoch: 19 | Batch_idx: 20 |  Loss: (0.8148) |  Loss2: (0.0000) | Acc: (71.00%) (1915/2688)
Epoch: 19 | Batch_idx: 30 |  Loss: (0.8147) |  Loss2: (0.0000) | Acc: (71.00%) (2823/3968)
Epoch: 19 | Batch_idx: 40 |  Loss: (0.8135) |  Loss2: (0.0000) | Acc: (71.00%) (3746/5248)
Epoch: 19 | Batch_idx: 50 |  Loss: (0.8173) |  Loss2: (0.0000) | Acc: (71.00%) (4658/6528)
Epoch: 19 | Batch_idx: 60 |  Loss: (0.8148) |  Loss2: (0.0000) | Acc: (71.00%) (5575/7808)
Epoch: 19 | Batch_idx: 70 |  Loss: (0.8133) |  Loss2: (0.0000) | Acc: (71.00%) (6484/9088)
Epoch: 19 | Batch_idx: 80 |  Loss: (0.8156) |  Loss2: (0.0000) | Acc: (71.00%) (7385/10368)
Epoch: 19 | Batch_idx: 90 |  Loss: (0.8200) |  Loss2: (0.0000) | Acc: (70.00%) (8270/11648)
Epoch: 19 | Batch_idx: 100 |  Loss: (0.8200) |  Loss2: (0.0000) | Acc: (70.00%) (9165/12928)
Epoch: 19 | Batch_idx: 110 |  Loss: (0.8188) |  Loss2: (0.0000) | Acc: (71.00%) (10091/14208)
Epoch: 19 | Batch_idx: 120 |  Loss: (0.8150) |  Loss2: (0.0000) | Acc: (71.00%) (11023/15488)
Epoch: 19 | Batch_idx: 130 |  Loss: (0.8132) |  Loss2: (0.0000) | Acc: (71.00%) (11936/16768)
Epoch: 19 | Batch_idx: 140 |  Loss: (0.8130) |  Loss2: (0.0000) | Acc: (71.00%) (12840/18048)
Epoch: 19 | Batch_idx: 150 |  Loss: (0.8120) |  Loss2: (0.0000) | Acc: (71.00%) (13762/19328)
Epoch: 19 | Batch_idx: 160 |  Loss: (0.8124) |  Loss2: (0.0000) | Acc: (71.00%) (14677/20608)
Epoch: 19 | Batch_idx: 170 |  Loss: (0.8118) |  Loss2: (0.0000) | Acc: (71.00%) (15600/21888)
Epoch: 19 | Batch_idx: 180 |  Loss: (0.8143) |  Loss2: (0.0000) | Acc: (71.00%) (16482/23168)
Epoch: 19 | Batch_idx: 190 |  Loss: (0.8157) |  Loss2: (0.0000) | Acc: (71.00%) (17376/24448)
Epoch: 19 | Batch_idx: 200 |  Loss: (0.8141) |  Loss2: (0.0000) | Acc: (71.00%) (18304/25728)
Epoch: 19 | Batch_idx: 210 |  Loss: (0.8154) |  Loss2: (0.0000) | Acc: (71.00%) (19201/27008)
Epoch: 19 | Batch_idx: 220 |  Loss: (0.8173) |  Loss2: (0.0000) | Acc: (71.00%) (20089/28288)
Epoch: 19 | Batch_idx: 230 |  Loss: (0.8167) |  Loss2: (0.0000) | Acc: (70.00%) (20992/29568)
Epoch: 19 | Batch_idx: 240 |  Loss: (0.8164) |  Loss2: (0.0000) | Acc: (71.00%) (21914/30848)
Epoch: 19 | Batch_idx: 250 |  Loss: (0.8177) |  Loss2: (0.0000) | Acc: (71.00%) (22819/32128)
Epoch: 19 | Batch_idx: 260 |  Loss: (0.8177) |  Loss2: (0.0000) | Acc: (71.00%) (23732/33408)
Epoch: 19 | Batch_idx: 270 |  Loss: (0.8169) |  Loss2: (0.0000) | Acc: (71.00%) (24646/34688)
Epoch: 19 | Batch_idx: 280 |  Loss: (0.8180) |  Loss2: (0.0000) | Acc: (71.00%) (25552/35968)
Epoch: 19 | Batch_idx: 290 |  Loss: (0.8166) |  Loss2: (0.0000) | Acc: (71.00%) (26493/37248)
Epoch: 19 | Batch_idx: 300 |  Loss: (0.8159) |  Loss2: (0.0000) | Acc: (71.00%) (27399/38528)
Epoch: 19 | Batch_idx: 310 |  Loss: (0.8143) |  Loss2: (0.0000) | Acc: (71.00%) (28344/39808)
Epoch: 19 | Batch_idx: 320 |  Loss: (0.8130) |  Loss2: (0.0000) | Acc: (71.00%) (29290/41088)
Epoch: 19 | Batch_idx: 330 |  Loss: (0.8135) |  Loss2: (0.0000) | Acc: (71.00%) (30202/42368)
Epoch: 19 | Batch_idx: 340 |  Loss: (0.8138) |  Loss2: (0.0000) | Acc: (71.00%) (31113/43648)
Epoch: 19 | Batch_idx: 350 |  Loss: (0.8141) |  Loss2: (0.0000) | Acc: (71.00%) (32029/44928)
Epoch: 19 | Batch_idx: 360 |  Loss: (0.8143) |  Loss2: (0.0000) | Acc: (71.00%) (32935/46208)
Epoch: 19 | Batch_idx: 370 |  Loss: (0.8141) |  Loss2: (0.0000) | Acc: (71.00%) (33838/47488)
Epoch: 19 | Batch_idx: 380 |  Loss: (0.8140) |  Loss2: (0.0000) | Acc: (71.00%) (34741/48768)
Epoch: 19 | Batch_idx: 390 |  Loss: (0.8130) |  Loss2: (0.0000) | Acc: (71.00%) (35627/50000)
# TEST : Loss: (0.8354) | Acc: (70.00%) (7052/10000)
percent tensor([0.4938, 0.5062], device='cuda:0')
percent tensor([0.4448, 0.5552], device='cuda:0')
percent tensor([0.4870, 0.5130], device='cuda:0')
percent tensor([0.5085, 0.4915], device='cuda:0')
percent tensor([0.4987, 0.5013], device='cuda:0')
percent tensor([0.6069, 0.3931], device='cuda:0')
percent tensor([0.5902, 0.4098], device='cuda:0')
percent tensor([0.9738, 0.0262], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 20 | Batch_idx: 0 |  Loss: (0.8817) |  Loss2: (0.0000) | Acc: (70.00%) (90/128)
Epoch: 20 | Batch_idx: 10 |  Loss: (0.8220) |  Loss2: (0.0000) | Acc: (70.00%) (999/1408)
Epoch: 20 | Batch_idx: 20 |  Loss: (0.8289) |  Loss2: (0.0000) | Acc: (69.00%) (1874/2688)
Epoch: 20 | Batch_idx: 30 |  Loss: (0.8155) |  Loss2: (0.0000) | Acc: (69.00%) (2777/3968)
Epoch: 20 | Batch_idx: 40 |  Loss: (0.8173) |  Loss2: (0.0000) | Acc: (70.00%) (3687/5248)
Epoch: 20 | Batch_idx: 50 |  Loss: (0.8197) |  Loss2: (0.0000) | Acc: (70.00%) (4587/6528)
Epoch: 20 | Batch_idx: 60 |  Loss: (0.8179) |  Loss2: (0.0000) | Acc: (70.00%) (5501/7808)
Epoch: 20 | Batch_idx: 70 |  Loss: (0.8188) |  Loss2: (0.0000) | Acc: (70.00%) (6403/9088)
Epoch: 20 | Batch_idx: 80 |  Loss: (0.8237) |  Loss2: (0.0000) | Acc: (70.00%) (7276/10368)
Epoch: 20 | Batch_idx: 90 |  Loss: (0.8259) |  Loss2: (0.0000) | Acc: (70.00%) (8176/11648)
Epoch: 20 | Batch_idx: 100 |  Loss: (0.8215) |  Loss2: (0.0000) | Acc: (70.00%) (9110/12928)
Epoch: 20 | Batch_idx: 110 |  Loss: (0.8233) |  Loss2: (0.0000) | Acc: (70.00%) (10009/14208)
Epoch: 20 | Batch_idx: 120 |  Loss: (0.8205) |  Loss2: (0.0000) | Acc: (70.00%) (10928/15488)
Epoch: 20 | Batch_idx: 130 |  Loss: (0.8214) |  Loss2: (0.0000) | Acc: (70.00%) (11828/16768)
Epoch: 20 | Batch_idx: 140 |  Loss: (0.8188) |  Loss2: (0.0000) | Acc: (70.00%) (12751/18048)
Epoch: 20 | Batch_idx: 150 |  Loss: (0.8190) |  Loss2: (0.0000) | Acc: (70.00%) (13655/19328)
Epoch: 20 | Batch_idx: 160 |  Loss: (0.8183) |  Loss2: (0.0000) | Acc: (70.00%) (14562/20608)
Epoch: 20 | Batch_idx: 170 |  Loss: (0.8180) |  Loss2: (0.0000) | Acc: (70.00%) (15465/21888)
Epoch: 20 | Batch_idx: 180 |  Loss: (0.8182) |  Loss2: (0.0000) | Acc: (70.00%) (16389/23168)
Epoch: 20 | Batch_idx: 190 |  Loss: (0.8203) |  Loss2: (0.0000) | Acc: (70.00%) (17281/24448)
Epoch: 20 | Batch_idx: 200 |  Loss: (0.8211) |  Loss2: (0.0000) | Acc: (70.00%) (18181/25728)
Epoch: 20 | Batch_idx: 210 |  Loss: (0.8192) |  Loss2: (0.0000) | Acc: (70.00%) (19102/27008)
Epoch: 20 | Batch_idx: 220 |  Loss: (0.8172) |  Loss2: (0.0000) | Acc: (70.00%) (20019/28288)
Epoch: 20 | Batch_idx: 230 |  Loss: (0.8159) |  Loss2: (0.0000) | Acc: (70.00%) (20949/29568)
Epoch: 20 | Batch_idx: 240 |  Loss: (0.8174) |  Loss2: (0.0000) | Acc: (70.00%) (21841/30848)
Epoch: 20 | Batch_idx: 250 |  Loss: (0.8164) |  Loss2: (0.0000) | Acc: (70.00%) (22761/32128)
Epoch: 20 | Batch_idx: 260 |  Loss: (0.8172) |  Loss2: (0.0000) | Acc: (70.00%) (23676/33408)
Epoch: 20 | Batch_idx: 270 |  Loss: (0.8154) |  Loss2: (0.0000) | Acc: (70.00%) (24601/34688)
Epoch: 20 | Batch_idx: 280 |  Loss: (0.8120) |  Loss2: (0.0000) | Acc: (71.00%) (25565/35968)
Epoch: 20 | Batch_idx: 290 |  Loss: (0.8109) |  Loss2: (0.0000) | Acc: (71.00%) (26487/37248)
Epoch: 20 | Batch_idx: 300 |  Loss: (0.8100) |  Loss2: (0.0000) | Acc: (71.00%) (27417/38528)
Epoch: 20 | Batch_idx: 310 |  Loss: (0.8099) |  Loss2: (0.0000) | Acc: (71.00%) (28346/39808)
Epoch: 20 | Batch_idx: 320 |  Loss: (0.8103) |  Loss2: (0.0000) | Acc: (71.00%) (29250/41088)
Epoch: 20 | Batch_idx: 330 |  Loss: (0.8101) |  Loss2: (0.0000) | Acc: (71.00%) (30162/42368)
Epoch: 20 | Batch_idx: 340 |  Loss: (0.8102) |  Loss2: (0.0000) | Acc: (71.00%) (31078/43648)
Epoch: 20 | Batch_idx: 350 |  Loss: (0.8089) |  Loss2: (0.0000) | Acc: (71.00%) (32019/44928)
Epoch: 20 | Batch_idx: 360 |  Loss: (0.8071) |  Loss2: (0.0000) | Acc: (71.00%) (32952/46208)
Epoch: 20 | Batch_idx: 370 |  Loss: (0.8068) |  Loss2: (0.0000) | Acc: (71.00%) (33858/47488)
Epoch: 20 | Batch_idx: 380 |  Loss: (0.8072) |  Loss2: (0.0000) | Acc: (71.00%) (34763/48768)
Epoch: 20 | Batch_idx: 390 |  Loss: (0.8069) |  Loss2: (0.0000) | Acc: (71.00%) (35660/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_020.pth.tar'
# TEST : Loss: (0.9369) | Acc: (67.00%) (6727/10000)
percent tensor([0.4940, 0.5060], device='cuda:0')
percent tensor([0.4444, 0.5556], device='cuda:0')
percent tensor([0.4880, 0.5120], device='cuda:0')
percent tensor([0.5086, 0.4914], device='cuda:0')
percent tensor([0.4987, 0.5013], device='cuda:0')
percent tensor([0.6060, 0.3940], device='cuda:0')
percent tensor([0.5935, 0.4065], device='cuda:0')
percent tensor([0.9736, 0.0264], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(169.3348, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(785.1106, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(777.1408, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1524.8564, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(516.4657, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2168.6819, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4311.1138, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1434.1517, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6097.3057, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12183.8496, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4053.5610, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(17203.8477, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 21 | Batch_idx: 0 |  Loss: (0.7995) |  Loss2: (0.0000) | Acc: (72.00%) (93/128)
Epoch: 21 | Batch_idx: 10 |  Loss: (0.7557) |  Loss2: (0.0000) | Acc: (73.00%) (1028/1408)
Epoch: 21 | Batch_idx: 20 |  Loss: (0.7542) |  Loss2: (0.0000) | Acc: (72.00%) (1959/2688)
Epoch: 21 | Batch_idx: 30 |  Loss: (0.7620) |  Loss2: (0.0000) | Acc: (72.00%) (2891/3968)
Epoch: 21 | Batch_idx: 40 |  Loss: (0.7569) |  Loss2: (0.0000) | Acc: (72.00%) (3825/5248)
Epoch: 21 | Batch_idx: 50 |  Loss: (0.7605) |  Loss2: (0.0000) | Acc: (72.00%) (4752/6528)
Epoch: 21 | Batch_idx: 60 |  Loss: (0.7606) |  Loss2: (0.0000) | Acc: (72.00%) (5695/7808)
Epoch: 21 | Batch_idx: 70 |  Loss: (0.7659) |  Loss2: (0.0000) | Acc: (72.00%) (6610/9088)
Epoch: 21 | Batch_idx: 80 |  Loss: (0.7620) |  Loss2: (0.0000) | Acc: (72.00%) (7568/10368)
Epoch: 21 | Batch_idx: 90 |  Loss: (0.7626) |  Loss2: (0.0000) | Acc: (73.00%) (8507/11648)
Epoch: 21 | Batch_idx: 100 |  Loss: (0.7651) |  Loss2: (0.0000) | Acc: (72.00%) (9431/12928)
Epoch: 21 | Batch_idx: 110 |  Loss: (0.7663) |  Loss2: (0.0000) | Acc: (72.00%) (10344/14208)
Epoch: 21 | Batch_idx: 120 |  Loss: (0.7703) |  Loss2: (0.0000) | Acc: (72.00%) (11261/15488)
Epoch: 21 | Batch_idx: 130 |  Loss: (0.7657) |  Loss2: (0.0000) | Acc: (72.00%) (12221/16768)
Epoch: 21 | Batch_idx: 140 |  Loss: (0.7636) |  Loss2: (0.0000) | Acc: (72.00%) (13156/18048)
Epoch: 21 | Batch_idx: 150 |  Loss: (0.7627) |  Loss2: (0.0000) | Acc: (72.00%) (14097/19328)
Epoch: 21 | Batch_idx: 160 |  Loss: (0.7641) |  Loss2: (0.0000) | Acc: (72.00%) (15026/20608)
Epoch: 21 | Batch_idx: 170 |  Loss: (0.7649) |  Loss2: (0.0000) | Acc: (72.00%) (15959/21888)
Epoch: 21 | Batch_idx: 180 |  Loss: (0.7646) |  Loss2: (0.0000) | Acc: (72.00%) (16898/23168)
Epoch: 21 | Batch_idx: 190 |  Loss: (0.7631) |  Loss2: (0.0000) | Acc: (73.00%) (17855/24448)
Epoch: 21 | Batch_idx: 200 |  Loss: (0.7632) |  Loss2: (0.0000) | Acc: (73.00%) (18782/25728)
Epoch: 21 | Batch_idx: 210 |  Loss: (0.7639) |  Loss2: (0.0000) | Acc: (73.00%) (19730/27008)
Epoch: 21 | Batch_idx: 220 |  Loss: (0.7617) |  Loss2: (0.0000) | Acc: (73.00%) (20700/28288)
Epoch: 21 | Batch_idx: 230 |  Loss: (0.7650) |  Loss2: (0.0000) | Acc: (73.00%) (21609/29568)
Epoch: 21 | Batch_idx: 240 |  Loss: (0.7666) |  Loss2: (0.0000) | Acc: (73.00%) (22520/30848)
Epoch: 21 | Batch_idx: 250 |  Loss: (0.7671) |  Loss2: (0.0000) | Acc: (72.00%) (23451/32128)
Epoch: 21 | Batch_idx: 260 |  Loss: (0.7658) |  Loss2: (0.0000) | Acc: (73.00%) (24413/33408)
Epoch: 21 | Batch_idx: 270 |  Loss: (0.7645) |  Loss2: (0.0000) | Acc: (73.00%) (25372/34688)
Epoch: 21 | Batch_idx: 280 |  Loss: (0.7642) |  Loss2: (0.0000) | Acc: (73.00%) (26313/35968)
Epoch: 21 | Batch_idx: 290 |  Loss: (0.7626) |  Loss2: (0.0000) | Acc: (73.00%) (27280/37248)
Epoch: 21 | Batch_idx: 300 |  Loss: (0.7627) |  Loss2: (0.0000) | Acc: (73.00%) (28227/38528)
Epoch: 21 | Batch_idx: 310 |  Loss: (0.7630) |  Loss2: (0.0000) | Acc: (73.00%) (29162/39808)
Epoch: 21 | Batch_idx: 320 |  Loss: (0.7637) |  Loss2: (0.0000) | Acc: (73.00%) (30081/41088)
Epoch: 21 | Batch_idx: 330 |  Loss: (0.7631) |  Loss2: (0.0000) | Acc: (73.00%) (31006/42368)
Epoch: 21 | Batch_idx: 340 |  Loss: (0.7631) |  Loss2: (0.0000) | Acc: (73.00%) (31947/43648)
Epoch: 21 | Batch_idx: 350 |  Loss: (0.7619) |  Loss2: (0.0000) | Acc: (73.00%) (32905/44928)
Epoch: 21 | Batch_idx: 360 |  Loss: (0.7609) |  Loss2: (0.0000) | Acc: (73.00%) (33848/46208)
Epoch: 21 | Batch_idx: 370 |  Loss: (0.7608) |  Loss2: (0.0000) | Acc: (73.00%) (34789/47488)
Epoch: 21 | Batch_idx: 380 |  Loss: (0.7586) |  Loss2: (0.0000) | Acc: (73.00%) (35754/48768)
Epoch: 21 | Batch_idx: 390 |  Loss: (0.7591) |  Loss2: (0.0000) | Acc: (73.00%) (36653/50000)
# TEST : Loss: (0.8186) | Acc: (70.00%) (7063/10000)
percent tensor([0.4941, 0.5059], device='cuda:0')
percent tensor([0.4457, 0.5543], device='cuda:0')
percent tensor([0.4876, 0.5124], device='cuda:0')
percent tensor([0.5086, 0.4914], device='cuda:0')
percent tensor([0.4987, 0.5013], device='cuda:0')
percent tensor([0.6018, 0.3982], device='cuda:0')
percent tensor([0.5869, 0.4131], device='cuda:0')
percent tensor([0.9714, 0.0286], device='cuda:0')
Epoch: 22 | Batch_idx: 0 |  Loss: (0.6524) |  Loss2: (0.0000) | Acc: (75.00%) (97/128)
Epoch: 22 | Batch_idx: 10 |  Loss: (0.7124) |  Loss2: (0.0000) | Acc: (74.00%) (1043/1408)
Epoch: 22 | Batch_idx: 20 |  Loss: (0.7203) |  Loss2: (0.0000) | Acc: (74.00%) (1999/2688)
Epoch: 22 | Batch_idx: 30 |  Loss: (0.7140) |  Loss2: (0.0000) | Acc: (74.00%) (2960/3968)
Epoch: 22 | Batch_idx: 40 |  Loss: (0.7243) |  Loss2: (0.0000) | Acc: (74.00%) (3910/5248)
Epoch: 22 | Batch_idx: 50 |  Loss: (0.7214) |  Loss2: (0.0000) | Acc: (74.00%) (4871/6528)
Epoch: 22 | Batch_idx: 60 |  Loss: (0.7147) |  Loss2: (0.0000) | Acc: (75.00%) (5867/7808)
Epoch: 22 | Batch_idx: 70 |  Loss: (0.7106) |  Loss2: (0.0000) | Acc: (75.00%) (6847/9088)
Epoch: 22 | Batch_idx: 80 |  Loss: (0.7153) |  Loss2: (0.0000) | Acc: (75.00%) (7801/10368)
Epoch: 22 | Batch_idx: 90 |  Loss: (0.7153) |  Loss2: (0.0000) | Acc: (75.00%) (8748/11648)
Epoch: 22 | Batch_idx: 100 |  Loss: (0.7153) |  Loss2: (0.0000) | Acc: (75.00%) (9706/12928)
Epoch: 22 | Batch_idx: 110 |  Loss: (0.7140) |  Loss2: (0.0000) | Acc: (75.00%) (10663/14208)
Epoch: 22 | Batch_idx: 120 |  Loss: (0.7159) |  Loss2: (0.0000) | Acc: (74.00%) (11602/15488)
Epoch: 22 | Batch_idx: 130 |  Loss: (0.7176) |  Loss2: (0.0000) | Acc: (74.00%) (12550/16768)
Epoch: 22 | Batch_idx: 140 |  Loss: (0.7208) |  Loss2: (0.0000) | Acc: (74.00%) (13485/18048)
Epoch: 22 | Batch_idx: 150 |  Loss: (0.7235) |  Loss2: (0.0000) | Acc: (74.00%) (14433/19328)
Epoch: 22 | Batch_idx: 160 |  Loss: (0.7257) |  Loss2: (0.0000) | Acc: (74.00%) (15377/20608)
Epoch: 22 | Batch_idx: 170 |  Loss: (0.7229) |  Loss2: (0.0000) | Acc: (74.00%) (16355/21888)
Epoch: 22 | Batch_idx: 180 |  Loss: (0.7241) |  Loss2: (0.0000) | Acc: (74.00%) (17293/23168)
Epoch: 22 | Batch_idx: 190 |  Loss: (0.7196) |  Loss2: (0.0000) | Acc: (74.00%) (18300/24448)
Epoch: 22 | Batch_idx: 200 |  Loss: (0.7187) |  Loss2: (0.0000) | Acc: (74.00%) (19273/25728)
Epoch: 22 | Batch_idx: 210 |  Loss: (0.7169) |  Loss2: (0.0000) | Acc: (74.00%) (20248/27008)
Epoch: 22 | Batch_idx: 220 |  Loss: (0.7174) |  Loss2: (0.0000) | Acc: (74.00%) (21197/28288)
Epoch: 22 | Batch_idx: 230 |  Loss: (0.7175) |  Loss2: (0.0000) | Acc: (74.00%) (22150/29568)
Epoch: 22 | Batch_idx: 240 |  Loss: (0.7182) |  Loss2: (0.0000) | Acc: (74.00%) (23097/30848)
Epoch: 22 | Batch_idx: 250 |  Loss: (0.7174) |  Loss2: (0.0000) | Acc: (74.00%) (24060/32128)
Epoch: 22 | Batch_idx: 260 |  Loss: (0.7192) |  Loss2: (0.0000) | Acc: (74.00%) (25012/33408)
Epoch: 22 | Batch_idx: 270 |  Loss: (0.7207) |  Loss2: (0.0000) | Acc: (74.00%) (25948/34688)
Epoch: 22 | Batch_idx: 280 |  Loss: (0.7201) |  Loss2: (0.0000) | Acc: (74.00%) (26908/35968)
Epoch: 22 | Batch_idx: 290 |  Loss: (0.7205) |  Loss2: (0.0000) | Acc: (74.00%) (27857/37248)
Epoch: 22 | Batch_idx: 300 |  Loss: (0.7200) |  Loss2: (0.0000) | Acc: (74.00%) (28817/38528)
Epoch: 22 | Batch_idx: 310 |  Loss: (0.7206) |  Loss2: (0.0000) | Acc: (74.00%) (29771/39808)
Epoch: 22 | Batch_idx: 320 |  Loss: (0.7192) |  Loss2: (0.0000) | Acc: (74.00%) (30752/41088)
Epoch: 22 | Batch_idx: 330 |  Loss: (0.7172) |  Loss2: (0.0000) | Acc: (74.00%) (31734/42368)
Epoch: 22 | Batch_idx: 340 |  Loss: (0.7161) |  Loss2: (0.0000) | Acc: (74.00%) (32713/43648)
Epoch: 22 | Batch_idx: 350 |  Loss: (0.7153) |  Loss2: (0.0000) | Acc: (74.00%) (33685/44928)
Epoch: 22 | Batch_idx: 360 |  Loss: (0.7144) |  Loss2: (0.0000) | Acc: (74.00%) (34647/46208)
Epoch: 22 | Batch_idx: 370 |  Loss: (0.7134) |  Loss2: (0.0000) | Acc: (75.00%) (35622/47488)
Epoch: 22 | Batch_idx: 380 |  Loss: (0.7124) |  Loss2: (0.0000) | Acc: (75.00%) (36604/48768)
Epoch: 22 | Batch_idx: 390 |  Loss: (0.7112) |  Loss2: (0.0000) | Acc: (75.00%) (37539/50000)
# TEST : Loss: (0.8739) | Acc: (69.00%) (6980/10000)
percent tensor([0.4939, 0.5061], device='cuda:0')
percent tensor([0.4469, 0.5531], device='cuda:0')
percent tensor([0.4869, 0.5131], device='cuda:0')
percent tensor([0.5088, 0.4912], device='cuda:0')
percent tensor([0.4987, 0.5013], device='cuda:0')
percent tensor([0.6017, 0.3983], device='cuda:0')
percent tensor([0.5902, 0.4098], device='cuda:0')
percent tensor([0.9729, 0.0271], device='cuda:0')
Epoch: 23 | Batch_idx: 0 |  Loss: (0.6889) |  Loss2: (0.0000) | Acc: (78.00%) (101/128)
Epoch: 23 | Batch_idx: 10 |  Loss: (0.6986) |  Loss2: (0.0000) | Acc: (75.00%) (1064/1408)
Epoch: 23 | Batch_idx: 20 |  Loss: (0.6867) |  Loss2: (0.0000) | Acc: (75.00%) (2027/2688)
Epoch: 23 | Batch_idx: 30 |  Loss: (0.6818) |  Loss2: (0.0000) | Acc: (75.00%) (3006/3968)
Epoch: 23 | Batch_idx: 40 |  Loss: (0.6931) |  Loss2: (0.0000) | Acc: (75.00%) (3966/5248)
Epoch: 23 | Batch_idx: 50 |  Loss: (0.6970) |  Loss2: (0.0000) | Acc: (75.00%) (4921/6528)
Epoch: 23 | Batch_idx: 60 |  Loss: (0.6987) |  Loss2: (0.0000) | Acc: (75.00%) (5909/7808)
Epoch: 23 | Batch_idx: 70 |  Loss: (0.6953) |  Loss2: (0.0000) | Acc: (75.00%) (6869/9088)
Epoch: 23 | Batch_idx: 80 |  Loss: (0.6910) |  Loss2: (0.0000) | Acc: (75.00%) (7849/10368)
Epoch: 23 | Batch_idx: 90 |  Loss: (0.6904) |  Loss2: (0.0000) | Acc: (75.00%) (8819/11648)
Epoch: 23 | Batch_idx: 100 |  Loss: (0.6881) |  Loss2: (0.0000) | Acc: (75.00%) (9805/12928)
Epoch: 23 | Batch_idx: 110 |  Loss: (0.6865) |  Loss2: (0.0000) | Acc: (75.00%) (10772/14208)
Epoch: 23 | Batch_idx: 120 |  Loss: (0.6842) |  Loss2: (0.0000) | Acc: (75.00%) (11747/15488)
Epoch: 23 | Batch_idx: 130 |  Loss: (0.6864) |  Loss2: (0.0000) | Acc: (75.00%) (12705/16768)
Epoch: 23 | Batch_idx: 140 |  Loss: (0.6871) |  Loss2: (0.0000) | Acc: (75.00%) (13671/18048)
Epoch: 23 | Batch_idx: 150 |  Loss: (0.6859) |  Loss2: (0.0000) | Acc: (75.00%) (14649/19328)
Epoch: 23 | Batch_idx: 160 |  Loss: (0.6845) |  Loss2: (0.0000) | Acc: (75.00%) (15648/20608)
Epoch: 23 | Batch_idx: 170 |  Loss: (0.6855) |  Loss2: (0.0000) | Acc: (75.00%) (16608/21888)
Epoch: 23 | Batch_idx: 180 |  Loss: (0.6861) |  Loss2: (0.0000) | Acc: (75.00%) (17566/23168)
Epoch: 23 | Batch_idx: 190 |  Loss: (0.6843) |  Loss2: (0.0000) | Acc: (75.00%) (18558/24448)
Epoch: 23 | Batch_idx: 200 |  Loss: (0.6836) |  Loss2: (0.0000) | Acc: (75.00%) (19518/25728)
Epoch: 23 | Batch_idx: 210 |  Loss: (0.6840) |  Loss2: (0.0000) | Acc: (75.00%) (20499/27008)
Epoch: 23 | Batch_idx: 220 |  Loss: (0.6837) |  Loss2: (0.0000) | Acc: (75.00%) (21473/28288)
Epoch: 23 | Batch_idx: 230 |  Loss: (0.6823) |  Loss2: (0.0000) | Acc: (75.00%) (22465/29568)
Epoch: 23 | Batch_idx: 240 |  Loss: (0.6840) |  Loss2: (0.0000) | Acc: (75.00%) (23410/30848)
Epoch: 23 | Batch_idx: 250 |  Loss: (0.6828) |  Loss2: (0.0000) | Acc: (75.00%) (24403/32128)
Epoch: 23 | Batch_idx: 260 |  Loss: (0.6814) |  Loss2: (0.0000) | Acc: (76.00%) (25393/33408)
Epoch: 23 | Batch_idx: 270 |  Loss: (0.6816) |  Loss2: (0.0000) | Acc: (75.00%) (26360/34688)
Epoch: 23 | Batch_idx: 280 |  Loss: (0.6821) |  Loss2: (0.0000) | Acc: (75.00%) (27313/35968)
Epoch: 23 | Batch_idx: 290 |  Loss: (0.6818) |  Loss2: (0.0000) | Acc: (75.00%) (28287/37248)
Epoch: 23 | Batch_idx: 300 |  Loss: (0.6832) |  Loss2: (0.0000) | Acc: (75.00%) (29238/38528)
Epoch: 23 | Batch_idx: 310 |  Loss: (0.6817) |  Loss2: (0.0000) | Acc: (75.00%) (30249/39808)
Epoch: 23 | Batch_idx: 320 |  Loss: (0.6819) |  Loss2: (0.0000) | Acc: (75.00%) (31212/41088)
Epoch: 23 | Batch_idx: 330 |  Loss: (0.6808) |  Loss2: (0.0000) | Acc: (76.00%) (32200/42368)
Epoch: 23 | Batch_idx: 340 |  Loss: (0.6804) |  Loss2: (0.0000) | Acc: (76.00%) (33177/43648)
Epoch: 23 | Batch_idx: 350 |  Loss: (0.6792) |  Loss2: (0.0000) | Acc: (76.00%) (34178/44928)
Epoch: 23 | Batch_idx: 360 |  Loss: (0.6791) |  Loss2: (0.0000) | Acc: (76.00%) (35166/46208)
Epoch: 23 | Batch_idx: 370 |  Loss: (0.6793) |  Loss2: (0.0000) | Acc: (76.00%) (36135/47488)
Epoch: 23 | Batch_idx: 380 |  Loss: (0.6775) |  Loss2: (0.0000) | Acc: (76.00%) (37159/48768)
Epoch: 23 | Batch_idx: 390 |  Loss: (0.6776) |  Loss2: (0.0000) | Acc: (76.00%) (38121/50000)
# TEST : Loss: (0.8411) | Acc: (71.00%) (7187/10000)
percent tensor([0.4940, 0.5060], device='cuda:0')
percent tensor([0.4459, 0.5541], device='cuda:0')
percent tensor([0.4867, 0.5133], device='cuda:0')
percent tensor([0.5086, 0.4914], device='cuda:0')
percent tensor([0.4987, 0.5013], device='cuda:0')
percent tensor([0.6036, 0.3964], device='cuda:0')
percent tensor([0.5941, 0.4059], device='cuda:0')
percent tensor([0.9751, 0.0249], device='cuda:0')
Epoch: 24 | Batch_idx: 0 |  Loss: (0.6194) |  Loss2: (0.0000) | Acc: (80.00%) (103/128)
Epoch: 24 | Batch_idx: 10 |  Loss: (0.6310) |  Loss2: (0.0000) | Acc: (77.00%) (1090/1408)
Epoch: 24 | Batch_idx: 20 |  Loss: (0.6286) |  Loss2: (0.0000) | Acc: (77.00%) (2072/2688)
Epoch: 24 | Batch_idx: 30 |  Loss: (0.6383) |  Loss2: (0.0000) | Acc: (77.00%) (3065/3968)
Epoch: 24 | Batch_idx: 40 |  Loss: (0.6409) |  Loss2: (0.0000) | Acc: (77.00%) (4066/5248)
Epoch: 24 | Batch_idx: 50 |  Loss: (0.6413) |  Loss2: (0.0000) | Acc: (77.00%) (5069/6528)
Epoch: 24 | Batch_idx: 60 |  Loss: (0.6413) |  Loss2: (0.0000) | Acc: (77.00%) (6057/7808)
Epoch: 24 | Batch_idx: 70 |  Loss: (0.6429) |  Loss2: (0.0000) | Acc: (77.00%) (7012/9088)
Epoch: 24 | Batch_idx: 80 |  Loss: (0.6434) |  Loss2: (0.0000) | Acc: (77.00%) (8013/10368)
Epoch: 24 | Batch_idx: 90 |  Loss: (0.6470) |  Loss2: (0.0000) | Acc: (77.00%) (8997/11648)
Epoch: 24 | Batch_idx: 100 |  Loss: (0.6521) |  Loss2: (0.0000) | Acc: (77.00%) (9980/12928)
Epoch: 24 | Batch_idx: 110 |  Loss: (0.6546) |  Loss2: (0.0000) | Acc: (77.00%) (10952/14208)
Epoch: 24 | Batch_idx: 120 |  Loss: (0.6548) |  Loss2: (0.0000) | Acc: (77.00%) (11945/15488)
Epoch: 24 | Batch_idx: 130 |  Loss: (0.6563) |  Loss2: (0.0000) | Acc: (77.00%) (12933/16768)
Epoch: 24 | Batch_idx: 140 |  Loss: (0.6544) |  Loss2: (0.0000) | Acc: (77.00%) (13921/18048)
Epoch: 24 | Batch_idx: 150 |  Loss: (0.6532) |  Loss2: (0.0000) | Acc: (77.00%) (14927/19328)
Epoch: 24 | Batch_idx: 160 |  Loss: (0.6538) |  Loss2: (0.0000) | Acc: (77.00%) (15899/20608)
Epoch: 24 | Batch_idx: 170 |  Loss: (0.6563) |  Loss2: (0.0000) | Acc: (77.00%) (16873/21888)
Epoch: 24 | Batch_idx: 180 |  Loss: (0.6557) |  Loss2: (0.0000) | Acc: (77.00%) (17876/23168)
Epoch: 24 | Batch_idx: 190 |  Loss: (0.6545) |  Loss2: (0.0000) | Acc: (77.00%) (18872/24448)
Epoch: 24 | Batch_idx: 200 |  Loss: (0.6541) |  Loss2: (0.0000) | Acc: (77.00%) (19858/25728)
Epoch: 24 | Batch_idx: 210 |  Loss: (0.6538) |  Loss2: (0.0000) | Acc: (77.00%) (20842/27008)
Epoch: 24 | Batch_idx: 220 |  Loss: (0.6527) |  Loss2: (0.0000) | Acc: (77.00%) (21848/28288)
Epoch: 24 | Batch_idx: 230 |  Loss: (0.6520) |  Loss2: (0.0000) | Acc: (77.00%) (22838/29568)
Epoch: 24 | Batch_idx: 240 |  Loss: (0.6519) |  Loss2: (0.0000) | Acc: (77.00%) (23834/30848)
Epoch: 24 | Batch_idx: 250 |  Loss: (0.6503) |  Loss2: (0.0000) | Acc: (77.00%) (24832/32128)
Epoch: 24 | Batch_idx: 260 |  Loss: (0.6509) |  Loss2: (0.0000) | Acc: (77.00%) (25807/33408)
Epoch: 24 | Batch_idx: 270 |  Loss: (0.6512) |  Loss2: (0.0000) | Acc: (77.00%) (26786/34688)
Epoch: 24 | Batch_idx: 280 |  Loss: (0.6512) |  Loss2: (0.0000) | Acc: (77.00%) (27788/35968)
Epoch: 24 | Batch_idx: 290 |  Loss: (0.6499) |  Loss2: (0.0000) | Acc: (77.00%) (28802/37248)
Epoch: 24 | Batch_idx: 300 |  Loss: (0.6489) |  Loss2: (0.0000) | Acc: (77.00%) (29801/38528)
Epoch: 24 | Batch_idx: 310 |  Loss: (0.6485) |  Loss2: (0.0000) | Acc: (77.00%) (30786/39808)
Epoch: 24 | Batch_idx: 320 |  Loss: (0.6488) |  Loss2: (0.0000) | Acc: (77.00%) (31762/41088)
Epoch: 24 | Batch_idx: 330 |  Loss: (0.6496) |  Loss2: (0.0000) | Acc: (77.00%) (32753/42368)
Epoch: 24 | Batch_idx: 340 |  Loss: (0.6493) |  Loss2: (0.0000) | Acc: (77.00%) (33738/43648)
Epoch: 24 | Batch_idx: 350 |  Loss: (0.6477) |  Loss2: (0.0000) | Acc: (77.00%) (34735/44928)
Epoch: 24 | Batch_idx: 360 |  Loss: (0.6464) |  Loss2: (0.0000) | Acc: (77.00%) (35743/46208)
Epoch: 24 | Batch_idx: 370 |  Loss: (0.6460) |  Loss2: (0.0000) | Acc: (77.00%) (36749/47488)
Epoch: 24 | Batch_idx: 380 |  Loss: (0.6469) |  Loss2: (0.0000) | Acc: (77.00%) (37733/48768)
Epoch: 24 | Batch_idx: 390 |  Loss: (0.6462) |  Loss2: (0.0000) | Acc: (77.00%) (38698/50000)
# TEST : Loss: (0.6948) | Acc: (75.00%) (7579/10000)
percent tensor([0.4940, 0.5060], device='cuda:0')
percent tensor([0.4462, 0.5538], device='cuda:0')
percent tensor([0.4866, 0.5134], device='cuda:0')
percent tensor([0.5090, 0.4910], device='cuda:0')
percent tensor([0.4987, 0.5013], device='cuda:0')
percent tensor([0.6069, 0.3931], device='cuda:0')
percent tensor([0.5939, 0.4061], device='cuda:0')
percent tensor([0.9759, 0.0241], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 25 | Batch_idx: 0 |  Loss: (0.6734) |  Loss2: (0.0000) | Acc: (82.00%) (105/128)
Epoch: 25 | Batch_idx: 10 |  Loss: (0.7273) |  Loss2: (0.0000) | Acc: (75.00%) (1067/1408)
Epoch: 25 | Batch_idx: 20 |  Loss: (0.7663) |  Loss2: (0.0000) | Acc: (74.00%) (2001/2688)
Epoch: 25 | Batch_idx: 30 |  Loss: (0.7905) |  Loss2: (0.0000) | Acc: (73.00%) (2919/3968)
Epoch: 25 | Batch_idx: 40 |  Loss: (0.8098) |  Loss2: (0.0000) | Acc: (73.00%) (3834/5248)
Epoch: 25 | Batch_idx: 50 |  Loss: (0.8114) |  Loss2: (0.0000) | Acc: (72.00%) (4751/6528)
Epoch: 25 | Batch_idx: 60 |  Loss: (0.8186) |  Loss2: (0.0000) | Acc: (72.00%) (5641/7808)
Epoch: 25 | Batch_idx: 70 |  Loss: (0.8162) |  Loss2: (0.0000) | Acc: (72.00%) (6559/9088)
Epoch: 25 | Batch_idx: 80 |  Loss: (0.8119) |  Loss2: (0.0000) | Acc: (72.00%) (7491/10368)
Epoch: 25 | Batch_idx: 90 |  Loss: (0.8076) |  Loss2: (0.0000) | Acc: (72.00%) (8437/11648)
Epoch: 25 | Batch_idx: 100 |  Loss: (0.8044) |  Loss2: (0.0000) | Acc: (72.00%) (9368/12928)
Epoch: 25 | Batch_idx: 110 |  Loss: (0.7983) |  Loss2: (0.0000) | Acc: (72.00%) (10319/14208)
Epoch: 25 | Batch_idx: 120 |  Loss: (0.7885) |  Loss2: (0.0000) | Acc: (72.00%) (11284/15488)
Epoch: 25 | Batch_idx: 130 |  Loss: (0.7844) |  Loss2: (0.0000) | Acc: (72.00%) (12239/16768)
Epoch: 25 | Batch_idx: 140 |  Loss: (0.7794) |  Loss2: (0.0000) | Acc: (73.00%) (13204/18048)
Epoch: 25 | Batch_idx: 150 |  Loss: (0.7740) |  Loss2: (0.0000) | Acc: (73.00%) (14177/19328)
Epoch: 25 | Batch_idx: 160 |  Loss: (0.7678) |  Loss2: (0.0000) | Acc: (73.00%) (15143/20608)
Epoch: 25 | Batch_idx: 170 |  Loss: (0.7671) |  Loss2: (0.0000) | Acc: (73.00%) (16058/21888)
Epoch: 25 | Batch_idx: 180 |  Loss: (0.7636) |  Loss2: (0.0000) | Acc: (73.00%) (17009/23168)
Epoch: 25 | Batch_idx: 190 |  Loss: (0.7611) |  Loss2: (0.0000) | Acc: (73.00%) (17965/24448)
Epoch: 25 | Batch_idx: 200 |  Loss: (0.7600) |  Loss2: (0.0000) | Acc: (73.00%) (18923/25728)
Epoch: 25 | Batch_idx: 210 |  Loss: (0.7577) |  Loss2: (0.0000) | Acc: (73.00%) (19896/27008)
Epoch: 25 | Batch_idx: 220 |  Loss: (0.7548) |  Loss2: (0.0000) | Acc: (73.00%) (20858/28288)
Epoch: 25 | Batch_idx: 230 |  Loss: (0.7506) |  Loss2: (0.0000) | Acc: (73.00%) (21836/29568)
Epoch: 25 | Batch_idx: 240 |  Loss: (0.7493) |  Loss2: (0.0000) | Acc: (73.00%) (22801/30848)
Epoch: 25 | Batch_idx: 250 |  Loss: (0.7459) |  Loss2: (0.0000) | Acc: (74.00%) (23780/32128)
Epoch: 25 | Batch_idx: 260 |  Loss: (0.7438) |  Loss2: (0.0000) | Acc: (74.00%) (24746/33408)
Epoch: 25 | Batch_idx: 270 |  Loss: (0.7425) |  Loss2: (0.0000) | Acc: (74.00%) (25704/34688)
Epoch: 25 | Batch_idx: 280 |  Loss: (0.7421) |  Loss2: (0.0000) | Acc: (74.00%) (26659/35968)
Epoch: 25 | Batch_idx: 290 |  Loss: (0.7386) |  Loss2: (0.0000) | Acc: (74.00%) (27653/37248)
Epoch: 25 | Batch_idx: 300 |  Loss: (0.7372) |  Loss2: (0.0000) | Acc: (74.00%) (28604/38528)
Epoch: 25 | Batch_idx: 310 |  Loss: (0.7357) |  Loss2: (0.0000) | Acc: (74.00%) (29571/39808)
Epoch: 25 | Batch_idx: 320 |  Loss: (0.7338) |  Loss2: (0.0000) | Acc: (74.00%) (30546/41088)
Epoch: 25 | Batch_idx: 330 |  Loss: (0.7315) |  Loss2: (0.0000) | Acc: (74.00%) (31522/42368)
Epoch: 25 | Batch_idx: 340 |  Loss: (0.7290) |  Loss2: (0.0000) | Acc: (74.00%) (32508/43648)
Epoch: 25 | Batch_idx: 350 |  Loss: (0.7273) |  Loss2: (0.0000) | Acc: (74.00%) (33482/44928)
Epoch: 25 | Batch_idx: 360 |  Loss: (0.7262) |  Loss2: (0.0000) | Acc: (74.00%) (34449/46208)
Epoch: 25 | Batch_idx: 370 |  Loss: (0.7248) |  Loss2: (0.0000) | Acc: (74.00%) (35438/47488)
Epoch: 25 | Batch_idx: 380 |  Loss: (0.7249) |  Loss2: (0.0000) | Acc: (74.00%) (36397/48768)
Epoch: 25 | Batch_idx: 390 |  Loss: (0.7230) |  Loss2: (0.0000) | Acc: (74.00%) (37340/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_025.pth.tar'
# TEST : Loss: (0.7211) | Acc: (75.00%) (7520/10000)
percent tensor([0.4760, 0.5240], device='cuda:0')
percent tensor([0.4723, 0.5277], device='cuda:0')
percent tensor([0.5017, 0.4983], device='cuda:0')
percent tensor([0.5036, 0.4964], device='cuda:0')
percent tensor([0.5034, 0.4966], device='cuda:0')
percent tensor([0.6297, 0.3703], device='cuda:0')
percent tensor([0.6083, 0.3917], device='cuda:0')
percent tensor([0.9801, 0.0199], device='cuda:0')
Epoch: 26 | Batch_idx: 0 |  Loss: (0.6240) |  Loss2: (0.0000) | Acc: (77.00%) (99/128)
Epoch: 26 | Batch_idx: 10 |  Loss: (0.7072) |  Loss2: (0.0000) | Acc: (75.00%) (1060/1408)
Epoch: 26 | Batch_idx: 20 |  Loss: (0.6983) |  Loss2: (0.0000) | Acc: (75.00%) (2024/2688)
Epoch: 26 | Batch_idx: 30 |  Loss: (0.6895) |  Loss2: (0.0000) | Acc: (75.00%) (3004/3968)
Epoch: 26 | Batch_idx: 40 |  Loss: (0.6903) |  Loss2: (0.0000) | Acc: (75.00%) (3978/5248)
Epoch: 26 | Batch_idx: 50 |  Loss: (0.6870) |  Loss2: (0.0000) | Acc: (75.00%) (4960/6528)
Epoch: 26 | Batch_idx: 60 |  Loss: (0.6836) |  Loss2: (0.0000) | Acc: (76.00%) (5942/7808)
Epoch: 26 | Batch_idx: 70 |  Loss: (0.6834) |  Loss2: (0.0000) | Acc: (76.00%) (6917/9088)
Epoch: 26 | Batch_idx: 80 |  Loss: (0.6868) |  Loss2: (0.0000) | Acc: (75.00%) (7866/10368)
Epoch: 26 | Batch_idx: 90 |  Loss: (0.6892) |  Loss2: (0.0000) | Acc: (75.00%) (8816/11648)
Epoch: 26 | Batch_idx: 100 |  Loss: (0.6868) |  Loss2: (0.0000) | Acc: (75.00%) (9796/12928)
Epoch: 26 | Batch_idx: 110 |  Loss: (0.6861) |  Loss2: (0.0000) | Acc: (75.00%) (10779/14208)
Epoch: 26 | Batch_idx: 120 |  Loss: (0.6830) |  Loss2: (0.0000) | Acc: (75.00%) (11767/15488)
Epoch: 26 | Batch_idx: 130 |  Loss: (0.6827) |  Loss2: (0.0000) | Acc: (75.00%) (12736/16768)
Epoch: 26 | Batch_idx: 140 |  Loss: (0.6789) |  Loss2: (0.0000) | Acc: (76.00%) (13730/18048)
Epoch: 26 | Batch_idx: 150 |  Loss: (0.6772) |  Loss2: (0.0000) | Acc: (76.00%) (14727/19328)
Epoch: 26 | Batch_idx: 160 |  Loss: (0.6760) |  Loss2: (0.0000) | Acc: (76.00%) (15709/20608)
Epoch: 26 | Batch_idx: 170 |  Loss: (0.6752) |  Loss2: (0.0000) | Acc: (76.00%) (16698/21888)
Epoch: 26 | Batch_idx: 180 |  Loss: (0.6719) |  Loss2: (0.0000) | Acc: (76.00%) (17687/23168)
Epoch: 26 | Batch_idx: 190 |  Loss: (0.6731) |  Loss2: (0.0000) | Acc: (76.00%) (18649/24448)
Epoch: 26 | Batch_idx: 200 |  Loss: (0.6749) |  Loss2: (0.0000) | Acc: (76.00%) (19616/25728)
Epoch: 26 | Batch_idx: 210 |  Loss: (0.6741) |  Loss2: (0.0000) | Acc: (76.00%) (20604/27008)
Epoch: 26 | Batch_idx: 220 |  Loss: (0.6753) |  Loss2: (0.0000) | Acc: (76.00%) (21579/28288)
Epoch: 26 | Batch_idx: 230 |  Loss: (0.6740) |  Loss2: (0.0000) | Acc: (76.00%) (22566/29568)
Epoch: 26 | Batch_idx: 240 |  Loss: (0.6711) |  Loss2: (0.0000) | Acc: (76.00%) (23580/30848)
Epoch: 26 | Batch_idx: 250 |  Loss: (0.6727) |  Loss2: (0.0000) | Acc: (76.00%) (24548/32128)
Epoch: 26 | Batch_idx: 260 |  Loss: (0.6735) |  Loss2: (0.0000) | Acc: (76.00%) (25508/33408)
Epoch: 26 | Batch_idx: 270 |  Loss: (0.6743) |  Loss2: (0.0000) | Acc: (76.00%) (26489/34688)
Epoch: 26 | Batch_idx: 280 |  Loss: (0.6734) |  Loss2: (0.0000) | Acc: (76.00%) (27481/35968)
Epoch: 26 | Batch_idx: 290 |  Loss: (0.6739) |  Loss2: (0.0000) | Acc: (76.00%) (28454/37248)
Epoch: 26 | Batch_idx: 300 |  Loss: (0.6729) |  Loss2: (0.0000) | Acc: (76.00%) (29449/38528)
Epoch: 26 | Batch_idx: 310 |  Loss: (0.6730) |  Loss2: (0.0000) | Acc: (76.00%) (30427/39808)
Epoch: 26 | Batch_idx: 320 |  Loss: (0.6731) |  Loss2: (0.0000) | Acc: (76.00%) (31402/41088)
Epoch: 26 | Batch_idx: 330 |  Loss: (0.6730) |  Loss2: (0.0000) | Acc: (76.00%) (32380/42368)
Epoch: 26 | Batch_idx: 340 |  Loss: (0.6724) |  Loss2: (0.0000) | Acc: (76.00%) (33363/43648)
Epoch: 26 | Batch_idx: 350 |  Loss: (0.6730) |  Loss2: (0.0000) | Acc: (76.00%) (34326/44928)
Epoch: 26 | Batch_idx: 360 |  Loss: (0.6729) |  Loss2: (0.0000) | Acc: (76.00%) (35309/46208)
Epoch: 26 | Batch_idx: 370 |  Loss: (0.6721) |  Loss2: (0.0000) | Acc: (76.00%) (36298/47488)
Epoch: 26 | Batch_idx: 380 |  Loss: (0.6708) |  Loss2: (0.0000) | Acc: (76.00%) (37305/48768)
Epoch: 26 | Batch_idx: 390 |  Loss: (0.6701) |  Loss2: (0.0000) | Acc: (76.00%) (38265/50000)
# TEST : Loss: (0.6946) | Acc: (75.00%) (7589/10000)
percent tensor([0.4801, 0.5199], device='cuda:0')
percent tensor([0.4759, 0.5241], device='cuda:0')
percent tensor([0.5046, 0.4954], device='cuda:0')
percent tensor([0.5036, 0.4964], device='cuda:0')
percent tensor([0.5097, 0.4903], device='cuda:0')
percent tensor([0.6547, 0.3453], device='cuda:0')
percent tensor([0.6207, 0.3793], device='cuda:0')
percent tensor([0.9853, 0.0147], device='cuda:0')
Epoch: 27 | Batch_idx: 0 |  Loss: (0.6279) |  Loss2: (0.0000) | Acc: (75.00%) (97/128)
Epoch: 27 | Batch_idx: 10 |  Loss: (0.6793) |  Loss2: (0.0000) | Acc: (75.00%) (1068/1408)
Epoch: 27 | Batch_idx: 20 |  Loss: (0.6439) |  Loss2: (0.0000) | Acc: (76.00%) (2068/2688)
Epoch: 27 | Batch_idx: 30 |  Loss: (0.6326) |  Loss2: (0.0000) | Acc: (77.00%) (3075/3968)
Epoch: 27 | Batch_idx: 40 |  Loss: (0.6330) |  Loss2: (0.0000) | Acc: (77.00%) (4085/5248)
Epoch: 27 | Batch_idx: 50 |  Loss: (0.6382) |  Loss2: (0.0000) | Acc: (77.00%) (5062/6528)
Epoch: 27 | Batch_idx: 60 |  Loss: (0.6459) |  Loss2: (0.0000) | Acc: (77.00%) (6045/7808)
Epoch: 27 | Batch_idx: 70 |  Loss: (0.6472) |  Loss2: (0.0000) | Acc: (77.00%) (7015/9088)
Epoch: 27 | Batch_idx: 80 |  Loss: (0.6525) |  Loss2: (0.0000) | Acc: (77.00%) (7987/10368)
Epoch: 27 | Batch_idx: 90 |  Loss: (0.6509) |  Loss2: (0.0000) | Acc: (77.00%) (8985/11648)
Epoch: 27 | Batch_idx: 100 |  Loss: (0.6557) |  Loss2: (0.0000) | Acc: (77.00%) (9956/12928)
Epoch: 27 | Batch_idx: 110 |  Loss: (0.6544) |  Loss2: (0.0000) | Acc: (77.00%) (10945/14208)
Epoch: 27 | Batch_idx: 120 |  Loss: (0.6549) |  Loss2: (0.0000) | Acc: (77.00%) (11937/15488)
Epoch: 27 | Batch_idx: 130 |  Loss: (0.6568) |  Loss2: (0.0000) | Acc: (77.00%) (12920/16768)
Epoch: 27 | Batch_idx: 140 |  Loss: (0.6603) |  Loss2: (0.0000) | Acc: (76.00%) (13881/18048)
Epoch: 27 | Batch_idx: 150 |  Loss: (0.6622) |  Loss2: (0.0000) | Acc: (76.00%) (14863/19328)
Epoch: 27 | Batch_idx: 160 |  Loss: (0.6618) |  Loss2: (0.0000) | Acc: (76.00%) (15855/20608)
Epoch: 27 | Batch_idx: 170 |  Loss: (0.6616) |  Loss2: (0.0000) | Acc: (76.00%) (16839/21888)
Epoch: 27 | Batch_idx: 180 |  Loss: (0.6595) |  Loss2: (0.0000) | Acc: (76.00%) (17819/23168)
Epoch: 27 | Batch_idx: 190 |  Loss: (0.6595) |  Loss2: (0.0000) | Acc: (76.00%) (18820/24448)
Epoch: 27 | Batch_idx: 200 |  Loss: (0.6590) |  Loss2: (0.0000) | Acc: (77.00%) (19819/25728)
Epoch: 27 | Batch_idx: 210 |  Loss: (0.6576) |  Loss2: (0.0000) | Acc: (77.00%) (20812/27008)
Epoch: 27 | Batch_idx: 220 |  Loss: (0.6568) |  Loss2: (0.0000) | Acc: (77.00%) (21806/28288)
Epoch: 27 | Batch_idx: 230 |  Loss: (0.6567) |  Loss2: (0.0000) | Acc: (77.00%) (22787/29568)
Epoch: 27 | Batch_idx: 240 |  Loss: (0.6564) |  Loss2: (0.0000) | Acc: (77.00%) (23775/30848)
Epoch: 27 | Batch_idx: 250 |  Loss: (0.6559) |  Loss2: (0.0000) | Acc: (77.00%) (24768/32128)
Epoch: 27 | Batch_idx: 260 |  Loss: (0.6554) |  Loss2: (0.0000) | Acc: (77.00%) (25775/33408)
Epoch: 27 | Batch_idx: 270 |  Loss: (0.6552) |  Loss2: (0.0000) | Acc: (77.00%) (26770/34688)
Epoch: 27 | Batch_idx: 280 |  Loss: (0.6540) |  Loss2: (0.0000) | Acc: (77.00%) (27783/35968)
Epoch: 27 | Batch_idx: 290 |  Loss: (0.6533) |  Loss2: (0.0000) | Acc: (77.00%) (28784/37248)
Epoch: 27 | Batch_idx: 300 |  Loss: (0.6532) |  Loss2: (0.0000) | Acc: (77.00%) (29774/38528)
Epoch: 27 | Batch_idx: 310 |  Loss: (0.6537) |  Loss2: (0.0000) | Acc: (77.00%) (30752/39808)
Epoch: 27 | Batch_idx: 320 |  Loss: (0.6536) |  Loss2: (0.0000) | Acc: (77.00%) (31750/41088)
Epoch: 27 | Batch_idx: 330 |  Loss: (0.6540) |  Loss2: (0.0000) | Acc: (77.00%) (32736/42368)
Epoch: 27 | Batch_idx: 340 |  Loss: (0.6528) |  Loss2: (0.0000) | Acc: (77.00%) (33758/43648)
Epoch: 27 | Batch_idx: 350 |  Loss: (0.6524) |  Loss2: (0.0000) | Acc: (77.00%) (34752/44928)
Epoch: 27 | Batch_idx: 360 |  Loss: (0.6521) |  Loss2: (0.0000) | Acc: (77.00%) (35735/46208)
Epoch: 27 | Batch_idx: 370 |  Loss: (0.6517) |  Loss2: (0.0000) | Acc: (77.00%) (36741/47488)
Epoch: 27 | Batch_idx: 380 |  Loss: (0.6515) |  Loss2: (0.0000) | Acc: (77.00%) (37741/48768)
Epoch: 27 | Batch_idx: 390 |  Loss: (0.6521) |  Loss2: (0.0000) | Acc: (77.00%) (38668/50000)
# TEST : Loss: (0.6846) | Acc: (76.00%) (7620/10000)
percent tensor([0.4806, 0.5194], device='cuda:0')
percent tensor([0.4757, 0.5243], device='cuda:0')
percent tensor([0.5072, 0.4928], device='cuda:0')
percent tensor([0.5057, 0.4943], device='cuda:0')
percent tensor([0.5159, 0.4841], device='cuda:0')
percent tensor([0.6632, 0.3368], device='cuda:0')
percent tensor([0.6273, 0.3727], device='cuda:0')
percent tensor([0.9888, 0.0112], device='cuda:0')
Epoch: 28 | Batch_idx: 0 |  Loss: (0.5577) |  Loss2: (0.0000) | Acc: (79.00%) (102/128)
Epoch: 28 | Batch_idx: 10 |  Loss: (0.6271) |  Loss2: (0.0000) | Acc: (77.00%) (1087/1408)
Epoch: 28 | Batch_idx: 20 |  Loss: (0.6439) |  Loss2: (0.0000) | Acc: (77.00%) (2075/2688)
Epoch: 28 | Batch_idx: 30 |  Loss: (0.6407) |  Loss2: (0.0000) | Acc: (77.00%) (3085/3968)
Epoch: 28 | Batch_idx: 40 |  Loss: (0.6454) |  Loss2: (0.0000) | Acc: (77.00%) (4076/5248)
Epoch: 28 | Batch_idx: 50 |  Loss: (0.6436) |  Loss2: (0.0000) | Acc: (77.00%) (5078/6528)
Epoch: 28 | Batch_idx: 60 |  Loss: (0.6475) |  Loss2: (0.0000) | Acc: (77.00%) (6058/7808)
Epoch: 28 | Batch_idx: 70 |  Loss: (0.6392) |  Loss2: (0.0000) | Acc: (77.00%) (7075/9088)
Epoch: 28 | Batch_idx: 80 |  Loss: (0.6416) |  Loss2: (0.0000) | Acc: (77.00%) (8067/10368)
Epoch: 28 | Batch_idx: 90 |  Loss: (0.6385) |  Loss2: (0.0000) | Acc: (78.00%) (9088/11648)
Epoch: 28 | Batch_idx: 100 |  Loss: (0.6424) |  Loss2: (0.0000) | Acc: (77.00%) (10079/12928)
Epoch: 28 | Batch_idx: 110 |  Loss: (0.6442) |  Loss2: (0.0000) | Acc: (77.00%) (11053/14208)
Epoch: 28 | Batch_idx: 120 |  Loss: (0.6438) |  Loss2: (0.0000) | Acc: (77.00%) (12040/15488)
Epoch: 28 | Batch_idx: 130 |  Loss: (0.6449) |  Loss2: (0.0000) | Acc: (77.00%) (13018/16768)
Epoch: 28 | Batch_idx: 140 |  Loss: (0.6457) |  Loss2: (0.0000) | Acc: (77.00%) (14014/18048)
Epoch: 28 | Batch_idx: 150 |  Loss: (0.6455) |  Loss2: (0.0000) | Acc: (77.00%) (14997/19328)
Epoch: 28 | Batch_idx: 160 |  Loss: (0.6425) |  Loss2: (0.0000) | Acc: (77.00%) (16019/20608)
Epoch: 28 | Batch_idx: 170 |  Loss: (0.6398) |  Loss2: (0.0000) | Acc: (77.00%) (17021/21888)
Epoch: 28 | Batch_idx: 180 |  Loss: (0.6406) |  Loss2: (0.0000) | Acc: (77.00%) (18009/23168)
Epoch: 28 | Batch_idx: 190 |  Loss: (0.6412) |  Loss2: (0.0000) | Acc: (77.00%) (19008/24448)
Epoch: 28 | Batch_idx: 200 |  Loss: (0.6436) |  Loss2: (0.0000) | Acc: (77.00%) (19978/25728)
Epoch: 28 | Batch_idx: 210 |  Loss: (0.6432) |  Loss2: (0.0000) | Acc: (77.00%) (20975/27008)
Epoch: 28 | Batch_idx: 220 |  Loss: (0.6420) |  Loss2: (0.0000) | Acc: (77.00%) (21986/28288)
Epoch: 28 | Batch_idx: 230 |  Loss: (0.6448) |  Loss2: (0.0000) | Acc: (77.00%) (22964/29568)
Epoch: 28 | Batch_idx: 240 |  Loss: (0.6453) |  Loss2: (0.0000) | Acc: (77.00%) (23965/30848)
Epoch: 28 | Batch_idx: 250 |  Loss: (0.6445) |  Loss2: (0.0000) | Acc: (77.00%) (24978/32128)
Epoch: 28 | Batch_idx: 260 |  Loss: (0.6439) |  Loss2: (0.0000) | Acc: (77.00%) (25970/33408)
Epoch: 28 | Batch_idx: 270 |  Loss: (0.6436) |  Loss2: (0.0000) | Acc: (77.00%) (26966/34688)
Epoch: 28 | Batch_idx: 280 |  Loss: (0.6436) |  Loss2: (0.0000) | Acc: (77.00%) (27964/35968)
Epoch: 28 | Batch_idx: 290 |  Loss: (0.6434) |  Loss2: (0.0000) | Acc: (77.00%) (28957/37248)
Epoch: 28 | Batch_idx: 300 |  Loss: (0.6433) |  Loss2: (0.0000) | Acc: (77.00%) (29962/38528)
Epoch: 28 | Batch_idx: 310 |  Loss: (0.6426) |  Loss2: (0.0000) | Acc: (77.00%) (30968/39808)
Epoch: 28 | Batch_idx: 320 |  Loss: (0.6418) |  Loss2: (0.0000) | Acc: (77.00%) (31993/41088)
Epoch: 28 | Batch_idx: 330 |  Loss: (0.6422) |  Loss2: (0.0000) | Acc: (77.00%) (32980/42368)
Epoch: 28 | Batch_idx: 340 |  Loss: (0.6412) |  Loss2: (0.0000) | Acc: (77.00%) (33990/43648)
Epoch: 28 | Batch_idx: 350 |  Loss: (0.6419) |  Loss2: (0.0000) | Acc: (77.00%) (34974/44928)
Epoch: 28 | Batch_idx: 360 |  Loss: (0.6411) |  Loss2: (0.0000) | Acc: (77.00%) (35981/46208)
Epoch: 28 | Batch_idx: 370 |  Loss: (0.6408) |  Loss2: (0.0000) | Acc: (77.00%) (36980/47488)
Epoch: 28 | Batch_idx: 380 |  Loss: (0.6409) |  Loss2: (0.0000) | Acc: (77.00%) (37989/48768)
Epoch: 28 | Batch_idx: 390 |  Loss: (0.6417) |  Loss2: (0.0000) | Acc: (77.00%) (38929/50000)
# TEST : Loss: (0.6726) | Acc: (76.00%) (7676/10000)
percent tensor([0.4828, 0.5172], device='cuda:0')
percent tensor([0.4776, 0.5224], device='cuda:0')
percent tensor([0.5086, 0.4914], device='cuda:0')
percent tensor([0.5084, 0.4916], device='cuda:0')
percent tensor([0.5204, 0.4796], device='cuda:0')
percent tensor([0.6684, 0.3316], device='cuda:0')
percent tensor([0.6320, 0.3680], device='cuda:0')
percent tensor([0.9911, 0.0089], device='cuda:0')
Epoch: 29 | Batch_idx: 0 |  Loss: (0.5746) |  Loss2: (0.0000) | Acc: (78.00%) (101/128)
Epoch: 29 | Batch_idx: 10 |  Loss: (0.6492) |  Loss2: (0.0000) | Acc: (78.00%) (1108/1408)
Epoch: 29 | Batch_idx: 20 |  Loss: (0.6376) |  Loss2: (0.0000) | Acc: (78.00%) (2112/2688)
Epoch: 29 | Batch_idx: 30 |  Loss: (0.6298) |  Loss2: (0.0000) | Acc: (78.00%) (3108/3968)
Epoch: 29 | Batch_idx: 40 |  Loss: (0.6372) |  Loss2: (0.0000) | Acc: (78.00%) (4096/5248)
Epoch: 29 | Batch_idx: 50 |  Loss: (0.6395) |  Loss2: (0.0000) | Acc: (78.00%) (5096/6528)
Epoch: 29 | Batch_idx: 60 |  Loss: (0.6471) |  Loss2: (0.0000) | Acc: (77.00%) (6051/7808)
Epoch: 29 | Batch_idx: 70 |  Loss: (0.6477) |  Loss2: (0.0000) | Acc: (77.00%) (7017/9088)
Epoch: 29 | Batch_idx: 80 |  Loss: (0.6506) |  Loss2: (0.0000) | Acc: (77.00%) (8001/10368)
Epoch: 29 | Batch_idx: 90 |  Loss: (0.6439) |  Loss2: (0.0000) | Acc: (77.00%) (9013/11648)
Epoch: 29 | Batch_idx: 100 |  Loss: (0.6415) |  Loss2: (0.0000) | Acc: (77.00%) (10022/12928)
Epoch: 29 | Batch_idx: 110 |  Loss: (0.6374) |  Loss2: (0.0000) | Acc: (77.00%) (11021/14208)
Epoch: 29 | Batch_idx: 120 |  Loss: (0.6377) |  Loss2: (0.0000) | Acc: (77.00%) (12019/15488)
Epoch: 29 | Batch_idx: 130 |  Loss: (0.6364) |  Loss2: (0.0000) | Acc: (77.00%) (13029/16768)
Epoch: 29 | Batch_idx: 140 |  Loss: (0.6353) |  Loss2: (0.0000) | Acc: (77.00%) (14030/18048)
Epoch: 29 | Batch_idx: 150 |  Loss: (0.6376) |  Loss2: (0.0000) | Acc: (77.00%) (15026/19328)
Epoch: 29 | Batch_idx: 160 |  Loss: (0.6379) |  Loss2: (0.0000) | Acc: (77.00%) (16018/20608)
Epoch: 29 | Batch_idx: 170 |  Loss: (0.6360) |  Loss2: (0.0000) | Acc: (77.00%) (17024/21888)
Epoch: 29 | Batch_idx: 180 |  Loss: (0.6345) |  Loss2: (0.0000) | Acc: (77.00%) (18032/23168)
Epoch: 29 | Batch_idx: 190 |  Loss: (0.6347) |  Loss2: (0.0000) | Acc: (77.00%) (19028/24448)
Epoch: 29 | Batch_idx: 200 |  Loss: (0.6362) |  Loss2: (0.0000) | Acc: (77.00%) (20008/25728)
Epoch: 29 | Batch_idx: 210 |  Loss: (0.6342) |  Loss2: (0.0000) | Acc: (77.00%) (21024/27008)
Epoch: 29 | Batch_idx: 220 |  Loss: (0.6348) |  Loss2: (0.0000) | Acc: (77.00%) (22016/28288)
Epoch: 29 | Batch_idx: 230 |  Loss: (0.6339) |  Loss2: (0.0000) | Acc: (77.00%) (23030/29568)
Epoch: 29 | Batch_idx: 240 |  Loss: (0.6314) |  Loss2: (0.0000) | Acc: (78.00%) (24062/30848)
Epoch: 29 | Batch_idx: 250 |  Loss: (0.6315) |  Loss2: (0.0000) | Acc: (78.00%) (25063/32128)
Epoch: 29 | Batch_idx: 260 |  Loss: (0.6310) |  Loss2: (0.0000) | Acc: (78.00%) (26071/33408)
Epoch: 29 | Batch_idx: 270 |  Loss: (0.6317) |  Loss2: (0.0000) | Acc: (78.00%) (27067/34688)
Epoch: 29 | Batch_idx: 280 |  Loss: (0.6320) |  Loss2: (0.0000) | Acc: (78.00%) (28062/35968)
Epoch: 29 | Batch_idx: 290 |  Loss: (0.6318) |  Loss2: (0.0000) | Acc: (78.00%) (29082/37248)
Epoch: 29 | Batch_idx: 300 |  Loss: (0.6315) |  Loss2: (0.0000) | Acc: (78.00%) (30091/38528)
Epoch: 29 | Batch_idx: 310 |  Loss: (0.6316) |  Loss2: (0.0000) | Acc: (78.00%) (31092/39808)
Epoch: 29 | Batch_idx: 320 |  Loss: (0.6322) |  Loss2: (0.0000) | Acc: (78.00%) (32077/41088)
Epoch: 29 | Batch_idx: 330 |  Loss: (0.6323) |  Loss2: (0.0000) | Acc: (78.00%) (33067/42368)
Epoch: 29 | Batch_idx: 340 |  Loss: (0.6318) |  Loss2: (0.0000) | Acc: (78.00%) (34068/43648)
Epoch: 29 | Batch_idx: 350 |  Loss: (0.6319) |  Loss2: (0.0000) | Acc: (78.00%) (35059/44928)
Epoch: 29 | Batch_idx: 360 |  Loss: (0.6322) |  Loss2: (0.0000) | Acc: (78.00%) (36050/46208)
Epoch: 29 | Batch_idx: 370 |  Loss: (0.6324) |  Loss2: (0.0000) | Acc: (77.00%) (37035/47488)
Epoch: 29 | Batch_idx: 380 |  Loss: (0.6328) |  Loss2: (0.0000) | Acc: (77.00%) (38034/48768)
Epoch: 29 | Batch_idx: 390 |  Loss: (0.6327) |  Loss2: (0.0000) | Acc: (78.00%) (39001/50000)
# TEST : Loss: (0.6640) | Acc: (77.00%) (7712/10000)
percent tensor([0.4824, 0.5176], device='cuda:0')
percent tensor([0.4768, 0.5232], device='cuda:0')
percent tensor([0.5086, 0.4914], device='cuda:0')
percent tensor([0.5101, 0.4899], device='cuda:0')
percent tensor([0.5224, 0.4776], device='cuda:0')
percent tensor([0.6752, 0.3248], device='cuda:0')
percent tensor([0.6255, 0.3745], device='cuda:0')
percent tensor([0.9927, 0.0073], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 30 | Batch_idx: 0 |  Loss: (0.6352) |  Loss2: (0.0000) | Acc: (79.00%) (102/128)
Epoch: 30 | Batch_idx: 10 |  Loss: (0.6390) |  Loss2: (0.0000) | Acc: (78.00%) (1107/1408)
Epoch: 30 | Batch_idx: 20 |  Loss: (0.6479) |  Loss2: (0.0000) | Acc: (78.00%) (2101/2688)
Epoch: 30 | Batch_idx: 30 |  Loss: (0.6510) |  Loss2: (0.0000) | Acc: (77.00%) (3084/3968)
Epoch: 30 | Batch_idx: 40 |  Loss: (0.6351) |  Loss2: (0.0000) | Acc: (78.00%) (4103/5248)
Epoch: 30 | Batch_idx: 50 |  Loss: (0.6280) |  Loss2: (0.0000) | Acc: (78.00%) (5123/6528)
Epoch: 30 | Batch_idx: 60 |  Loss: (0.6322) |  Loss2: (0.0000) | Acc: (78.00%) (6112/7808)
Epoch: 30 | Batch_idx: 70 |  Loss: (0.6287) |  Loss2: (0.0000) | Acc: (78.00%) (7119/9088)
Epoch: 30 | Batch_idx: 80 |  Loss: (0.6313) |  Loss2: (0.0000) | Acc: (78.00%) (8110/10368)
Epoch: 30 | Batch_idx: 90 |  Loss: (0.6338) |  Loss2: (0.0000) | Acc: (78.00%) (9091/11648)
Epoch: 30 | Batch_idx: 100 |  Loss: (0.6321) |  Loss2: (0.0000) | Acc: (78.00%) (10106/12928)
Epoch: 30 | Batch_idx: 110 |  Loss: (0.6317) |  Loss2: (0.0000) | Acc: (78.00%) (11102/14208)
Epoch: 30 | Batch_idx: 120 |  Loss: (0.6303) |  Loss2: (0.0000) | Acc: (78.00%) (12107/15488)
Epoch: 30 | Batch_idx: 130 |  Loss: (0.6294) |  Loss2: (0.0000) | Acc: (78.00%) (13116/16768)
Epoch: 30 | Batch_idx: 140 |  Loss: (0.6280) |  Loss2: (0.0000) | Acc: (78.00%) (14131/18048)
Epoch: 30 | Batch_idx: 150 |  Loss: (0.6280) |  Loss2: (0.0000) | Acc: (78.00%) (15131/19328)
Epoch: 30 | Batch_idx: 160 |  Loss: (0.6276) |  Loss2: (0.0000) | Acc: (78.00%) (16148/20608)
Epoch: 30 | Batch_idx: 170 |  Loss: (0.6289) |  Loss2: (0.0000) | Acc: (78.00%) (17137/21888)
Epoch: 30 | Batch_idx: 180 |  Loss: (0.6282) |  Loss2: (0.0000) | Acc: (78.00%) (18138/23168)
Epoch: 30 | Batch_idx: 190 |  Loss: (0.6271) |  Loss2: (0.0000) | Acc: (78.00%) (19145/24448)
Epoch: 30 | Batch_idx: 200 |  Loss: (0.6248) |  Loss2: (0.0000) | Acc: (78.00%) (20176/25728)
Epoch: 30 | Batch_idx: 210 |  Loss: (0.6257) |  Loss2: (0.0000) | Acc: (78.00%) (21151/27008)
Epoch: 30 | Batch_idx: 220 |  Loss: (0.6268) |  Loss2: (0.0000) | Acc: (78.00%) (22137/28288)
Epoch: 30 | Batch_idx: 230 |  Loss: (0.6244) |  Loss2: (0.0000) | Acc: (78.00%) (23166/29568)
Epoch: 30 | Batch_idx: 240 |  Loss: (0.6260) |  Loss2: (0.0000) | Acc: (78.00%) (24155/30848)
Epoch: 30 | Batch_idx: 250 |  Loss: (0.6265) |  Loss2: (0.0000) | Acc: (78.00%) (25134/32128)
Epoch: 30 | Batch_idx: 260 |  Loss: (0.6251) |  Loss2: (0.0000) | Acc: (78.00%) (26145/33408)
Epoch: 30 | Batch_idx: 270 |  Loss: (0.6250) |  Loss2: (0.0000) | Acc: (78.00%) (27152/34688)
Epoch: 30 | Batch_idx: 280 |  Loss: (0.6241) |  Loss2: (0.0000) | Acc: (78.00%) (28161/35968)
Epoch: 30 | Batch_idx: 290 |  Loss: (0.6229) |  Loss2: (0.0000) | Acc: (78.00%) (29191/37248)
Epoch: 30 | Batch_idx: 300 |  Loss: (0.6236) |  Loss2: (0.0000) | Acc: (78.00%) (30187/38528)
Epoch: 30 | Batch_idx: 310 |  Loss: (0.6224) |  Loss2: (0.0000) | Acc: (78.00%) (31204/39808)
Epoch: 30 | Batch_idx: 320 |  Loss: (0.6219) |  Loss2: (0.0000) | Acc: (78.00%) (32213/41088)
Epoch: 30 | Batch_idx: 330 |  Loss: (0.6209) |  Loss2: (0.0000) | Acc: (78.00%) (33236/42368)
Epoch: 30 | Batch_idx: 340 |  Loss: (0.6197) |  Loss2: (0.0000) | Acc: (78.00%) (34261/43648)
Epoch: 30 | Batch_idx: 350 |  Loss: (0.6192) |  Loss2: (0.0000) | Acc: (78.00%) (35265/44928)
Epoch: 30 | Batch_idx: 360 |  Loss: (0.6180) |  Loss2: (0.0000) | Acc: (78.00%) (36288/46208)
Epoch: 30 | Batch_idx: 370 |  Loss: (0.6167) |  Loss2: (0.0000) | Acc: (78.00%) (37306/47488)
Epoch: 30 | Batch_idx: 380 |  Loss: (0.6169) |  Loss2: (0.0000) | Acc: (78.00%) (38299/48768)
Epoch: 30 | Batch_idx: 390 |  Loss: (0.6167) |  Loss2: (0.0000) | Acc: (78.00%) (39279/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_030.pth.tar'
# TEST : Loss: (0.8294) | Acc: (72.00%) (7247/10000)
percent tensor([0.4828, 0.5172], device='cuda:0')
percent tensor([0.4769, 0.5231], device='cuda:0')
percent tensor([0.5083, 0.4917], device='cuda:0')
percent tensor([0.5101, 0.4899], device='cuda:0')
percent tensor([0.5227, 0.4773], device='cuda:0')
percent tensor([0.6724, 0.3276], device='cuda:0')
percent tensor([0.6291, 0.3709], device='cuda:0')
percent tensor([0.9926, 0.0074], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(171.4595, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(791.2344, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(782.5692, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1524.1936, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(514.8881, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2175.9382, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4303.1338, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1429.2252, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6094.5366, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12139.0479, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4037.7922, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(17128.1250, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 31 | Batch_idx: 0 |  Loss: (0.6727) |  Loss2: (0.0000) | Acc: (78.00%) (101/128)
Epoch: 31 | Batch_idx: 10 |  Loss: (0.5889) |  Loss2: (0.0000) | Acc: (80.00%) (1130/1408)
Epoch: 31 | Batch_idx: 20 |  Loss: (0.5953) |  Loss2: (0.0000) | Acc: (80.00%) (2158/2688)
Epoch: 31 | Batch_idx: 30 |  Loss: (0.5951) |  Loss2: (0.0000) | Acc: (80.00%) (3182/3968)
Epoch: 31 | Batch_idx: 40 |  Loss: (0.5924) |  Loss2: (0.0000) | Acc: (79.00%) (4195/5248)
Epoch: 31 | Batch_idx: 50 |  Loss: (0.5874) |  Loss2: (0.0000) | Acc: (80.00%) (5224/6528)
Epoch: 31 | Batch_idx: 60 |  Loss: (0.5834) |  Loss2: (0.0000) | Acc: (80.00%) (6249/7808)
Epoch: 31 | Batch_idx: 70 |  Loss: (0.5834) |  Loss2: (0.0000) | Acc: (80.00%) (7272/9088)
Epoch: 31 | Batch_idx: 80 |  Loss: (0.5884) |  Loss2: (0.0000) | Acc: (79.00%) (8280/10368)
Epoch: 31 | Batch_idx: 90 |  Loss: (0.5861) |  Loss2: (0.0000) | Acc: (79.00%) (9316/11648)
Epoch: 31 | Batch_idx: 100 |  Loss: (0.5804) |  Loss2: (0.0000) | Acc: (80.00%) (10371/12928)
Epoch: 31 | Batch_idx: 110 |  Loss: (0.5830) |  Loss2: (0.0000) | Acc: (80.00%) (11375/14208)
Epoch: 31 | Batch_idx: 120 |  Loss: (0.5850) |  Loss2: (0.0000) | Acc: (80.00%) (12392/15488)
Epoch: 31 | Batch_idx: 130 |  Loss: (0.5866) |  Loss2: (0.0000) | Acc: (79.00%) (13406/16768)
Epoch: 31 | Batch_idx: 140 |  Loss: (0.5882) |  Loss2: (0.0000) | Acc: (79.00%) (14417/18048)
Epoch: 31 | Batch_idx: 150 |  Loss: (0.5892) |  Loss2: (0.0000) | Acc: (79.00%) (15441/19328)
Epoch: 31 | Batch_idx: 160 |  Loss: (0.5919) |  Loss2: (0.0000) | Acc: (79.00%) (16428/20608)
Epoch: 31 | Batch_idx: 170 |  Loss: (0.5897) |  Loss2: (0.0000) | Acc: (79.00%) (17471/21888)
Epoch: 31 | Batch_idx: 180 |  Loss: (0.5918) |  Loss2: (0.0000) | Acc: (79.00%) (18472/23168)
Epoch: 31 | Batch_idx: 190 |  Loss: (0.5933) |  Loss2: (0.0000) | Acc: (79.00%) (19487/24448)
Epoch: 31 | Batch_idx: 200 |  Loss: (0.5934) |  Loss2: (0.0000) | Acc: (79.00%) (20505/25728)
Epoch: 31 | Batch_idx: 210 |  Loss: (0.5910) |  Loss2: (0.0000) | Acc: (79.00%) (21547/27008)
Epoch: 31 | Batch_idx: 220 |  Loss: (0.5894) |  Loss2: (0.0000) | Acc: (79.00%) (22573/28288)
Epoch: 31 | Batch_idx: 230 |  Loss: (0.5909) |  Loss2: (0.0000) | Acc: (79.00%) (23568/29568)
Epoch: 31 | Batch_idx: 240 |  Loss: (0.5902) |  Loss2: (0.0000) | Acc: (79.00%) (24574/30848)
Epoch: 31 | Batch_idx: 250 |  Loss: (0.5891) |  Loss2: (0.0000) | Acc: (79.00%) (25593/32128)
Epoch: 31 | Batch_idx: 260 |  Loss: (0.5883) |  Loss2: (0.0000) | Acc: (79.00%) (26616/33408)
Epoch: 31 | Batch_idx: 270 |  Loss: (0.5882) |  Loss2: (0.0000) | Acc: (79.00%) (27643/34688)
Epoch: 31 | Batch_idx: 280 |  Loss: (0.5867) |  Loss2: (0.0000) | Acc: (79.00%) (28675/35968)
Epoch: 31 | Batch_idx: 290 |  Loss: (0.5879) |  Loss2: (0.0000) | Acc: (79.00%) (29675/37248)
Epoch: 31 | Batch_idx: 300 |  Loss: (0.5879) |  Loss2: (0.0000) | Acc: (79.00%) (30699/38528)
Epoch: 31 | Batch_idx: 310 |  Loss: (0.5878) |  Loss2: (0.0000) | Acc: (79.00%) (31714/39808)
Epoch: 31 | Batch_idx: 320 |  Loss: (0.5890) |  Loss2: (0.0000) | Acc: (79.00%) (32708/41088)
Epoch: 31 | Batch_idx: 330 |  Loss: (0.5876) |  Loss2: (0.0000) | Acc: (79.00%) (33760/42368)
Epoch: 31 | Batch_idx: 340 |  Loss: (0.5883) |  Loss2: (0.0000) | Acc: (79.00%) (34765/43648)
Epoch: 31 | Batch_idx: 350 |  Loss: (0.5889) |  Loss2: (0.0000) | Acc: (79.00%) (35768/44928)
Epoch: 31 | Batch_idx: 360 |  Loss: (0.5886) |  Loss2: (0.0000) | Acc: (79.00%) (36807/46208)
Epoch: 31 | Batch_idx: 370 |  Loss: (0.5883) |  Loss2: (0.0000) | Acc: (79.00%) (37825/47488)
Epoch: 31 | Batch_idx: 380 |  Loss: (0.5885) |  Loss2: (0.0000) | Acc: (79.00%) (38853/48768)
Epoch: 31 | Batch_idx: 390 |  Loss: (0.5886) |  Loss2: (0.0000) | Acc: (79.00%) (39832/50000)
# TEST : Loss: (0.6959) | Acc: (76.00%) (7629/10000)
percent tensor([0.4823, 0.5177], device='cuda:0')
percent tensor([0.4768, 0.5232], device='cuda:0')
percent tensor([0.5085, 0.4915], device='cuda:0')
percent tensor([0.5105, 0.4895], device='cuda:0')
percent tensor([0.5230, 0.4770], device='cuda:0')
percent tensor([0.6790, 0.3210], device='cuda:0')
percent tensor([0.6373, 0.3627], device='cuda:0')
percent tensor([0.9944, 0.0056], device='cuda:0')
Epoch: 32 | Batch_idx: 0 |  Loss: (0.6200) |  Loss2: (0.0000) | Acc: (77.00%) (99/128)
Epoch: 32 | Batch_idx: 10 |  Loss: (0.5185) |  Loss2: (0.0000) | Acc: (82.00%) (1157/1408)
Epoch: 32 | Batch_idx: 20 |  Loss: (0.5468) |  Loss2: (0.0000) | Acc: (81.00%) (2188/2688)
Epoch: 32 | Batch_idx: 30 |  Loss: (0.5533) |  Loss2: (0.0000) | Acc: (81.00%) (3215/3968)
Epoch: 32 | Batch_idx: 40 |  Loss: (0.5604) |  Loss2: (0.0000) | Acc: (80.00%) (4224/5248)
Epoch: 32 | Batch_idx: 50 |  Loss: (0.5668) |  Loss2: (0.0000) | Acc: (80.00%) (5251/6528)
Epoch: 32 | Batch_idx: 60 |  Loss: (0.5722) |  Loss2: (0.0000) | Acc: (80.00%) (6250/7808)
Epoch: 32 | Batch_idx: 70 |  Loss: (0.5709) |  Loss2: (0.0000) | Acc: (80.00%) (7277/9088)
Epoch: 32 | Batch_idx: 80 |  Loss: (0.5651) |  Loss2: (0.0000) | Acc: (80.00%) (8327/10368)
Epoch: 32 | Batch_idx: 90 |  Loss: (0.5674) |  Loss2: (0.0000) | Acc: (80.00%) (9351/11648)
Epoch: 32 | Batch_idx: 100 |  Loss: (0.5657) |  Loss2: (0.0000) | Acc: (80.00%) (10378/12928)
Epoch: 32 | Batch_idx: 110 |  Loss: (0.5667) |  Loss2: (0.0000) | Acc: (80.00%) (11400/14208)
Epoch: 32 | Batch_idx: 120 |  Loss: (0.5659) |  Loss2: (0.0000) | Acc: (80.00%) (12428/15488)
Epoch: 32 | Batch_idx: 130 |  Loss: (0.5677) |  Loss2: (0.0000) | Acc: (80.00%) (13431/16768)
Epoch: 32 | Batch_idx: 140 |  Loss: (0.5664) |  Loss2: (0.0000) | Acc: (80.00%) (14466/18048)
Epoch: 32 | Batch_idx: 150 |  Loss: (0.5635) |  Loss2: (0.0000) | Acc: (80.00%) (15511/19328)
Epoch: 32 | Batch_idx: 160 |  Loss: (0.5637) |  Loss2: (0.0000) | Acc: (80.00%) (16541/20608)
Epoch: 32 | Batch_idx: 170 |  Loss: (0.5626) |  Loss2: (0.0000) | Acc: (80.00%) (17569/21888)
Epoch: 32 | Batch_idx: 180 |  Loss: (0.5630) |  Loss2: (0.0000) | Acc: (80.00%) (18599/23168)
Epoch: 32 | Batch_idx: 190 |  Loss: (0.5631) |  Loss2: (0.0000) | Acc: (80.00%) (19622/24448)
Epoch: 32 | Batch_idx: 200 |  Loss: (0.5602) |  Loss2: (0.0000) | Acc: (80.00%) (20682/25728)
Epoch: 32 | Batch_idx: 210 |  Loss: (0.5595) |  Loss2: (0.0000) | Acc: (80.00%) (21705/27008)
Epoch: 32 | Batch_idx: 220 |  Loss: (0.5592) |  Loss2: (0.0000) | Acc: (80.00%) (22746/28288)
Epoch: 32 | Batch_idx: 230 |  Loss: (0.5590) |  Loss2: (0.0000) | Acc: (80.00%) (23787/29568)
Epoch: 32 | Batch_idx: 240 |  Loss: (0.5587) |  Loss2: (0.0000) | Acc: (80.00%) (24824/30848)
Epoch: 32 | Batch_idx: 250 |  Loss: (0.5588) |  Loss2: (0.0000) | Acc: (80.00%) (25841/32128)
Epoch: 32 | Batch_idx: 260 |  Loss: (0.5598) |  Loss2: (0.0000) | Acc: (80.00%) (26865/33408)
Epoch: 32 | Batch_idx: 270 |  Loss: (0.5602) |  Loss2: (0.0000) | Acc: (80.00%) (27884/34688)
Epoch: 32 | Batch_idx: 280 |  Loss: (0.5630) |  Loss2: (0.0000) | Acc: (80.00%) (28877/35968)
Epoch: 32 | Batch_idx: 290 |  Loss: (0.5631) |  Loss2: (0.0000) | Acc: (80.00%) (29905/37248)
Epoch: 32 | Batch_idx: 300 |  Loss: (0.5634) |  Loss2: (0.0000) | Acc: (80.00%) (30919/38528)
Epoch: 32 | Batch_idx: 310 |  Loss: (0.5638) |  Loss2: (0.0000) | Acc: (80.00%) (31942/39808)
Epoch: 32 | Batch_idx: 320 |  Loss: (0.5630) |  Loss2: (0.0000) | Acc: (80.00%) (32980/41088)
Epoch: 32 | Batch_idx: 330 |  Loss: (0.5617) |  Loss2: (0.0000) | Acc: (80.00%) (34025/42368)
Epoch: 32 | Batch_idx: 340 |  Loss: (0.5607) |  Loss2: (0.0000) | Acc: (80.00%) (35083/43648)
Epoch: 32 | Batch_idx: 350 |  Loss: (0.5616) |  Loss2: (0.0000) | Acc: (80.00%) (36109/44928)
Epoch: 32 | Batch_idx: 360 |  Loss: (0.5619) |  Loss2: (0.0000) | Acc: (80.00%) (37152/46208)
Epoch: 32 | Batch_idx: 370 |  Loss: (0.5617) |  Loss2: (0.0000) | Acc: (80.00%) (38192/47488)
Epoch: 32 | Batch_idx: 380 |  Loss: (0.5623) |  Loss2: (0.0000) | Acc: (80.00%) (39217/48768)
Epoch: 32 | Batch_idx: 390 |  Loss: (0.5622) |  Loss2: (0.0000) | Acc: (80.00%) (40205/50000)
# TEST : Loss: (0.8264) | Acc: (73.00%) (7324/10000)
percent tensor([0.4822, 0.5178], device='cuda:0')
percent tensor([0.4768, 0.5232], device='cuda:0')
percent tensor([0.5081, 0.4919], device='cuda:0')
percent tensor([0.5102, 0.4898], device='cuda:0')
percent tensor([0.5228, 0.4772], device='cuda:0')
percent tensor([0.6755, 0.3245], device='cuda:0')
percent tensor([0.6302, 0.3698], device='cuda:0')
percent tensor([0.9934, 0.0066], device='cuda:0')
Epoch: 33 | Batch_idx: 0 |  Loss: (0.5071) |  Loss2: (0.0000) | Acc: (82.00%) (105/128)
Epoch: 33 | Batch_idx: 10 |  Loss: (0.5465) |  Loss2: (0.0000) | Acc: (80.00%) (1128/1408)
Epoch: 33 | Batch_idx: 20 |  Loss: (0.5429) |  Loss2: (0.0000) | Acc: (80.00%) (2170/2688)
Epoch: 33 | Batch_idx: 30 |  Loss: (0.5395) |  Loss2: (0.0000) | Acc: (81.00%) (3215/3968)
Epoch: 33 | Batch_idx: 40 |  Loss: (0.5412) |  Loss2: (0.0000) | Acc: (80.00%) (4240/5248)
Epoch: 33 | Batch_idx: 50 |  Loss: (0.5396) |  Loss2: (0.0000) | Acc: (80.00%) (5282/6528)
Epoch: 33 | Batch_idx: 60 |  Loss: (0.5332) |  Loss2: (0.0000) | Acc: (81.00%) (6345/7808)
Epoch: 33 | Batch_idx: 70 |  Loss: (0.5323) |  Loss2: (0.0000) | Acc: (81.00%) (7394/9088)
Epoch: 33 | Batch_idx: 80 |  Loss: (0.5315) |  Loss2: (0.0000) | Acc: (81.00%) (8438/10368)
Epoch: 33 | Batch_idx: 90 |  Loss: (0.5357) |  Loss2: (0.0000) | Acc: (81.00%) (9466/11648)
Epoch: 33 | Batch_idx: 100 |  Loss: (0.5383) |  Loss2: (0.0000) | Acc: (81.00%) (10496/12928)
Epoch: 33 | Batch_idx: 110 |  Loss: (0.5360) |  Loss2: (0.0000) | Acc: (81.00%) (11553/14208)
Epoch: 33 | Batch_idx: 120 |  Loss: (0.5373) |  Loss2: (0.0000) | Acc: (81.00%) (12571/15488)
Epoch: 33 | Batch_idx: 130 |  Loss: (0.5369) |  Loss2: (0.0000) | Acc: (81.00%) (13616/16768)
Epoch: 33 | Batch_idx: 140 |  Loss: (0.5363) |  Loss2: (0.0000) | Acc: (81.00%) (14670/18048)
Epoch: 33 | Batch_idx: 150 |  Loss: (0.5397) |  Loss2: (0.0000) | Acc: (81.00%) (15684/19328)
Epoch: 33 | Batch_idx: 160 |  Loss: (0.5439) |  Loss2: (0.0000) | Acc: (81.00%) (16694/20608)
Epoch: 33 | Batch_idx: 170 |  Loss: (0.5431) |  Loss2: (0.0000) | Acc: (81.00%) (17738/21888)
Epoch: 33 | Batch_idx: 180 |  Loss: (0.5441) |  Loss2: (0.0000) | Acc: (81.00%) (18769/23168)
Epoch: 33 | Batch_idx: 190 |  Loss: (0.5430) |  Loss2: (0.0000) | Acc: (81.00%) (19835/24448)
Epoch: 33 | Batch_idx: 200 |  Loss: (0.5432) |  Loss2: (0.0000) | Acc: (81.00%) (20882/25728)
Epoch: 33 | Batch_idx: 210 |  Loss: (0.5394) |  Loss2: (0.0000) | Acc: (81.00%) (21975/27008)
Epoch: 33 | Batch_idx: 220 |  Loss: (0.5386) |  Loss2: (0.0000) | Acc: (81.00%) (23022/28288)
Epoch: 33 | Batch_idx: 230 |  Loss: (0.5389) |  Loss2: (0.0000) | Acc: (81.00%) (24066/29568)
Epoch: 33 | Batch_idx: 240 |  Loss: (0.5391) |  Loss2: (0.0000) | Acc: (81.00%) (25104/30848)
Epoch: 33 | Batch_idx: 250 |  Loss: (0.5380) |  Loss2: (0.0000) | Acc: (81.00%) (26157/32128)
Epoch: 33 | Batch_idx: 260 |  Loss: (0.5376) |  Loss2: (0.0000) | Acc: (81.00%) (27212/33408)
Epoch: 33 | Batch_idx: 270 |  Loss: (0.5376) |  Loss2: (0.0000) | Acc: (81.00%) (28266/34688)
Epoch: 33 | Batch_idx: 280 |  Loss: (0.5362) |  Loss2: (0.0000) | Acc: (81.00%) (29324/35968)
Epoch: 33 | Batch_idx: 290 |  Loss: (0.5358) |  Loss2: (0.0000) | Acc: (81.00%) (30378/37248)
Epoch: 33 | Batch_idx: 300 |  Loss: (0.5356) |  Loss2: (0.0000) | Acc: (81.00%) (31422/38528)
Epoch: 33 | Batch_idx: 310 |  Loss: (0.5359) |  Loss2: (0.0000) | Acc: (81.00%) (32449/39808)
Epoch: 33 | Batch_idx: 320 |  Loss: (0.5371) |  Loss2: (0.0000) | Acc: (81.00%) (33476/41088)
Epoch: 33 | Batch_idx: 330 |  Loss: (0.5372) |  Loss2: (0.0000) | Acc: (81.00%) (34513/42368)
Epoch: 33 | Batch_idx: 340 |  Loss: (0.5370) |  Loss2: (0.0000) | Acc: (81.00%) (35554/43648)
Epoch: 33 | Batch_idx: 350 |  Loss: (0.5373) |  Loss2: (0.0000) | Acc: (81.00%) (36597/44928)
Epoch: 33 | Batch_idx: 360 |  Loss: (0.5381) |  Loss2: (0.0000) | Acc: (81.00%) (37627/46208)
Epoch: 33 | Batch_idx: 370 |  Loss: (0.5379) |  Loss2: (0.0000) | Acc: (81.00%) (38678/47488)
Epoch: 33 | Batch_idx: 380 |  Loss: (0.5372) |  Loss2: (0.0000) | Acc: (81.00%) (39725/48768)
Epoch: 33 | Batch_idx: 390 |  Loss: (0.5367) |  Loss2: (0.0000) | Acc: (81.00%) (40750/50000)
# TEST : Loss: (0.7211) | Acc: (75.00%) (7504/10000)
percent tensor([0.4826, 0.5174], device='cuda:0')
percent tensor([0.4768, 0.5232], device='cuda:0')
percent tensor([0.5086, 0.4914], device='cuda:0')
percent tensor([0.5102, 0.4898], device='cuda:0')
percent tensor([0.5219, 0.4781], device='cuda:0')
percent tensor([0.6731, 0.3269], device='cuda:0')
percent tensor([0.6248, 0.3752], device='cuda:0')
percent tensor([0.9931, 0.0069], device='cuda:0')
Epoch: 34 | Batch_idx: 0 |  Loss: (0.4645) |  Loss2: (0.0000) | Acc: (80.00%) (103/128)
Epoch: 34 | Batch_idx: 10 |  Loss: (0.5149) |  Loss2: (0.0000) | Acc: (82.00%) (1167/1408)
Epoch: 34 | Batch_idx: 20 |  Loss: (0.5008) |  Loss2: (0.0000) | Acc: (83.00%) (2241/2688)
Epoch: 34 | Batch_idx: 30 |  Loss: (0.5004) |  Loss2: (0.0000) | Acc: (83.00%) (3298/3968)
Epoch: 34 | Batch_idx: 40 |  Loss: (0.5076) |  Loss2: (0.0000) | Acc: (82.00%) (4355/5248)
Epoch: 34 | Batch_idx: 50 |  Loss: (0.5171) |  Loss2: (0.0000) | Acc: (82.00%) (5384/6528)
Epoch: 34 | Batch_idx: 60 |  Loss: (0.5181) |  Loss2: (0.0000) | Acc: (82.00%) (6429/7808)
Epoch: 34 | Batch_idx: 70 |  Loss: (0.5225) |  Loss2: (0.0000) | Acc: (82.00%) (7464/9088)
Epoch: 34 | Batch_idx: 80 |  Loss: (0.5187) |  Loss2: (0.0000) | Acc: (82.00%) (8534/10368)
Epoch: 34 | Batch_idx: 90 |  Loss: (0.5193) |  Loss2: (0.0000) | Acc: (82.00%) (9581/11648)
Epoch: 34 | Batch_idx: 100 |  Loss: (0.5179) |  Loss2: (0.0000) | Acc: (82.00%) (10654/12928)
Epoch: 34 | Batch_idx: 110 |  Loss: (0.5162) |  Loss2: (0.0000) | Acc: (82.00%) (11719/14208)
Epoch: 34 | Batch_idx: 120 |  Loss: (0.5165) |  Loss2: (0.0000) | Acc: (82.00%) (12764/15488)
Epoch: 34 | Batch_idx: 130 |  Loss: (0.5139) |  Loss2: (0.0000) | Acc: (82.00%) (13832/16768)
Epoch: 34 | Batch_idx: 140 |  Loss: (0.5136) |  Loss2: (0.0000) | Acc: (82.00%) (14886/18048)
Epoch: 34 | Batch_idx: 150 |  Loss: (0.5130) |  Loss2: (0.0000) | Acc: (82.00%) (15943/19328)
Epoch: 34 | Batch_idx: 160 |  Loss: (0.5110) |  Loss2: (0.0000) | Acc: (82.00%) (17008/20608)
Epoch: 34 | Batch_idx: 170 |  Loss: (0.5121) |  Loss2: (0.0000) | Acc: (82.00%) (18049/21888)
Epoch: 34 | Batch_idx: 180 |  Loss: (0.5104) |  Loss2: (0.0000) | Acc: (82.00%) (19119/23168)
Epoch: 34 | Batch_idx: 190 |  Loss: (0.5105) |  Loss2: (0.0000) | Acc: (82.00%) (20181/24448)
Epoch: 34 | Batch_idx: 200 |  Loss: (0.5118) |  Loss2: (0.0000) | Acc: (82.00%) (21218/25728)
Epoch: 34 | Batch_idx: 210 |  Loss: (0.5127) |  Loss2: (0.0000) | Acc: (82.00%) (22266/27008)
Epoch: 34 | Batch_idx: 220 |  Loss: (0.5128) |  Loss2: (0.0000) | Acc: (82.00%) (23327/28288)
Epoch: 34 | Batch_idx: 230 |  Loss: (0.5147) |  Loss2: (0.0000) | Acc: (82.00%) (24352/29568)
Epoch: 34 | Batch_idx: 240 |  Loss: (0.5163) |  Loss2: (0.0000) | Acc: (82.00%) (25392/30848)
Epoch: 34 | Batch_idx: 250 |  Loss: (0.5159) |  Loss2: (0.0000) | Acc: (82.00%) (26459/32128)
Epoch: 34 | Batch_idx: 260 |  Loss: (0.5175) |  Loss2: (0.0000) | Acc: (82.00%) (27487/33408)
Epoch: 34 | Batch_idx: 270 |  Loss: (0.5177) |  Loss2: (0.0000) | Acc: (82.00%) (28546/34688)
Epoch: 34 | Batch_idx: 280 |  Loss: (0.5172) |  Loss2: (0.0000) | Acc: (82.00%) (29609/35968)
Epoch: 34 | Batch_idx: 290 |  Loss: (0.5172) |  Loss2: (0.0000) | Acc: (82.00%) (30649/37248)
Epoch: 34 | Batch_idx: 300 |  Loss: (0.5179) |  Loss2: (0.0000) | Acc: (82.00%) (31691/38528)
Epoch: 34 | Batch_idx: 310 |  Loss: (0.5183) |  Loss2: (0.0000) | Acc: (82.00%) (32730/39808)
Epoch: 34 | Batch_idx: 320 |  Loss: (0.5184) |  Loss2: (0.0000) | Acc: (82.00%) (33791/41088)
Epoch: 34 | Batch_idx: 330 |  Loss: (0.5180) |  Loss2: (0.0000) | Acc: (82.00%) (34848/42368)
Epoch: 34 | Batch_idx: 340 |  Loss: (0.5171) |  Loss2: (0.0000) | Acc: (82.00%) (35916/43648)
Epoch: 34 | Batch_idx: 350 |  Loss: (0.5169) |  Loss2: (0.0000) | Acc: (82.00%) (36974/44928)
Epoch: 34 | Batch_idx: 360 |  Loss: (0.5176) |  Loss2: (0.0000) | Acc: (82.00%) (38020/46208)
Epoch: 34 | Batch_idx: 370 |  Loss: (0.5180) |  Loss2: (0.0000) | Acc: (82.00%) (39056/47488)
Epoch: 34 | Batch_idx: 380 |  Loss: (0.5182) |  Loss2: (0.0000) | Acc: (82.00%) (40106/48768)
Epoch: 34 | Batch_idx: 390 |  Loss: (0.5181) |  Loss2: (0.0000) | Acc: (82.00%) (41117/50000)
# TEST : Loss: (0.6679) | Acc: (77.00%) (7729/10000)
percent tensor([0.4826, 0.5174], device='cuda:0')
percent tensor([0.4764, 0.5236], device='cuda:0')
percent tensor([0.5086, 0.4914], device='cuda:0')
percent tensor([0.5104, 0.4896], device='cuda:0')
percent tensor([0.5224, 0.4776], device='cuda:0')
percent tensor([0.6750, 0.3250], device='cuda:0')
percent tensor([0.6337, 0.3663], device='cuda:0')
percent tensor([0.9940, 0.0060], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 35 | Batch_idx: 0 |  Loss: (0.4409) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 35 | Batch_idx: 10 |  Loss: (0.5120) |  Loss2: (0.0000) | Acc: (82.00%) (1159/1408)
Epoch: 35 | Batch_idx: 20 |  Loss: (0.5605) |  Loss2: (0.0000) | Acc: (80.00%) (2164/2688)
Epoch: 35 | Batch_idx: 30 |  Loss: (0.5977) |  Loss2: (0.0000) | Acc: (79.00%) (3137/3968)
Epoch: 35 | Batch_idx: 40 |  Loss: (0.6159) |  Loss2: (0.0000) | Acc: (78.00%) (4110/5248)
Epoch: 35 | Batch_idx: 50 |  Loss: (0.6350) |  Loss2: (0.0000) | Acc: (77.00%) (5064/6528)
Epoch: 35 | Batch_idx: 60 |  Loss: (0.6343) |  Loss2: (0.0000) | Acc: (77.00%) (6060/7808)
Epoch: 35 | Batch_idx: 70 |  Loss: (0.6310) |  Loss2: (0.0000) | Acc: (77.00%) (7062/9088)
Epoch: 35 | Batch_idx: 80 |  Loss: (0.6330) |  Loss2: (0.0000) | Acc: (77.00%) (8045/10368)
Epoch: 35 | Batch_idx: 90 |  Loss: (0.6375) |  Loss2: (0.0000) | Acc: (77.00%) (9021/11648)
Epoch: 35 | Batch_idx: 100 |  Loss: (0.6399) |  Loss2: (0.0000) | Acc: (77.00%) (10002/12928)
Epoch: 35 | Batch_idx: 110 |  Loss: (0.6374) |  Loss2: (0.0000) | Acc: (77.00%) (11008/14208)
Epoch: 35 | Batch_idx: 120 |  Loss: (0.6387) |  Loss2: (0.0000) | Acc: (77.00%) (12001/15488)
Epoch: 35 | Batch_idx: 130 |  Loss: (0.6354) |  Loss2: (0.0000) | Acc: (77.00%) (12999/16768)
Epoch: 35 | Batch_idx: 140 |  Loss: (0.6345) |  Loss2: (0.0000) | Acc: (77.00%) (13993/18048)
Epoch: 35 | Batch_idx: 150 |  Loss: (0.6311) |  Loss2: (0.0000) | Acc: (77.00%) (15020/19328)
Epoch: 35 | Batch_idx: 160 |  Loss: (0.6315) |  Loss2: (0.0000) | Acc: (77.00%) (16020/20608)
Epoch: 35 | Batch_idx: 170 |  Loss: (0.6298) |  Loss2: (0.0000) | Acc: (77.00%) (17034/21888)
Epoch: 35 | Batch_idx: 180 |  Loss: (0.6296) |  Loss2: (0.0000) | Acc: (77.00%) (18034/23168)
Epoch: 35 | Batch_idx: 190 |  Loss: (0.6283) |  Loss2: (0.0000) | Acc: (77.00%) (19050/24448)
Epoch: 35 | Batch_idx: 200 |  Loss: (0.6265) |  Loss2: (0.0000) | Acc: (77.00%) (20066/25728)
Epoch: 35 | Batch_idx: 210 |  Loss: (0.6285) |  Loss2: (0.0000) | Acc: (77.00%) (21053/27008)
Epoch: 35 | Batch_idx: 220 |  Loss: (0.6272) |  Loss2: (0.0000) | Acc: (77.00%) (22049/28288)
Epoch: 35 | Batch_idx: 230 |  Loss: (0.6253) |  Loss2: (0.0000) | Acc: (78.00%) (23092/29568)
Epoch: 35 | Batch_idx: 240 |  Loss: (0.6236) |  Loss2: (0.0000) | Acc: (78.00%) (24120/30848)
Epoch: 35 | Batch_idx: 250 |  Loss: (0.6243) |  Loss2: (0.0000) | Acc: (78.00%) (25103/32128)
Epoch: 35 | Batch_idx: 260 |  Loss: (0.6231) |  Loss2: (0.0000) | Acc: (78.00%) (26116/33408)
Epoch: 35 | Batch_idx: 270 |  Loss: (0.6217) |  Loss2: (0.0000) | Acc: (78.00%) (27138/34688)
Epoch: 35 | Batch_idx: 280 |  Loss: (0.6206) |  Loss2: (0.0000) | Acc: (78.00%) (28155/35968)
Epoch: 35 | Batch_idx: 290 |  Loss: (0.6183) |  Loss2: (0.0000) | Acc: (78.00%) (29191/37248)
Epoch: 35 | Batch_idx: 300 |  Loss: (0.6170) |  Loss2: (0.0000) | Acc: (78.00%) (30211/38528)
Epoch: 35 | Batch_idx: 310 |  Loss: (0.6149) |  Loss2: (0.0000) | Acc: (78.00%) (31247/39808)
Epoch: 35 | Batch_idx: 320 |  Loss: (0.6136) |  Loss2: (0.0000) | Acc: (78.00%) (32258/41088)
Epoch: 35 | Batch_idx: 330 |  Loss: (0.6132) |  Loss2: (0.0000) | Acc: (78.00%) (33263/42368)
Epoch: 35 | Batch_idx: 340 |  Loss: (0.6107) |  Loss2: (0.0000) | Acc: (78.00%) (34314/43648)
Epoch: 35 | Batch_idx: 350 |  Loss: (0.6096) |  Loss2: (0.0000) | Acc: (78.00%) (35318/44928)
Epoch: 35 | Batch_idx: 360 |  Loss: (0.6081) |  Loss2: (0.0000) | Acc: (78.00%) (36359/46208)
Epoch: 35 | Batch_idx: 370 |  Loss: (0.6072) |  Loss2: (0.0000) | Acc: (78.00%) (37379/47488)
Epoch: 35 | Batch_idx: 380 |  Loss: (0.6066) |  Loss2: (0.0000) | Acc: (78.00%) (38409/48768)
Epoch: 35 | Batch_idx: 390 |  Loss: (0.6058) |  Loss2: (0.0000) | Acc: (78.00%) (39394/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_035.pth.tar'
# TEST : Loss: (0.6393) | Acc: (78.00%) (7813/10000)
percent tensor([0.4786, 0.5214], device='cuda:0')
percent tensor([0.4690, 0.5310], device='cuda:0')
percent tensor([0.5052, 0.4948], device='cuda:0')
percent tensor([0.5033, 0.4967], device='cuda:0')
percent tensor([0.5171, 0.4829], device='cuda:0')
percent tensor([0.6304, 0.3696], device='cuda:0')
percent tensor([0.6019, 0.3981], device='cuda:0')
percent tensor([0.9933, 0.0067], device='cuda:0')
Epoch: 36 | Batch_idx: 0 |  Loss: (0.5718) |  Loss2: (0.0000) | Acc: (80.00%) (103/128)
Epoch: 36 | Batch_idx: 10 |  Loss: (0.5628) |  Loss2: (0.0000) | Acc: (79.00%) (1118/1408)
Epoch: 36 | Batch_idx: 20 |  Loss: (0.5644) |  Loss2: (0.0000) | Acc: (79.00%) (2147/2688)
Epoch: 36 | Batch_idx: 30 |  Loss: (0.5643) |  Loss2: (0.0000) | Acc: (80.00%) (3176/3968)
Epoch: 36 | Batch_idx: 40 |  Loss: (0.5639) |  Loss2: (0.0000) | Acc: (80.00%) (4212/5248)
Epoch: 36 | Batch_idx: 50 |  Loss: (0.5688) |  Loss2: (0.0000) | Acc: (80.00%) (5237/6528)
Epoch: 36 | Batch_idx: 60 |  Loss: (0.5666) |  Loss2: (0.0000) | Acc: (80.00%) (6269/7808)
Epoch: 36 | Batch_idx: 70 |  Loss: (0.5645) |  Loss2: (0.0000) | Acc: (80.00%) (7304/9088)
Epoch: 36 | Batch_idx: 80 |  Loss: (0.5591) |  Loss2: (0.0000) | Acc: (80.00%) (8360/10368)
Epoch: 36 | Batch_idx: 90 |  Loss: (0.5564) |  Loss2: (0.0000) | Acc: (80.00%) (9402/11648)
Epoch: 36 | Batch_idx: 100 |  Loss: (0.5528) |  Loss2: (0.0000) | Acc: (80.00%) (10463/12928)
Epoch: 36 | Batch_idx: 110 |  Loss: (0.5536) |  Loss2: (0.0000) | Acc: (80.00%) (11496/14208)
Epoch: 36 | Batch_idx: 120 |  Loss: (0.5575) |  Loss2: (0.0000) | Acc: (80.00%) (12518/15488)
Epoch: 36 | Batch_idx: 130 |  Loss: (0.5597) |  Loss2: (0.0000) | Acc: (80.00%) (13539/16768)
Epoch: 36 | Batch_idx: 140 |  Loss: (0.5582) |  Loss2: (0.0000) | Acc: (80.00%) (14574/18048)
Epoch: 36 | Batch_idx: 150 |  Loss: (0.5557) |  Loss2: (0.0000) | Acc: (80.00%) (15636/19328)
Epoch: 36 | Batch_idx: 160 |  Loss: (0.5541) |  Loss2: (0.0000) | Acc: (80.00%) (16679/20608)
Epoch: 36 | Batch_idx: 170 |  Loss: (0.5536) |  Loss2: (0.0000) | Acc: (80.00%) (17706/21888)
Epoch: 36 | Batch_idx: 180 |  Loss: (0.5515) |  Loss2: (0.0000) | Acc: (80.00%) (18766/23168)
Epoch: 36 | Batch_idx: 190 |  Loss: (0.5526) |  Loss2: (0.0000) | Acc: (80.00%) (19798/24448)
Epoch: 36 | Batch_idx: 200 |  Loss: (0.5505) |  Loss2: (0.0000) | Acc: (81.00%) (20868/25728)
Epoch: 36 | Batch_idx: 210 |  Loss: (0.5501) |  Loss2: (0.0000) | Acc: (81.00%) (21920/27008)
Epoch: 36 | Batch_idx: 220 |  Loss: (0.5483) |  Loss2: (0.0000) | Acc: (81.00%) (22975/28288)
Epoch: 36 | Batch_idx: 230 |  Loss: (0.5505) |  Loss2: (0.0000) | Acc: (81.00%) (23988/29568)
Epoch: 36 | Batch_idx: 240 |  Loss: (0.5496) |  Loss2: (0.0000) | Acc: (81.00%) (25035/30848)
Epoch: 36 | Batch_idx: 250 |  Loss: (0.5503) |  Loss2: (0.0000) | Acc: (81.00%) (26068/32128)
Epoch: 36 | Batch_idx: 260 |  Loss: (0.5494) |  Loss2: (0.0000) | Acc: (81.00%) (27113/33408)
Epoch: 36 | Batch_idx: 270 |  Loss: (0.5505) |  Loss2: (0.0000) | Acc: (81.00%) (28137/34688)
Epoch: 36 | Batch_idx: 280 |  Loss: (0.5527) |  Loss2: (0.0000) | Acc: (80.00%) (29134/35968)
Epoch: 36 | Batch_idx: 290 |  Loss: (0.5525) |  Loss2: (0.0000) | Acc: (80.00%) (30168/37248)
Epoch: 36 | Batch_idx: 300 |  Loss: (0.5522) |  Loss2: (0.0000) | Acc: (80.00%) (31204/38528)
Epoch: 36 | Batch_idx: 310 |  Loss: (0.5517) |  Loss2: (0.0000) | Acc: (81.00%) (32259/39808)
Epoch: 36 | Batch_idx: 320 |  Loss: (0.5535) |  Loss2: (0.0000) | Acc: (80.00%) (33277/41088)
Epoch: 36 | Batch_idx: 330 |  Loss: (0.5535) |  Loss2: (0.0000) | Acc: (81.00%) (34324/42368)
Epoch: 36 | Batch_idx: 340 |  Loss: (0.5537) |  Loss2: (0.0000) | Acc: (81.00%) (35362/43648)
Epoch: 36 | Batch_idx: 350 |  Loss: (0.5558) |  Loss2: (0.0000) | Acc: (80.00%) (36376/44928)
Epoch: 36 | Batch_idx: 360 |  Loss: (0.5555) |  Loss2: (0.0000) | Acc: (80.00%) (37411/46208)
Epoch: 36 | Batch_idx: 370 |  Loss: (0.5551) |  Loss2: (0.0000) | Acc: (80.00%) (38457/47488)
Epoch: 36 | Batch_idx: 380 |  Loss: (0.5551) |  Loss2: (0.0000) | Acc: (80.00%) (39493/48768)
Epoch: 36 | Batch_idx: 390 |  Loss: (0.5556) |  Loss2: (0.0000) | Acc: (80.00%) (40468/50000)
# TEST : Loss: (0.6011) | Acc: (79.00%) (7926/10000)
percent tensor([0.4809, 0.5191], device='cuda:0')
percent tensor([0.4671, 0.5329], device='cuda:0')
percent tensor([0.5076, 0.4924], device='cuda:0')
percent tensor([0.5035, 0.4965], device='cuda:0')
percent tensor([0.5211, 0.4789], device='cuda:0')
percent tensor([0.6319, 0.3681], device='cuda:0')
percent tensor([0.6063, 0.3937], device='cuda:0')
percent tensor([0.9941, 0.0059], device='cuda:0')
Epoch: 37 | Batch_idx: 0 |  Loss: (0.7102) |  Loss2: (0.0000) | Acc: (76.00%) (98/128)
Epoch: 37 | Batch_idx: 10 |  Loss: (0.5427) |  Loss2: (0.0000) | Acc: (82.00%) (1160/1408)
Epoch: 37 | Batch_idx: 20 |  Loss: (0.5325) |  Loss2: (0.0000) | Acc: (82.00%) (2210/2688)
Epoch: 37 | Batch_idx: 30 |  Loss: (0.5541) |  Loss2: (0.0000) | Acc: (81.00%) (3217/3968)
Epoch: 37 | Batch_idx: 40 |  Loss: (0.5545) |  Loss2: (0.0000) | Acc: (80.00%) (4245/5248)
Epoch: 37 | Batch_idx: 50 |  Loss: (0.5566) |  Loss2: (0.0000) | Acc: (80.00%) (5276/6528)
Epoch: 37 | Batch_idx: 60 |  Loss: (0.5633) |  Loss2: (0.0000) | Acc: (80.00%) (6298/7808)
Epoch: 37 | Batch_idx: 70 |  Loss: (0.5669) |  Loss2: (0.0000) | Acc: (80.00%) (7322/9088)
Epoch: 37 | Batch_idx: 80 |  Loss: (0.5623) |  Loss2: (0.0000) | Acc: (80.00%) (8359/10368)
Epoch: 37 | Batch_idx: 90 |  Loss: (0.5528) |  Loss2: (0.0000) | Acc: (80.00%) (9432/11648)
Epoch: 37 | Batch_idx: 100 |  Loss: (0.5543) |  Loss2: (0.0000) | Acc: (80.00%) (10467/12928)
Epoch: 37 | Batch_idx: 110 |  Loss: (0.5487) |  Loss2: (0.0000) | Acc: (81.00%) (11526/14208)
Epoch: 37 | Batch_idx: 120 |  Loss: (0.5465) |  Loss2: (0.0000) | Acc: (81.00%) (12574/15488)
Epoch: 37 | Batch_idx: 130 |  Loss: (0.5479) |  Loss2: (0.0000) | Acc: (81.00%) (13594/16768)
Epoch: 37 | Batch_idx: 140 |  Loss: (0.5439) |  Loss2: (0.0000) | Acc: (81.00%) (14651/18048)
Epoch: 37 | Batch_idx: 150 |  Loss: (0.5422) |  Loss2: (0.0000) | Acc: (81.00%) (15708/19328)
Epoch: 37 | Batch_idx: 160 |  Loss: (0.5436) |  Loss2: (0.0000) | Acc: (81.00%) (16733/20608)
Epoch: 37 | Batch_idx: 170 |  Loss: (0.5453) |  Loss2: (0.0000) | Acc: (81.00%) (17759/21888)
Epoch: 37 | Batch_idx: 180 |  Loss: (0.5435) |  Loss2: (0.0000) | Acc: (81.00%) (18820/23168)
Epoch: 37 | Batch_idx: 190 |  Loss: (0.5410) |  Loss2: (0.0000) | Acc: (81.00%) (19888/24448)
Epoch: 37 | Batch_idx: 200 |  Loss: (0.5399) |  Loss2: (0.0000) | Acc: (81.00%) (20951/25728)
Epoch: 37 | Batch_idx: 210 |  Loss: (0.5405) |  Loss2: (0.0000) | Acc: (81.00%) (21996/27008)
Epoch: 37 | Batch_idx: 220 |  Loss: (0.5389) |  Loss2: (0.0000) | Acc: (81.00%) (23057/28288)
Epoch: 37 | Batch_idx: 230 |  Loss: (0.5362) |  Loss2: (0.0000) | Acc: (81.00%) (24124/29568)
Epoch: 37 | Batch_idx: 240 |  Loss: (0.5359) |  Loss2: (0.0000) | Acc: (81.00%) (25171/30848)
Epoch: 37 | Batch_idx: 250 |  Loss: (0.5362) |  Loss2: (0.0000) | Acc: (81.00%) (26215/32128)
Epoch: 37 | Batch_idx: 260 |  Loss: (0.5366) |  Loss2: (0.0000) | Acc: (81.00%) (27255/33408)
Epoch: 37 | Batch_idx: 270 |  Loss: (0.5377) |  Loss2: (0.0000) | Acc: (81.00%) (28266/34688)
Epoch: 37 | Batch_idx: 280 |  Loss: (0.5375) |  Loss2: (0.0000) | Acc: (81.00%) (29319/35968)
Epoch: 37 | Batch_idx: 290 |  Loss: (0.5375) |  Loss2: (0.0000) | Acc: (81.00%) (30381/37248)
Epoch: 37 | Batch_idx: 300 |  Loss: (0.5365) |  Loss2: (0.0000) | Acc: (81.00%) (31433/38528)
Epoch: 37 | Batch_idx: 310 |  Loss: (0.5358) |  Loss2: (0.0000) | Acc: (81.00%) (32479/39808)
Epoch: 37 | Batch_idx: 320 |  Loss: (0.5352) |  Loss2: (0.0000) | Acc: (81.00%) (33540/41088)
Epoch: 37 | Batch_idx: 330 |  Loss: (0.5349) |  Loss2: (0.0000) | Acc: (81.00%) (34610/42368)
Epoch: 37 | Batch_idx: 340 |  Loss: (0.5358) |  Loss2: (0.0000) | Acc: (81.00%) (35637/43648)
Epoch: 37 | Batch_idx: 350 |  Loss: (0.5354) |  Loss2: (0.0000) | Acc: (81.00%) (36679/44928)
Epoch: 37 | Batch_idx: 360 |  Loss: (0.5352) |  Loss2: (0.0000) | Acc: (81.00%) (37723/46208)
Epoch: 37 | Batch_idx: 370 |  Loss: (0.5345) |  Loss2: (0.0000) | Acc: (81.00%) (38767/47488)
Epoch: 37 | Batch_idx: 380 |  Loss: (0.5339) |  Loss2: (0.0000) | Acc: (81.00%) (39824/48768)
Epoch: 37 | Batch_idx: 390 |  Loss: (0.5343) |  Loss2: (0.0000) | Acc: (81.00%) (40823/50000)
# TEST : Loss: (0.5830) | Acc: (79.00%) (7984/10000)
percent tensor([0.4840, 0.5160], device='cuda:0')
percent tensor([0.4684, 0.5316], device='cuda:0')
percent tensor([0.5115, 0.4885], device='cuda:0')
percent tensor([0.5060, 0.4940], device='cuda:0')
percent tensor([0.5260, 0.4740], device='cuda:0')
percent tensor([0.6388, 0.3612], device='cuda:0')
percent tensor([0.6223, 0.3777], device='cuda:0')
percent tensor([0.9950, 0.0050], device='cuda:0')
Epoch: 38 | Batch_idx: 0 |  Loss: (0.5311) |  Loss2: (0.0000) | Acc: (82.00%) (105/128)
Epoch: 38 | Batch_idx: 10 |  Loss: (0.5377) |  Loss2: (0.0000) | Acc: (81.00%) (1150/1408)
Epoch: 38 | Batch_idx: 20 |  Loss: (0.5321) |  Loss2: (0.0000) | Acc: (81.00%) (2199/2688)
Epoch: 38 | Batch_idx: 30 |  Loss: (0.5374) |  Loss2: (0.0000) | Acc: (82.00%) (3256/3968)
Epoch: 38 | Batch_idx: 40 |  Loss: (0.5396) |  Loss2: (0.0000) | Acc: (81.00%) (4303/5248)
Epoch: 38 | Batch_idx: 50 |  Loss: (0.5383) |  Loss2: (0.0000) | Acc: (82.00%) (5367/6528)
Epoch: 38 | Batch_idx: 60 |  Loss: (0.5372) |  Loss2: (0.0000) | Acc: (82.00%) (6411/7808)
Epoch: 38 | Batch_idx: 70 |  Loss: (0.5355) |  Loss2: (0.0000) | Acc: (82.00%) (7473/9088)
Epoch: 38 | Batch_idx: 80 |  Loss: (0.5347) |  Loss2: (0.0000) | Acc: (82.00%) (8523/10368)
Epoch: 38 | Batch_idx: 90 |  Loss: (0.5341) |  Loss2: (0.0000) | Acc: (82.00%) (9579/11648)
Epoch: 38 | Batch_idx: 100 |  Loss: (0.5320) |  Loss2: (0.0000) | Acc: (82.00%) (10629/12928)
Epoch: 38 | Batch_idx: 110 |  Loss: (0.5329) |  Loss2: (0.0000) | Acc: (82.00%) (11684/14208)
Epoch: 38 | Batch_idx: 120 |  Loss: (0.5320) |  Loss2: (0.0000) | Acc: (82.00%) (12727/15488)
Epoch: 38 | Batch_idx: 130 |  Loss: (0.5321) |  Loss2: (0.0000) | Acc: (82.00%) (13765/16768)
Epoch: 38 | Batch_idx: 140 |  Loss: (0.5287) |  Loss2: (0.0000) | Acc: (82.00%) (14820/18048)
Epoch: 38 | Batch_idx: 150 |  Loss: (0.5281) |  Loss2: (0.0000) | Acc: (82.00%) (15857/19328)
Epoch: 38 | Batch_idx: 160 |  Loss: (0.5281) |  Loss2: (0.0000) | Acc: (82.00%) (16902/20608)
Epoch: 38 | Batch_idx: 170 |  Loss: (0.5268) |  Loss2: (0.0000) | Acc: (81.00%) (17946/21888)
Epoch: 38 | Batch_idx: 180 |  Loss: (0.5259) |  Loss2: (0.0000) | Acc: (81.00%) (18983/23168)
Epoch: 38 | Batch_idx: 190 |  Loss: (0.5243) |  Loss2: (0.0000) | Acc: (81.00%) (20046/24448)
Epoch: 38 | Batch_idx: 200 |  Loss: (0.5268) |  Loss2: (0.0000) | Acc: (81.00%) (21075/25728)
Epoch: 38 | Batch_idx: 210 |  Loss: (0.5259) |  Loss2: (0.0000) | Acc: (81.00%) (22126/27008)
Epoch: 38 | Batch_idx: 220 |  Loss: (0.5249) |  Loss2: (0.0000) | Acc: (81.00%) (23190/28288)
Epoch: 38 | Batch_idx: 230 |  Loss: (0.5220) |  Loss2: (0.0000) | Acc: (82.00%) (24279/29568)
Epoch: 38 | Batch_idx: 240 |  Loss: (0.5217) |  Loss2: (0.0000) | Acc: (82.00%) (25328/30848)
Epoch: 38 | Batch_idx: 250 |  Loss: (0.5218) |  Loss2: (0.0000) | Acc: (82.00%) (26377/32128)
Epoch: 38 | Batch_idx: 260 |  Loss: (0.5236) |  Loss2: (0.0000) | Acc: (82.00%) (27407/33408)
Epoch: 38 | Batch_idx: 270 |  Loss: (0.5226) |  Loss2: (0.0000) | Acc: (82.00%) (28462/34688)
Epoch: 38 | Batch_idx: 280 |  Loss: (0.5226) |  Loss2: (0.0000) | Acc: (82.00%) (29511/35968)
Epoch: 38 | Batch_idx: 290 |  Loss: (0.5216) |  Loss2: (0.0000) | Acc: (82.00%) (30582/37248)
Epoch: 38 | Batch_idx: 300 |  Loss: (0.5215) |  Loss2: (0.0000) | Acc: (82.00%) (31634/38528)
Epoch: 38 | Batch_idx: 310 |  Loss: (0.5225) |  Loss2: (0.0000) | Acc: (82.00%) (32659/39808)
Epoch: 38 | Batch_idx: 320 |  Loss: (0.5223) |  Loss2: (0.0000) | Acc: (82.00%) (33724/41088)
Epoch: 38 | Batch_idx: 330 |  Loss: (0.5239) |  Loss2: (0.0000) | Acc: (82.00%) (34748/42368)
Epoch: 38 | Batch_idx: 340 |  Loss: (0.5232) |  Loss2: (0.0000) | Acc: (82.00%) (35819/43648)
Epoch: 38 | Batch_idx: 350 |  Loss: (0.5231) |  Loss2: (0.0000) | Acc: (82.00%) (36874/44928)
Epoch: 38 | Batch_idx: 360 |  Loss: (0.5234) |  Loss2: (0.0000) | Acc: (82.00%) (37915/46208)
Epoch: 38 | Batch_idx: 370 |  Loss: (0.5229) |  Loss2: (0.0000) | Acc: (82.00%) (38977/47488)
Epoch: 38 | Batch_idx: 380 |  Loss: (0.5236) |  Loss2: (0.0000) | Acc: (82.00%) (40019/48768)
Epoch: 38 | Batch_idx: 390 |  Loss: (0.5235) |  Loss2: (0.0000) | Acc: (82.00%) (41032/50000)
# TEST : Loss: (0.5729) | Acc: (80.00%) (8039/10000)
percent tensor([0.4846, 0.5154], device='cuda:0')
percent tensor([0.4687, 0.5313], device='cuda:0')
percent tensor([0.5141, 0.4859], device='cuda:0')
percent tensor([0.5072, 0.4928], device='cuda:0')
percent tensor([0.5276, 0.4724], device='cuda:0')
percent tensor([0.6507, 0.3493], device='cuda:0')
percent tensor([0.6365, 0.3635], device='cuda:0')
percent tensor([0.9957, 0.0043], device='cuda:0')
Epoch: 39 | Batch_idx: 0 |  Loss: (0.5170) |  Loss2: (0.0000) | Acc: (82.00%) (105/128)
Epoch: 39 | Batch_idx: 10 |  Loss: (0.5133) |  Loss2: (0.0000) | Acc: (83.00%) (1170/1408)
Epoch: 39 | Batch_idx: 20 |  Loss: (0.5261) |  Loss2: (0.0000) | Acc: (81.00%) (2194/2688)
Epoch: 39 | Batch_idx: 30 |  Loss: (0.5221) |  Loss2: (0.0000) | Acc: (81.00%) (3246/3968)
Epoch: 39 | Batch_idx: 40 |  Loss: (0.5209) |  Loss2: (0.0000) | Acc: (81.00%) (4278/5248)
Epoch: 39 | Batch_idx: 50 |  Loss: (0.5279) |  Loss2: (0.0000) | Acc: (81.00%) (5325/6528)
Epoch: 39 | Batch_idx: 60 |  Loss: (0.5236) |  Loss2: (0.0000) | Acc: (81.00%) (6373/7808)
Epoch: 39 | Batch_idx: 70 |  Loss: (0.5261) |  Loss2: (0.0000) | Acc: (81.00%) (7422/9088)
Epoch: 39 | Batch_idx: 80 |  Loss: (0.5192) |  Loss2: (0.0000) | Acc: (81.00%) (8497/10368)
Epoch: 39 | Batch_idx: 90 |  Loss: (0.5193) |  Loss2: (0.0000) | Acc: (81.00%) (9539/11648)
Epoch: 39 | Batch_idx: 100 |  Loss: (0.5190) |  Loss2: (0.0000) | Acc: (81.00%) (10587/12928)
Epoch: 39 | Batch_idx: 110 |  Loss: (0.5217) |  Loss2: (0.0000) | Acc: (81.00%) (11624/14208)
Epoch: 39 | Batch_idx: 120 |  Loss: (0.5221) |  Loss2: (0.0000) | Acc: (81.00%) (12670/15488)
Epoch: 39 | Batch_idx: 130 |  Loss: (0.5221) |  Loss2: (0.0000) | Acc: (81.00%) (13725/16768)
Epoch: 39 | Batch_idx: 140 |  Loss: (0.5221) |  Loss2: (0.0000) | Acc: (81.00%) (14784/18048)
Epoch: 39 | Batch_idx: 150 |  Loss: (0.5225) |  Loss2: (0.0000) | Acc: (81.00%) (15830/19328)
Epoch: 39 | Batch_idx: 160 |  Loss: (0.5226) |  Loss2: (0.0000) | Acc: (81.00%) (16886/20608)
Epoch: 39 | Batch_idx: 170 |  Loss: (0.5244) |  Loss2: (0.0000) | Acc: (81.00%) (17925/21888)
Epoch: 39 | Batch_idx: 180 |  Loss: (0.5248) |  Loss2: (0.0000) | Acc: (81.00%) (18960/23168)
Epoch: 39 | Batch_idx: 190 |  Loss: (0.5249) |  Loss2: (0.0000) | Acc: (81.00%) (20002/24448)
Epoch: 39 | Batch_idx: 200 |  Loss: (0.5249) |  Loss2: (0.0000) | Acc: (81.00%) (21050/25728)
Epoch: 39 | Batch_idx: 210 |  Loss: (0.5253) |  Loss2: (0.0000) | Acc: (81.00%) (22086/27008)
Epoch: 39 | Batch_idx: 220 |  Loss: (0.5228) |  Loss2: (0.0000) | Acc: (81.00%) (23164/28288)
Epoch: 39 | Batch_idx: 230 |  Loss: (0.5235) |  Loss2: (0.0000) | Acc: (81.00%) (24209/29568)
Epoch: 39 | Batch_idx: 240 |  Loss: (0.5224) |  Loss2: (0.0000) | Acc: (81.00%) (25271/30848)
Epoch: 39 | Batch_idx: 250 |  Loss: (0.5225) |  Loss2: (0.0000) | Acc: (81.00%) (26302/32128)
Epoch: 39 | Batch_idx: 260 |  Loss: (0.5224) |  Loss2: (0.0000) | Acc: (81.00%) (27353/33408)
Epoch: 39 | Batch_idx: 270 |  Loss: (0.5215) |  Loss2: (0.0000) | Acc: (81.00%) (28408/34688)
Epoch: 39 | Batch_idx: 280 |  Loss: (0.5217) |  Loss2: (0.0000) | Acc: (81.00%) (29459/35968)
Epoch: 39 | Batch_idx: 290 |  Loss: (0.5222) |  Loss2: (0.0000) | Acc: (81.00%) (30511/37248)
Epoch: 39 | Batch_idx: 300 |  Loss: (0.5209) |  Loss2: (0.0000) | Acc: (81.00%) (31581/38528)
Epoch: 39 | Batch_idx: 310 |  Loss: (0.5201) |  Loss2: (0.0000) | Acc: (82.00%) (32647/39808)
Epoch: 39 | Batch_idx: 320 |  Loss: (0.5189) |  Loss2: (0.0000) | Acc: (82.00%) (33721/41088)
Epoch: 39 | Batch_idx: 330 |  Loss: (0.5191) |  Loss2: (0.0000) | Acc: (82.00%) (34788/42368)
Epoch: 39 | Batch_idx: 340 |  Loss: (0.5187) |  Loss2: (0.0000) | Acc: (82.00%) (35839/43648)
Epoch: 39 | Batch_idx: 350 |  Loss: (0.5189) |  Loss2: (0.0000) | Acc: (82.00%) (36902/44928)
Epoch: 39 | Batch_idx: 360 |  Loss: (0.5188) |  Loss2: (0.0000) | Acc: (82.00%) (37960/46208)
Epoch: 39 | Batch_idx: 370 |  Loss: (0.5199) |  Loss2: (0.0000) | Acc: (82.00%) (38990/47488)
Epoch: 39 | Batch_idx: 380 |  Loss: (0.5197) |  Loss2: (0.0000) | Acc: (82.00%) (40051/48768)
Epoch: 39 | Batch_idx: 390 |  Loss: (0.5193) |  Loss2: (0.0000) | Acc: (82.00%) (41076/50000)
# TEST : Loss: (0.5658) | Acc: (80.00%) (8074/10000)
percent tensor([0.4846, 0.5154], device='cuda:0')
percent tensor([0.4696, 0.5304], device='cuda:0')
percent tensor([0.5154, 0.4846], device='cuda:0')
percent tensor([0.5082, 0.4918], device='cuda:0')
percent tensor([0.5302, 0.4698], device='cuda:0')
percent tensor([0.6604, 0.3396], device='cuda:0')
percent tensor([0.6436, 0.3564], device='cuda:0')
percent tensor([0.9962, 0.0038], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 40 | Batch_idx: 0 |  Loss: (0.4825) |  Loss2: (0.0000) | Acc: (82.00%) (105/128)
Epoch: 40 | Batch_idx: 10 |  Loss: (0.5137) |  Loss2: (0.0000) | Acc: (82.00%) (1163/1408)
Epoch: 40 | Batch_idx: 20 |  Loss: (0.5032) |  Loss2: (0.0000) | Acc: (82.00%) (2227/2688)
Epoch: 40 | Batch_idx: 30 |  Loss: (0.5099) |  Loss2: (0.0000) | Acc: (82.00%) (3284/3968)
Epoch: 40 | Batch_idx: 40 |  Loss: (0.5126) |  Loss2: (0.0000) | Acc: (82.00%) (4321/5248)
Epoch: 40 | Batch_idx: 50 |  Loss: (0.5074) |  Loss2: (0.0000) | Acc: (82.00%) (5394/6528)
Epoch: 40 | Batch_idx: 60 |  Loss: (0.5049) |  Loss2: (0.0000) | Acc: (82.00%) (6467/7808)
Epoch: 40 | Batch_idx: 70 |  Loss: (0.5083) |  Loss2: (0.0000) | Acc: (82.00%) (7512/9088)
Epoch: 40 | Batch_idx: 80 |  Loss: (0.5085) |  Loss2: (0.0000) | Acc: (82.00%) (8560/10368)
Epoch: 40 | Batch_idx: 90 |  Loss: (0.5108) |  Loss2: (0.0000) | Acc: (82.00%) (9616/11648)
Epoch: 40 | Batch_idx: 100 |  Loss: (0.5041) |  Loss2: (0.0000) | Acc: (82.00%) (10707/12928)
Epoch: 40 | Batch_idx: 110 |  Loss: (0.5048) |  Loss2: (0.0000) | Acc: (82.00%) (11759/14208)
Epoch: 40 | Batch_idx: 120 |  Loss: (0.5036) |  Loss2: (0.0000) | Acc: (82.00%) (12826/15488)
Epoch: 40 | Batch_idx: 130 |  Loss: (0.5033) |  Loss2: (0.0000) | Acc: (82.00%) (13888/16768)
Epoch: 40 | Batch_idx: 140 |  Loss: (0.5045) |  Loss2: (0.0000) | Acc: (82.00%) (14943/18048)
Epoch: 40 | Batch_idx: 150 |  Loss: (0.5081) |  Loss2: (0.0000) | Acc: (82.00%) (15988/19328)
Epoch: 40 | Batch_idx: 160 |  Loss: (0.5119) |  Loss2: (0.0000) | Acc: (82.00%) (17030/20608)
Epoch: 40 | Batch_idx: 170 |  Loss: (0.5123) |  Loss2: (0.0000) | Acc: (82.00%) (18082/21888)
Epoch: 40 | Batch_idx: 180 |  Loss: (0.5091) |  Loss2: (0.0000) | Acc: (82.00%) (19171/23168)
Epoch: 40 | Batch_idx: 190 |  Loss: (0.5089) |  Loss2: (0.0000) | Acc: (82.00%) (20228/24448)
Epoch: 40 | Batch_idx: 200 |  Loss: (0.5056) |  Loss2: (0.0000) | Acc: (82.00%) (21320/25728)
Epoch: 40 | Batch_idx: 210 |  Loss: (0.5054) |  Loss2: (0.0000) | Acc: (82.00%) (22368/27008)
Epoch: 40 | Batch_idx: 220 |  Loss: (0.5051) |  Loss2: (0.0000) | Acc: (82.00%) (23427/28288)
Epoch: 40 | Batch_idx: 230 |  Loss: (0.5071) |  Loss2: (0.0000) | Acc: (82.00%) (24454/29568)
Epoch: 40 | Batch_idx: 240 |  Loss: (0.5078) |  Loss2: (0.0000) | Acc: (82.00%) (25501/30848)
Epoch: 40 | Batch_idx: 250 |  Loss: (0.5105) |  Loss2: (0.0000) | Acc: (82.00%) (26521/32128)
Epoch: 40 | Batch_idx: 260 |  Loss: (0.5103) |  Loss2: (0.0000) | Acc: (82.00%) (27576/33408)
Epoch: 40 | Batch_idx: 270 |  Loss: (0.5099) |  Loss2: (0.0000) | Acc: (82.00%) (28634/34688)
Epoch: 40 | Batch_idx: 280 |  Loss: (0.5105) |  Loss2: (0.0000) | Acc: (82.00%) (29683/35968)
Epoch: 40 | Batch_idx: 290 |  Loss: (0.5101) |  Loss2: (0.0000) | Acc: (82.00%) (30728/37248)
Epoch: 40 | Batch_idx: 300 |  Loss: (0.5091) |  Loss2: (0.0000) | Acc: (82.00%) (31785/38528)
Epoch: 40 | Batch_idx: 310 |  Loss: (0.5093) |  Loss2: (0.0000) | Acc: (82.00%) (32832/39808)
Epoch: 40 | Batch_idx: 320 |  Loss: (0.5083) |  Loss2: (0.0000) | Acc: (82.00%) (33903/41088)
Epoch: 40 | Batch_idx: 330 |  Loss: (0.5082) |  Loss2: (0.0000) | Acc: (82.00%) (34963/42368)
Epoch: 40 | Batch_idx: 340 |  Loss: (0.5081) |  Loss2: (0.0000) | Acc: (82.00%) (36023/43648)
Epoch: 40 | Batch_idx: 350 |  Loss: (0.5087) |  Loss2: (0.0000) | Acc: (82.00%) (37080/44928)
Epoch: 40 | Batch_idx: 360 |  Loss: (0.5090) |  Loss2: (0.0000) | Acc: (82.00%) (38125/46208)
Epoch: 40 | Batch_idx: 370 |  Loss: (0.5093) |  Loss2: (0.0000) | Acc: (82.00%) (39178/47488)
Epoch: 40 | Batch_idx: 380 |  Loss: (0.5087) |  Loss2: (0.0000) | Acc: (82.00%) (40248/48768)
Epoch: 40 | Batch_idx: 390 |  Loss: (0.5091) |  Loss2: (0.0000) | Acc: (82.00%) (41271/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_040.pth.tar'
# TEST : Loss: (0.5971) | Acc: (79.00%) (7988/10000)
percent tensor([0.4846, 0.5154], device='cuda:0')
percent tensor([0.4698, 0.5302], device='cuda:0')
percent tensor([0.5156, 0.4844], device='cuda:0')
percent tensor([0.5081, 0.4919], device='cuda:0')
percent tensor([0.5307, 0.4693], device='cuda:0')
percent tensor([0.6622, 0.3378], device='cuda:0')
percent tensor([0.6523, 0.3477], device='cuda:0')
percent tensor([0.9967, 0.0033], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(173.3459, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(795.5269, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(786.8863, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1523.0380, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(513.1813, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2184.1340, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4296.7358, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1424.2054, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6098.9497, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12097.9209, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4022.0383, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(17055.4238, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 41 | Batch_idx: 0 |  Loss: (0.5547) |  Loss2: (0.0000) | Acc: (82.00%) (106/128)
Epoch: 41 | Batch_idx: 10 |  Loss: (0.4996) |  Loss2: (0.0000) | Acc: (82.00%) (1167/1408)
Epoch: 41 | Batch_idx: 20 |  Loss: (0.4813) |  Loss2: (0.0000) | Acc: (83.00%) (2244/2688)
Epoch: 41 | Batch_idx: 30 |  Loss: (0.4894) |  Loss2: (0.0000) | Acc: (83.00%) (3303/3968)
Epoch: 41 | Batch_idx: 40 |  Loss: (0.4816) |  Loss2: (0.0000) | Acc: (83.00%) (4381/5248)
Epoch: 41 | Batch_idx: 50 |  Loss: (0.4676) |  Loss2: (0.0000) | Acc: (83.00%) (5478/6528)
Epoch: 41 | Batch_idx: 60 |  Loss: (0.4756) |  Loss2: (0.0000) | Acc: (83.00%) (6524/7808)
Epoch: 41 | Batch_idx: 70 |  Loss: (0.4794) |  Loss2: (0.0000) | Acc: (83.00%) (7576/9088)
Epoch: 41 | Batch_idx: 80 |  Loss: (0.4809) |  Loss2: (0.0000) | Acc: (83.00%) (8646/10368)
Epoch: 41 | Batch_idx: 90 |  Loss: (0.4775) |  Loss2: (0.0000) | Acc: (83.00%) (9743/11648)
Epoch: 41 | Batch_idx: 100 |  Loss: (0.4742) |  Loss2: (0.0000) | Acc: (83.00%) (10832/12928)
Epoch: 41 | Batch_idx: 110 |  Loss: (0.4769) |  Loss2: (0.0000) | Acc: (83.00%) (11893/14208)
Epoch: 41 | Batch_idx: 120 |  Loss: (0.4792) |  Loss2: (0.0000) | Acc: (83.00%) (12947/15488)
Epoch: 41 | Batch_idx: 130 |  Loss: (0.4816) |  Loss2: (0.0000) | Acc: (83.00%) (14006/16768)
Epoch: 41 | Batch_idx: 140 |  Loss: (0.4836) |  Loss2: (0.0000) | Acc: (83.00%) (15044/18048)
Epoch: 41 | Batch_idx: 150 |  Loss: (0.4816) |  Loss2: (0.0000) | Acc: (83.00%) (16129/19328)
Epoch: 41 | Batch_idx: 160 |  Loss: (0.4818) |  Loss2: (0.0000) | Acc: (83.00%) (17196/20608)
Epoch: 41 | Batch_idx: 170 |  Loss: (0.4822) |  Loss2: (0.0000) | Acc: (83.00%) (18257/21888)
Epoch: 41 | Batch_idx: 180 |  Loss: (0.4787) |  Loss2: (0.0000) | Acc: (83.00%) (19345/23168)
Epoch: 41 | Batch_idx: 190 |  Loss: (0.4791) |  Loss2: (0.0000) | Acc: (83.00%) (20404/24448)
Epoch: 41 | Batch_idx: 200 |  Loss: (0.4799) |  Loss2: (0.0000) | Acc: (83.00%) (21462/25728)
Epoch: 41 | Batch_idx: 210 |  Loss: (0.4810) |  Loss2: (0.0000) | Acc: (83.00%) (22497/27008)
Epoch: 41 | Batch_idx: 220 |  Loss: (0.4810) |  Loss2: (0.0000) | Acc: (83.00%) (23557/28288)
Epoch: 41 | Batch_idx: 230 |  Loss: (0.4809) |  Loss2: (0.0000) | Acc: (83.00%) (24620/29568)
Epoch: 41 | Batch_idx: 240 |  Loss: (0.4821) |  Loss2: (0.0000) | Acc: (83.00%) (25675/30848)
Epoch: 41 | Batch_idx: 250 |  Loss: (0.4830) |  Loss2: (0.0000) | Acc: (83.00%) (26742/32128)
Epoch: 41 | Batch_idx: 260 |  Loss: (0.4828) |  Loss2: (0.0000) | Acc: (83.00%) (27811/33408)
Epoch: 41 | Batch_idx: 270 |  Loss: (0.4851) |  Loss2: (0.0000) | Acc: (83.00%) (28849/34688)
Epoch: 41 | Batch_idx: 280 |  Loss: (0.4852) |  Loss2: (0.0000) | Acc: (83.00%) (29901/35968)
Epoch: 41 | Batch_idx: 290 |  Loss: (0.4852) |  Loss2: (0.0000) | Acc: (83.00%) (30973/37248)
Epoch: 41 | Batch_idx: 300 |  Loss: (0.4852) |  Loss2: (0.0000) | Acc: (83.00%) (32043/38528)
Epoch: 41 | Batch_idx: 310 |  Loss: (0.4848) |  Loss2: (0.0000) | Acc: (83.00%) (33124/39808)
Epoch: 41 | Batch_idx: 320 |  Loss: (0.4846) |  Loss2: (0.0000) | Acc: (83.00%) (34203/41088)
Epoch: 41 | Batch_idx: 330 |  Loss: (0.4852) |  Loss2: (0.0000) | Acc: (83.00%) (35259/42368)
Epoch: 41 | Batch_idx: 340 |  Loss: (0.4867) |  Loss2: (0.0000) | Acc: (83.00%) (36302/43648)
Epoch: 41 | Batch_idx: 350 |  Loss: (0.4858) |  Loss2: (0.0000) | Acc: (83.00%) (37379/44928)
Epoch: 41 | Batch_idx: 360 |  Loss: (0.4859) |  Loss2: (0.0000) | Acc: (83.00%) (38451/46208)
Epoch: 41 | Batch_idx: 370 |  Loss: (0.4857) |  Loss2: (0.0000) | Acc: (83.00%) (39521/47488)
Epoch: 41 | Batch_idx: 380 |  Loss: (0.4863) |  Loss2: (0.0000) | Acc: (83.00%) (40574/48768)
Epoch: 41 | Batch_idx: 390 |  Loss: (0.4867) |  Loss2: (0.0000) | Acc: (83.00%) (41598/50000)
# TEST : Loss: (0.5475) | Acc: (80.00%) (8091/10000)
percent tensor([0.4847, 0.5153], device='cuda:0')
percent tensor([0.4705, 0.5295], device='cuda:0')
percent tensor([0.5159, 0.4841], device='cuda:0')
percent tensor([0.5083, 0.4917], device='cuda:0')
percent tensor([0.5304, 0.4696], device='cuda:0')
percent tensor([0.6606, 0.3394], device='cuda:0')
percent tensor([0.6467, 0.3533], device='cuda:0')
percent tensor([0.9966, 0.0034], device='cuda:0')
Epoch: 42 | Batch_idx: 0 |  Loss: (0.5814) |  Loss2: (0.0000) | Acc: (82.00%) (106/128)
Epoch: 42 | Batch_idx: 10 |  Loss: (0.4621) |  Loss2: (0.0000) | Acc: (83.00%) (1176/1408)
Epoch: 42 | Batch_idx: 20 |  Loss: (0.4647) |  Loss2: (0.0000) | Acc: (83.00%) (2237/2688)
Epoch: 42 | Batch_idx: 30 |  Loss: (0.4661) |  Loss2: (0.0000) | Acc: (83.00%) (3306/3968)
Epoch: 42 | Batch_idx: 40 |  Loss: (0.4683) |  Loss2: (0.0000) | Acc: (83.00%) (4376/5248)
Epoch: 42 | Batch_idx: 50 |  Loss: (0.4626) |  Loss2: (0.0000) | Acc: (83.00%) (5467/6528)
Epoch: 42 | Batch_idx: 60 |  Loss: (0.4584) |  Loss2: (0.0000) | Acc: (83.00%) (6554/7808)
Epoch: 42 | Batch_idx: 70 |  Loss: (0.4609) |  Loss2: (0.0000) | Acc: (83.00%) (7611/9088)
Epoch: 42 | Batch_idx: 80 |  Loss: (0.4651) |  Loss2: (0.0000) | Acc: (83.00%) (8670/10368)
Epoch: 42 | Batch_idx: 90 |  Loss: (0.4628) |  Loss2: (0.0000) | Acc: (83.00%) (9752/11648)
Epoch: 42 | Batch_idx: 100 |  Loss: (0.4654) |  Loss2: (0.0000) | Acc: (83.00%) (10816/12928)
Epoch: 42 | Batch_idx: 110 |  Loss: (0.4657) |  Loss2: (0.0000) | Acc: (83.00%) (11884/14208)
Epoch: 42 | Batch_idx: 120 |  Loss: (0.4660) |  Loss2: (0.0000) | Acc: (83.00%) (12963/15488)
Epoch: 42 | Batch_idx: 130 |  Loss: (0.4670) |  Loss2: (0.0000) | Acc: (83.00%) (14038/16768)
Epoch: 42 | Batch_idx: 140 |  Loss: (0.4653) |  Loss2: (0.0000) | Acc: (83.00%) (15117/18048)
Epoch: 42 | Batch_idx: 150 |  Loss: (0.4622) |  Loss2: (0.0000) | Acc: (83.00%) (16201/19328)
Epoch: 42 | Batch_idx: 160 |  Loss: (0.4632) |  Loss2: (0.0000) | Acc: (83.00%) (17271/20608)
Epoch: 42 | Batch_idx: 170 |  Loss: (0.4637) |  Loss2: (0.0000) | Acc: (83.00%) (18336/21888)
Epoch: 42 | Batch_idx: 180 |  Loss: (0.4667) |  Loss2: (0.0000) | Acc: (83.00%) (19401/23168)
Epoch: 42 | Batch_idx: 190 |  Loss: (0.4668) |  Loss2: (0.0000) | Acc: (83.00%) (20472/24448)
Epoch: 42 | Batch_idx: 200 |  Loss: (0.4667) |  Loss2: (0.0000) | Acc: (83.00%) (21549/25728)
Epoch: 42 | Batch_idx: 210 |  Loss: (0.4679) |  Loss2: (0.0000) | Acc: (83.00%) (22613/27008)
Epoch: 42 | Batch_idx: 220 |  Loss: (0.4690) |  Loss2: (0.0000) | Acc: (83.00%) (23675/28288)
Epoch: 42 | Batch_idx: 230 |  Loss: (0.4700) |  Loss2: (0.0000) | Acc: (83.00%) (24747/29568)
Epoch: 42 | Batch_idx: 240 |  Loss: (0.4712) |  Loss2: (0.0000) | Acc: (83.00%) (25812/30848)
Epoch: 42 | Batch_idx: 250 |  Loss: (0.4700) |  Loss2: (0.0000) | Acc: (83.00%) (26911/32128)
Epoch: 42 | Batch_idx: 260 |  Loss: (0.4710) |  Loss2: (0.0000) | Acc: (83.00%) (27966/33408)
Epoch: 42 | Batch_idx: 270 |  Loss: (0.4708) |  Loss2: (0.0000) | Acc: (83.00%) (29043/34688)
Epoch: 42 | Batch_idx: 280 |  Loss: (0.4710) |  Loss2: (0.0000) | Acc: (83.00%) (30132/35968)
Epoch: 42 | Batch_idx: 290 |  Loss: (0.4713) |  Loss2: (0.0000) | Acc: (83.00%) (31200/37248)
Epoch: 42 | Batch_idx: 300 |  Loss: (0.4720) |  Loss2: (0.0000) | Acc: (83.00%) (32244/38528)
Epoch: 42 | Batch_idx: 310 |  Loss: (0.4720) |  Loss2: (0.0000) | Acc: (83.00%) (33311/39808)
Epoch: 42 | Batch_idx: 320 |  Loss: (0.4724) |  Loss2: (0.0000) | Acc: (83.00%) (34389/41088)
Epoch: 42 | Batch_idx: 330 |  Loss: (0.4723) |  Loss2: (0.0000) | Acc: (83.00%) (35470/42368)
Epoch: 42 | Batch_idx: 340 |  Loss: (0.4715) |  Loss2: (0.0000) | Acc: (83.00%) (36542/43648)
Epoch: 42 | Batch_idx: 350 |  Loss: (0.4717) |  Loss2: (0.0000) | Acc: (83.00%) (37615/44928)
Epoch: 42 | Batch_idx: 360 |  Loss: (0.4710) |  Loss2: (0.0000) | Acc: (83.00%) (38699/46208)
Epoch: 42 | Batch_idx: 370 |  Loss: (0.4708) |  Loss2: (0.0000) | Acc: (83.00%) (39779/47488)
Epoch: 42 | Batch_idx: 380 |  Loss: (0.4708) |  Loss2: (0.0000) | Acc: (83.00%) (40857/48768)
Epoch: 42 | Batch_idx: 390 |  Loss: (0.4702) |  Loss2: (0.0000) | Acc: (83.00%) (41887/50000)
# TEST : Loss: (0.5588) | Acc: (80.00%) (8083/10000)
percent tensor([0.4848, 0.5152], device='cuda:0')
percent tensor([0.4704, 0.5296], device='cuda:0')
percent tensor([0.5157, 0.4843], device='cuda:0')
percent tensor([0.5081, 0.4919], device='cuda:0')
percent tensor([0.5304, 0.4696], device='cuda:0')
percent tensor([0.6574, 0.3426], device='cuda:0')
percent tensor([0.6478, 0.3522], device='cuda:0')
percent tensor([0.9966, 0.0034], device='cuda:0')
Epoch: 43 | Batch_idx: 0 |  Loss: (0.4289) |  Loss2: (0.0000) | Acc: (83.00%) (107/128)
Epoch: 43 | Batch_idx: 10 |  Loss: (0.4543) |  Loss2: (0.0000) | Acc: (84.00%) (1187/1408)
Epoch: 43 | Batch_idx: 20 |  Loss: (0.4479) |  Loss2: (0.0000) | Acc: (84.00%) (2284/2688)
Epoch: 43 | Batch_idx: 30 |  Loss: (0.4441) |  Loss2: (0.0000) | Acc: (85.00%) (3376/3968)
Epoch: 43 | Batch_idx: 40 |  Loss: (0.4478) |  Loss2: (0.0000) | Acc: (84.00%) (4460/5248)
Epoch: 43 | Batch_idx: 50 |  Loss: (0.4492) |  Loss2: (0.0000) | Acc: (85.00%) (5553/6528)
Epoch: 43 | Batch_idx: 60 |  Loss: (0.4502) |  Loss2: (0.0000) | Acc: (84.00%) (6623/7808)
Epoch: 43 | Batch_idx: 70 |  Loss: (0.4472) |  Loss2: (0.0000) | Acc: (85.00%) (7725/9088)
Epoch: 43 | Batch_idx: 80 |  Loss: (0.4498) |  Loss2: (0.0000) | Acc: (84.00%) (8805/10368)
Epoch: 43 | Batch_idx: 90 |  Loss: (0.4491) |  Loss2: (0.0000) | Acc: (84.00%) (9882/11648)
Epoch: 43 | Batch_idx: 100 |  Loss: (0.4550) |  Loss2: (0.0000) | Acc: (84.00%) (10930/12928)
Epoch: 43 | Batch_idx: 110 |  Loss: (0.4539) |  Loss2: (0.0000) | Acc: (84.00%) (12003/14208)
Epoch: 43 | Batch_idx: 120 |  Loss: (0.4563) |  Loss2: (0.0000) | Acc: (84.00%) (13077/15488)
Epoch: 43 | Batch_idx: 130 |  Loss: (0.4578) |  Loss2: (0.0000) | Acc: (84.00%) (14147/16768)
Epoch: 43 | Batch_idx: 140 |  Loss: (0.4597) |  Loss2: (0.0000) | Acc: (84.00%) (15216/18048)
Epoch: 43 | Batch_idx: 150 |  Loss: (0.4586) |  Loss2: (0.0000) | Acc: (84.00%) (16301/19328)
Epoch: 43 | Batch_idx: 160 |  Loss: (0.4568) |  Loss2: (0.0000) | Acc: (84.00%) (17412/20608)
Epoch: 43 | Batch_idx: 170 |  Loss: (0.4546) |  Loss2: (0.0000) | Acc: (84.00%) (18506/21888)
Epoch: 43 | Batch_idx: 180 |  Loss: (0.4560) |  Loss2: (0.0000) | Acc: (84.00%) (19568/23168)
Epoch: 43 | Batch_idx: 190 |  Loss: (0.4553) |  Loss2: (0.0000) | Acc: (84.00%) (20653/24448)
Epoch: 43 | Batch_idx: 200 |  Loss: (0.4577) |  Loss2: (0.0000) | Acc: (84.00%) (21707/25728)
Epoch: 43 | Batch_idx: 210 |  Loss: (0.4591) |  Loss2: (0.0000) | Acc: (84.00%) (22762/27008)
Epoch: 43 | Batch_idx: 220 |  Loss: (0.4582) |  Loss2: (0.0000) | Acc: (84.00%) (23847/28288)
Epoch: 43 | Batch_idx: 230 |  Loss: (0.4565) |  Loss2: (0.0000) | Acc: (84.00%) (24938/29568)
Epoch: 43 | Batch_idx: 240 |  Loss: (0.4562) |  Loss2: (0.0000) | Acc: (84.00%) (26004/30848)
Epoch: 43 | Batch_idx: 250 |  Loss: (0.4569) |  Loss2: (0.0000) | Acc: (84.00%) (27069/32128)
Epoch: 43 | Batch_idx: 260 |  Loss: (0.4573) |  Loss2: (0.0000) | Acc: (84.00%) (28155/33408)
Epoch: 43 | Batch_idx: 270 |  Loss: (0.4562) |  Loss2: (0.0000) | Acc: (84.00%) (29255/34688)
Epoch: 43 | Batch_idx: 280 |  Loss: (0.4560) |  Loss2: (0.0000) | Acc: (84.00%) (30340/35968)
Epoch: 43 | Batch_idx: 290 |  Loss: (0.4565) |  Loss2: (0.0000) | Acc: (84.00%) (31414/37248)
Epoch: 43 | Batch_idx: 300 |  Loss: (0.4567) |  Loss2: (0.0000) | Acc: (84.00%) (32503/38528)
Epoch: 43 | Batch_idx: 310 |  Loss: (0.4571) |  Loss2: (0.0000) | Acc: (84.00%) (33579/39808)
Epoch: 43 | Batch_idx: 320 |  Loss: (0.4556) |  Loss2: (0.0000) | Acc: (84.00%) (34681/41088)
Epoch: 43 | Batch_idx: 330 |  Loss: (0.4557) |  Loss2: (0.0000) | Acc: (84.00%) (35753/42368)
Epoch: 43 | Batch_idx: 340 |  Loss: (0.4552) |  Loss2: (0.0000) | Acc: (84.00%) (36843/43648)
Epoch: 43 | Batch_idx: 350 |  Loss: (0.4554) |  Loss2: (0.0000) | Acc: (84.00%) (37924/44928)
Epoch: 43 | Batch_idx: 360 |  Loss: (0.4546) |  Loss2: (0.0000) | Acc: (84.00%) (39015/46208)
Epoch: 43 | Batch_idx: 370 |  Loss: (0.4541) |  Loss2: (0.0000) | Acc: (84.00%) (40103/47488)
Epoch: 43 | Batch_idx: 380 |  Loss: (0.4538) |  Loss2: (0.0000) | Acc: (84.00%) (41181/48768)
Epoch: 43 | Batch_idx: 390 |  Loss: (0.4542) |  Loss2: (0.0000) | Acc: (84.00%) (42213/50000)
# TEST : Loss: (0.6350) | Acc: (78.00%) (7870/10000)
percent tensor([0.4846, 0.5154], device='cuda:0')
percent tensor([0.4696, 0.5304], device='cuda:0')
percent tensor([0.5162, 0.4838], device='cuda:0')
percent tensor([0.5083, 0.4917], device='cuda:0')
percent tensor([0.5304, 0.4696], device='cuda:0')
percent tensor([0.6584, 0.3416], device='cuda:0')
percent tensor([0.6381, 0.3619], device='cuda:0')
percent tensor([0.9960, 0.0040], device='cuda:0')
Epoch: 44 | Batch_idx: 0 |  Loss: (0.5565) |  Loss2: (0.0000) | Acc: (78.00%) (100/128)
Epoch: 44 | Batch_idx: 10 |  Loss: (0.4647) |  Loss2: (0.0000) | Acc: (84.00%) (1184/1408)
Epoch: 44 | Batch_idx: 20 |  Loss: (0.4500) |  Loss2: (0.0000) | Acc: (84.00%) (2282/2688)
Epoch: 44 | Batch_idx: 30 |  Loss: (0.4553) |  Loss2: (0.0000) | Acc: (84.00%) (3354/3968)
Epoch: 44 | Batch_idx: 40 |  Loss: (0.4552) |  Loss2: (0.0000) | Acc: (84.00%) (4426/5248)
Epoch: 44 | Batch_idx: 50 |  Loss: (0.4487) |  Loss2: (0.0000) | Acc: (84.00%) (5510/6528)
Epoch: 44 | Batch_idx: 60 |  Loss: (0.4459) |  Loss2: (0.0000) | Acc: (84.00%) (6597/7808)
Epoch: 44 | Batch_idx: 70 |  Loss: (0.4445) |  Loss2: (0.0000) | Acc: (84.00%) (7684/9088)
Epoch: 44 | Batch_idx: 80 |  Loss: (0.4465) |  Loss2: (0.0000) | Acc: (84.00%) (8754/10368)
Epoch: 44 | Batch_idx: 90 |  Loss: (0.4415) |  Loss2: (0.0000) | Acc: (84.00%) (9853/11648)
Epoch: 44 | Batch_idx: 100 |  Loss: (0.4428) |  Loss2: (0.0000) | Acc: (84.00%) (10925/12928)
Epoch: 44 | Batch_idx: 110 |  Loss: (0.4411) |  Loss2: (0.0000) | Acc: (84.00%) (12011/14208)
Epoch: 44 | Batch_idx: 120 |  Loss: (0.4421) |  Loss2: (0.0000) | Acc: (84.00%) (13094/15488)
Epoch: 44 | Batch_idx: 130 |  Loss: (0.4442) |  Loss2: (0.0000) | Acc: (84.00%) (14161/16768)
Epoch: 44 | Batch_idx: 140 |  Loss: (0.4426) |  Loss2: (0.0000) | Acc: (84.00%) (15261/18048)
Epoch: 44 | Batch_idx: 150 |  Loss: (0.4404) |  Loss2: (0.0000) | Acc: (84.00%) (16360/19328)
Epoch: 44 | Batch_idx: 160 |  Loss: (0.4396) |  Loss2: (0.0000) | Acc: (84.00%) (17456/20608)
Epoch: 44 | Batch_idx: 170 |  Loss: (0.4405) |  Loss2: (0.0000) | Acc: (84.00%) (18538/21888)
Epoch: 44 | Batch_idx: 180 |  Loss: (0.4426) |  Loss2: (0.0000) | Acc: (84.00%) (19607/23168)
Epoch: 44 | Batch_idx: 190 |  Loss: (0.4434) |  Loss2: (0.0000) | Acc: (84.00%) (20674/24448)
Epoch: 44 | Batch_idx: 200 |  Loss: (0.4455) |  Loss2: (0.0000) | Acc: (84.00%) (21733/25728)
Epoch: 44 | Batch_idx: 210 |  Loss: (0.4447) |  Loss2: (0.0000) | Acc: (84.00%) (22824/27008)
Epoch: 44 | Batch_idx: 220 |  Loss: (0.4446) |  Loss2: (0.0000) | Acc: (84.00%) (23912/28288)
Epoch: 44 | Batch_idx: 230 |  Loss: (0.4436) |  Loss2: (0.0000) | Acc: (84.00%) (24999/29568)
Epoch: 44 | Batch_idx: 240 |  Loss: (0.4443) |  Loss2: (0.0000) | Acc: (84.00%) (26078/30848)
Epoch: 44 | Batch_idx: 250 |  Loss: (0.4435) |  Loss2: (0.0000) | Acc: (84.00%) (27174/32128)
Epoch: 44 | Batch_idx: 260 |  Loss: (0.4427) |  Loss2: (0.0000) | Acc: (84.00%) (28272/33408)
Epoch: 44 | Batch_idx: 270 |  Loss: (0.4423) |  Loss2: (0.0000) | Acc: (84.00%) (29372/34688)
Epoch: 44 | Batch_idx: 280 |  Loss: (0.4420) |  Loss2: (0.0000) | Acc: (84.00%) (30470/35968)
Epoch: 44 | Batch_idx: 290 |  Loss: (0.4419) |  Loss2: (0.0000) | Acc: (84.00%) (31553/37248)
Epoch: 44 | Batch_idx: 300 |  Loss: (0.4414) |  Loss2: (0.0000) | Acc: (84.00%) (32639/38528)
Epoch: 44 | Batch_idx: 310 |  Loss: (0.4409) |  Loss2: (0.0000) | Acc: (84.00%) (33730/39808)
Epoch: 44 | Batch_idx: 320 |  Loss: (0.4413) |  Loss2: (0.0000) | Acc: (84.00%) (34803/41088)
Epoch: 44 | Batch_idx: 330 |  Loss: (0.4412) |  Loss2: (0.0000) | Acc: (84.00%) (35893/42368)
Epoch: 44 | Batch_idx: 340 |  Loss: (0.4407) |  Loss2: (0.0000) | Acc: (84.00%) (36990/43648)
Epoch: 44 | Batch_idx: 350 |  Loss: (0.4404) |  Loss2: (0.0000) | Acc: (84.00%) (38083/44928)
Epoch: 44 | Batch_idx: 360 |  Loss: (0.4400) |  Loss2: (0.0000) | Acc: (84.00%) (39181/46208)
Epoch: 44 | Batch_idx: 370 |  Loss: (0.4390) |  Loss2: (0.0000) | Acc: (84.00%) (40296/47488)
Epoch: 44 | Batch_idx: 380 |  Loss: (0.4396) |  Loss2: (0.0000) | Acc: (84.00%) (41370/48768)
Epoch: 44 | Batch_idx: 390 |  Loss: (0.4402) |  Loss2: (0.0000) | Acc: (84.00%) (42405/50000)
# TEST : Loss: (0.6114) | Acc: (79.00%) (7966/10000)
percent tensor([0.4847, 0.5153], device='cuda:0')
percent tensor([0.4705, 0.5295], device='cuda:0')
percent tensor([0.5161, 0.4839], device='cuda:0')
percent tensor([0.5083, 0.4917], device='cuda:0')
percent tensor([0.5315, 0.4685], device='cuda:0')
percent tensor([0.6612, 0.3388], device='cuda:0')
percent tensor([0.6463, 0.3537], device='cuda:0')
percent tensor([0.9969, 0.0031], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 45 | Batch_idx: 0 |  Loss: (0.4146) |  Loss2: (0.0000) | Acc: (82.00%) (106/128)
Epoch: 45 | Batch_idx: 10 |  Loss: (0.6918) |  Loss2: (0.0000) | Acc: (76.00%) (1071/1408)
Epoch: 45 | Batch_idx: 20 |  Loss: (0.8134) |  Loss2: (0.0000) | Acc: (72.00%) (1943/2688)
Epoch: 45 | Batch_idx: 30 |  Loss: (0.8855) |  Loss2: (0.0000) | Acc: (70.00%) (2801/3968)
Epoch: 45 | Batch_idx: 40 |  Loss: (0.9202) |  Loss2: (0.0000) | Acc: (69.00%) (3652/5248)
Epoch: 45 | Batch_idx: 50 |  Loss: (0.9410) |  Loss2: (0.0000) | Acc: (69.00%) (4510/6528)
Epoch: 45 | Batch_idx: 60 |  Loss: (0.9302) |  Loss2: (0.0000) | Acc: (69.00%) (5411/7808)
Epoch: 45 | Batch_idx: 70 |  Loss: (0.9230) |  Loss2: (0.0000) | Acc: (69.00%) (6312/9088)
Epoch: 45 | Batch_idx: 80 |  Loss: (0.9117) |  Loss2: (0.0000) | Acc: (69.00%) (7240/10368)
Epoch: 45 | Batch_idx: 90 |  Loss: (0.8942) |  Loss2: (0.0000) | Acc: (70.00%) (8210/11648)
Epoch: 45 | Batch_idx: 100 |  Loss: (0.8795) |  Loss2: (0.0000) | Acc: (70.00%) (9161/12928)
Epoch: 45 | Batch_idx: 110 |  Loss: (0.8670) |  Loss2: (0.0000) | Acc: (71.00%) (10117/14208)
Epoch: 45 | Batch_idx: 120 |  Loss: (0.8504) |  Loss2: (0.0000) | Acc: (71.00%) (11108/15488)
Epoch: 45 | Batch_idx: 130 |  Loss: (0.8402) |  Loss2: (0.0000) | Acc: (72.00%) (12077/16768)
Epoch: 45 | Batch_idx: 140 |  Loss: (0.8295) |  Loss2: (0.0000) | Acc: (72.00%) (13058/18048)
Epoch: 45 | Batch_idx: 150 |  Loss: (0.8164) |  Loss2: (0.0000) | Acc: (72.00%) (14057/19328)
Epoch: 45 | Batch_idx: 160 |  Loss: (0.8065) |  Loss2: (0.0000) | Acc: (72.00%) (15038/20608)
Epoch: 45 | Batch_idx: 170 |  Loss: (0.7995) |  Loss2: (0.0000) | Acc: (73.00%) (16009/21888)
Epoch: 45 | Batch_idx: 180 |  Loss: (0.7935) |  Loss2: (0.0000) | Acc: (73.00%) (16998/23168)
Epoch: 45 | Batch_idx: 190 |  Loss: (0.7858) |  Loss2: (0.0000) | Acc: (73.00%) (17993/24448)
Epoch: 45 | Batch_idx: 200 |  Loss: (0.7821) |  Loss2: (0.0000) | Acc: (73.00%) (18951/25728)
Epoch: 45 | Batch_idx: 210 |  Loss: (0.7759) |  Loss2: (0.0000) | Acc: (73.00%) (19941/27008)
Epoch: 45 | Batch_idx: 220 |  Loss: (0.7690) |  Loss2: (0.0000) | Acc: (73.00%) (20930/28288)
Epoch: 45 | Batch_idx: 230 |  Loss: (0.7642) |  Loss2: (0.0000) | Acc: (74.00%) (21925/29568)
Epoch: 45 | Batch_idx: 240 |  Loss: (0.7568) |  Loss2: (0.0000) | Acc: (74.00%) (22944/30848)
Epoch: 45 | Batch_idx: 250 |  Loss: (0.7510) |  Loss2: (0.0000) | Acc: (74.00%) (23959/32128)
Epoch: 45 | Batch_idx: 260 |  Loss: (0.7454) |  Loss2: (0.0000) | Acc: (74.00%) (24966/33408)
Epoch: 45 | Batch_idx: 270 |  Loss: (0.7387) |  Loss2: (0.0000) | Acc: (74.00%) (25993/34688)
Epoch: 45 | Batch_idx: 280 |  Loss: (0.7334) |  Loss2: (0.0000) | Acc: (75.00%) (27003/35968)
Epoch: 45 | Batch_idx: 290 |  Loss: (0.7284) |  Loss2: (0.0000) | Acc: (75.00%) (28014/37248)
Epoch: 45 | Batch_idx: 300 |  Loss: (0.7242) |  Loss2: (0.0000) | Acc: (75.00%) (29038/38528)
Epoch: 45 | Batch_idx: 310 |  Loss: (0.7190) |  Loss2: (0.0000) | Acc: (75.00%) (30077/39808)
Epoch: 45 | Batch_idx: 320 |  Loss: (0.7167) |  Loss2: (0.0000) | Acc: (75.00%) (31089/41088)
Epoch: 45 | Batch_idx: 330 |  Loss: (0.7133) |  Loss2: (0.0000) | Acc: (75.00%) (32085/42368)
Epoch: 45 | Batch_idx: 340 |  Loss: (0.7098) |  Loss2: (0.0000) | Acc: (75.00%) (33094/43648)
Epoch: 45 | Batch_idx: 350 |  Loss: (0.7066) |  Loss2: (0.0000) | Acc: (75.00%) (34103/44928)
Epoch: 45 | Batch_idx: 360 |  Loss: (0.7036) |  Loss2: (0.0000) | Acc: (76.00%) (35127/46208)
Epoch: 45 | Batch_idx: 370 |  Loss: (0.7004) |  Loss2: (0.0000) | Acc: (76.00%) (36129/47488)
Epoch: 45 | Batch_idx: 380 |  Loss: (0.6958) |  Loss2: (0.0000) | Acc: (76.00%) (37184/48768)
Epoch: 45 | Batch_idx: 390 |  Loss: (0.6923) |  Loss2: (0.0000) | Acc: (76.00%) (38171/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_045.pth.tar'
# TEST : Loss: (0.6555) | Acc: (77.00%) (7763/10000)
percent tensor([0.4931, 0.5069], device='cuda:0')
percent tensor([0.4495, 0.5505], device='cuda:0')
percent tensor([0.4988, 0.5012], device='cuda:0')
percent tensor([0.5207, 0.4793], device='cuda:0')
percent tensor([0.5304, 0.4696], device='cuda:0')
percent tensor([0.5950, 0.4050], device='cuda:0')
percent tensor([0.5528, 0.4472], device='cuda:0')
percent tensor([0.9960, 0.0040], device='cuda:0')
Epoch: 46 | Batch_idx: 0 |  Loss: (0.5123) |  Loss2: (0.0000) | Acc: (82.00%) (106/128)
Epoch: 46 | Batch_idx: 10 |  Loss: (0.5447) |  Loss2: (0.0000) | Acc: (80.00%) (1131/1408)
Epoch: 46 | Batch_idx: 20 |  Loss: (0.5632) |  Loss2: (0.0000) | Acc: (80.00%) (2163/2688)
Epoch: 46 | Batch_idx: 30 |  Loss: (0.5595) |  Loss2: (0.0000) | Acc: (80.00%) (3191/3968)
Epoch: 46 | Batch_idx: 40 |  Loss: (0.5587) |  Loss2: (0.0000) | Acc: (80.00%) (4207/5248)
Epoch: 46 | Batch_idx: 50 |  Loss: (0.5539) |  Loss2: (0.0000) | Acc: (80.00%) (5238/6528)
Epoch: 46 | Batch_idx: 60 |  Loss: (0.5519) |  Loss2: (0.0000) | Acc: (80.00%) (6274/7808)
Epoch: 46 | Batch_idx: 70 |  Loss: (0.5520) |  Loss2: (0.0000) | Acc: (80.00%) (7316/9088)
Epoch: 46 | Batch_idx: 80 |  Loss: (0.5578) |  Loss2: (0.0000) | Acc: (80.00%) (8332/10368)
Epoch: 46 | Batch_idx: 90 |  Loss: (0.5554) |  Loss2: (0.0000) | Acc: (80.00%) (9368/11648)
Epoch: 46 | Batch_idx: 100 |  Loss: (0.5550) |  Loss2: (0.0000) | Acc: (80.00%) (10414/12928)
Epoch: 46 | Batch_idx: 110 |  Loss: (0.5537) |  Loss2: (0.0000) | Acc: (80.00%) (11459/14208)
Epoch: 46 | Batch_idx: 120 |  Loss: (0.5538) |  Loss2: (0.0000) | Acc: (80.00%) (12487/15488)
Epoch: 46 | Batch_idx: 130 |  Loss: (0.5543) |  Loss2: (0.0000) | Acc: (80.00%) (13500/16768)
Epoch: 46 | Batch_idx: 140 |  Loss: (0.5553) |  Loss2: (0.0000) | Acc: (80.00%) (14522/18048)
Epoch: 46 | Batch_idx: 150 |  Loss: (0.5570) |  Loss2: (0.0000) | Acc: (80.00%) (15537/19328)
Epoch: 46 | Batch_idx: 160 |  Loss: (0.5555) |  Loss2: (0.0000) | Acc: (80.00%) (16577/20608)
Epoch: 46 | Batch_idx: 170 |  Loss: (0.5546) |  Loss2: (0.0000) | Acc: (80.00%) (17620/21888)
Epoch: 46 | Batch_idx: 180 |  Loss: (0.5544) |  Loss2: (0.0000) | Acc: (80.00%) (18654/23168)
Epoch: 46 | Batch_idx: 190 |  Loss: (0.5540) |  Loss2: (0.0000) | Acc: (80.00%) (19685/24448)
Epoch: 46 | Batch_idx: 200 |  Loss: (0.5522) |  Loss2: (0.0000) | Acc: (80.00%) (20754/25728)
Epoch: 46 | Batch_idx: 210 |  Loss: (0.5499) |  Loss2: (0.0000) | Acc: (80.00%) (21826/27008)
Epoch: 46 | Batch_idx: 220 |  Loss: (0.5498) |  Loss2: (0.0000) | Acc: (80.00%) (22860/28288)
Epoch: 46 | Batch_idx: 230 |  Loss: (0.5482) |  Loss2: (0.0000) | Acc: (80.00%) (23916/29568)
Epoch: 46 | Batch_idx: 240 |  Loss: (0.5473) |  Loss2: (0.0000) | Acc: (80.00%) (24964/30848)
Epoch: 46 | Batch_idx: 250 |  Loss: (0.5476) |  Loss2: (0.0000) | Acc: (80.00%) (25990/32128)
Epoch: 46 | Batch_idx: 260 |  Loss: (0.5475) |  Loss2: (0.0000) | Acc: (80.00%) (27039/33408)
Epoch: 46 | Batch_idx: 270 |  Loss: (0.5476) |  Loss2: (0.0000) | Acc: (80.00%) (28067/34688)
Epoch: 46 | Batch_idx: 280 |  Loss: (0.5464) |  Loss2: (0.0000) | Acc: (80.00%) (29129/35968)
Epoch: 46 | Batch_idx: 290 |  Loss: (0.5459) |  Loss2: (0.0000) | Acc: (81.00%) (30174/37248)
Epoch: 46 | Batch_idx: 300 |  Loss: (0.5449) |  Loss2: (0.0000) | Acc: (81.00%) (31228/38528)
Epoch: 46 | Batch_idx: 310 |  Loss: (0.5450) |  Loss2: (0.0000) | Acc: (81.00%) (32256/39808)
Epoch: 46 | Batch_idx: 320 |  Loss: (0.5444) |  Loss2: (0.0000) | Acc: (81.00%) (33296/41088)
Epoch: 46 | Batch_idx: 330 |  Loss: (0.5428) |  Loss2: (0.0000) | Acc: (81.00%) (34365/42368)
Epoch: 46 | Batch_idx: 340 |  Loss: (0.5422) |  Loss2: (0.0000) | Acc: (81.00%) (35396/43648)
Epoch: 46 | Batch_idx: 350 |  Loss: (0.5413) |  Loss2: (0.0000) | Acc: (81.00%) (36449/44928)
Epoch: 46 | Batch_idx: 360 |  Loss: (0.5404) |  Loss2: (0.0000) | Acc: (81.00%) (37511/46208)
Epoch: 46 | Batch_idx: 370 |  Loss: (0.5394) |  Loss2: (0.0000) | Acc: (81.00%) (38571/47488)
Epoch: 46 | Batch_idx: 380 |  Loss: (0.5389) |  Loss2: (0.0000) | Acc: (81.00%) (39611/48768)
Epoch: 46 | Batch_idx: 390 |  Loss: (0.5382) |  Loss2: (0.0000) | Acc: (81.00%) (40626/50000)
# TEST : Loss: (0.5961) | Acc: (79.00%) (7953/10000)
percent tensor([0.4952, 0.5048], device='cuda:0')
percent tensor([0.4479, 0.5521], device='cuda:0')
percent tensor([0.5009, 0.4991], device='cuda:0')
percent tensor([0.5231, 0.4769], device='cuda:0')
percent tensor([0.5308, 0.4692], device='cuda:0')
percent tensor([0.6034, 0.3966], device='cuda:0')
percent tensor([0.5580, 0.4420], device='cuda:0')
percent tensor([0.9962, 0.0038], device='cuda:0')
Epoch: 47 | Batch_idx: 0 |  Loss: (0.6153) |  Loss2: (0.0000) | Acc: (79.00%) (102/128)
Epoch: 47 | Batch_idx: 10 |  Loss: (0.5093) |  Loss2: (0.0000) | Acc: (82.00%) (1165/1408)
Epoch: 47 | Batch_idx: 20 |  Loss: (0.4957) |  Loss2: (0.0000) | Acc: (82.00%) (2211/2688)
Epoch: 47 | Batch_idx: 30 |  Loss: (0.4939) |  Loss2: (0.0000) | Acc: (82.00%) (3273/3968)
Epoch: 47 | Batch_idx: 40 |  Loss: (0.4915) |  Loss2: (0.0000) | Acc: (82.00%) (4337/5248)
Epoch: 47 | Batch_idx: 50 |  Loss: (0.5016) |  Loss2: (0.0000) | Acc: (82.00%) (5376/6528)
Epoch: 47 | Batch_idx: 60 |  Loss: (0.5045) |  Loss2: (0.0000) | Acc: (82.00%) (6433/7808)
Epoch: 47 | Batch_idx: 70 |  Loss: (0.5163) |  Loss2: (0.0000) | Acc: (81.00%) (7439/9088)
Epoch: 47 | Batch_idx: 80 |  Loss: (0.5160) |  Loss2: (0.0000) | Acc: (81.00%) (8497/10368)
Epoch: 47 | Batch_idx: 90 |  Loss: (0.5178) |  Loss2: (0.0000) | Acc: (81.00%) (9543/11648)
Epoch: 47 | Batch_idx: 100 |  Loss: (0.5188) |  Loss2: (0.0000) | Acc: (81.00%) (10594/12928)
Epoch: 47 | Batch_idx: 110 |  Loss: (0.5174) |  Loss2: (0.0000) | Acc: (81.00%) (11643/14208)
Epoch: 47 | Batch_idx: 120 |  Loss: (0.5199) |  Loss2: (0.0000) | Acc: (81.00%) (12675/15488)
Epoch: 47 | Batch_idx: 130 |  Loss: (0.5227) |  Loss2: (0.0000) | Acc: (81.00%) (13691/16768)
Epoch: 47 | Batch_idx: 140 |  Loss: (0.5221) |  Loss2: (0.0000) | Acc: (81.00%) (14734/18048)
Epoch: 47 | Batch_idx: 150 |  Loss: (0.5251) |  Loss2: (0.0000) | Acc: (81.00%) (15768/19328)
Epoch: 47 | Batch_idx: 160 |  Loss: (0.5229) |  Loss2: (0.0000) | Acc: (81.00%) (16835/20608)
Epoch: 47 | Batch_idx: 170 |  Loss: (0.5196) |  Loss2: (0.0000) | Acc: (81.00%) (17911/21888)
Epoch: 47 | Batch_idx: 180 |  Loss: (0.5204) |  Loss2: (0.0000) | Acc: (81.00%) (18947/23168)
Epoch: 47 | Batch_idx: 190 |  Loss: (0.5230) |  Loss2: (0.0000) | Acc: (81.00%) (19973/24448)
Epoch: 47 | Batch_idx: 200 |  Loss: (0.5239) |  Loss2: (0.0000) | Acc: (81.00%) (21014/25728)
Epoch: 47 | Batch_idx: 210 |  Loss: (0.5216) |  Loss2: (0.0000) | Acc: (81.00%) (22080/27008)
Epoch: 47 | Batch_idx: 220 |  Loss: (0.5218) |  Loss2: (0.0000) | Acc: (81.00%) (23122/28288)
Epoch: 47 | Batch_idx: 230 |  Loss: (0.5205) |  Loss2: (0.0000) | Acc: (81.00%) (24194/29568)
Epoch: 47 | Batch_idx: 240 |  Loss: (0.5212) |  Loss2: (0.0000) | Acc: (81.00%) (25245/30848)
Epoch: 47 | Batch_idx: 250 |  Loss: (0.5197) |  Loss2: (0.0000) | Acc: (81.00%) (26301/32128)
Epoch: 47 | Batch_idx: 260 |  Loss: (0.5202) |  Loss2: (0.0000) | Acc: (81.00%) (27347/33408)
Epoch: 47 | Batch_idx: 270 |  Loss: (0.5205) |  Loss2: (0.0000) | Acc: (81.00%) (28392/34688)
Epoch: 47 | Batch_idx: 280 |  Loss: (0.5190) |  Loss2: (0.0000) | Acc: (81.00%) (29464/35968)
Epoch: 47 | Batch_idx: 290 |  Loss: (0.5207) |  Loss2: (0.0000) | Acc: (81.00%) (30498/37248)
Epoch: 47 | Batch_idx: 300 |  Loss: (0.5205) |  Loss2: (0.0000) | Acc: (81.00%) (31555/38528)
Epoch: 47 | Batch_idx: 310 |  Loss: (0.5192) |  Loss2: (0.0000) | Acc: (81.00%) (32617/39808)
Epoch: 47 | Batch_idx: 320 |  Loss: (0.5188) |  Loss2: (0.0000) | Acc: (81.00%) (33680/41088)
Epoch: 47 | Batch_idx: 330 |  Loss: (0.5185) |  Loss2: (0.0000) | Acc: (81.00%) (34740/42368)
Epoch: 47 | Batch_idx: 340 |  Loss: (0.5182) |  Loss2: (0.0000) | Acc: (81.00%) (35791/43648)
Epoch: 47 | Batch_idx: 350 |  Loss: (0.5177) |  Loss2: (0.0000) | Acc: (82.00%) (36858/44928)
Epoch: 47 | Batch_idx: 360 |  Loss: (0.5166) |  Loss2: (0.0000) | Acc: (82.00%) (37940/46208)
Epoch: 47 | Batch_idx: 370 |  Loss: (0.5158) |  Loss2: (0.0000) | Acc: (82.00%) (39015/47488)
Epoch: 47 | Batch_idx: 380 |  Loss: (0.5153) |  Loss2: (0.0000) | Acc: (82.00%) (40080/48768)
Epoch: 47 | Batch_idx: 390 |  Loss: (0.5145) |  Loss2: (0.0000) | Acc: (82.00%) (41103/50000)
# TEST : Loss: (0.5688) | Acc: (80.00%) (8040/10000)
percent tensor([0.4956, 0.5044], device='cuda:0')
percent tensor([0.4494, 0.5506], device='cuda:0')
percent tensor([0.5032, 0.4968], device='cuda:0')
percent tensor([0.5216, 0.4784], device='cuda:0')
percent tensor([0.5312, 0.4688], device='cuda:0')
percent tensor([0.6095, 0.3905], device='cuda:0')
percent tensor([0.5662, 0.4338], device='cuda:0')
percent tensor([0.9965, 0.0035], device='cuda:0')
Epoch: 48 | Batch_idx: 0 |  Loss: (0.5202) |  Loss2: (0.0000) | Acc: (82.00%) (105/128)
Epoch: 48 | Batch_idx: 10 |  Loss: (0.5603) |  Loss2: (0.0000) | Acc: (80.00%) (1129/1408)
Epoch: 48 | Batch_idx: 20 |  Loss: (0.5569) |  Loss2: (0.0000) | Acc: (80.00%) (2165/2688)
Epoch: 48 | Batch_idx: 30 |  Loss: (0.5437) |  Loss2: (0.0000) | Acc: (81.00%) (3223/3968)
Epoch: 48 | Batch_idx: 40 |  Loss: (0.5345) |  Loss2: (0.0000) | Acc: (81.00%) (4278/5248)
Epoch: 48 | Batch_idx: 50 |  Loss: (0.5347) |  Loss2: (0.0000) | Acc: (81.00%) (5322/6528)
Epoch: 48 | Batch_idx: 60 |  Loss: (0.5243) |  Loss2: (0.0000) | Acc: (81.00%) (6401/7808)
Epoch: 48 | Batch_idx: 70 |  Loss: (0.5220) |  Loss2: (0.0000) | Acc: (81.00%) (7452/9088)
Epoch: 48 | Batch_idx: 80 |  Loss: (0.5175) |  Loss2: (0.0000) | Acc: (82.00%) (8519/10368)
Epoch: 48 | Batch_idx: 90 |  Loss: (0.5128) |  Loss2: (0.0000) | Acc: (82.00%) (9596/11648)
Epoch: 48 | Batch_idx: 100 |  Loss: (0.5170) |  Loss2: (0.0000) | Acc: (82.00%) (10634/12928)
Epoch: 48 | Batch_idx: 110 |  Loss: (0.5134) |  Loss2: (0.0000) | Acc: (82.00%) (11709/14208)
Epoch: 48 | Batch_idx: 120 |  Loss: (0.5116) |  Loss2: (0.0000) | Acc: (82.00%) (12768/15488)
Epoch: 48 | Batch_idx: 130 |  Loss: (0.5081) |  Loss2: (0.0000) | Acc: (82.00%) (13851/16768)
Epoch: 48 | Batch_idx: 140 |  Loss: (0.5083) |  Loss2: (0.0000) | Acc: (82.00%) (14905/18048)
Epoch: 48 | Batch_idx: 150 |  Loss: (0.5076) |  Loss2: (0.0000) | Acc: (82.00%) (15969/19328)
Epoch: 48 | Batch_idx: 160 |  Loss: (0.5069) |  Loss2: (0.0000) | Acc: (82.00%) (17023/20608)
Epoch: 48 | Batch_idx: 170 |  Loss: (0.5058) |  Loss2: (0.0000) | Acc: (82.00%) (18085/21888)
Epoch: 48 | Batch_idx: 180 |  Loss: (0.5041) |  Loss2: (0.0000) | Acc: (82.00%) (19141/23168)
Epoch: 48 | Batch_idx: 190 |  Loss: (0.5050) |  Loss2: (0.0000) | Acc: (82.00%) (20173/24448)
Epoch: 48 | Batch_idx: 200 |  Loss: (0.5040) |  Loss2: (0.0000) | Acc: (82.00%) (21244/25728)
Epoch: 48 | Batch_idx: 210 |  Loss: (0.5020) |  Loss2: (0.0000) | Acc: (82.00%) (22321/27008)
Epoch: 48 | Batch_idx: 220 |  Loss: (0.5021) |  Loss2: (0.0000) | Acc: (82.00%) (23392/28288)
Epoch: 48 | Batch_idx: 230 |  Loss: (0.5025) |  Loss2: (0.0000) | Acc: (82.00%) (24445/29568)
Epoch: 48 | Batch_idx: 240 |  Loss: (0.5039) |  Loss2: (0.0000) | Acc: (82.00%) (25473/30848)
Epoch: 48 | Batch_idx: 250 |  Loss: (0.5040) |  Loss2: (0.0000) | Acc: (82.00%) (26520/32128)
Epoch: 48 | Batch_idx: 260 |  Loss: (0.5023) |  Loss2: (0.0000) | Acc: (82.00%) (27590/33408)
Epoch: 48 | Batch_idx: 270 |  Loss: (0.5027) |  Loss2: (0.0000) | Acc: (82.00%) (28648/34688)
Epoch: 48 | Batch_idx: 280 |  Loss: (0.5012) |  Loss2: (0.0000) | Acc: (82.00%) (29722/35968)
Epoch: 48 | Batch_idx: 290 |  Loss: (0.5005) |  Loss2: (0.0000) | Acc: (82.00%) (30784/37248)
Epoch: 48 | Batch_idx: 300 |  Loss: (0.4988) |  Loss2: (0.0000) | Acc: (82.00%) (31874/38528)
Epoch: 48 | Batch_idx: 310 |  Loss: (0.5000) |  Loss2: (0.0000) | Acc: (82.00%) (32923/39808)
Epoch: 48 | Batch_idx: 320 |  Loss: (0.4999) |  Loss2: (0.0000) | Acc: (82.00%) (33977/41088)
Epoch: 48 | Batch_idx: 330 |  Loss: (0.4989) |  Loss2: (0.0000) | Acc: (82.00%) (35043/42368)
Epoch: 48 | Batch_idx: 340 |  Loss: (0.4986) |  Loss2: (0.0000) | Acc: (82.00%) (36096/43648)
Epoch: 48 | Batch_idx: 350 |  Loss: (0.4992) |  Loss2: (0.0000) | Acc: (82.00%) (37149/44928)
Epoch: 48 | Batch_idx: 360 |  Loss: (0.4990) |  Loss2: (0.0000) | Acc: (82.00%) (38207/46208)
Epoch: 48 | Batch_idx: 370 |  Loss: (0.4995) |  Loss2: (0.0000) | Acc: (82.00%) (39252/47488)
Epoch: 48 | Batch_idx: 380 |  Loss: (0.4998) |  Loss2: (0.0000) | Acc: (82.00%) (40299/48768)
Epoch: 48 | Batch_idx: 390 |  Loss: (0.4991) |  Loss2: (0.0000) | Acc: (82.00%) (41319/50000)
# TEST : Loss: (0.5497) | Acc: (81.00%) (8103/10000)
percent tensor([0.4957, 0.5043], device='cuda:0')
percent tensor([0.4519, 0.5481], device='cuda:0')
percent tensor([0.5071, 0.4929], device='cuda:0')
percent tensor([0.5202, 0.4798], device='cuda:0')
percent tensor([0.5320, 0.4680], device='cuda:0')
percent tensor([0.6155, 0.3845], device='cuda:0')
percent tensor([0.5799, 0.4201], device='cuda:0')
percent tensor([0.9968, 0.0032], device='cuda:0')
Epoch: 49 | Batch_idx: 0 |  Loss: (0.4585) |  Loss2: (0.0000) | Acc: (84.00%) (108/128)
Epoch: 49 | Batch_idx: 10 |  Loss: (0.4779) |  Loss2: (0.0000) | Acc: (83.00%) (1180/1408)
Epoch: 49 | Batch_idx: 20 |  Loss: (0.4799) |  Loss2: (0.0000) | Acc: (83.00%) (2250/2688)
Epoch: 49 | Batch_idx: 30 |  Loss: (0.4942) |  Loss2: (0.0000) | Acc: (83.00%) (3305/3968)
Epoch: 49 | Batch_idx: 40 |  Loss: (0.5032) |  Loss2: (0.0000) | Acc: (82.00%) (4333/5248)
Epoch: 49 | Batch_idx: 50 |  Loss: (0.5011) |  Loss2: (0.0000) | Acc: (82.00%) (5392/6528)
Epoch: 49 | Batch_idx: 60 |  Loss: (0.4991) |  Loss2: (0.0000) | Acc: (82.00%) (6462/7808)
Epoch: 49 | Batch_idx: 70 |  Loss: (0.4984) |  Loss2: (0.0000) | Acc: (82.00%) (7532/9088)
Epoch: 49 | Batch_idx: 80 |  Loss: (0.4984) |  Loss2: (0.0000) | Acc: (82.00%) (8590/10368)
Epoch: 49 | Batch_idx: 90 |  Loss: (0.4951) |  Loss2: (0.0000) | Acc: (82.00%) (9661/11648)
Epoch: 49 | Batch_idx: 100 |  Loss: (0.4944) |  Loss2: (0.0000) | Acc: (82.00%) (10724/12928)
Epoch: 49 | Batch_idx: 110 |  Loss: (0.4976) |  Loss2: (0.0000) | Acc: (82.00%) (11774/14208)
Epoch: 49 | Batch_idx: 120 |  Loss: (0.4955) |  Loss2: (0.0000) | Acc: (82.00%) (12842/15488)
Epoch: 49 | Batch_idx: 130 |  Loss: (0.4957) |  Loss2: (0.0000) | Acc: (82.00%) (13899/16768)
Epoch: 49 | Batch_idx: 140 |  Loss: (0.4927) |  Loss2: (0.0000) | Acc: (82.00%) (14979/18048)
Epoch: 49 | Batch_idx: 150 |  Loss: (0.4927) |  Loss2: (0.0000) | Acc: (83.00%) (16059/19328)
Epoch: 49 | Batch_idx: 160 |  Loss: (0.4942) |  Loss2: (0.0000) | Acc: (83.00%) (17112/20608)
Epoch: 49 | Batch_idx: 170 |  Loss: (0.4942) |  Loss2: (0.0000) | Acc: (83.00%) (18175/21888)
Epoch: 49 | Batch_idx: 180 |  Loss: (0.4928) |  Loss2: (0.0000) | Acc: (83.00%) (19263/23168)
Epoch: 49 | Batch_idx: 190 |  Loss: (0.4935) |  Loss2: (0.0000) | Acc: (83.00%) (20333/24448)
Epoch: 49 | Batch_idx: 200 |  Loss: (0.4950) |  Loss2: (0.0000) | Acc: (83.00%) (21372/25728)
Epoch: 49 | Batch_idx: 210 |  Loss: (0.4943) |  Loss2: (0.0000) | Acc: (83.00%) (22447/27008)
Epoch: 49 | Batch_idx: 220 |  Loss: (0.4946) |  Loss2: (0.0000) | Acc: (83.00%) (23502/28288)
Epoch: 49 | Batch_idx: 230 |  Loss: (0.4925) |  Loss2: (0.0000) | Acc: (83.00%) (24592/29568)
Epoch: 49 | Batch_idx: 240 |  Loss: (0.4942) |  Loss2: (0.0000) | Acc: (83.00%) (25637/30848)
Epoch: 49 | Batch_idx: 250 |  Loss: (0.4918) |  Loss2: (0.0000) | Acc: (83.00%) (26739/32128)
Epoch: 49 | Batch_idx: 260 |  Loss: (0.4913) |  Loss2: (0.0000) | Acc: (83.00%) (27808/33408)
Epoch: 49 | Batch_idx: 270 |  Loss: (0.4891) |  Loss2: (0.0000) | Acc: (83.00%) (28897/34688)
Epoch: 49 | Batch_idx: 280 |  Loss: (0.4901) |  Loss2: (0.0000) | Acc: (83.00%) (29933/35968)
Epoch: 49 | Batch_idx: 290 |  Loss: (0.4907) |  Loss2: (0.0000) | Acc: (83.00%) (30992/37248)
Epoch: 49 | Batch_idx: 300 |  Loss: (0.4911) |  Loss2: (0.0000) | Acc: (83.00%) (32041/38528)
Epoch: 49 | Batch_idx: 310 |  Loss: (0.4914) |  Loss2: (0.0000) | Acc: (83.00%) (33104/39808)
Epoch: 49 | Batch_idx: 320 |  Loss: (0.4908) |  Loss2: (0.0000) | Acc: (83.00%) (34182/41088)
Epoch: 49 | Batch_idx: 330 |  Loss: (0.4913) |  Loss2: (0.0000) | Acc: (83.00%) (35250/42368)
Epoch: 49 | Batch_idx: 340 |  Loss: (0.4919) |  Loss2: (0.0000) | Acc: (83.00%) (36294/43648)
Epoch: 49 | Batch_idx: 350 |  Loss: (0.4921) |  Loss2: (0.0000) | Acc: (83.00%) (37357/44928)
Epoch: 49 | Batch_idx: 360 |  Loss: (0.4922) |  Loss2: (0.0000) | Acc: (83.00%) (38432/46208)
Epoch: 49 | Batch_idx: 370 |  Loss: (0.4921) |  Loss2: (0.0000) | Acc: (83.00%) (39513/47488)
Epoch: 49 | Batch_idx: 380 |  Loss: (0.4916) |  Loss2: (0.0000) | Acc: (83.00%) (40588/48768)
Epoch: 49 | Batch_idx: 390 |  Loss: (0.4913) |  Loss2: (0.0000) | Acc: (83.00%) (41629/50000)
# TEST : Loss: (0.5429) | Acc: (81.00%) (8109/10000)
percent tensor([0.4947, 0.5053], device='cuda:0')
percent tensor([0.4541, 0.5459], device='cuda:0')
percent tensor([0.5086, 0.4914], device='cuda:0')
percent tensor([0.5192, 0.4808], device='cuda:0')
percent tensor([0.5355, 0.4645], device='cuda:0')
percent tensor([0.6158, 0.3842], device='cuda:0')
percent tensor([0.5871, 0.4129], device='cuda:0')
percent tensor([0.9971, 0.0029], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 50 | Batch_idx: 0 |  Loss: (0.5093) |  Loss2: (0.0000) | Acc: (82.00%) (106/128)
Epoch: 50 | Batch_idx: 10 |  Loss: (0.4867) |  Loss2: (0.0000) | Acc: (83.00%) (1182/1408)
Epoch: 50 | Batch_idx: 20 |  Loss: (0.5017) |  Loss2: (0.0000) | Acc: (83.00%) (2235/2688)
Epoch: 50 | Batch_idx: 30 |  Loss: (0.5014) |  Loss2: (0.0000) | Acc: (82.00%) (3292/3968)
Epoch: 50 | Batch_idx: 40 |  Loss: (0.4921) |  Loss2: (0.0000) | Acc: (83.00%) (4359/5248)
Epoch: 50 | Batch_idx: 50 |  Loss: (0.4904) |  Loss2: (0.0000) | Acc: (82.00%) (5403/6528)
Epoch: 50 | Batch_idx: 60 |  Loss: (0.4883) |  Loss2: (0.0000) | Acc: (82.00%) (6471/7808)
Epoch: 50 | Batch_idx: 70 |  Loss: (0.4880) |  Loss2: (0.0000) | Acc: (83.00%) (7555/9088)
Epoch: 50 | Batch_idx: 80 |  Loss: (0.4845) |  Loss2: (0.0000) | Acc: (83.00%) (8636/10368)
Epoch: 50 | Batch_idx: 90 |  Loss: (0.4777) |  Loss2: (0.0000) | Acc: (83.00%) (9721/11648)
Epoch: 50 | Batch_idx: 100 |  Loss: (0.4724) |  Loss2: (0.0000) | Acc: (83.00%) (10805/12928)
Epoch: 50 | Batch_idx: 110 |  Loss: (0.4713) |  Loss2: (0.0000) | Acc: (83.00%) (11882/14208)
Epoch: 50 | Batch_idx: 120 |  Loss: (0.4715) |  Loss2: (0.0000) | Acc: (83.00%) (12957/15488)
Epoch: 50 | Batch_idx: 130 |  Loss: (0.4695) |  Loss2: (0.0000) | Acc: (83.00%) (14034/16768)
Epoch: 50 | Batch_idx: 140 |  Loss: (0.4682) |  Loss2: (0.0000) | Acc: (83.00%) (15106/18048)
Epoch: 50 | Batch_idx: 150 |  Loss: (0.4655) |  Loss2: (0.0000) | Acc: (83.00%) (16198/19328)
Epoch: 50 | Batch_idx: 160 |  Loss: (0.4642) |  Loss2: (0.0000) | Acc: (83.00%) (17286/20608)
Epoch: 50 | Batch_idx: 170 |  Loss: (0.4643) |  Loss2: (0.0000) | Acc: (83.00%) (18354/21888)
Epoch: 50 | Batch_idx: 180 |  Loss: (0.4612) |  Loss2: (0.0000) | Acc: (84.00%) (19463/23168)
Epoch: 50 | Batch_idx: 190 |  Loss: (0.4603) |  Loss2: (0.0000) | Acc: (84.00%) (20545/24448)
Epoch: 50 | Batch_idx: 200 |  Loss: (0.4608) |  Loss2: (0.0000) | Acc: (83.00%) (21611/25728)
Epoch: 50 | Batch_idx: 210 |  Loss: (0.4608) |  Loss2: (0.0000) | Acc: (84.00%) (22692/27008)
Epoch: 50 | Batch_idx: 220 |  Loss: (0.4593) |  Loss2: (0.0000) | Acc: (84.00%) (23783/28288)
Epoch: 50 | Batch_idx: 230 |  Loss: (0.4570) |  Loss2: (0.0000) | Acc: (84.00%) (24874/29568)
Epoch: 50 | Batch_idx: 240 |  Loss: (0.4558) |  Loss2: (0.0000) | Acc: (84.00%) (25965/30848)
Epoch: 50 | Batch_idx: 250 |  Loss: (0.4556) |  Loss2: (0.0000) | Acc: (84.00%) (27042/32128)
Epoch: 50 | Batch_idx: 260 |  Loss: (0.4538) |  Loss2: (0.0000) | Acc: (84.00%) (28136/33408)
Epoch: 50 | Batch_idx: 270 |  Loss: (0.4527) |  Loss2: (0.0000) | Acc: (84.00%) (29234/34688)
Epoch: 50 | Batch_idx: 280 |  Loss: (0.4531) |  Loss2: (0.0000) | Acc: (84.00%) (30302/35968)
Epoch: 50 | Batch_idx: 290 |  Loss: (0.4537) |  Loss2: (0.0000) | Acc: (84.00%) (31380/37248)
Epoch: 50 | Batch_idx: 300 |  Loss: (0.4527) |  Loss2: (0.0000) | Acc: (84.00%) (32455/38528)
Epoch: 50 | Batch_idx: 310 |  Loss: (0.4529) |  Loss2: (0.0000) | Acc: (84.00%) (33527/39808)
Epoch: 50 | Batch_idx: 320 |  Loss: (0.4542) |  Loss2: (0.0000) | Acc: (84.00%) (34594/41088)
Epoch: 50 | Batch_idx: 330 |  Loss: (0.4539) |  Loss2: (0.0000) | Acc: (84.00%) (35680/42368)
Epoch: 50 | Batch_idx: 340 |  Loss: (0.4537) |  Loss2: (0.0000) | Acc: (84.00%) (36752/43648)
Epoch: 50 | Batch_idx: 350 |  Loss: (0.4533) |  Loss2: (0.0000) | Acc: (84.00%) (37842/44928)
Epoch: 50 | Batch_idx: 360 |  Loss: (0.4527) |  Loss2: (0.0000) | Acc: (84.00%) (38923/46208)
Epoch: 50 | Batch_idx: 370 |  Loss: (0.4522) |  Loss2: (0.0000) | Acc: (84.00%) (40020/47488)
Epoch: 50 | Batch_idx: 380 |  Loss: (0.4521) |  Loss2: (0.0000) | Acc: (84.00%) (41111/48768)
Epoch: 50 | Batch_idx: 390 |  Loss: (0.4517) |  Loss2: (0.0000) | Acc: (84.00%) (42167/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_050.pth.tar'
# TEST : Loss: (0.6228) | Acc: (79.00%) (7934/10000)
percent tensor([0.4945, 0.5055], device='cuda:0')
percent tensor([0.4541, 0.5459], device='cuda:0')
percent tensor([0.5090, 0.4910], device='cuda:0')
percent tensor([0.5194, 0.4806], device='cuda:0')
percent tensor([0.5356, 0.4644], device='cuda:0')
percent tensor([0.6148, 0.3852], device='cuda:0')
percent tensor([0.5861, 0.4139], device='cuda:0')
percent tensor([0.9969, 0.0031], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(174.7578, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(799.2773, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(790.8948, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1521.9656, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(511.4439, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2191.7561, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4291.2891, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1419.2577, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6107.1919, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12058.5498, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4006.4846, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16984.6348, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 51 | Batch_idx: 0 |  Loss: (0.4447) |  Loss2: (0.0000) | Acc: (82.00%) (105/128)
Epoch: 51 | Batch_idx: 10 |  Loss: (0.4506) |  Loss2: (0.0000) | Acc: (84.00%) (1186/1408)
Epoch: 51 | Batch_idx: 20 |  Loss: (0.4510) |  Loss2: (0.0000) | Acc: (84.00%) (2270/2688)
Epoch: 51 | Batch_idx: 30 |  Loss: (0.4575) |  Loss2: (0.0000) | Acc: (84.00%) (3345/3968)
Epoch: 51 | Batch_idx: 40 |  Loss: (0.4604) |  Loss2: (0.0000) | Acc: (84.00%) (4410/5248)
Epoch: 51 | Batch_idx: 50 |  Loss: (0.4553) |  Loss2: (0.0000) | Acc: (84.00%) (5494/6528)
Epoch: 51 | Batch_idx: 60 |  Loss: (0.4502) |  Loss2: (0.0000) | Acc: (84.00%) (6583/7808)
Epoch: 51 | Batch_idx: 70 |  Loss: (0.4455) |  Loss2: (0.0000) | Acc: (84.00%) (7680/9088)
Epoch: 51 | Batch_idx: 80 |  Loss: (0.4436) |  Loss2: (0.0000) | Acc: (84.00%) (8777/10368)
Epoch: 51 | Batch_idx: 90 |  Loss: (0.4385) |  Loss2: (0.0000) | Acc: (84.00%) (9887/11648)
Epoch: 51 | Batch_idx: 100 |  Loss: (0.4365) |  Loss2: (0.0000) | Acc: (84.00%) (10979/12928)
Epoch: 51 | Batch_idx: 110 |  Loss: (0.4337) |  Loss2: (0.0000) | Acc: (85.00%) (12079/14208)
Epoch: 51 | Batch_idx: 120 |  Loss: (0.4321) |  Loss2: (0.0000) | Acc: (85.00%) (13188/15488)
Epoch: 51 | Batch_idx: 130 |  Loss: (0.4298) |  Loss2: (0.0000) | Acc: (85.00%) (14283/16768)
Epoch: 51 | Batch_idx: 140 |  Loss: (0.4316) |  Loss2: (0.0000) | Acc: (85.00%) (15367/18048)
Epoch: 51 | Batch_idx: 150 |  Loss: (0.4335) |  Loss2: (0.0000) | Acc: (85.00%) (16445/19328)
Epoch: 51 | Batch_idx: 160 |  Loss: (0.4337) |  Loss2: (0.0000) | Acc: (85.00%) (17532/20608)
Epoch: 51 | Batch_idx: 170 |  Loss: (0.4329) |  Loss2: (0.0000) | Acc: (85.00%) (18640/21888)
Epoch: 51 | Batch_idx: 180 |  Loss: (0.4334) |  Loss2: (0.0000) | Acc: (85.00%) (19737/23168)
Epoch: 51 | Batch_idx: 190 |  Loss: (0.4318) |  Loss2: (0.0000) | Acc: (85.00%) (20838/24448)
Epoch: 51 | Batch_idx: 200 |  Loss: (0.4325) |  Loss2: (0.0000) | Acc: (85.00%) (21935/25728)
Epoch: 51 | Batch_idx: 210 |  Loss: (0.4342) |  Loss2: (0.0000) | Acc: (85.00%) (23010/27008)
Epoch: 51 | Batch_idx: 220 |  Loss: (0.4326) |  Loss2: (0.0000) | Acc: (85.00%) (24127/28288)
Epoch: 51 | Batch_idx: 230 |  Loss: (0.4331) |  Loss2: (0.0000) | Acc: (85.00%) (25222/29568)
Epoch: 51 | Batch_idx: 240 |  Loss: (0.4320) |  Loss2: (0.0000) | Acc: (85.00%) (26332/30848)
Epoch: 51 | Batch_idx: 250 |  Loss: (0.4317) |  Loss2: (0.0000) | Acc: (85.00%) (27424/32128)
Epoch: 51 | Batch_idx: 260 |  Loss: (0.4315) |  Loss2: (0.0000) | Acc: (85.00%) (28512/33408)
Epoch: 51 | Batch_idx: 270 |  Loss: (0.4314) |  Loss2: (0.0000) | Acc: (85.00%) (29610/34688)
Epoch: 51 | Batch_idx: 280 |  Loss: (0.4311) |  Loss2: (0.0000) | Acc: (85.00%) (30697/35968)
Epoch: 51 | Batch_idx: 290 |  Loss: (0.4303) |  Loss2: (0.0000) | Acc: (85.00%) (31796/37248)
Epoch: 51 | Batch_idx: 300 |  Loss: (0.4304) |  Loss2: (0.0000) | Acc: (85.00%) (32883/38528)
Epoch: 51 | Batch_idx: 310 |  Loss: (0.4296) |  Loss2: (0.0000) | Acc: (85.00%) (33982/39808)
Epoch: 51 | Batch_idx: 320 |  Loss: (0.4309) |  Loss2: (0.0000) | Acc: (85.00%) (35051/41088)
Epoch: 51 | Batch_idx: 330 |  Loss: (0.4297) |  Loss2: (0.0000) | Acc: (85.00%) (36167/42368)
Epoch: 51 | Batch_idx: 340 |  Loss: (0.4302) |  Loss2: (0.0000) | Acc: (85.00%) (37249/43648)
Epoch: 51 | Batch_idx: 350 |  Loss: (0.4290) |  Loss2: (0.0000) | Acc: (85.00%) (38355/44928)
Epoch: 51 | Batch_idx: 360 |  Loss: (0.4293) |  Loss2: (0.0000) | Acc: (85.00%) (39445/46208)
Epoch: 51 | Batch_idx: 370 |  Loss: (0.4288) |  Loss2: (0.0000) | Acc: (85.00%) (40537/47488)
Epoch: 51 | Batch_idx: 380 |  Loss: (0.4285) |  Loss2: (0.0000) | Acc: (85.00%) (41636/48768)
Epoch: 51 | Batch_idx: 390 |  Loss: (0.4277) |  Loss2: (0.0000) | Acc: (85.00%) (42698/50000)
# TEST : Loss: (0.5321) | Acc: (82.00%) (8265/10000)
percent tensor([0.4945, 0.5055], device='cuda:0')
percent tensor([0.4548, 0.5452], device='cuda:0')
percent tensor([0.5089, 0.4911], device='cuda:0')
percent tensor([0.5191, 0.4809], device='cuda:0')
percent tensor([0.5358, 0.4642], device='cuda:0')
percent tensor([0.6160, 0.3840], device='cuda:0')
percent tensor([0.5871, 0.4129], device='cuda:0')
percent tensor([0.9968, 0.0032], device='cuda:0')
Epoch: 52 | Batch_idx: 0 |  Loss: (0.3983) |  Loss2: (0.0000) | Acc: (85.00%) (110/128)
Epoch: 52 | Batch_idx: 10 |  Loss: (0.3725) |  Loss2: (0.0000) | Acc: (87.00%) (1238/1408)
Epoch: 52 | Batch_idx: 20 |  Loss: (0.4103) |  Loss2: (0.0000) | Acc: (86.00%) (2326/2688)
Epoch: 52 | Batch_idx: 30 |  Loss: (0.4077) |  Loss2: (0.0000) | Acc: (86.00%) (3427/3968)
Epoch: 52 | Batch_idx: 40 |  Loss: (0.4085) |  Loss2: (0.0000) | Acc: (86.00%) (4523/5248)
Epoch: 52 | Batch_idx: 50 |  Loss: (0.4060) |  Loss2: (0.0000) | Acc: (86.00%) (5640/6528)
Epoch: 52 | Batch_idx: 60 |  Loss: (0.4058) |  Loss2: (0.0000) | Acc: (86.00%) (6759/7808)
Epoch: 52 | Batch_idx: 70 |  Loss: (0.4108) |  Loss2: (0.0000) | Acc: (86.00%) (7837/9088)
Epoch: 52 | Batch_idx: 80 |  Loss: (0.4105) |  Loss2: (0.0000) | Acc: (86.00%) (8939/10368)
Epoch: 52 | Batch_idx: 90 |  Loss: (0.4090) |  Loss2: (0.0000) | Acc: (86.00%) (10044/11648)
Epoch: 52 | Batch_idx: 100 |  Loss: (0.4094) |  Loss2: (0.0000) | Acc: (86.00%) (11145/12928)
Epoch: 52 | Batch_idx: 110 |  Loss: (0.4082) |  Loss2: (0.0000) | Acc: (86.00%) (12258/14208)
Epoch: 52 | Batch_idx: 120 |  Loss: (0.4089) |  Loss2: (0.0000) | Acc: (86.00%) (13361/15488)
Epoch: 52 | Batch_idx: 130 |  Loss: (0.4112) |  Loss2: (0.0000) | Acc: (86.00%) (14444/16768)
Epoch: 52 | Batch_idx: 140 |  Loss: (0.4082) |  Loss2: (0.0000) | Acc: (86.00%) (15578/18048)
Epoch: 52 | Batch_idx: 150 |  Loss: (0.4089) |  Loss2: (0.0000) | Acc: (86.00%) (16663/19328)
Epoch: 52 | Batch_idx: 160 |  Loss: (0.4093) |  Loss2: (0.0000) | Acc: (86.00%) (17759/20608)
Epoch: 52 | Batch_idx: 170 |  Loss: (0.4124) |  Loss2: (0.0000) | Acc: (86.00%) (18838/21888)
Epoch: 52 | Batch_idx: 180 |  Loss: (0.4134) |  Loss2: (0.0000) | Acc: (86.00%) (19939/23168)
Epoch: 52 | Batch_idx: 190 |  Loss: (0.4138) |  Loss2: (0.0000) | Acc: (86.00%) (21035/24448)
Epoch: 52 | Batch_idx: 200 |  Loss: (0.4119) |  Loss2: (0.0000) | Acc: (86.00%) (22150/25728)
Epoch: 52 | Batch_idx: 210 |  Loss: (0.4125) |  Loss2: (0.0000) | Acc: (86.00%) (23253/27008)
Epoch: 52 | Batch_idx: 220 |  Loss: (0.4123) |  Loss2: (0.0000) | Acc: (86.00%) (24364/28288)
Epoch: 52 | Batch_idx: 230 |  Loss: (0.4123) |  Loss2: (0.0000) | Acc: (86.00%) (25460/29568)
Epoch: 52 | Batch_idx: 240 |  Loss: (0.4117) |  Loss2: (0.0000) | Acc: (86.00%) (26570/30848)
Epoch: 52 | Batch_idx: 250 |  Loss: (0.4121) |  Loss2: (0.0000) | Acc: (86.00%) (27670/32128)
Epoch: 52 | Batch_idx: 260 |  Loss: (0.4120) |  Loss2: (0.0000) | Acc: (86.00%) (28775/33408)
Epoch: 52 | Batch_idx: 270 |  Loss: (0.4143) |  Loss2: (0.0000) | Acc: (86.00%) (29839/34688)
Epoch: 52 | Batch_idx: 280 |  Loss: (0.4123) |  Loss2: (0.0000) | Acc: (86.00%) (30973/35968)
Epoch: 52 | Batch_idx: 290 |  Loss: (0.4126) |  Loss2: (0.0000) | Acc: (86.00%) (32075/37248)
Epoch: 52 | Batch_idx: 300 |  Loss: (0.4118) |  Loss2: (0.0000) | Acc: (86.00%) (33182/38528)
Epoch: 52 | Batch_idx: 310 |  Loss: (0.4106) |  Loss2: (0.0000) | Acc: (86.00%) (34302/39808)
Epoch: 52 | Batch_idx: 320 |  Loss: (0.4111) |  Loss2: (0.0000) | Acc: (86.00%) (35400/41088)
Epoch: 52 | Batch_idx: 330 |  Loss: (0.4111) |  Loss2: (0.0000) | Acc: (86.00%) (36496/42368)
Epoch: 52 | Batch_idx: 340 |  Loss: (0.4110) |  Loss2: (0.0000) | Acc: (86.00%) (37597/43648)
Epoch: 52 | Batch_idx: 350 |  Loss: (0.4124) |  Loss2: (0.0000) | Acc: (86.00%) (38669/44928)
Epoch: 52 | Batch_idx: 360 |  Loss: (0.4122) |  Loss2: (0.0000) | Acc: (86.00%) (39774/46208)
Epoch: 52 | Batch_idx: 370 |  Loss: (0.4121) |  Loss2: (0.0000) | Acc: (86.00%) (40875/47488)
Epoch: 52 | Batch_idx: 380 |  Loss: (0.4131) |  Loss2: (0.0000) | Acc: (86.00%) (41965/48768)
Epoch: 52 | Batch_idx: 390 |  Loss: (0.4140) |  Loss2: (0.0000) | Acc: (86.00%) (43003/50000)
# TEST : Loss: (0.5679) | Acc: (80.00%) (8070/10000)
percent tensor([0.4945, 0.5055], device='cuda:0')
percent tensor([0.4537, 0.5463], device='cuda:0')
percent tensor([0.5085, 0.4915], device='cuda:0')
percent tensor([0.5183, 0.4817], device='cuda:0')
percent tensor([0.5358, 0.4642], device='cuda:0')
percent tensor([0.6147, 0.3853], device='cuda:0')
percent tensor([0.5908, 0.4092], device='cuda:0')
percent tensor([0.9970, 0.0030], device='cuda:0')
Epoch: 53 | Batch_idx: 0 |  Loss: (0.2855) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 53 | Batch_idx: 10 |  Loss: (0.4228) |  Loss2: (0.0000) | Acc: (85.00%) (1199/1408)
Epoch: 53 | Batch_idx: 20 |  Loss: (0.4053) |  Loss2: (0.0000) | Acc: (85.00%) (2305/2688)
Epoch: 53 | Batch_idx: 30 |  Loss: (0.4060) |  Loss2: (0.0000) | Acc: (85.00%) (3404/3968)
Epoch: 53 | Batch_idx: 40 |  Loss: (0.4112) |  Loss2: (0.0000) | Acc: (85.00%) (4504/5248)
Epoch: 53 | Batch_idx: 50 |  Loss: (0.4101) |  Loss2: (0.0000) | Acc: (85.00%) (5612/6528)
Epoch: 53 | Batch_idx: 60 |  Loss: (0.4129) |  Loss2: (0.0000) | Acc: (85.00%) (6703/7808)
Epoch: 53 | Batch_idx: 70 |  Loss: (0.4143) |  Loss2: (0.0000) | Acc: (85.00%) (7796/9088)
Epoch: 53 | Batch_idx: 80 |  Loss: (0.4143) |  Loss2: (0.0000) | Acc: (85.00%) (8893/10368)
Epoch: 53 | Batch_idx: 90 |  Loss: (0.4111) |  Loss2: (0.0000) | Acc: (85.00%) (10006/11648)
Epoch: 53 | Batch_idx: 100 |  Loss: (0.4085) |  Loss2: (0.0000) | Acc: (85.00%) (11117/12928)
Epoch: 53 | Batch_idx: 110 |  Loss: (0.4090) |  Loss2: (0.0000) | Acc: (85.00%) (12211/14208)
Epoch: 53 | Batch_idx: 120 |  Loss: (0.4070) |  Loss2: (0.0000) | Acc: (85.00%) (13317/15488)
Epoch: 53 | Batch_idx: 130 |  Loss: (0.4046) |  Loss2: (0.0000) | Acc: (85.00%) (14420/16768)
Epoch: 53 | Batch_idx: 140 |  Loss: (0.4039) |  Loss2: (0.0000) | Acc: (86.00%) (15530/18048)
Epoch: 53 | Batch_idx: 150 |  Loss: (0.4035) |  Loss2: (0.0000) | Acc: (86.00%) (16636/19328)
Epoch: 53 | Batch_idx: 160 |  Loss: (0.4012) |  Loss2: (0.0000) | Acc: (86.00%) (17763/20608)
Epoch: 53 | Batch_idx: 170 |  Loss: (0.4018) |  Loss2: (0.0000) | Acc: (86.00%) (18863/21888)
Epoch: 53 | Batch_idx: 180 |  Loss: (0.4031) |  Loss2: (0.0000) | Acc: (86.00%) (19963/23168)
Epoch: 53 | Batch_idx: 190 |  Loss: (0.4023) |  Loss2: (0.0000) | Acc: (86.00%) (21067/24448)
Epoch: 53 | Batch_idx: 200 |  Loss: (0.4016) |  Loss2: (0.0000) | Acc: (86.00%) (22166/25728)
Epoch: 53 | Batch_idx: 210 |  Loss: (0.4020) |  Loss2: (0.0000) | Acc: (86.00%) (23265/27008)
Epoch: 53 | Batch_idx: 220 |  Loss: (0.4001) |  Loss2: (0.0000) | Acc: (86.00%) (24385/28288)
Epoch: 53 | Batch_idx: 230 |  Loss: (0.4006) |  Loss2: (0.0000) | Acc: (86.00%) (25480/29568)
Epoch: 53 | Batch_idx: 240 |  Loss: (0.4010) |  Loss2: (0.0000) | Acc: (86.00%) (26574/30848)
Epoch: 53 | Batch_idx: 250 |  Loss: (0.3997) |  Loss2: (0.0000) | Acc: (86.00%) (27693/32128)
Epoch: 53 | Batch_idx: 260 |  Loss: (0.4000) |  Loss2: (0.0000) | Acc: (86.00%) (28799/33408)
Epoch: 53 | Batch_idx: 270 |  Loss: (0.3992) |  Loss2: (0.0000) | Acc: (86.00%) (29917/34688)
Epoch: 53 | Batch_idx: 280 |  Loss: (0.3997) |  Loss2: (0.0000) | Acc: (86.00%) (31013/35968)
Epoch: 53 | Batch_idx: 290 |  Loss: (0.3996) |  Loss2: (0.0000) | Acc: (86.00%) (32126/37248)
Epoch: 53 | Batch_idx: 300 |  Loss: (0.4009) |  Loss2: (0.0000) | Acc: (86.00%) (33217/38528)
Epoch: 53 | Batch_idx: 310 |  Loss: (0.4018) |  Loss2: (0.0000) | Acc: (86.00%) (34303/39808)
Epoch: 53 | Batch_idx: 320 |  Loss: (0.4008) |  Loss2: (0.0000) | Acc: (86.00%) (35417/41088)
Epoch: 53 | Batch_idx: 330 |  Loss: (0.4008) |  Loss2: (0.0000) | Acc: (86.00%) (36513/42368)
Epoch: 53 | Batch_idx: 340 |  Loss: (0.4021) |  Loss2: (0.0000) | Acc: (86.00%) (37600/43648)
Epoch: 53 | Batch_idx: 350 |  Loss: (0.4031) |  Loss2: (0.0000) | Acc: (86.00%) (38687/44928)
Epoch: 53 | Batch_idx: 360 |  Loss: (0.4034) |  Loss2: (0.0000) | Acc: (86.00%) (39795/46208)
Epoch: 53 | Batch_idx: 370 |  Loss: (0.4029) |  Loss2: (0.0000) | Acc: (86.00%) (40899/47488)
Epoch: 53 | Batch_idx: 380 |  Loss: (0.4028) |  Loss2: (0.0000) | Acc: (86.00%) (42008/48768)
Epoch: 53 | Batch_idx: 390 |  Loss: (0.4033) |  Loss2: (0.0000) | Acc: (86.00%) (43053/50000)
# TEST : Loss: (0.5011) | Acc: (83.00%) (8343/10000)
percent tensor([0.4944, 0.5056], device='cuda:0')
percent tensor([0.4544, 0.5456], device='cuda:0')
percent tensor([0.5087, 0.4913], device='cuda:0')
percent tensor([0.5189, 0.4811], device='cuda:0')
percent tensor([0.5359, 0.4641], device='cuda:0')
percent tensor([0.6152, 0.3848], device='cuda:0')
percent tensor([0.5885, 0.4115], device='cuda:0')
percent tensor([0.9969, 0.0031], device='cuda:0')
Epoch: 54 | Batch_idx: 0 |  Loss: (0.5200) |  Loss2: (0.0000) | Acc: (82.00%) (105/128)
Epoch: 54 | Batch_idx: 10 |  Loss: (0.4139) |  Loss2: (0.0000) | Acc: (85.00%) (1209/1408)
Epoch: 54 | Batch_idx: 20 |  Loss: (0.3990) |  Loss2: (0.0000) | Acc: (86.00%) (2318/2688)
Epoch: 54 | Batch_idx: 30 |  Loss: (0.3886) |  Loss2: (0.0000) | Acc: (86.00%) (3447/3968)
Epoch: 54 | Batch_idx: 40 |  Loss: (0.3909) |  Loss2: (0.0000) | Acc: (86.00%) (4559/5248)
Epoch: 54 | Batch_idx: 50 |  Loss: (0.4044) |  Loss2: (0.0000) | Acc: (86.00%) (5633/6528)
Epoch: 54 | Batch_idx: 60 |  Loss: (0.3984) |  Loss2: (0.0000) | Acc: (86.00%) (6744/7808)
Epoch: 54 | Batch_idx: 70 |  Loss: (0.3965) |  Loss2: (0.0000) | Acc: (86.00%) (7846/9088)
Epoch: 54 | Batch_idx: 80 |  Loss: (0.3900) |  Loss2: (0.0000) | Acc: (86.00%) (8971/10368)
Epoch: 54 | Batch_idx: 90 |  Loss: (0.3875) |  Loss2: (0.0000) | Acc: (86.00%) (10093/11648)
Epoch: 54 | Batch_idx: 100 |  Loss: (0.3871) |  Loss2: (0.0000) | Acc: (86.00%) (11195/12928)
Epoch: 54 | Batch_idx: 110 |  Loss: (0.3841) |  Loss2: (0.0000) | Acc: (86.00%) (12320/14208)
Epoch: 54 | Batch_idx: 120 |  Loss: (0.3845) |  Loss2: (0.0000) | Acc: (86.00%) (13431/15488)
Epoch: 54 | Batch_idx: 130 |  Loss: (0.3817) |  Loss2: (0.0000) | Acc: (86.00%) (14566/16768)
Epoch: 54 | Batch_idx: 140 |  Loss: (0.3826) |  Loss2: (0.0000) | Acc: (86.00%) (15677/18048)
Epoch: 54 | Batch_idx: 150 |  Loss: (0.3828) |  Loss2: (0.0000) | Acc: (86.00%) (16785/19328)
Epoch: 54 | Batch_idx: 160 |  Loss: (0.3846) |  Loss2: (0.0000) | Acc: (86.00%) (17878/20608)
Epoch: 54 | Batch_idx: 170 |  Loss: (0.3867) |  Loss2: (0.0000) | Acc: (86.00%) (18978/21888)
Epoch: 54 | Batch_idx: 180 |  Loss: (0.3875) |  Loss2: (0.0000) | Acc: (86.00%) (20079/23168)
Epoch: 54 | Batch_idx: 190 |  Loss: (0.3872) |  Loss2: (0.0000) | Acc: (86.00%) (21195/24448)
Epoch: 54 | Batch_idx: 200 |  Loss: (0.3866) |  Loss2: (0.0000) | Acc: (86.00%) (22309/25728)
Epoch: 54 | Batch_idx: 210 |  Loss: (0.3888) |  Loss2: (0.0000) | Acc: (86.00%) (23402/27008)
Epoch: 54 | Batch_idx: 220 |  Loss: (0.3881) |  Loss2: (0.0000) | Acc: (86.00%) (24504/28288)
Epoch: 54 | Batch_idx: 230 |  Loss: (0.3900) |  Loss2: (0.0000) | Acc: (86.00%) (25600/29568)
Epoch: 54 | Batch_idx: 240 |  Loss: (0.3897) |  Loss2: (0.0000) | Acc: (86.00%) (26709/30848)
Epoch: 54 | Batch_idx: 250 |  Loss: (0.3915) |  Loss2: (0.0000) | Acc: (86.00%) (27788/32128)
Epoch: 54 | Batch_idx: 260 |  Loss: (0.3911) |  Loss2: (0.0000) | Acc: (86.00%) (28898/33408)
Epoch: 54 | Batch_idx: 270 |  Loss: (0.3893) |  Loss2: (0.0000) | Acc: (86.00%) (30030/34688)
Epoch: 54 | Batch_idx: 280 |  Loss: (0.3890) |  Loss2: (0.0000) | Acc: (86.00%) (31149/35968)
Epoch: 54 | Batch_idx: 290 |  Loss: (0.3888) |  Loss2: (0.0000) | Acc: (86.00%) (32255/37248)
Epoch: 54 | Batch_idx: 300 |  Loss: (0.3881) |  Loss2: (0.0000) | Acc: (86.00%) (33369/38528)
Epoch: 54 | Batch_idx: 310 |  Loss: (0.3873) |  Loss2: (0.0000) | Acc: (86.00%) (34489/39808)
Epoch: 54 | Batch_idx: 320 |  Loss: (0.3880) |  Loss2: (0.0000) | Acc: (86.00%) (35585/41088)
Epoch: 54 | Batch_idx: 330 |  Loss: (0.3883) |  Loss2: (0.0000) | Acc: (86.00%) (36699/42368)
Epoch: 54 | Batch_idx: 340 |  Loss: (0.3894) |  Loss2: (0.0000) | Acc: (86.00%) (37801/43648)
Epoch: 54 | Batch_idx: 350 |  Loss: (0.3896) |  Loss2: (0.0000) | Acc: (86.00%) (38905/44928)
Epoch: 54 | Batch_idx: 360 |  Loss: (0.3902) |  Loss2: (0.0000) | Acc: (86.00%) (40001/46208)
Epoch: 54 | Batch_idx: 370 |  Loss: (0.3907) |  Loss2: (0.0000) | Acc: (86.00%) (41102/47488)
Epoch: 54 | Batch_idx: 380 |  Loss: (0.3910) |  Loss2: (0.0000) | Acc: (86.00%) (42220/48768)
Epoch: 54 | Batch_idx: 390 |  Loss: (0.3907) |  Loss2: (0.0000) | Acc: (86.00%) (43284/50000)
# TEST : Loss: (0.5580) | Acc: (81.00%) (8192/10000)
percent tensor([0.4944, 0.5056], device='cuda:0')
percent tensor([0.4543, 0.5457], device='cuda:0')
percent tensor([0.5088, 0.4912], device='cuda:0')
percent tensor([0.5192, 0.4808], device='cuda:0')
percent tensor([0.5363, 0.4637], device='cuda:0')
percent tensor([0.6153, 0.3847], device='cuda:0')
percent tensor([0.5890, 0.4110], device='cuda:0')
percent tensor([0.9970, 0.0030], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 55 | Batch_idx: 0 |  Loss: (0.2117) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 55 | Batch_idx: 10 |  Loss: (0.3902) |  Loss2: (0.0000) | Acc: (86.00%) (1222/1408)
Epoch: 55 | Batch_idx: 20 |  Loss: (0.3976) |  Loss2: (0.0000) | Acc: (86.00%) (2315/2688)
Epoch: 55 | Batch_idx: 30 |  Loss: (0.4079) |  Loss2: (0.0000) | Acc: (85.00%) (3399/3968)
Epoch: 55 | Batch_idx: 40 |  Loss: (0.4275) |  Loss2: (0.0000) | Acc: (85.00%) (4466/5248)
Epoch: 55 | Batch_idx: 50 |  Loss: (0.4372) |  Loss2: (0.0000) | Acc: (84.00%) (5533/6528)
Epoch: 55 | Batch_idx: 60 |  Loss: (0.4394) |  Loss2: (0.0000) | Acc: (84.00%) (6601/7808)
Epoch: 55 | Batch_idx: 70 |  Loss: (0.4402) |  Loss2: (0.0000) | Acc: (84.00%) (7672/9088)
Epoch: 55 | Batch_idx: 80 |  Loss: (0.4441) |  Loss2: (0.0000) | Acc: (84.00%) (8737/10368)
Epoch: 55 | Batch_idx: 90 |  Loss: (0.4421) |  Loss2: (0.0000) | Acc: (84.00%) (9831/11648)
Epoch: 55 | Batch_idx: 100 |  Loss: (0.4425) |  Loss2: (0.0000) | Acc: (84.00%) (10922/12928)
Epoch: 55 | Batch_idx: 110 |  Loss: (0.4445) |  Loss2: (0.0000) | Acc: (84.00%) (11987/14208)
Epoch: 55 | Batch_idx: 120 |  Loss: (0.4459) |  Loss2: (0.0000) | Acc: (84.00%) (13067/15488)
Epoch: 55 | Batch_idx: 130 |  Loss: (0.4486) |  Loss2: (0.0000) | Acc: (84.00%) (14138/16768)
Epoch: 55 | Batch_idx: 140 |  Loss: (0.4489) |  Loss2: (0.0000) | Acc: (84.00%) (15228/18048)
Epoch: 55 | Batch_idx: 150 |  Loss: (0.4477) |  Loss2: (0.0000) | Acc: (84.00%) (16310/19328)
Epoch: 55 | Batch_idx: 160 |  Loss: (0.4439) |  Loss2: (0.0000) | Acc: (84.00%) (17428/20608)
Epoch: 55 | Batch_idx: 170 |  Loss: (0.4447) |  Loss2: (0.0000) | Acc: (84.00%) (18518/21888)
Epoch: 55 | Batch_idx: 180 |  Loss: (0.4434) |  Loss2: (0.0000) | Acc: (84.00%) (19597/23168)
Epoch: 55 | Batch_idx: 190 |  Loss: (0.4438) |  Loss2: (0.0000) | Acc: (84.00%) (20683/24448)
Epoch: 55 | Batch_idx: 200 |  Loss: (0.4420) |  Loss2: (0.0000) | Acc: (84.00%) (21793/25728)
Epoch: 55 | Batch_idx: 210 |  Loss: (0.4424) |  Loss2: (0.0000) | Acc: (84.00%) (22871/27008)
Epoch: 55 | Batch_idx: 220 |  Loss: (0.4432) |  Loss2: (0.0000) | Acc: (84.00%) (23949/28288)
Epoch: 55 | Batch_idx: 230 |  Loss: (0.4421) |  Loss2: (0.0000) | Acc: (84.00%) (25043/29568)
Epoch: 55 | Batch_idx: 240 |  Loss: (0.4408) |  Loss2: (0.0000) | Acc: (84.00%) (26140/30848)
Epoch: 55 | Batch_idx: 250 |  Loss: (0.4420) |  Loss2: (0.0000) | Acc: (84.00%) (27211/32128)
Epoch: 55 | Batch_idx: 260 |  Loss: (0.4420) |  Loss2: (0.0000) | Acc: (84.00%) (28303/33408)
Epoch: 55 | Batch_idx: 270 |  Loss: (0.4419) |  Loss2: (0.0000) | Acc: (84.00%) (29383/34688)
Epoch: 55 | Batch_idx: 280 |  Loss: (0.4416) |  Loss2: (0.0000) | Acc: (84.00%) (30467/35968)
Epoch: 55 | Batch_idx: 290 |  Loss: (0.4413) |  Loss2: (0.0000) | Acc: (84.00%) (31559/37248)
Epoch: 55 | Batch_idx: 300 |  Loss: (0.4427) |  Loss2: (0.0000) | Acc: (84.00%) (32616/38528)
Epoch: 55 | Batch_idx: 310 |  Loss: (0.4414) |  Loss2: (0.0000) | Acc: (84.00%) (33716/39808)
Epoch: 55 | Batch_idx: 320 |  Loss: (0.4414) |  Loss2: (0.0000) | Acc: (84.00%) (34797/41088)
Epoch: 55 | Batch_idx: 330 |  Loss: (0.4412) |  Loss2: (0.0000) | Acc: (84.00%) (35894/42368)
Epoch: 55 | Batch_idx: 340 |  Loss: (0.4404) |  Loss2: (0.0000) | Acc: (84.00%) (37001/43648)
Epoch: 55 | Batch_idx: 350 |  Loss: (0.4403) |  Loss2: (0.0000) | Acc: (84.00%) (38084/44928)
Epoch: 55 | Batch_idx: 360 |  Loss: (0.4392) |  Loss2: (0.0000) | Acc: (84.00%) (39178/46208)
Epoch: 55 | Batch_idx: 370 |  Loss: (0.4392) |  Loss2: (0.0000) | Acc: (84.00%) (40244/47488)
Epoch: 55 | Batch_idx: 380 |  Loss: (0.4384) |  Loss2: (0.0000) | Acc: (84.00%) (41336/48768)
Epoch: 55 | Batch_idx: 390 |  Loss: (0.4392) |  Loss2: (0.0000) | Acc: (84.00%) (42370/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_055.pth.tar'
# TEST : Loss: (0.5007) | Acc: (82.00%) (8292/10000)
percent tensor([0.5002, 0.4998], device='cuda:0')
percent tensor([0.4551, 0.5449], device='cuda:0')
percent tensor([0.5071, 0.4929], device='cuda:0')
percent tensor([0.5284, 0.4716], device='cuda:0')
percent tensor([0.5295, 0.4705], device='cuda:0')
percent tensor([0.6170, 0.3830], device='cuda:0')
percent tensor([0.5502, 0.4498], device='cuda:0')
percent tensor([0.9970, 0.0030], device='cuda:0')
Epoch: 56 | Batch_idx: 0 |  Loss: (0.4489) |  Loss2: (0.0000) | Acc: (84.00%) (108/128)
Epoch: 56 | Batch_idx: 10 |  Loss: (0.4389) |  Loss2: (0.0000) | Acc: (84.00%) (1192/1408)
Epoch: 56 | Batch_idx: 20 |  Loss: (0.4304) |  Loss2: (0.0000) | Acc: (84.00%) (2269/2688)
Epoch: 56 | Batch_idx: 30 |  Loss: (0.4153) |  Loss2: (0.0000) | Acc: (85.00%) (3376/3968)
Epoch: 56 | Batch_idx: 40 |  Loss: (0.4059) |  Loss2: (0.0000) | Acc: (85.00%) (4488/5248)
Epoch: 56 | Batch_idx: 50 |  Loss: (0.4086) |  Loss2: (0.0000) | Acc: (85.00%) (5581/6528)
Epoch: 56 | Batch_idx: 60 |  Loss: (0.4131) |  Loss2: (0.0000) | Acc: (85.00%) (6667/7808)
Epoch: 56 | Batch_idx: 70 |  Loss: (0.4169) |  Loss2: (0.0000) | Acc: (85.00%) (7749/9088)
Epoch: 56 | Batch_idx: 80 |  Loss: (0.4211) |  Loss2: (0.0000) | Acc: (85.00%) (8818/10368)
Epoch: 56 | Batch_idx: 90 |  Loss: (0.4173) |  Loss2: (0.0000) | Acc: (85.00%) (9915/11648)
Epoch: 56 | Batch_idx: 100 |  Loss: (0.4201) |  Loss2: (0.0000) | Acc: (85.00%) (10993/12928)
Epoch: 56 | Batch_idx: 110 |  Loss: (0.4185) |  Loss2: (0.0000) | Acc: (85.00%) (12095/14208)
Epoch: 56 | Batch_idx: 120 |  Loss: (0.4196) |  Loss2: (0.0000) | Acc: (85.00%) (13188/15488)
Epoch: 56 | Batch_idx: 130 |  Loss: (0.4207) |  Loss2: (0.0000) | Acc: (85.00%) (14282/16768)
Epoch: 56 | Batch_idx: 140 |  Loss: (0.4211) |  Loss2: (0.0000) | Acc: (85.00%) (15384/18048)
Epoch: 56 | Batch_idx: 150 |  Loss: (0.4201) |  Loss2: (0.0000) | Acc: (85.00%) (16485/19328)
Epoch: 56 | Batch_idx: 160 |  Loss: (0.4179) |  Loss2: (0.0000) | Acc: (85.00%) (17580/20608)
Epoch: 56 | Batch_idx: 170 |  Loss: (0.4183) |  Loss2: (0.0000) | Acc: (85.00%) (18668/21888)
Epoch: 56 | Batch_idx: 180 |  Loss: (0.4165) |  Loss2: (0.0000) | Acc: (85.00%) (19769/23168)
Epoch: 56 | Batch_idx: 190 |  Loss: (0.4157) |  Loss2: (0.0000) | Acc: (85.00%) (20879/24448)
Epoch: 56 | Batch_idx: 200 |  Loss: (0.4130) |  Loss2: (0.0000) | Acc: (85.00%) (21998/25728)
Epoch: 56 | Batch_idx: 210 |  Loss: (0.4136) |  Loss2: (0.0000) | Acc: (85.00%) (23088/27008)
Epoch: 56 | Batch_idx: 220 |  Loss: (0.4131) |  Loss2: (0.0000) | Acc: (85.00%) (24209/28288)
Epoch: 56 | Batch_idx: 230 |  Loss: (0.4132) |  Loss2: (0.0000) | Acc: (85.00%) (25323/29568)
Epoch: 56 | Batch_idx: 240 |  Loss: (0.4139) |  Loss2: (0.0000) | Acc: (85.00%) (26422/30848)
Epoch: 56 | Batch_idx: 250 |  Loss: (0.4131) |  Loss2: (0.0000) | Acc: (85.00%) (27536/32128)
Epoch: 56 | Batch_idx: 260 |  Loss: (0.4136) |  Loss2: (0.0000) | Acc: (85.00%) (28624/33408)
Epoch: 56 | Batch_idx: 270 |  Loss: (0.4134) |  Loss2: (0.0000) | Acc: (85.00%) (29725/34688)
Epoch: 56 | Batch_idx: 280 |  Loss: (0.4137) |  Loss2: (0.0000) | Acc: (85.00%) (30821/35968)
Epoch: 56 | Batch_idx: 290 |  Loss: (0.4136) |  Loss2: (0.0000) | Acc: (85.00%) (31920/37248)
Epoch: 56 | Batch_idx: 300 |  Loss: (0.4129) |  Loss2: (0.0000) | Acc: (85.00%) (33031/38528)
Epoch: 56 | Batch_idx: 310 |  Loss: (0.4113) |  Loss2: (0.0000) | Acc: (85.00%) (34158/39808)
Epoch: 56 | Batch_idx: 320 |  Loss: (0.4111) |  Loss2: (0.0000) | Acc: (85.00%) (35261/41088)
Epoch: 56 | Batch_idx: 330 |  Loss: (0.4108) |  Loss2: (0.0000) | Acc: (85.00%) (36366/42368)
Epoch: 56 | Batch_idx: 340 |  Loss: (0.4112) |  Loss2: (0.0000) | Acc: (85.00%) (37463/43648)
Epoch: 56 | Batch_idx: 350 |  Loss: (0.4111) |  Loss2: (0.0000) | Acc: (85.00%) (38555/44928)
Epoch: 56 | Batch_idx: 360 |  Loss: (0.4113) |  Loss2: (0.0000) | Acc: (85.00%) (39646/46208)
Epoch: 56 | Batch_idx: 370 |  Loss: (0.4101) |  Loss2: (0.0000) | Acc: (85.00%) (40782/47488)
Epoch: 56 | Batch_idx: 380 |  Loss: (0.4091) |  Loss2: (0.0000) | Acc: (85.00%) (41905/48768)
Epoch: 56 | Batch_idx: 390 |  Loss: (0.4075) |  Loss2: (0.0000) | Acc: (85.00%) (42998/50000)
# TEST : Loss: (0.4765) | Acc: (83.00%) (8370/10000)
percent tensor([0.5020, 0.4980], device='cuda:0')
percent tensor([0.4553, 0.5447], device='cuda:0')
percent tensor([0.5109, 0.4891], device='cuda:0')
percent tensor([0.5395, 0.4605], device='cuda:0')
percent tensor([0.5354, 0.4646], device='cuda:0')
percent tensor([0.6195, 0.3805], device='cuda:0')
percent tensor([0.5532, 0.4468], device='cuda:0')
percent tensor([0.9971, 0.0029], device='cuda:0')
Epoch: 57 | Batch_idx: 0 |  Loss: (0.4383) |  Loss2: (0.0000) | Acc: (86.00%) (111/128)
Epoch: 57 | Batch_idx: 10 |  Loss: (0.3890) |  Loss2: (0.0000) | Acc: (87.00%) (1225/1408)
Epoch: 57 | Batch_idx: 20 |  Loss: (0.3905) |  Loss2: (0.0000) | Acc: (86.00%) (2328/2688)
Epoch: 57 | Batch_idx: 30 |  Loss: (0.3975) |  Loss2: (0.0000) | Acc: (86.00%) (3435/3968)
Epoch: 57 | Batch_idx: 40 |  Loss: (0.3939) |  Loss2: (0.0000) | Acc: (86.00%) (4541/5248)
Epoch: 57 | Batch_idx: 50 |  Loss: (0.3945) |  Loss2: (0.0000) | Acc: (86.00%) (5648/6528)
Epoch: 57 | Batch_idx: 60 |  Loss: (0.4013) |  Loss2: (0.0000) | Acc: (86.00%) (6728/7808)
Epoch: 57 | Batch_idx: 70 |  Loss: (0.3999) |  Loss2: (0.0000) | Acc: (86.00%) (7845/9088)
Epoch: 57 | Batch_idx: 80 |  Loss: (0.4051) |  Loss2: (0.0000) | Acc: (86.00%) (8935/10368)
Epoch: 57 | Batch_idx: 90 |  Loss: (0.4039) |  Loss2: (0.0000) | Acc: (86.00%) (10050/11648)
Epoch: 57 | Batch_idx: 100 |  Loss: (0.4055) |  Loss2: (0.0000) | Acc: (86.00%) (11137/12928)
Epoch: 57 | Batch_idx: 110 |  Loss: (0.4062) |  Loss2: (0.0000) | Acc: (86.00%) (12246/14208)
Epoch: 57 | Batch_idx: 120 |  Loss: (0.4073) |  Loss2: (0.0000) | Acc: (86.00%) (13339/15488)
Epoch: 57 | Batch_idx: 130 |  Loss: (0.4068) |  Loss2: (0.0000) | Acc: (86.00%) (14429/16768)
Epoch: 57 | Batch_idx: 140 |  Loss: (0.4088) |  Loss2: (0.0000) | Acc: (85.00%) (15506/18048)
Epoch: 57 | Batch_idx: 150 |  Loss: (0.4070) |  Loss2: (0.0000) | Acc: (85.00%) (16618/19328)
Epoch: 57 | Batch_idx: 160 |  Loss: (0.4068) |  Loss2: (0.0000) | Acc: (85.00%) (17715/20608)
Epoch: 57 | Batch_idx: 170 |  Loss: (0.4038) |  Loss2: (0.0000) | Acc: (86.00%) (18840/21888)
Epoch: 57 | Batch_idx: 180 |  Loss: (0.4036) |  Loss2: (0.0000) | Acc: (86.00%) (19931/23168)
Epoch: 57 | Batch_idx: 190 |  Loss: (0.4022) |  Loss2: (0.0000) | Acc: (86.00%) (21050/24448)
Epoch: 57 | Batch_idx: 200 |  Loss: (0.4024) |  Loss2: (0.0000) | Acc: (86.00%) (22150/25728)
Epoch: 57 | Batch_idx: 210 |  Loss: (0.4028) |  Loss2: (0.0000) | Acc: (86.00%) (23250/27008)
Epoch: 57 | Batch_idx: 220 |  Loss: (0.4015) |  Loss2: (0.0000) | Acc: (86.00%) (24360/28288)
Epoch: 57 | Batch_idx: 230 |  Loss: (0.4001) |  Loss2: (0.0000) | Acc: (86.00%) (25486/29568)
Epoch: 57 | Batch_idx: 240 |  Loss: (0.4015) |  Loss2: (0.0000) | Acc: (86.00%) (26579/30848)
Epoch: 57 | Batch_idx: 250 |  Loss: (0.3992) |  Loss2: (0.0000) | Acc: (86.00%) (27714/32128)
Epoch: 57 | Batch_idx: 260 |  Loss: (0.3977) |  Loss2: (0.0000) | Acc: (86.00%) (28833/33408)
Epoch: 57 | Batch_idx: 270 |  Loss: (0.3977) |  Loss2: (0.0000) | Acc: (86.00%) (29934/34688)
Epoch: 57 | Batch_idx: 280 |  Loss: (0.3952) |  Loss2: (0.0000) | Acc: (86.00%) (31076/35968)
Epoch: 57 | Batch_idx: 290 |  Loss: (0.3957) |  Loss2: (0.0000) | Acc: (86.00%) (32167/37248)
Epoch: 57 | Batch_idx: 300 |  Loss: (0.3963) |  Loss2: (0.0000) | Acc: (86.00%) (33274/38528)
Epoch: 57 | Batch_idx: 310 |  Loss: (0.3973) |  Loss2: (0.0000) | Acc: (86.00%) (34344/39808)
Epoch: 57 | Batch_idx: 320 |  Loss: (0.3963) |  Loss2: (0.0000) | Acc: (86.00%) (35474/41088)
Epoch: 57 | Batch_idx: 330 |  Loss: (0.3956) |  Loss2: (0.0000) | Acc: (86.00%) (36576/42368)
Epoch: 57 | Batch_idx: 340 |  Loss: (0.3949) |  Loss2: (0.0000) | Acc: (86.00%) (37693/43648)
Epoch: 57 | Batch_idx: 350 |  Loss: (0.3954) |  Loss2: (0.0000) | Acc: (86.00%) (38792/44928)
Epoch: 57 | Batch_idx: 360 |  Loss: (0.3959) |  Loss2: (0.0000) | Acc: (86.00%) (39890/46208)
Epoch: 57 | Batch_idx: 370 |  Loss: (0.3965) |  Loss2: (0.0000) | Acc: (86.00%) (40976/47488)
Epoch: 57 | Batch_idx: 380 |  Loss: (0.3958) |  Loss2: (0.0000) | Acc: (86.00%) (42108/48768)
Epoch: 57 | Batch_idx: 390 |  Loss: (0.3962) |  Loss2: (0.0000) | Acc: (86.00%) (43146/50000)
# TEST : Loss: (0.4649) | Acc: (84.00%) (8421/10000)
percent tensor([0.5030, 0.4970], device='cuda:0')
percent tensor([0.4531, 0.5469], device='cuda:0')
percent tensor([0.5137, 0.4863], device='cuda:0')
percent tensor([0.5416, 0.4584], device='cuda:0')
percent tensor([0.5363, 0.4637], device='cuda:0')
percent tensor([0.6192, 0.3808], device='cuda:0')
percent tensor([0.5599, 0.4401], device='cuda:0')
percent tensor([0.9973, 0.0027], device='cuda:0')
Epoch: 58 | Batch_idx: 0 |  Loss: (0.4392) |  Loss2: (0.0000) | Acc: (83.00%) (107/128)
Epoch: 58 | Batch_idx: 10 |  Loss: (0.3749) |  Loss2: (0.0000) | Acc: (87.00%) (1226/1408)
Epoch: 58 | Batch_idx: 20 |  Loss: (0.3743) |  Loss2: (0.0000) | Acc: (86.00%) (2336/2688)
Epoch: 58 | Batch_idx: 30 |  Loss: (0.3915) |  Loss2: (0.0000) | Acc: (86.00%) (3426/3968)
Epoch: 58 | Batch_idx: 40 |  Loss: (0.3905) |  Loss2: (0.0000) | Acc: (86.00%) (4536/5248)
Epoch: 58 | Batch_idx: 50 |  Loss: (0.3882) |  Loss2: (0.0000) | Acc: (86.00%) (5635/6528)
Epoch: 58 | Batch_idx: 60 |  Loss: (0.3842) |  Loss2: (0.0000) | Acc: (86.00%) (6747/7808)
Epoch: 58 | Batch_idx: 70 |  Loss: (0.3846) |  Loss2: (0.0000) | Acc: (86.00%) (7854/9088)
Epoch: 58 | Batch_idx: 80 |  Loss: (0.3877) |  Loss2: (0.0000) | Acc: (86.00%) (8956/10368)
Epoch: 58 | Batch_idx: 90 |  Loss: (0.3845) |  Loss2: (0.0000) | Acc: (86.00%) (10079/11648)
Epoch: 58 | Batch_idx: 100 |  Loss: (0.3870) |  Loss2: (0.0000) | Acc: (86.00%) (11182/12928)
Epoch: 58 | Batch_idx: 110 |  Loss: (0.3848) |  Loss2: (0.0000) | Acc: (86.00%) (12307/14208)
Epoch: 58 | Batch_idx: 120 |  Loss: (0.3845) |  Loss2: (0.0000) | Acc: (86.00%) (13429/15488)
Epoch: 58 | Batch_idx: 130 |  Loss: (0.3830) |  Loss2: (0.0000) | Acc: (86.00%) (14553/16768)
Epoch: 58 | Batch_idx: 140 |  Loss: (0.3829) |  Loss2: (0.0000) | Acc: (86.00%) (15677/18048)
Epoch: 58 | Batch_idx: 150 |  Loss: (0.3853) |  Loss2: (0.0000) | Acc: (86.00%) (16774/19328)
Epoch: 58 | Batch_idx: 160 |  Loss: (0.3842) |  Loss2: (0.0000) | Acc: (86.00%) (17907/20608)
Epoch: 58 | Batch_idx: 170 |  Loss: (0.3859) |  Loss2: (0.0000) | Acc: (86.00%) (18998/21888)
Epoch: 58 | Batch_idx: 180 |  Loss: (0.3851) |  Loss2: (0.0000) | Acc: (86.00%) (20100/23168)
Epoch: 58 | Batch_idx: 190 |  Loss: (0.3836) |  Loss2: (0.0000) | Acc: (86.00%) (21220/24448)
Epoch: 58 | Batch_idx: 200 |  Loss: (0.3826) |  Loss2: (0.0000) | Acc: (86.00%) (22351/25728)
Epoch: 58 | Batch_idx: 210 |  Loss: (0.3819) |  Loss2: (0.0000) | Acc: (86.00%) (23469/27008)
Epoch: 58 | Batch_idx: 220 |  Loss: (0.3821) |  Loss2: (0.0000) | Acc: (86.00%) (24575/28288)
Epoch: 58 | Batch_idx: 230 |  Loss: (0.3828) |  Loss2: (0.0000) | Acc: (86.00%) (25665/29568)
Epoch: 58 | Batch_idx: 240 |  Loss: (0.3834) |  Loss2: (0.0000) | Acc: (86.00%) (26773/30848)
Epoch: 58 | Batch_idx: 250 |  Loss: (0.3834) |  Loss2: (0.0000) | Acc: (86.00%) (27886/32128)
Epoch: 58 | Batch_idx: 260 |  Loss: (0.3825) |  Loss2: (0.0000) | Acc: (86.00%) (29021/33408)
Epoch: 58 | Batch_idx: 270 |  Loss: (0.3820) |  Loss2: (0.0000) | Acc: (86.00%) (30145/34688)
Epoch: 58 | Batch_idx: 280 |  Loss: (0.3826) |  Loss2: (0.0000) | Acc: (86.00%) (31245/35968)
Epoch: 58 | Batch_idx: 290 |  Loss: (0.3831) |  Loss2: (0.0000) | Acc: (86.00%) (32349/37248)
Epoch: 58 | Batch_idx: 300 |  Loss: (0.3824) |  Loss2: (0.0000) | Acc: (86.00%) (33466/38528)
Epoch: 58 | Batch_idx: 310 |  Loss: (0.3825) |  Loss2: (0.0000) | Acc: (86.00%) (34596/39808)
Epoch: 58 | Batch_idx: 320 |  Loss: (0.3828) |  Loss2: (0.0000) | Acc: (86.00%) (35707/41088)
Epoch: 58 | Batch_idx: 330 |  Loss: (0.3823) |  Loss2: (0.0000) | Acc: (86.00%) (36827/42368)
Epoch: 58 | Batch_idx: 340 |  Loss: (0.3820) |  Loss2: (0.0000) | Acc: (86.00%) (37950/43648)
Epoch: 58 | Batch_idx: 350 |  Loss: (0.3824) |  Loss2: (0.0000) | Acc: (86.00%) (39055/44928)
Epoch: 58 | Batch_idx: 360 |  Loss: (0.3827) |  Loss2: (0.0000) | Acc: (86.00%) (40165/46208)
Epoch: 58 | Batch_idx: 370 |  Loss: (0.3832) |  Loss2: (0.0000) | Acc: (86.00%) (41258/47488)
Epoch: 58 | Batch_idx: 380 |  Loss: (0.3838) |  Loss2: (0.0000) | Acc: (86.00%) (42349/48768)
Epoch: 58 | Batch_idx: 390 |  Loss: (0.3837) |  Loss2: (0.0000) | Acc: (86.00%) (43419/50000)
# TEST : Loss: (0.4588) | Acc: (84.00%) (8443/10000)
percent tensor([0.5041, 0.4959], device='cuda:0')
percent tensor([0.4551, 0.5449], device='cuda:0')
percent tensor([0.5166, 0.4834], device='cuda:0')
percent tensor([0.5431, 0.4569], device='cuda:0')
percent tensor([0.5377, 0.4623], device='cuda:0')
percent tensor([0.6233, 0.3767], device='cuda:0')
percent tensor([0.5706, 0.4294], device='cuda:0')
percent tensor([0.9976, 0.0024], device='cuda:0')
Epoch: 59 | Batch_idx: 0 |  Loss: (0.4107) |  Loss2: (0.0000) | Acc: (83.00%) (107/128)
Epoch: 59 | Batch_idx: 10 |  Loss: (0.3614) |  Loss2: (0.0000) | Acc: (87.00%) (1226/1408)
Epoch: 59 | Batch_idx: 20 |  Loss: (0.3658) |  Loss2: (0.0000) | Acc: (87.00%) (2346/2688)
Epoch: 59 | Batch_idx: 30 |  Loss: (0.3791) |  Loss2: (0.0000) | Acc: (86.00%) (3437/3968)
Epoch: 59 | Batch_idx: 40 |  Loss: (0.3860) |  Loss2: (0.0000) | Acc: (86.00%) (4527/5248)
Epoch: 59 | Batch_idx: 50 |  Loss: (0.3855) |  Loss2: (0.0000) | Acc: (86.00%) (5622/6528)
Epoch: 59 | Batch_idx: 60 |  Loss: (0.3887) |  Loss2: (0.0000) | Acc: (86.00%) (6719/7808)
Epoch: 59 | Batch_idx: 70 |  Loss: (0.3855) |  Loss2: (0.0000) | Acc: (86.00%) (7845/9088)
Epoch: 59 | Batch_idx: 80 |  Loss: (0.3857) |  Loss2: (0.0000) | Acc: (86.00%) (8958/10368)
Epoch: 59 | Batch_idx: 90 |  Loss: (0.3830) |  Loss2: (0.0000) | Acc: (86.00%) (10083/11648)
Epoch: 59 | Batch_idx: 100 |  Loss: (0.3840) |  Loss2: (0.0000) | Acc: (86.00%) (11191/12928)
Epoch: 59 | Batch_idx: 110 |  Loss: (0.3863) |  Loss2: (0.0000) | Acc: (86.00%) (12307/14208)
Epoch: 59 | Batch_idx: 120 |  Loss: (0.3864) |  Loss2: (0.0000) | Acc: (86.00%) (13416/15488)
Epoch: 59 | Batch_idx: 130 |  Loss: (0.3843) |  Loss2: (0.0000) | Acc: (86.00%) (14542/16768)
Epoch: 59 | Batch_idx: 140 |  Loss: (0.3849) |  Loss2: (0.0000) | Acc: (86.00%) (15648/18048)
Epoch: 59 | Batch_idx: 150 |  Loss: (0.3859) |  Loss2: (0.0000) | Acc: (86.00%) (16754/19328)
Epoch: 59 | Batch_idx: 160 |  Loss: (0.3848) |  Loss2: (0.0000) | Acc: (86.00%) (17876/20608)
Epoch: 59 | Batch_idx: 170 |  Loss: (0.3840) |  Loss2: (0.0000) | Acc: (86.00%) (18987/21888)
Epoch: 59 | Batch_idx: 180 |  Loss: (0.3834) |  Loss2: (0.0000) | Acc: (86.00%) (20114/23168)
Epoch: 59 | Batch_idx: 190 |  Loss: (0.3832) |  Loss2: (0.0000) | Acc: (86.00%) (21222/24448)
Epoch: 59 | Batch_idx: 200 |  Loss: (0.3837) |  Loss2: (0.0000) | Acc: (86.00%) (22338/25728)
Epoch: 59 | Batch_idx: 210 |  Loss: (0.3835) |  Loss2: (0.0000) | Acc: (86.00%) (23448/27008)
Epoch: 59 | Batch_idx: 220 |  Loss: (0.3820) |  Loss2: (0.0000) | Acc: (86.00%) (24578/28288)
Epoch: 59 | Batch_idx: 230 |  Loss: (0.3822) |  Loss2: (0.0000) | Acc: (86.00%) (25685/29568)
Epoch: 59 | Batch_idx: 240 |  Loss: (0.3833) |  Loss2: (0.0000) | Acc: (86.00%) (26781/30848)
Epoch: 59 | Batch_idx: 250 |  Loss: (0.3818) |  Loss2: (0.0000) | Acc: (86.00%) (27902/32128)
Epoch: 59 | Batch_idx: 260 |  Loss: (0.3803) |  Loss2: (0.0000) | Acc: (86.00%) (29025/33408)
Epoch: 59 | Batch_idx: 270 |  Loss: (0.3805) |  Loss2: (0.0000) | Acc: (86.00%) (30140/34688)
Epoch: 59 | Batch_idx: 280 |  Loss: (0.3803) |  Loss2: (0.0000) | Acc: (86.00%) (31258/35968)
Epoch: 59 | Batch_idx: 290 |  Loss: (0.3806) |  Loss2: (0.0000) | Acc: (86.00%) (32365/37248)
Epoch: 59 | Batch_idx: 300 |  Loss: (0.3804) |  Loss2: (0.0000) | Acc: (86.00%) (33466/38528)
Epoch: 59 | Batch_idx: 310 |  Loss: (0.3808) |  Loss2: (0.0000) | Acc: (86.00%) (34556/39808)
Epoch: 59 | Batch_idx: 320 |  Loss: (0.3814) |  Loss2: (0.0000) | Acc: (86.00%) (35667/41088)
Epoch: 59 | Batch_idx: 330 |  Loss: (0.3820) |  Loss2: (0.0000) | Acc: (86.00%) (36754/42368)
Epoch: 59 | Batch_idx: 340 |  Loss: (0.3817) |  Loss2: (0.0000) | Acc: (86.00%) (37874/43648)
Epoch: 59 | Batch_idx: 350 |  Loss: (0.3819) |  Loss2: (0.0000) | Acc: (86.00%) (38982/44928)
Epoch: 59 | Batch_idx: 360 |  Loss: (0.3817) |  Loss2: (0.0000) | Acc: (86.00%) (40087/46208)
Epoch: 59 | Batch_idx: 370 |  Loss: (0.3814) |  Loss2: (0.0000) | Acc: (86.00%) (41208/47488)
Epoch: 59 | Batch_idx: 380 |  Loss: (0.3810) |  Loss2: (0.0000) | Acc: (86.00%) (42327/48768)
Epoch: 59 | Batch_idx: 390 |  Loss: (0.3810) |  Loss2: (0.0000) | Acc: (86.00%) (43397/50000)
# TEST : Loss: (0.4505) | Acc: (84.00%) (8465/10000)
percent tensor([0.5042, 0.4958], device='cuda:0')
percent tensor([0.4523, 0.5477], device='cuda:0')
percent tensor([0.5170, 0.4830], device='cuda:0')
percent tensor([0.5435, 0.4565], device='cuda:0')
percent tensor([0.5413, 0.4587], device='cuda:0')
percent tensor([0.6275, 0.3725], device='cuda:0')
percent tensor([0.5810, 0.4190], device='cuda:0')
percent tensor([0.9977, 0.0023], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 60 | Batch_idx: 0 |  Loss: (0.3667) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 60 | Batch_idx: 10 |  Loss: (0.3634) |  Loss2: (0.0000) | Acc: (88.00%) (1242/1408)
Epoch: 60 | Batch_idx: 20 |  Loss: (0.3807) |  Loss2: (0.0000) | Acc: (87.00%) (2350/2688)
Epoch: 60 | Batch_idx: 30 |  Loss: (0.3876) |  Loss2: (0.0000) | Acc: (87.00%) (3464/3968)
Epoch: 60 | Batch_idx: 40 |  Loss: (0.3879) |  Loss2: (0.0000) | Acc: (87.00%) (4574/5248)
Epoch: 60 | Batch_idx: 50 |  Loss: (0.3893) |  Loss2: (0.0000) | Acc: (87.00%) (5685/6528)
Epoch: 60 | Batch_idx: 60 |  Loss: (0.3888) |  Loss2: (0.0000) | Acc: (86.00%) (6792/7808)
Epoch: 60 | Batch_idx: 70 |  Loss: (0.3855) |  Loss2: (0.0000) | Acc: (87.00%) (7913/9088)
Epoch: 60 | Batch_idx: 80 |  Loss: (0.3807) |  Loss2: (0.0000) | Acc: (87.00%) (9040/10368)
Epoch: 60 | Batch_idx: 90 |  Loss: (0.3775) |  Loss2: (0.0000) | Acc: (87.00%) (10175/11648)
Epoch: 60 | Batch_idx: 100 |  Loss: (0.3743) |  Loss2: (0.0000) | Acc: (87.00%) (11309/12928)
Epoch: 60 | Batch_idx: 110 |  Loss: (0.3765) |  Loss2: (0.0000) | Acc: (87.00%) (12412/14208)
Epoch: 60 | Batch_idx: 120 |  Loss: (0.3755) |  Loss2: (0.0000) | Acc: (87.00%) (13529/15488)
Epoch: 60 | Batch_idx: 130 |  Loss: (0.3785) |  Loss2: (0.0000) | Acc: (87.00%) (14635/16768)
Epoch: 60 | Batch_idx: 140 |  Loss: (0.3808) |  Loss2: (0.0000) | Acc: (87.00%) (15737/18048)
Epoch: 60 | Batch_idx: 150 |  Loss: (0.3795) |  Loss2: (0.0000) | Acc: (87.00%) (16854/19328)
Epoch: 60 | Batch_idx: 160 |  Loss: (0.3782) |  Loss2: (0.0000) | Acc: (87.00%) (17972/20608)
Epoch: 60 | Batch_idx: 170 |  Loss: (0.3799) |  Loss2: (0.0000) | Acc: (87.00%) (19088/21888)
Epoch: 60 | Batch_idx: 180 |  Loss: (0.3793) |  Loss2: (0.0000) | Acc: (87.00%) (20211/23168)
Epoch: 60 | Batch_idx: 190 |  Loss: (0.3797) |  Loss2: (0.0000) | Acc: (87.00%) (21305/24448)
Epoch: 60 | Batch_idx: 200 |  Loss: (0.3799) |  Loss2: (0.0000) | Acc: (87.00%) (22421/25728)
Epoch: 60 | Batch_idx: 210 |  Loss: (0.3789) |  Loss2: (0.0000) | Acc: (87.00%) (23540/27008)
Epoch: 60 | Batch_idx: 220 |  Loss: (0.3786) |  Loss2: (0.0000) | Acc: (87.00%) (24649/28288)
Epoch: 60 | Batch_idx: 230 |  Loss: (0.3785) |  Loss2: (0.0000) | Acc: (87.00%) (25774/29568)
Epoch: 60 | Batch_idx: 240 |  Loss: (0.3789) |  Loss2: (0.0000) | Acc: (87.00%) (26880/30848)
Epoch: 60 | Batch_idx: 250 |  Loss: (0.3780) |  Loss2: (0.0000) | Acc: (87.00%) (28009/32128)
Epoch: 60 | Batch_idx: 260 |  Loss: (0.3781) |  Loss2: (0.0000) | Acc: (87.00%) (29129/33408)
Epoch: 60 | Batch_idx: 270 |  Loss: (0.3806) |  Loss2: (0.0000) | Acc: (87.00%) (30215/34688)
Epoch: 60 | Batch_idx: 280 |  Loss: (0.3807) |  Loss2: (0.0000) | Acc: (87.00%) (31325/35968)
Epoch: 60 | Batch_idx: 290 |  Loss: (0.3802) |  Loss2: (0.0000) | Acc: (87.00%) (32445/37248)
Epoch: 60 | Batch_idx: 300 |  Loss: (0.3802) |  Loss2: (0.0000) | Acc: (87.00%) (33554/38528)
Epoch: 60 | Batch_idx: 310 |  Loss: (0.3804) |  Loss2: (0.0000) | Acc: (87.00%) (34661/39808)
Epoch: 60 | Batch_idx: 320 |  Loss: (0.3800) |  Loss2: (0.0000) | Acc: (87.00%) (35782/41088)
Epoch: 60 | Batch_idx: 330 |  Loss: (0.3802) |  Loss2: (0.0000) | Acc: (87.00%) (36891/42368)
Epoch: 60 | Batch_idx: 340 |  Loss: (0.3811) |  Loss2: (0.0000) | Acc: (87.00%) (37993/43648)
Epoch: 60 | Batch_idx: 350 |  Loss: (0.3806) |  Loss2: (0.0000) | Acc: (87.00%) (39115/44928)
Epoch: 60 | Batch_idx: 360 |  Loss: (0.3809) |  Loss2: (0.0000) | Acc: (87.00%) (40204/46208)
Epoch: 60 | Batch_idx: 370 |  Loss: (0.3804) |  Loss2: (0.0000) | Acc: (87.00%) (41326/47488)
Epoch: 60 | Batch_idx: 380 |  Loss: (0.3802) |  Loss2: (0.0000) | Acc: (87.00%) (42433/48768)
Epoch: 60 | Batch_idx: 390 |  Loss: (0.3799) |  Loss2: (0.0000) | Acc: (87.00%) (43521/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_060.pth.tar'
# TEST : Loss: (0.5263) | Acc: (82.00%) (8249/10000)
percent tensor([0.5042, 0.4958], device='cuda:0')
percent tensor([0.4525, 0.5475], device='cuda:0')
percent tensor([0.5174, 0.4826], device='cuda:0')
percent tensor([0.5439, 0.4561], device='cuda:0')
percent tensor([0.5415, 0.4585], device='cuda:0')
percent tensor([0.6240, 0.3760], device='cuda:0')
percent tensor([0.5801, 0.4199], device='cuda:0')
percent tensor([0.9975, 0.0025], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(176.0744, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(802.8675, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(793.8819, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1520.3204, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(509.7896, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2200.1914, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4287.3887, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1414.2598, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6116.5371, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12019.6758, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3990.9797, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16915.0938, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 61 | Batch_idx: 0 |  Loss: (0.3692) |  Loss2: (0.0000) | Acc: (85.00%) (110/128)
Epoch: 61 | Batch_idx: 10 |  Loss: (0.3734) |  Loss2: (0.0000) | Acc: (87.00%) (1226/1408)
Epoch: 61 | Batch_idx: 20 |  Loss: (0.3498) |  Loss2: (0.0000) | Acc: (87.00%) (2363/2688)
Epoch: 61 | Batch_idx: 30 |  Loss: (0.3428) |  Loss2: (0.0000) | Acc: (87.00%) (3490/3968)
Epoch: 61 | Batch_idx: 40 |  Loss: (0.3475) |  Loss2: (0.0000) | Acc: (87.00%) (4613/5248)
Epoch: 61 | Batch_idx: 50 |  Loss: (0.3518) |  Loss2: (0.0000) | Acc: (87.00%) (5740/6528)
Epoch: 61 | Batch_idx: 60 |  Loss: (0.3476) |  Loss2: (0.0000) | Acc: (88.00%) (6876/7808)
Epoch: 61 | Batch_idx: 70 |  Loss: (0.3541) |  Loss2: (0.0000) | Acc: (87.00%) (7983/9088)
Epoch: 61 | Batch_idx: 80 |  Loss: (0.3582) |  Loss2: (0.0000) | Acc: (87.00%) (9102/10368)
Epoch: 61 | Batch_idx: 90 |  Loss: (0.3607) |  Loss2: (0.0000) | Acc: (87.00%) (10205/11648)
Epoch: 61 | Batch_idx: 100 |  Loss: (0.3600) |  Loss2: (0.0000) | Acc: (87.00%) (11328/12928)
Epoch: 61 | Batch_idx: 110 |  Loss: (0.3659) |  Loss2: (0.0000) | Acc: (87.00%) (12417/14208)
Epoch: 61 | Batch_idx: 120 |  Loss: (0.3622) |  Loss2: (0.0000) | Acc: (87.00%) (13566/15488)
Epoch: 61 | Batch_idx: 130 |  Loss: (0.3674) |  Loss2: (0.0000) | Acc: (87.00%) (14661/16768)
Epoch: 61 | Batch_idx: 140 |  Loss: (0.3689) |  Loss2: (0.0000) | Acc: (87.00%) (15760/18048)
Epoch: 61 | Batch_idx: 150 |  Loss: (0.3677) |  Loss2: (0.0000) | Acc: (87.00%) (16889/19328)
Epoch: 61 | Batch_idx: 160 |  Loss: (0.3684) |  Loss2: (0.0000) | Acc: (87.00%) (17998/20608)
Epoch: 61 | Batch_idx: 170 |  Loss: (0.3680) |  Loss2: (0.0000) | Acc: (87.00%) (19125/21888)
Epoch: 61 | Batch_idx: 180 |  Loss: (0.3692) |  Loss2: (0.0000) | Acc: (87.00%) (20235/23168)
Epoch: 61 | Batch_idx: 190 |  Loss: (0.3694) |  Loss2: (0.0000) | Acc: (87.00%) (21355/24448)
Epoch: 61 | Batch_idx: 200 |  Loss: (0.3697) |  Loss2: (0.0000) | Acc: (87.00%) (22476/25728)
Epoch: 61 | Batch_idx: 210 |  Loss: (0.3710) |  Loss2: (0.0000) | Acc: (87.00%) (23589/27008)
Epoch: 61 | Batch_idx: 220 |  Loss: (0.3705) |  Loss2: (0.0000) | Acc: (87.00%) (24706/28288)
Epoch: 61 | Batch_idx: 230 |  Loss: (0.3686) |  Loss2: (0.0000) | Acc: (87.00%) (25843/29568)
Epoch: 61 | Batch_idx: 240 |  Loss: (0.3682) |  Loss2: (0.0000) | Acc: (87.00%) (26953/30848)
Epoch: 61 | Batch_idx: 250 |  Loss: (0.3678) |  Loss2: (0.0000) | Acc: (87.00%) (28083/32128)
Epoch: 61 | Batch_idx: 260 |  Loss: (0.3692) |  Loss2: (0.0000) | Acc: (87.00%) (29187/33408)
Epoch: 61 | Batch_idx: 270 |  Loss: (0.3682) |  Loss2: (0.0000) | Acc: (87.00%) (30318/34688)
Epoch: 61 | Batch_idx: 280 |  Loss: (0.3684) |  Loss2: (0.0000) | Acc: (87.00%) (31431/35968)
Epoch: 61 | Batch_idx: 290 |  Loss: (0.3681) |  Loss2: (0.0000) | Acc: (87.00%) (32552/37248)
Epoch: 61 | Batch_idx: 300 |  Loss: (0.3686) |  Loss2: (0.0000) | Acc: (87.00%) (33654/38528)
Epoch: 61 | Batch_idx: 310 |  Loss: (0.3691) |  Loss2: (0.0000) | Acc: (87.00%) (34760/39808)
Epoch: 61 | Batch_idx: 320 |  Loss: (0.3697) |  Loss2: (0.0000) | Acc: (87.00%) (35859/41088)
Epoch: 61 | Batch_idx: 330 |  Loss: (0.3706) |  Loss2: (0.0000) | Acc: (87.00%) (36955/42368)
Epoch: 61 | Batch_idx: 340 |  Loss: (0.3711) |  Loss2: (0.0000) | Acc: (87.00%) (38076/43648)
Epoch: 61 | Batch_idx: 350 |  Loss: (0.3712) |  Loss2: (0.0000) | Acc: (87.00%) (39196/44928)
Epoch: 61 | Batch_idx: 360 |  Loss: (0.3719) |  Loss2: (0.0000) | Acc: (87.00%) (40301/46208)
Epoch: 61 | Batch_idx: 370 |  Loss: (0.3718) |  Loss2: (0.0000) | Acc: (87.00%) (41419/47488)
Epoch: 61 | Batch_idx: 380 |  Loss: (0.3723) |  Loss2: (0.0000) | Acc: (87.00%) (42527/48768)
Epoch: 61 | Batch_idx: 390 |  Loss: (0.3724) |  Loss2: (0.0000) | Acc: (87.00%) (43598/50000)
# TEST : Loss: (0.5342) | Acc: (82.00%) (8222/10000)
percent tensor([0.5042, 0.4958], device='cuda:0')
percent tensor([0.4522, 0.5478], device='cuda:0')
percent tensor([0.5174, 0.4826], device='cuda:0')
percent tensor([0.5445, 0.4555], device='cuda:0')
percent tensor([0.5418, 0.4582], device='cuda:0')
percent tensor([0.6289, 0.3711], device='cuda:0')
percent tensor([0.5840, 0.4160], device='cuda:0')
percent tensor([0.9979, 0.0021], device='cuda:0')
Epoch: 62 | Batch_idx: 0 |  Loss: (0.3273) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 62 | Batch_idx: 10 |  Loss: (0.3663) |  Loss2: (0.0000) | Acc: (87.00%) (1231/1408)
Epoch: 62 | Batch_idx: 20 |  Loss: (0.3470) |  Loss2: (0.0000) | Acc: (88.00%) (2370/2688)
Epoch: 62 | Batch_idx: 30 |  Loss: (0.3619) |  Loss2: (0.0000) | Acc: (87.00%) (3470/3968)
Epoch: 62 | Batch_idx: 40 |  Loss: (0.3599) |  Loss2: (0.0000) | Acc: (87.00%) (4596/5248)
Epoch: 62 | Batch_idx: 50 |  Loss: (0.3648) |  Loss2: (0.0000) | Acc: (87.00%) (5702/6528)
Epoch: 62 | Batch_idx: 60 |  Loss: (0.3638) |  Loss2: (0.0000) | Acc: (87.00%) (6820/7808)
Epoch: 62 | Batch_idx: 70 |  Loss: (0.3644) |  Loss2: (0.0000) | Acc: (87.00%) (7932/9088)
Epoch: 62 | Batch_idx: 80 |  Loss: (0.3627) |  Loss2: (0.0000) | Acc: (87.00%) (9049/10368)
Epoch: 62 | Batch_idx: 90 |  Loss: (0.3629) |  Loss2: (0.0000) | Acc: (87.00%) (10159/11648)
Epoch: 62 | Batch_idx: 100 |  Loss: (0.3659) |  Loss2: (0.0000) | Acc: (87.00%) (11268/12928)
Epoch: 62 | Batch_idx: 110 |  Loss: (0.3655) |  Loss2: (0.0000) | Acc: (87.00%) (12375/14208)
Epoch: 62 | Batch_idx: 120 |  Loss: (0.3674) |  Loss2: (0.0000) | Acc: (87.00%) (13481/15488)
Epoch: 62 | Batch_idx: 130 |  Loss: (0.3684) |  Loss2: (0.0000) | Acc: (86.00%) (14581/16768)
Epoch: 62 | Batch_idx: 140 |  Loss: (0.3678) |  Loss2: (0.0000) | Acc: (87.00%) (15707/18048)
Epoch: 62 | Batch_idx: 150 |  Loss: (0.3707) |  Loss2: (0.0000) | Acc: (86.00%) (16813/19328)
Epoch: 62 | Batch_idx: 160 |  Loss: (0.3703) |  Loss2: (0.0000) | Acc: (87.00%) (17937/20608)
Epoch: 62 | Batch_idx: 170 |  Loss: (0.3694) |  Loss2: (0.0000) | Acc: (87.00%) (19076/21888)
Epoch: 62 | Batch_idx: 180 |  Loss: (0.3684) |  Loss2: (0.0000) | Acc: (87.00%) (20201/23168)
Epoch: 62 | Batch_idx: 190 |  Loss: (0.3678) |  Loss2: (0.0000) | Acc: (87.00%) (21323/24448)
Epoch: 62 | Batch_idx: 200 |  Loss: (0.3673) |  Loss2: (0.0000) | Acc: (87.00%) (22455/25728)
Epoch: 62 | Batch_idx: 210 |  Loss: (0.3671) |  Loss2: (0.0000) | Acc: (87.00%) (23575/27008)
Epoch: 62 | Batch_idx: 220 |  Loss: (0.3657) |  Loss2: (0.0000) | Acc: (87.00%) (24704/28288)
Epoch: 62 | Batch_idx: 230 |  Loss: (0.3651) |  Loss2: (0.0000) | Acc: (87.00%) (25831/29568)
Epoch: 62 | Batch_idx: 240 |  Loss: (0.3635) |  Loss2: (0.0000) | Acc: (87.00%) (26966/30848)
Epoch: 62 | Batch_idx: 250 |  Loss: (0.3622) |  Loss2: (0.0000) | Acc: (87.00%) (28106/32128)
Epoch: 62 | Batch_idx: 260 |  Loss: (0.3617) |  Loss2: (0.0000) | Acc: (87.00%) (29228/33408)
Epoch: 62 | Batch_idx: 270 |  Loss: (0.3618) |  Loss2: (0.0000) | Acc: (87.00%) (30346/34688)
Epoch: 62 | Batch_idx: 280 |  Loss: (0.3625) |  Loss2: (0.0000) | Acc: (87.00%) (31457/35968)
Epoch: 62 | Batch_idx: 290 |  Loss: (0.3612) |  Loss2: (0.0000) | Acc: (87.00%) (32596/37248)
Epoch: 62 | Batch_idx: 300 |  Loss: (0.3610) |  Loss2: (0.0000) | Acc: (87.00%) (33727/38528)
Epoch: 62 | Batch_idx: 310 |  Loss: (0.3612) |  Loss2: (0.0000) | Acc: (87.00%) (34853/39808)
Epoch: 62 | Batch_idx: 320 |  Loss: (0.3614) |  Loss2: (0.0000) | Acc: (87.00%) (35965/41088)
Epoch: 62 | Batch_idx: 330 |  Loss: (0.3614) |  Loss2: (0.0000) | Acc: (87.00%) (37085/42368)
Epoch: 62 | Batch_idx: 340 |  Loss: (0.3607) |  Loss2: (0.0000) | Acc: (87.00%) (38218/43648)
Epoch: 62 | Batch_idx: 350 |  Loss: (0.3612) |  Loss2: (0.0000) | Acc: (87.00%) (39341/44928)
Epoch: 62 | Batch_idx: 360 |  Loss: (0.3608) |  Loss2: (0.0000) | Acc: (87.00%) (40471/46208)
Epoch: 62 | Batch_idx: 370 |  Loss: (0.3605) |  Loss2: (0.0000) | Acc: (87.00%) (41586/47488)
Epoch: 62 | Batch_idx: 380 |  Loss: (0.3609) |  Loss2: (0.0000) | Acc: (87.00%) (42711/48768)
Epoch: 62 | Batch_idx: 390 |  Loss: (0.3609) |  Loss2: (0.0000) | Acc: (87.00%) (43791/50000)
# TEST : Loss: (0.4852) | Acc: (83.00%) (8369/10000)
percent tensor([0.5041, 0.4959], device='cuda:0')
percent tensor([0.4522, 0.5478], device='cuda:0')
percent tensor([0.5174, 0.4826], device='cuda:0')
percent tensor([0.5433, 0.4567], device='cuda:0')
percent tensor([0.5408, 0.4592], device='cuda:0')
percent tensor([0.6243, 0.3757], device='cuda:0')
percent tensor([0.5826, 0.4174], device='cuda:0')
percent tensor([0.9976, 0.0024], device='cuda:0')
Epoch: 63 | Batch_idx: 0 |  Loss: (0.3276) |  Loss2: (0.0000) | Acc: (87.00%) (112/128)
Epoch: 63 | Batch_idx: 10 |  Loss: (0.3621) |  Loss2: (0.0000) | Acc: (88.00%) (1247/1408)
Epoch: 63 | Batch_idx: 20 |  Loss: (0.3668) |  Loss2: (0.0000) | Acc: (87.00%) (2361/2688)
Epoch: 63 | Batch_idx: 30 |  Loss: (0.3682) |  Loss2: (0.0000) | Acc: (87.00%) (3456/3968)
Epoch: 63 | Batch_idx: 40 |  Loss: (0.3586) |  Loss2: (0.0000) | Acc: (87.00%) (4594/5248)
Epoch: 63 | Batch_idx: 50 |  Loss: (0.3562) |  Loss2: (0.0000) | Acc: (87.00%) (5722/6528)
Epoch: 63 | Batch_idx: 60 |  Loss: (0.3528) |  Loss2: (0.0000) | Acc: (87.00%) (6852/7808)
Epoch: 63 | Batch_idx: 70 |  Loss: (0.3528) |  Loss2: (0.0000) | Acc: (87.00%) (7974/9088)
Epoch: 63 | Batch_idx: 80 |  Loss: (0.3501) |  Loss2: (0.0000) | Acc: (87.00%) (9119/10368)
Epoch: 63 | Batch_idx: 90 |  Loss: (0.3519) |  Loss2: (0.0000) | Acc: (87.00%) (10236/11648)
Epoch: 63 | Batch_idx: 100 |  Loss: (0.3515) |  Loss2: (0.0000) | Acc: (87.00%) (11361/12928)
Epoch: 63 | Batch_idx: 110 |  Loss: (0.3516) |  Loss2: (0.0000) | Acc: (87.00%) (12486/14208)
Epoch: 63 | Batch_idx: 120 |  Loss: (0.3528) |  Loss2: (0.0000) | Acc: (87.00%) (13607/15488)
Epoch: 63 | Batch_idx: 130 |  Loss: (0.3490) |  Loss2: (0.0000) | Acc: (87.00%) (14752/16768)
Epoch: 63 | Batch_idx: 140 |  Loss: (0.3478) |  Loss2: (0.0000) | Acc: (88.00%) (15894/18048)
Epoch: 63 | Batch_idx: 150 |  Loss: (0.3508) |  Loss2: (0.0000) | Acc: (87.00%) (17007/19328)
Epoch: 63 | Batch_idx: 160 |  Loss: (0.3519) |  Loss2: (0.0000) | Acc: (87.00%) (18134/20608)
Epoch: 63 | Batch_idx: 170 |  Loss: (0.3515) |  Loss2: (0.0000) | Acc: (87.00%) (19257/21888)
Epoch: 63 | Batch_idx: 180 |  Loss: (0.3519) |  Loss2: (0.0000) | Acc: (87.00%) (20383/23168)
Epoch: 63 | Batch_idx: 190 |  Loss: (0.3505) |  Loss2: (0.0000) | Acc: (88.00%) (21529/24448)
Epoch: 63 | Batch_idx: 200 |  Loss: (0.3500) |  Loss2: (0.0000) | Acc: (88.00%) (22655/25728)
Epoch: 63 | Batch_idx: 210 |  Loss: (0.3486) |  Loss2: (0.0000) | Acc: (88.00%) (23790/27008)
Epoch: 63 | Batch_idx: 220 |  Loss: (0.3469) |  Loss2: (0.0000) | Acc: (88.00%) (24930/28288)
Epoch: 63 | Batch_idx: 230 |  Loss: (0.3452) |  Loss2: (0.0000) | Acc: (88.00%) (26076/29568)
Epoch: 63 | Batch_idx: 240 |  Loss: (0.3462) |  Loss2: (0.0000) | Acc: (88.00%) (27188/30848)
Epoch: 63 | Batch_idx: 250 |  Loss: (0.3470) |  Loss2: (0.0000) | Acc: (88.00%) (28311/32128)
Epoch: 63 | Batch_idx: 260 |  Loss: (0.3464) |  Loss2: (0.0000) | Acc: (88.00%) (29441/33408)
Epoch: 63 | Batch_idx: 270 |  Loss: (0.3473) |  Loss2: (0.0000) | Acc: (88.00%) (30567/34688)
Epoch: 63 | Batch_idx: 280 |  Loss: (0.3468) |  Loss2: (0.0000) | Acc: (88.00%) (31704/35968)
Epoch: 63 | Batch_idx: 290 |  Loss: (0.3478) |  Loss2: (0.0000) | Acc: (88.00%) (32824/37248)
Epoch: 63 | Batch_idx: 300 |  Loss: (0.3492) |  Loss2: (0.0000) | Acc: (88.00%) (33921/38528)
Epoch: 63 | Batch_idx: 310 |  Loss: (0.3500) |  Loss2: (0.0000) | Acc: (88.00%) (35040/39808)
Epoch: 63 | Batch_idx: 320 |  Loss: (0.3501) |  Loss2: (0.0000) | Acc: (88.00%) (36175/41088)
Epoch: 63 | Batch_idx: 330 |  Loss: (0.3501) |  Loss2: (0.0000) | Acc: (88.00%) (37310/42368)
Epoch: 63 | Batch_idx: 340 |  Loss: (0.3504) |  Loss2: (0.0000) | Acc: (88.00%) (38435/43648)
Epoch: 63 | Batch_idx: 350 |  Loss: (0.3510) |  Loss2: (0.0000) | Acc: (88.00%) (39550/44928)
Epoch: 63 | Batch_idx: 360 |  Loss: (0.3522) |  Loss2: (0.0000) | Acc: (87.00%) (40652/46208)
Epoch: 63 | Batch_idx: 370 |  Loss: (0.3525) |  Loss2: (0.0000) | Acc: (87.00%) (41760/47488)
Epoch: 63 | Batch_idx: 380 |  Loss: (0.3520) |  Loss2: (0.0000) | Acc: (87.00%) (42901/48768)
Epoch: 63 | Batch_idx: 390 |  Loss: (0.3528) |  Loss2: (0.0000) | Acc: (87.00%) (43959/50000)
# TEST : Loss: (0.4788) | Acc: (84.00%) (8402/10000)
percent tensor([0.5042, 0.4958], device='cuda:0')
percent tensor([0.4526, 0.5474], device='cuda:0')
percent tensor([0.5173, 0.4827], device='cuda:0')
percent tensor([0.5437, 0.4563], device='cuda:0')
percent tensor([0.5412, 0.4588], device='cuda:0')
percent tensor([0.6257, 0.3743], device='cuda:0')
percent tensor([0.5824, 0.4176], device='cuda:0')
percent tensor([0.9978, 0.0022], device='cuda:0')
Epoch: 64 | Batch_idx: 0 |  Loss: (0.3018) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 64 | Batch_idx: 10 |  Loss: (0.3262) |  Loss2: (0.0000) | Acc: (87.00%) (1237/1408)
Epoch: 64 | Batch_idx: 20 |  Loss: (0.3378) |  Loss2: (0.0000) | Acc: (87.00%) (2361/2688)
Epoch: 64 | Batch_idx: 30 |  Loss: (0.3236) |  Loss2: (0.0000) | Acc: (88.00%) (3524/3968)
Epoch: 64 | Batch_idx: 40 |  Loss: (0.3224) |  Loss2: (0.0000) | Acc: (89.00%) (4676/5248)
Epoch: 64 | Batch_idx: 50 |  Loss: (0.3254) |  Loss2: (0.0000) | Acc: (89.00%) (5813/6528)
Epoch: 64 | Batch_idx: 60 |  Loss: (0.3274) |  Loss2: (0.0000) | Acc: (88.00%) (6942/7808)
Epoch: 64 | Batch_idx: 70 |  Loss: (0.3314) |  Loss2: (0.0000) | Acc: (88.00%) (8070/9088)
Epoch: 64 | Batch_idx: 80 |  Loss: (0.3319) |  Loss2: (0.0000) | Acc: (88.00%) (9209/10368)
Epoch: 64 | Batch_idx: 90 |  Loss: (0.3330) |  Loss2: (0.0000) | Acc: (88.00%) (10345/11648)
Epoch: 64 | Batch_idx: 100 |  Loss: (0.3328) |  Loss2: (0.0000) | Acc: (88.00%) (11486/12928)
Epoch: 64 | Batch_idx: 110 |  Loss: (0.3343) |  Loss2: (0.0000) | Acc: (88.00%) (12617/14208)
Epoch: 64 | Batch_idx: 120 |  Loss: (0.3324) |  Loss2: (0.0000) | Acc: (88.00%) (13769/15488)
Epoch: 64 | Batch_idx: 130 |  Loss: (0.3314) |  Loss2: (0.0000) | Acc: (88.00%) (14905/16768)
Epoch: 64 | Batch_idx: 140 |  Loss: (0.3307) |  Loss2: (0.0000) | Acc: (88.00%) (16043/18048)
Epoch: 64 | Batch_idx: 150 |  Loss: (0.3286) |  Loss2: (0.0000) | Acc: (88.00%) (17190/19328)
Epoch: 64 | Batch_idx: 160 |  Loss: (0.3299) |  Loss2: (0.0000) | Acc: (88.00%) (18315/20608)
Epoch: 64 | Batch_idx: 170 |  Loss: (0.3308) |  Loss2: (0.0000) | Acc: (88.00%) (19448/21888)
Epoch: 64 | Batch_idx: 180 |  Loss: (0.3318) |  Loss2: (0.0000) | Acc: (88.00%) (20575/23168)
Epoch: 64 | Batch_idx: 190 |  Loss: (0.3330) |  Loss2: (0.0000) | Acc: (88.00%) (21700/24448)
Epoch: 64 | Batch_idx: 200 |  Loss: (0.3329) |  Loss2: (0.0000) | Acc: (88.00%) (22838/25728)
Epoch: 64 | Batch_idx: 210 |  Loss: (0.3329) |  Loss2: (0.0000) | Acc: (88.00%) (23965/27008)
Epoch: 64 | Batch_idx: 220 |  Loss: (0.3336) |  Loss2: (0.0000) | Acc: (88.00%) (25103/28288)
Epoch: 64 | Batch_idx: 230 |  Loss: (0.3331) |  Loss2: (0.0000) | Acc: (88.00%) (26238/29568)
Epoch: 64 | Batch_idx: 240 |  Loss: (0.3336) |  Loss2: (0.0000) | Acc: (88.00%) (27353/30848)
Epoch: 64 | Batch_idx: 250 |  Loss: (0.3339) |  Loss2: (0.0000) | Acc: (88.00%) (28496/32128)
Epoch: 64 | Batch_idx: 260 |  Loss: (0.3354) |  Loss2: (0.0000) | Acc: (88.00%) (29608/33408)
Epoch: 64 | Batch_idx: 270 |  Loss: (0.3353) |  Loss2: (0.0000) | Acc: (88.00%) (30750/34688)
Epoch: 64 | Batch_idx: 280 |  Loss: (0.3353) |  Loss2: (0.0000) | Acc: (88.00%) (31882/35968)
Epoch: 64 | Batch_idx: 290 |  Loss: (0.3347) |  Loss2: (0.0000) | Acc: (88.00%) (33023/37248)
Epoch: 64 | Batch_idx: 300 |  Loss: (0.3349) |  Loss2: (0.0000) | Acc: (88.00%) (34155/38528)
Epoch: 64 | Batch_idx: 310 |  Loss: (0.3345) |  Loss2: (0.0000) | Acc: (88.00%) (35306/39808)
Epoch: 64 | Batch_idx: 320 |  Loss: (0.3345) |  Loss2: (0.0000) | Acc: (88.00%) (36445/41088)
Epoch: 64 | Batch_idx: 330 |  Loss: (0.3343) |  Loss2: (0.0000) | Acc: (88.00%) (37588/42368)
Epoch: 64 | Batch_idx: 340 |  Loss: (0.3346) |  Loss2: (0.0000) | Acc: (88.00%) (38707/43648)
Epoch: 64 | Batch_idx: 350 |  Loss: (0.3346) |  Loss2: (0.0000) | Acc: (88.00%) (39831/44928)
Epoch: 64 | Batch_idx: 360 |  Loss: (0.3361) |  Loss2: (0.0000) | Acc: (88.00%) (40939/46208)
Epoch: 64 | Batch_idx: 370 |  Loss: (0.3358) |  Loss2: (0.0000) | Acc: (88.00%) (42078/47488)
Epoch: 64 | Batch_idx: 380 |  Loss: (0.3366) |  Loss2: (0.0000) | Acc: (88.00%) (43189/48768)
Epoch: 64 | Batch_idx: 390 |  Loss: (0.3382) |  Loss2: (0.0000) | Acc: (88.00%) (44243/50000)
# TEST : Loss: (0.5127) | Acc: (83.00%) (8308/10000)
percent tensor([0.5039, 0.4961], device='cuda:0')
percent tensor([0.4528, 0.5472], device='cuda:0')
percent tensor([0.5167, 0.4833], device='cuda:0')
percent tensor([0.5436, 0.4564], device='cuda:0')
percent tensor([0.5421, 0.4579], device='cuda:0')
percent tensor([0.6251, 0.3749], device='cuda:0')
percent tensor([0.5852, 0.4148], device='cuda:0')
percent tensor([0.9979, 0.0021], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 65 | Batch_idx: 0 |  Loss: (0.2307) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 65 | Batch_idx: 10 |  Loss: (0.3327) |  Loss2: (0.0000) | Acc: (88.00%) (1246/1408)
Epoch: 65 | Batch_idx: 20 |  Loss: (0.3775) |  Loss2: (0.0000) | Acc: (87.00%) (2342/2688)
Epoch: 65 | Batch_idx: 30 |  Loss: (0.4091) |  Loss2: (0.0000) | Acc: (85.00%) (3409/3968)
Epoch: 65 | Batch_idx: 40 |  Loss: (0.4173) |  Loss2: (0.0000) | Acc: (85.00%) (4481/5248)
Epoch: 65 | Batch_idx: 50 |  Loss: (0.4241) |  Loss2: (0.0000) | Acc: (85.00%) (5555/6528)
Epoch: 65 | Batch_idx: 60 |  Loss: (0.4237) |  Loss2: (0.0000) | Acc: (85.00%) (6652/7808)
Epoch: 65 | Batch_idx: 70 |  Loss: (0.4264) |  Loss2: (0.0000) | Acc: (85.00%) (7725/9088)
Epoch: 65 | Batch_idx: 80 |  Loss: (0.4291) |  Loss2: (0.0000) | Acc: (84.00%) (8799/10368)
Epoch: 65 | Batch_idx: 90 |  Loss: (0.4295) |  Loss2: (0.0000) | Acc: (84.00%) (9890/11648)
Epoch: 65 | Batch_idx: 100 |  Loss: (0.4273) |  Loss2: (0.0000) | Acc: (84.00%) (10979/12928)
Epoch: 65 | Batch_idx: 110 |  Loss: (0.4255) |  Loss2: (0.0000) | Acc: (84.00%) (12075/14208)
Epoch: 65 | Batch_idx: 120 |  Loss: (0.4272) |  Loss2: (0.0000) | Acc: (84.00%) (13159/15488)
Epoch: 65 | Batch_idx: 130 |  Loss: (0.4253) |  Loss2: (0.0000) | Acc: (85.00%) (14266/16768)
Epoch: 65 | Batch_idx: 140 |  Loss: (0.4250) |  Loss2: (0.0000) | Acc: (85.00%) (15353/18048)
Epoch: 65 | Batch_idx: 150 |  Loss: (0.4249) |  Loss2: (0.0000) | Acc: (85.00%) (16441/19328)
Epoch: 65 | Batch_idx: 160 |  Loss: (0.4247) |  Loss2: (0.0000) | Acc: (85.00%) (17533/20608)
Epoch: 65 | Batch_idx: 170 |  Loss: (0.4233) |  Loss2: (0.0000) | Acc: (85.00%) (18632/21888)
Epoch: 65 | Batch_idx: 180 |  Loss: (0.4199) |  Loss2: (0.0000) | Acc: (85.00%) (19743/23168)
Epoch: 65 | Batch_idx: 190 |  Loss: (0.4189) |  Loss2: (0.0000) | Acc: (85.00%) (20848/24448)
Epoch: 65 | Batch_idx: 200 |  Loss: (0.4174) |  Loss2: (0.0000) | Acc: (85.00%) (21964/25728)
Epoch: 65 | Batch_idx: 210 |  Loss: (0.4184) |  Loss2: (0.0000) | Acc: (85.00%) (23054/27008)
Epoch: 65 | Batch_idx: 220 |  Loss: (0.4169) |  Loss2: (0.0000) | Acc: (85.00%) (24160/28288)
Epoch: 65 | Batch_idx: 230 |  Loss: (0.4161) |  Loss2: (0.0000) | Acc: (85.00%) (25271/29568)
Epoch: 65 | Batch_idx: 240 |  Loss: (0.4174) |  Loss2: (0.0000) | Acc: (85.00%) (26352/30848)
Epoch: 65 | Batch_idx: 250 |  Loss: (0.4184) |  Loss2: (0.0000) | Acc: (85.00%) (27441/32128)
Epoch: 65 | Batch_idx: 260 |  Loss: (0.4185) |  Loss2: (0.0000) | Acc: (85.00%) (28531/33408)
Epoch: 65 | Batch_idx: 270 |  Loss: (0.4192) |  Loss2: (0.0000) | Acc: (85.00%) (29613/34688)
Epoch: 65 | Batch_idx: 280 |  Loss: (0.4194) |  Loss2: (0.0000) | Acc: (85.00%) (30708/35968)
Epoch: 65 | Batch_idx: 290 |  Loss: (0.4186) |  Loss2: (0.0000) | Acc: (85.00%) (31803/37248)
Epoch: 65 | Batch_idx: 300 |  Loss: (0.4177) |  Loss2: (0.0000) | Acc: (85.00%) (32922/38528)
Epoch: 65 | Batch_idx: 310 |  Loss: (0.4173) |  Loss2: (0.0000) | Acc: (85.00%) (34028/39808)
Epoch: 65 | Batch_idx: 320 |  Loss: (0.4168) |  Loss2: (0.0000) | Acc: (85.00%) (35118/41088)
Epoch: 65 | Batch_idx: 330 |  Loss: (0.4173) |  Loss2: (0.0000) | Acc: (85.00%) (36194/42368)
Epoch: 65 | Batch_idx: 340 |  Loss: (0.4157) |  Loss2: (0.0000) | Acc: (85.00%) (37311/43648)
Epoch: 65 | Batch_idx: 350 |  Loss: (0.4149) |  Loss2: (0.0000) | Acc: (85.00%) (38418/44928)
Epoch: 65 | Batch_idx: 360 |  Loss: (0.4152) |  Loss2: (0.0000) | Acc: (85.00%) (39519/46208)
Epoch: 65 | Batch_idx: 370 |  Loss: (0.4149) |  Loss2: (0.0000) | Acc: (85.00%) (40627/47488)
Epoch: 65 | Batch_idx: 380 |  Loss: (0.4131) |  Loss2: (0.0000) | Acc: (85.00%) (41760/48768)
Epoch: 65 | Batch_idx: 390 |  Loss: (0.4125) |  Loss2: (0.0000) | Acc: (85.00%) (42819/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_065.pth.tar'
# TEST : Loss: (0.5064) | Acc: (83.00%) (8362/10000)
percent tensor([0.5100, 0.4900], device='cuda:0')
percent tensor([0.4619, 0.5381], device='cuda:0')
percent tensor([0.5416, 0.4584], device='cuda:0')
percent tensor([0.5563, 0.4437], device='cuda:0')
percent tensor([0.5272, 0.4728], device='cuda:0')
percent tensor([0.6405, 0.3595], device='cuda:0')
percent tensor([0.5675, 0.4325], device='cuda:0')
percent tensor([0.9977, 0.0023], device='cuda:0')
Epoch: 66 | Batch_idx: 0 |  Loss: (0.3656) |  Loss2: (0.0000) | Acc: (85.00%) (109/128)
Epoch: 66 | Batch_idx: 10 |  Loss: (0.3373) |  Loss2: (0.0000) | Acc: (87.00%) (1238/1408)
Epoch: 66 | Batch_idx: 20 |  Loss: (0.3762) |  Loss2: (0.0000) | Acc: (86.00%) (2327/2688)
Epoch: 66 | Batch_idx: 30 |  Loss: (0.3824) |  Loss2: (0.0000) | Acc: (86.00%) (3437/3968)
Epoch: 66 | Batch_idx: 40 |  Loss: (0.3815) |  Loss2: (0.0000) | Acc: (86.00%) (4554/5248)
Epoch: 66 | Batch_idx: 50 |  Loss: (0.3872) |  Loss2: (0.0000) | Acc: (86.00%) (5663/6528)
Epoch: 66 | Batch_idx: 60 |  Loss: (0.3909) |  Loss2: (0.0000) | Acc: (86.00%) (6760/7808)
Epoch: 66 | Batch_idx: 70 |  Loss: (0.3864) |  Loss2: (0.0000) | Acc: (86.00%) (7879/9088)
Epoch: 66 | Batch_idx: 80 |  Loss: (0.3868) |  Loss2: (0.0000) | Acc: (86.00%) (9008/10368)
Epoch: 66 | Batch_idx: 90 |  Loss: (0.3871) |  Loss2: (0.0000) | Acc: (86.00%) (10116/11648)
Epoch: 66 | Batch_idx: 100 |  Loss: (0.3881) |  Loss2: (0.0000) | Acc: (86.00%) (11229/12928)
Epoch: 66 | Batch_idx: 110 |  Loss: (0.3872) |  Loss2: (0.0000) | Acc: (86.00%) (12347/14208)
Epoch: 66 | Batch_idx: 120 |  Loss: (0.3877) |  Loss2: (0.0000) | Acc: (86.00%) (13448/15488)
Epoch: 66 | Batch_idx: 130 |  Loss: (0.3897) |  Loss2: (0.0000) | Acc: (86.00%) (14542/16768)
Epoch: 66 | Batch_idx: 140 |  Loss: (0.3863) |  Loss2: (0.0000) | Acc: (86.00%) (15674/18048)
Epoch: 66 | Batch_idx: 150 |  Loss: (0.3840) |  Loss2: (0.0000) | Acc: (86.00%) (16807/19328)
Epoch: 66 | Batch_idx: 160 |  Loss: (0.3839) |  Loss2: (0.0000) | Acc: (86.00%) (17917/20608)
Epoch: 66 | Batch_idx: 170 |  Loss: (0.3840) |  Loss2: (0.0000) | Acc: (86.00%) (19031/21888)
Epoch: 66 | Batch_idx: 180 |  Loss: (0.3830) |  Loss2: (0.0000) | Acc: (86.00%) (20149/23168)
Epoch: 66 | Batch_idx: 190 |  Loss: (0.3843) |  Loss2: (0.0000) | Acc: (86.00%) (21259/24448)
Epoch: 66 | Batch_idx: 200 |  Loss: (0.3851) |  Loss2: (0.0000) | Acc: (86.00%) (22356/25728)
Epoch: 66 | Batch_idx: 210 |  Loss: (0.3849) |  Loss2: (0.0000) | Acc: (86.00%) (23460/27008)
Epoch: 66 | Batch_idx: 220 |  Loss: (0.3840) |  Loss2: (0.0000) | Acc: (86.00%) (24582/28288)
Epoch: 66 | Batch_idx: 230 |  Loss: (0.3837) |  Loss2: (0.0000) | Acc: (86.00%) (25694/29568)
Epoch: 66 | Batch_idx: 240 |  Loss: (0.3832) |  Loss2: (0.0000) | Acc: (86.00%) (26811/30848)
Epoch: 66 | Batch_idx: 250 |  Loss: (0.3824) |  Loss2: (0.0000) | Acc: (86.00%) (27926/32128)
Epoch: 66 | Batch_idx: 260 |  Loss: (0.3831) |  Loss2: (0.0000) | Acc: (86.00%) (29013/33408)
Epoch: 66 | Batch_idx: 270 |  Loss: (0.3827) |  Loss2: (0.0000) | Acc: (86.00%) (30134/34688)
Epoch: 66 | Batch_idx: 280 |  Loss: (0.3812) |  Loss2: (0.0000) | Acc: (86.00%) (31263/35968)
Epoch: 66 | Batch_idx: 290 |  Loss: (0.3817) |  Loss2: (0.0000) | Acc: (86.00%) (32368/37248)
Epoch: 66 | Batch_idx: 300 |  Loss: (0.3806) |  Loss2: (0.0000) | Acc: (86.00%) (33504/38528)
Epoch: 66 | Batch_idx: 310 |  Loss: (0.3805) |  Loss2: (0.0000) | Acc: (86.00%) (34618/39808)
Epoch: 66 | Batch_idx: 320 |  Loss: (0.3801) |  Loss2: (0.0000) | Acc: (86.00%) (35742/41088)
Epoch: 66 | Batch_idx: 330 |  Loss: (0.3797) |  Loss2: (0.0000) | Acc: (87.00%) (36872/42368)
Epoch: 66 | Batch_idx: 340 |  Loss: (0.3790) |  Loss2: (0.0000) | Acc: (87.00%) (37997/43648)
Epoch: 66 | Batch_idx: 350 |  Loss: (0.3796) |  Loss2: (0.0000) | Acc: (87.00%) (39103/44928)
Epoch: 66 | Batch_idx: 360 |  Loss: (0.3791) |  Loss2: (0.0000) | Acc: (87.00%) (40226/46208)
Epoch: 66 | Batch_idx: 370 |  Loss: (0.3784) |  Loss2: (0.0000) | Acc: (87.00%) (41353/47488)
Epoch: 66 | Batch_idx: 380 |  Loss: (0.3773) |  Loss2: (0.0000) | Acc: (87.00%) (42487/48768)
Epoch: 66 | Batch_idx: 390 |  Loss: (0.3780) |  Loss2: (0.0000) | Acc: (87.00%) (43563/50000)
# TEST : Loss: (0.4702) | Acc: (84.00%) (8463/10000)
percent tensor([0.5122, 0.4878], device='cuda:0')
percent tensor([0.4608, 0.5392], device='cuda:0')
percent tensor([0.5493, 0.4507], device='cuda:0')
percent tensor([0.5664, 0.4336], device='cuda:0')
percent tensor([0.5259, 0.4741], device='cuda:0')
percent tensor([0.6349, 0.3651], device='cuda:0')
percent tensor([0.5747, 0.4253], device='cuda:0')
percent tensor([0.9978, 0.0022], device='cuda:0')
Epoch: 67 | Batch_idx: 0 |  Loss: (0.3779) |  Loss2: (0.0000) | Acc: (85.00%) (110/128)
Epoch: 67 | Batch_idx: 10 |  Loss: (0.3600) |  Loss2: (0.0000) | Acc: (87.00%) (1239/1408)
Epoch: 67 | Batch_idx: 20 |  Loss: (0.3588) |  Loss2: (0.0000) | Acc: (87.00%) (2357/2688)
Epoch: 67 | Batch_idx: 30 |  Loss: (0.3589) |  Loss2: (0.0000) | Acc: (87.00%) (3485/3968)
Epoch: 67 | Batch_idx: 40 |  Loss: (0.3590) |  Loss2: (0.0000) | Acc: (87.00%) (4615/5248)
Epoch: 67 | Batch_idx: 50 |  Loss: (0.3594) |  Loss2: (0.0000) | Acc: (87.00%) (5743/6528)
Epoch: 67 | Batch_idx: 60 |  Loss: (0.3620) |  Loss2: (0.0000) | Acc: (87.00%) (6852/7808)
Epoch: 67 | Batch_idx: 70 |  Loss: (0.3595) |  Loss2: (0.0000) | Acc: (87.00%) (7980/9088)
Epoch: 67 | Batch_idx: 80 |  Loss: (0.3625) |  Loss2: (0.0000) | Acc: (87.00%) (9090/10368)
Epoch: 67 | Batch_idx: 90 |  Loss: (0.3649) |  Loss2: (0.0000) | Acc: (87.00%) (10207/11648)
Epoch: 67 | Batch_idx: 100 |  Loss: (0.3648) |  Loss2: (0.0000) | Acc: (87.00%) (11326/12928)
Epoch: 67 | Batch_idx: 110 |  Loss: (0.3633) |  Loss2: (0.0000) | Acc: (87.00%) (12448/14208)
Epoch: 67 | Batch_idx: 120 |  Loss: (0.3634) |  Loss2: (0.0000) | Acc: (87.00%) (13571/15488)
Epoch: 67 | Batch_idx: 130 |  Loss: (0.3623) |  Loss2: (0.0000) | Acc: (87.00%) (14691/16768)
Epoch: 67 | Batch_idx: 140 |  Loss: (0.3611) |  Loss2: (0.0000) | Acc: (87.00%) (15823/18048)
Epoch: 67 | Batch_idx: 150 |  Loss: (0.3619) |  Loss2: (0.0000) | Acc: (87.00%) (16932/19328)
Epoch: 67 | Batch_idx: 160 |  Loss: (0.3611) |  Loss2: (0.0000) | Acc: (87.00%) (18063/20608)
Epoch: 67 | Batch_idx: 170 |  Loss: (0.3627) |  Loss2: (0.0000) | Acc: (87.00%) (19173/21888)
Epoch: 67 | Batch_idx: 180 |  Loss: (0.3640) |  Loss2: (0.0000) | Acc: (87.00%) (20276/23168)
Epoch: 67 | Batch_idx: 190 |  Loss: (0.3641) |  Loss2: (0.0000) | Acc: (87.00%) (21393/24448)
Epoch: 67 | Batch_idx: 200 |  Loss: (0.3633) |  Loss2: (0.0000) | Acc: (87.00%) (22529/25728)
Epoch: 67 | Batch_idx: 210 |  Loss: (0.3631) |  Loss2: (0.0000) | Acc: (87.00%) (23653/27008)
Epoch: 67 | Batch_idx: 220 |  Loss: (0.3633) |  Loss2: (0.0000) | Acc: (87.00%) (24757/28288)
Epoch: 67 | Batch_idx: 230 |  Loss: (0.3640) |  Loss2: (0.0000) | Acc: (87.00%) (25868/29568)
Epoch: 67 | Batch_idx: 240 |  Loss: (0.3629) |  Loss2: (0.0000) | Acc: (87.00%) (26996/30848)
Epoch: 67 | Batch_idx: 250 |  Loss: (0.3646) |  Loss2: (0.0000) | Acc: (87.00%) (28099/32128)
Epoch: 67 | Batch_idx: 260 |  Loss: (0.3645) |  Loss2: (0.0000) | Acc: (87.00%) (29221/33408)
Epoch: 67 | Batch_idx: 270 |  Loss: (0.3656) |  Loss2: (0.0000) | Acc: (87.00%) (30324/34688)
Epoch: 67 | Batch_idx: 280 |  Loss: (0.3636) |  Loss2: (0.0000) | Acc: (87.00%) (31458/35968)
Epoch: 67 | Batch_idx: 290 |  Loss: (0.3639) |  Loss2: (0.0000) | Acc: (87.00%) (32566/37248)
Epoch: 67 | Batch_idx: 300 |  Loss: (0.3637) |  Loss2: (0.0000) | Acc: (87.00%) (33679/38528)
Epoch: 67 | Batch_idx: 310 |  Loss: (0.3619) |  Loss2: (0.0000) | Acc: (87.00%) (34819/39808)
Epoch: 67 | Batch_idx: 320 |  Loss: (0.3614) |  Loss2: (0.0000) | Acc: (87.00%) (35951/41088)
Epoch: 67 | Batch_idx: 330 |  Loss: (0.3606) |  Loss2: (0.0000) | Acc: (87.00%) (37087/42368)
Epoch: 67 | Batch_idx: 340 |  Loss: (0.3600) |  Loss2: (0.0000) | Acc: (87.00%) (38212/43648)
Epoch: 67 | Batch_idx: 350 |  Loss: (0.3601) |  Loss2: (0.0000) | Acc: (87.00%) (39338/44928)
Epoch: 67 | Batch_idx: 360 |  Loss: (0.3596) |  Loss2: (0.0000) | Acc: (87.00%) (40469/46208)
Epoch: 67 | Batch_idx: 370 |  Loss: (0.3596) |  Loss2: (0.0000) | Acc: (87.00%) (41587/47488)
Epoch: 67 | Batch_idx: 380 |  Loss: (0.3590) |  Loss2: (0.0000) | Acc: (87.00%) (42713/48768)
Epoch: 67 | Batch_idx: 390 |  Loss: (0.3592) |  Loss2: (0.0000) | Acc: (87.00%) (43786/50000)
# TEST : Loss: (0.4556) | Acc: (85.00%) (8508/10000)
percent tensor([0.5124, 0.4876], device='cuda:0')
percent tensor([0.4603, 0.5397], device='cuda:0')
percent tensor([0.5499, 0.4501], device='cuda:0')
percent tensor([0.5677, 0.4323], device='cuda:0')
percent tensor([0.5280, 0.4720], device='cuda:0')
percent tensor([0.6357, 0.3643], device='cuda:0')
percent tensor([0.5874, 0.4126], device='cuda:0')
percent tensor([0.9978, 0.0022], device='cuda:0')
Epoch: 68 | Batch_idx: 0 |  Loss: (0.3652) |  Loss2: (0.0000) | Acc: (87.00%) (112/128)
Epoch: 68 | Batch_idx: 10 |  Loss: (0.3672) |  Loss2: (0.0000) | Acc: (87.00%) (1227/1408)
Epoch: 68 | Batch_idx: 20 |  Loss: (0.3687) |  Loss2: (0.0000) | Acc: (87.00%) (2346/2688)
Epoch: 68 | Batch_idx: 30 |  Loss: (0.3537) |  Loss2: (0.0000) | Acc: (87.00%) (3485/3968)
Epoch: 68 | Batch_idx: 40 |  Loss: (0.3554) |  Loss2: (0.0000) | Acc: (87.00%) (4608/5248)
Epoch: 68 | Batch_idx: 50 |  Loss: (0.3513) |  Loss2: (0.0000) | Acc: (87.00%) (5741/6528)
Epoch: 68 | Batch_idx: 60 |  Loss: (0.3556) |  Loss2: (0.0000) | Acc: (87.00%) (6853/7808)
Epoch: 68 | Batch_idx: 70 |  Loss: (0.3578) |  Loss2: (0.0000) | Acc: (87.00%) (7981/9088)
Epoch: 68 | Batch_idx: 80 |  Loss: (0.3589) |  Loss2: (0.0000) | Acc: (87.00%) (9108/10368)
Epoch: 68 | Batch_idx: 90 |  Loss: (0.3562) |  Loss2: (0.0000) | Acc: (87.00%) (10236/11648)
Epoch: 68 | Batch_idx: 100 |  Loss: (0.3544) |  Loss2: (0.0000) | Acc: (87.00%) (11373/12928)
Epoch: 68 | Batch_idx: 110 |  Loss: (0.3526) |  Loss2: (0.0000) | Acc: (88.00%) (12510/14208)
Epoch: 68 | Batch_idx: 120 |  Loss: (0.3511) |  Loss2: (0.0000) | Acc: (88.00%) (13649/15488)
Epoch: 68 | Batch_idx: 130 |  Loss: (0.3522) |  Loss2: (0.0000) | Acc: (87.00%) (14753/16768)
Epoch: 68 | Batch_idx: 140 |  Loss: (0.3533) |  Loss2: (0.0000) | Acc: (87.00%) (15867/18048)
Epoch: 68 | Batch_idx: 150 |  Loss: (0.3522) |  Loss2: (0.0000) | Acc: (87.00%) (17000/19328)
Epoch: 68 | Batch_idx: 160 |  Loss: (0.3509) |  Loss2: (0.0000) | Acc: (87.00%) (18120/20608)
Epoch: 68 | Batch_idx: 170 |  Loss: (0.3513) |  Loss2: (0.0000) | Acc: (87.00%) (19241/21888)
Epoch: 68 | Batch_idx: 180 |  Loss: (0.3514) |  Loss2: (0.0000) | Acc: (87.00%) (20360/23168)
Epoch: 68 | Batch_idx: 190 |  Loss: (0.3533) |  Loss2: (0.0000) | Acc: (87.00%) (21483/24448)
Epoch: 68 | Batch_idx: 200 |  Loss: (0.3527) |  Loss2: (0.0000) | Acc: (87.00%) (22614/25728)
Epoch: 68 | Batch_idx: 210 |  Loss: (0.3529) |  Loss2: (0.0000) | Acc: (87.00%) (23734/27008)
Epoch: 68 | Batch_idx: 220 |  Loss: (0.3511) |  Loss2: (0.0000) | Acc: (87.00%) (24873/28288)
Epoch: 68 | Batch_idx: 230 |  Loss: (0.3501) |  Loss2: (0.0000) | Acc: (87.00%) (26009/29568)
Epoch: 68 | Batch_idx: 240 |  Loss: (0.3509) |  Loss2: (0.0000) | Acc: (87.00%) (27121/30848)
Epoch: 68 | Batch_idx: 250 |  Loss: (0.3505) |  Loss2: (0.0000) | Acc: (87.00%) (28249/32128)
Epoch: 68 | Batch_idx: 260 |  Loss: (0.3515) |  Loss2: (0.0000) | Acc: (87.00%) (29369/33408)
Epoch: 68 | Batch_idx: 270 |  Loss: (0.3508) |  Loss2: (0.0000) | Acc: (87.00%) (30491/34688)
Epoch: 68 | Batch_idx: 280 |  Loss: (0.3493) |  Loss2: (0.0000) | Acc: (87.00%) (31650/35968)
Epoch: 68 | Batch_idx: 290 |  Loss: (0.3498) |  Loss2: (0.0000) | Acc: (87.00%) (32771/37248)
Epoch: 68 | Batch_idx: 300 |  Loss: (0.3493) |  Loss2: (0.0000) | Acc: (88.00%) (33911/38528)
Epoch: 68 | Batch_idx: 310 |  Loss: (0.3491) |  Loss2: (0.0000) | Acc: (88.00%) (35045/39808)
Epoch: 68 | Batch_idx: 320 |  Loss: (0.3499) |  Loss2: (0.0000) | Acc: (87.00%) (36155/41088)
Epoch: 68 | Batch_idx: 330 |  Loss: (0.3497) |  Loss2: (0.0000) | Acc: (88.00%) (37293/42368)
Epoch: 68 | Batch_idx: 340 |  Loss: (0.3490) |  Loss2: (0.0000) | Acc: (88.00%) (38430/43648)
Epoch: 68 | Batch_idx: 350 |  Loss: (0.3501) |  Loss2: (0.0000) | Acc: (87.00%) (39527/44928)
Epoch: 68 | Batch_idx: 360 |  Loss: (0.3494) |  Loss2: (0.0000) | Acc: (87.00%) (40656/46208)
Epoch: 68 | Batch_idx: 370 |  Loss: (0.3506) |  Loss2: (0.0000) | Acc: (87.00%) (41747/47488)
Epoch: 68 | Batch_idx: 380 |  Loss: (0.3497) |  Loss2: (0.0000) | Acc: (87.00%) (42898/48768)
Epoch: 68 | Batch_idx: 390 |  Loss: (0.3495) |  Loss2: (0.0000) | Acc: (87.00%) (43982/50000)
# TEST : Loss: (0.4442) | Acc: (85.00%) (8531/10000)
percent tensor([0.5125, 0.4875], device='cuda:0')
percent tensor([0.4601, 0.5399], device='cuda:0')
percent tensor([0.5512, 0.4488], device='cuda:0')
percent tensor([0.5728, 0.4272], device='cuda:0')
percent tensor([0.5306, 0.4694], device='cuda:0')
percent tensor([0.6360, 0.3640], device='cuda:0')
percent tensor([0.5893, 0.4107], device='cuda:0')
percent tensor([0.9979, 0.0021], device='cuda:0')
Epoch: 69 | Batch_idx: 0 |  Loss: (0.3296) |  Loss2: (0.0000) | Acc: (86.00%) (111/128)
Epoch: 69 | Batch_idx: 10 |  Loss: (0.3734) |  Loss2: (0.0000) | Acc: (86.00%) (1215/1408)
Epoch: 69 | Batch_idx: 20 |  Loss: (0.3667) |  Loss2: (0.0000) | Acc: (87.00%) (2339/2688)
Epoch: 69 | Batch_idx: 30 |  Loss: (0.3510) |  Loss2: (0.0000) | Acc: (87.00%) (3472/3968)
Epoch: 69 | Batch_idx: 40 |  Loss: (0.3566) |  Loss2: (0.0000) | Acc: (87.00%) (4591/5248)
Epoch: 69 | Batch_idx: 50 |  Loss: (0.3515) |  Loss2: (0.0000) | Acc: (87.00%) (5722/6528)
Epoch: 69 | Batch_idx: 60 |  Loss: (0.3534) |  Loss2: (0.0000) | Acc: (87.00%) (6841/7808)
Epoch: 69 | Batch_idx: 70 |  Loss: (0.3517) |  Loss2: (0.0000) | Acc: (87.00%) (7973/9088)
Epoch: 69 | Batch_idx: 80 |  Loss: (0.3504) |  Loss2: (0.0000) | Acc: (87.00%) (9115/10368)
Epoch: 69 | Batch_idx: 90 |  Loss: (0.3487) |  Loss2: (0.0000) | Acc: (87.00%) (10247/11648)
Epoch: 69 | Batch_idx: 100 |  Loss: (0.3451) |  Loss2: (0.0000) | Acc: (88.00%) (11386/12928)
Epoch: 69 | Batch_idx: 110 |  Loss: (0.3435) |  Loss2: (0.0000) | Acc: (88.00%) (12520/14208)
Epoch: 69 | Batch_idx: 120 |  Loss: (0.3428) |  Loss2: (0.0000) | Acc: (88.00%) (13661/15488)
Epoch: 69 | Batch_idx: 130 |  Loss: (0.3415) |  Loss2: (0.0000) | Acc: (88.00%) (14800/16768)
Epoch: 69 | Batch_idx: 140 |  Loss: (0.3432) |  Loss2: (0.0000) | Acc: (88.00%) (15933/18048)
Epoch: 69 | Batch_idx: 150 |  Loss: (0.3444) |  Loss2: (0.0000) | Acc: (88.00%) (17066/19328)
Epoch: 69 | Batch_idx: 160 |  Loss: (0.3443) |  Loss2: (0.0000) | Acc: (88.00%) (18199/20608)
Epoch: 69 | Batch_idx: 170 |  Loss: (0.3445) |  Loss2: (0.0000) | Acc: (88.00%) (19319/21888)
Epoch: 69 | Batch_idx: 180 |  Loss: (0.3442) |  Loss2: (0.0000) | Acc: (88.00%) (20452/23168)
Epoch: 69 | Batch_idx: 190 |  Loss: (0.3469) |  Loss2: (0.0000) | Acc: (88.00%) (21552/24448)
Epoch: 69 | Batch_idx: 200 |  Loss: (0.3484) |  Loss2: (0.0000) | Acc: (88.00%) (22655/25728)
Epoch: 69 | Batch_idx: 210 |  Loss: (0.3475) |  Loss2: (0.0000) | Acc: (88.00%) (23788/27008)
Epoch: 69 | Batch_idx: 220 |  Loss: (0.3487) |  Loss2: (0.0000) | Acc: (88.00%) (24898/28288)
Epoch: 69 | Batch_idx: 230 |  Loss: (0.3493) |  Loss2: (0.0000) | Acc: (87.00%) (26011/29568)
Epoch: 69 | Batch_idx: 240 |  Loss: (0.3493) |  Loss2: (0.0000) | Acc: (87.00%) (27146/30848)
Epoch: 69 | Batch_idx: 250 |  Loss: (0.3486) |  Loss2: (0.0000) | Acc: (88.00%) (28296/32128)
Epoch: 69 | Batch_idx: 260 |  Loss: (0.3469) |  Loss2: (0.0000) | Acc: (88.00%) (29452/33408)
Epoch: 69 | Batch_idx: 270 |  Loss: (0.3470) |  Loss2: (0.0000) | Acc: (88.00%) (30567/34688)
Epoch: 69 | Batch_idx: 280 |  Loss: (0.3460) |  Loss2: (0.0000) | Acc: (88.00%) (31714/35968)
Epoch: 69 | Batch_idx: 290 |  Loss: (0.3453) |  Loss2: (0.0000) | Acc: (88.00%) (32856/37248)
Epoch: 69 | Batch_idx: 300 |  Loss: (0.3453) |  Loss2: (0.0000) | Acc: (88.00%) (33977/38528)
Epoch: 69 | Batch_idx: 310 |  Loss: (0.3448) |  Loss2: (0.0000) | Acc: (88.00%) (35105/39808)
Epoch: 69 | Batch_idx: 320 |  Loss: (0.3456) |  Loss2: (0.0000) | Acc: (88.00%) (36223/41088)
Epoch: 69 | Batch_idx: 330 |  Loss: (0.3452) |  Loss2: (0.0000) | Acc: (88.00%) (37362/42368)
Epoch: 69 | Batch_idx: 340 |  Loss: (0.3452) |  Loss2: (0.0000) | Acc: (88.00%) (38482/43648)
Epoch: 69 | Batch_idx: 350 |  Loss: (0.3452) |  Loss2: (0.0000) | Acc: (88.00%) (39609/44928)
Epoch: 69 | Batch_idx: 360 |  Loss: (0.3450) |  Loss2: (0.0000) | Acc: (88.00%) (40750/46208)
Epoch: 69 | Batch_idx: 370 |  Loss: (0.3451) |  Loss2: (0.0000) | Acc: (88.00%) (41872/47488)
Epoch: 69 | Batch_idx: 380 |  Loss: (0.3450) |  Loss2: (0.0000) | Acc: (88.00%) (43005/48768)
Epoch: 69 | Batch_idx: 390 |  Loss: (0.3444) |  Loss2: (0.0000) | Acc: (88.00%) (44096/50000)
# TEST : Loss: (0.4345) | Acc: (85.00%) (8542/10000)
percent tensor([0.5123, 0.4877], device='cuda:0')
percent tensor([0.4611, 0.5389], device='cuda:0')
percent tensor([0.5467, 0.4533], device='cuda:0')
percent tensor([0.5707, 0.4293], device='cuda:0')
percent tensor([0.5330, 0.4670], device='cuda:0')
percent tensor([0.6351, 0.3649], device='cuda:0')
percent tensor([0.5996, 0.4004], device='cuda:0')
percent tensor([0.9980, 0.0020], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 70 | Batch_idx: 0 |  Loss: (0.3108) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 70 | Batch_idx: 10 |  Loss: (0.3323) |  Loss2: (0.0000) | Acc: (88.00%) (1253/1408)
Epoch: 70 | Batch_idx: 20 |  Loss: (0.3312) |  Loss2: (0.0000) | Acc: (89.00%) (2393/2688)
Epoch: 70 | Batch_idx: 30 |  Loss: (0.3431) |  Loss2: (0.0000) | Acc: (88.00%) (3507/3968)
Epoch: 70 | Batch_idx: 40 |  Loss: (0.3432) |  Loss2: (0.0000) | Acc: (88.00%) (4630/5248)
Epoch: 70 | Batch_idx: 50 |  Loss: (0.3475) |  Loss2: (0.0000) | Acc: (87.00%) (5742/6528)
Epoch: 70 | Batch_idx: 60 |  Loss: (0.3464) |  Loss2: (0.0000) | Acc: (87.00%) (6863/7808)
Epoch: 70 | Batch_idx: 70 |  Loss: (0.3425) |  Loss2: (0.0000) | Acc: (88.00%) (8011/9088)
Epoch: 70 | Batch_idx: 80 |  Loss: (0.3407) |  Loss2: (0.0000) | Acc: (88.00%) (9151/10368)
Epoch: 70 | Batch_idx: 90 |  Loss: (0.3402) |  Loss2: (0.0000) | Acc: (88.00%) (10284/11648)
Epoch: 70 | Batch_idx: 100 |  Loss: (0.3395) |  Loss2: (0.0000) | Acc: (88.00%) (11421/12928)
Epoch: 70 | Batch_idx: 110 |  Loss: (0.3371) |  Loss2: (0.0000) | Acc: (88.00%) (12563/14208)
Epoch: 70 | Batch_idx: 120 |  Loss: (0.3359) |  Loss2: (0.0000) | Acc: (88.00%) (13691/15488)
Epoch: 70 | Batch_idx: 130 |  Loss: (0.3361) |  Loss2: (0.0000) | Acc: (88.00%) (14815/16768)
Epoch: 70 | Batch_idx: 140 |  Loss: (0.3363) |  Loss2: (0.0000) | Acc: (88.00%) (15946/18048)
Epoch: 70 | Batch_idx: 150 |  Loss: (0.3340) |  Loss2: (0.0000) | Acc: (88.00%) (17095/19328)
Epoch: 70 | Batch_idx: 160 |  Loss: (0.3343) |  Loss2: (0.0000) | Acc: (88.00%) (18221/20608)
Epoch: 70 | Batch_idx: 170 |  Loss: (0.3341) |  Loss2: (0.0000) | Acc: (88.00%) (19358/21888)
Epoch: 70 | Batch_idx: 180 |  Loss: (0.3324) |  Loss2: (0.0000) | Acc: (88.00%) (20512/23168)
Epoch: 70 | Batch_idx: 190 |  Loss: (0.3321) |  Loss2: (0.0000) | Acc: (88.00%) (21645/24448)
Epoch: 70 | Batch_idx: 200 |  Loss: (0.3336) |  Loss2: (0.0000) | Acc: (88.00%) (22784/25728)
Epoch: 70 | Batch_idx: 210 |  Loss: (0.3341) |  Loss2: (0.0000) | Acc: (88.00%) (23904/27008)
Epoch: 70 | Batch_idx: 220 |  Loss: (0.3358) |  Loss2: (0.0000) | Acc: (88.00%) (25004/28288)
Epoch: 70 | Batch_idx: 230 |  Loss: (0.3377) |  Loss2: (0.0000) | Acc: (88.00%) (26118/29568)
Epoch: 70 | Batch_idx: 240 |  Loss: (0.3377) |  Loss2: (0.0000) | Acc: (88.00%) (27256/30848)
Epoch: 70 | Batch_idx: 250 |  Loss: (0.3374) |  Loss2: (0.0000) | Acc: (88.00%) (28397/32128)
Epoch: 70 | Batch_idx: 260 |  Loss: (0.3377) |  Loss2: (0.0000) | Acc: (88.00%) (29538/33408)
Epoch: 70 | Batch_idx: 270 |  Loss: (0.3390) |  Loss2: (0.0000) | Acc: (88.00%) (30644/34688)
Epoch: 70 | Batch_idx: 280 |  Loss: (0.3389) |  Loss2: (0.0000) | Acc: (88.00%) (31782/35968)
Epoch: 70 | Batch_idx: 290 |  Loss: (0.3388) |  Loss2: (0.0000) | Acc: (88.00%) (32918/37248)
Epoch: 70 | Batch_idx: 300 |  Loss: (0.3387) |  Loss2: (0.0000) | Acc: (88.00%) (34058/38528)
Epoch: 70 | Batch_idx: 310 |  Loss: (0.3378) |  Loss2: (0.0000) | Acc: (88.00%) (35209/39808)
Epoch: 70 | Batch_idx: 320 |  Loss: (0.3384) |  Loss2: (0.0000) | Acc: (88.00%) (36327/41088)
Epoch: 70 | Batch_idx: 330 |  Loss: (0.3389) |  Loss2: (0.0000) | Acc: (88.00%) (37446/42368)
Epoch: 70 | Batch_idx: 340 |  Loss: (0.3385) |  Loss2: (0.0000) | Acc: (88.00%) (38573/43648)
Epoch: 70 | Batch_idx: 350 |  Loss: (0.3380) |  Loss2: (0.0000) | Acc: (88.00%) (39721/44928)
Epoch: 70 | Batch_idx: 360 |  Loss: (0.3384) |  Loss2: (0.0000) | Acc: (88.00%) (40850/46208)
Epoch: 70 | Batch_idx: 370 |  Loss: (0.3375) |  Loss2: (0.0000) | Acc: (88.00%) (42003/47488)
Epoch: 70 | Batch_idx: 380 |  Loss: (0.3377) |  Loss2: (0.0000) | Acc: (88.00%) (43128/48768)
Epoch: 70 | Batch_idx: 390 |  Loss: (0.3381) |  Loss2: (0.0000) | Acc: (88.00%) (44201/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_070.pth.tar'
# TEST : Loss: (0.4518) | Acc: (84.00%) (8476/10000)
percent tensor([0.5119, 0.4881], device='cuda:0')
percent tensor([0.4611, 0.5389], device='cuda:0')
percent tensor([0.5471, 0.4529], device='cuda:0')
percent tensor([0.5713, 0.4287], device='cuda:0')
percent tensor([0.5333, 0.4667], device='cuda:0')
percent tensor([0.6363, 0.3637], device='cuda:0')
percent tensor([0.6017, 0.3983], device='cuda:0')
percent tensor([0.9982, 0.0018], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(177.3239, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(805.8654, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(796.7286, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1519.0846, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(508.0699, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2208.5417, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4284.1353, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1409.1840, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6127.8037, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11981.9990, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3975.5818, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16846.3320, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 71 | Batch_idx: 0 |  Loss: (0.4006) |  Loss2: (0.0000) | Acc: (85.00%) (110/128)
Epoch: 71 | Batch_idx: 10 |  Loss: (0.3167) |  Loss2: (0.0000) | Acc: (88.00%) (1245/1408)
Epoch: 71 | Batch_idx: 20 |  Loss: (0.3252) |  Loss2: (0.0000) | Acc: (87.00%) (2363/2688)
Epoch: 71 | Batch_idx: 30 |  Loss: (0.3198) |  Loss2: (0.0000) | Acc: (88.00%) (3508/3968)
Epoch: 71 | Batch_idx: 40 |  Loss: (0.3151) |  Loss2: (0.0000) | Acc: (88.00%) (4658/5248)
Epoch: 71 | Batch_idx: 50 |  Loss: (0.3224) |  Loss2: (0.0000) | Acc: (88.00%) (5781/6528)
Epoch: 71 | Batch_idx: 60 |  Loss: (0.3264) |  Loss2: (0.0000) | Acc: (88.00%) (6911/7808)
Epoch: 71 | Batch_idx: 70 |  Loss: (0.3203) |  Loss2: (0.0000) | Acc: (88.00%) (8066/9088)
Epoch: 71 | Batch_idx: 80 |  Loss: (0.3167) |  Loss2: (0.0000) | Acc: (88.00%) (9222/10368)
Epoch: 71 | Batch_idx: 90 |  Loss: (0.3182) |  Loss2: (0.0000) | Acc: (88.00%) (10346/11648)
Epoch: 71 | Batch_idx: 100 |  Loss: (0.3179) |  Loss2: (0.0000) | Acc: (88.00%) (11479/12928)
Epoch: 71 | Batch_idx: 110 |  Loss: (0.3191) |  Loss2: (0.0000) | Acc: (88.00%) (12609/14208)
Epoch: 71 | Batch_idx: 120 |  Loss: (0.3182) |  Loss2: (0.0000) | Acc: (88.00%) (13757/15488)
Epoch: 71 | Batch_idx: 130 |  Loss: (0.3178) |  Loss2: (0.0000) | Acc: (88.00%) (14916/16768)
Epoch: 71 | Batch_idx: 140 |  Loss: (0.3171) |  Loss2: (0.0000) | Acc: (89.00%) (16066/18048)
Epoch: 71 | Batch_idx: 150 |  Loss: (0.3190) |  Loss2: (0.0000) | Acc: (88.00%) (17197/19328)
Epoch: 71 | Batch_idx: 160 |  Loss: (0.3195) |  Loss2: (0.0000) | Acc: (89.00%) (18342/20608)
Epoch: 71 | Batch_idx: 170 |  Loss: (0.3204) |  Loss2: (0.0000) | Acc: (88.00%) (19479/21888)
Epoch: 71 | Batch_idx: 180 |  Loss: (0.3204) |  Loss2: (0.0000) | Acc: (88.00%) (20612/23168)
Epoch: 71 | Batch_idx: 190 |  Loss: (0.3199) |  Loss2: (0.0000) | Acc: (89.00%) (21762/24448)
Epoch: 71 | Batch_idx: 200 |  Loss: (0.3194) |  Loss2: (0.0000) | Acc: (89.00%) (22905/25728)
Epoch: 71 | Batch_idx: 210 |  Loss: (0.3204) |  Loss2: (0.0000) | Acc: (88.00%) (24034/27008)
Epoch: 71 | Batch_idx: 220 |  Loss: (0.3194) |  Loss2: (0.0000) | Acc: (89.00%) (25195/28288)
Epoch: 71 | Batch_idx: 230 |  Loss: (0.3199) |  Loss2: (0.0000) | Acc: (89.00%) (26332/29568)
Epoch: 71 | Batch_idx: 240 |  Loss: (0.3213) |  Loss2: (0.0000) | Acc: (88.00%) (27454/30848)
Epoch: 71 | Batch_idx: 250 |  Loss: (0.3236) |  Loss2: (0.0000) | Acc: (88.00%) (28571/32128)
Epoch: 71 | Batch_idx: 260 |  Loss: (0.3245) |  Loss2: (0.0000) | Acc: (88.00%) (29711/33408)
Epoch: 71 | Batch_idx: 270 |  Loss: (0.3258) |  Loss2: (0.0000) | Acc: (88.00%) (30839/34688)
Epoch: 71 | Batch_idx: 280 |  Loss: (0.3267) |  Loss2: (0.0000) | Acc: (88.00%) (31966/35968)
Epoch: 71 | Batch_idx: 290 |  Loss: (0.3262) |  Loss2: (0.0000) | Acc: (88.00%) (33108/37248)
Epoch: 71 | Batch_idx: 300 |  Loss: (0.3256) |  Loss2: (0.0000) | Acc: (88.00%) (34262/38528)
Epoch: 71 | Batch_idx: 310 |  Loss: (0.3260) |  Loss2: (0.0000) | Acc: (88.00%) (35381/39808)
Epoch: 71 | Batch_idx: 320 |  Loss: (0.3248) |  Loss2: (0.0000) | Acc: (88.00%) (36536/41088)
Epoch: 71 | Batch_idx: 330 |  Loss: (0.3247) |  Loss2: (0.0000) | Acc: (88.00%) (37675/42368)
Epoch: 71 | Batch_idx: 340 |  Loss: (0.3252) |  Loss2: (0.0000) | Acc: (88.00%) (38801/43648)
Epoch: 71 | Batch_idx: 350 |  Loss: (0.3255) |  Loss2: (0.0000) | Acc: (88.00%) (39937/44928)
Epoch: 71 | Batch_idx: 360 |  Loss: (0.3247) |  Loss2: (0.0000) | Acc: (88.00%) (41085/46208)
Epoch: 71 | Batch_idx: 370 |  Loss: (0.3268) |  Loss2: (0.0000) | Acc: (88.00%) (42182/47488)
Epoch: 71 | Batch_idx: 380 |  Loss: (0.3267) |  Loss2: (0.0000) | Acc: (88.00%) (43310/48768)
Epoch: 71 | Batch_idx: 390 |  Loss: (0.3270) |  Loss2: (0.0000) | Acc: (88.00%) (44392/50000)
# TEST : Loss: (0.4978) | Acc: (83.00%) (8364/10000)
percent tensor([0.5121, 0.4879], device='cuda:0')
percent tensor([0.4609, 0.5391], device='cuda:0')
percent tensor([0.5455, 0.4545], device='cuda:0')
percent tensor([0.5702, 0.4298], device='cuda:0')
percent tensor([0.5328, 0.4672], device='cuda:0')
percent tensor([0.6328, 0.3672], device='cuda:0')
percent tensor([0.6004, 0.3996], device='cuda:0')
percent tensor([0.9981, 0.0019], device='cuda:0')
Epoch: 72 | Batch_idx: 0 |  Loss: (0.4639) |  Loss2: (0.0000) | Acc: (86.00%) (111/128)
Epoch: 72 | Batch_idx: 10 |  Loss: (0.3406) |  Loss2: (0.0000) | Acc: (88.00%) (1244/1408)
Epoch: 72 | Batch_idx: 20 |  Loss: (0.3209) |  Loss2: (0.0000) | Acc: (89.00%) (2398/2688)
Epoch: 72 | Batch_idx: 30 |  Loss: (0.3153) |  Loss2: (0.0000) | Acc: (89.00%) (3537/3968)
Epoch: 72 | Batch_idx: 40 |  Loss: (0.3179) |  Loss2: (0.0000) | Acc: (89.00%) (4678/5248)
Epoch: 72 | Batch_idx: 50 |  Loss: (0.3122) |  Loss2: (0.0000) | Acc: (89.00%) (5844/6528)
Epoch: 72 | Batch_idx: 60 |  Loss: (0.3101) |  Loss2: (0.0000) | Acc: (89.00%) (6994/7808)
Epoch: 72 | Batch_idx: 70 |  Loss: (0.3109) |  Loss2: (0.0000) | Acc: (89.00%) (8128/9088)
Epoch: 72 | Batch_idx: 80 |  Loss: (0.3077) |  Loss2: (0.0000) | Acc: (89.00%) (9274/10368)
Epoch: 72 | Batch_idx: 90 |  Loss: (0.3084) |  Loss2: (0.0000) | Acc: (89.00%) (10413/11648)
Epoch: 72 | Batch_idx: 100 |  Loss: (0.3103) |  Loss2: (0.0000) | Acc: (89.00%) (11559/12928)
Epoch: 72 | Batch_idx: 110 |  Loss: (0.3132) |  Loss2: (0.0000) | Acc: (89.00%) (12687/14208)
Epoch: 72 | Batch_idx: 120 |  Loss: (0.3135) |  Loss2: (0.0000) | Acc: (89.00%) (13824/15488)
Epoch: 72 | Batch_idx: 130 |  Loss: (0.3141) |  Loss2: (0.0000) | Acc: (89.00%) (14968/16768)
Epoch: 72 | Batch_idx: 140 |  Loss: (0.3161) |  Loss2: (0.0000) | Acc: (89.00%) (16111/18048)
Epoch: 72 | Batch_idx: 150 |  Loss: (0.3150) |  Loss2: (0.0000) | Acc: (89.00%) (17262/19328)
Epoch: 72 | Batch_idx: 160 |  Loss: (0.3138) |  Loss2: (0.0000) | Acc: (89.00%) (18406/20608)
Epoch: 72 | Batch_idx: 170 |  Loss: (0.3152) |  Loss2: (0.0000) | Acc: (89.00%) (19556/21888)
Epoch: 72 | Batch_idx: 180 |  Loss: (0.3140) |  Loss2: (0.0000) | Acc: (89.00%) (20704/23168)
Epoch: 72 | Batch_idx: 190 |  Loss: (0.3140) |  Loss2: (0.0000) | Acc: (89.00%) (21842/24448)
Epoch: 72 | Batch_idx: 200 |  Loss: (0.3158) |  Loss2: (0.0000) | Acc: (89.00%) (22969/25728)
Epoch: 72 | Batch_idx: 210 |  Loss: (0.3162) |  Loss2: (0.0000) | Acc: (89.00%) (24103/27008)
Epoch: 72 | Batch_idx: 220 |  Loss: (0.3159) |  Loss2: (0.0000) | Acc: (89.00%) (25244/28288)
Epoch: 72 | Batch_idx: 230 |  Loss: (0.3157) |  Loss2: (0.0000) | Acc: (89.00%) (26385/29568)
Epoch: 72 | Batch_idx: 240 |  Loss: (0.3157) |  Loss2: (0.0000) | Acc: (89.00%) (27518/30848)
Epoch: 72 | Batch_idx: 250 |  Loss: (0.3151) |  Loss2: (0.0000) | Acc: (89.00%) (28668/32128)
Epoch: 72 | Batch_idx: 260 |  Loss: (0.3157) |  Loss2: (0.0000) | Acc: (89.00%) (29801/33408)
Epoch: 72 | Batch_idx: 270 |  Loss: (0.3169) |  Loss2: (0.0000) | Acc: (89.00%) (30933/34688)
Epoch: 72 | Batch_idx: 280 |  Loss: (0.3164) |  Loss2: (0.0000) | Acc: (89.00%) (32075/35968)
Epoch: 72 | Batch_idx: 290 |  Loss: (0.3163) |  Loss2: (0.0000) | Acc: (89.00%) (33210/37248)
Epoch: 72 | Batch_idx: 300 |  Loss: (0.3154) |  Loss2: (0.0000) | Acc: (89.00%) (34370/38528)
Epoch: 72 | Batch_idx: 310 |  Loss: (0.3175) |  Loss2: (0.0000) | Acc: (89.00%) (35491/39808)
Epoch: 72 | Batch_idx: 320 |  Loss: (0.3176) |  Loss2: (0.0000) | Acc: (89.00%) (36615/41088)
Epoch: 72 | Batch_idx: 330 |  Loss: (0.3175) |  Loss2: (0.0000) | Acc: (89.00%) (37758/42368)
Epoch: 72 | Batch_idx: 340 |  Loss: (0.3178) |  Loss2: (0.0000) | Acc: (89.00%) (38893/43648)
Epoch: 72 | Batch_idx: 350 |  Loss: (0.3184) |  Loss2: (0.0000) | Acc: (89.00%) (40022/44928)
Epoch: 72 | Batch_idx: 360 |  Loss: (0.3190) |  Loss2: (0.0000) | Acc: (89.00%) (41141/46208)
Epoch: 72 | Batch_idx: 370 |  Loss: (0.3185) |  Loss2: (0.0000) | Acc: (89.00%) (42281/47488)
Epoch: 72 | Batch_idx: 380 |  Loss: (0.3185) |  Loss2: (0.0000) | Acc: (89.00%) (43432/48768)
Epoch: 72 | Batch_idx: 390 |  Loss: (0.3182) |  Loss2: (0.0000) | Acc: (89.00%) (44524/50000)
# TEST : Loss: (0.5010) | Acc: (83.00%) (8387/10000)
percent tensor([0.5120, 0.4880], device='cuda:0')
percent tensor([0.4608, 0.5392], device='cuda:0')
percent tensor([0.5473, 0.4527], device='cuda:0')
percent tensor([0.5712, 0.4288], device='cuda:0')
percent tensor([0.5326, 0.4674], device='cuda:0')
percent tensor([0.6343, 0.3657], device='cuda:0')
percent tensor([0.6001, 0.3999], device='cuda:0')
percent tensor([0.9981, 0.0019], device='cuda:0')
Epoch: 73 | Batch_idx: 0 |  Loss: (0.2640) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 73 | Batch_idx: 10 |  Loss: (0.3226) |  Loss2: (0.0000) | Acc: (89.00%) (1264/1408)
Epoch: 73 | Batch_idx: 20 |  Loss: (0.3188) |  Loss2: (0.0000) | Acc: (89.00%) (2408/2688)
Epoch: 73 | Batch_idx: 30 |  Loss: (0.3269) |  Loss2: (0.0000) | Acc: (89.00%) (3537/3968)
Epoch: 73 | Batch_idx: 40 |  Loss: (0.3219) |  Loss2: (0.0000) | Acc: (89.00%) (4678/5248)
Epoch: 73 | Batch_idx: 50 |  Loss: (0.3114) |  Loss2: (0.0000) | Acc: (89.00%) (5831/6528)
Epoch: 73 | Batch_idx: 60 |  Loss: (0.3119) |  Loss2: (0.0000) | Acc: (89.00%) (6970/7808)
Epoch: 73 | Batch_idx: 70 |  Loss: (0.3150) |  Loss2: (0.0000) | Acc: (89.00%) (8108/9088)
Epoch: 73 | Batch_idx: 80 |  Loss: (0.3132) |  Loss2: (0.0000) | Acc: (89.00%) (9250/10368)
Epoch: 73 | Batch_idx: 90 |  Loss: (0.3138) |  Loss2: (0.0000) | Acc: (89.00%) (10390/11648)
Epoch: 73 | Batch_idx: 100 |  Loss: (0.3158) |  Loss2: (0.0000) | Acc: (89.00%) (11531/12928)
Epoch: 73 | Batch_idx: 110 |  Loss: (0.3133) |  Loss2: (0.0000) | Acc: (89.00%) (12697/14208)
Epoch: 73 | Batch_idx: 120 |  Loss: (0.3109) |  Loss2: (0.0000) | Acc: (89.00%) (13843/15488)
Epoch: 73 | Batch_idx: 130 |  Loss: (0.3109) |  Loss2: (0.0000) | Acc: (89.00%) (14984/16768)
Epoch: 73 | Batch_idx: 140 |  Loss: (0.3126) |  Loss2: (0.0000) | Acc: (89.00%) (16131/18048)
Epoch: 73 | Batch_idx: 150 |  Loss: (0.3121) |  Loss2: (0.0000) | Acc: (89.00%) (17284/19328)
Epoch: 73 | Batch_idx: 160 |  Loss: (0.3131) |  Loss2: (0.0000) | Acc: (89.00%) (18417/20608)
Epoch: 73 | Batch_idx: 170 |  Loss: (0.3147) |  Loss2: (0.0000) | Acc: (89.00%) (19559/21888)
Epoch: 73 | Batch_idx: 180 |  Loss: (0.3149) |  Loss2: (0.0000) | Acc: (89.00%) (20697/23168)
Epoch: 73 | Batch_idx: 190 |  Loss: (0.3148) |  Loss2: (0.0000) | Acc: (89.00%) (21840/24448)
Epoch: 73 | Batch_idx: 200 |  Loss: (0.3148) |  Loss2: (0.0000) | Acc: (89.00%) (22977/25728)
Epoch: 73 | Batch_idx: 210 |  Loss: (0.3158) |  Loss2: (0.0000) | Acc: (89.00%) (24110/27008)
Epoch: 73 | Batch_idx: 220 |  Loss: (0.3181) |  Loss2: (0.0000) | Acc: (89.00%) (25221/28288)
Epoch: 73 | Batch_idx: 230 |  Loss: (0.3176) |  Loss2: (0.0000) | Acc: (89.00%) (26364/29568)
Epoch: 73 | Batch_idx: 240 |  Loss: (0.3167) |  Loss2: (0.0000) | Acc: (89.00%) (27506/30848)
Epoch: 73 | Batch_idx: 250 |  Loss: (0.3159) |  Loss2: (0.0000) | Acc: (89.00%) (28651/32128)
Epoch: 73 | Batch_idx: 260 |  Loss: (0.3155) |  Loss2: (0.0000) | Acc: (89.00%) (29798/33408)
Epoch: 73 | Batch_idx: 270 |  Loss: (0.3158) |  Loss2: (0.0000) | Acc: (89.00%) (30937/34688)
Epoch: 73 | Batch_idx: 280 |  Loss: (0.3166) |  Loss2: (0.0000) | Acc: (89.00%) (32076/35968)
Epoch: 73 | Batch_idx: 290 |  Loss: (0.3156) |  Loss2: (0.0000) | Acc: (89.00%) (33221/37248)
Epoch: 73 | Batch_idx: 300 |  Loss: (0.3152) |  Loss2: (0.0000) | Acc: (89.00%) (34365/38528)
Epoch: 73 | Batch_idx: 310 |  Loss: (0.3144) |  Loss2: (0.0000) | Acc: (89.00%) (35514/39808)
Epoch: 73 | Batch_idx: 320 |  Loss: (0.3136) |  Loss2: (0.0000) | Acc: (89.00%) (36672/41088)
Epoch: 73 | Batch_idx: 330 |  Loss: (0.3133) |  Loss2: (0.0000) | Acc: (89.00%) (37815/42368)
Epoch: 73 | Batch_idx: 340 |  Loss: (0.3135) |  Loss2: (0.0000) | Acc: (89.00%) (38945/43648)
Epoch: 73 | Batch_idx: 350 |  Loss: (0.3139) |  Loss2: (0.0000) | Acc: (89.00%) (40094/44928)
Epoch: 73 | Batch_idx: 360 |  Loss: (0.3142) |  Loss2: (0.0000) | Acc: (89.00%) (41224/46208)
Epoch: 73 | Batch_idx: 370 |  Loss: (0.3140) |  Loss2: (0.0000) | Acc: (89.00%) (42370/47488)
Epoch: 73 | Batch_idx: 380 |  Loss: (0.3134) |  Loss2: (0.0000) | Acc: (89.00%) (43528/48768)
Epoch: 73 | Batch_idx: 390 |  Loss: (0.3129) |  Loss2: (0.0000) | Acc: (89.00%) (44631/50000)
# TEST : Loss: (0.4258) | Acc: (85.00%) (8592/10000)
percent tensor([0.5123, 0.4877], device='cuda:0')
percent tensor([0.4611, 0.5389], device='cuda:0')
percent tensor([0.5468, 0.4532], device='cuda:0')
percent tensor([0.5702, 0.4298], device='cuda:0')
percent tensor([0.5332, 0.4668], device='cuda:0')
percent tensor([0.6328, 0.3672], device='cuda:0')
percent tensor([0.5998, 0.4002], device='cuda:0')
percent tensor([0.9982, 0.0018], device='cuda:0')
Epoch: 74 | Batch_idx: 0 |  Loss: (0.4561) |  Loss2: (0.0000) | Acc: (87.00%) (112/128)
Epoch: 74 | Batch_idx: 10 |  Loss: (0.3174) |  Loss2: (0.0000) | Acc: (89.00%) (1261/1408)
Epoch: 74 | Batch_idx: 20 |  Loss: (0.3100) |  Loss2: (0.0000) | Acc: (89.00%) (2401/2688)
Epoch: 74 | Batch_idx: 30 |  Loss: (0.3036) |  Loss2: (0.0000) | Acc: (89.00%) (3545/3968)
Epoch: 74 | Batch_idx: 40 |  Loss: (0.3018) |  Loss2: (0.0000) | Acc: (89.00%) (4697/5248)
Epoch: 74 | Batch_idx: 50 |  Loss: (0.3061) |  Loss2: (0.0000) | Acc: (89.00%) (5835/6528)
Epoch: 74 | Batch_idx: 60 |  Loss: (0.3036) |  Loss2: (0.0000) | Acc: (89.00%) (6990/7808)
Epoch: 74 | Batch_idx: 70 |  Loss: (0.3076) |  Loss2: (0.0000) | Acc: (89.00%) (8114/9088)
Epoch: 74 | Batch_idx: 80 |  Loss: (0.3118) |  Loss2: (0.0000) | Acc: (89.00%) (9248/10368)
Epoch: 74 | Batch_idx: 90 |  Loss: (0.3113) |  Loss2: (0.0000) | Acc: (89.00%) (10378/11648)
Epoch: 74 | Batch_idx: 100 |  Loss: (0.3097) |  Loss2: (0.0000) | Acc: (89.00%) (11531/12928)
Epoch: 74 | Batch_idx: 110 |  Loss: (0.3062) |  Loss2: (0.0000) | Acc: (89.00%) (12692/14208)
Epoch: 74 | Batch_idx: 120 |  Loss: (0.3047) |  Loss2: (0.0000) | Acc: (89.00%) (13848/15488)
Epoch: 74 | Batch_idx: 130 |  Loss: (0.3037) |  Loss2: (0.0000) | Acc: (89.00%) (15017/16768)
Epoch: 74 | Batch_idx: 140 |  Loss: (0.3026) |  Loss2: (0.0000) | Acc: (89.00%) (16169/18048)
Epoch: 74 | Batch_idx: 150 |  Loss: (0.3011) |  Loss2: (0.0000) | Acc: (89.00%) (17318/19328)
Epoch: 74 | Batch_idx: 160 |  Loss: (0.2986) |  Loss2: (0.0000) | Acc: (89.00%) (18476/20608)
Epoch: 74 | Batch_idx: 170 |  Loss: (0.2968) |  Loss2: (0.0000) | Acc: (89.00%) (19639/21888)
Epoch: 74 | Batch_idx: 180 |  Loss: (0.2958) |  Loss2: (0.0000) | Acc: (89.00%) (20804/23168)
Epoch: 74 | Batch_idx: 190 |  Loss: (0.2954) |  Loss2: (0.0000) | Acc: (89.00%) (21951/24448)
Epoch: 74 | Batch_idx: 200 |  Loss: (0.2959) |  Loss2: (0.0000) | Acc: (89.00%) (23097/25728)
Epoch: 74 | Batch_idx: 210 |  Loss: (0.2953) |  Loss2: (0.0000) | Acc: (89.00%) (24245/27008)
Epoch: 74 | Batch_idx: 220 |  Loss: (0.2955) |  Loss2: (0.0000) | Acc: (89.00%) (25396/28288)
Epoch: 74 | Batch_idx: 230 |  Loss: (0.2954) |  Loss2: (0.0000) | Acc: (89.00%) (26543/29568)
Epoch: 74 | Batch_idx: 240 |  Loss: (0.2950) |  Loss2: (0.0000) | Acc: (89.00%) (27703/30848)
Epoch: 74 | Batch_idx: 250 |  Loss: (0.2960) |  Loss2: (0.0000) | Acc: (89.00%) (28841/32128)
Epoch: 74 | Batch_idx: 260 |  Loss: (0.2969) |  Loss2: (0.0000) | Acc: (89.00%) (29986/33408)
Epoch: 74 | Batch_idx: 270 |  Loss: (0.2976) |  Loss2: (0.0000) | Acc: (89.00%) (31137/34688)
Epoch: 74 | Batch_idx: 280 |  Loss: (0.2983) |  Loss2: (0.0000) | Acc: (89.00%) (32271/35968)
Epoch: 74 | Batch_idx: 290 |  Loss: (0.2977) |  Loss2: (0.0000) | Acc: (89.00%) (33425/37248)
Epoch: 74 | Batch_idx: 300 |  Loss: (0.2976) |  Loss2: (0.0000) | Acc: (89.00%) (34580/38528)
Epoch: 74 | Batch_idx: 310 |  Loss: (0.2979) |  Loss2: (0.0000) | Acc: (89.00%) (35732/39808)
Epoch: 74 | Batch_idx: 320 |  Loss: (0.2982) |  Loss2: (0.0000) | Acc: (89.00%) (36888/41088)
Epoch: 74 | Batch_idx: 330 |  Loss: (0.2989) |  Loss2: (0.0000) | Acc: (89.00%) (38038/42368)
Epoch: 74 | Batch_idx: 340 |  Loss: (0.2997) |  Loss2: (0.0000) | Acc: (89.00%) (39176/43648)
Epoch: 74 | Batch_idx: 350 |  Loss: (0.3010) |  Loss2: (0.0000) | Acc: (89.00%) (40305/44928)
Epoch: 74 | Batch_idx: 360 |  Loss: (0.3010) |  Loss2: (0.0000) | Acc: (89.00%) (41452/46208)
Epoch: 74 | Batch_idx: 370 |  Loss: (0.3011) |  Loss2: (0.0000) | Acc: (89.00%) (42597/47488)
Epoch: 74 | Batch_idx: 380 |  Loss: (0.3005) |  Loss2: (0.0000) | Acc: (89.00%) (43756/48768)
Epoch: 74 | Batch_idx: 390 |  Loss: (0.3013) |  Loss2: (0.0000) | Acc: (89.00%) (44848/50000)
# TEST : Loss: (0.4709) | Acc: (84.00%) (8456/10000)
percent tensor([0.5122, 0.4878], device='cuda:0')
percent tensor([0.4621, 0.5379], device='cuda:0')
percent tensor([0.5451, 0.4549], device='cuda:0')
percent tensor([0.5692, 0.4308], device='cuda:0')
percent tensor([0.5323, 0.4677], device='cuda:0')
percent tensor([0.6326, 0.3674], device='cuda:0')
percent tensor([0.5989, 0.4011], device='cuda:0')
percent tensor([0.9980, 0.0020], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 75 | Batch_idx: 0 |  Loss: (0.3293) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 75 | Batch_idx: 10 |  Loss: (0.3560) |  Loss2: (0.0000) | Acc: (87.00%) (1227/1408)
Epoch: 75 | Batch_idx: 20 |  Loss: (0.3713) |  Loss2: (0.0000) | Acc: (86.00%) (2337/2688)
Epoch: 75 | Batch_idx: 30 |  Loss: (0.3786) |  Loss2: (0.0000) | Acc: (86.00%) (3433/3968)
Epoch: 75 | Batch_idx: 40 |  Loss: (0.3672) |  Loss2: (0.0000) | Acc: (86.00%) (4560/5248)
Epoch: 75 | Batch_idx: 50 |  Loss: (0.3674) |  Loss2: (0.0000) | Acc: (86.00%) (5672/6528)
Epoch: 75 | Batch_idx: 60 |  Loss: (0.3736) |  Loss2: (0.0000) | Acc: (86.00%) (6756/7808)
Epoch: 75 | Batch_idx: 70 |  Loss: (0.3790) |  Loss2: (0.0000) | Acc: (86.00%) (7846/9088)
Epoch: 75 | Batch_idx: 80 |  Loss: (0.3784) |  Loss2: (0.0000) | Acc: (86.00%) (8961/10368)
Epoch: 75 | Batch_idx: 90 |  Loss: (0.3744) |  Loss2: (0.0000) | Acc: (86.00%) (10096/11648)
Epoch: 75 | Batch_idx: 100 |  Loss: (0.3732) |  Loss2: (0.0000) | Acc: (86.00%) (11216/12928)
Epoch: 75 | Batch_idx: 110 |  Loss: (0.3722) |  Loss2: (0.0000) | Acc: (86.00%) (12327/14208)
Epoch: 75 | Batch_idx: 120 |  Loss: (0.3744) |  Loss2: (0.0000) | Acc: (86.00%) (13428/15488)
Epoch: 75 | Batch_idx: 130 |  Loss: (0.3733) |  Loss2: (0.0000) | Acc: (86.00%) (14530/16768)
Epoch: 75 | Batch_idx: 140 |  Loss: (0.3727) |  Loss2: (0.0000) | Acc: (86.00%) (15638/18048)
Epoch: 75 | Batch_idx: 150 |  Loss: (0.3723) |  Loss2: (0.0000) | Acc: (86.00%) (16759/19328)
Epoch: 75 | Batch_idx: 160 |  Loss: (0.3726) |  Loss2: (0.0000) | Acc: (86.00%) (17874/20608)
Epoch: 75 | Batch_idx: 170 |  Loss: (0.3733) |  Loss2: (0.0000) | Acc: (86.00%) (18975/21888)
Epoch: 75 | Batch_idx: 180 |  Loss: (0.3736) |  Loss2: (0.0000) | Acc: (86.00%) (20085/23168)
Epoch: 75 | Batch_idx: 190 |  Loss: (0.3717) |  Loss2: (0.0000) | Acc: (86.00%) (21208/24448)
Epoch: 75 | Batch_idx: 200 |  Loss: (0.3718) |  Loss2: (0.0000) | Acc: (86.00%) (22325/25728)
Epoch: 75 | Batch_idx: 210 |  Loss: (0.3712) |  Loss2: (0.0000) | Acc: (86.00%) (23439/27008)
Epoch: 75 | Batch_idx: 220 |  Loss: (0.3698) |  Loss2: (0.0000) | Acc: (86.00%) (24570/28288)
Epoch: 75 | Batch_idx: 230 |  Loss: (0.3689) |  Loss2: (0.0000) | Acc: (86.00%) (25684/29568)
Epoch: 75 | Batch_idx: 240 |  Loss: (0.3682) |  Loss2: (0.0000) | Acc: (86.00%) (26824/30848)
Epoch: 75 | Batch_idx: 250 |  Loss: (0.3676) |  Loss2: (0.0000) | Acc: (86.00%) (27944/32128)
Epoch: 75 | Batch_idx: 260 |  Loss: (0.3664) |  Loss2: (0.0000) | Acc: (87.00%) (29081/33408)
Epoch: 75 | Batch_idx: 270 |  Loss: (0.3658) |  Loss2: (0.0000) | Acc: (87.00%) (30208/34688)
Epoch: 75 | Batch_idx: 280 |  Loss: (0.3655) |  Loss2: (0.0000) | Acc: (87.00%) (31320/35968)
Epoch: 75 | Batch_idx: 290 |  Loss: (0.3659) |  Loss2: (0.0000) | Acc: (87.00%) (32432/37248)
Epoch: 75 | Batch_idx: 300 |  Loss: (0.3653) |  Loss2: (0.0000) | Acc: (87.00%) (33544/38528)
Epoch: 75 | Batch_idx: 310 |  Loss: (0.3638) |  Loss2: (0.0000) | Acc: (87.00%) (34677/39808)
Epoch: 75 | Batch_idx: 320 |  Loss: (0.3631) |  Loss2: (0.0000) | Acc: (87.00%) (35806/41088)
Epoch: 75 | Batch_idx: 330 |  Loss: (0.3621) |  Loss2: (0.0000) | Acc: (87.00%) (36948/42368)
Epoch: 75 | Batch_idx: 340 |  Loss: (0.3612) |  Loss2: (0.0000) | Acc: (87.00%) (38070/43648)
Epoch: 75 | Batch_idx: 350 |  Loss: (0.3608) |  Loss2: (0.0000) | Acc: (87.00%) (39190/44928)
Epoch: 75 | Batch_idx: 360 |  Loss: (0.3596) |  Loss2: (0.0000) | Acc: (87.00%) (40336/46208)
Epoch: 75 | Batch_idx: 370 |  Loss: (0.3589) |  Loss2: (0.0000) | Acc: (87.00%) (41462/47488)
Epoch: 75 | Batch_idx: 380 |  Loss: (0.3581) |  Loss2: (0.0000) | Acc: (87.00%) (42605/48768)
Epoch: 75 | Batch_idx: 390 |  Loss: (0.3583) |  Loss2: (0.0000) | Acc: (87.00%) (43674/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_075.pth.tar'
# TEST : Loss: (0.4699) | Acc: (84.00%) (8433/10000)
percent tensor([0.5142, 0.4858], device='cuda:0')
percent tensor([0.4619, 0.5381], device='cuda:0')
percent tensor([0.5349, 0.4651], device='cuda:0')
percent tensor([0.5863, 0.4137], device='cuda:0')
percent tensor([0.5529, 0.4471], device='cuda:0')
percent tensor([0.6282, 0.3718], device='cuda:0')
percent tensor([0.5687, 0.4313], device='cuda:0')
percent tensor([0.9981, 0.0019], device='cuda:0')
Epoch: 76 | Batch_idx: 0 |  Loss: (0.4049) |  Loss2: (0.0000) | Acc: (86.00%) (111/128)
Epoch: 76 | Batch_idx: 10 |  Loss: (0.3466) |  Loss2: (0.0000) | Acc: (88.00%) (1240/1408)
Epoch: 76 | Batch_idx: 20 |  Loss: (0.3524) |  Loss2: (0.0000) | Acc: (87.00%) (2358/2688)
Epoch: 76 | Batch_idx: 30 |  Loss: (0.3451) |  Loss2: (0.0000) | Acc: (87.00%) (3486/3968)
Epoch: 76 | Batch_idx: 40 |  Loss: (0.3443) |  Loss2: (0.0000) | Acc: (87.00%) (4611/5248)
Epoch: 76 | Batch_idx: 50 |  Loss: (0.3429) |  Loss2: (0.0000) | Acc: (87.00%) (5741/6528)
Epoch: 76 | Batch_idx: 60 |  Loss: (0.3391) |  Loss2: (0.0000) | Acc: (88.00%) (6873/7808)
Epoch: 76 | Batch_idx: 70 |  Loss: (0.3361) |  Loss2: (0.0000) | Acc: (88.00%) (8006/9088)
Epoch: 76 | Batch_idx: 80 |  Loss: (0.3340) |  Loss2: (0.0000) | Acc: (88.00%) (9150/10368)
Epoch: 76 | Batch_idx: 90 |  Loss: (0.3352) |  Loss2: (0.0000) | Acc: (88.00%) (10277/11648)
Epoch: 76 | Batch_idx: 100 |  Loss: (0.3329) |  Loss2: (0.0000) | Acc: (88.00%) (11415/12928)
Epoch: 76 | Batch_idx: 110 |  Loss: (0.3340) |  Loss2: (0.0000) | Acc: (88.00%) (12546/14208)
Epoch: 76 | Batch_idx: 120 |  Loss: (0.3325) |  Loss2: (0.0000) | Acc: (88.00%) (13691/15488)
Epoch: 76 | Batch_idx: 130 |  Loss: (0.3328) |  Loss2: (0.0000) | Acc: (88.00%) (14823/16768)
Epoch: 76 | Batch_idx: 140 |  Loss: (0.3318) |  Loss2: (0.0000) | Acc: (88.00%) (15958/18048)
Epoch: 76 | Batch_idx: 150 |  Loss: (0.3303) |  Loss2: (0.0000) | Acc: (88.00%) (17103/19328)
Epoch: 76 | Batch_idx: 160 |  Loss: (0.3310) |  Loss2: (0.0000) | Acc: (88.00%) (18232/20608)
Epoch: 76 | Batch_idx: 170 |  Loss: (0.3299) |  Loss2: (0.0000) | Acc: (88.00%) (19371/21888)
Epoch: 76 | Batch_idx: 180 |  Loss: (0.3280) |  Loss2: (0.0000) | Acc: (88.00%) (20526/23168)
Epoch: 76 | Batch_idx: 190 |  Loss: (0.3281) |  Loss2: (0.0000) | Acc: (88.00%) (21657/24448)
Epoch: 76 | Batch_idx: 200 |  Loss: (0.3262) |  Loss2: (0.0000) | Acc: (88.00%) (22809/25728)
Epoch: 76 | Batch_idx: 210 |  Loss: (0.3277) |  Loss2: (0.0000) | Acc: (88.00%) (23939/27008)
Epoch: 76 | Batch_idx: 220 |  Loss: (0.3284) |  Loss2: (0.0000) | Acc: (88.00%) (25072/28288)
Epoch: 76 | Batch_idx: 230 |  Loss: (0.3289) |  Loss2: (0.0000) | Acc: (88.00%) (26197/29568)
Epoch: 76 | Batch_idx: 240 |  Loss: (0.3285) |  Loss2: (0.0000) | Acc: (88.00%) (27333/30848)
Epoch: 76 | Batch_idx: 250 |  Loss: (0.3269) |  Loss2: (0.0000) | Acc: (88.00%) (28491/32128)
Epoch: 76 | Batch_idx: 260 |  Loss: (0.3268) |  Loss2: (0.0000) | Acc: (88.00%) (29620/33408)
Epoch: 76 | Batch_idx: 270 |  Loss: (0.3256) |  Loss2: (0.0000) | Acc: (88.00%) (30775/34688)
Epoch: 76 | Batch_idx: 280 |  Loss: (0.3256) |  Loss2: (0.0000) | Acc: (88.00%) (31908/35968)
Epoch: 76 | Batch_idx: 290 |  Loss: (0.3247) |  Loss2: (0.0000) | Acc: (88.00%) (33062/37248)
Epoch: 76 | Batch_idx: 300 |  Loss: (0.3243) |  Loss2: (0.0000) | Acc: (88.00%) (34204/38528)
Epoch: 76 | Batch_idx: 310 |  Loss: (0.3240) |  Loss2: (0.0000) | Acc: (88.00%) (35333/39808)
Epoch: 76 | Batch_idx: 320 |  Loss: (0.3226) |  Loss2: (0.0000) | Acc: (88.00%) (36492/41088)
Epoch: 76 | Batch_idx: 330 |  Loss: (0.3221) |  Loss2: (0.0000) | Acc: (88.00%) (37634/42368)
Epoch: 76 | Batch_idx: 340 |  Loss: (0.3217) |  Loss2: (0.0000) | Acc: (88.00%) (38786/43648)
Epoch: 76 | Batch_idx: 350 |  Loss: (0.3215) |  Loss2: (0.0000) | Acc: (88.00%) (39917/44928)
Epoch: 76 | Batch_idx: 360 |  Loss: (0.3214) |  Loss2: (0.0000) | Acc: (88.00%) (41060/46208)
Epoch: 76 | Batch_idx: 370 |  Loss: (0.3213) |  Loss2: (0.0000) | Acc: (88.00%) (42208/47488)
Epoch: 76 | Batch_idx: 380 |  Loss: (0.3207) |  Loss2: (0.0000) | Acc: (88.00%) (43353/48768)
Epoch: 76 | Batch_idx: 390 |  Loss: (0.3208) |  Loss2: (0.0000) | Acc: (88.00%) (44446/50000)
# TEST : Loss: (0.4452) | Acc: (85.00%) (8524/10000)
percent tensor([0.5153, 0.4847], device='cuda:0')
percent tensor([0.4609, 0.5391], device='cuda:0')
percent tensor([0.5357, 0.4643], device='cuda:0')
percent tensor([0.5862, 0.4138], device='cuda:0')
percent tensor([0.5553, 0.4447], device='cuda:0')
percent tensor([0.6297, 0.3703], device='cuda:0')
percent tensor([0.5712, 0.4288], device='cuda:0')
percent tensor([0.9982, 0.0018], device='cuda:0')
Epoch: 77 | Batch_idx: 0 |  Loss: (0.4149) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 77 | Batch_idx: 10 |  Loss: (0.3154) |  Loss2: (0.0000) | Acc: (90.00%) (1268/1408)
Epoch: 77 | Batch_idx: 20 |  Loss: (0.3307) |  Loss2: (0.0000) | Acc: (89.00%) (2394/2688)
Epoch: 77 | Batch_idx: 30 |  Loss: (0.3266) |  Loss2: (0.0000) | Acc: (88.00%) (3519/3968)
Epoch: 77 | Batch_idx: 40 |  Loss: (0.3230) |  Loss2: (0.0000) | Acc: (88.00%) (4661/5248)
Epoch: 77 | Batch_idx: 50 |  Loss: (0.3169) |  Loss2: (0.0000) | Acc: (89.00%) (5827/6528)
Epoch: 77 | Batch_idx: 60 |  Loss: (0.3150) |  Loss2: (0.0000) | Acc: (89.00%) (6974/7808)
Epoch: 77 | Batch_idx: 70 |  Loss: (0.3137) |  Loss2: (0.0000) | Acc: (89.00%) (8114/9088)
Epoch: 77 | Batch_idx: 80 |  Loss: (0.3180) |  Loss2: (0.0000) | Acc: (89.00%) (9243/10368)
Epoch: 77 | Batch_idx: 90 |  Loss: (0.3193) |  Loss2: (0.0000) | Acc: (89.00%) (10376/11648)
Epoch: 77 | Batch_idx: 100 |  Loss: (0.3180) |  Loss2: (0.0000) | Acc: (89.00%) (11528/12928)
Epoch: 77 | Batch_idx: 110 |  Loss: (0.3146) |  Loss2: (0.0000) | Acc: (89.00%) (12689/14208)
Epoch: 77 | Batch_idx: 120 |  Loss: (0.3130) |  Loss2: (0.0000) | Acc: (89.00%) (13839/15488)
Epoch: 77 | Batch_idx: 130 |  Loss: (0.3120) |  Loss2: (0.0000) | Acc: (89.00%) (14995/16768)
Epoch: 77 | Batch_idx: 140 |  Loss: (0.3124) |  Loss2: (0.0000) | Acc: (89.00%) (16144/18048)
Epoch: 77 | Batch_idx: 150 |  Loss: (0.3101) |  Loss2: (0.0000) | Acc: (89.00%) (17299/19328)
Epoch: 77 | Batch_idx: 160 |  Loss: (0.3104) |  Loss2: (0.0000) | Acc: (89.00%) (18442/20608)
Epoch: 77 | Batch_idx: 170 |  Loss: (0.3105) |  Loss2: (0.0000) | Acc: (89.00%) (19589/21888)
Epoch: 77 | Batch_idx: 180 |  Loss: (0.3120) |  Loss2: (0.0000) | Acc: (89.00%) (20714/23168)
Epoch: 77 | Batch_idx: 190 |  Loss: (0.3130) |  Loss2: (0.0000) | Acc: (89.00%) (21849/24448)
Epoch: 77 | Batch_idx: 200 |  Loss: (0.3129) |  Loss2: (0.0000) | Acc: (89.00%) (22987/25728)
Epoch: 77 | Batch_idx: 210 |  Loss: (0.3124) |  Loss2: (0.0000) | Acc: (89.00%) (24143/27008)
Epoch: 77 | Batch_idx: 220 |  Loss: (0.3116) |  Loss2: (0.0000) | Acc: (89.00%) (25290/28288)
Epoch: 77 | Batch_idx: 230 |  Loss: (0.3114) |  Loss2: (0.0000) | Acc: (89.00%) (26439/29568)
Epoch: 77 | Batch_idx: 240 |  Loss: (0.3127) |  Loss2: (0.0000) | Acc: (89.00%) (27565/30848)
Epoch: 77 | Batch_idx: 250 |  Loss: (0.3115) |  Loss2: (0.0000) | Acc: (89.00%) (28720/32128)
Epoch: 77 | Batch_idx: 260 |  Loss: (0.3107) |  Loss2: (0.0000) | Acc: (89.00%) (29874/33408)
Epoch: 77 | Batch_idx: 270 |  Loss: (0.3099) |  Loss2: (0.0000) | Acc: (89.00%) (31030/34688)
Epoch: 77 | Batch_idx: 280 |  Loss: (0.3098) |  Loss2: (0.0000) | Acc: (89.00%) (32177/35968)
Epoch: 77 | Batch_idx: 290 |  Loss: (0.3087) |  Loss2: (0.0000) | Acc: (89.00%) (33341/37248)
Epoch: 77 | Batch_idx: 300 |  Loss: (0.3090) |  Loss2: (0.0000) | Acc: (89.00%) (34481/38528)
Epoch: 77 | Batch_idx: 310 |  Loss: (0.3092) |  Loss2: (0.0000) | Acc: (89.00%) (35618/39808)
Epoch: 77 | Batch_idx: 320 |  Loss: (0.3084) |  Loss2: (0.0000) | Acc: (89.00%) (36778/41088)
Epoch: 77 | Batch_idx: 330 |  Loss: (0.3077) |  Loss2: (0.0000) | Acc: (89.00%) (37939/42368)
Epoch: 77 | Batch_idx: 340 |  Loss: (0.3079) |  Loss2: (0.0000) | Acc: (89.00%) (39083/43648)
Epoch: 77 | Batch_idx: 350 |  Loss: (0.3070) |  Loss2: (0.0000) | Acc: (89.00%) (40235/44928)
Epoch: 77 | Batch_idx: 360 |  Loss: (0.3067) |  Loss2: (0.0000) | Acc: (89.00%) (41395/46208)
Epoch: 77 | Batch_idx: 370 |  Loss: (0.3066) |  Loss2: (0.0000) | Acc: (89.00%) (42540/47488)
Epoch: 77 | Batch_idx: 380 |  Loss: (0.3064) |  Loss2: (0.0000) | Acc: (89.00%) (43691/48768)
Epoch: 77 | Batch_idx: 390 |  Loss: (0.3061) |  Loss2: (0.0000) | Acc: (89.00%) (44798/50000)
# TEST : Loss: (0.4310) | Acc: (85.00%) (8577/10000)
percent tensor([0.5159, 0.4841], device='cuda:0')
percent tensor([0.4611, 0.5389], device='cuda:0')
percent tensor([0.5359, 0.4641], device='cuda:0')
percent tensor([0.5837, 0.4163], device='cuda:0')
percent tensor([0.5555, 0.4445], device='cuda:0')
percent tensor([0.6381, 0.3619], device='cuda:0')
percent tensor([0.5777, 0.4223], device='cuda:0')
percent tensor([0.9983, 0.0017], device='cuda:0')
Epoch: 78 | Batch_idx: 0 |  Loss: (0.3936) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 78 | Batch_idx: 10 |  Loss: (0.3450) |  Loss2: (0.0000) | Acc: (88.00%) (1248/1408)
Epoch: 78 | Batch_idx: 20 |  Loss: (0.3199) |  Loss2: (0.0000) | Acc: (89.00%) (2398/2688)
Epoch: 78 | Batch_idx: 30 |  Loss: (0.3293) |  Loss2: (0.0000) | Acc: (88.00%) (3514/3968)
Epoch: 78 | Batch_idx: 40 |  Loss: (0.3229) |  Loss2: (0.0000) | Acc: (88.00%) (4670/5248)
Epoch: 78 | Batch_idx: 50 |  Loss: (0.3159) |  Loss2: (0.0000) | Acc: (89.00%) (5820/6528)
Epoch: 78 | Batch_idx: 60 |  Loss: (0.3089) |  Loss2: (0.0000) | Acc: (89.00%) (6971/7808)
Epoch: 78 | Batch_idx: 70 |  Loss: (0.3073) |  Loss2: (0.0000) | Acc: (89.00%) (8140/9088)
Epoch: 78 | Batch_idx: 80 |  Loss: (0.3068) |  Loss2: (0.0000) | Acc: (89.00%) (9275/10368)
Epoch: 78 | Batch_idx: 90 |  Loss: (0.3072) |  Loss2: (0.0000) | Acc: (89.00%) (10417/11648)
Epoch: 78 | Batch_idx: 100 |  Loss: (0.3063) |  Loss2: (0.0000) | Acc: (89.00%) (11576/12928)
Epoch: 78 | Batch_idx: 110 |  Loss: (0.3039) |  Loss2: (0.0000) | Acc: (89.00%) (12730/14208)
Epoch: 78 | Batch_idx: 120 |  Loss: (0.3036) |  Loss2: (0.0000) | Acc: (89.00%) (13870/15488)
Epoch: 78 | Batch_idx: 130 |  Loss: (0.3017) |  Loss2: (0.0000) | Acc: (89.00%) (15024/16768)
Epoch: 78 | Batch_idx: 140 |  Loss: (0.3004) |  Loss2: (0.0000) | Acc: (89.00%) (16171/18048)
Epoch: 78 | Batch_idx: 150 |  Loss: (0.2991) |  Loss2: (0.0000) | Acc: (89.00%) (17328/19328)
Epoch: 78 | Batch_idx: 160 |  Loss: (0.2977) |  Loss2: (0.0000) | Acc: (89.00%) (18492/20608)
Epoch: 78 | Batch_idx: 170 |  Loss: (0.2987) |  Loss2: (0.0000) | Acc: (89.00%) (19627/21888)
Epoch: 78 | Batch_idx: 180 |  Loss: (0.3005) |  Loss2: (0.0000) | Acc: (89.00%) (20763/23168)
Epoch: 78 | Batch_idx: 190 |  Loss: (0.2999) |  Loss2: (0.0000) | Acc: (89.00%) (21920/24448)
Epoch: 78 | Batch_idx: 200 |  Loss: (0.3000) |  Loss2: (0.0000) | Acc: (89.00%) (23066/25728)
Epoch: 78 | Batch_idx: 210 |  Loss: (0.3004) |  Loss2: (0.0000) | Acc: (89.00%) (24208/27008)
Epoch: 78 | Batch_idx: 220 |  Loss: (0.2995) |  Loss2: (0.0000) | Acc: (89.00%) (25368/28288)
Epoch: 78 | Batch_idx: 230 |  Loss: (0.3003) |  Loss2: (0.0000) | Acc: (89.00%) (26512/29568)
Epoch: 78 | Batch_idx: 240 |  Loss: (0.2998) |  Loss2: (0.0000) | Acc: (89.00%) (27660/30848)
Epoch: 78 | Batch_idx: 250 |  Loss: (0.3000) |  Loss2: (0.0000) | Acc: (89.00%) (28817/32128)
Epoch: 78 | Batch_idx: 260 |  Loss: (0.2998) |  Loss2: (0.0000) | Acc: (89.00%) (29960/33408)
Epoch: 78 | Batch_idx: 270 |  Loss: (0.2994) |  Loss2: (0.0000) | Acc: (89.00%) (31116/34688)
Epoch: 78 | Batch_idx: 280 |  Loss: (0.2992) |  Loss2: (0.0000) | Acc: (89.00%) (32270/35968)
Epoch: 78 | Batch_idx: 290 |  Loss: (0.2991) |  Loss2: (0.0000) | Acc: (89.00%) (33426/37248)
Epoch: 78 | Batch_idx: 300 |  Loss: (0.2981) |  Loss2: (0.0000) | Acc: (89.00%) (34585/38528)
Epoch: 78 | Batch_idx: 310 |  Loss: (0.2978) |  Loss2: (0.0000) | Acc: (89.00%) (35739/39808)
Epoch: 78 | Batch_idx: 320 |  Loss: (0.2985) |  Loss2: (0.0000) | Acc: (89.00%) (36879/41088)
Epoch: 78 | Batch_idx: 330 |  Loss: (0.2979) |  Loss2: (0.0000) | Acc: (89.00%) (38030/42368)
Epoch: 78 | Batch_idx: 340 |  Loss: (0.2983) |  Loss2: (0.0000) | Acc: (89.00%) (39179/43648)
Epoch: 78 | Batch_idx: 350 |  Loss: (0.2985) |  Loss2: (0.0000) | Acc: (89.00%) (40336/44928)
Epoch: 78 | Batch_idx: 360 |  Loss: (0.2986) |  Loss2: (0.0000) | Acc: (89.00%) (41483/46208)
Epoch: 78 | Batch_idx: 370 |  Loss: (0.2990) |  Loss2: (0.0000) | Acc: (89.00%) (42620/47488)
Epoch: 78 | Batch_idx: 380 |  Loss: (0.2990) |  Loss2: (0.0000) | Acc: (89.00%) (43766/48768)
Epoch: 78 | Batch_idx: 390 |  Loss: (0.2991) |  Loss2: (0.0000) | Acc: (89.00%) (44872/50000)
# TEST : Loss: (0.4236) | Acc: (86.00%) (8615/10000)
percent tensor([0.5159, 0.4841], device='cuda:0')
percent tensor([0.4614, 0.5386], device='cuda:0')
percent tensor([0.5375, 0.4625], device='cuda:0')
percent tensor([0.5862, 0.4138], device='cuda:0')
percent tensor([0.5526, 0.4474], device='cuda:0')
percent tensor([0.6391, 0.3609], device='cuda:0')
percent tensor([0.5873, 0.4127], device='cuda:0')
percent tensor([0.9984, 0.0016], device='cuda:0')
Epoch: 79 | Batch_idx: 0 |  Loss: (0.2922) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 79 | Batch_idx: 10 |  Loss: (0.3072) |  Loss2: (0.0000) | Acc: (89.00%) (1256/1408)
Epoch: 79 | Batch_idx: 20 |  Loss: (0.3017) |  Loss2: (0.0000) | Acc: (89.00%) (2406/2688)
Epoch: 79 | Batch_idx: 30 |  Loss: (0.2902) |  Loss2: (0.0000) | Acc: (89.00%) (3568/3968)
Epoch: 79 | Batch_idx: 40 |  Loss: (0.2896) |  Loss2: (0.0000) | Acc: (90.00%) (4725/5248)
Epoch: 79 | Batch_idx: 50 |  Loss: (0.2909) |  Loss2: (0.0000) | Acc: (89.00%) (5868/6528)
Epoch: 79 | Batch_idx: 60 |  Loss: (0.2892) |  Loss2: (0.0000) | Acc: (90.00%) (7032/7808)
Epoch: 79 | Batch_idx: 70 |  Loss: (0.2886) |  Loss2: (0.0000) | Acc: (90.00%) (8184/9088)
Epoch: 79 | Batch_idx: 80 |  Loss: (0.2892) |  Loss2: (0.0000) | Acc: (90.00%) (9342/10368)
Epoch: 79 | Batch_idx: 90 |  Loss: (0.2893) |  Loss2: (0.0000) | Acc: (90.00%) (10496/11648)
Epoch: 79 | Batch_idx: 100 |  Loss: (0.2917) |  Loss2: (0.0000) | Acc: (90.00%) (11645/12928)
Epoch: 79 | Batch_idx: 110 |  Loss: (0.2939) |  Loss2: (0.0000) | Acc: (89.00%) (12775/14208)
Epoch: 79 | Batch_idx: 120 |  Loss: (0.2941) |  Loss2: (0.0000) | Acc: (89.00%) (13929/15488)
Epoch: 79 | Batch_idx: 130 |  Loss: (0.2945) |  Loss2: (0.0000) | Acc: (89.00%) (15076/16768)
Epoch: 79 | Batch_idx: 140 |  Loss: (0.2924) |  Loss2: (0.0000) | Acc: (90.00%) (16250/18048)
Epoch: 79 | Batch_idx: 150 |  Loss: (0.2937) |  Loss2: (0.0000) | Acc: (90.00%) (17406/19328)
Epoch: 79 | Batch_idx: 160 |  Loss: (0.2933) |  Loss2: (0.0000) | Acc: (90.00%) (18552/20608)
Epoch: 79 | Batch_idx: 170 |  Loss: (0.2926) |  Loss2: (0.0000) | Acc: (90.00%) (19705/21888)
Epoch: 79 | Batch_idx: 180 |  Loss: (0.2921) |  Loss2: (0.0000) | Acc: (89.00%) (20851/23168)
Epoch: 79 | Batch_idx: 190 |  Loss: (0.2920) |  Loss2: (0.0000) | Acc: (89.00%) (22001/24448)
Epoch: 79 | Batch_idx: 200 |  Loss: (0.2911) |  Loss2: (0.0000) | Acc: (90.00%) (23164/25728)
Epoch: 79 | Batch_idx: 210 |  Loss: (0.2890) |  Loss2: (0.0000) | Acc: (90.00%) (24343/27008)
Epoch: 79 | Batch_idx: 220 |  Loss: (0.2899) |  Loss2: (0.0000) | Acc: (90.00%) (25489/28288)
Epoch: 79 | Batch_idx: 230 |  Loss: (0.2895) |  Loss2: (0.0000) | Acc: (90.00%) (26640/29568)
Epoch: 79 | Batch_idx: 240 |  Loss: (0.2904) |  Loss2: (0.0000) | Acc: (90.00%) (27791/30848)
Epoch: 79 | Batch_idx: 250 |  Loss: (0.2898) |  Loss2: (0.0000) | Acc: (90.00%) (28956/32128)
Epoch: 79 | Batch_idx: 260 |  Loss: (0.2899) |  Loss2: (0.0000) | Acc: (90.00%) (30119/33408)
Epoch: 79 | Batch_idx: 270 |  Loss: (0.2906) |  Loss2: (0.0000) | Acc: (90.00%) (31265/34688)
Epoch: 79 | Batch_idx: 280 |  Loss: (0.2921) |  Loss2: (0.0000) | Acc: (90.00%) (32400/35968)
Epoch: 79 | Batch_idx: 290 |  Loss: (0.2923) |  Loss2: (0.0000) | Acc: (90.00%) (33557/37248)
Epoch: 79 | Batch_idx: 300 |  Loss: (0.2915) |  Loss2: (0.0000) | Acc: (90.00%) (34721/38528)
Epoch: 79 | Batch_idx: 310 |  Loss: (0.2908) |  Loss2: (0.0000) | Acc: (90.00%) (35894/39808)
Epoch: 79 | Batch_idx: 320 |  Loss: (0.2914) |  Loss2: (0.0000) | Acc: (90.00%) (37039/41088)
Epoch: 79 | Batch_idx: 330 |  Loss: (0.2913) |  Loss2: (0.0000) | Acc: (90.00%) (38197/42368)
Epoch: 79 | Batch_idx: 340 |  Loss: (0.2924) |  Loss2: (0.0000) | Acc: (90.00%) (39335/43648)
Epoch: 79 | Batch_idx: 350 |  Loss: (0.2925) |  Loss2: (0.0000) | Acc: (90.00%) (40492/44928)
Epoch: 79 | Batch_idx: 360 |  Loss: (0.2922) |  Loss2: (0.0000) | Acc: (90.00%) (41641/46208)
Epoch: 79 | Batch_idx: 370 |  Loss: (0.2929) |  Loss2: (0.0000) | Acc: (90.00%) (42777/47488)
Epoch: 79 | Batch_idx: 380 |  Loss: (0.2927) |  Loss2: (0.0000) | Acc: (90.00%) (43943/48768)
Epoch: 79 | Batch_idx: 390 |  Loss: (0.2920) |  Loss2: (0.0000) | Acc: (90.00%) (45058/50000)
# TEST : Loss: (0.4193) | Acc: (86.00%) (8610/10000)
percent tensor([0.5160, 0.4840], device='cuda:0')
percent tensor([0.4617, 0.5383], device='cuda:0')
percent tensor([0.5385, 0.4615], device='cuda:0')
percent tensor([0.5863, 0.4137], device='cuda:0')
percent tensor([0.5521, 0.4479], device='cuda:0')
percent tensor([0.6417, 0.3583], device='cuda:0')
percent tensor([0.5916, 0.4084], device='cuda:0')
percent tensor([0.9984, 0.0016], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 80 | Batch_idx: 0 |  Loss: (0.3278) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 80 | Batch_idx: 10 |  Loss: (0.3004) |  Loss2: (0.0000) | Acc: (89.00%) (1261/1408)
Epoch: 80 | Batch_idx: 20 |  Loss: (0.2948) |  Loss2: (0.0000) | Acc: (89.00%) (2413/2688)
Epoch: 80 | Batch_idx: 30 |  Loss: (0.2870) |  Loss2: (0.0000) | Acc: (90.00%) (3582/3968)
Epoch: 80 | Batch_idx: 40 |  Loss: (0.2848) |  Loss2: (0.0000) | Acc: (90.00%) (4733/5248)
Epoch: 80 | Batch_idx: 50 |  Loss: (0.2885) |  Loss2: (0.0000) | Acc: (90.00%) (5883/6528)
Epoch: 80 | Batch_idx: 60 |  Loss: (0.2928) |  Loss2: (0.0000) | Acc: (89.00%) (7020/7808)
Epoch: 80 | Batch_idx: 70 |  Loss: (0.2880) |  Loss2: (0.0000) | Acc: (90.00%) (8194/9088)
Epoch: 80 | Batch_idx: 80 |  Loss: (0.2864) |  Loss2: (0.0000) | Acc: (90.00%) (9350/10368)
Epoch: 80 | Batch_idx: 90 |  Loss: (0.2880) |  Loss2: (0.0000) | Acc: (90.00%) (10498/11648)
Epoch: 80 | Batch_idx: 100 |  Loss: (0.2897) |  Loss2: (0.0000) | Acc: (90.00%) (11644/12928)
Epoch: 80 | Batch_idx: 110 |  Loss: (0.2892) |  Loss2: (0.0000) | Acc: (90.00%) (12805/14208)
Epoch: 80 | Batch_idx: 120 |  Loss: (0.2883) |  Loss2: (0.0000) | Acc: (90.00%) (13956/15488)
Epoch: 80 | Batch_idx: 130 |  Loss: (0.2912) |  Loss2: (0.0000) | Acc: (89.00%) (15081/16768)
Epoch: 80 | Batch_idx: 140 |  Loss: (0.2905) |  Loss2: (0.0000) | Acc: (89.00%) (16234/18048)
Epoch: 80 | Batch_idx: 150 |  Loss: (0.2904) |  Loss2: (0.0000) | Acc: (90.00%) (17397/19328)
Epoch: 80 | Batch_idx: 160 |  Loss: (0.2917) |  Loss2: (0.0000) | Acc: (89.00%) (18547/20608)
Epoch: 80 | Batch_idx: 170 |  Loss: (0.2927) |  Loss2: (0.0000) | Acc: (89.00%) (19697/21888)
Epoch: 80 | Batch_idx: 180 |  Loss: (0.2914) |  Loss2: (0.0000) | Acc: (90.00%) (20871/23168)
Epoch: 80 | Batch_idx: 190 |  Loss: (0.2914) |  Loss2: (0.0000) | Acc: (90.00%) (22016/24448)
Epoch: 80 | Batch_idx: 200 |  Loss: (0.2911) |  Loss2: (0.0000) | Acc: (90.00%) (23175/25728)
Epoch: 80 | Batch_idx: 210 |  Loss: (0.2921) |  Loss2: (0.0000) | Acc: (90.00%) (24314/27008)
Epoch: 80 | Batch_idx: 220 |  Loss: (0.2930) |  Loss2: (0.0000) | Acc: (89.00%) (25455/28288)
Epoch: 80 | Batch_idx: 230 |  Loss: (0.2932) |  Loss2: (0.0000) | Acc: (89.00%) (26605/29568)
Epoch: 80 | Batch_idx: 240 |  Loss: (0.2934) |  Loss2: (0.0000) | Acc: (89.00%) (27750/30848)
Epoch: 80 | Batch_idx: 250 |  Loss: (0.2942) |  Loss2: (0.0000) | Acc: (89.00%) (28880/32128)
Epoch: 80 | Batch_idx: 260 |  Loss: (0.2943) |  Loss2: (0.0000) | Acc: (89.00%) (30029/33408)
Epoch: 80 | Batch_idx: 270 |  Loss: (0.2935) |  Loss2: (0.0000) | Acc: (89.00%) (31187/34688)
Epoch: 80 | Batch_idx: 280 |  Loss: (0.2932) |  Loss2: (0.0000) | Acc: (89.00%) (32347/35968)
Epoch: 80 | Batch_idx: 290 |  Loss: (0.2941) |  Loss2: (0.0000) | Acc: (89.00%) (33488/37248)
Epoch: 80 | Batch_idx: 300 |  Loss: (0.2949) |  Loss2: (0.0000) | Acc: (89.00%) (34623/38528)
Epoch: 80 | Batch_idx: 310 |  Loss: (0.2945) |  Loss2: (0.0000) | Acc: (89.00%) (35779/39808)
Epoch: 80 | Batch_idx: 320 |  Loss: (0.2953) |  Loss2: (0.0000) | Acc: (89.00%) (36918/41088)
Epoch: 80 | Batch_idx: 330 |  Loss: (0.2954) |  Loss2: (0.0000) | Acc: (89.00%) (38066/42368)
Epoch: 80 | Batch_idx: 340 |  Loss: (0.2966) |  Loss2: (0.0000) | Acc: (89.00%) (39190/43648)
Epoch: 80 | Batch_idx: 350 |  Loss: (0.2965) |  Loss2: (0.0000) | Acc: (89.00%) (40335/44928)
Epoch: 80 | Batch_idx: 360 |  Loss: (0.2963) |  Loss2: (0.0000) | Acc: (89.00%) (41482/46208)
Epoch: 80 | Batch_idx: 370 |  Loss: (0.2967) |  Loss2: (0.0000) | Acc: (89.00%) (42632/47488)
Epoch: 80 | Batch_idx: 380 |  Loss: (0.2973) |  Loss2: (0.0000) | Acc: (89.00%) (43755/48768)
Epoch: 80 | Batch_idx: 390 |  Loss: (0.2974) |  Loss2: (0.0000) | Acc: (89.00%) (44839/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_080.pth.tar'
# TEST : Loss: (0.4733) | Acc: (84.00%) (8499/10000)
percent tensor([0.5158, 0.4842], device='cuda:0')
percent tensor([0.4620, 0.5380], device='cuda:0')
percent tensor([0.5394, 0.4606], device='cuda:0')
percent tensor([0.5864, 0.4136], device='cuda:0')
percent tensor([0.5520, 0.4480], device='cuda:0')
percent tensor([0.6415, 0.3585], device='cuda:0')
percent tensor([0.5929, 0.4071], device='cuda:0')
percent tensor([0.9985, 0.0015], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(178.3325, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(808.1806, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(799.2275, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1517.8506, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(506.3567, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2216.4490, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4280.1230, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1404.2413, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6140.6919, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11945.0078, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3960.2556, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16778.2305, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 81 | Batch_idx: 0 |  Loss: (0.2679) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 81 | Batch_idx: 10 |  Loss: (0.2992) |  Loss2: (0.0000) | Acc: (89.00%) (1267/1408)
Epoch: 81 | Batch_idx: 20 |  Loss: (0.2911) |  Loss2: (0.0000) | Acc: (90.00%) (2421/2688)
Epoch: 81 | Batch_idx: 30 |  Loss: (0.2897) |  Loss2: (0.0000) | Acc: (90.00%) (3582/3968)
Epoch: 81 | Batch_idx: 40 |  Loss: (0.2863) |  Loss2: (0.0000) | Acc: (90.00%) (4736/5248)
Epoch: 81 | Batch_idx: 50 |  Loss: (0.2865) |  Loss2: (0.0000) | Acc: (90.00%) (5888/6528)
Epoch: 81 | Batch_idx: 60 |  Loss: (0.2849) |  Loss2: (0.0000) | Acc: (90.00%) (7039/7808)
Epoch: 81 | Batch_idx: 70 |  Loss: (0.2837) |  Loss2: (0.0000) | Acc: (90.00%) (8200/9088)
Epoch: 81 | Batch_idx: 80 |  Loss: (0.2848) |  Loss2: (0.0000) | Acc: (90.00%) (9352/10368)
Epoch: 81 | Batch_idx: 90 |  Loss: (0.2848) |  Loss2: (0.0000) | Acc: (90.00%) (10502/11648)
Epoch: 81 | Batch_idx: 100 |  Loss: (0.2823) |  Loss2: (0.0000) | Acc: (90.00%) (11654/12928)
Epoch: 81 | Batch_idx: 110 |  Loss: (0.2828) |  Loss2: (0.0000) | Acc: (90.00%) (12805/14208)
Epoch: 81 | Batch_idx: 120 |  Loss: (0.2808) |  Loss2: (0.0000) | Acc: (90.00%) (13984/15488)
Epoch: 81 | Batch_idx: 130 |  Loss: (0.2801) |  Loss2: (0.0000) | Acc: (90.00%) (15150/16768)
Epoch: 81 | Batch_idx: 140 |  Loss: (0.2803) |  Loss2: (0.0000) | Acc: (90.00%) (16296/18048)
Epoch: 81 | Batch_idx: 150 |  Loss: (0.2819) |  Loss2: (0.0000) | Acc: (90.00%) (17444/19328)
Epoch: 81 | Batch_idx: 160 |  Loss: (0.2815) |  Loss2: (0.0000) | Acc: (90.00%) (18605/20608)
Epoch: 81 | Batch_idx: 170 |  Loss: (0.2809) |  Loss2: (0.0000) | Acc: (90.00%) (19765/21888)
Epoch: 81 | Batch_idx: 180 |  Loss: (0.2816) |  Loss2: (0.0000) | Acc: (90.00%) (20916/23168)
Epoch: 81 | Batch_idx: 190 |  Loss: (0.2826) |  Loss2: (0.0000) | Acc: (90.00%) (22066/24448)
Epoch: 81 | Batch_idx: 200 |  Loss: (0.2825) |  Loss2: (0.0000) | Acc: (90.00%) (23218/25728)
Epoch: 81 | Batch_idx: 210 |  Loss: (0.2844) |  Loss2: (0.0000) | Acc: (90.00%) (24359/27008)
Epoch: 81 | Batch_idx: 220 |  Loss: (0.2855) |  Loss2: (0.0000) | Acc: (90.00%) (25501/28288)
Epoch: 81 | Batch_idx: 230 |  Loss: (0.2855) |  Loss2: (0.0000) | Acc: (90.00%) (26653/29568)
Epoch: 81 | Batch_idx: 240 |  Loss: (0.2866) |  Loss2: (0.0000) | Acc: (90.00%) (27797/30848)
Epoch: 81 | Batch_idx: 250 |  Loss: (0.2873) |  Loss2: (0.0000) | Acc: (90.00%) (28949/32128)
Epoch: 81 | Batch_idx: 260 |  Loss: (0.2874) |  Loss2: (0.0000) | Acc: (90.00%) (30101/33408)
Epoch: 81 | Batch_idx: 270 |  Loss: (0.2878) |  Loss2: (0.0000) | Acc: (90.00%) (31247/34688)
Epoch: 81 | Batch_idx: 280 |  Loss: (0.2873) |  Loss2: (0.0000) | Acc: (90.00%) (32417/35968)
Epoch: 81 | Batch_idx: 290 |  Loss: (0.2867) |  Loss2: (0.0000) | Acc: (90.00%) (33571/37248)
Epoch: 81 | Batch_idx: 300 |  Loss: (0.2878) |  Loss2: (0.0000) | Acc: (90.00%) (34716/38528)
Epoch: 81 | Batch_idx: 310 |  Loss: (0.2872) |  Loss2: (0.0000) | Acc: (90.00%) (35872/39808)
Epoch: 81 | Batch_idx: 320 |  Loss: (0.2886) |  Loss2: (0.0000) | Acc: (90.00%) (37004/41088)
Epoch: 81 | Batch_idx: 330 |  Loss: (0.2884) |  Loss2: (0.0000) | Acc: (90.00%) (38167/42368)
Epoch: 81 | Batch_idx: 340 |  Loss: (0.2882) |  Loss2: (0.0000) | Acc: (90.00%) (39323/43648)
Epoch: 81 | Batch_idx: 350 |  Loss: (0.2888) |  Loss2: (0.0000) | Acc: (90.00%) (40465/44928)
Epoch: 81 | Batch_idx: 360 |  Loss: (0.2893) |  Loss2: (0.0000) | Acc: (90.00%) (41611/46208)
Epoch: 81 | Batch_idx: 370 |  Loss: (0.2882) |  Loss2: (0.0000) | Acc: (90.00%) (42780/47488)
Epoch: 81 | Batch_idx: 380 |  Loss: (0.2885) |  Loss2: (0.0000) | Acc: (90.00%) (43934/48768)
Epoch: 81 | Batch_idx: 390 |  Loss: (0.2886) |  Loss2: (0.0000) | Acc: (90.00%) (45049/50000)
# TEST : Loss: (0.4808) | Acc: (84.00%) (8487/10000)
percent tensor([0.5160, 0.4840], device='cuda:0')
percent tensor([0.4618, 0.5382], device='cuda:0')
percent tensor([0.5388, 0.4612], device='cuda:0')
percent tensor([0.5860, 0.4140], device='cuda:0')
percent tensor([0.5524, 0.4476], device='cuda:0')
percent tensor([0.6422, 0.3578], device='cuda:0')
percent tensor([0.5920, 0.4080], device='cuda:0')
percent tensor([0.9986, 0.0014], device='cuda:0')
Epoch: 82 | Batch_idx: 0 |  Loss: (0.2654) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 82 | Batch_idx: 10 |  Loss: (0.2551) |  Loss2: (0.0000) | Acc: (91.00%) (1290/1408)
Epoch: 82 | Batch_idx: 20 |  Loss: (0.2608) |  Loss2: (0.0000) | Acc: (91.00%) (2454/2688)
Epoch: 82 | Batch_idx: 30 |  Loss: (0.2629) |  Loss2: (0.0000) | Acc: (91.00%) (3620/3968)
Epoch: 82 | Batch_idx: 40 |  Loss: (0.2605) |  Loss2: (0.0000) | Acc: (91.00%) (4797/5248)
Epoch: 82 | Batch_idx: 50 |  Loss: (0.2627) |  Loss2: (0.0000) | Acc: (91.00%) (5952/6528)
Epoch: 82 | Batch_idx: 60 |  Loss: (0.2609) |  Loss2: (0.0000) | Acc: (91.00%) (7118/7808)
Epoch: 82 | Batch_idx: 70 |  Loss: (0.2608) |  Loss2: (0.0000) | Acc: (91.00%) (8278/9088)
Epoch: 82 | Batch_idx: 80 |  Loss: (0.2622) |  Loss2: (0.0000) | Acc: (91.00%) (9442/10368)
Epoch: 82 | Batch_idx: 90 |  Loss: (0.2629) |  Loss2: (0.0000) | Acc: (91.00%) (10604/11648)
Epoch: 82 | Batch_idx: 100 |  Loss: (0.2615) |  Loss2: (0.0000) | Acc: (91.00%) (11766/12928)
Epoch: 82 | Batch_idx: 110 |  Loss: (0.2656) |  Loss2: (0.0000) | Acc: (90.00%) (12918/14208)
Epoch: 82 | Batch_idx: 120 |  Loss: (0.2630) |  Loss2: (0.0000) | Acc: (91.00%) (14098/15488)
Epoch: 82 | Batch_idx: 130 |  Loss: (0.2635) |  Loss2: (0.0000) | Acc: (91.00%) (15263/16768)
Epoch: 82 | Batch_idx: 140 |  Loss: (0.2666) |  Loss2: (0.0000) | Acc: (90.00%) (16407/18048)
Epoch: 82 | Batch_idx: 150 |  Loss: (0.2667) |  Loss2: (0.0000) | Acc: (90.00%) (17572/19328)
Epoch: 82 | Batch_idx: 160 |  Loss: (0.2673) |  Loss2: (0.0000) | Acc: (90.00%) (18725/20608)
Epoch: 82 | Batch_idx: 170 |  Loss: (0.2681) |  Loss2: (0.0000) | Acc: (90.00%) (19872/21888)
Epoch: 82 | Batch_idx: 180 |  Loss: (0.2695) |  Loss2: (0.0000) | Acc: (90.00%) (21022/23168)
Epoch: 82 | Batch_idx: 190 |  Loss: (0.2692) |  Loss2: (0.0000) | Acc: (90.00%) (22184/24448)
Epoch: 82 | Batch_idx: 200 |  Loss: (0.2693) |  Loss2: (0.0000) | Acc: (90.00%) (23336/25728)
Epoch: 82 | Batch_idx: 210 |  Loss: (0.2696) |  Loss2: (0.0000) | Acc: (90.00%) (24489/27008)
Epoch: 82 | Batch_idx: 220 |  Loss: (0.2704) |  Loss2: (0.0000) | Acc: (90.00%) (25648/28288)
Epoch: 82 | Batch_idx: 230 |  Loss: (0.2719) |  Loss2: (0.0000) | Acc: (90.00%) (26788/29568)
Epoch: 82 | Batch_idx: 240 |  Loss: (0.2731) |  Loss2: (0.0000) | Acc: (90.00%) (27928/30848)
Epoch: 82 | Batch_idx: 250 |  Loss: (0.2733) |  Loss2: (0.0000) | Acc: (90.00%) (29098/32128)
Epoch: 82 | Batch_idx: 260 |  Loss: (0.2740) |  Loss2: (0.0000) | Acc: (90.00%) (30251/33408)
Epoch: 82 | Batch_idx: 270 |  Loss: (0.2758) |  Loss2: (0.0000) | Acc: (90.00%) (31387/34688)
Epoch: 82 | Batch_idx: 280 |  Loss: (0.2762) |  Loss2: (0.0000) | Acc: (90.00%) (32522/35968)
Epoch: 82 | Batch_idx: 290 |  Loss: (0.2771) |  Loss2: (0.0000) | Acc: (90.00%) (33658/37248)
Epoch: 82 | Batch_idx: 300 |  Loss: (0.2769) |  Loss2: (0.0000) | Acc: (90.00%) (34802/38528)
Epoch: 82 | Batch_idx: 310 |  Loss: (0.2774) |  Loss2: (0.0000) | Acc: (90.00%) (35957/39808)
Epoch: 82 | Batch_idx: 320 |  Loss: (0.2780) |  Loss2: (0.0000) | Acc: (90.00%) (37096/41088)
Epoch: 82 | Batch_idx: 330 |  Loss: (0.2777) |  Loss2: (0.0000) | Acc: (90.00%) (38259/42368)
Epoch: 82 | Batch_idx: 340 |  Loss: (0.2786) |  Loss2: (0.0000) | Acc: (90.00%) (39420/43648)
Epoch: 82 | Batch_idx: 350 |  Loss: (0.2792) |  Loss2: (0.0000) | Acc: (90.00%) (40566/44928)
Epoch: 82 | Batch_idx: 360 |  Loss: (0.2791) |  Loss2: (0.0000) | Acc: (90.00%) (41726/46208)
Epoch: 82 | Batch_idx: 370 |  Loss: (0.2792) |  Loss2: (0.0000) | Acc: (90.00%) (42876/47488)
Epoch: 82 | Batch_idx: 380 |  Loss: (0.2801) |  Loss2: (0.0000) | Acc: (90.00%) (44023/48768)
Epoch: 82 | Batch_idx: 390 |  Loss: (0.2808) |  Loss2: (0.0000) | Acc: (90.00%) (45127/50000)
# TEST : Loss: (0.4957) | Acc: (84.00%) (8415/10000)
percent tensor([0.5162, 0.4838], device='cuda:0')
percent tensor([0.4620, 0.5380], device='cuda:0')
percent tensor([0.5388, 0.4612], device='cuda:0')
percent tensor([0.5862, 0.4138], device='cuda:0')
percent tensor([0.5515, 0.4485], device='cuda:0')
percent tensor([0.6415, 0.3585], device='cuda:0')
percent tensor([0.5943, 0.4057], device='cuda:0')
percent tensor([0.9986, 0.0014], device='cuda:0')
Epoch: 83 | Batch_idx: 0 |  Loss: (0.2689) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 83 | Batch_idx: 10 |  Loss: (0.2440) |  Loss2: (0.0000) | Acc: (92.00%) (1296/1408)
Epoch: 83 | Batch_idx: 20 |  Loss: (0.2539) |  Loss2: (0.0000) | Acc: (91.00%) (2451/2688)
Epoch: 83 | Batch_idx: 30 |  Loss: (0.2619) |  Loss2: (0.0000) | Acc: (90.00%) (3601/3968)
Epoch: 83 | Batch_idx: 40 |  Loss: (0.2727) |  Loss2: (0.0000) | Acc: (90.00%) (4738/5248)
Epoch: 83 | Batch_idx: 50 |  Loss: (0.2728) |  Loss2: (0.0000) | Acc: (90.00%) (5900/6528)
Epoch: 83 | Batch_idx: 60 |  Loss: (0.2722) |  Loss2: (0.0000) | Acc: (90.00%) (7068/7808)
Epoch: 83 | Batch_idx: 70 |  Loss: (0.2746) |  Loss2: (0.0000) | Acc: (90.00%) (8223/9088)
Epoch: 83 | Batch_idx: 80 |  Loss: (0.2739) |  Loss2: (0.0000) | Acc: (90.00%) (9388/10368)
Epoch: 83 | Batch_idx: 90 |  Loss: (0.2733) |  Loss2: (0.0000) | Acc: (90.00%) (10556/11648)
Epoch: 83 | Batch_idx: 100 |  Loss: (0.2694) |  Loss2: (0.0000) | Acc: (90.00%) (11730/12928)
Epoch: 83 | Batch_idx: 110 |  Loss: (0.2688) |  Loss2: (0.0000) | Acc: (90.00%) (12898/14208)
Epoch: 83 | Batch_idx: 120 |  Loss: (0.2681) |  Loss2: (0.0000) | Acc: (90.00%) (14058/15488)
Epoch: 83 | Batch_idx: 130 |  Loss: (0.2668) |  Loss2: (0.0000) | Acc: (90.00%) (15239/16768)
Epoch: 83 | Batch_idx: 140 |  Loss: (0.2673) |  Loss2: (0.0000) | Acc: (90.00%) (16405/18048)
Epoch: 83 | Batch_idx: 150 |  Loss: (0.2670) |  Loss2: (0.0000) | Acc: (90.00%) (17567/19328)
Epoch: 83 | Batch_idx: 160 |  Loss: (0.2680) |  Loss2: (0.0000) | Acc: (90.00%) (18728/20608)
Epoch: 83 | Batch_idx: 170 |  Loss: (0.2677) |  Loss2: (0.0000) | Acc: (90.00%) (19891/21888)
Epoch: 83 | Batch_idx: 180 |  Loss: (0.2697) |  Loss2: (0.0000) | Acc: (90.00%) (21031/23168)
Epoch: 83 | Batch_idx: 190 |  Loss: (0.2711) |  Loss2: (0.0000) | Acc: (90.00%) (22177/24448)
Epoch: 83 | Batch_idx: 200 |  Loss: (0.2704) |  Loss2: (0.0000) | Acc: (90.00%) (23345/25728)
Epoch: 83 | Batch_idx: 210 |  Loss: (0.2694) |  Loss2: (0.0000) | Acc: (90.00%) (24522/27008)
Epoch: 83 | Batch_idx: 220 |  Loss: (0.2695) |  Loss2: (0.0000) | Acc: (90.00%) (25682/28288)
Epoch: 83 | Batch_idx: 230 |  Loss: (0.2696) |  Loss2: (0.0000) | Acc: (90.00%) (26842/29568)
Epoch: 83 | Batch_idx: 240 |  Loss: (0.2685) |  Loss2: (0.0000) | Acc: (90.00%) (28023/30848)
Epoch: 83 | Batch_idx: 250 |  Loss: (0.2690) |  Loss2: (0.0000) | Acc: (90.00%) (29184/32128)
Epoch: 83 | Batch_idx: 260 |  Loss: (0.2692) |  Loss2: (0.0000) | Acc: (90.00%) (30349/33408)
Epoch: 83 | Batch_idx: 270 |  Loss: (0.2700) |  Loss2: (0.0000) | Acc: (90.00%) (31493/34688)
Epoch: 83 | Batch_idx: 280 |  Loss: (0.2721) |  Loss2: (0.0000) | Acc: (90.00%) (32633/35968)
Epoch: 83 | Batch_idx: 290 |  Loss: (0.2733) |  Loss2: (0.0000) | Acc: (90.00%) (33787/37248)
Epoch: 83 | Batch_idx: 300 |  Loss: (0.2736) |  Loss2: (0.0000) | Acc: (90.00%) (34935/38528)
Epoch: 83 | Batch_idx: 310 |  Loss: (0.2734) |  Loss2: (0.0000) | Acc: (90.00%) (36099/39808)
Epoch: 83 | Batch_idx: 320 |  Loss: (0.2739) |  Loss2: (0.0000) | Acc: (90.00%) (37250/41088)
Epoch: 83 | Batch_idx: 330 |  Loss: (0.2729) |  Loss2: (0.0000) | Acc: (90.00%) (38426/42368)
Epoch: 83 | Batch_idx: 340 |  Loss: (0.2734) |  Loss2: (0.0000) | Acc: (90.00%) (39579/43648)
Epoch: 83 | Batch_idx: 350 |  Loss: (0.2742) |  Loss2: (0.0000) | Acc: (90.00%) (40723/44928)
Epoch: 83 | Batch_idx: 360 |  Loss: (0.2739) |  Loss2: (0.0000) | Acc: (90.00%) (41891/46208)
Epoch: 83 | Batch_idx: 370 |  Loss: (0.2746) |  Loss2: (0.0000) | Acc: (90.00%) (43034/47488)
Epoch: 83 | Batch_idx: 380 |  Loss: (0.2747) |  Loss2: (0.0000) | Acc: (90.00%) (44193/48768)
Epoch: 83 | Batch_idx: 390 |  Loss: (0.2752) |  Loss2: (0.0000) | Acc: (90.00%) (45298/50000)
# TEST : Loss: (0.4733) | Acc: (84.00%) (8455/10000)
percent tensor([0.5167, 0.4833], device='cuda:0')
percent tensor([0.4622, 0.5378], device='cuda:0')
percent tensor([0.5403, 0.4597], device='cuda:0')
percent tensor([0.5858, 0.4142], device='cuda:0')
percent tensor([0.5517, 0.4483], device='cuda:0')
percent tensor([0.6407, 0.3593], device='cuda:0')
percent tensor([0.5890, 0.4110], device='cuda:0')
percent tensor([0.9984, 0.0016], device='cuda:0')
Epoch: 84 | Batch_idx: 0 |  Loss: (0.2382) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 84 | Batch_idx: 10 |  Loss: (0.2661) |  Loss2: (0.0000) | Acc: (90.00%) (1273/1408)
Epoch: 84 | Batch_idx: 20 |  Loss: (0.2570) |  Loss2: (0.0000) | Acc: (90.00%) (2440/2688)
Epoch: 84 | Batch_idx: 30 |  Loss: (0.2558) |  Loss2: (0.0000) | Acc: (91.00%) (3614/3968)
Epoch: 84 | Batch_idx: 40 |  Loss: (0.2516) |  Loss2: (0.0000) | Acc: (91.00%) (4796/5248)
Epoch: 84 | Batch_idx: 50 |  Loss: (0.2598) |  Loss2: (0.0000) | Acc: (91.00%) (5949/6528)
Epoch: 84 | Batch_idx: 60 |  Loss: (0.2610) |  Loss2: (0.0000) | Acc: (91.00%) (7109/7808)
Epoch: 84 | Batch_idx: 70 |  Loss: (0.2604) |  Loss2: (0.0000) | Acc: (91.00%) (8272/9088)
Epoch: 84 | Batch_idx: 80 |  Loss: (0.2618) |  Loss2: (0.0000) | Acc: (91.00%) (9435/10368)
Epoch: 84 | Batch_idx: 90 |  Loss: (0.2631) |  Loss2: (0.0000) | Acc: (90.00%) (10596/11648)
Epoch: 84 | Batch_idx: 100 |  Loss: (0.2633) |  Loss2: (0.0000) | Acc: (91.00%) (11767/12928)
Epoch: 84 | Batch_idx: 110 |  Loss: (0.2635) |  Loss2: (0.0000) | Acc: (91.00%) (12937/14208)
Epoch: 84 | Batch_idx: 120 |  Loss: (0.2656) |  Loss2: (0.0000) | Acc: (90.00%) (14087/15488)
Epoch: 84 | Batch_idx: 130 |  Loss: (0.2664) |  Loss2: (0.0000) | Acc: (90.00%) (15245/16768)
Epoch: 84 | Batch_idx: 140 |  Loss: (0.2665) |  Loss2: (0.0000) | Acc: (90.00%) (16409/18048)
Epoch: 84 | Batch_idx: 150 |  Loss: (0.2683) |  Loss2: (0.0000) | Acc: (90.00%) (17556/19328)
Epoch: 84 | Batch_idx: 160 |  Loss: (0.2665) |  Loss2: (0.0000) | Acc: (90.00%) (18731/20608)
Epoch: 84 | Batch_idx: 170 |  Loss: (0.2669) |  Loss2: (0.0000) | Acc: (90.00%) (19889/21888)
Epoch: 84 | Batch_idx: 180 |  Loss: (0.2650) |  Loss2: (0.0000) | Acc: (90.00%) (21067/23168)
Epoch: 84 | Batch_idx: 190 |  Loss: (0.2655) |  Loss2: (0.0000) | Acc: (90.00%) (22233/24448)
Epoch: 84 | Batch_idx: 200 |  Loss: (0.2662) |  Loss2: (0.0000) | Acc: (90.00%) (23404/25728)
Epoch: 84 | Batch_idx: 210 |  Loss: (0.2669) |  Loss2: (0.0000) | Acc: (90.00%) (24563/27008)
Epoch: 84 | Batch_idx: 220 |  Loss: (0.2672) |  Loss2: (0.0000) | Acc: (90.00%) (25722/28288)
Epoch: 84 | Batch_idx: 230 |  Loss: (0.2672) |  Loss2: (0.0000) | Acc: (90.00%) (26889/29568)
Epoch: 84 | Batch_idx: 240 |  Loss: (0.2675) |  Loss2: (0.0000) | Acc: (90.00%) (28055/30848)
Epoch: 84 | Batch_idx: 250 |  Loss: (0.2678) |  Loss2: (0.0000) | Acc: (90.00%) (29212/32128)
Epoch: 84 | Batch_idx: 260 |  Loss: (0.2669) |  Loss2: (0.0000) | Acc: (90.00%) (30395/33408)
Epoch: 84 | Batch_idx: 270 |  Loss: (0.2670) |  Loss2: (0.0000) | Acc: (90.00%) (31549/34688)
Epoch: 84 | Batch_idx: 280 |  Loss: (0.2683) |  Loss2: (0.0000) | Acc: (90.00%) (32703/35968)
Epoch: 84 | Batch_idx: 290 |  Loss: (0.2678) |  Loss2: (0.0000) | Acc: (90.00%) (33871/37248)
Epoch: 84 | Batch_idx: 300 |  Loss: (0.2671) |  Loss2: (0.0000) | Acc: (90.00%) (35044/38528)
Epoch: 84 | Batch_idx: 310 |  Loss: (0.2671) |  Loss2: (0.0000) | Acc: (90.00%) (36205/39808)
Epoch: 84 | Batch_idx: 320 |  Loss: (0.2677) |  Loss2: (0.0000) | Acc: (90.00%) (37366/41088)
Epoch: 84 | Batch_idx: 330 |  Loss: (0.2683) |  Loss2: (0.0000) | Acc: (90.00%) (38519/42368)
Epoch: 84 | Batch_idx: 340 |  Loss: (0.2680) |  Loss2: (0.0000) | Acc: (90.00%) (39697/43648)
Epoch: 84 | Batch_idx: 350 |  Loss: (0.2687) |  Loss2: (0.0000) | Acc: (90.00%) (40838/44928)
Epoch: 84 | Batch_idx: 360 |  Loss: (0.2682) |  Loss2: (0.0000) | Acc: (90.00%) (42005/46208)
Epoch: 84 | Batch_idx: 370 |  Loss: (0.2681) |  Loss2: (0.0000) | Acc: (90.00%) (43170/47488)
Epoch: 84 | Batch_idx: 380 |  Loss: (0.2685) |  Loss2: (0.0000) | Acc: (90.00%) (44322/48768)
Epoch: 84 | Batch_idx: 390 |  Loss: (0.2689) |  Loss2: (0.0000) | Acc: (90.00%) (45435/50000)
# TEST : Loss: (0.4513) | Acc: (85.00%) (8523/10000)
percent tensor([0.5163, 0.4837], device='cuda:0')
percent tensor([0.4623, 0.5377], device='cuda:0')
percent tensor([0.5391, 0.4609], device='cuda:0')
percent tensor([0.5861, 0.4139], device='cuda:0')
percent tensor([0.5522, 0.4478], device='cuda:0')
percent tensor([0.6385, 0.3615], device='cuda:0')
percent tensor([0.5908, 0.4092], device='cuda:0')
percent tensor([0.9984, 0.0016], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 85 | Batch_idx: 0 |  Loss: (0.3480) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 85 | Batch_idx: 10 |  Loss: (0.2942) |  Loss2: (0.0000) | Acc: (89.00%) (1265/1408)
Epoch: 85 | Batch_idx: 20 |  Loss: (0.3033) |  Loss2: (0.0000) | Acc: (89.00%) (2399/2688)
Epoch: 85 | Batch_idx: 30 |  Loss: (0.3045) |  Loss2: (0.0000) | Acc: (89.00%) (3543/3968)
Epoch: 85 | Batch_idx: 40 |  Loss: (0.3172) |  Loss2: (0.0000) | Acc: (88.00%) (4661/5248)
Epoch: 85 | Batch_idx: 50 |  Loss: (0.3246) |  Loss2: (0.0000) | Acc: (88.00%) (5789/6528)
Epoch: 85 | Batch_idx: 60 |  Loss: (0.3310) |  Loss2: (0.0000) | Acc: (88.00%) (6893/7808)
Epoch: 85 | Batch_idx: 70 |  Loss: (0.3303) |  Loss2: (0.0000) | Acc: (88.00%) (8035/9088)
Epoch: 85 | Batch_idx: 80 |  Loss: (0.3316) |  Loss2: (0.0000) | Acc: (88.00%) (9163/10368)
Epoch: 85 | Batch_idx: 90 |  Loss: (0.3334) |  Loss2: (0.0000) | Acc: (88.00%) (10300/11648)
Epoch: 85 | Batch_idx: 100 |  Loss: (0.3306) |  Loss2: (0.0000) | Acc: (88.00%) (11441/12928)
Epoch: 85 | Batch_idx: 110 |  Loss: (0.3274) |  Loss2: (0.0000) | Acc: (88.00%) (12595/14208)
Epoch: 85 | Batch_idx: 120 |  Loss: (0.3249) |  Loss2: (0.0000) | Acc: (88.00%) (13742/15488)
Epoch: 85 | Batch_idx: 130 |  Loss: (0.3220) |  Loss2: (0.0000) | Acc: (88.00%) (14890/16768)
Epoch: 85 | Batch_idx: 140 |  Loss: (0.3197) |  Loss2: (0.0000) | Acc: (88.00%) (16039/18048)
Epoch: 85 | Batch_idx: 150 |  Loss: (0.3196) |  Loss2: (0.0000) | Acc: (88.00%) (17174/19328)
Epoch: 85 | Batch_idx: 160 |  Loss: (0.3175) |  Loss2: (0.0000) | Acc: (88.00%) (18334/20608)
Epoch: 85 | Batch_idx: 170 |  Loss: (0.3178) |  Loss2: (0.0000) | Acc: (88.00%) (19465/21888)
Epoch: 85 | Batch_idx: 180 |  Loss: (0.3174) |  Loss2: (0.0000) | Acc: (88.00%) (20593/23168)
Epoch: 85 | Batch_idx: 190 |  Loss: (0.3161) |  Loss2: (0.0000) | Acc: (88.00%) (21744/24448)
Epoch: 85 | Batch_idx: 200 |  Loss: (0.3159) |  Loss2: (0.0000) | Acc: (88.00%) (22882/25728)
Epoch: 85 | Batch_idx: 210 |  Loss: (0.3157) |  Loss2: (0.0000) | Acc: (88.00%) (24017/27008)
Epoch: 85 | Batch_idx: 220 |  Loss: (0.3142) |  Loss2: (0.0000) | Acc: (88.00%) (25164/28288)
Epoch: 85 | Batch_idx: 230 |  Loss: (0.3122) |  Loss2: (0.0000) | Acc: (89.00%) (26326/29568)
Epoch: 85 | Batch_idx: 240 |  Loss: (0.3112) |  Loss2: (0.0000) | Acc: (89.00%) (27482/30848)
Epoch: 85 | Batch_idx: 250 |  Loss: (0.3111) |  Loss2: (0.0000) | Acc: (89.00%) (28619/32128)
Epoch: 85 | Batch_idx: 260 |  Loss: (0.3118) |  Loss2: (0.0000) | Acc: (89.00%) (29740/33408)
Epoch: 85 | Batch_idx: 270 |  Loss: (0.3104) |  Loss2: (0.0000) | Acc: (89.00%) (30901/34688)
Epoch: 85 | Batch_idx: 280 |  Loss: (0.3101) |  Loss2: (0.0000) | Acc: (89.00%) (32048/35968)
Epoch: 85 | Batch_idx: 290 |  Loss: (0.3095) |  Loss2: (0.0000) | Acc: (89.00%) (33191/37248)
Epoch: 85 | Batch_idx: 300 |  Loss: (0.3092) |  Loss2: (0.0000) | Acc: (89.00%) (34329/38528)
Epoch: 85 | Batch_idx: 310 |  Loss: (0.3080) |  Loss2: (0.0000) | Acc: (89.00%) (35480/39808)
Epoch: 85 | Batch_idx: 320 |  Loss: (0.3071) |  Loss2: (0.0000) | Acc: (89.00%) (36633/41088)
Epoch: 85 | Batch_idx: 330 |  Loss: (0.3072) |  Loss2: (0.0000) | Acc: (89.00%) (37775/42368)
Epoch: 85 | Batch_idx: 340 |  Loss: (0.3074) |  Loss2: (0.0000) | Acc: (89.00%) (38914/43648)
Epoch: 85 | Batch_idx: 350 |  Loss: (0.3072) |  Loss2: (0.0000) | Acc: (89.00%) (40063/44928)
Epoch: 85 | Batch_idx: 360 |  Loss: (0.3072) |  Loss2: (0.0000) | Acc: (89.00%) (41203/46208)
Epoch: 85 | Batch_idx: 370 |  Loss: (0.3071) |  Loss2: (0.0000) | Acc: (89.00%) (42343/47488)
Epoch: 85 | Batch_idx: 380 |  Loss: (0.3070) |  Loss2: (0.0000) | Acc: (89.00%) (43489/48768)
Epoch: 85 | Batch_idx: 390 |  Loss: (0.3066) |  Loss2: (0.0000) | Acc: (89.00%) (44591/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_085.pth.tar'
# TEST : Loss: (0.4626) | Acc: (85.00%) (8551/10000)
percent tensor([0.5160, 0.4840], device='cuda:0')
percent tensor([0.4591, 0.5409], device='cuda:0')
percent tensor([0.5511, 0.4489], device='cuda:0')
percent tensor([0.5873, 0.4127], device='cuda:0')
percent tensor([0.5810, 0.4190], device='cuda:0')
percent tensor([0.6853, 0.3147], device='cuda:0')
percent tensor([0.6188, 0.3812], device='cuda:0')
percent tensor([0.9985, 0.0015], device='cuda:0')
Epoch: 86 | Batch_idx: 0 |  Loss: (0.3086) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 86 | Batch_idx: 10 |  Loss: (0.3152) |  Loss2: (0.0000) | Acc: (88.00%) (1252/1408)
Epoch: 86 | Batch_idx: 20 |  Loss: (0.2921) |  Loss2: (0.0000) | Acc: (89.00%) (2410/2688)
Epoch: 86 | Batch_idx: 30 |  Loss: (0.2998) |  Loss2: (0.0000) | Acc: (89.00%) (3550/3968)
Epoch: 86 | Batch_idx: 40 |  Loss: (0.3050) |  Loss2: (0.0000) | Acc: (89.00%) (4684/5248)
Epoch: 86 | Batch_idx: 50 |  Loss: (0.3012) |  Loss2: (0.0000) | Acc: (89.00%) (5830/6528)
Epoch: 86 | Batch_idx: 60 |  Loss: (0.2961) |  Loss2: (0.0000) | Acc: (89.00%) (6989/7808)
Epoch: 86 | Batch_idx: 70 |  Loss: (0.2927) |  Loss2: (0.0000) | Acc: (89.00%) (8144/9088)
Epoch: 86 | Batch_idx: 80 |  Loss: (0.2909) |  Loss2: (0.0000) | Acc: (89.00%) (9310/10368)
Epoch: 86 | Batch_idx: 90 |  Loss: (0.2884) |  Loss2: (0.0000) | Acc: (89.00%) (10468/11648)
Epoch: 86 | Batch_idx: 100 |  Loss: (0.2878) |  Loss2: (0.0000) | Acc: (89.00%) (11623/12928)
Epoch: 86 | Batch_idx: 110 |  Loss: (0.2874) |  Loss2: (0.0000) | Acc: (89.00%) (12786/14208)
Epoch: 86 | Batch_idx: 120 |  Loss: (0.2846) |  Loss2: (0.0000) | Acc: (90.00%) (13960/15488)
Epoch: 86 | Batch_idx: 130 |  Loss: (0.2868) |  Loss2: (0.0000) | Acc: (90.00%) (15096/16768)
Epoch: 86 | Batch_idx: 140 |  Loss: (0.2873) |  Loss2: (0.0000) | Acc: (90.00%) (16251/18048)
Epoch: 86 | Batch_idx: 150 |  Loss: (0.2872) |  Loss2: (0.0000) | Acc: (90.00%) (17397/19328)
Epoch: 86 | Batch_idx: 160 |  Loss: (0.2884) |  Loss2: (0.0000) | Acc: (89.00%) (18530/20608)
Epoch: 86 | Batch_idx: 170 |  Loss: (0.2881) |  Loss2: (0.0000) | Acc: (89.00%) (19676/21888)
Epoch: 86 | Batch_idx: 180 |  Loss: (0.2882) |  Loss2: (0.0000) | Acc: (89.00%) (20824/23168)
Epoch: 86 | Batch_idx: 190 |  Loss: (0.2886) |  Loss2: (0.0000) | Acc: (89.00%) (21966/24448)
Epoch: 86 | Batch_idx: 200 |  Loss: (0.2867) |  Loss2: (0.0000) | Acc: (89.00%) (23130/25728)
Epoch: 86 | Batch_idx: 210 |  Loss: (0.2850) |  Loss2: (0.0000) | Acc: (89.00%) (24293/27008)
Epoch: 86 | Batch_idx: 220 |  Loss: (0.2847) |  Loss2: (0.0000) | Acc: (89.00%) (25447/28288)
Epoch: 86 | Batch_idx: 230 |  Loss: (0.2836) |  Loss2: (0.0000) | Acc: (90.00%) (26614/29568)
Epoch: 86 | Batch_idx: 240 |  Loss: (0.2836) |  Loss2: (0.0000) | Acc: (89.00%) (27761/30848)
Epoch: 86 | Batch_idx: 250 |  Loss: (0.2841) |  Loss2: (0.0000) | Acc: (89.00%) (28903/32128)
Epoch: 86 | Batch_idx: 260 |  Loss: (0.2836) |  Loss2: (0.0000) | Acc: (89.00%) (30061/33408)
Epoch: 86 | Batch_idx: 270 |  Loss: (0.2819) |  Loss2: (0.0000) | Acc: (90.00%) (31231/34688)
Epoch: 86 | Batch_idx: 280 |  Loss: (0.2810) |  Loss2: (0.0000) | Acc: (90.00%) (32390/35968)
Epoch: 86 | Batch_idx: 290 |  Loss: (0.2812) |  Loss2: (0.0000) | Acc: (90.00%) (33532/37248)
Epoch: 86 | Batch_idx: 300 |  Loss: (0.2817) |  Loss2: (0.0000) | Acc: (90.00%) (34694/38528)
Epoch: 86 | Batch_idx: 310 |  Loss: (0.2815) |  Loss2: (0.0000) | Acc: (90.00%) (35836/39808)
Epoch: 86 | Batch_idx: 320 |  Loss: (0.2805) |  Loss2: (0.0000) | Acc: (90.00%) (37004/41088)
Epoch: 86 | Batch_idx: 330 |  Loss: (0.2807) |  Loss2: (0.0000) | Acc: (90.00%) (38162/42368)
Epoch: 86 | Batch_idx: 340 |  Loss: (0.2811) |  Loss2: (0.0000) | Acc: (90.00%) (39324/43648)
Epoch: 86 | Batch_idx: 350 |  Loss: (0.2803) |  Loss2: (0.0000) | Acc: (90.00%) (40502/44928)
Epoch: 86 | Batch_idx: 360 |  Loss: (0.2797) |  Loss2: (0.0000) | Acc: (90.00%) (41669/46208)
Epoch: 86 | Batch_idx: 370 |  Loss: (0.2795) |  Loss2: (0.0000) | Acc: (90.00%) (42820/47488)
Epoch: 86 | Batch_idx: 380 |  Loss: (0.2801) |  Loss2: (0.0000) | Acc: (90.00%) (43970/48768)
Epoch: 86 | Batch_idx: 390 |  Loss: (0.2803) |  Loss2: (0.0000) | Acc: (90.00%) (45089/50000)
# TEST : Loss: (0.4372) | Acc: (86.00%) (8626/10000)
percent tensor([0.5143, 0.4857], device='cuda:0')
percent tensor([0.4563, 0.5437], device='cuda:0')
percent tensor([0.5483, 0.4517], device='cuda:0')
percent tensor([0.5908, 0.4092], device='cuda:0')
percent tensor([0.5816, 0.4184], device='cuda:0')
percent tensor([0.6938, 0.3062], device='cuda:0')
percent tensor([0.6231, 0.3769], device='cuda:0')
percent tensor([0.9985, 0.0015], device='cuda:0')
Epoch: 87 | Batch_idx: 0 |  Loss: (0.3073) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 87 | Batch_idx: 10 |  Loss: (0.2730) |  Loss2: (0.0000) | Acc: (91.00%) (1282/1408)
Epoch: 87 | Batch_idx: 20 |  Loss: (0.2673) |  Loss2: (0.0000) | Acc: (90.00%) (2434/2688)
Epoch: 87 | Batch_idx: 30 |  Loss: (0.2627) |  Loss2: (0.0000) | Acc: (90.00%) (3593/3968)
Epoch: 87 | Batch_idx: 40 |  Loss: (0.2622) |  Loss2: (0.0000) | Acc: (90.00%) (4756/5248)
Epoch: 87 | Batch_idx: 50 |  Loss: (0.2661) |  Loss2: (0.0000) | Acc: (90.00%) (5913/6528)
Epoch: 87 | Batch_idx: 60 |  Loss: (0.2660) |  Loss2: (0.0000) | Acc: (90.00%) (7074/7808)
Epoch: 87 | Batch_idx: 70 |  Loss: (0.2680) |  Loss2: (0.0000) | Acc: (90.00%) (8235/9088)
Epoch: 87 | Batch_idx: 80 |  Loss: (0.2697) |  Loss2: (0.0000) | Acc: (90.00%) (9392/10368)
Epoch: 87 | Batch_idx: 90 |  Loss: (0.2680) |  Loss2: (0.0000) | Acc: (90.00%) (10552/11648)
Epoch: 87 | Batch_idx: 100 |  Loss: (0.2691) |  Loss2: (0.0000) | Acc: (90.00%) (11708/12928)
Epoch: 87 | Batch_idx: 110 |  Loss: (0.2709) |  Loss2: (0.0000) | Acc: (90.00%) (12860/14208)
Epoch: 87 | Batch_idx: 120 |  Loss: (0.2695) |  Loss2: (0.0000) | Acc: (90.00%) (14025/15488)
Epoch: 87 | Batch_idx: 130 |  Loss: (0.2685) |  Loss2: (0.0000) | Acc: (90.00%) (15191/16768)
Epoch: 87 | Batch_idx: 140 |  Loss: (0.2689) |  Loss2: (0.0000) | Acc: (90.00%) (16360/18048)
Epoch: 87 | Batch_idx: 150 |  Loss: (0.2691) |  Loss2: (0.0000) | Acc: (90.00%) (17514/19328)
Epoch: 87 | Batch_idx: 160 |  Loss: (0.2685) |  Loss2: (0.0000) | Acc: (90.00%) (18685/20608)
Epoch: 87 | Batch_idx: 170 |  Loss: (0.2677) |  Loss2: (0.0000) | Acc: (90.00%) (19855/21888)
Epoch: 87 | Batch_idx: 180 |  Loss: (0.2669) |  Loss2: (0.0000) | Acc: (90.00%) (21025/23168)
Epoch: 87 | Batch_idx: 190 |  Loss: (0.2667) |  Loss2: (0.0000) | Acc: (90.00%) (22200/24448)
Epoch: 87 | Batch_idx: 200 |  Loss: (0.2674) |  Loss2: (0.0000) | Acc: (90.00%) (23353/25728)
Epoch: 87 | Batch_idx: 210 |  Loss: (0.2690) |  Loss2: (0.0000) | Acc: (90.00%) (24496/27008)
Epoch: 87 | Batch_idx: 220 |  Loss: (0.2689) |  Loss2: (0.0000) | Acc: (90.00%) (25660/28288)
Epoch: 87 | Batch_idx: 230 |  Loss: (0.2696) |  Loss2: (0.0000) | Acc: (90.00%) (26824/29568)
Epoch: 87 | Batch_idx: 240 |  Loss: (0.2703) |  Loss2: (0.0000) | Acc: (90.00%) (27968/30848)
Epoch: 87 | Batch_idx: 250 |  Loss: (0.2702) |  Loss2: (0.0000) | Acc: (90.00%) (29138/32128)
Epoch: 87 | Batch_idx: 260 |  Loss: (0.2697) |  Loss2: (0.0000) | Acc: (90.00%) (30300/33408)
Epoch: 87 | Batch_idx: 270 |  Loss: (0.2699) |  Loss2: (0.0000) | Acc: (90.00%) (31457/34688)
Epoch: 87 | Batch_idx: 280 |  Loss: (0.2701) |  Loss2: (0.0000) | Acc: (90.00%) (32616/35968)
Epoch: 87 | Batch_idx: 290 |  Loss: (0.2712) |  Loss2: (0.0000) | Acc: (90.00%) (33770/37248)
Epoch: 87 | Batch_idx: 300 |  Loss: (0.2711) |  Loss2: (0.0000) | Acc: (90.00%) (34938/38528)
Epoch: 87 | Batch_idx: 310 |  Loss: (0.2705) |  Loss2: (0.0000) | Acc: (90.00%) (36116/39808)
Epoch: 87 | Batch_idx: 320 |  Loss: (0.2715) |  Loss2: (0.0000) | Acc: (90.00%) (37254/41088)
Epoch: 87 | Batch_idx: 330 |  Loss: (0.2712) |  Loss2: (0.0000) | Acc: (90.00%) (38418/42368)
Epoch: 87 | Batch_idx: 340 |  Loss: (0.2720) |  Loss2: (0.0000) | Acc: (90.00%) (39560/43648)
Epoch: 87 | Batch_idx: 350 |  Loss: (0.2709) |  Loss2: (0.0000) | Acc: (90.00%) (40746/44928)
Epoch: 87 | Batch_idx: 360 |  Loss: (0.2697) |  Loss2: (0.0000) | Acc: (90.00%) (41925/46208)
Epoch: 87 | Batch_idx: 370 |  Loss: (0.2697) |  Loss2: (0.0000) | Acc: (90.00%) (43086/47488)
Epoch: 87 | Batch_idx: 380 |  Loss: (0.2692) |  Loss2: (0.0000) | Acc: (90.00%) (44270/48768)
Epoch: 87 | Batch_idx: 390 |  Loss: (0.2690) |  Loss2: (0.0000) | Acc: (90.00%) (45391/50000)
# TEST : Loss: (0.4246) | Acc: (86.00%) (8661/10000)
percent tensor([0.5134, 0.4866], device='cuda:0')
percent tensor([0.4571, 0.5429], device='cuda:0')
percent tensor([0.5445, 0.4555], device='cuda:0')
percent tensor([0.5898, 0.4102], device='cuda:0')
percent tensor([0.5810, 0.4190], device='cuda:0')
percent tensor([0.6930, 0.3070], device='cuda:0')
percent tensor([0.6326, 0.3674], device='cuda:0')
percent tensor([0.9986, 0.0014], device='cuda:0')
Epoch: 88 | Batch_idx: 0 |  Loss: (0.2535) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 88 | Batch_idx: 10 |  Loss: (0.2843) |  Loss2: (0.0000) | Acc: (90.00%) (1272/1408)
Epoch: 88 | Batch_idx: 20 |  Loss: (0.2829) |  Loss2: (0.0000) | Acc: (90.00%) (2432/2688)
Epoch: 88 | Batch_idx: 30 |  Loss: (0.2685) |  Loss2: (0.0000) | Acc: (90.00%) (3607/3968)
Epoch: 88 | Batch_idx: 40 |  Loss: (0.2617) |  Loss2: (0.0000) | Acc: (91.00%) (4784/5248)
Epoch: 88 | Batch_idx: 50 |  Loss: (0.2658) |  Loss2: (0.0000) | Acc: (91.00%) (5945/6528)
Epoch: 88 | Batch_idx: 60 |  Loss: (0.2712) |  Loss2: (0.0000) | Acc: (90.00%) (7093/7808)
Epoch: 88 | Batch_idx: 70 |  Loss: (0.2732) |  Loss2: (0.0000) | Acc: (90.00%) (8256/9088)
Epoch: 88 | Batch_idx: 80 |  Loss: (0.2711) |  Loss2: (0.0000) | Acc: (90.00%) (9426/10368)
Epoch: 88 | Batch_idx: 90 |  Loss: (0.2721) |  Loss2: (0.0000) | Acc: (90.00%) (10578/11648)
Epoch: 88 | Batch_idx: 100 |  Loss: (0.2720) |  Loss2: (0.0000) | Acc: (90.00%) (11740/12928)
Epoch: 88 | Batch_idx: 110 |  Loss: (0.2721) |  Loss2: (0.0000) | Acc: (90.00%) (12907/14208)
Epoch: 88 | Batch_idx: 120 |  Loss: (0.2708) |  Loss2: (0.0000) | Acc: (90.00%) (14070/15488)
Epoch: 88 | Batch_idx: 130 |  Loss: (0.2695) |  Loss2: (0.0000) | Acc: (90.00%) (15236/16768)
Epoch: 88 | Batch_idx: 140 |  Loss: (0.2711) |  Loss2: (0.0000) | Acc: (90.00%) (16393/18048)
Epoch: 88 | Batch_idx: 150 |  Loss: (0.2703) |  Loss2: (0.0000) | Acc: (90.00%) (17552/19328)
Epoch: 88 | Batch_idx: 160 |  Loss: (0.2710) |  Loss2: (0.0000) | Acc: (90.00%) (18705/20608)
Epoch: 88 | Batch_idx: 170 |  Loss: (0.2706) |  Loss2: (0.0000) | Acc: (90.00%) (19871/21888)
Epoch: 88 | Batch_idx: 180 |  Loss: (0.2682) |  Loss2: (0.0000) | Acc: (90.00%) (21055/23168)
Epoch: 88 | Batch_idx: 190 |  Loss: (0.2679) |  Loss2: (0.0000) | Acc: (90.00%) (22221/24448)
Epoch: 88 | Batch_idx: 200 |  Loss: (0.2676) |  Loss2: (0.0000) | Acc: (90.00%) (23397/25728)
Epoch: 88 | Batch_idx: 210 |  Loss: (0.2679) |  Loss2: (0.0000) | Acc: (90.00%) (24545/27008)
Epoch: 88 | Batch_idx: 220 |  Loss: (0.2674) |  Loss2: (0.0000) | Acc: (90.00%) (25710/28288)
Epoch: 88 | Batch_idx: 230 |  Loss: (0.2670) |  Loss2: (0.0000) | Acc: (90.00%) (26876/29568)
Epoch: 88 | Batch_idx: 240 |  Loss: (0.2676) |  Loss2: (0.0000) | Acc: (90.00%) (28034/30848)
Epoch: 88 | Batch_idx: 250 |  Loss: (0.2667) |  Loss2: (0.0000) | Acc: (90.00%) (29206/32128)
Epoch: 88 | Batch_idx: 260 |  Loss: (0.2659) |  Loss2: (0.0000) | Acc: (90.00%) (30376/33408)
Epoch: 88 | Batch_idx: 270 |  Loss: (0.2652) |  Loss2: (0.0000) | Acc: (90.00%) (31550/34688)
Epoch: 88 | Batch_idx: 280 |  Loss: (0.2648) |  Loss2: (0.0000) | Acc: (90.00%) (32706/35968)
Epoch: 88 | Batch_idx: 290 |  Loss: (0.2651) |  Loss2: (0.0000) | Acc: (90.00%) (33870/37248)
Epoch: 88 | Batch_idx: 300 |  Loss: (0.2640) |  Loss2: (0.0000) | Acc: (90.00%) (35053/38528)
Epoch: 88 | Batch_idx: 310 |  Loss: (0.2636) |  Loss2: (0.0000) | Acc: (90.00%) (36223/39808)
Epoch: 88 | Batch_idx: 320 |  Loss: (0.2639) |  Loss2: (0.0000) | Acc: (90.00%) (37386/41088)
Epoch: 88 | Batch_idx: 330 |  Loss: (0.2641) |  Loss2: (0.0000) | Acc: (90.00%) (38540/42368)
Epoch: 88 | Batch_idx: 340 |  Loss: (0.2637) |  Loss2: (0.0000) | Acc: (90.00%) (39712/43648)
Epoch: 88 | Batch_idx: 350 |  Loss: (0.2634) |  Loss2: (0.0000) | Acc: (90.00%) (40875/44928)
Epoch: 88 | Batch_idx: 360 |  Loss: (0.2636) |  Loss2: (0.0000) | Acc: (90.00%) (42030/46208)
Epoch: 88 | Batch_idx: 370 |  Loss: (0.2643) |  Loss2: (0.0000) | Acc: (90.00%) (43181/47488)
Epoch: 88 | Batch_idx: 380 |  Loss: (0.2638) |  Loss2: (0.0000) | Acc: (90.00%) (44349/48768)
Epoch: 88 | Batch_idx: 390 |  Loss: (0.2637) |  Loss2: (0.0000) | Acc: (90.00%) (45463/50000)
# TEST : Loss: (0.4158) | Acc: (86.00%) (8674/10000)
percent tensor([0.5133, 0.4867], device='cuda:0')
percent tensor([0.4572, 0.5428], device='cuda:0')
percent tensor([0.5474, 0.4526], device='cuda:0')
percent tensor([0.5920, 0.4080], device='cuda:0')
percent tensor([0.5790, 0.4210], device='cuda:0')
percent tensor([0.6932, 0.3068], device='cuda:0')
percent tensor([0.6363, 0.3637], device='cuda:0')
percent tensor([0.9986, 0.0014], device='cuda:0')
Epoch: 89 | Batch_idx: 0 |  Loss: (0.2391) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 89 | Batch_idx: 10 |  Loss: (0.2555) |  Loss2: (0.0000) | Acc: (90.00%) (1278/1408)
Epoch: 89 | Batch_idx: 20 |  Loss: (0.2522) |  Loss2: (0.0000) | Acc: (91.00%) (2453/2688)
Epoch: 89 | Batch_idx: 30 |  Loss: (0.2570) |  Loss2: (0.0000) | Acc: (91.00%) (3611/3968)
Epoch: 89 | Batch_idx: 40 |  Loss: (0.2529) |  Loss2: (0.0000) | Acc: (91.00%) (4783/5248)
Epoch: 89 | Batch_idx: 50 |  Loss: (0.2527) |  Loss2: (0.0000) | Acc: (91.00%) (5950/6528)
Epoch: 89 | Batch_idx: 60 |  Loss: (0.2509) |  Loss2: (0.0000) | Acc: (91.00%) (7125/7808)
Epoch: 89 | Batch_idx: 70 |  Loss: (0.2534) |  Loss2: (0.0000) | Acc: (91.00%) (8292/9088)
Epoch: 89 | Batch_idx: 80 |  Loss: (0.2497) |  Loss2: (0.0000) | Acc: (91.00%) (9483/10368)
Epoch: 89 | Batch_idx: 90 |  Loss: (0.2492) |  Loss2: (0.0000) | Acc: (91.00%) (10661/11648)
Epoch: 89 | Batch_idx: 100 |  Loss: (0.2490) |  Loss2: (0.0000) | Acc: (91.00%) (11824/12928)
Epoch: 89 | Batch_idx: 110 |  Loss: (0.2520) |  Loss2: (0.0000) | Acc: (91.00%) (12981/14208)
Epoch: 89 | Batch_idx: 120 |  Loss: (0.2549) |  Loss2: (0.0000) | Acc: (91.00%) (14137/15488)
Epoch: 89 | Batch_idx: 130 |  Loss: (0.2562) |  Loss2: (0.0000) | Acc: (91.00%) (15297/16768)
Epoch: 89 | Batch_idx: 140 |  Loss: (0.2565) |  Loss2: (0.0000) | Acc: (91.00%) (16455/18048)
Epoch: 89 | Batch_idx: 150 |  Loss: (0.2548) |  Loss2: (0.0000) | Acc: (91.00%) (17636/19328)
Epoch: 89 | Batch_idx: 160 |  Loss: (0.2548) |  Loss2: (0.0000) | Acc: (91.00%) (18805/20608)
Epoch: 89 | Batch_idx: 170 |  Loss: (0.2556) |  Loss2: (0.0000) | Acc: (91.00%) (19956/21888)
Epoch: 89 | Batch_idx: 180 |  Loss: (0.2556) |  Loss2: (0.0000) | Acc: (91.00%) (21125/23168)
Epoch: 89 | Batch_idx: 190 |  Loss: (0.2567) |  Loss2: (0.0000) | Acc: (91.00%) (22287/24448)
Epoch: 89 | Batch_idx: 200 |  Loss: (0.2567) |  Loss2: (0.0000) | Acc: (91.00%) (23454/25728)
Epoch: 89 | Batch_idx: 210 |  Loss: (0.2575) |  Loss2: (0.0000) | Acc: (91.00%) (24604/27008)
Epoch: 89 | Batch_idx: 220 |  Loss: (0.2571) |  Loss2: (0.0000) | Acc: (91.00%) (25783/28288)
Epoch: 89 | Batch_idx: 230 |  Loss: (0.2566) |  Loss2: (0.0000) | Acc: (91.00%) (26948/29568)
Epoch: 89 | Batch_idx: 240 |  Loss: (0.2557) |  Loss2: (0.0000) | Acc: (91.00%) (28124/30848)
Epoch: 89 | Batch_idx: 250 |  Loss: (0.2555) |  Loss2: (0.0000) | Acc: (91.00%) (29294/32128)
Epoch: 89 | Batch_idx: 260 |  Loss: (0.2554) |  Loss2: (0.0000) | Acc: (91.00%) (30460/33408)
Epoch: 89 | Batch_idx: 270 |  Loss: (0.2541) |  Loss2: (0.0000) | Acc: (91.00%) (31642/34688)
Epoch: 89 | Batch_idx: 280 |  Loss: (0.2544) |  Loss2: (0.0000) | Acc: (91.00%) (32806/35968)
Epoch: 89 | Batch_idx: 290 |  Loss: (0.2551) |  Loss2: (0.0000) | Acc: (91.00%) (33968/37248)
Epoch: 89 | Batch_idx: 300 |  Loss: (0.2557) |  Loss2: (0.0000) | Acc: (91.00%) (35123/38528)
Epoch: 89 | Batch_idx: 310 |  Loss: (0.2553) |  Loss2: (0.0000) | Acc: (91.00%) (36288/39808)
Epoch: 89 | Batch_idx: 320 |  Loss: (0.2568) |  Loss2: (0.0000) | Acc: (91.00%) (37429/41088)
Epoch: 89 | Batch_idx: 330 |  Loss: (0.2573) |  Loss2: (0.0000) | Acc: (91.00%) (38584/42368)
Epoch: 89 | Batch_idx: 340 |  Loss: (0.2570) |  Loss2: (0.0000) | Acc: (91.00%) (39756/43648)
Epoch: 89 | Batch_idx: 350 |  Loss: (0.2584) |  Loss2: (0.0000) | Acc: (91.00%) (40910/44928)
Epoch: 89 | Batch_idx: 360 |  Loss: (0.2585) |  Loss2: (0.0000) | Acc: (91.00%) (42067/46208)
Epoch: 89 | Batch_idx: 370 |  Loss: (0.2591) |  Loss2: (0.0000) | Acc: (91.00%) (43229/47488)
Epoch: 89 | Batch_idx: 380 |  Loss: (0.2591) |  Loss2: (0.0000) | Acc: (91.00%) (44397/48768)
Epoch: 89 | Batch_idx: 390 |  Loss: (0.2588) |  Loss2: (0.0000) | Acc: (91.00%) (45514/50000)
# TEST : Loss: (0.4110) | Acc: (86.00%) (8693/10000)
percent tensor([0.5133, 0.4867], device='cuda:0')
percent tensor([0.4583, 0.5417], device='cuda:0')
percent tensor([0.5472, 0.4528], device='cuda:0')
percent tensor([0.5915, 0.4085], device='cuda:0')
percent tensor([0.5757, 0.4243], device='cuda:0')
percent tensor([0.6914, 0.3086], device='cuda:0')
percent tensor([0.6411, 0.3589], device='cuda:0')
percent tensor([0.9986, 0.0014], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 90 | Batch_idx: 0 |  Loss: (0.2387) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 90 | Batch_idx: 10 |  Loss: (0.2643) |  Loss2: (0.0000) | Acc: (91.00%) (1285/1408)
Epoch: 90 | Batch_idx: 20 |  Loss: (0.2471) |  Loss2: (0.0000) | Acc: (91.00%) (2471/2688)
Epoch: 90 | Batch_idx: 30 |  Loss: (0.2484) |  Loss2: (0.0000) | Acc: (91.00%) (3636/3968)
Epoch: 90 | Batch_idx: 40 |  Loss: (0.2566) |  Loss2: (0.0000) | Acc: (90.00%) (4772/5248)
Epoch: 90 | Batch_idx: 50 |  Loss: (0.2568) |  Loss2: (0.0000) | Acc: (90.00%) (5928/6528)
Epoch: 90 | Batch_idx: 60 |  Loss: (0.2571) |  Loss2: (0.0000) | Acc: (90.00%) (7089/7808)
Epoch: 90 | Batch_idx: 70 |  Loss: (0.2543) |  Loss2: (0.0000) | Acc: (90.00%) (8261/9088)
Epoch: 90 | Batch_idx: 80 |  Loss: (0.2543) |  Loss2: (0.0000) | Acc: (90.00%) (9431/10368)
Epoch: 90 | Batch_idx: 90 |  Loss: (0.2520) |  Loss2: (0.0000) | Acc: (91.00%) (10611/11648)
Epoch: 90 | Batch_idx: 100 |  Loss: (0.2539) |  Loss2: (0.0000) | Acc: (91.00%) (11771/12928)
Epoch: 90 | Batch_idx: 110 |  Loss: (0.2546) |  Loss2: (0.0000) | Acc: (91.00%) (12932/14208)
Epoch: 90 | Batch_idx: 120 |  Loss: (0.2545) |  Loss2: (0.0000) | Acc: (91.00%) (14105/15488)
Epoch: 90 | Batch_idx: 130 |  Loss: (0.2548) |  Loss2: (0.0000) | Acc: (91.00%) (15267/16768)
Epoch: 90 | Batch_idx: 140 |  Loss: (0.2559) |  Loss2: (0.0000) | Acc: (91.00%) (16426/18048)
Epoch: 90 | Batch_idx: 150 |  Loss: (0.2586) |  Loss2: (0.0000) | Acc: (90.00%) (17573/19328)
Epoch: 90 | Batch_idx: 160 |  Loss: (0.2605) |  Loss2: (0.0000) | Acc: (90.00%) (18726/20608)
Epoch: 90 | Batch_idx: 170 |  Loss: (0.2625) |  Loss2: (0.0000) | Acc: (90.00%) (19875/21888)
Epoch: 90 | Batch_idx: 180 |  Loss: (0.2635) |  Loss2: (0.0000) | Acc: (90.00%) (21030/23168)
Epoch: 90 | Batch_idx: 190 |  Loss: (0.2640) |  Loss2: (0.0000) | Acc: (90.00%) (22200/24448)
Epoch: 90 | Batch_idx: 200 |  Loss: (0.2656) |  Loss2: (0.0000) | Acc: (90.00%) (23355/25728)
Epoch: 90 | Batch_idx: 210 |  Loss: (0.2643) |  Loss2: (0.0000) | Acc: (90.00%) (24517/27008)
Epoch: 90 | Batch_idx: 220 |  Loss: (0.2637) |  Loss2: (0.0000) | Acc: (90.00%) (25688/28288)
Epoch: 90 | Batch_idx: 230 |  Loss: (0.2639) |  Loss2: (0.0000) | Acc: (90.00%) (26849/29568)
Epoch: 90 | Batch_idx: 240 |  Loss: (0.2648) |  Loss2: (0.0000) | Acc: (90.00%) (28012/30848)
Epoch: 90 | Batch_idx: 250 |  Loss: (0.2645) |  Loss2: (0.0000) | Acc: (90.00%) (29175/32128)
Epoch: 90 | Batch_idx: 260 |  Loss: (0.2644) |  Loss2: (0.0000) | Acc: (90.00%) (30347/33408)
Epoch: 90 | Batch_idx: 270 |  Loss: (0.2649) |  Loss2: (0.0000) | Acc: (90.00%) (31493/34688)
Epoch: 90 | Batch_idx: 280 |  Loss: (0.2645) |  Loss2: (0.0000) | Acc: (90.00%) (32665/35968)
Epoch: 90 | Batch_idx: 290 |  Loss: (0.2643) |  Loss2: (0.0000) | Acc: (90.00%) (33827/37248)
Epoch: 90 | Batch_idx: 300 |  Loss: (0.2643) |  Loss2: (0.0000) | Acc: (90.00%) (34991/38528)
Epoch: 90 | Batch_idx: 310 |  Loss: (0.2644) |  Loss2: (0.0000) | Acc: (90.00%) (36151/39808)
Epoch: 90 | Batch_idx: 320 |  Loss: (0.2650) |  Loss2: (0.0000) | Acc: (90.00%) (37306/41088)
Epoch: 90 | Batch_idx: 330 |  Loss: (0.2649) |  Loss2: (0.0000) | Acc: (90.00%) (38463/42368)
Epoch: 90 | Batch_idx: 340 |  Loss: (0.2656) |  Loss2: (0.0000) | Acc: (90.00%) (39623/43648)
Epoch: 90 | Batch_idx: 350 |  Loss: (0.2658) |  Loss2: (0.0000) | Acc: (90.00%) (40781/44928)
Epoch: 90 | Batch_idx: 360 |  Loss: (0.2653) |  Loss2: (0.0000) | Acc: (90.00%) (41944/46208)
Epoch: 90 | Batch_idx: 370 |  Loss: (0.2647) |  Loss2: (0.0000) | Acc: (90.00%) (43121/47488)
Epoch: 90 | Batch_idx: 380 |  Loss: (0.2633) |  Loss2: (0.0000) | Acc: (90.00%) (44323/48768)
Epoch: 90 | Batch_idx: 390 |  Loss: (0.2644) |  Loss2: (0.0000) | Acc: (90.00%) (45429/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_090.pth.tar'
# TEST : Loss: (0.4759) | Acc: (84.00%) (8456/10000)
percent tensor([0.5130, 0.4870], device='cuda:0')
percent tensor([0.4583, 0.5417], device='cuda:0')
percent tensor([0.5476, 0.4524], device='cuda:0')
percent tensor([0.5901, 0.4099], device='cuda:0')
percent tensor([0.5736, 0.4264], device='cuda:0')
percent tensor([0.6895, 0.3105], device='cuda:0')
percent tensor([0.6382, 0.3618], device='cuda:0')
percent tensor([0.9985, 0.0015], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(179.3275, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(810.3481, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(801.4761, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1516.7422, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(504.5753, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2223.9951, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4276.9971, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1399.2454, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6154.4019, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11909.0967, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3945.0779, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16710.4844, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 91 | Batch_idx: 0 |  Loss: (0.1295) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 91 | Batch_idx: 10 |  Loss: (0.2191) |  Loss2: (0.0000) | Acc: (92.00%) (1303/1408)
Epoch: 91 | Batch_idx: 20 |  Loss: (0.2433) |  Loss2: (0.0000) | Acc: (91.00%) (2459/2688)
Epoch: 91 | Batch_idx: 30 |  Loss: (0.2397) |  Loss2: (0.0000) | Acc: (91.00%) (3636/3968)
Epoch: 91 | Batch_idx: 40 |  Loss: (0.2403) |  Loss2: (0.0000) | Acc: (91.00%) (4818/5248)
Epoch: 91 | Batch_idx: 50 |  Loss: (0.2482) |  Loss2: (0.0000) | Acc: (91.00%) (5981/6528)
Epoch: 91 | Batch_idx: 60 |  Loss: (0.2463) |  Loss2: (0.0000) | Acc: (91.00%) (7159/7808)
Epoch: 91 | Batch_idx: 70 |  Loss: (0.2443) |  Loss2: (0.0000) | Acc: (91.00%) (8333/9088)
Epoch: 91 | Batch_idx: 80 |  Loss: (0.2461) |  Loss2: (0.0000) | Acc: (91.00%) (9497/10368)
Epoch: 91 | Batch_idx: 90 |  Loss: (0.2468) |  Loss2: (0.0000) | Acc: (91.00%) (10663/11648)
Epoch: 91 | Batch_idx: 100 |  Loss: (0.2487) |  Loss2: (0.0000) | Acc: (91.00%) (11823/12928)
Epoch: 91 | Batch_idx: 110 |  Loss: (0.2480) |  Loss2: (0.0000) | Acc: (91.00%) (12992/14208)
Epoch: 91 | Batch_idx: 120 |  Loss: (0.2481) |  Loss2: (0.0000) | Acc: (91.00%) (14177/15488)
Epoch: 91 | Batch_idx: 130 |  Loss: (0.2503) |  Loss2: (0.0000) | Acc: (91.00%) (15327/16768)
Epoch: 91 | Batch_idx: 140 |  Loss: (0.2510) |  Loss2: (0.0000) | Acc: (91.00%) (16488/18048)
Epoch: 91 | Batch_idx: 150 |  Loss: (0.2502) |  Loss2: (0.0000) | Acc: (91.00%) (17665/19328)
Epoch: 91 | Batch_idx: 160 |  Loss: (0.2497) |  Loss2: (0.0000) | Acc: (91.00%) (18831/20608)
Epoch: 91 | Batch_idx: 170 |  Loss: (0.2516) |  Loss2: (0.0000) | Acc: (91.00%) (19996/21888)
Epoch: 91 | Batch_idx: 180 |  Loss: (0.2519) |  Loss2: (0.0000) | Acc: (91.00%) (21159/23168)
Epoch: 91 | Batch_idx: 190 |  Loss: (0.2515) |  Loss2: (0.0000) | Acc: (91.00%) (22329/24448)
Epoch: 91 | Batch_idx: 200 |  Loss: (0.2524) |  Loss2: (0.0000) | Acc: (91.00%) (23486/25728)
Epoch: 91 | Batch_idx: 210 |  Loss: (0.2525) |  Loss2: (0.0000) | Acc: (91.00%) (24658/27008)
Epoch: 91 | Batch_idx: 220 |  Loss: (0.2522) |  Loss2: (0.0000) | Acc: (91.00%) (25830/28288)
Epoch: 91 | Batch_idx: 230 |  Loss: (0.2498) |  Loss2: (0.0000) | Acc: (91.00%) (27017/29568)
Epoch: 91 | Batch_idx: 240 |  Loss: (0.2510) |  Loss2: (0.0000) | Acc: (91.00%) (28174/30848)
Epoch: 91 | Batch_idx: 250 |  Loss: (0.2519) |  Loss2: (0.0000) | Acc: (91.00%) (29341/32128)
Epoch: 91 | Batch_idx: 260 |  Loss: (0.2511) |  Loss2: (0.0000) | Acc: (91.00%) (30533/33408)
Epoch: 91 | Batch_idx: 270 |  Loss: (0.2519) |  Loss2: (0.0000) | Acc: (91.00%) (31692/34688)
Epoch: 91 | Batch_idx: 280 |  Loss: (0.2521) |  Loss2: (0.0000) | Acc: (91.00%) (32861/35968)
Epoch: 91 | Batch_idx: 290 |  Loss: (0.2531) |  Loss2: (0.0000) | Acc: (91.00%) (34023/37248)
Epoch: 91 | Batch_idx: 300 |  Loss: (0.2543) |  Loss2: (0.0000) | Acc: (91.00%) (35173/38528)
Epoch: 91 | Batch_idx: 310 |  Loss: (0.2552) |  Loss2: (0.0000) | Acc: (91.00%) (36329/39808)
Epoch: 91 | Batch_idx: 320 |  Loss: (0.2561) |  Loss2: (0.0000) | Acc: (91.00%) (37485/41088)
Epoch: 91 | Batch_idx: 330 |  Loss: (0.2565) |  Loss2: (0.0000) | Acc: (91.00%) (38642/42368)
Epoch: 91 | Batch_idx: 340 |  Loss: (0.2562) |  Loss2: (0.0000) | Acc: (91.00%) (39821/43648)
Epoch: 91 | Batch_idx: 350 |  Loss: (0.2571) |  Loss2: (0.0000) | Acc: (91.00%) (40987/44928)
Epoch: 91 | Batch_idx: 360 |  Loss: (0.2571) |  Loss2: (0.0000) | Acc: (91.00%) (42162/46208)
Epoch: 91 | Batch_idx: 370 |  Loss: (0.2572) |  Loss2: (0.0000) | Acc: (91.00%) (43324/47488)
Epoch: 91 | Batch_idx: 380 |  Loss: (0.2573) |  Loss2: (0.0000) | Acc: (91.00%) (44488/48768)
Epoch: 91 | Batch_idx: 390 |  Loss: (0.2577) |  Loss2: (0.0000) | Acc: (91.00%) (45591/50000)
# TEST : Loss: (0.4305) | Acc: (86.00%) (8628/10000)
percent tensor([0.5131, 0.4869], device='cuda:0')
percent tensor([0.4584, 0.5416], device='cuda:0')
percent tensor([0.5466, 0.4534], device='cuda:0')
percent tensor([0.5909, 0.4091], device='cuda:0')
percent tensor([0.5740, 0.4260], device='cuda:0')
percent tensor([0.6913, 0.3087], device='cuda:0')
percent tensor([0.6423, 0.3577], device='cuda:0')
percent tensor([0.9987, 0.0013], device='cuda:0')
Epoch: 92 | Batch_idx: 0 |  Loss: (0.2360) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 92 | Batch_idx: 10 |  Loss: (0.2478) |  Loss2: (0.0000) | Acc: (91.00%) (1292/1408)
Epoch: 92 | Batch_idx: 20 |  Loss: (0.2385) |  Loss2: (0.0000) | Acc: (92.00%) (2486/2688)
Epoch: 92 | Batch_idx: 30 |  Loss: (0.2456) |  Loss2: (0.0000) | Acc: (91.00%) (3647/3968)
Epoch: 92 | Batch_idx: 40 |  Loss: (0.2461) |  Loss2: (0.0000) | Acc: (91.00%) (4811/5248)
Epoch: 92 | Batch_idx: 50 |  Loss: (0.2458) |  Loss2: (0.0000) | Acc: (91.00%) (5988/6528)
Epoch: 92 | Batch_idx: 60 |  Loss: (0.2434) |  Loss2: (0.0000) | Acc: (91.00%) (7162/7808)
Epoch: 92 | Batch_idx: 70 |  Loss: (0.2447) |  Loss2: (0.0000) | Acc: (91.00%) (8334/9088)
Epoch: 92 | Batch_idx: 80 |  Loss: (0.2425) |  Loss2: (0.0000) | Acc: (91.00%) (9524/10368)
Epoch: 92 | Batch_idx: 90 |  Loss: (0.2477) |  Loss2: (0.0000) | Acc: (91.00%) (10668/11648)
Epoch: 92 | Batch_idx: 100 |  Loss: (0.2468) |  Loss2: (0.0000) | Acc: (91.00%) (11848/12928)
Epoch: 92 | Batch_idx: 110 |  Loss: (0.2469) |  Loss2: (0.0000) | Acc: (91.00%) (13022/14208)
Epoch: 92 | Batch_idx: 120 |  Loss: (0.2469) |  Loss2: (0.0000) | Acc: (91.00%) (14203/15488)
Epoch: 92 | Batch_idx: 130 |  Loss: (0.2468) |  Loss2: (0.0000) | Acc: (91.00%) (15386/16768)
Epoch: 92 | Batch_idx: 140 |  Loss: (0.2466) |  Loss2: (0.0000) | Acc: (91.00%) (16558/18048)
Epoch: 92 | Batch_idx: 150 |  Loss: (0.2461) |  Loss2: (0.0000) | Acc: (91.00%) (17731/19328)
Epoch: 92 | Batch_idx: 160 |  Loss: (0.2457) |  Loss2: (0.0000) | Acc: (91.00%) (18897/20608)
Epoch: 92 | Batch_idx: 170 |  Loss: (0.2444) |  Loss2: (0.0000) | Acc: (91.00%) (20081/21888)
Epoch: 92 | Batch_idx: 180 |  Loss: (0.2440) |  Loss2: (0.0000) | Acc: (91.00%) (21261/23168)
Epoch: 92 | Batch_idx: 190 |  Loss: (0.2443) |  Loss2: (0.0000) | Acc: (91.00%) (22441/24448)
Epoch: 92 | Batch_idx: 200 |  Loss: (0.2438) |  Loss2: (0.0000) | Acc: (91.00%) (23620/25728)
Epoch: 92 | Batch_idx: 210 |  Loss: (0.2441) |  Loss2: (0.0000) | Acc: (91.00%) (24783/27008)
Epoch: 92 | Batch_idx: 220 |  Loss: (0.2428) |  Loss2: (0.0000) | Acc: (91.00%) (25965/28288)
Epoch: 92 | Batch_idx: 230 |  Loss: (0.2428) |  Loss2: (0.0000) | Acc: (91.00%) (27148/29568)
Epoch: 92 | Batch_idx: 240 |  Loss: (0.2430) |  Loss2: (0.0000) | Acc: (91.00%) (28318/30848)
Epoch: 92 | Batch_idx: 250 |  Loss: (0.2434) |  Loss2: (0.0000) | Acc: (91.00%) (29476/32128)
Epoch: 92 | Batch_idx: 260 |  Loss: (0.2433) |  Loss2: (0.0000) | Acc: (91.00%) (30646/33408)
Epoch: 92 | Batch_idx: 270 |  Loss: (0.2433) |  Loss2: (0.0000) | Acc: (91.00%) (31821/34688)
Epoch: 92 | Batch_idx: 280 |  Loss: (0.2430) |  Loss2: (0.0000) | Acc: (91.00%) (32996/35968)
Epoch: 92 | Batch_idx: 290 |  Loss: (0.2432) |  Loss2: (0.0000) | Acc: (91.00%) (34167/37248)
Epoch: 92 | Batch_idx: 300 |  Loss: (0.2450) |  Loss2: (0.0000) | Acc: (91.00%) (35322/38528)
Epoch: 92 | Batch_idx: 310 |  Loss: (0.2450) |  Loss2: (0.0000) | Acc: (91.00%) (36485/39808)
Epoch: 92 | Batch_idx: 320 |  Loss: (0.2452) |  Loss2: (0.0000) | Acc: (91.00%) (37648/41088)
Epoch: 92 | Batch_idx: 330 |  Loss: (0.2466) |  Loss2: (0.0000) | Acc: (91.00%) (38800/42368)
Epoch: 92 | Batch_idx: 340 |  Loss: (0.2470) |  Loss2: (0.0000) | Acc: (91.00%) (39960/43648)
Epoch: 92 | Batch_idx: 350 |  Loss: (0.2475) |  Loss2: (0.0000) | Acc: (91.00%) (41134/44928)
Epoch: 92 | Batch_idx: 360 |  Loss: (0.2476) |  Loss2: (0.0000) | Acc: (91.00%) (42305/46208)
Epoch: 92 | Batch_idx: 370 |  Loss: (0.2481) |  Loss2: (0.0000) | Acc: (91.00%) (43471/47488)
Epoch: 92 | Batch_idx: 380 |  Loss: (0.2479) |  Loss2: (0.0000) | Acc: (91.00%) (44646/48768)
Epoch: 92 | Batch_idx: 390 |  Loss: (0.2477) |  Loss2: (0.0000) | Acc: (91.00%) (45785/50000)
# TEST : Loss: (0.4125) | Acc: (87.00%) (8700/10000)
percent tensor([0.5129, 0.4871], device='cuda:0')
percent tensor([0.4583, 0.5417], device='cuda:0')
percent tensor([0.5470, 0.4530], device='cuda:0')
percent tensor([0.5910, 0.4090], device='cuda:0')
percent tensor([0.5736, 0.4264], device='cuda:0')
percent tensor([0.6918, 0.3082], device='cuda:0')
percent tensor([0.6392, 0.3608], device='cuda:0')
percent tensor([0.9987, 0.0013], device='cuda:0')
Epoch: 93 | Batch_idx: 0 |  Loss: (0.2047) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 93 | Batch_idx: 10 |  Loss: (0.2261) |  Loss2: (0.0000) | Acc: (92.00%) (1305/1408)
Epoch: 93 | Batch_idx: 20 |  Loss: (0.2477) |  Loss2: (0.0000) | Acc: (91.00%) (2464/2688)
Epoch: 93 | Batch_idx: 30 |  Loss: (0.2527) |  Loss2: (0.0000) | Acc: (91.00%) (3627/3968)
Epoch: 93 | Batch_idx: 40 |  Loss: (0.2555) |  Loss2: (0.0000) | Acc: (91.00%) (4796/5248)
Epoch: 93 | Batch_idx: 50 |  Loss: (0.2519) |  Loss2: (0.0000) | Acc: (91.00%) (5956/6528)
Epoch: 93 | Batch_idx: 60 |  Loss: (0.2521) |  Loss2: (0.0000) | Acc: (91.00%) (7126/7808)
Epoch: 93 | Batch_idx: 70 |  Loss: (0.2502) |  Loss2: (0.0000) | Acc: (91.00%) (8300/9088)
Epoch: 93 | Batch_idx: 80 |  Loss: (0.2487) |  Loss2: (0.0000) | Acc: (91.00%) (9480/10368)
Epoch: 93 | Batch_idx: 90 |  Loss: (0.2457) |  Loss2: (0.0000) | Acc: (91.00%) (10659/11648)
Epoch: 93 | Batch_idx: 100 |  Loss: (0.2431) |  Loss2: (0.0000) | Acc: (91.00%) (11843/12928)
Epoch: 93 | Batch_idx: 110 |  Loss: (0.2466) |  Loss2: (0.0000) | Acc: (91.00%) (12997/14208)
Epoch: 93 | Batch_idx: 120 |  Loss: (0.2443) |  Loss2: (0.0000) | Acc: (91.00%) (14178/15488)
Epoch: 93 | Batch_idx: 130 |  Loss: (0.2431) |  Loss2: (0.0000) | Acc: (91.00%) (15364/16768)
Epoch: 93 | Batch_idx: 140 |  Loss: (0.2454) |  Loss2: (0.0000) | Acc: (91.00%) (16521/18048)
Epoch: 93 | Batch_idx: 150 |  Loss: (0.2450) |  Loss2: (0.0000) | Acc: (91.00%) (17680/19328)
Epoch: 93 | Batch_idx: 160 |  Loss: (0.2434) |  Loss2: (0.0000) | Acc: (91.00%) (18854/20608)
Epoch: 93 | Batch_idx: 170 |  Loss: (0.2424) |  Loss2: (0.0000) | Acc: (91.00%) (20040/21888)
Epoch: 93 | Batch_idx: 180 |  Loss: (0.2417) |  Loss2: (0.0000) | Acc: (91.00%) (21217/23168)
Epoch: 93 | Batch_idx: 190 |  Loss: (0.2428) |  Loss2: (0.0000) | Acc: (91.00%) (22376/24448)
Epoch: 93 | Batch_idx: 200 |  Loss: (0.2425) |  Loss2: (0.0000) | Acc: (91.00%) (23549/25728)
Epoch: 93 | Batch_idx: 210 |  Loss: (0.2418) |  Loss2: (0.0000) | Acc: (91.00%) (24730/27008)
Epoch: 93 | Batch_idx: 220 |  Loss: (0.2417) |  Loss2: (0.0000) | Acc: (91.00%) (25905/28288)
Epoch: 93 | Batch_idx: 230 |  Loss: (0.2425) |  Loss2: (0.0000) | Acc: (91.00%) (27066/29568)
Epoch: 93 | Batch_idx: 240 |  Loss: (0.2422) |  Loss2: (0.0000) | Acc: (91.00%) (28247/30848)
Epoch: 93 | Batch_idx: 250 |  Loss: (0.2428) |  Loss2: (0.0000) | Acc: (91.00%) (29407/32128)
Epoch: 93 | Batch_idx: 260 |  Loss: (0.2428) |  Loss2: (0.0000) | Acc: (91.00%) (30579/33408)
Epoch: 93 | Batch_idx: 270 |  Loss: (0.2441) |  Loss2: (0.0000) | Acc: (91.00%) (31745/34688)
Epoch: 93 | Batch_idx: 280 |  Loss: (0.2441) |  Loss2: (0.0000) | Acc: (91.00%) (32926/35968)
Epoch: 93 | Batch_idx: 290 |  Loss: (0.2436) |  Loss2: (0.0000) | Acc: (91.00%) (34109/37248)
Epoch: 93 | Batch_idx: 300 |  Loss: (0.2436) |  Loss2: (0.0000) | Acc: (91.00%) (35273/38528)
Epoch: 93 | Batch_idx: 310 |  Loss: (0.2442) |  Loss2: (0.0000) | Acc: (91.00%) (36445/39808)
Epoch: 93 | Batch_idx: 320 |  Loss: (0.2445) |  Loss2: (0.0000) | Acc: (91.00%) (37618/41088)
Epoch: 93 | Batch_idx: 330 |  Loss: (0.2451) |  Loss2: (0.0000) | Acc: (91.00%) (38781/42368)
Epoch: 93 | Batch_idx: 340 |  Loss: (0.2450) |  Loss2: (0.0000) | Acc: (91.00%) (39956/43648)
Epoch: 93 | Batch_idx: 350 |  Loss: (0.2449) |  Loss2: (0.0000) | Acc: (91.00%) (41128/44928)
Epoch: 93 | Batch_idx: 360 |  Loss: (0.2447) |  Loss2: (0.0000) | Acc: (91.00%) (42297/46208)
Epoch: 93 | Batch_idx: 370 |  Loss: (0.2447) |  Loss2: (0.0000) | Acc: (91.00%) (43473/47488)
Epoch: 93 | Batch_idx: 380 |  Loss: (0.2451) |  Loss2: (0.0000) | Acc: (91.00%) (44644/48768)
Epoch: 93 | Batch_idx: 390 |  Loss: (0.2454) |  Loss2: (0.0000) | Acc: (91.00%) (45769/50000)
# TEST : Loss: (0.4927) | Acc: (85.00%) (8528/10000)
percent tensor([0.5130, 0.4870], device='cuda:0')
percent tensor([0.4589, 0.5411], device='cuda:0')
percent tensor([0.5469, 0.4531], device='cuda:0')
percent tensor([0.5914, 0.4086], device='cuda:0')
percent tensor([0.5750, 0.4250], device='cuda:0')
percent tensor([0.6920, 0.3080], device='cuda:0')
percent tensor([0.6464, 0.3536], device='cuda:0')
percent tensor([0.9988, 0.0012], device='cuda:0')
Epoch: 94 | Batch_idx: 0 |  Loss: (0.2473) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 94 | Batch_idx: 10 |  Loss: (0.2385) |  Loss2: (0.0000) | Acc: (92.00%) (1302/1408)
Epoch: 94 | Batch_idx: 20 |  Loss: (0.2226) |  Loss2: (0.0000) | Acc: (92.00%) (2490/2688)
Epoch: 94 | Batch_idx: 30 |  Loss: (0.2240) |  Loss2: (0.0000) | Acc: (92.00%) (3676/3968)
Epoch: 94 | Batch_idx: 40 |  Loss: (0.2326) |  Loss2: (0.0000) | Acc: (92.00%) (4848/5248)
Epoch: 94 | Batch_idx: 50 |  Loss: (0.2332) |  Loss2: (0.0000) | Acc: (92.00%) (6025/6528)
Epoch: 94 | Batch_idx: 60 |  Loss: (0.2334) |  Loss2: (0.0000) | Acc: (92.00%) (7209/7808)
Epoch: 94 | Batch_idx: 70 |  Loss: (0.2334) |  Loss2: (0.0000) | Acc: (92.00%) (8390/9088)
Epoch: 94 | Batch_idx: 80 |  Loss: (0.2372) |  Loss2: (0.0000) | Acc: (92.00%) (9558/10368)
Epoch: 94 | Batch_idx: 90 |  Loss: (0.2384) |  Loss2: (0.0000) | Acc: (92.00%) (10732/11648)
Epoch: 94 | Batch_idx: 100 |  Loss: (0.2377) |  Loss2: (0.0000) | Acc: (92.00%) (11909/12928)
Epoch: 94 | Batch_idx: 110 |  Loss: (0.2374) |  Loss2: (0.0000) | Acc: (92.00%) (13088/14208)
Epoch: 94 | Batch_idx: 120 |  Loss: (0.2379) |  Loss2: (0.0000) | Acc: (92.00%) (14256/15488)
Epoch: 94 | Batch_idx: 130 |  Loss: (0.2374) |  Loss2: (0.0000) | Acc: (92.00%) (15433/16768)
Epoch: 94 | Batch_idx: 140 |  Loss: (0.2379) |  Loss2: (0.0000) | Acc: (91.00%) (16591/18048)
Epoch: 94 | Batch_idx: 150 |  Loss: (0.2407) |  Loss2: (0.0000) | Acc: (91.00%) (17737/19328)
Epoch: 94 | Batch_idx: 160 |  Loss: (0.2398) |  Loss2: (0.0000) | Acc: (91.00%) (18918/20608)
Epoch: 94 | Batch_idx: 170 |  Loss: (0.2394) |  Loss2: (0.0000) | Acc: (91.00%) (20093/21888)
Epoch: 94 | Batch_idx: 180 |  Loss: (0.2399) |  Loss2: (0.0000) | Acc: (91.00%) (21249/23168)
Epoch: 94 | Batch_idx: 190 |  Loss: (0.2396) |  Loss2: (0.0000) | Acc: (91.00%) (22429/24448)
Epoch: 94 | Batch_idx: 200 |  Loss: (0.2401) |  Loss2: (0.0000) | Acc: (91.00%) (23599/25728)
Epoch: 94 | Batch_idx: 210 |  Loss: (0.2402) |  Loss2: (0.0000) | Acc: (91.00%) (24768/27008)
Epoch: 94 | Batch_idx: 220 |  Loss: (0.2400) |  Loss2: (0.0000) | Acc: (91.00%) (25940/28288)
Epoch: 94 | Batch_idx: 230 |  Loss: (0.2400) |  Loss2: (0.0000) | Acc: (91.00%) (27106/29568)
Epoch: 94 | Batch_idx: 240 |  Loss: (0.2394) |  Loss2: (0.0000) | Acc: (91.00%) (28282/30848)
Epoch: 94 | Batch_idx: 250 |  Loss: (0.2386) |  Loss2: (0.0000) | Acc: (91.00%) (29462/32128)
Epoch: 94 | Batch_idx: 260 |  Loss: (0.2382) |  Loss2: (0.0000) | Acc: (91.00%) (30646/33408)
Epoch: 94 | Batch_idx: 270 |  Loss: (0.2379) |  Loss2: (0.0000) | Acc: (91.00%) (31829/34688)
Epoch: 94 | Batch_idx: 280 |  Loss: (0.2380) |  Loss2: (0.0000) | Acc: (91.00%) (32999/35968)
Epoch: 94 | Batch_idx: 290 |  Loss: (0.2379) |  Loss2: (0.0000) | Acc: (91.00%) (34175/37248)
Epoch: 94 | Batch_idx: 300 |  Loss: (0.2375) |  Loss2: (0.0000) | Acc: (91.00%) (35359/38528)
Epoch: 94 | Batch_idx: 310 |  Loss: (0.2376) |  Loss2: (0.0000) | Acc: (91.00%) (36530/39808)
Epoch: 94 | Batch_idx: 320 |  Loss: (0.2395) |  Loss2: (0.0000) | Acc: (91.00%) (37672/41088)
Epoch: 94 | Batch_idx: 330 |  Loss: (0.2397) |  Loss2: (0.0000) | Acc: (91.00%) (38837/42368)
Epoch: 94 | Batch_idx: 340 |  Loss: (0.2397) |  Loss2: (0.0000) | Acc: (91.00%) (40010/43648)
Epoch: 94 | Batch_idx: 350 |  Loss: (0.2399) |  Loss2: (0.0000) | Acc: (91.00%) (41179/44928)
Epoch: 94 | Batch_idx: 360 |  Loss: (0.2399) |  Loss2: (0.0000) | Acc: (91.00%) (42345/46208)
Epoch: 94 | Batch_idx: 370 |  Loss: (0.2410) |  Loss2: (0.0000) | Acc: (91.00%) (43508/47488)
Epoch: 94 | Batch_idx: 380 |  Loss: (0.2411) |  Loss2: (0.0000) | Acc: (91.00%) (44680/48768)
Epoch: 94 | Batch_idx: 390 |  Loss: (0.2416) |  Loss2: (0.0000) | Acc: (91.00%) (45792/50000)
# TEST : Loss: (0.4521) | Acc: (85.00%) (8534/10000)
percent tensor([0.5132, 0.4868], device='cuda:0')
percent tensor([0.4587, 0.5413], device='cuda:0')
percent tensor([0.5462, 0.4538], device='cuda:0')
percent tensor([0.5897, 0.4103], device='cuda:0')
percent tensor([0.5748, 0.4252], device='cuda:0')
percent tensor([0.6883, 0.3117], device='cuda:0')
percent tensor([0.6407, 0.3593], device='cuda:0')
percent tensor([0.9987, 0.0013], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 95 | Batch_idx: 0 |  Loss: (0.2739) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 95 | Batch_idx: 10 |  Loss: (0.2550) |  Loss2: (0.0000) | Acc: (91.00%) (1285/1408)
Epoch: 95 | Batch_idx: 20 |  Loss: (0.2887) |  Loss2: (0.0000) | Acc: (90.00%) (2429/2688)
Epoch: 95 | Batch_idx: 30 |  Loss: (0.2936) |  Loss2: (0.0000) | Acc: (89.00%) (3570/3968)
Epoch: 95 | Batch_idx: 40 |  Loss: (0.2968) |  Loss2: (0.0000) | Acc: (89.00%) (4718/5248)
Epoch: 95 | Batch_idx: 50 |  Loss: (0.3023) |  Loss2: (0.0000) | Acc: (89.00%) (5852/6528)
Epoch: 95 | Batch_idx: 60 |  Loss: (0.3041) |  Loss2: (0.0000) | Acc: (89.00%) (6998/7808)
Epoch: 95 | Batch_idx: 70 |  Loss: (0.3010) |  Loss2: (0.0000) | Acc: (89.00%) (8153/9088)
Epoch: 95 | Batch_idx: 80 |  Loss: (0.3022) |  Loss2: (0.0000) | Acc: (89.00%) (9293/10368)
Epoch: 95 | Batch_idx: 90 |  Loss: (0.3078) |  Loss2: (0.0000) | Acc: (89.00%) (10419/11648)
Epoch: 95 | Batch_idx: 100 |  Loss: (0.3066) |  Loss2: (0.0000) | Acc: (89.00%) (11567/12928)
Epoch: 95 | Batch_idx: 110 |  Loss: (0.3079) |  Loss2: (0.0000) | Acc: (89.00%) (12703/14208)
Epoch: 95 | Batch_idx: 120 |  Loss: (0.3071) |  Loss2: (0.0000) | Acc: (89.00%) (13846/15488)
Epoch: 95 | Batch_idx: 130 |  Loss: (0.3077) |  Loss2: (0.0000) | Acc: (89.00%) (14985/16768)
Epoch: 95 | Batch_idx: 140 |  Loss: (0.3057) |  Loss2: (0.0000) | Acc: (89.00%) (16150/18048)
Epoch: 95 | Batch_idx: 150 |  Loss: (0.3052) |  Loss2: (0.0000) | Acc: (89.00%) (17302/19328)
Epoch: 95 | Batch_idx: 160 |  Loss: (0.3056) |  Loss2: (0.0000) | Acc: (89.00%) (18444/20608)
Epoch: 95 | Batch_idx: 170 |  Loss: (0.3039) |  Loss2: (0.0000) | Acc: (89.00%) (19601/21888)
Epoch: 95 | Batch_idx: 180 |  Loss: (0.3023) |  Loss2: (0.0000) | Acc: (89.00%) (20751/23168)
Epoch: 95 | Batch_idx: 190 |  Loss: (0.3027) |  Loss2: (0.0000) | Acc: (89.00%) (21886/24448)
Epoch: 95 | Batch_idx: 200 |  Loss: (0.3020) |  Loss2: (0.0000) | Acc: (89.00%) (23044/25728)
Epoch: 95 | Batch_idx: 210 |  Loss: (0.3033) |  Loss2: (0.0000) | Acc: (89.00%) (24173/27008)
Epoch: 95 | Batch_idx: 220 |  Loss: (0.3012) |  Loss2: (0.0000) | Acc: (89.00%) (25336/28288)
Epoch: 95 | Batch_idx: 230 |  Loss: (0.2998) |  Loss2: (0.0000) | Acc: (89.00%) (26490/29568)
Epoch: 95 | Batch_idx: 240 |  Loss: (0.2998) |  Loss2: (0.0000) | Acc: (89.00%) (27642/30848)
Epoch: 95 | Batch_idx: 250 |  Loss: (0.2989) |  Loss2: (0.0000) | Acc: (89.00%) (28806/32128)
Epoch: 95 | Batch_idx: 260 |  Loss: (0.2988) |  Loss2: (0.0000) | Acc: (89.00%) (29960/33408)
Epoch: 95 | Batch_idx: 270 |  Loss: (0.2967) |  Loss2: (0.0000) | Acc: (89.00%) (31132/34688)
Epoch: 95 | Batch_idx: 280 |  Loss: (0.2971) |  Loss2: (0.0000) | Acc: (89.00%) (32269/35968)
Epoch: 95 | Batch_idx: 290 |  Loss: (0.2960) |  Loss2: (0.0000) | Acc: (89.00%) (33428/37248)
Epoch: 95 | Batch_idx: 300 |  Loss: (0.2951) |  Loss2: (0.0000) | Acc: (89.00%) (34588/38528)
Epoch: 95 | Batch_idx: 310 |  Loss: (0.2942) |  Loss2: (0.0000) | Acc: (89.00%) (35753/39808)
Epoch: 95 | Batch_idx: 320 |  Loss: (0.2944) |  Loss2: (0.0000) | Acc: (89.00%) (36885/41088)
Epoch: 95 | Batch_idx: 330 |  Loss: (0.2940) |  Loss2: (0.0000) | Acc: (89.00%) (38051/42368)
Epoch: 95 | Batch_idx: 340 |  Loss: (0.2931) |  Loss2: (0.0000) | Acc: (89.00%) (39211/43648)
Epoch: 95 | Batch_idx: 350 |  Loss: (0.2923) |  Loss2: (0.0000) | Acc: (89.00%) (40366/44928)
Epoch: 95 | Batch_idx: 360 |  Loss: (0.2918) |  Loss2: (0.0000) | Acc: (89.00%) (41512/46208)
Epoch: 95 | Batch_idx: 370 |  Loss: (0.2916) |  Loss2: (0.0000) | Acc: (89.00%) (42668/47488)
Epoch: 95 | Batch_idx: 380 |  Loss: (0.2904) |  Loss2: (0.0000) | Acc: (89.00%) (43841/48768)
Epoch: 95 | Batch_idx: 390 |  Loss: (0.2899) |  Loss2: (0.0000) | Acc: (89.00%) (44963/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_095.pth.tar'
# TEST : Loss: (0.4604) | Acc: (85.00%) (8556/10000)
percent tensor([0.5168, 0.4832], device='cuda:0')
percent tensor([0.4650, 0.5350], device='cuda:0')
percent tensor([0.5350, 0.4650], device='cuda:0')
percent tensor([0.5861, 0.4139], device='cuda:0')
percent tensor([0.5587, 0.4413], device='cuda:0')
percent tensor([0.5903, 0.4097], device='cuda:0')
percent tensor([0.6358, 0.3642], device='cuda:0')
percent tensor([0.9988, 0.0012], device='cuda:0')
Epoch: 96 | Batch_idx: 0 |  Loss: (0.2446) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 96 | Batch_idx: 10 |  Loss: (0.2686) |  Loss2: (0.0000) | Acc: (90.00%) (1269/1408)
Epoch: 96 | Batch_idx: 20 |  Loss: (0.2520) |  Loss2: (0.0000) | Acc: (90.00%) (2444/2688)
Epoch: 96 | Batch_idx: 30 |  Loss: (0.2592) |  Loss2: (0.0000) | Acc: (90.00%) (3610/3968)
Epoch: 96 | Batch_idx: 40 |  Loss: (0.2538) |  Loss2: (0.0000) | Acc: (91.00%) (4784/5248)
Epoch: 96 | Batch_idx: 50 |  Loss: (0.2560) |  Loss2: (0.0000) | Acc: (91.00%) (5943/6528)
Epoch: 96 | Batch_idx: 60 |  Loss: (0.2600) |  Loss2: (0.0000) | Acc: (90.00%) (7098/7808)
Epoch: 96 | Batch_idx: 70 |  Loss: (0.2574) |  Loss2: (0.0000) | Acc: (90.00%) (8265/9088)
Epoch: 96 | Batch_idx: 80 |  Loss: (0.2572) |  Loss2: (0.0000) | Acc: (91.00%) (9441/10368)
Epoch: 96 | Batch_idx: 90 |  Loss: (0.2560) |  Loss2: (0.0000) | Acc: (91.00%) (10613/11648)
Epoch: 96 | Batch_idx: 100 |  Loss: (0.2548) |  Loss2: (0.0000) | Acc: (91.00%) (11785/12928)
Epoch: 96 | Batch_idx: 110 |  Loss: (0.2558) |  Loss2: (0.0000) | Acc: (91.00%) (12944/14208)
Epoch: 96 | Batch_idx: 120 |  Loss: (0.2553) |  Loss2: (0.0000) | Acc: (91.00%) (14122/15488)
Epoch: 96 | Batch_idx: 130 |  Loss: (0.2564) |  Loss2: (0.0000) | Acc: (91.00%) (15277/16768)
Epoch: 96 | Batch_idx: 140 |  Loss: (0.2581) |  Loss2: (0.0000) | Acc: (91.00%) (16435/18048)
Epoch: 96 | Batch_idx: 150 |  Loss: (0.2569) |  Loss2: (0.0000) | Acc: (91.00%) (17607/19328)
Epoch: 96 | Batch_idx: 160 |  Loss: (0.2580) |  Loss2: (0.0000) | Acc: (91.00%) (18771/20608)
Epoch: 96 | Batch_idx: 170 |  Loss: (0.2581) |  Loss2: (0.0000) | Acc: (91.00%) (19939/21888)
Epoch: 96 | Batch_idx: 180 |  Loss: (0.2564) |  Loss2: (0.0000) | Acc: (91.00%) (21119/23168)
Epoch: 96 | Batch_idx: 190 |  Loss: (0.2570) |  Loss2: (0.0000) | Acc: (91.00%) (22273/24448)
Epoch: 96 | Batch_idx: 200 |  Loss: (0.2572) |  Loss2: (0.0000) | Acc: (91.00%) (23443/25728)
Epoch: 96 | Batch_idx: 210 |  Loss: (0.2563) |  Loss2: (0.0000) | Acc: (91.00%) (24617/27008)
Epoch: 96 | Batch_idx: 220 |  Loss: (0.2568) |  Loss2: (0.0000) | Acc: (91.00%) (25776/28288)
Epoch: 96 | Batch_idx: 230 |  Loss: (0.2565) |  Loss2: (0.0000) | Acc: (91.00%) (26946/29568)
Epoch: 96 | Batch_idx: 240 |  Loss: (0.2573) |  Loss2: (0.0000) | Acc: (91.00%) (28106/30848)
Epoch: 96 | Batch_idx: 250 |  Loss: (0.2572) |  Loss2: (0.0000) | Acc: (91.00%) (29271/32128)
Epoch: 96 | Batch_idx: 260 |  Loss: (0.2583) |  Loss2: (0.0000) | Acc: (91.00%) (30426/33408)
Epoch: 96 | Batch_idx: 270 |  Loss: (0.2577) |  Loss2: (0.0000) | Acc: (91.00%) (31600/34688)
Epoch: 96 | Batch_idx: 280 |  Loss: (0.2578) |  Loss2: (0.0000) | Acc: (91.00%) (32762/35968)
Epoch: 96 | Batch_idx: 290 |  Loss: (0.2580) |  Loss2: (0.0000) | Acc: (91.00%) (33925/37248)
Epoch: 96 | Batch_idx: 300 |  Loss: (0.2571) |  Loss2: (0.0000) | Acc: (91.00%) (35095/38528)
Epoch: 96 | Batch_idx: 310 |  Loss: (0.2574) |  Loss2: (0.0000) | Acc: (91.00%) (36258/39808)
Epoch: 96 | Batch_idx: 320 |  Loss: (0.2566) |  Loss2: (0.0000) | Acc: (91.00%) (37425/41088)
Epoch: 96 | Batch_idx: 330 |  Loss: (0.2566) |  Loss2: (0.0000) | Acc: (91.00%) (38592/42368)
Epoch: 96 | Batch_idx: 340 |  Loss: (0.2571) |  Loss2: (0.0000) | Acc: (91.00%) (39751/43648)
Epoch: 96 | Batch_idx: 350 |  Loss: (0.2567) |  Loss2: (0.0000) | Acc: (91.00%) (40918/44928)
Epoch: 96 | Batch_idx: 360 |  Loss: (0.2567) |  Loss2: (0.0000) | Acc: (91.00%) (42080/46208)
Epoch: 96 | Batch_idx: 370 |  Loss: (0.2567) |  Loss2: (0.0000) | Acc: (91.00%) (43247/47488)
Epoch: 96 | Batch_idx: 380 |  Loss: (0.2566) |  Loss2: (0.0000) | Acc: (91.00%) (44422/48768)
Epoch: 96 | Batch_idx: 390 |  Loss: (0.2560) |  Loss2: (0.0000) | Acc: (91.00%) (45551/50000)
# TEST : Loss: (0.4395) | Acc: (86.00%) (8618/10000)
percent tensor([0.5182, 0.4818], device='cuda:0')
percent tensor([0.4631, 0.5369], device='cuda:0')
percent tensor([0.5389, 0.4611], device='cuda:0')
percent tensor([0.5866, 0.4134], device='cuda:0')
percent tensor([0.5576, 0.4424], device='cuda:0')
percent tensor([0.5998, 0.4002], device='cuda:0')
percent tensor([0.6549, 0.3451], device='cuda:0')
percent tensor([0.9988, 0.0012], device='cuda:0')
Epoch: 97 | Batch_idx: 0 |  Loss: (0.2220) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 97 | Batch_idx: 10 |  Loss: (0.2575) |  Loss2: (0.0000) | Acc: (90.00%) (1280/1408)
Epoch: 97 | Batch_idx: 20 |  Loss: (0.2515) |  Loss2: (0.0000) | Acc: (91.00%) (2454/2688)
Epoch: 97 | Batch_idx: 30 |  Loss: (0.2487) |  Loss2: (0.0000) | Acc: (91.00%) (3615/3968)
Epoch: 97 | Batch_idx: 40 |  Loss: (0.2494) |  Loss2: (0.0000) | Acc: (91.00%) (4781/5248)
Epoch: 97 | Batch_idx: 50 |  Loss: (0.2483) |  Loss2: (0.0000) | Acc: (91.00%) (5962/6528)
Epoch: 97 | Batch_idx: 60 |  Loss: (0.2505) |  Loss2: (0.0000) | Acc: (91.00%) (7125/7808)
Epoch: 97 | Batch_idx: 70 |  Loss: (0.2515) |  Loss2: (0.0000) | Acc: (91.00%) (8286/9088)
Epoch: 97 | Batch_idx: 80 |  Loss: (0.2538) |  Loss2: (0.0000) | Acc: (91.00%) (9452/10368)
Epoch: 97 | Batch_idx: 90 |  Loss: (0.2530) |  Loss2: (0.0000) | Acc: (91.00%) (10630/11648)
Epoch: 97 | Batch_idx: 100 |  Loss: (0.2521) |  Loss2: (0.0000) | Acc: (91.00%) (11803/12928)
Epoch: 97 | Batch_idx: 110 |  Loss: (0.2519) |  Loss2: (0.0000) | Acc: (91.00%) (12965/14208)
Epoch: 97 | Batch_idx: 120 |  Loss: (0.2521) |  Loss2: (0.0000) | Acc: (91.00%) (14133/15488)
Epoch: 97 | Batch_idx: 130 |  Loss: (0.2502) |  Loss2: (0.0000) | Acc: (91.00%) (15306/16768)
Epoch: 97 | Batch_idx: 140 |  Loss: (0.2500) |  Loss2: (0.0000) | Acc: (91.00%) (16479/18048)
Epoch: 97 | Batch_idx: 150 |  Loss: (0.2494) |  Loss2: (0.0000) | Acc: (91.00%) (17652/19328)
Epoch: 97 | Batch_idx: 160 |  Loss: (0.2487) |  Loss2: (0.0000) | Acc: (91.00%) (18823/20608)
Epoch: 97 | Batch_idx: 170 |  Loss: (0.2488) |  Loss2: (0.0000) | Acc: (91.00%) (20001/21888)
Epoch: 97 | Batch_idx: 180 |  Loss: (0.2476) |  Loss2: (0.0000) | Acc: (91.00%) (21185/23168)
Epoch: 97 | Batch_idx: 190 |  Loss: (0.2466) |  Loss2: (0.0000) | Acc: (91.00%) (22367/24448)
Epoch: 97 | Batch_idx: 200 |  Loss: (0.2461) |  Loss2: (0.0000) | Acc: (91.00%) (23550/25728)
Epoch: 97 | Batch_idx: 210 |  Loss: (0.2463) |  Loss2: (0.0000) | Acc: (91.00%) (24716/27008)
Epoch: 97 | Batch_idx: 220 |  Loss: (0.2467) |  Loss2: (0.0000) | Acc: (91.00%) (25889/28288)
Epoch: 97 | Batch_idx: 230 |  Loss: (0.2460) |  Loss2: (0.0000) | Acc: (91.00%) (27064/29568)
Epoch: 97 | Batch_idx: 240 |  Loss: (0.2467) |  Loss2: (0.0000) | Acc: (91.00%) (28229/30848)
Epoch: 97 | Batch_idx: 250 |  Loss: (0.2465) |  Loss2: (0.0000) | Acc: (91.00%) (29397/32128)
Epoch: 97 | Batch_idx: 260 |  Loss: (0.2459) |  Loss2: (0.0000) | Acc: (91.00%) (30573/33408)
Epoch: 97 | Batch_idx: 270 |  Loss: (0.2466) |  Loss2: (0.0000) | Acc: (91.00%) (31739/34688)
Epoch: 97 | Batch_idx: 280 |  Loss: (0.2454) |  Loss2: (0.0000) | Acc: (91.00%) (32932/35968)
Epoch: 97 | Batch_idx: 290 |  Loss: (0.2455) |  Loss2: (0.0000) | Acc: (91.00%) (34101/37248)
Epoch: 97 | Batch_idx: 300 |  Loss: (0.2451) |  Loss2: (0.0000) | Acc: (91.00%) (35288/38528)
Epoch: 97 | Batch_idx: 310 |  Loss: (0.2452) |  Loss2: (0.0000) | Acc: (91.00%) (36451/39808)
Epoch: 97 | Batch_idx: 320 |  Loss: (0.2450) |  Loss2: (0.0000) | Acc: (91.00%) (37626/41088)
Epoch: 97 | Batch_idx: 330 |  Loss: (0.2445) |  Loss2: (0.0000) | Acc: (91.00%) (38808/42368)
Epoch: 97 | Batch_idx: 340 |  Loss: (0.2447) |  Loss2: (0.0000) | Acc: (91.00%) (39977/43648)
Epoch: 97 | Batch_idx: 350 |  Loss: (0.2447) |  Loss2: (0.0000) | Acc: (91.00%) (41150/44928)
Epoch: 97 | Batch_idx: 360 |  Loss: (0.2444) |  Loss2: (0.0000) | Acc: (91.00%) (42322/46208)
Epoch: 97 | Batch_idx: 370 |  Loss: (0.2442) |  Loss2: (0.0000) | Acc: (91.00%) (43503/47488)
Epoch: 97 | Batch_idx: 380 |  Loss: (0.2435) |  Loss2: (0.0000) | Acc: (91.00%) (44686/48768)
Epoch: 97 | Batch_idx: 390 |  Loss: (0.2427) |  Loss2: (0.0000) | Acc: (91.00%) (45832/50000)
# TEST : Loss: (0.4284) | Acc: (86.00%) (8622/10000)
percent tensor([0.5191, 0.4809], device='cuda:0')
percent tensor([0.4639, 0.5361], device='cuda:0')
percent tensor([0.5439, 0.4561], device='cuda:0')
percent tensor([0.5835, 0.4165], device='cuda:0')
percent tensor([0.5639, 0.4361], device='cuda:0')
percent tensor([0.6095, 0.3905], device='cuda:0')
percent tensor([0.6619, 0.3381], device='cuda:0')
percent tensor([0.9988, 0.0012], device='cuda:0')
Epoch: 98 | Batch_idx: 0 |  Loss: (0.2367) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 98 | Batch_idx: 10 |  Loss: (0.2402) |  Loss2: (0.0000) | Acc: (91.00%) (1291/1408)
Epoch: 98 | Batch_idx: 20 |  Loss: (0.2329) |  Loss2: (0.0000) | Acc: (92.00%) (2475/2688)
Epoch: 98 | Batch_idx: 30 |  Loss: (0.2348) |  Loss2: (0.0000) | Acc: (91.00%) (3648/3968)
Epoch: 98 | Batch_idx: 40 |  Loss: (0.2344) |  Loss2: (0.0000) | Acc: (92.00%) (4832/5248)
Epoch: 98 | Batch_idx: 50 |  Loss: (0.2419) |  Loss2: (0.0000) | Acc: (91.00%) (5995/6528)
Epoch: 98 | Batch_idx: 60 |  Loss: (0.2400) |  Loss2: (0.0000) | Acc: (91.00%) (7174/7808)
Epoch: 98 | Batch_idx: 70 |  Loss: (0.2402) |  Loss2: (0.0000) | Acc: (91.00%) (8346/9088)
Epoch: 98 | Batch_idx: 80 |  Loss: (0.2411) |  Loss2: (0.0000) | Acc: (91.00%) (9516/10368)
Epoch: 98 | Batch_idx: 90 |  Loss: (0.2413) |  Loss2: (0.0000) | Acc: (91.00%) (10703/11648)
Epoch: 98 | Batch_idx: 100 |  Loss: (0.2428) |  Loss2: (0.0000) | Acc: (91.00%) (11872/12928)
Epoch: 98 | Batch_idx: 110 |  Loss: (0.2445) |  Loss2: (0.0000) | Acc: (91.00%) (13039/14208)
Epoch: 98 | Batch_idx: 120 |  Loss: (0.2455) |  Loss2: (0.0000) | Acc: (91.00%) (14210/15488)
Epoch: 98 | Batch_idx: 130 |  Loss: (0.2469) |  Loss2: (0.0000) | Acc: (91.00%) (15371/16768)
Epoch: 98 | Batch_idx: 140 |  Loss: (0.2465) |  Loss2: (0.0000) | Acc: (91.00%) (16553/18048)
Epoch: 98 | Batch_idx: 150 |  Loss: (0.2452) |  Loss2: (0.0000) | Acc: (91.00%) (17732/19328)
Epoch: 98 | Batch_idx: 160 |  Loss: (0.2437) |  Loss2: (0.0000) | Acc: (91.00%) (18908/20608)
Epoch: 98 | Batch_idx: 170 |  Loss: (0.2430) |  Loss2: (0.0000) | Acc: (91.00%) (20083/21888)
Epoch: 98 | Batch_idx: 180 |  Loss: (0.2428) |  Loss2: (0.0000) | Acc: (91.00%) (21252/23168)
Epoch: 98 | Batch_idx: 190 |  Loss: (0.2422) |  Loss2: (0.0000) | Acc: (91.00%) (22432/24448)
Epoch: 98 | Batch_idx: 200 |  Loss: (0.2444) |  Loss2: (0.0000) | Acc: (91.00%) (23598/25728)
Epoch: 98 | Batch_idx: 210 |  Loss: (0.2429) |  Loss2: (0.0000) | Acc: (91.00%) (24786/27008)
Epoch: 98 | Batch_idx: 220 |  Loss: (0.2427) |  Loss2: (0.0000) | Acc: (91.00%) (25965/28288)
Epoch: 98 | Batch_idx: 230 |  Loss: (0.2415) |  Loss2: (0.0000) | Acc: (91.00%) (27159/29568)
Epoch: 98 | Batch_idx: 240 |  Loss: (0.2408) |  Loss2: (0.0000) | Acc: (91.00%) (28337/30848)
Epoch: 98 | Batch_idx: 250 |  Loss: (0.2407) |  Loss2: (0.0000) | Acc: (91.00%) (29505/32128)
Epoch: 98 | Batch_idx: 260 |  Loss: (0.2401) |  Loss2: (0.0000) | Acc: (91.00%) (30693/33408)
Epoch: 98 | Batch_idx: 270 |  Loss: (0.2405) |  Loss2: (0.0000) | Acc: (91.00%) (31857/34688)
Epoch: 98 | Batch_idx: 280 |  Loss: (0.2414) |  Loss2: (0.0000) | Acc: (91.00%) (33020/35968)
Epoch: 98 | Batch_idx: 290 |  Loss: (0.2412) |  Loss2: (0.0000) | Acc: (91.00%) (34191/37248)
Epoch: 98 | Batch_idx: 300 |  Loss: (0.2415) |  Loss2: (0.0000) | Acc: (91.00%) (35362/38528)
Epoch: 98 | Batch_idx: 310 |  Loss: (0.2407) |  Loss2: (0.0000) | Acc: (91.00%) (36549/39808)
Epoch: 98 | Batch_idx: 320 |  Loss: (0.2408) |  Loss2: (0.0000) | Acc: (91.00%) (37712/41088)
Epoch: 98 | Batch_idx: 330 |  Loss: (0.2408) |  Loss2: (0.0000) | Acc: (91.00%) (38880/42368)
Epoch: 98 | Batch_idx: 340 |  Loss: (0.2402) |  Loss2: (0.0000) | Acc: (91.00%) (40066/43648)
Epoch: 98 | Batch_idx: 350 |  Loss: (0.2402) |  Loss2: (0.0000) | Acc: (91.00%) (41242/44928)
Epoch: 98 | Batch_idx: 360 |  Loss: (0.2395) |  Loss2: (0.0000) | Acc: (91.00%) (42430/46208)
Epoch: 98 | Batch_idx: 370 |  Loss: (0.2385) |  Loss2: (0.0000) | Acc: (91.00%) (43621/47488)
Epoch: 98 | Batch_idx: 380 |  Loss: (0.2386) |  Loss2: (0.0000) | Acc: (91.00%) (44790/48768)
Epoch: 98 | Batch_idx: 390 |  Loss: (0.2384) |  Loss2: (0.0000) | Acc: (91.00%) (45932/50000)
# TEST : Loss: (0.4151) | Acc: (86.00%) (8648/10000)
percent tensor([0.5200, 0.4800], device='cuda:0')
percent tensor([0.4625, 0.5375], device='cuda:0')
percent tensor([0.5470, 0.4530], device='cuda:0')
percent tensor([0.5847, 0.4153], device='cuda:0')
percent tensor([0.5650, 0.4350], device='cuda:0')
percent tensor([0.6227, 0.3773], device='cuda:0')
percent tensor([0.6620, 0.3380], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
Epoch: 99 | Batch_idx: 0 |  Loss: (0.3162) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 99 | Batch_idx: 10 |  Loss: (0.2245) |  Loss2: (0.0000) | Acc: (92.00%) (1308/1408)
Epoch: 99 | Batch_idx: 20 |  Loss: (0.2166) |  Loss2: (0.0000) | Acc: (93.00%) (2504/2688)
Epoch: 99 | Batch_idx: 30 |  Loss: (0.2305) |  Loss2: (0.0000) | Acc: (92.00%) (3680/3968)
Epoch: 99 | Batch_idx: 40 |  Loss: (0.2261) |  Loss2: (0.0000) | Acc: (92.00%) (4874/5248)
Epoch: 99 | Batch_idx: 50 |  Loss: (0.2298) |  Loss2: (0.0000) | Acc: (92.00%) (6045/6528)
Epoch: 99 | Batch_idx: 60 |  Loss: (0.2301) |  Loss2: (0.0000) | Acc: (92.00%) (7222/7808)
Epoch: 99 | Batch_idx: 70 |  Loss: (0.2279) |  Loss2: (0.0000) | Acc: (92.00%) (8408/9088)
Epoch: 99 | Batch_idx: 80 |  Loss: (0.2318) |  Loss2: (0.0000) | Acc: (92.00%) (9574/10368)
Epoch: 99 | Batch_idx: 90 |  Loss: (0.2319) |  Loss2: (0.0000) | Acc: (92.00%) (10753/11648)
Epoch: 99 | Batch_idx: 100 |  Loss: (0.2302) |  Loss2: (0.0000) | Acc: (92.00%) (11948/12928)
Epoch: 99 | Batch_idx: 110 |  Loss: (0.2328) |  Loss2: (0.0000) | Acc: (92.00%) (13120/14208)
Epoch: 99 | Batch_idx: 120 |  Loss: (0.2348) |  Loss2: (0.0000) | Acc: (92.00%) (14291/15488)
Epoch: 99 | Batch_idx: 130 |  Loss: (0.2338) |  Loss2: (0.0000) | Acc: (92.00%) (15477/16768)
Epoch: 99 | Batch_idx: 140 |  Loss: (0.2352) |  Loss2: (0.0000) | Acc: (92.00%) (16649/18048)
Epoch: 99 | Batch_idx: 150 |  Loss: (0.2334) |  Loss2: (0.0000) | Acc: (92.00%) (17835/19328)
Epoch: 99 | Batch_idx: 160 |  Loss: (0.2340) |  Loss2: (0.0000) | Acc: (92.00%) (19013/20608)
Epoch: 99 | Batch_idx: 170 |  Loss: (0.2340) |  Loss2: (0.0000) | Acc: (92.00%) (20193/21888)
Epoch: 99 | Batch_idx: 180 |  Loss: (0.2329) |  Loss2: (0.0000) | Acc: (92.00%) (21384/23168)
Epoch: 99 | Batch_idx: 190 |  Loss: (0.2337) |  Loss2: (0.0000) | Acc: (92.00%) (22559/24448)
Epoch: 99 | Batch_idx: 200 |  Loss: (0.2341) |  Loss2: (0.0000) | Acc: (92.00%) (23731/25728)
Epoch: 99 | Batch_idx: 210 |  Loss: (0.2331) |  Loss2: (0.0000) | Acc: (92.00%) (24922/27008)
Epoch: 99 | Batch_idx: 220 |  Loss: (0.2321) |  Loss2: (0.0000) | Acc: (92.00%) (26111/28288)
Epoch: 99 | Batch_idx: 230 |  Loss: (0.2319) |  Loss2: (0.0000) | Acc: (92.00%) (27295/29568)
Epoch: 99 | Batch_idx: 240 |  Loss: (0.2311) |  Loss2: (0.0000) | Acc: (92.00%) (28478/30848)
Epoch: 99 | Batch_idx: 250 |  Loss: (0.2323) |  Loss2: (0.0000) | Acc: (92.00%) (29648/32128)
Epoch: 99 | Batch_idx: 260 |  Loss: (0.2331) |  Loss2: (0.0000) | Acc: (92.00%) (30811/33408)
Epoch: 99 | Batch_idx: 270 |  Loss: (0.2323) |  Loss2: (0.0000) | Acc: (92.00%) (32001/34688)
Epoch: 99 | Batch_idx: 280 |  Loss: (0.2319) |  Loss2: (0.0000) | Acc: (92.00%) (33179/35968)
Epoch: 99 | Batch_idx: 290 |  Loss: (0.2316) |  Loss2: (0.0000) | Acc: (92.00%) (34363/37248)
Epoch: 99 | Batch_idx: 300 |  Loss: (0.2317) |  Loss2: (0.0000) | Acc: (92.00%) (35538/38528)
Epoch: 99 | Batch_idx: 310 |  Loss: (0.2323) |  Loss2: (0.0000) | Acc: (92.00%) (36709/39808)
Epoch: 99 | Batch_idx: 320 |  Loss: (0.2316) |  Loss2: (0.0000) | Acc: (92.00%) (37899/41088)
Epoch: 99 | Batch_idx: 330 |  Loss: (0.2311) |  Loss2: (0.0000) | Acc: (92.00%) (39089/42368)
Epoch: 99 | Batch_idx: 340 |  Loss: (0.2309) |  Loss2: (0.0000) | Acc: (92.00%) (40281/43648)
Epoch: 99 | Batch_idx: 350 |  Loss: (0.2309) |  Loss2: (0.0000) | Acc: (92.00%) (41460/44928)
Epoch: 99 | Batch_idx: 360 |  Loss: (0.2305) |  Loss2: (0.0000) | Acc: (92.00%) (42640/46208)
Epoch: 99 | Batch_idx: 370 |  Loss: (0.2313) |  Loss2: (0.0000) | Acc: (92.00%) (43809/47488)
Epoch: 99 | Batch_idx: 380 |  Loss: (0.2315) |  Loss2: (0.0000) | Acc: (92.00%) (44975/48768)
Epoch: 99 | Batch_idx: 390 |  Loss: (0.2314) |  Loss2: (0.0000) | Acc: (92.00%) (46112/50000)
# TEST : Loss: (0.4127) | Acc: (86.00%) (8669/10000)
percent tensor([0.5204, 0.4796], device='cuda:0')
percent tensor([0.4626, 0.5374], device='cuda:0')
percent tensor([0.5474, 0.4526], device='cuda:0')
percent tensor([0.5865, 0.4135], device='cuda:0')
percent tensor([0.5617, 0.4383], device='cuda:0')
percent tensor([0.6296, 0.3704], device='cuda:0')
percent tensor([0.6675, 0.3325], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 100 | Batch_idx: 0 |  Loss: (0.1862) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 100 | Batch_idx: 10 |  Loss: (0.2418) |  Loss2: (0.0000) | Acc: (91.00%) (1294/1408)
Epoch: 100 | Batch_idx: 20 |  Loss: (0.2494) |  Loss2: (0.0000) | Acc: (92.00%) (2473/2688)
Epoch: 100 | Batch_idx: 30 |  Loss: (0.2520) |  Loss2: (0.0000) | Acc: (91.00%) (3643/3968)
Epoch: 100 | Batch_idx: 40 |  Loss: (0.2493) |  Loss2: (0.0000) | Acc: (91.00%) (4816/5248)
Epoch: 100 | Batch_idx: 50 |  Loss: (0.2454) |  Loss2: (0.0000) | Acc: (91.00%) (6004/6528)
Epoch: 100 | Batch_idx: 60 |  Loss: (0.2429) |  Loss2: (0.0000) | Acc: (92.00%) (7185/7808)
Epoch: 100 | Batch_idx: 70 |  Loss: (0.2440) |  Loss2: (0.0000) | Acc: (91.00%) (8349/9088)
Epoch: 100 | Batch_idx: 80 |  Loss: (0.2440) |  Loss2: (0.0000) | Acc: (91.00%) (9526/10368)
Epoch: 100 | Batch_idx: 90 |  Loss: (0.2446) |  Loss2: (0.0000) | Acc: (91.00%) (10690/11648)
Epoch: 100 | Batch_idx: 100 |  Loss: (0.2441) |  Loss2: (0.0000) | Acc: (91.00%) (11860/12928)
Epoch: 100 | Batch_idx: 110 |  Loss: (0.2421) |  Loss2: (0.0000) | Acc: (91.00%) (13051/14208)
Epoch: 100 | Batch_idx: 120 |  Loss: (0.2416) |  Loss2: (0.0000) | Acc: (91.00%) (14234/15488)
Epoch: 100 | Batch_idx: 130 |  Loss: (0.2401) |  Loss2: (0.0000) | Acc: (91.00%) (15400/16768)
Epoch: 100 | Batch_idx: 140 |  Loss: (0.2381) |  Loss2: (0.0000) | Acc: (91.00%) (16583/18048)
Epoch: 100 | Batch_idx: 150 |  Loss: (0.2392) |  Loss2: (0.0000) | Acc: (91.00%) (17753/19328)
Epoch: 100 | Batch_idx: 160 |  Loss: (0.2374) |  Loss2: (0.0000) | Acc: (91.00%) (18933/20608)
Epoch: 100 | Batch_idx: 170 |  Loss: (0.2389) |  Loss2: (0.0000) | Acc: (91.00%) (20098/21888)
Epoch: 100 | Batch_idx: 180 |  Loss: (0.2383) |  Loss2: (0.0000) | Acc: (91.00%) (21278/23168)
Epoch: 100 | Batch_idx: 190 |  Loss: (0.2383) |  Loss2: (0.0000) | Acc: (91.00%) (22454/24448)
Epoch: 100 | Batch_idx: 200 |  Loss: (0.2387) |  Loss2: (0.0000) | Acc: (91.00%) (23633/25728)
Epoch: 100 | Batch_idx: 210 |  Loss: (0.2395) |  Loss2: (0.0000) | Acc: (91.00%) (24796/27008)
Epoch: 100 | Batch_idx: 220 |  Loss: (0.2392) |  Loss2: (0.0000) | Acc: (91.00%) (25980/28288)
Epoch: 100 | Batch_idx: 230 |  Loss: (0.2387) |  Loss2: (0.0000) | Acc: (91.00%) (27159/29568)
Epoch: 100 | Batch_idx: 240 |  Loss: (0.2389) |  Loss2: (0.0000) | Acc: (91.00%) (28331/30848)
Epoch: 100 | Batch_idx: 250 |  Loss: (0.2389) |  Loss2: (0.0000) | Acc: (91.00%) (29506/32128)
Epoch: 100 | Batch_idx: 260 |  Loss: (0.2388) |  Loss2: (0.0000) | Acc: (91.00%) (30669/33408)
Epoch: 100 | Batch_idx: 270 |  Loss: (0.2402) |  Loss2: (0.0000) | Acc: (91.00%) (31816/34688)
Epoch: 100 | Batch_idx: 280 |  Loss: (0.2393) |  Loss2: (0.0000) | Acc: (91.00%) (33002/35968)
Epoch: 100 | Batch_idx: 290 |  Loss: (0.2396) |  Loss2: (0.0000) | Acc: (91.00%) (34169/37248)
Epoch: 100 | Batch_idx: 300 |  Loss: (0.2400) |  Loss2: (0.0000) | Acc: (91.00%) (35336/38528)
Epoch: 100 | Batch_idx: 310 |  Loss: (0.2404) |  Loss2: (0.0000) | Acc: (91.00%) (36498/39808)
Epoch: 100 | Batch_idx: 320 |  Loss: (0.2410) |  Loss2: (0.0000) | Acc: (91.00%) (37664/41088)
Epoch: 100 | Batch_idx: 330 |  Loss: (0.2408) |  Loss2: (0.0000) | Acc: (91.00%) (38837/42368)
Epoch: 100 | Batch_idx: 340 |  Loss: (0.2415) |  Loss2: (0.0000) | Acc: (91.00%) (39994/43648)
Epoch: 100 | Batch_idx: 350 |  Loss: (0.2420) |  Loss2: (0.0000) | Acc: (91.00%) (41163/44928)
Epoch: 100 | Batch_idx: 360 |  Loss: (0.2415) |  Loss2: (0.0000) | Acc: (91.00%) (42346/46208)
Epoch: 100 | Batch_idx: 370 |  Loss: (0.2414) |  Loss2: (0.0000) | Acc: (91.00%) (43525/47488)
Epoch: 100 | Batch_idx: 380 |  Loss: (0.2411) |  Loss2: (0.0000) | Acc: (91.00%) (44708/48768)
Epoch: 100 | Batch_idx: 390 |  Loss: (0.2407) |  Loss2: (0.0000) | Acc: (91.00%) (45844/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_100.pth.tar'
# TEST : Loss: (0.4712) | Acc: (84.00%) (8477/10000)
percent tensor([0.5204, 0.4796], device='cuda:0')
percent tensor([0.4633, 0.5367], device='cuda:0')
percent tensor([0.5470, 0.4530], device='cuda:0')
percent tensor([0.5856, 0.4144], device='cuda:0')
percent tensor([0.5616, 0.4384], device='cuda:0')
percent tensor([0.6281, 0.3719], device='cuda:0')
percent tensor([0.6692, 0.3308], device='cuda:0')
percent tensor([0.9990, 0.0010], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(180.1171, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(811.9636, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(803.6728, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1515.3483, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(502.7923, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2230.9001, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4274.3154, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1394.2455, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6169.0474, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11874.9443, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3929.8618, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16643.3652, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 101 | Batch_idx: 0 |  Loss: (0.3131) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 101 | Batch_idx: 10 |  Loss: (0.2254) |  Loss2: (0.0000) | Acc: (92.00%) (1306/1408)
Epoch: 101 | Batch_idx: 20 |  Loss: (0.2137) |  Loss2: (0.0000) | Acc: (93.00%) (2501/2688)
Epoch: 101 | Batch_idx: 30 |  Loss: (0.2234) |  Loss2: (0.0000) | Acc: (92.00%) (3666/3968)
Epoch: 101 | Batch_idx: 40 |  Loss: (0.2212) |  Loss2: (0.0000) | Acc: (92.00%) (4859/5248)
Epoch: 101 | Batch_idx: 50 |  Loss: (0.2201) |  Loss2: (0.0000) | Acc: (92.00%) (6055/6528)
Epoch: 101 | Batch_idx: 60 |  Loss: (0.2172) |  Loss2: (0.0000) | Acc: (92.00%) (7242/7808)
Epoch: 101 | Batch_idx: 70 |  Loss: (0.2173) |  Loss2: (0.0000) | Acc: (92.00%) (8421/9088)
Epoch: 101 | Batch_idx: 80 |  Loss: (0.2172) |  Loss2: (0.0000) | Acc: (92.00%) (9607/10368)
Epoch: 101 | Batch_idx: 90 |  Loss: (0.2177) |  Loss2: (0.0000) | Acc: (92.00%) (10787/11648)
Epoch: 101 | Batch_idx: 100 |  Loss: (0.2198) |  Loss2: (0.0000) | Acc: (92.00%) (11958/12928)
Epoch: 101 | Batch_idx: 110 |  Loss: (0.2200) |  Loss2: (0.0000) | Acc: (92.00%) (13150/14208)
Epoch: 101 | Batch_idx: 120 |  Loss: (0.2211) |  Loss2: (0.0000) | Acc: (92.00%) (14327/15488)
Epoch: 101 | Batch_idx: 130 |  Loss: (0.2207) |  Loss2: (0.0000) | Acc: (92.00%) (15519/16768)
Epoch: 101 | Batch_idx: 140 |  Loss: (0.2212) |  Loss2: (0.0000) | Acc: (92.00%) (16681/18048)
Epoch: 101 | Batch_idx: 150 |  Loss: (0.2235) |  Loss2: (0.0000) | Acc: (92.00%) (17844/19328)
Epoch: 101 | Batch_idx: 160 |  Loss: (0.2230) |  Loss2: (0.0000) | Acc: (92.00%) (19024/20608)
Epoch: 101 | Batch_idx: 170 |  Loss: (0.2226) |  Loss2: (0.0000) | Acc: (92.00%) (20210/21888)
Epoch: 101 | Batch_idx: 180 |  Loss: (0.2228) |  Loss2: (0.0000) | Acc: (92.00%) (21394/23168)
Epoch: 101 | Batch_idx: 190 |  Loss: (0.2232) |  Loss2: (0.0000) | Acc: (92.00%) (22573/24448)
Epoch: 101 | Batch_idx: 200 |  Loss: (0.2237) |  Loss2: (0.0000) | Acc: (92.00%) (23755/25728)
Epoch: 101 | Batch_idx: 210 |  Loss: (0.2256) |  Loss2: (0.0000) | Acc: (92.00%) (24916/27008)
Epoch: 101 | Batch_idx: 220 |  Loss: (0.2249) |  Loss2: (0.0000) | Acc: (92.00%) (26100/28288)
Epoch: 101 | Batch_idx: 230 |  Loss: (0.2240) |  Loss2: (0.0000) | Acc: (92.00%) (27289/29568)
Epoch: 101 | Batch_idx: 240 |  Loss: (0.2250) |  Loss2: (0.0000) | Acc: (92.00%) (28471/30848)
Epoch: 101 | Batch_idx: 250 |  Loss: (0.2247) |  Loss2: (0.0000) | Acc: (92.00%) (29661/32128)
Epoch: 101 | Batch_idx: 260 |  Loss: (0.2249) |  Loss2: (0.0000) | Acc: (92.00%) (30839/33408)
Epoch: 101 | Batch_idx: 270 |  Loss: (0.2254) |  Loss2: (0.0000) | Acc: (92.00%) (32019/34688)
Epoch: 101 | Batch_idx: 280 |  Loss: (0.2260) |  Loss2: (0.0000) | Acc: (92.00%) (33193/35968)
Epoch: 101 | Batch_idx: 290 |  Loss: (0.2260) |  Loss2: (0.0000) | Acc: (92.00%) (34385/37248)
Epoch: 101 | Batch_idx: 300 |  Loss: (0.2269) |  Loss2: (0.0000) | Acc: (92.00%) (35554/38528)
Epoch: 101 | Batch_idx: 310 |  Loss: (0.2278) |  Loss2: (0.0000) | Acc: (92.00%) (36712/39808)
Epoch: 101 | Batch_idx: 320 |  Loss: (0.2292) |  Loss2: (0.0000) | Acc: (92.00%) (37867/41088)
Epoch: 101 | Batch_idx: 330 |  Loss: (0.2294) |  Loss2: (0.0000) | Acc: (92.00%) (39033/42368)
Epoch: 101 | Batch_idx: 340 |  Loss: (0.2295) |  Loss2: (0.0000) | Acc: (92.00%) (40220/43648)
Epoch: 101 | Batch_idx: 350 |  Loss: (0.2297) |  Loss2: (0.0000) | Acc: (92.00%) (41394/44928)
Epoch: 101 | Batch_idx: 360 |  Loss: (0.2299) |  Loss2: (0.0000) | Acc: (92.00%) (42576/46208)
Epoch: 101 | Batch_idx: 370 |  Loss: (0.2307) |  Loss2: (0.0000) | Acc: (92.00%) (43745/47488)
Epoch: 101 | Batch_idx: 380 |  Loss: (0.2315) |  Loss2: (0.0000) | Acc: (92.00%) (44911/48768)
Epoch: 101 | Batch_idx: 390 |  Loss: (0.2319) |  Loss2: (0.0000) | Acc: (92.00%) (46033/50000)
# TEST : Loss: (0.4380) | Acc: (86.00%) (8602/10000)
percent tensor([0.5202, 0.4798], device='cuda:0')
percent tensor([0.4629, 0.5371], device='cuda:0')
percent tensor([0.5478, 0.4522], device='cuda:0')
percent tensor([0.5868, 0.4132], device='cuda:0')
percent tensor([0.5613, 0.4387], device='cuda:0')
percent tensor([0.6281, 0.3719], device='cuda:0')
percent tensor([0.6601, 0.3399], device='cuda:0')
percent tensor([0.9988, 0.0012], device='cuda:0')
Epoch: 102 | Batch_idx: 0 |  Loss: (0.1552) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 102 | Batch_idx: 10 |  Loss: (0.2240) |  Loss2: (0.0000) | Acc: (92.00%) (1298/1408)
Epoch: 102 | Batch_idx: 20 |  Loss: (0.2146) |  Loss2: (0.0000) | Acc: (92.00%) (2485/2688)
Epoch: 102 | Batch_idx: 30 |  Loss: (0.2105) |  Loss2: (0.0000) | Acc: (92.00%) (3681/3968)
Epoch: 102 | Batch_idx: 40 |  Loss: (0.2062) |  Loss2: (0.0000) | Acc: (92.00%) (4870/5248)
Epoch: 102 | Batch_idx: 50 |  Loss: (0.2068) |  Loss2: (0.0000) | Acc: (92.00%) (6065/6528)
Epoch: 102 | Batch_idx: 60 |  Loss: (0.2082) |  Loss2: (0.0000) | Acc: (92.00%) (7259/7808)
Epoch: 102 | Batch_idx: 70 |  Loss: (0.2082) |  Loss2: (0.0000) | Acc: (92.00%) (8442/9088)
Epoch: 102 | Batch_idx: 80 |  Loss: (0.2103) |  Loss2: (0.0000) | Acc: (92.00%) (9616/10368)
Epoch: 102 | Batch_idx: 90 |  Loss: (0.2154) |  Loss2: (0.0000) | Acc: (92.00%) (10772/11648)
Epoch: 102 | Batch_idx: 100 |  Loss: (0.2154) |  Loss2: (0.0000) | Acc: (92.00%) (11961/12928)
Epoch: 102 | Batch_idx: 110 |  Loss: (0.2165) |  Loss2: (0.0000) | Acc: (92.00%) (13147/14208)
Epoch: 102 | Batch_idx: 120 |  Loss: (0.2179) |  Loss2: (0.0000) | Acc: (92.00%) (14323/15488)
Epoch: 102 | Batch_idx: 130 |  Loss: (0.2201) |  Loss2: (0.0000) | Acc: (92.00%) (15495/16768)
Epoch: 102 | Batch_idx: 140 |  Loss: (0.2215) |  Loss2: (0.0000) | Acc: (92.00%) (16674/18048)
Epoch: 102 | Batch_idx: 150 |  Loss: (0.2243) |  Loss2: (0.0000) | Acc: (92.00%) (17839/19328)
Epoch: 102 | Batch_idx: 160 |  Loss: (0.2241) |  Loss2: (0.0000) | Acc: (92.00%) (19023/20608)
Epoch: 102 | Batch_idx: 170 |  Loss: (0.2240) |  Loss2: (0.0000) | Acc: (92.00%) (20204/21888)
Epoch: 102 | Batch_idx: 180 |  Loss: (0.2264) |  Loss2: (0.0000) | Acc: (92.00%) (21362/23168)
Epoch: 102 | Batch_idx: 190 |  Loss: (0.2278) |  Loss2: (0.0000) | Acc: (92.00%) (22538/24448)
Epoch: 102 | Batch_idx: 200 |  Loss: (0.2269) |  Loss2: (0.0000) | Acc: (92.00%) (23727/25728)
Epoch: 102 | Batch_idx: 210 |  Loss: (0.2273) |  Loss2: (0.0000) | Acc: (92.00%) (24906/27008)
Epoch: 102 | Batch_idx: 220 |  Loss: (0.2282) |  Loss2: (0.0000) | Acc: (92.00%) (26084/28288)
Epoch: 102 | Batch_idx: 230 |  Loss: (0.2272) |  Loss2: (0.0000) | Acc: (92.00%) (27272/29568)
Epoch: 102 | Batch_idx: 240 |  Loss: (0.2281) |  Loss2: (0.0000) | Acc: (92.00%) (28447/30848)
Epoch: 102 | Batch_idx: 250 |  Loss: (0.2285) |  Loss2: (0.0000) | Acc: (92.00%) (29628/32128)
Epoch: 102 | Batch_idx: 260 |  Loss: (0.2289) |  Loss2: (0.0000) | Acc: (92.00%) (30807/33408)
Epoch: 102 | Batch_idx: 270 |  Loss: (0.2300) |  Loss2: (0.0000) | Acc: (92.00%) (31970/34688)
Epoch: 102 | Batch_idx: 280 |  Loss: (0.2300) |  Loss2: (0.0000) | Acc: (92.00%) (33148/35968)
Epoch: 102 | Batch_idx: 290 |  Loss: (0.2303) |  Loss2: (0.0000) | Acc: (92.00%) (34320/37248)
Epoch: 102 | Batch_idx: 300 |  Loss: (0.2305) |  Loss2: (0.0000) | Acc: (92.00%) (35503/38528)
Epoch: 102 | Batch_idx: 310 |  Loss: (0.2304) |  Loss2: (0.0000) | Acc: (92.00%) (36675/39808)
Epoch: 102 | Batch_idx: 320 |  Loss: (0.2304) |  Loss2: (0.0000) | Acc: (92.00%) (37851/41088)
Epoch: 102 | Batch_idx: 330 |  Loss: (0.2298) |  Loss2: (0.0000) | Acc: (92.00%) (39035/42368)
Epoch: 102 | Batch_idx: 340 |  Loss: (0.2295) |  Loss2: (0.0000) | Acc: (92.00%) (40218/43648)
Epoch: 102 | Batch_idx: 350 |  Loss: (0.2287) |  Loss2: (0.0000) | Acc: (92.00%) (41409/44928)
Epoch: 102 | Batch_idx: 360 |  Loss: (0.2290) |  Loss2: (0.0000) | Acc: (92.00%) (42584/46208)
Epoch: 102 | Batch_idx: 370 |  Loss: (0.2295) |  Loss2: (0.0000) | Acc: (92.00%) (43759/47488)
Epoch: 102 | Batch_idx: 380 |  Loss: (0.2288) |  Loss2: (0.0000) | Acc: (92.00%) (44950/48768)
Epoch: 102 | Batch_idx: 390 |  Loss: (0.2288) |  Loss2: (0.0000) | Acc: (92.00%) (46079/50000)
# TEST : Loss: (0.4122) | Acc: (87.00%) (8705/10000)
percent tensor([0.5205, 0.4795], device='cuda:0')
percent tensor([0.4629, 0.5371], device='cuda:0')
percent tensor([0.5468, 0.4532], device='cuda:0')
percent tensor([0.5876, 0.4124], device='cuda:0')
percent tensor([0.5612, 0.4388], device='cuda:0')
percent tensor([0.6303, 0.3697], device='cuda:0')
percent tensor([0.6677, 0.3323], device='cuda:0')
percent tensor([0.9990, 0.0010], device='cuda:0')
Epoch: 103 | Batch_idx: 0 |  Loss: (0.1451) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 103 | Batch_idx: 10 |  Loss: (0.1883) |  Loss2: (0.0000) | Acc: (93.00%) (1322/1408)
Epoch: 103 | Batch_idx: 20 |  Loss: (0.1919) |  Loss2: (0.0000) | Acc: (93.00%) (2511/2688)
Epoch: 103 | Batch_idx: 30 |  Loss: (0.1833) |  Loss2: (0.0000) | Acc: (93.00%) (3724/3968)
Epoch: 103 | Batch_idx: 40 |  Loss: (0.1917) |  Loss2: (0.0000) | Acc: (93.00%) (4912/5248)
Epoch: 103 | Batch_idx: 50 |  Loss: (0.1986) |  Loss2: (0.0000) | Acc: (93.00%) (6088/6528)
Epoch: 103 | Batch_idx: 60 |  Loss: (0.2006) |  Loss2: (0.0000) | Acc: (93.00%) (7280/7808)
Epoch: 103 | Batch_idx: 70 |  Loss: (0.2033) |  Loss2: (0.0000) | Acc: (93.00%) (8462/9088)
Epoch: 103 | Batch_idx: 80 |  Loss: (0.2080) |  Loss2: (0.0000) | Acc: (93.00%) (9643/10368)
Epoch: 103 | Batch_idx: 90 |  Loss: (0.2100) |  Loss2: (0.0000) | Acc: (92.00%) (10826/11648)
Epoch: 103 | Batch_idx: 100 |  Loss: (0.2113) |  Loss2: (0.0000) | Acc: (92.00%) (12003/12928)
Epoch: 103 | Batch_idx: 110 |  Loss: (0.2130) |  Loss2: (0.0000) | Acc: (92.00%) (13178/14208)
Epoch: 103 | Batch_idx: 120 |  Loss: (0.2136) |  Loss2: (0.0000) | Acc: (92.00%) (14360/15488)
Epoch: 103 | Batch_idx: 130 |  Loss: (0.2145) |  Loss2: (0.0000) | Acc: (92.00%) (15538/16768)
Epoch: 103 | Batch_idx: 140 |  Loss: (0.2149) |  Loss2: (0.0000) | Acc: (92.00%) (16722/18048)
Epoch: 103 | Batch_idx: 150 |  Loss: (0.2160) |  Loss2: (0.0000) | Acc: (92.00%) (17891/19328)
Epoch: 103 | Batch_idx: 160 |  Loss: (0.2163) |  Loss2: (0.0000) | Acc: (92.00%) (19077/20608)
Epoch: 103 | Batch_idx: 170 |  Loss: (0.2163) |  Loss2: (0.0000) | Acc: (92.00%) (20259/21888)
Epoch: 103 | Batch_idx: 180 |  Loss: (0.2167) |  Loss2: (0.0000) | Acc: (92.00%) (21447/23168)
Epoch: 103 | Batch_idx: 190 |  Loss: (0.2156) |  Loss2: (0.0000) | Acc: (92.00%) (22652/24448)
Epoch: 103 | Batch_idx: 200 |  Loss: (0.2152) |  Loss2: (0.0000) | Acc: (92.00%) (23838/25728)
Epoch: 103 | Batch_idx: 210 |  Loss: (0.2164) |  Loss2: (0.0000) | Acc: (92.00%) (25016/27008)
Epoch: 103 | Batch_idx: 220 |  Loss: (0.2177) |  Loss2: (0.0000) | Acc: (92.00%) (26193/28288)
Epoch: 103 | Batch_idx: 230 |  Loss: (0.2180) |  Loss2: (0.0000) | Acc: (92.00%) (27362/29568)
Epoch: 103 | Batch_idx: 240 |  Loss: (0.2182) |  Loss2: (0.0000) | Acc: (92.00%) (28548/30848)
Epoch: 103 | Batch_idx: 250 |  Loss: (0.2182) |  Loss2: (0.0000) | Acc: (92.00%) (29726/32128)
Epoch: 103 | Batch_idx: 260 |  Loss: (0.2183) |  Loss2: (0.0000) | Acc: (92.00%) (30913/33408)
Epoch: 103 | Batch_idx: 270 |  Loss: (0.2179) |  Loss2: (0.0000) | Acc: (92.00%) (32099/34688)
Epoch: 103 | Batch_idx: 280 |  Loss: (0.2179) |  Loss2: (0.0000) | Acc: (92.00%) (33287/35968)
Epoch: 103 | Batch_idx: 290 |  Loss: (0.2180) |  Loss2: (0.0000) | Acc: (92.00%) (34464/37248)
Epoch: 103 | Batch_idx: 300 |  Loss: (0.2186) |  Loss2: (0.0000) | Acc: (92.00%) (35634/38528)
Epoch: 103 | Batch_idx: 310 |  Loss: (0.2175) |  Loss2: (0.0000) | Acc: (92.00%) (36840/39808)
Epoch: 103 | Batch_idx: 320 |  Loss: (0.2176) |  Loss2: (0.0000) | Acc: (92.00%) (38022/41088)
Epoch: 103 | Batch_idx: 330 |  Loss: (0.2181) |  Loss2: (0.0000) | Acc: (92.00%) (39204/42368)
Epoch: 103 | Batch_idx: 340 |  Loss: (0.2182) |  Loss2: (0.0000) | Acc: (92.00%) (40382/43648)
Epoch: 103 | Batch_idx: 350 |  Loss: (0.2189) |  Loss2: (0.0000) | Acc: (92.00%) (41565/44928)
Epoch: 103 | Batch_idx: 360 |  Loss: (0.2189) |  Loss2: (0.0000) | Acc: (92.00%) (42752/46208)
Epoch: 103 | Batch_idx: 370 |  Loss: (0.2196) |  Loss2: (0.0000) | Acc: (92.00%) (43928/47488)
Epoch: 103 | Batch_idx: 380 |  Loss: (0.2202) |  Loss2: (0.0000) | Acc: (92.00%) (45106/48768)
Epoch: 103 | Batch_idx: 390 |  Loss: (0.2200) |  Loss2: (0.0000) | Acc: (92.00%) (46247/50000)
# TEST : Loss: (0.4849) | Acc: (84.00%) (8474/10000)
percent tensor([0.5202, 0.4798], device='cuda:0')
percent tensor([0.4625, 0.5375], device='cuda:0')
percent tensor([0.5477, 0.4523], device='cuda:0')
percent tensor([0.5868, 0.4132], device='cuda:0')
percent tensor([0.5616, 0.4384], device='cuda:0')
percent tensor([0.6281, 0.3719], device='cuda:0')
percent tensor([0.6640, 0.3360], device='cuda:0')
percent tensor([0.9990, 0.0010], device='cuda:0')
Epoch: 104 | Batch_idx: 0 |  Loss: (0.1656) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 104 | Batch_idx: 10 |  Loss: (0.2096) |  Loss2: (0.0000) | Acc: (92.00%) (1308/1408)
Epoch: 104 | Batch_idx: 20 |  Loss: (0.2151) |  Loss2: (0.0000) | Acc: (92.00%) (2488/2688)
Epoch: 104 | Batch_idx: 30 |  Loss: (0.2145) |  Loss2: (0.0000) | Acc: (92.00%) (3664/3968)
Epoch: 104 | Batch_idx: 40 |  Loss: (0.2138) |  Loss2: (0.0000) | Acc: (92.00%) (4847/5248)
Epoch: 104 | Batch_idx: 50 |  Loss: (0.2141) |  Loss2: (0.0000) | Acc: (92.00%) (6032/6528)
Epoch: 104 | Batch_idx: 60 |  Loss: (0.2166) |  Loss2: (0.0000) | Acc: (92.00%) (7212/7808)
Epoch: 104 | Batch_idx: 70 |  Loss: (0.2135) |  Loss2: (0.0000) | Acc: (92.00%) (8408/9088)
Epoch: 104 | Batch_idx: 80 |  Loss: (0.2134) |  Loss2: (0.0000) | Acc: (92.00%) (9597/10368)
Epoch: 104 | Batch_idx: 90 |  Loss: (0.2120) |  Loss2: (0.0000) | Acc: (92.00%) (10792/11648)
Epoch: 104 | Batch_idx: 100 |  Loss: (0.2128) |  Loss2: (0.0000) | Acc: (92.00%) (11975/12928)
Epoch: 104 | Batch_idx: 110 |  Loss: (0.2142) |  Loss2: (0.0000) | Acc: (92.00%) (13153/14208)
Epoch: 104 | Batch_idx: 120 |  Loss: (0.2116) |  Loss2: (0.0000) | Acc: (92.00%) (14353/15488)
Epoch: 104 | Batch_idx: 130 |  Loss: (0.2099) |  Loss2: (0.0000) | Acc: (92.00%) (15556/16768)
Epoch: 104 | Batch_idx: 140 |  Loss: (0.2089) |  Loss2: (0.0000) | Acc: (92.00%) (16750/18048)
Epoch: 104 | Batch_idx: 150 |  Loss: (0.2083) |  Loss2: (0.0000) | Acc: (92.00%) (17940/19328)
Epoch: 104 | Batch_idx: 160 |  Loss: (0.2088) |  Loss2: (0.0000) | Acc: (92.00%) (19127/20608)
Epoch: 104 | Batch_idx: 170 |  Loss: (0.2076) |  Loss2: (0.0000) | Acc: (92.00%) (20324/21888)
Epoch: 104 | Batch_idx: 180 |  Loss: (0.2076) |  Loss2: (0.0000) | Acc: (92.00%) (21511/23168)
Epoch: 104 | Batch_idx: 190 |  Loss: (0.2085) |  Loss2: (0.0000) | Acc: (92.00%) (22696/24448)
Epoch: 104 | Batch_idx: 200 |  Loss: (0.2075) |  Loss2: (0.0000) | Acc: (92.00%) (23898/25728)
Epoch: 104 | Batch_idx: 210 |  Loss: (0.2074) |  Loss2: (0.0000) | Acc: (92.00%) (25093/27008)
Epoch: 104 | Batch_idx: 220 |  Loss: (0.2079) |  Loss2: (0.0000) | Acc: (92.00%) (26280/28288)
Epoch: 104 | Batch_idx: 230 |  Loss: (0.2088) |  Loss2: (0.0000) | Acc: (92.00%) (27467/29568)
Epoch: 104 | Batch_idx: 240 |  Loss: (0.2101) |  Loss2: (0.0000) | Acc: (92.00%) (28652/30848)
Epoch: 104 | Batch_idx: 250 |  Loss: (0.2097) |  Loss2: (0.0000) | Acc: (92.00%) (29839/32128)
Epoch: 104 | Batch_idx: 260 |  Loss: (0.2096) |  Loss2: (0.0000) | Acc: (92.00%) (31024/33408)
Epoch: 104 | Batch_idx: 270 |  Loss: (0.2106) |  Loss2: (0.0000) | Acc: (92.00%) (32207/34688)
Epoch: 104 | Batch_idx: 280 |  Loss: (0.2119) |  Loss2: (0.0000) | Acc: (92.00%) (33377/35968)
Epoch: 104 | Batch_idx: 290 |  Loss: (0.2120) |  Loss2: (0.0000) | Acc: (92.00%) (34560/37248)
Epoch: 104 | Batch_idx: 300 |  Loss: (0.2119) |  Loss2: (0.0000) | Acc: (92.00%) (35749/38528)
Epoch: 104 | Batch_idx: 310 |  Loss: (0.2125) |  Loss2: (0.0000) | Acc: (92.00%) (36927/39808)
Epoch: 104 | Batch_idx: 320 |  Loss: (0.2128) |  Loss2: (0.0000) | Acc: (92.00%) (38108/41088)
Epoch: 104 | Batch_idx: 330 |  Loss: (0.2131) |  Loss2: (0.0000) | Acc: (92.00%) (39283/42368)
Epoch: 104 | Batch_idx: 340 |  Loss: (0.2134) |  Loss2: (0.0000) | Acc: (92.00%) (40463/43648)
Epoch: 104 | Batch_idx: 350 |  Loss: (0.2135) |  Loss2: (0.0000) | Acc: (92.00%) (41647/44928)
Epoch: 104 | Batch_idx: 360 |  Loss: (0.2135) |  Loss2: (0.0000) | Acc: (92.00%) (42827/46208)
Epoch: 104 | Batch_idx: 370 |  Loss: (0.2130) |  Loss2: (0.0000) | Acc: (92.00%) (44020/47488)
Epoch: 104 | Batch_idx: 380 |  Loss: (0.2128) |  Loss2: (0.0000) | Acc: (92.00%) (45209/48768)
Epoch: 104 | Batch_idx: 390 |  Loss: (0.2118) |  Loss2: (0.0000) | Acc: (92.00%) (46368/50000)
# TEST : Loss: (0.4476) | Acc: (86.00%) (8638/10000)
percent tensor([0.5207, 0.4793], device='cuda:0')
percent tensor([0.4635, 0.5365], device='cuda:0')
percent tensor([0.5477, 0.4523], device='cuda:0')
percent tensor([0.5870, 0.4130], device='cuda:0')
percent tensor([0.5612, 0.4388], device='cuda:0')
percent tensor([0.6289, 0.3711], device='cuda:0')
percent tensor([0.6650, 0.3350], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 105 | Batch_idx: 0 |  Loss: (0.2433) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 105 | Batch_idx: 10 |  Loss: (0.2195) |  Loss2: (0.0000) | Acc: (92.00%) (1301/1408)
Epoch: 105 | Batch_idx: 20 |  Loss: (0.2361) |  Loss2: (0.0000) | Acc: (91.00%) (2472/2688)
Epoch: 105 | Batch_idx: 30 |  Loss: (0.2504) |  Loss2: (0.0000) | Acc: (91.00%) (3633/3968)
Epoch: 105 | Batch_idx: 40 |  Loss: (0.2582) |  Loss2: (0.0000) | Acc: (91.00%) (4785/5248)
Epoch: 105 | Batch_idx: 50 |  Loss: (0.2579) |  Loss2: (0.0000) | Acc: (90.00%) (5938/6528)
Epoch: 105 | Batch_idx: 60 |  Loss: (0.2569) |  Loss2: (0.0000) | Acc: (91.00%) (7111/7808)
Epoch: 105 | Batch_idx: 70 |  Loss: (0.2571) |  Loss2: (0.0000) | Acc: (91.00%) (8271/9088)
Epoch: 105 | Batch_idx: 80 |  Loss: (0.2619) |  Loss2: (0.0000) | Acc: (90.00%) (9412/10368)
Epoch: 105 | Batch_idx: 90 |  Loss: (0.2660) |  Loss2: (0.0000) | Acc: (90.00%) (10572/11648)
Epoch: 105 | Batch_idx: 100 |  Loss: (0.2642) |  Loss2: (0.0000) | Acc: (90.00%) (11741/12928)
Epoch: 105 | Batch_idx: 110 |  Loss: (0.2625) |  Loss2: (0.0000) | Acc: (90.00%) (12905/14208)
Epoch: 105 | Batch_idx: 120 |  Loss: (0.2634) |  Loss2: (0.0000) | Acc: (90.00%) (14062/15488)
Epoch: 105 | Batch_idx: 130 |  Loss: (0.2652) |  Loss2: (0.0000) | Acc: (90.00%) (15219/16768)
Epoch: 105 | Batch_idx: 140 |  Loss: (0.2663) |  Loss2: (0.0000) | Acc: (90.00%) (16375/18048)
Epoch: 105 | Batch_idx: 150 |  Loss: (0.2650) |  Loss2: (0.0000) | Acc: (90.00%) (17537/19328)
Epoch: 105 | Batch_idx: 160 |  Loss: (0.2652) |  Loss2: (0.0000) | Acc: (90.00%) (18704/20608)
Epoch: 105 | Batch_idx: 170 |  Loss: (0.2659) |  Loss2: (0.0000) | Acc: (90.00%) (19872/21888)
Epoch: 105 | Batch_idx: 180 |  Loss: (0.2670) |  Loss2: (0.0000) | Acc: (90.00%) (21021/23168)
Epoch: 105 | Batch_idx: 190 |  Loss: (0.2656) |  Loss2: (0.0000) | Acc: (90.00%) (22185/24448)
Epoch: 105 | Batch_idx: 200 |  Loss: (0.2654) |  Loss2: (0.0000) | Acc: (90.00%) (23347/25728)
Epoch: 105 | Batch_idx: 210 |  Loss: (0.2663) |  Loss2: (0.0000) | Acc: (90.00%) (24503/27008)
Epoch: 105 | Batch_idx: 220 |  Loss: (0.2664) |  Loss2: (0.0000) | Acc: (90.00%) (25668/28288)
Epoch: 105 | Batch_idx: 230 |  Loss: (0.2657) |  Loss2: (0.0000) | Acc: (90.00%) (26838/29568)
Epoch: 105 | Batch_idx: 240 |  Loss: (0.2646) |  Loss2: (0.0000) | Acc: (90.00%) (28005/30848)
Epoch: 105 | Batch_idx: 250 |  Loss: (0.2646) |  Loss2: (0.0000) | Acc: (90.00%) (29161/32128)
Epoch: 105 | Batch_idx: 260 |  Loss: (0.2659) |  Loss2: (0.0000) | Acc: (90.00%) (30300/33408)
Epoch: 105 | Batch_idx: 270 |  Loss: (0.2645) |  Loss2: (0.0000) | Acc: (90.00%) (31465/34688)
Epoch: 105 | Batch_idx: 280 |  Loss: (0.2634) |  Loss2: (0.0000) | Acc: (90.00%) (32639/35968)
Epoch: 105 | Batch_idx: 290 |  Loss: (0.2636) |  Loss2: (0.0000) | Acc: (90.00%) (33787/37248)
Epoch: 105 | Batch_idx: 300 |  Loss: (0.2636) |  Loss2: (0.0000) | Acc: (90.00%) (34950/38528)
Epoch: 105 | Batch_idx: 310 |  Loss: (0.2634) |  Loss2: (0.0000) | Acc: (90.00%) (36130/39808)
Epoch: 105 | Batch_idx: 320 |  Loss: (0.2632) |  Loss2: (0.0000) | Acc: (90.00%) (37302/41088)
Epoch: 105 | Batch_idx: 330 |  Loss: (0.2620) |  Loss2: (0.0000) | Acc: (90.00%) (38476/42368)
Epoch: 105 | Batch_idx: 340 |  Loss: (0.2614) |  Loss2: (0.0000) | Acc: (90.00%) (39649/43648)
Epoch: 105 | Batch_idx: 350 |  Loss: (0.2613) |  Loss2: (0.0000) | Acc: (90.00%) (40809/44928)
Epoch: 105 | Batch_idx: 360 |  Loss: (0.2605) |  Loss2: (0.0000) | Acc: (90.00%) (41989/46208)
Epoch: 105 | Batch_idx: 370 |  Loss: (0.2604) |  Loss2: (0.0000) | Acc: (90.00%) (43152/47488)
Epoch: 105 | Batch_idx: 380 |  Loss: (0.2605) |  Loss2: (0.0000) | Acc: (90.00%) (44329/48768)
Epoch: 105 | Batch_idx: 390 |  Loss: (0.2603) |  Loss2: (0.0000) | Acc: (90.00%) (45454/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_105.pth.tar'
# TEST : Loss: (0.4579) | Acc: (85.00%) (8580/10000)
percent tensor([0.5231, 0.4769], device='cuda:0')
percent tensor([0.4627, 0.5373], device='cuda:0')
percent tensor([0.5414, 0.4586], device='cuda:0')
percent tensor([0.5718, 0.4282], device='cuda:0')
percent tensor([0.5730, 0.4270], device='cuda:0')
percent tensor([0.6053, 0.3947], device='cuda:0')
percent tensor([0.6603, 0.3397], device='cuda:0')
percent tensor([0.9988, 0.0012], device='cuda:0')
Epoch: 106 | Batch_idx: 0 |  Loss: (0.2543) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 106 | Batch_idx: 10 |  Loss: (0.2430) |  Loss2: (0.0000) | Acc: (91.00%) (1295/1408)
Epoch: 106 | Batch_idx: 20 |  Loss: (0.2406) |  Loss2: (0.0000) | Acc: (92.00%) (2475/2688)
Epoch: 106 | Batch_idx: 30 |  Loss: (0.2401) |  Loss2: (0.0000) | Acc: (92.00%) (3653/3968)
Epoch: 106 | Batch_idx: 40 |  Loss: (0.2432) |  Loss2: (0.0000) | Acc: (91.00%) (4819/5248)
Epoch: 106 | Batch_idx: 50 |  Loss: (0.2423) |  Loss2: (0.0000) | Acc: (91.00%) (5992/6528)
Epoch: 106 | Batch_idx: 60 |  Loss: (0.2443) |  Loss2: (0.0000) | Acc: (91.00%) (7167/7808)
Epoch: 106 | Batch_idx: 70 |  Loss: (0.2408) |  Loss2: (0.0000) | Acc: (91.00%) (8358/9088)
Epoch: 106 | Batch_idx: 80 |  Loss: (0.2407) |  Loss2: (0.0000) | Acc: (92.00%) (9539/10368)
Epoch: 106 | Batch_idx: 90 |  Loss: (0.2451) |  Loss2: (0.0000) | Acc: (91.00%) (10691/11648)
Epoch: 106 | Batch_idx: 100 |  Loss: (0.2464) |  Loss2: (0.0000) | Acc: (91.00%) (11854/12928)
Epoch: 106 | Batch_idx: 110 |  Loss: (0.2467) |  Loss2: (0.0000) | Acc: (91.00%) (13025/14208)
Epoch: 106 | Batch_idx: 120 |  Loss: (0.2478) |  Loss2: (0.0000) | Acc: (91.00%) (14196/15488)
Epoch: 106 | Batch_idx: 130 |  Loss: (0.2456) |  Loss2: (0.0000) | Acc: (91.00%) (15376/16768)
Epoch: 106 | Batch_idx: 140 |  Loss: (0.2440) |  Loss2: (0.0000) | Acc: (91.00%) (16556/18048)
Epoch: 106 | Batch_idx: 150 |  Loss: (0.2428) |  Loss2: (0.0000) | Acc: (91.00%) (17724/19328)
Epoch: 106 | Batch_idx: 160 |  Loss: (0.2431) |  Loss2: (0.0000) | Acc: (91.00%) (18904/20608)
Epoch: 106 | Batch_idx: 170 |  Loss: (0.2426) |  Loss2: (0.0000) | Acc: (91.00%) (20080/21888)
Epoch: 106 | Batch_idx: 180 |  Loss: (0.2410) |  Loss2: (0.0000) | Acc: (91.00%) (21266/23168)
Epoch: 106 | Batch_idx: 190 |  Loss: (0.2399) |  Loss2: (0.0000) | Acc: (91.00%) (22457/24448)
Epoch: 106 | Batch_idx: 200 |  Loss: (0.2412) |  Loss2: (0.0000) | Acc: (91.00%) (23608/25728)
Epoch: 106 | Batch_idx: 210 |  Loss: (0.2416) |  Loss2: (0.0000) | Acc: (91.00%) (24765/27008)
Epoch: 106 | Batch_idx: 220 |  Loss: (0.2413) |  Loss2: (0.0000) | Acc: (91.00%) (25942/28288)
Epoch: 106 | Batch_idx: 230 |  Loss: (0.2418) |  Loss2: (0.0000) | Acc: (91.00%) (27111/29568)
Epoch: 106 | Batch_idx: 240 |  Loss: (0.2412) |  Loss2: (0.0000) | Acc: (91.00%) (28283/30848)
Epoch: 106 | Batch_idx: 250 |  Loss: (0.2416) |  Loss2: (0.0000) | Acc: (91.00%) (29450/32128)
Epoch: 106 | Batch_idx: 260 |  Loss: (0.2408) |  Loss2: (0.0000) | Acc: (91.00%) (30634/33408)
Epoch: 106 | Batch_idx: 270 |  Loss: (0.2404) |  Loss2: (0.0000) | Acc: (91.00%) (31811/34688)
Epoch: 106 | Batch_idx: 280 |  Loss: (0.2396) |  Loss2: (0.0000) | Acc: (91.00%) (33003/35968)
Epoch: 106 | Batch_idx: 290 |  Loss: (0.2382) |  Loss2: (0.0000) | Acc: (91.00%) (34193/37248)
Epoch: 106 | Batch_idx: 300 |  Loss: (0.2366) |  Loss2: (0.0000) | Acc: (91.00%) (35397/38528)
Epoch: 106 | Batch_idx: 310 |  Loss: (0.2368) |  Loss2: (0.0000) | Acc: (91.00%) (36570/39808)
Epoch: 106 | Batch_idx: 320 |  Loss: (0.2364) |  Loss2: (0.0000) | Acc: (91.00%) (37758/41088)
Epoch: 106 | Batch_idx: 330 |  Loss: (0.2364) |  Loss2: (0.0000) | Acc: (91.00%) (38931/42368)
Epoch: 106 | Batch_idx: 340 |  Loss: (0.2364) |  Loss2: (0.0000) | Acc: (91.00%) (40104/43648)
Epoch: 106 | Batch_idx: 350 |  Loss: (0.2358) |  Loss2: (0.0000) | Acc: (91.00%) (41289/44928)
Epoch: 106 | Batch_idx: 360 |  Loss: (0.2350) |  Loss2: (0.0000) | Acc: (91.00%) (42485/46208)
Epoch: 106 | Batch_idx: 370 |  Loss: (0.2345) |  Loss2: (0.0000) | Acc: (91.00%) (43675/47488)
Epoch: 106 | Batch_idx: 380 |  Loss: (0.2341) |  Loss2: (0.0000) | Acc: (91.00%) (44853/48768)
Epoch: 106 | Batch_idx: 390 |  Loss: (0.2337) |  Loss2: (0.0000) | Acc: (91.00%) (45996/50000)
# TEST : Loss: (0.4332) | Acc: (86.00%) (8675/10000)
percent tensor([0.5239, 0.4761], device='cuda:0')
percent tensor([0.4628, 0.5372], device='cuda:0')
percent tensor([0.5399, 0.4601], device='cuda:0')
percent tensor([0.5716, 0.4284], device='cuda:0')
percent tensor([0.5748, 0.4252], device='cuda:0')
percent tensor([0.6080, 0.3920], device='cuda:0')
percent tensor([0.6630, 0.3370], device='cuda:0')
percent tensor([0.9988, 0.0012], device='cuda:0')
Epoch: 107 | Batch_idx: 0 |  Loss: (0.2322) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 107 | Batch_idx: 10 |  Loss: (0.2213) |  Loss2: (0.0000) | Acc: (91.00%) (1293/1408)
Epoch: 107 | Batch_idx: 20 |  Loss: (0.2252) |  Loss2: (0.0000) | Acc: (91.00%) (2467/2688)
Epoch: 107 | Batch_idx: 30 |  Loss: (0.2147) |  Loss2: (0.0000) | Acc: (92.00%) (3658/3968)
Epoch: 107 | Batch_idx: 40 |  Loss: (0.2171) |  Loss2: (0.0000) | Acc: (92.00%) (4844/5248)
Epoch: 107 | Batch_idx: 50 |  Loss: (0.2226) |  Loss2: (0.0000) | Acc: (92.00%) (6013/6528)
Epoch: 107 | Batch_idx: 60 |  Loss: (0.2249) |  Loss2: (0.0000) | Acc: (92.00%) (7189/7808)
Epoch: 107 | Batch_idx: 70 |  Loss: (0.2253) |  Loss2: (0.0000) | Acc: (92.00%) (8371/9088)
Epoch: 107 | Batch_idx: 80 |  Loss: (0.2256) |  Loss2: (0.0000) | Acc: (92.00%) (9555/10368)
Epoch: 107 | Batch_idx: 90 |  Loss: (0.2238) |  Loss2: (0.0000) | Acc: (92.00%) (10746/11648)
Epoch: 107 | Batch_idx: 100 |  Loss: (0.2246) |  Loss2: (0.0000) | Acc: (92.00%) (11934/12928)
Epoch: 107 | Batch_idx: 110 |  Loss: (0.2267) |  Loss2: (0.0000) | Acc: (92.00%) (13092/14208)
Epoch: 107 | Batch_idx: 120 |  Loss: (0.2265) |  Loss2: (0.0000) | Acc: (92.00%) (14262/15488)
Epoch: 107 | Batch_idx: 130 |  Loss: (0.2289) |  Loss2: (0.0000) | Acc: (92.00%) (15433/16768)
Epoch: 107 | Batch_idx: 140 |  Loss: (0.2285) |  Loss2: (0.0000) | Acc: (92.00%) (16616/18048)
Epoch: 107 | Batch_idx: 150 |  Loss: (0.2292) |  Loss2: (0.0000) | Acc: (92.00%) (17787/19328)
Epoch: 107 | Batch_idx: 160 |  Loss: (0.2271) |  Loss2: (0.0000) | Acc: (92.00%) (18987/20608)
Epoch: 107 | Batch_idx: 170 |  Loss: (0.2266) |  Loss2: (0.0000) | Acc: (92.00%) (20175/21888)
Epoch: 107 | Batch_idx: 180 |  Loss: (0.2252) |  Loss2: (0.0000) | Acc: (92.00%) (21370/23168)
Epoch: 107 | Batch_idx: 190 |  Loss: (0.2242) |  Loss2: (0.0000) | Acc: (92.00%) (22564/24448)
Epoch: 107 | Batch_idx: 200 |  Loss: (0.2238) |  Loss2: (0.0000) | Acc: (92.00%) (23742/25728)
Epoch: 107 | Batch_idx: 210 |  Loss: (0.2235) |  Loss2: (0.0000) | Acc: (92.00%) (24924/27008)
Epoch: 107 | Batch_idx: 220 |  Loss: (0.2232) |  Loss2: (0.0000) | Acc: (92.00%) (26111/28288)
Epoch: 107 | Batch_idx: 230 |  Loss: (0.2219) |  Loss2: (0.0000) | Acc: (92.00%) (27302/29568)
Epoch: 107 | Batch_idx: 240 |  Loss: (0.2217) |  Loss2: (0.0000) | Acc: (92.00%) (28490/30848)
Epoch: 107 | Batch_idx: 250 |  Loss: (0.2233) |  Loss2: (0.0000) | Acc: (92.00%) (29649/32128)
Epoch: 107 | Batch_idx: 260 |  Loss: (0.2229) |  Loss2: (0.0000) | Acc: (92.00%) (30834/33408)
Epoch: 107 | Batch_idx: 270 |  Loss: (0.2221) |  Loss2: (0.0000) | Acc: (92.00%) (32027/34688)
Epoch: 107 | Batch_idx: 280 |  Loss: (0.2238) |  Loss2: (0.0000) | Acc: (92.00%) (33178/35968)
Epoch: 107 | Batch_idx: 290 |  Loss: (0.2236) |  Loss2: (0.0000) | Acc: (92.00%) (34365/37248)
Epoch: 107 | Batch_idx: 300 |  Loss: (0.2243) |  Loss2: (0.0000) | Acc: (92.00%) (35525/38528)
Epoch: 107 | Batch_idx: 310 |  Loss: (0.2243) |  Loss2: (0.0000) | Acc: (92.00%) (36711/39808)
Epoch: 107 | Batch_idx: 320 |  Loss: (0.2239) |  Loss2: (0.0000) | Acc: (92.00%) (37888/41088)
Epoch: 107 | Batch_idx: 330 |  Loss: (0.2238) |  Loss2: (0.0000) | Acc: (92.00%) (39066/42368)
Epoch: 107 | Batch_idx: 340 |  Loss: (0.2241) |  Loss2: (0.0000) | Acc: (92.00%) (40247/43648)
Epoch: 107 | Batch_idx: 350 |  Loss: (0.2239) |  Loss2: (0.0000) | Acc: (92.00%) (41438/44928)
Epoch: 107 | Batch_idx: 360 |  Loss: (0.2233) |  Loss2: (0.0000) | Acc: (92.00%) (42628/46208)
Epoch: 107 | Batch_idx: 370 |  Loss: (0.2231) |  Loss2: (0.0000) | Acc: (92.00%) (43806/47488)
Epoch: 107 | Batch_idx: 380 |  Loss: (0.2230) |  Loss2: (0.0000) | Acc: (92.00%) (44993/48768)
Epoch: 107 | Batch_idx: 390 |  Loss: (0.2237) |  Loss2: (0.0000) | Acc: (92.00%) (46117/50000)
# TEST : Loss: (0.4222) | Acc: (86.00%) (8681/10000)
percent tensor([0.5244, 0.4756], device='cuda:0')
percent tensor([0.4655, 0.5345], device='cuda:0')
percent tensor([0.5417, 0.4583], device='cuda:0')
percent tensor([0.5726, 0.4274], device='cuda:0')
percent tensor([0.5700, 0.4300], device='cuda:0')
percent tensor([0.6120, 0.3880], device='cuda:0')
percent tensor([0.6755, 0.3245], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
Epoch: 108 | Batch_idx: 0 |  Loss: (0.1586) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 108 | Batch_idx: 10 |  Loss: (0.2238) |  Loss2: (0.0000) | Acc: (92.00%) (1301/1408)
Epoch: 108 | Batch_idx: 20 |  Loss: (0.2142) |  Loss2: (0.0000) | Acc: (92.00%) (2497/2688)
Epoch: 108 | Batch_idx: 30 |  Loss: (0.2258) |  Loss2: (0.0000) | Acc: (92.00%) (3672/3968)
Epoch: 108 | Batch_idx: 40 |  Loss: (0.2277) |  Loss2: (0.0000) | Acc: (92.00%) (4844/5248)
Epoch: 108 | Batch_idx: 50 |  Loss: (0.2211) |  Loss2: (0.0000) | Acc: (92.00%) (6038/6528)
Epoch: 108 | Batch_idx: 60 |  Loss: (0.2236) |  Loss2: (0.0000) | Acc: (92.00%) (7213/7808)
Epoch: 108 | Batch_idx: 70 |  Loss: (0.2229) |  Loss2: (0.0000) | Acc: (92.00%) (8394/9088)
Epoch: 108 | Batch_idx: 80 |  Loss: (0.2243) |  Loss2: (0.0000) | Acc: (92.00%) (9567/10368)
Epoch: 108 | Batch_idx: 90 |  Loss: (0.2263) |  Loss2: (0.0000) | Acc: (92.00%) (10741/11648)
Epoch: 108 | Batch_idx: 100 |  Loss: (0.2248) |  Loss2: (0.0000) | Acc: (92.00%) (11928/12928)
Epoch: 108 | Batch_idx: 110 |  Loss: (0.2260) |  Loss2: (0.0000) | Acc: (92.00%) (13108/14208)
Epoch: 108 | Batch_idx: 120 |  Loss: (0.2287) |  Loss2: (0.0000) | Acc: (92.00%) (14279/15488)
Epoch: 108 | Batch_idx: 130 |  Loss: (0.2252) |  Loss2: (0.0000) | Acc: (92.00%) (15485/16768)
Epoch: 108 | Batch_idx: 140 |  Loss: (0.2225) |  Loss2: (0.0000) | Acc: (92.00%) (16692/18048)
Epoch: 108 | Batch_idx: 150 |  Loss: (0.2217) |  Loss2: (0.0000) | Acc: (92.00%) (17870/19328)
Epoch: 108 | Batch_idx: 160 |  Loss: (0.2208) |  Loss2: (0.0000) | Acc: (92.00%) (19058/20608)
Epoch: 108 | Batch_idx: 170 |  Loss: (0.2227) |  Loss2: (0.0000) | Acc: (92.00%) (20233/21888)
Epoch: 108 | Batch_idx: 180 |  Loss: (0.2209) |  Loss2: (0.0000) | Acc: (92.00%) (21429/23168)
Epoch: 108 | Batch_idx: 190 |  Loss: (0.2223) |  Loss2: (0.0000) | Acc: (92.00%) (22596/24448)
Epoch: 108 | Batch_idx: 200 |  Loss: (0.2218) |  Loss2: (0.0000) | Acc: (92.00%) (23778/25728)
Epoch: 108 | Batch_idx: 210 |  Loss: (0.2215) |  Loss2: (0.0000) | Acc: (92.00%) (24963/27008)
Epoch: 108 | Batch_idx: 220 |  Loss: (0.2204) |  Loss2: (0.0000) | Acc: (92.00%) (26159/28288)
Epoch: 108 | Batch_idx: 230 |  Loss: (0.2195) |  Loss2: (0.0000) | Acc: (92.00%) (27350/29568)
Epoch: 108 | Batch_idx: 240 |  Loss: (0.2200) |  Loss2: (0.0000) | Acc: (92.00%) (28527/30848)
Epoch: 108 | Batch_idx: 250 |  Loss: (0.2195) |  Loss2: (0.0000) | Acc: (92.00%) (29713/32128)
Epoch: 108 | Batch_idx: 260 |  Loss: (0.2199) |  Loss2: (0.0000) | Acc: (92.00%) (30898/33408)
Epoch: 108 | Batch_idx: 270 |  Loss: (0.2200) |  Loss2: (0.0000) | Acc: (92.00%) (32087/34688)
Epoch: 108 | Batch_idx: 280 |  Loss: (0.2198) |  Loss2: (0.0000) | Acc: (92.00%) (33273/35968)
Epoch: 108 | Batch_idx: 290 |  Loss: (0.2199) |  Loss2: (0.0000) | Acc: (92.00%) (34446/37248)
Epoch: 108 | Batch_idx: 300 |  Loss: (0.2192) |  Loss2: (0.0000) | Acc: (92.00%) (35635/38528)
Epoch: 108 | Batch_idx: 310 |  Loss: (0.2183) |  Loss2: (0.0000) | Acc: (92.00%) (36833/39808)
Epoch: 108 | Batch_idx: 320 |  Loss: (0.2183) |  Loss2: (0.0000) | Acc: (92.00%) (38023/41088)
Epoch: 108 | Batch_idx: 330 |  Loss: (0.2185) |  Loss2: (0.0000) | Acc: (92.00%) (39196/42368)
Epoch: 108 | Batch_idx: 340 |  Loss: (0.2178) |  Loss2: (0.0000) | Acc: (92.00%) (40395/43648)
Epoch: 108 | Batch_idx: 350 |  Loss: (0.2181) |  Loss2: (0.0000) | Acc: (92.00%) (41574/44928)
Epoch: 108 | Batch_idx: 360 |  Loss: (0.2180) |  Loss2: (0.0000) | Acc: (92.00%) (42772/46208)
Epoch: 108 | Batch_idx: 370 |  Loss: (0.2172) |  Loss2: (0.0000) | Acc: (92.00%) (43974/47488)
Epoch: 108 | Batch_idx: 380 |  Loss: (0.2168) |  Loss2: (0.0000) | Acc: (92.00%) (45159/48768)
Epoch: 108 | Batch_idx: 390 |  Loss: (0.2160) |  Loss2: (0.0000) | Acc: (92.00%) (46314/50000)
# TEST : Loss: (0.4105) | Acc: (87.00%) (8711/10000)
percent tensor([0.5234, 0.4766], device='cuda:0')
percent tensor([0.4666, 0.5334], device='cuda:0')
percent tensor([0.5430, 0.4570], device='cuda:0')
percent tensor([0.5750, 0.4250], device='cuda:0')
percent tensor([0.5736, 0.4264], device='cuda:0')
percent tensor([0.6194, 0.3806], device='cuda:0')
percent tensor([0.6767, 0.3233], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
Epoch: 109 | Batch_idx: 0 |  Loss: (0.1451) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 109 | Batch_idx: 10 |  Loss: (0.2051) |  Loss2: (0.0000) | Acc: (93.00%) (1312/1408)
Epoch: 109 | Batch_idx: 20 |  Loss: (0.1943) |  Loss2: (0.0000) | Acc: (93.00%) (2518/2688)
Epoch: 109 | Batch_idx: 30 |  Loss: (0.2037) |  Loss2: (0.0000) | Acc: (93.00%) (3695/3968)
Epoch: 109 | Batch_idx: 40 |  Loss: (0.2071) |  Loss2: (0.0000) | Acc: (92.00%) (4877/5248)
Epoch: 109 | Batch_idx: 50 |  Loss: (0.2064) |  Loss2: (0.0000) | Acc: (92.00%) (6071/6528)
Epoch: 109 | Batch_idx: 60 |  Loss: (0.2096) |  Loss2: (0.0000) | Acc: (92.00%) (7253/7808)
Epoch: 109 | Batch_idx: 70 |  Loss: (0.2127) |  Loss2: (0.0000) | Acc: (92.00%) (8431/9088)
Epoch: 109 | Batch_idx: 80 |  Loss: (0.2108) |  Loss2: (0.0000) | Acc: (92.00%) (9622/10368)
Epoch: 109 | Batch_idx: 90 |  Loss: (0.2114) |  Loss2: (0.0000) | Acc: (92.00%) (10807/11648)
Epoch: 109 | Batch_idx: 100 |  Loss: (0.2084) |  Loss2: (0.0000) | Acc: (92.00%) (12004/12928)
Epoch: 109 | Batch_idx: 110 |  Loss: (0.2081) |  Loss2: (0.0000) | Acc: (92.00%) (13201/14208)
Epoch: 109 | Batch_idx: 120 |  Loss: (0.2079) |  Loss2: (0.0000) | Acc: (92.00%) (14390/15488)
Epoch: 109 | Batch_idx: 130 |  Loss: (0.2078) |  Loss2: (0.0000) | Acc: (92.00%) (15578/16768)
Epoch: 109 | Batch_idx: 140 |  Loss: (0.2070) |  Loss2: (0.0000) | Acc: (92.00%) (16779/18048)
Epoch: 109 | Batch_idx: 150 |  Loss: (0.2068) |  Loss2: (0.0000) | Acc: (93.00%) (17981/19328)
Epoch: 109 | Batch_idx: 160 |  Loss: (0.2067) |  Loss2: (0.0000) | Acc: (93.00%) (19168/20608)
Epoch: 109 | Batch_idx: 170 |  Loss: (0.2068) |  Loss2: (0.0000) | Acc: (93.00%) (20359/21888)
Epoch: 109 | Batch_idx: 180 |  Loss: (0.2059) |  Loss2: (0.0000) | Acc: (93.00%) (21559/23168)
Epoch: 109 | Batch_idx: 190 |  Loss: (0.2059) |  Loss2: (0.0000) | Acc: (93.00%) (22755/24448)
Epoch: 109 | Batch_idx: 200 |  Loss: (0.2071) |  Loss2: (0.0000) | Acc: (93.00%) (23938/25728)
Epoch: 109 | Batch_idx: 210 |  Loss: (0.2085) |  Loss2: (0.0000) | Acc: (93.00%) (25119/27008)
Epoch: 109 | Batch_idx: 220 |  Loss: (0.2086) |  Loss2: (0.0000) | Acc: (92.00%) (26305/28288)
Epoch: 109 | Batch_idx: 230 |  Loss: (0.2088) |  Loss2: (0.0000) | Acc: (92.00%) (27493/29568)
Epoch: 109 | Batch_idx: 240 |  Loss: (0.2096) |  Loss2: (0.0000) | Acc: (92.00%) (28672/30848)
Epoch: 109 | Batch_idx: 250 |  Loss: (0.2094) |  Loss2: (0.0000) | Acc: (92.00%) (29865/32128)
Epoch: 109 | Batch_idx: 260 |  Loss: (0.2092) |  Loss2: (0.0000) | Acc: (93.00%) (31070/33408)
Epoch: 109 | Batch_idx: 270 |  Loss: (0.2099) |  Loss2: (0.0000) | Acc: (92.00%) (32249/34688)
Epoch: 109 | Batch_idx: 280 |  Loss: (0.2105) |  Loss2: (0.0000) | Acc: (92.00%) (33426/35968)
Epoch: 109 | Batch_idx: 290 |  Loss: (0.2116) |  Loss2: (0.0000) | Acc: (92.00%) (34602/37248)
Epoch: 109 | Batch_idx: 300 |  Loss: (0.2110) |  Loss2: (0.0000) | Acc: (92.00%) (35799/38528)
Epoch: 109 | Batch_idx: 310 |  Loss: (0.2115) |  Loss2: (0.0000) | Acc: (92.00%) (36982/39808)
Epoch: 109 | Batch_idx: 320 |  Loss: (0.2116) |  Loss2: (0.0000) | Acc: (92.00%) (38165/41088)
Epoch: 109 | Batch_idx: 330 |  Loss: (0.2109) |  Loss2: (0.0000) | Acc: (92.00%) (39368/42368)
Epoch: 109 | Batch_idx: 340 |  Loss: (0.2099) |  Loss2: (0.0000) | Acc: (92.00%) (40584/43648)
Epoch: 109 | Batch_idx: 350 |  Loss: (0.2102) |  Loss2: (0.0000) | Acc: (92.00%) (41770/44928)
Epoch: 109 | Batch_idx: 360 |  Loss: (0.2108) |  Loss2: (0.0000) | Acc: (92.00%) (42950/46208)
Epoch: 109 | Batch_idx: 370 |  Loss: (0.2096) |  Loss2: (0.0000) | Acc: (93.00%) (44168/47488)
Epoch: 109 | Batch_idx: 380 |  Loss: (0.2097) |  Loss2: (0.0000) | Acc: (92.00%) (45353/48768)
Epoch: 109 | Batch_idx: 390 |  Loss: (0.2094) |  Loss2: (0.0000) | Acc: (93.00%) (46503/50000)
# TEST : Loss: (0.4062) | Acc: (87.00%) (8743/10000)
percent tensor([0.5235, 0.4765], device='cuda:0')
percent tensor([0.4672, 0.5328], device='cuda:0')
percent tensor([0.5432, 0.4568], device='cuda:0')
percent tensor([0.5736, 0.4264], device='cuda:0')
percent tensor([0.5702, 0.4298], device='cuda:0')
percent tensor([0.6262, 0.3738], device='cuda:0')
percent tensor([0.6783, 0.3217], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 110 | Batch_idx: 0 |  Loss: (0.1978) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 110 | Batch_idx: 10 |  Loss: (0.1988) |  Loss2: (0.0000) | Acc: (93.00%) (1311/1408)
Epoch: 110 | Batch_idx: 20 |  Loss: (0.1952) |  Loss2: (0.0000) | Acc: (93.00%) (2506/2688)
Epoch: 110 | Batch_idx: 30 |  Loss: (0.1976) |  Loss2: (0.0000) | Acc: (93.00%) (3692/3968)
Epoch: 110 | Batch_idx: 40 |  Loss: (0.2066) |  Loss2: (0.0000) | Acc: (92.00%) (4868/5248)
Epoch: 110 | Batch_idx: 50 |  Loss: (0.2061) |  Loss2: (0.0000) | Acc: (92.00%) (6066/6528)
Epoch: 110 | Batch_idx: 60 |  Loss: (0.2081) |  Loss2: (0.0000) | Acc: (92.00%) (7246/7808)
Epoch: 110 | Batch_idx: 70 |  Loss: (0.2119) |  Loss2: (0.0000) | Acc: (92.00%) (8422/9088)
Epoch: 110 | Batch_idx: 80 |  Loss: (0.2136) |  Loss2: (0.0000) | Acc: (92.00%) (9603/10368)
Epoch: 110 | Batch_idx: 90 |  Loss: (0.2153) |  Loss2: (0.0000) | Acc: (92.00%) (10781/11648)
Epoch: 110 | Batch_idx: 100 |  Loss: (0.2162) |  Loss2: (0.0000) | Acc: (92.00%) (11959/12928)
Epoch: 110 | Batch_idx: 110 |  Loss: (0.2162) |  Loss2: (0.0000) | Acc: (92.00%) (13143/14208)
Epoch: 110 | Batch_idx: 120 |  Loss: (0.2159) |  Loss2: (0.0000) | Acc: (92.00%) (14329/15488)
Epoch: 110 | Batch_idx: 130 |  Loss: (0.2165) |  Loss2: (0.0000) | Acc: (92.00%) (15511/16768)
Epoch: 110 | Batch_idx: 140 |  Loss: (0.2167) |  Loss2: (0.0000) | Acc: (92.00%) (16685/18048)
Epoch: 110 | Batch_idx: 150 |  Loss: (0.2164) |  Loss2: (0.0000) | Acc: (92.00%) (17869/19328)
Epoch: 110 | Batch_idx: 160 |  Loss: (0.2164) |  Loss2: (0.0000) | Acc: (92.00%) (19050/20608)
Epoch: 110 | Batch_idx: 170 |  Loss: (0.2162) |  Loss2: (0.0000) | Acc: (92.00%) (20238/21888)
Epoch: 110 | Batch_idx: 180 |  Loss: (0.2155) |  Loss2: (0.0000) | Acc: (92.00%) (21429/23168)
Epoch: 110 | Batch_idx: 190 |  Loss: (0.2141) |  Loss2: (0.0000) | Acc: (92.00%) (22628/24448)
Epoch: 110 | Batch_idx: 200 |  Loss: (0.2139) |  Loss2: (0.0000) | Acc: (92.00%) (23813/25728)
Epoch: 110 | Batch_idx: 210 |  Loss: (0.2137) |  Loss2: (0.0000) | Acc: (92.00%) (24997/27008)
Epoch: 110 | Batch_idx: 220 |  Loss: (0.2138) |  Loss2: (0.0000) | Acc: (92.00%) (26183/28288)
Epoch: 110 | Batch_idx: 230 |  Loss: (0.2149) |  Loss2: (0.0000) | Acc: (92.00%) (27355/29568)
Epoch: 110 | Batch_idx: 240 |  Loss: (0.2144) |  Loss2: (0.0000) | Acc: (92.00%) (28543/30848)
Epoch: 110 | Batch_idx: 250 |  Loss: (0.2136) |  Loss2: (0.0000) | Acc: (92.00%) (29746/32128)
Epoch: 110 | Batch_idx: 260 |  Loss: (0.2141) |  Loss2: (0.0000) | Acc: (92.00%) (30929/33408)
Epoch: 110 | Batch_idx: 270 |  Loss: (0.2138) |  Loss2: (0.0000) | Acc: (92.00%) (32123/34688)
Epoch: 110 | Batch_idx: 280 |  Loss: (0.2134) |  Loss2: (0.0000) | Acc: (92.00%) (33310/35968)
Epoch: 110 | Batch_idx: 290 |  Loss: (0.2138) |  Loss2: (0.0000) | Acc: (92.00%) (34500/37248)
Epoch: 110 | Batch_idx: 300 |  Loss: (0.2137) |  Loss2: (0.0000) | Acc: (92.00%) (35696/38528)
Epoch: 110 | Batch_idx: 310 |  Loss: (0.2133) |  Loss2: (0.0000) | Acc: (92.00%) (36895/39808)
Epoch: 110 | Batch_idx: 320 |  Loss: (0.2128) |  Loss2: (0.0000) | Acc: (92.00%) (38089/41088)
Epoch: 110 | Batch_idx: 330 |  Loss: (0.2134) |  Loss2: (0.0000) | Acc: (92.00%) (39266/42368)
Epoch: 110 | Batch_idx: 340 |  Loss: (0.2133) |  Loss2: (0.0000) | Acc: (92.00%) (40457/43648)
Epoch: 110 | Batch_idx: 350 |  Loss: (0.2134) |  Loss2: (0.0000) | Acc: (92.00%) (41633/44928)
Epoch: 110 | Batch_idx: 360 |  Loss: (0.2130) |  Loss2: (0.0000) | Acc: (92.00%) (42817/46208)
Epoch: 110 | Batch_idx: 370 |  Loss: (0.2135) |  Loss2: (0.0000) | Acc: (92.00%) (43997/47488)
Epoch: 110 | Batch_idx: 380 |  Loss: (0.2142) |  Loss2: (0.0000) | Acc: (92.00%) (45179/48768)
Epoch: 110 | Batch_idx: 390 |  Loss: (0.2139) |  Loss2: (0.0000) | Acc: (92.00%) (46322/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_110.pth.tar'
# TEST : Loss: (0.4623) | Acc: (85.00%) (8595/10000)
percent tensor([0.5232, 0.4768], device='cuda:0')
percent tensor([0.4677, 0.5323], device='cuda:0')
percent tensor([0.5433, 0.4567], device='cuda:0')
percent tensor([0.5734, 0.4266], device='cuda:0')
percent tensor([0.5727, 0.4273], device='cuda:0')
percent tensor([0.6270, 0.3730], device='cuda:0')
percent tensor([0.6770, 0.3230], device='cuda:0')
percent tensor([0.9990, 0.0010], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(180.7989, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(813.7266, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(805.8602, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1513.9132, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(501.1037, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2238.0481, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4271.6284, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1389.4530, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6182.6260, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11841.5127, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3914.6375, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16576.2480, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 111 | Batch_idx: 0 |  Loss: (0.2235) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 111 | Batch_idx: 10 |  Loss: (0.2081) |  Loss2: (0.0000) | Acc: (92.00%) (1307/1408)
Epoch: 111 | Batch_idx: 20 |  Loss: (0.1960) |  Loss2: (0.0000) | Acc: (93.00%) (2518/2688)
Epoch: 111 | Batch_idx: 30 |  Loss: (0.1982) |  Loss2: (0.0000) | Acc: (93.00%) (3705/3968)
Epoch: 111 | Batch_idx: 40 |  Loss: (0.1984) |  Loss2: (0.0000) | Acc: (93.00%) (4897/5248)
Epoch: 111 | Batch_idx: 50 |  Loss: (0.1983) |  Loss2: (0.0000) | Acc: (93.00%) (6089/6528)
Epoch: 111 | Batch_idx: 60 |  Loss: (0.1970) |  Loss2: (0.0000) | Acc: (93.00%) (7283/7808)
Epoch: 111 | Batch_idx: 70 |  Loss: (0.1982) |  Loss2: (0.0000) | Acc: (93.00%) (8474/9088)
Epoch: 111 | Batch_idx: 80 |  Loss: (0.1983) |  Loss2: (0.0000) | Acc: (93.00%) (9668/10368)
Epoch: 111 | Batch_idx: 90 |  Loss: (0.1962) |  Loss2: (0.0000) | Acc: (93.00%) (10872/11648)
Epoch: 111 | Batch_idx: 100 |  Loss: (0.1959) |  Loss2: (0.0000) | Acc: (93.00%) (12060/12928)
Epoch: 111 | Batch_idx: 110 |  Loss: (0.1961) |  Loss2: (0.0000) | Acc: (93.00%) (13249/14208)
Epoch: 111 | Batch_idx: 120 |  Loss: (0.1973) |  Loss2: (0.0000) | Acc: (93.00%) (14441/15488)
Epoch: 111 | Batch_idx: 130 |  Loss: (0.1967) |  Loss2: (0.0000) | Acc: (93.00%) (15640/16768)
Epoch: 111 | Batch_idx: 140 |  Loss: (0.1973) |  Loss2: (0.0000) | Acc: (93.00%) (16842/18048)
Epoch: 111 | Batch_idx: 150 |  Loss: (0.1977) |  Loss2: (0.0000) | Acc: (93.00%) (18028/19328)
Epoch: 111 | Batch_idx: 160 |  Loss: (0.1977) |  Loss2: (0.0000) | Acc: (93.00%) (19224/20608)
Epoch: 111 | Batch_idx: 170 |  Loss: (0.1994) |  Loss2: (0.0000) | Acc: (93.00%) (20396/21888)
Epoch: 111 | Batch_idx: 180 |  Loss: (0.1990) |  Loss2: (0.0000) | Acc: (93.00%) (21578/23168)
Epoch: 111 | Batch_idx: 190 |  Loss: (0.1986) |  Loss2: (0.0000) | Acc: (93.00%) (22776/24448)
Epoch: 111 | Batch_idx: 200 |  Loss: (0.1992) |  Loss2: (0.0000) | Acc: (93.00%) (23962/25728)
Epoch: 111 | Batch_idx: 210 |  Loss: (0.2000) |  Loss2: (0.0000) | Acc: (93.00%) (25146/27008)
Epoch: 111 | Batch_idx: 220 |  Loss: (0.2011) |  Loss2: (0.0000) | Acc: (93.00%) (26326/28288)
Epoch: 111 | Batch_idx: 230 |  Loss: (0.2019) |  Loss2: (0.0000) | Acc: (93.00%) (27515/29568)
Epoch: 111 | Batch_idx: 240 |  Loss: (0.2034) |  Loss2: (0.0000) | Acc: (92.00%) (28688/30848)
Epoch: 111 | Batch_idx: 250 |  Loss: (0.2042) |  Loss2: (0.0000) | Acc: (92.00%) (29871/32128)
Epoch: 111 | Batch_idx: 260 |  Loss: (0.2052) |  Loss2: (0.0000) | Acc: (92.00%) (31054/33408)
Epoch: 111 | Batch_idx: 270 |  Loss: (0.2061) |  Loss2: (0.0000) | Acc: (92.00%) (32229/34688)
Epoch: 111 | Batch_idx: 280 |  Loss: (0.2065) |  Loss2: (0.0000) | Acc: (92.00%) (33410/35968)
Epoch: 111 | Batch_idx: 290 |  Loss: (0.2066) |  Loss2: (0.0000) | Acc: (92.00%) (34600/37248)
Epoch: 111 | Batch_idx: 300 |  Loss: (0.2069) |  Loss2: (0.0000) | Acc: (92.00%) (35783/38528)
Epoch: 111 | Batch_idx: 310 |  Loss: (0.2069) |  Loss2: (0.0000) | Acc: (92.00%) (36979/39808)
Epoch: 111 | Batch_idx: 320 |  Loss: (0.2073) |  Loss2: (0.0000) | Acc: (92.00%) (38159/41088)
Epoch: 111 | Batch_idx: 330 |  Loss: (0.2069) |  Loss2: (0.0000) | Acc: (92.00%) (39359/42368)
Epoch: 111 | Batch_idx: 340 |  Loss: (0.2069) |  Loss2: (0.0000) | Acc: (92.00%) (40552/43648)
Epoch: 111 | Batch_idx: 350 |  Loss: (0.2074) |  Loss2: (0.0000) | Acc: (92.00%) (41744/44928)
Epoch: 111 | Batch_idx: 360 |  Loss: (0.2076) |  Loss2: (0.0000) | Acc: (92.00%) (42925/46208)
Epoch: 111 | Batch_idx: 370 |  Loss: (0.2077) |  Loss2: (0.0000) | Acc: (92.00%) (44109/47488)
Epoch: 111 | Batch_idx: 380 |  Loss: (0.2072) |  Loss2: (0.0000) | Acc: (92.00%) (45313/48768)
Epoch: 111 | Batch_idx: 390 |  Loss: (0.2072) |  Loss2: (0.0000) | Acc: (92.00%) (46464/50000)
# TEST : Loss: (0.4200) | Acc: (87.00%) (8705/10000)
percent tensor([0.5237, 0.4763], device='cuda:0')
percent tensor([0.4677, 0.5323], device='cuda:0')
percent tensor([0.5435, 0.4565], device='cuda:0')
percent tensor([0.5743, 0.4257], device='cuda:0')
percent tensor([0.5718, 0.4282], device='cuda:0')
percent tensor([0.6265, 0.3735], device='cuda:0')
percent tensor([0.6749, 0.3251], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
Epoch: 112 | Batch_idx: 0 |  Loss: (0.2089) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 112 | Batch_idx: 10 |  Loss: (0.1929) |  Loss2: (0.0000) | Acc: (93.00%) (1322/1408)
Epoch: 112 | Batch_idx: 20 |  Loss: (0.1993) |  Loss2: (0.0000) | Acc: (93.00%) (2512/2688)
Epoch: 112 | Batch_idx: 30 |  Loss: (0.1951) |  Loss2: (0.0000) | Acc: (93.00%) (3706/3968)
Epoch: 112 | Batch_idx: 40 |  Loss: (0.2023) |  Loss2: (0.0000) | Acc: (93.00%) (4885/5248)
Epoch: 112 | Batch_idx: 50 |  Loss: (0.2083) |  Loss2: (0.0000) | Acc: (92.00%) (6062/6528)
Epoch: 112 | Batch_idx: 60 |  Loss: (0.2075) |  Loss2: (0.0000) | Acc: (92.00%) (7245/7808)
Epoch: 112 | Batch_idx: 70 |  Loss: (0.2069) |  Loss2: (0.0000) | Acc: (92.00%) (8444/9088)
Epoch: 112 | Batch_idx: 80 |  Loss: (0.2075) |  Loss2: (0.0000) | Acc: (92.00%) (9632/10368)
Epoch: 112 | Batch_idx: 90 |  Loss: (0.2088) |  Loss2: (0.0000) | Acc: (92.00%) (10822/11648)
Epoch: 112 | Batch_idx: 100 |  Loss: (0.2076) |  Loss2: (0.0000) | Acc: (92.00%) (12019/12928)
Epoch: 112 | Batch_idx: 110 |  Loss: (0.2069) |  Loss2: (0.0000) | Acc: (93.00%) (13214/14208)
Epoch: 112 | Batch_idx: 120 |  Loss: (0.2063) |  Loss2: (0.0000) | Acc: (92.00%) (14401/15488)
Epoch: 112 | Batch_idx: 130 |  Loss: (0.2041) |  Loss2: (0.0000) | Acc: (93.00%) (15598/16768)
Epoch: 112 | Batch_idx: 140 |  Loss: (0.2036) |  Loss2: (0.0000) | Acc: (93.00%) (16786/18048)
Epoch: 112 | Batch_idx: 150 |  Loss: (0.2039) |  Loss2: (0.0000) | Acc: (92.00%) (17971/19328)
Epoch: 112 | Batch_idx: 160 |  Loss: (0.2034) |  Loss2: (0.0000) | Acc: (92.00%) (19159/20608)
Epoch: 112 | Batch_idx: 170 |  Loss: (0.2037) |  Loss2: (0.0000) | Acc: (92.00%) (20353/21888)
Epoch: 112 | Batch_idx: 180 |  Loss: (0.2028) |  Loss2: (0.0000) | Acc: (92.00%) (21546/23168)
Epoch: 112 | Batch_idx: 190 |  Loss: (0.2030) |  Loss2: (0.0000) | Acc: (92.00%) (22729/24448)
Epoch: 112 | Batch_idx: 200 |  Loss: (0.2035) |  Loss2: (0.0000) | Acc: (92.00%) (23918/25728)
Epoch: 112 | Batch_idx: 210 |  Loss: (0.2036) |  Loss2: (0.0000) | Acc: (92.00%) (25110/27008)
Epoch: 112 | Batch_idx: 220 |  Loss: (0.2039) |  Loss2: (0.0000) | Acc: (92.00%) (26296/28288)
Epoch: 112 | Batch_idx: 230 |  Loss: (0.2034) |  Loss2: (0.0000) | Acc: (92.00%) (27497/29568)
Epoch: 112 | Batch_idx: 240 |  Loss: (0.2020) |  Loss2: (0.0000) | Acc: (93.00%) (28717/30848)
Epoch: 112 | Batch_idx: 250 |  Loss: (0.2032) |  Loss2: (0.0000) | Acc: (93.00%) (29896/32128)
Epoch: 112 | Batch_idx: 260 |  Loss: (0.2031) |  Loss2: (0.0000) | Acc: (93.00%) (31088/33408)
Epoch: 112 | Batch_idx: 270 |  Loss: (0.2039) |  Loss2: (0.0000) | Acc: (93.00%) (32263/34688)
Epoch: 112 | Batch_idx: 280 |  Loss: (0.2038) |  Loss2: (0.0000) | Acc: (93.00%) (33454/35968)
Epoch: 112 | Batch_idx: 290 |  Loss: (0.2043) |  Loss2: (0.0000) | Acc: (92.00%) (34639/37248)
Epoch: 112 | Batch_idx: 300 |  Loss: (0.2038) |  Loss2: (0.0000) | Acc: (93.00%) (35840/38528)
Epoch: 112 | Batch_idx: 310 |  Loss: (0.2042) |  Loss2: (0.0000) | Acc: (92.00%) (37016/39808)
Epoch: 112 | Batch_idx: 320 |  Loss: (0.2045) |  Loss2: (0.0000) | Acc: (92.00%) (38210/41088)
Epoch: 112 | Batch_idx: 330 |  Loss: (0.2044) |  Loss2: (0.0000) | Acc: (93.00%) (39404/42368)
Epoch: 112 | Batch_idx: 340 |  Loss: (0.2048) |  Loss2: (0.0000) | Acc: (92.00%) (40585/43648)
Epoch: 112 | Batch_idx: 350 |  Loss: (0.2044) |  Loss2: (0.0000) | Acc: (92.00%) (41774/44928)
Epoch: 112 | Batch_idx: 360 |  Loss: (0.2040) |  Loss2: (0.0000) | Acc: (92.00%) (42972/46208)
Epoch: 112 | Batch_idx: 370 |  Loss: (0.2039) |  Loss2: (0.0000) | Acc: (92.00%) (44161/47488)
Epoch: 112 | Batch_idx: 380 |  Loss: (0.2040) |  Loss2: (0.0000) | Acc: (92.00%) (45350/48768)
Epoch: 112 | Batch_idx: 390 |  Loss: (0.2048) |  Loss2: (0.0000) | Acc: (92.00%) (46474/50000)
# TEST : Loss: (0.4691) | Acc: (86.00%) (8607/10000)
percent tensor([0.5234, 0.4766], device='cuda:0')
percent tensor([0.4672, 0.5328], device='cuda:0')
percent tensor([0.5438, 0.4562], device='cuda:0')
percent tensor([0.5736, 0.4264], device='cuda:0')
percent tensor([0.5709, 0.4291], device='cuda:0')
percent tensor([0.6266, 0.3734], device='cuda:0')
percent tensor([0.6796, 0.3204], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 113 | Batch_idx: 0 |  Loss: (0.2485) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 113 | Batch_idx: 10 |  Loss: (0.2138) |  Loss2: (0.0000) | Acc: (92.00%) (1302/1408)
Epoch: 113 | Batch_idx: 20 |  Loss: (0.1977) |  Loss2: (0.0000) | Acc: (93.00%) (2513/2688)
Epoch: 113 | Batch_idx: 30 |  Loss: (0.1943) |  Loss2: (0.0000) | Acc: (93.00%) (3714/3968)
Epoch: 113 | Batch_idx: 40 |  Loss: (0.1942) |  Loss2: (0.0000) | Acc: (93.00%) (4906/5248)
Epoch: 113 | Batch_idx: 50 |  Loss: (0.1948) |  Loss2: (0.0000) | Acc: (93.00%) (6101/6528)
Epoch: 113 | Batch_idx: 60 |  Loss: (0.1929) |  Loss2: (0.0000) | Acc: (93.00%) (7302/7808)
Epoch: 113 | Batch_idx: 70 |  Loss: (0.1927) |  Loss2: (0.0000) | Acc: (93.00%) (8496/9088)
Epoch: 113 | Batch_idx: 80 |  Loss: (0.1952) |  Loss2: (0.0000) | Acc: (93.00%) (9683/10368)
Epoch: 113 | Batch_idx: 90 |  Loss: (0.1964) |  Loss2: (0.0000) | Acc: (93.00%) (10871/11648)
Epoch: 113 | Batch_idx: 100 |  Loss: (0.1990) |  Loss2: (0.0000) | Acc: (93.00%) (12057/12928)
Epoch: 113 | Batch_idx: 110 |  Loss: (0.2002) |  Loss2: (0.0000) | Acc: (93.00%) (13249/14208)
Epoch: 113 | Batch_idx: 120 |  Loss: (0.2004) |  Loss2: (0.0000) | Acc: (93.00%) (14444/15488)
Epoch: 113 | Batch_idx: 130 |  Loss: (0.1990) |  Loss2: (0.0000) | Acc: (93.00%) (15638/16768)
Epoch: 113 | Batch_idx: 140 |  Loss: (0.2007) |  Loss2: (0.0000) | Acc: (93.00%) (16824/18048)
Epoch: 113 | Batch_idx: 150 |  Loss: (0.2001) |  Loss2: (0.0000) | Acc: (93.00%) (18026/19328)
Epoch: 113 | Batch_idx: 160 |  Loss: (0.1995) |  Loss2: (0.0000) | Acc: (93.00%) (19223/20608)
Epoch: 113 | Batch_idx: 170 |  Loss: (0.1987) |  Loss2: (0.0000) | Acc: (93.00%) (20430/21888)
Epoch: 113 | Batch_idx: 180 |  Loss: (0.1985) |  Loss2: (0.0000) | Acc: (93.00%) (21621/23168)
Epoch: 113 | Batch_idx: 190 |  Loss: (0.1987) |  Loss2: (0.0000) | Acc: (93.00%) (22809/24448)
Epoch: 113 | Batch_idx: 200 |  Loss: (0.1983) |  Loss2: (0.0000) | Acc: (93.00%) (24000/25728)
Epoch: 113 | Batch_idx: 210 |  Loss: (0.1984) |  Loss2: (0.0000) | Acc: (93.00%) (25192/27008)
Epoch: 113 | Batch_idx: 220 |  Loss: (0.1990) |  Loss2: (0.0000) | Acc: (93.00%) (26374/28288)
Epoch: 113 | Batch_idx: 230 |  Loss: (0.2003) |  Loss2: (0.0000) | Acc: (93.00%) (27545/29568)
Epoch: 113 | Batch_idx: 240 |  Loss: (0.1992) |  Loss2: (0.0000) | Acc: (93.00%) (28748/30848)
Epoch: 113 | Batch_idx: 250 |  Loss: (0.1991) |  Loss2: (0.0000) | Acc: (93.00%) (29932/32128)
Epoch: 113 | Batch_idx: 260 |  Loss: (0.1985) |  Loss2: (0.0000) | Acc: (93.00%) (31133/33408)
Epoch: 113 | Batch_idx: 270 |  Loss: (0.1995) |  Loss2: (0.0000) | Acc: (93.00%) (32315/34688)
Epoch: 113 | Batch_idx: 280 |  Loss: (0.1992) |  Loss2: (0.0000) | Acc: (93.00%) (33512/35968)
Epoch: 113 | Batch_idx: 290 |  Loss: (0.1998) |  Loss2: (0.0000) | Acc: (93.00%) (34695/37248)
Epoch: 113 | Batch_idx: 300 |  Loss: (0.1997) |  Loss2: (0.0000) | Acc: (93.00%) (35890/38528)
Epoch: 113 | Batch_idx: 310 |  Loss: (0.1997) |  Loss2: (0.0000) | Acc: (93.00%) (37076/39808)
Epoch: 113 | Batch_idx: 320 |  Loss: (0.1992) |  Loss2: (0.0000) | Acc: (93.00%) (38275/41088)
Epoch: 113 | Batch_idx: 330 |  Loss: (0.2003) |  Loss2: (0.0000) | Acc: (93.00%) (39450/42368)
Epoch: 113 | Batch_idx: 340 |  Loss: (0.2002) |  Loss2: (0.0000) | Acc: (93.00%) (40644/43648)
Epoch: 113 | Batch_idx: 350 |  Loss: (0.2005) |  Loss2: (0.0000) | Acc: (93.00%) (41840/44928)
Epoch: 113 | Batch_idx: 360 |  Loss: (0.2008) |  Loss2: (0.0000) | Acc: (93.00%) (43024/46208)
Epoch: 113 | Batch_idx: 370 |  Loss: (0.2007) |  Loss2: (0.0000) | Acc: (93.00%) (44215/47488)
Epoch: 113 | Batch_idx: 380 |  Loss: (0.2006) |  Loss2: (0.0000) | Acc: (93.00%) (45406/48768)
Epoch: 113 | Batch_idx: 390 |  Loss: (0.2002) |  Loss2: (0.0000) | Acc: (93.00%) (46562/50000)
# TEST : Loss: (0.4492) | Acc: (86.00%) (8654/10000)
percent tensor([0.5237, 0.4763], device='cuda:0')
percent tensor([0.4682, 0.5318], device='cuda:0')
percent tensor([0.5430, 0.4570], device='cuda:0')
percent tensor([0.5736, 0.4264], device='cuda:0')
percent tensor([0.5715, 0.4285], device='cuda:0')
percent tensor([0.6229, 0.3771], device='cuda:0')
percent tensor([0.6747, 0.3253], device='cuda:0')
percent tensor([0.9988, 0.0012], device='cuda:0')
Epoch: 114 | Batch_idx: 0 |  Loss: (0.1896) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 114 | Batch_idx: 10 |  Loss: (0.1614) |  Loss2: (0.0000) | Acc: (94.00%) (1328/1408)
Epoch: 114 | Batch_idx: 20 |  Loss: (0.1650) |  Loss2: (0.0000) | Acc: (94.00%) (2535/2688)
Epoch: 114 | Batch_idx: 30 |  Loss: (0.1725) |  Loss2: (0.0000) | Acc: (94.00%) (3731/3968)
Epoch: 114 | Batch_idx: 40 |  Loss: (0.1752) |  Loss2: (0.0000) | Acc: (94.00%) (4934/5248)
Epoch: 114 | Batch_idx: 50 |  Loss: (0.1755) |  Loss2: (0.0000) | Acc: (93.00%) (6132/6528)
Epoch: 114 | Batch_idx: 60 |  Loss: (0.1778) |  Loss2: (0.0000) | Acc: (93.00%) (7328/7808)
Epoch: 114 | Batch_idx: 70 |  Loss: (0.1805) |  Loss2: (0.0000) | Acc: (93.00%) (8530/9088)
Epoch: 114 | Batch_idx: 80 |  Loss: (0.1807) |  Loss2: (0.0000) | Acc: (93.00%) (9739/10368)
Epoch: 114 | Batch_idx: 90 |  Loss: (0.1821) |  Loss2: (0.0000) | Acc: (93.00%) (10934/11648)
Epoch: 114 | Batch_idx: 100 |  Loss: (0.1840) |  Loss2: (0.0000) | Acc: (93.00%) (12137/12928)
Epoch: 114 | Batch_idx: 110 |  Loss: (0.1845) |  Loss2: (0.0000) | Acc: (93.00%) (13340/14208)
Epoch: 114 | Batch_idx: 120 |  Loss: (0.1844) |  Loss2: (0.0000) | Acc: (93.00%) (14544/15488)
Epoch: 114 | Batch_idx: 130 |  Loss: (0.1853) |  Loss2: (0.0000) | Acc: (93.00%) (15735/16768)
Epoch: 114 | Batch_idx: 140 |  Loss: (0.1875) |  Loss2: (0.0000) | Acc: (93.00%) (16917/18048)
Epoch: 114 | Batch_idx: 150 |  Loss: (0.1880) |  Loss2: (0.0000) | Acc: (93.00%) (18119/19328)
Epoch: 114 | Batch_idx: 160 |  Loss: (0.1882) |  Loss2: (0.0000) | Acc: (93.00%) (19310/20608)
Epoch: 114 | Batch_idx: 170 |  Loss: (0.1892) |  Loss2: (0.0000) | Acc: (93.00%) (20494/21888)
Epoch: 114 | Batch_idx: 180 |  Loss: (0.1904) |  Loss2: (0.0000) | Acc: (93.00%) (21682/23168)
Epoch: 114 | Batch_idx: 190 |  Loss: (0.1908) |  Loss2: (0.0000) | Acc: (93.00%) (22877/24448)
Epoch: 114 | Batch_idx: 200 |  Loss: (0.1912) |  Loss2: (0.0000) | Acc: (93.00%) (24066/25728)
Epoch: 114 | Batch_idx: 210 |  Loss: (0.1911) |  Loss2: (0.0000) | Acc: (93.00%) (25259/27008)
Epoch: 114 | Batch_idx: 220 |  Loss: (0.1903) |  Loss2: (0.0000) | Acc: (93.00%) (26465/28288)
Epoch: 114 | Batch_idx: 230 |  Loss: (0.1898) |  Loss2: (0.0000) | Acc: (93.00%) (27666/29568)
Epoch: 114 | Batch_idx: 240 |  Loss: (0.1898) |  Loss2: (0.0000) | Acc: (93.00%) (28865/30848)
Epoch: 114 | Batch_idx: 250 |  Loss: (0.1896) |  Loss2: (0.0000) | Acc: (93.00%) (30071/32128)
Epoch: 114 | Batch_idx: 260 |  Loss: (0.1888) |  Loss2: (0.0000) | Acc: (93.00%) (31278/33408)
Epoch: 114 | Batch_idx: 270 |  Loss: (0.1882) |  Loss2: (0.0000) | Acc: (93.00%) (32481/34688)
Epoch: 114 | Batch_idx: 280 |  Loss: (0.1879) |  Loss2: (0.0000) | Acc: (93.00%) (33682/35968)
Epoch: 114 | Batch_idx: 290 |  Loss: (0.1885) |  Loss2: (0.0000) | Acc: (93.00%) (34871/37248)
Epoch: 114 | Batch_idx: 300 |  Loss: (0.1882) |  Loss2: (0.0000) | Acc: (93.00%) (36076/38528)
Epoch: 114 | Batch_idx: 310 |  Loss: (0.1895) |  Loss2: (0.0000) | Acc: (93.00%) (37256/39808)
Epoch: 114 | Batch_idx: 320 |  Loss: (0.1899) |  Loss2: (0.0000) | Acc: (93.00%) (38445/41088)
Epoch: 114 | Batch_idx: 330 |  Loss: (0.1894) |  Loss2: (0.0000) | Acc: (93.00%) (39651/42368)
Epoch: 114 | Batch_idx: 340 |  Loss: (0.1897) |  Loss2: (0.0000) | Acc: (93.00%) (40839/43648)
Epoch: 114 | Batch_idx: 350 |  Loss: (0.1894) |  Loss2: (0.0000) | Acc: (93.00%) (42046/44928)
Epoch: 114 | Batch_idx: 360 |  Loss: (0.1900) |  Loss2: (0.0000) | Acc: (93.00%) (43226/46208)
Epoch: 114 | Batch_idx: 370 |  Loss: (0.1903) |  Loss2: (0.0000) | Acc: (93.00%) (44423/47488)
Epoch: 114 | Batch_idx: 380 |  Loss: (0.1903) |  Loss2: (0.0000) | Acc: (93.00%) (45617/48768)
Epoch: 114 | Batch_idx: 390 |  Loss: (0.1909) |  Loss2: (0.0000) | Acc: (93.00%) (46755/50000)
# TEST : Loss: (0.4266) | Acc: (86.00%) (8663/10000)
percent tensor([0.5237, 0.4763], device='cuda:0')
percent tensor([0.4677, 0.5323], device='cuda:0')
percent tensor([0.5430, 0.4570], device='cuda:0')
percent tensor([0.5729, 0.4271], device='cuda:0')
percent tensor([0.5719, 0.4281], device='cuda:0')
percent tensor([0.6244, 0.3756], device='cuda:0')
percent tensor([0.6788, 0.3212], device='cuda:0')
percent tensor([0.9990, 0.0010], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 115 | Batch_idx: 0 |  Loss: (0.1481) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 115 | Batch_idx: 10 |  Loss: (0.1732) |  Loss2: (0.0000) | Acc: (93.00%) (1318/1408)
Epoch: 115 | Batch_idx: 20 |  Loss: (0.1939) |  Loss2: (0.0000) | Acc: (93.00%) (2511/2688)
Epoch: 115 | Batch_idx: 30 |  Loss: (0.2212) |  Loss2: (0.0000) | Acc: (92.00%) (3657/3968)
Epoch: 115 | Batch_idx: 40 |  Loss: (0.2327) |  Loss2: (0.0000) | Acc: (91.00%) (4811/5248)
Epoch: 115 | Batch_idx: 50 |  Loss: (0.2360) |  Loss2: (0.0000) | Acc: (91.00%) (5982/6528)
Epoch: 115 | Batch_idx: 60 |  Loss: (0.2391) |  Loss2: (0.0000) | Acc: (91.00%) (7141/7808)
Epoch: 115 | Batch_idx: 70 |  Loss: (0.2390) |  Loss2: (0.0000) | Acc: (91.00%) (8310/9088)
Epoch: 115 | Batch_idx: 80 |  Loss: (0.2374) |  Loss2: (0.0000) | Acc: (91.00%) (9495/10368)
Epoch: 115 | Batch_idx: 90 |  Loss: (0.2386) |  Loss2: (0.0000) | Acc: (91.00%) (10671/11648)
Epoch: 115 | Batch_idx: 100 |  Loss: (0.2401) |  Loss2: (0.0000) | Acc: (91.00%) (11827/12928)
Epoch: 115 | Batch_idx: 110 |  Loss: (0.2429) |  Loss2: (0.0000) | Acc: (91.00%) (12991/14208)
Epoch: 115 | Batch_idx: 120 |  Loss: (0.2428) |  Loss2: (0.0000) | Acc: (91.00%) (14164/15488)
Epoch: 115 | Batch_idx: 130 |  Loss: (0.2422) |  Loss2: (0.0000) | Acc: (91.00%) (15338/16768)
Epoch: 115 | Batch_idx: 140 |  Loss: (0.2393) |  Loss2: (0.0000) | Acc: (91.00%) (16518/18048)
Epoch: 115 | Batch_idx: 150 |  Loss: (0.2390) |  Loss2: (0.0000) | Acc: (91.00%) (17688/19328)
Epoch: 115 | Batch_idx: 160 |  Loss: (0.2402) |  Loss2: (0.0000) | Acc: (91.00%) (18856/20608)
Epoch: 115 | Batch_idx: 170 |  Loss: (0.2394) |  Loss2: (0.0000) | Acc: (91.00%) (20035/21888)
Epoch: 115 | Batch_idx: 180 |  Loss: (0.2396) |  Loss2: (0.0000) | Acc: (91.00%) (21206/23168)
Epoch: 115 | Batch_idx: 190 |  Loss: (0.2403) |  Loss2: (0.0000) | Acc: (91.00%) (22361/24448)
Epoch: 115 | Batch_idx: 200 |  Loss: (0.2394) |  Loss2: (0.0000) | Acc: (91.00%) (23536/25728)
Epoch: 115 | Batch_idx: 210 |  Loss: (0.2392) |  Loss2: (0.0000) | Acc: (91.00%) (24704/27008)
Epoch: 115 | Batch_idx: 220 |  Loss: (0.2385) |  Loss2: (0.0000) | Acc: (91.00%) (25891/28288)
Epoch: 115 | Batch_idx: 230 |  Loss: (0.2366) |  Loss2: (0.0000) | Acc: (91.00%) (27081/29568)
Epoch: 115 | Batch_idx: 240 |  Loss: (0.2373) |  Loss2: (0.0000) | Acc: (91.00%) (28249/30848)
Epoch: 115 | Batch_idx: 250 |  Loss: (0.2365) |  Loss2: (0.0000) | Acc: (91.00%) (29438/32128)
Epoch: 115 | Batch_idx: 260 |  Loss: (0.2362) |  Loss2: (0.0000) | Acc: (91.00%) (30612/33408)
Epoch: 115 | Batch_idx: 270 |  Loss: (0.2368) |  Loss2: (0.0000) | Acc: (91.00%) (31784/34688)
Epoch: 115 | Batch_idx: 280 |  Loss: (0.2371) |  Loss2: (0.0000) | Acc: (91.00%) (32959/35968)
Epoch: 115 | Batch_idx: 290 |  Loss: (0.2358) |  Loss2: (0.0000) | Acc: (91.00%) (34153/37248)
Epoch: 115 | Batch_idx: 300 |  Loss: (0.2354) |  Loss2: (0.0000) | Acc: (91.00%) (35338/38528)
Epoch: 115 | Batch_idx: 310 |  Loss: (0.2345) |  Loss2: (0.0000) | Acc: (91.00%) (36523/39808)
Epoch: 115 | Batch_idx: 320 |  Loss: (0.2346) |  Loss2: (0.0000) | Acc: (91.00%) (37695/41088)
Epoch: 115 | Batch_idx: 330 |  Loss: (0.2356) |  Loss2: (0.0000) | Acc: (91.00%) (38852/42368)
Epoch: 115 | Batch_idx: 340 |  Loss: (0.2357) |  Loss2: (0.0000) | Acc: (91.00%) (40029/43648)
Epoch: 115 | Batch_idx: 350 |  Loss: (0.2353) |  Loss2: (0.0000) | Acc: (91.00%) (41216/44928)
Epoch: 115 | Batch_idx: 360 |  Loss: (0.2351) |  Loss2: (0.0000) | Acc: (91.00%) (42401/46208)
Epoch: 115 | Batch_idx: 370 |  Loss: (0.2340) |  Loss2: (0.0000) | Acc: (91.00%) (43587/47488)
Epoch: 115 | Batch_idx: 380 |  Loss: (0.2336) |  Loss2: (0.0000) | Acc: (91.00%) (44780/48768)
Epoch: 115 | Batch_idx: 390 |  Loss: (0.2326) |  Loss2: (0.0000) | Acc: (91.00%) (45923/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_115.pth.tar'
# TEST : Loss: (0.4277) | Acc: (87.00%) (8702/10000)
percent tensor([0.5177, 0.4823], device='cuda:0')
percent tensor([0.4673, 0.5327], device='cuda:0')
percent tensor([0.5584, 0.4416], device='cuda:0')
percent tensor([0.5865, 0.4135], device='cuda:0')
percent tensor([0.5692, 0.4308], device='cuda:0')
percent tensor([0.6599, 0.3401], device='cuda:0')
percent tensor([0.6726, 0.3274], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
Epoch: 116 | Batch_idx: 0 |  Loss: (0.2208) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 116 | Batch_idx: 10 |  Loss: (0.2279) |  Loss2: (0.0000) | Acc: (92.00%) (1296/1408)
Epoch: 116 | Batch_idx: 20 |  Loss: (0.2251) |  Loss2: (0.0000) | Acc: (92.00%) (2479/2688)
Epoch: 116 | Batch_idx: 30 |  Loss: (0.2252) |  Loss2: (0.0000) | Acc: (92.00%) (3668/3968)
Epoch: 116 | Batch_idx: 40 |  Loss: (0.2165) |  Loss2: (0.0000) | Acc: (92.00%) (4864/5248)
Epoch: 116 | Batch_idx: 50 |  Loss: (0.2104) |  Loss2: (0.0000) | Acc: (92.00%) (6060/6528)
Epoch: 116 | Batch_idx: 60 |  Loss: (0.2091) |  Loss2: (0.0000) | Acc: (92.00%) (7246/7808)
Epoch: 116 | Batch_idx: 70 |  Loss: (0.2084) |  Loss2: (0.0000) | Acc: (92.00%) (8442/9088)
Epoch: 116 | Batch_idx: 80 |  Loss: (0.2067) |  Loss2: (0.0000) | Acc: (92.00%) (9640/10368)
Epoch: 116 | Batch_idx: 90 |  Loss: (0.2059) |  Loss2: (0.0000) | Acc: (92.00%) (10828/11648)
Epoch: 116 | Batch_idx: 100 |  Loss: (0.2063) |  Loss2: (0.0000) | Acc: (92.00%) (12011/12928)
Epoch: 116 | Batch_idx: 110 |  Loss: (0.2044) |  Loss2: (0.0000) | Acc: (93.00%) (13216/14208)
Epoch: 116 | Batch_idx: 120 |  Loss: (0.2086) |  Loss2: (0.0000) | Acc: (92.00%) (14383/15488)
Epoch: 116 | Batch_idx: 130 |  Loss: (0.2089) |  Loss2: (0.0000) | Acc: (92.00%) (15567/16768)
Epoch: 116 | Batch_idx: 140 |  Loss: (0.2096) |  Loss2: (0.0000) | Acc: (92.00%) (16750/18048)
Epoch: 116 | Batch_idx: 150 |  Loss: (0.2099) |  Loss2: (0.0000) | Acc: (92.00%) (17937/19328)
Epoch: 116 | Batch_idx: 160 |  Loss: (0.2100) |  Loss2: (0.0000) | Acc: (92.00%) (19135/20608)
Epoch: 116 | Batch_idx: 170 |  Loss: (0.2112) |  Loss2: (0.0000) | Acc: (92.00%) (20318/21888)
Epoch: 116 | Batch_idx: 180 |  Loss: (0.2101) |  Loss2: (0.0000) | Acc: (92.00%) (21520/23168)
Epoch: 116 | Batch_idx: 190 |  Loss: (0.2118) |  Loss2: (0.0000) | Acc: (92.00%) (22688/24448)
Epoch: 116 | Batch_idx: 200 |  Loss: (0.2093) |  Loss2: (0.0000) | Acc: (92.00%) (23898/25728)
Epoch: 116 | Batch_idx: 210 |  Loss: (0.2072) |  Loss2: (0.0000) | Acc: (92.00%) (25099/27008)
Epoch: 116 | Batch_idx: 220 |  Loss: (0.2081) |  Loss2: (0.0000) | Acc: (92.00%) (26286/28288)
Epoch: 116 | Batch_idx: 230 |  Loss: (0.2076) |  Loss2: (0.0000) | Acc: (92.00%) (27491/29568)
Epoch: 116 | Batch_idx: 240 |  Loss: (0.2071) |  Loss2: (0.0000) | Acc: (92.00%) (28686/30848)
Epoch: 116 | Batch_idx: 250 |  Loss: (0.2074) |  Loss2: (0.0000) | Acc: (92.00%) (29866/32128)
Epoch: 116 | Batch_idx: 260 |  Loss: (0.2087) |  Loss2: (0.0000) | Acc: (92.00%) (31041/33408)
Epoch: 116 | Batch_idx: 270 |  Loss: (0.2099) |  Loss2: (0.0000) | Acc: (92.00%) (32229/34688)
Epoch: 116 | Batch_idx: 280 |  Loss: (0.2102) |  Loss2: (0.0000) | Acc: (92.00%) (33420/35968)
Epoch: 116 | Batch_idx: 290 |  Loss: (0.2097) |  Loss2: (0.0000) | Acc: (92.00%) (34605/37248)
Epoch: 116 | Batch_idx: 300 |  Loss: (0.2092) |  Loss2: (0.0000) | Acc: (92.00%) (35798/38528)
Epoch: 116 | Batch_idx: 310 |  Loss: (0.2088) |  Loss2: (0.0000) | Acc: (92.00%) (36986/39808)
Epoch: 116 | Batch_idx: 320 |  Loss: (0.2092) |  Loss2: (0.0000) | Acc: (92.00%) (38167/41088)
Epoch: 116 | Batch_idx: 330 |  Loss: (0.2085) |  Loss2: (0.0000) | Acc: (92.00%) (39372/42368)
Epoch: 116 | Batch_idx: 340 |  Loss: (0.2085) |  Loss2: (0.0000) | Acc: (92.00%) (40559/43648)
Epoch: 116 | Batch_idx: 350 |  Loss: (0.2076) |  Loss2: (0.0000) | Acc: (92.00%) (41763/44928)
Epoch: 116 | Batch_idx: 360 |  Loss: (0.2080) |  Loss2: (0.0000) | Acc: (92.00%) (42953/46208)
Epoch: 116 | Batch_idx: 370 |  Loss: (0.2081) |  Loss2: (0.0000) | Acc: (92.00%) (44151/47488)
Epoch: 116 | Batch_idx: 380 |  Loss: (0.2083) |  Loss2: (0.0000) | Acc: (92.00%) (45330/48768)
Epoch: 116 | Batch_idx: 390 |  Loss: (0.2077) |  Loss2: (0.0000) | Acc: (92.00%) (46488/50000)
# TEST : Loss: (0.4076) | Acc: (87.00%) (8749/10000)
percent tensor([0.5173, 0.4827], device='cuda:0')
percent tensor([0.4657, 0.5343], device='cuda:0')
percent tensor([0.5612, 0.4388], device='cuda:0')
percent tensor([0.5906, 0.4094], device='cuda:0')
percent tensor([0.5740, 0.4260], device='cuda:0')
percent tensor([0.6662, 0.3338], device='cuda:0')
percent tensor([0.6822, 0.3178], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
Epoch: 117 | Batch_idx: 0 |  Loss: (0.2543) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 117 | Batch_idx: 10 |  Loss: (0.1974) |  Loss2: (0.0000) | Acc: (93.00%) (1315/1408)
Epoch: 117 | Batch_idx: 20 |  Loss: (0.1887) |  Loss2: (0.0000) | Acc: (93.00%) (2515/2688)
Epoch: 117 | Batch_idx: 30 |  Loss: (0.1895) |  Loss2: (0.0000) | Acc: (93.00%) (3716/3968)
Epoch: 117 | Batch_idx: 40 |  Loss: (0.1862) |  Loss2: (0.0000) | Acc: (93.00%) (4919/5248)
Epoch: 117 | Batch_idx: 50 |  Loss: (0.1921) |  Loss2: (0.0000) | Acc: (93.00%) (6101/6528)
Epoch: 117 | Batch_idx: 60 |  Loss: (0.1939) |  Loss2: (0.0000) | Acc: (93.00%) (7286/7808)
Epoch: 117 | Batch_idx: 70 |  Loss: (0.1941) |  Loss2: (0.0000) | Acc: (93.00%) (8488/9088)
Epoch: 117 | Batch_idx: 80 |  Loss: (0.1905) |  Loss2: (0.0000) | Acc: (93.00%) (9695/10368)
Epoch: 117 | Batch_idx: 90 |  Loss: (0.1910) |  Loss2: (0.0000) | Acc: (93.00%) (10880/11648)
Epoch: 117 | Batch_idx: 100 |  Loss: (0.1925) |  Loss2: (0.0000) | Acc: (93.00%) (12062/12928)
Epoch: 117 | Batch_idx: 110 |  Loss: (0.1909) |  Loss2: (0.0000) | Acc: (93.00%) (13264/14208)
Epoch: 117 | Batch_idx: 120 |  Loss: (0.1909) |  Loss2: (0.0000) | Acc: (93.00%) (14457/15488)
Epoch: 117 | Batch_idx: 130 |  Loss: (0.1930) |  Loss2: (0.0000) | Acc: (93.00%) (15641/16768)
Epoch: 117 | Batch_idx: 140 |  Loss: (0.1928) |  Loss2: (0.0000) | Acc: (93.00%) (16827/18048)
Epoch: 117 | Batch_idx: 150 |  Loss: (0.1920) |  Loss2: (0.0000) | Acc: (93.00%) (18029/19328)
Epoch: 117 | Batch_idx: 160 |  Loss: (0.1922) |  Loss2: (0.0000) | Acc: (93.00%) (19225/20608)
Epoch: 117 | Batch_idx: 170 |  Loss: (0.1931) |  Loss2: (0.0000) | Acc: (93.00%) (20415/21888)
Epoch: 117 | Batch_idx: 180 |  Loss: (0.1935) |  Loss2: (0.0000) | Acc: (93.00%) (21598/23168)
Epoch: 117 | Batch_idx: 190 |  Loss: (0.1950) |  Loss2: (0.0000) | Acc: (93.00%) (22788/24448)
Epoch: 117 | Batch_idx: 200 |  Loss: (0.1948) |  Loss2: (0.0000) | Acc: (93.00%) (23992/25728)
Epoch: 117 | Batch_idx: 210 |  Loss: (0.1937) |  Loss2: (0.0000) | Acc: (93.00%) (25189/27008)
Epoch: 117 | Batch_idx: 220 |  Loss: (0.1929) |  Loss2: (0.0000) | Acc: (93.00%) (26388/28288)
Epoch: 117 | Batch_idx: 230 |  Loss: (0.1922) |  Loss2: (0.0000) | Acc: (93.00%) (27583/29568)
Epoch: 117 | Batch_idx: 240 |  Loss: (0.1927) |  Loss2: (0.0000) | Acc: (93.00%) (28785/30848)
Epoch: 117 | Batch_idx: 250 |  Loss: (0.1923) |  Loss2: (0.0000) | Acc: (93.00%) (29987/32128)
Epoch: 117 | Batch_idx: 260 |  Loss: (0.1930) |  Loss2: (0.0000) | Acc: (93.00%) (31183/33408)
Epoch: 117 | Batch_idx: 270 |  Loss: (0.1921) |  Loss2: (0.0000) | Acc: (93.00%) (32389/34688)
Epoch: 117 | Batch_idx: 280 |  Loss: (0.1923) |  Loss2: (0.0000) | Acc: (93.00%) (33588/35968)
Epoch: 117 | Batch_idx: 290 |  Loss: (0.1923) |  Loss2: (0.0000) | Acc: (93.00%) (34772/37248)
Epoch: 117 | Batch_idx: 300 |  Loss: (0.1930) |  Loss2: (0.0000) | Acc: (93.00%) (35962/38528)
Epoch: 117 | Batch_idx: 310 |  Loss: (0.1935) |  Loss2: (0.0000) | Acc: (93.00%) (37149/39808)
Epoch: 117 | Batch_idx: 320 |  Loss: (0.1939) |  Loss2: (0.0000) | Acc: (93.00%) (38345/41088)
Epoch: 117 | Batch_idx: 330 |  Loss: (0.1936) |  Loss2: (0.0000) | Acc: (93.00%) (39549/42368)
Epoch: 117 | Batch_idx: 340 |  Loss: (0.1933) |  Loss2: (0.0000) | Acc: (93.00%) (40748/43648)
Epoch: 117 | Batch_idx: 350 |  Loss: (0.1928) |  Loss2: (0.0000) | Acc: (93.00%) (41953/44928)
Epoch: 117 | Batch_idx: 360 |  Loss: (0.1931) |  Loss2: (0.0000) | Acc: (93.00%) (43140/46208)
Epoch: 117 | Batch_idx: 370 |  Loss: (0.1931) |  Loss2: (0.0000) | Acc: (93.00%) (44341/47488)
Epoch: 117 | Batch_idx: 380 |  Loss: (0.1938) |  Loss2: (0.0000) | Acc: (93.00%) (45520/48768)
Epoch: 117 | Batch_idx: 390 |  Loss: (0.1935) |  Loss2: (0.0000) | Acc: (93.00%) (46673/50000)
# TEST : Loss: (0.3944) | Acc: (87.00%) (8782/10000)
percent tensor([0.5169, 0.4831], device='cuda:0')
percent tensor([0.4648, 0.5352], device='cuda:0')
percent tensor([0.5621, 0.4379], device='cuda:0')
percent tensor([0.5944, 0.4056], device='cuda:0')
percent tensor([0.5752, 0.4248], device='cuda:0')
percent tensor([0.6732, 0.3268], device='cuda:0')
percent tensor([0.6895, 0.3105], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
Epoch: 118 | Batch_idx: 0 |  Loss: (0.1776) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 118 | Batch_idx: 10 |  Loss: (0.1938) |  Loss2: (0.0000) | Acc: (93.00%) (1316/1408)
Epoch: 118 | Batch_idx: 20 |  Loss: (0.1863) |  Loss2: (0.0000) | Acc: (93.00%) (2515/2688)
Epoch: 118 | Batch_idx: 30 |  Loss: (0.1868) |  Loss2: (0.0000) | Acc: (93.00%) (3717/3968)
Epoch: 118 | Batch_idx: 40 |  Loss: (0.1904) |  Loss2: (0.0000) | Acc: (93.00%) (4907/5248)
Epoch: 118 | Batch_idx: 50 |  Loss: (0.1888) |  Loss2: (0.0000) | Acc: (93.00%) (6108/6528)
Epoch: 118 | Batch_idx: 60 |  Loss: (0.1852) |  Loss2: (0.0000) | Acc: (93.00%) (7314/7808)
Epoch: 118 | Batch_idx: 70 |  Loss: (0.1832) |  Loss2: (0.0000) | Acc: (93.00%) (8524/9088)
Epoch: 118 | Batch_idx: 80 |  Loss: (0.1858) |  Loss2: (0.0000) | Acc: (93.00%) (9715/10368)
Epoch: 118 | Batch_idx: 90 |  Loss: (0.1912) |  Loss2: (0.0000) | Acc: (93.00%) (10887/11648)
Epoch: 118 | Batch_idx: 100 |  Loss: (0.1902) |  Loss2: (0.0000) | Acc: (93.00%) (12088/12928)
Epoch: 118 | Batch_idx: 110 |  Loss: (0.1910) |  Loss2: (0.0000) | Acc: (93.00%) (13289/14208)
Epoch: 118 | Batch_idx: 120 |  Loss: (0.1906) |  Loss2: (0.0000) | Acc: (93.00%) (14489/15488)
Epoch: 118 | Batch_idx: 130 |  Loss: (0.1909) |  Loss2: (0.0000) | Acc: (93.00%) (15684/16768)
Epoch: 118 | Batch_idx: 140 |  Loss: (0.1922) |  Loss2: (0.0000) | Acc: (93.00%) (16871/18048)
Epoch: 118 | Batch_idx: 150 |  Loss: (0.1926) |  Loss2: (0.0000) | Acc: (93.00%) (18066/19328)
Epoch: 118 | Batch_idx: 160 |  Loss: (0.1926) |  Loss2: (0.0000) | Acc: (93.00%) (19264/20608)
Epoch: 118 | Batch_idx: 170 |  Loss: (0.1933) |  Loss2: (0.0000) | Acc: (93.00%) (20457/21888)
Epoch: 118 | Batch_idx: 180 |  Loss: (0.1931) |  Loss2: (0.0000) | Acc: (93.00%) (21657/23168)
Epoch: 118 | Batch_idx: 190 |  Loss: (0.1930) |  Loss2: (0.0000) | Acc: (93.00%) (22851/24448)
Epoch: 118 | Batch_idx: 200 |  Loss: (0.1920) |  Loss2: (0.0000) | Acc: (93.00%) (24054/25728)
Epoch: 118 | Batch_idx: 210 |  Loss: (0.1905) |  Loss2: (0.0000) | Acc: (93.00%) (25266/27008)
Epoch: 118 | Batch_idx: 220 |  Loss: (0.1902) |  Loss2: (0.0000) | Acc: (93.00%) (26462/28288)
Epoch: 118 | Batch_idx: 230 |  Loss: (0.1898) |  Loss2: (0.0000) | Acc: (93.00%) (27667/29568)
Epoch: 118 | Batch_idx: 240 |  Loss: (0.1903) |  Loss2: (0.0000) | Acc: (93.00%) (28854/30848)
Epoch: 118 | Batch_idx: 250 |  Loss: (0.1900) |  Loss2: (0.0000) | Acc: (93.00%) (30054/32128)
Epoch: 118 | Batch_idx: 260 |  Loss: (0.1896) |  Loss2: (0.0000) | Acc: (93.00%) (31257/33408)
Epoch: 118 | Batch_idx: 270 |  Loss: (0.1885) |  Loss2: (0.0000) | Acc: (93.00%) (32475/34688)
Epoch: 118 | Batch_idx: 280 |  Loss: (0.1872) |  Loss2: (0.0000) | Acc: (93.00%) (33688/35968)
Epoch: 118 | Batch_idx: 290 |  Loss: (0.1862) |  Loss2: (0.0000) | Acc: (93.00%) (34902/37248)
Epoch: 118 | Batch_idx: 300 |  Loss: (0.1863) |  Loss2: (0.0000) | Acc: (93.00%) (36103/38528)
Epoch: 118 | Batch_idx: 310 |  Loss: (0.1868) |  Loss2: (0.0000) | Acc: (93.00%) (37297/39808)
Epoch: 118 | Batch_idx: 320 |  Loss: (0.1873) |  Loss2: (0.0000) | Acc: (93.00%) (38484/41088)
Epoch: 118 | Batch_idx: 330 |  Loss: (0.1875) |  Loss2: (0.0000) | Acc: (93.00%) (39680/42368)
Epoch: 118 | Batch_idx: 340 |  Loss: (0.1879) |  Loss2: (0.0000) | Acc: (93.00%) (40868/43648)
Epoch: 118 | Batch_idx: 350 |  Loss: (0.1877) |  Loss2: (0.0000) | Acc: (93.00%) (42071/44928)
Epoch: 118 | Batch_idx: 360 |  Loss: (0.1871) |  Loss2: (0.0000) | Acc: (93.00%) (43278/46208)
Epoch: 118 | Batch_idx: 370 |  Loss: (0.1873) |  Loss2: (0.0000) | Acc: (93.00%) (44470/47488)
Epoch: 118 | Batch_idx: 380 |  Loss: (0.1870) |  Loss2: (0.0000) | Acc: (93.00%) (45673/48768)
Epoch: 118 | Batch_idx: 390 |  Loss: (0.1871) |  Loss2: (0.0000) | Acc: (93.00%) (46822/50000)
# TEST : Loss: (0.3905) | Acc: (88.00%) (8805/10000)
percent tensor([0.5175, 0.4825], device='cuda:0')
percent tensor([0.4652, 0.5348], device='cuda:0')
percent tensor([0.5614, 0.4386], device='cuda:0')
percent tensor([0.5919, 0.4081], device='cuda:0')
percent tensor([0.5763, 0.4237], device='cuda:0')
percent tensor([0.6707, 0.3293], device='cuda:0')
percent tensor([0.6921, 0.3079], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
Epoch: 119 | Batch_idx: 0 |  Loss: (0.1351) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 119 | Batch_idx: 10 |  Loss: (0.1720) |  Loss2: (0.0000) | Acc: (94.00%) (1334/1408)
Epoch: 119 | Batch_idx: 20 |  Loss: (0.1853) |  Loss2: (0.0000) | Acc: (94.00%) (2538/2688)
Epoch: 119 | Batch_idx: 30 |  Loss: (0.1901) |  Loss2: (0.0000) | Acc: (94.00%) (3736/3968)
Epoch: 119 | Batch_idx: 40 |  Loss: (0.1838) |  Loss2: (0.0000) | Acc: (94.00%) (4940/5248)
Epoch: 119 | Batch_idx: 50 |  Loss: (0.1756) |  Loss2: (0.0000) | Acc: (94.00%) (6158/6528)
Epoch: 119 | Batch_idx: 60 |  Loss: (0.1707) |  Loss2: (0.0000) | Acc: (94.00%) (7378/7808)
Epoch: 119 | Batch_idx: 70 |  Loss: (0.1742) |  Loss2: (0.0000) | Acc: (94.00%) (8571/9088)
Epoch: 119 | Batch_idx: 80 |  Loss: (0.1725) |  Loss2: (0.0000) | Acc: (94.00%) (9774/10368)
Epoch: 119 | Batch_idx: 90 |  Loss: (0.1730) |  Loss2: (0.0000) | Acc: (94.00%) (10976/11648)
Epoch: 119 | Batch_idx: 100 |  Loss: (0.1734) |  Loss2: (0.0000) | Acc: (94.00%) (12180/12928)
Epoch: 119 | Batch_idx: 110 |  Loss: (0.1739) |  Loss2: (0.0000) | Acc: (94.00%) (13386/14208)
Epoch: 119 | Batch_idx: 120 |  Loss: (0.1748) |  Loss2: (0.0000) | Acc: (94.00%) (14585/15488)
Epoch: 119 | Batch_idx: 130 |  Loss: (0.1767) |  Loss2: (0.0000) | Acc: (94.00%) (15781/16768)
Epoch: 119 | Batch_idx: 140 |  Loss: (0.1789) |  Loss2: (0.0000) | Acc: (94.00%) (16977/18048)
Epoch: 119 | Batch_idx: 150 |  Loss: (0.1782) |  Loss2: (0.0000) | Acc: (94.00%) (18183/19328)
Epoch: 119 | Batch_idx: 160 |  Loss: (0.1786) |  Loss2: (0.0000) | Acc: (94.00%) (19380/20608)
Epoch: 119 | Batch_idx: 170 |  Loss: (0.1798) |  Loss2: (0.0000) | Acc: (93.00%) (20569/21888)
Epoch: 119 | Batch_idx: 180 |  Loss: (0.1815) |  Loss2: (0.0000) | Acc: (93.00%) (21748/23168)
Epoch: 119 | Batch_idx: 190 |  Loss: (0.1815) |  Loss2: (0.0000) | Acc: (93.00%) (22947/24448)
Epoch: 119 | Batch_idx: 200 |  Loss: (0.1813) |  Loss2: (0.0000) | Acc: (93.00%) (24152/25728)
Epoch: 119 | Batch_idx: 210 |  Loss: (0.1807) |  Loss2: (0.0000) | Acc: (93.00%) (25361/27008)
Epoch: 119 | Batch_idx: 220 |  Loss: (0.1815) |  Loss2: (0.0000) | Acc: (93.00%) (26554/28288)
Epoch: 119 | Batch_idx: 230 |  Loss: (0.1822) |  Loss2: (0.0000) | Acc: (93.00%) (27754/29568)
Epoch: 119 | Batch_idx: 240 |  Loss: (0.1826) |  Loss2: (0.0000) | Acc: (93.00%) (28948/30848)
Epoch: 119 | Batch_idx: 250 |  Loss: (0.1827) |  Loss2: (0.0000) | Acc: (93.00%) (30147/32128)
Epoch: 119 | Batch_idx: 260 |  Loss: (0.1826) |  Loss2: (0.0000) | Acc: (93.00%) (31354/33408)
Epoch: 119 | Batch_idx: 270 |  Loss: (0.1829) |  Loss2: (0.0000) | Acc: (93.00%) (32547/34688)
Epoch: 119 | Batch_idx: 280 |  Loss: (0.1830) |  Loss2: (0.0000) | Acc: (93.00%) (33743/35968)
Epoch: 119 | Batch_idx: 290 |  Loss: (0.1838) |  Loss2: (0.0000) | Acc: (93.00%) (34930/37248)
Epoch: 119 | Batch_idx: 300 |  Loss: (0.1837) |  Loss2: (0.0000) | Acc: (93.00%) (36130/38528)
Epoch: 119 | Batch_idx: 310 |  Loss: (0.1835) |  Loss2: (0.0000) | Acc: (93.00%) (37339/39808)
Epoch: 119 | Batch_idx: 320 |  Loss: (0.1836) |  Loss2: (0.0000) | Acc: (93.00%) (38539/41088)
Epoch: 119 | Batch_idx: 330 |  Loss: (0.1834) |  Loss2: (0.0000) | Acc: (93.00%) (39749/42368)
Epoch: 119 | Batch_idx: 340 |  Loss: (0.1837) |  Loss2: (0.0000) | Acc: (93.00%) (40947/43648)
Epoch: 119 | Batch_idx: 350 |  Loss: (0.1838) |  Loss2: (0.0000) | Acc: (93.00%) (42147/44928)
Epoch: 119 | Batch_idx: 360 |  Loss: (0.1848) |  Loss2: (0.0000) | Acc: (93.00%) (43339/46208)
Epoch: 119 | Batch_idx: 370 |  Loss: (0.1842) |  Loss2: (0.0000) | Acc: (93.00%) (44551/47488)
Epoch: 119 | Batch_idx: 380 |  Loss: (0.1838) |  Loss2: (0.0000) | Acc: (93.00%) (45757/48768)
Epoch: 119 | Batch_idx: 390 |  Loss: (0.1839) |  Loss2: (0.0000) | Acc: (93.00%) (46906/50000)
# TEST : Loss: (0.3854) | Acc: (88.00%) (8806/10000)
percent tensor([0.5181, 0.4819], device='cuda:0')
percent tensor([0.4642, 0.5358], device='cuda:0')
percent tensor([0.5612, 0.4388], device='cuda:0')
percent tensor([0.5967, 0.4033], device='cuda:0')
percent tensor([0.5747, 0.4253], device='cuda:0')
percent tensor([0.6709, 0.3291], device='cuda:0')
percent tensor([0.6961, 0.3039], device='cuda:0')
percent tensor([0.9990, 0.0010], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 120 | Batch_idx: 0 |  Loss: (0.1557) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 120 | Batch_idx: 10 |  Loss: (0.1655) |  Loss2: (0.0000) | Acc: (94.00%) (1325/1408)
Epoch: 120 | Batch_idx: 20 |  Loss: (0.1713) |  Loss2: (0.0000) | Acc: (94.00%) (2531/2688)
Epoch: 120 | Batch_idx: 30 |  Loss: (0.1756) |  Loss2: (0.0000) | Acc: (93.00%) (3728/3968)
Epoch: 120 | Batch_idx: 40 |  Loss: (0.1749) |  Loss2: (0.0000) | Acc: (94.00%) (4934/5248)
Epoch: 120 | Batch_idx: 50 |  Loss: (0.1720) |  Loss2: (0.0000) | Acc: (94.00%) (6147/6528)
Epoch: 120 | Batch_idx: 60 |  Loss: (0.1767) |  Loss2: (0.0000) | Acc: (93.00%) (7338/7808)
Epoch: 120 | Batch_idx: 70 |  Loss: (0.1820) |  Loss2: (0.0000) | Acc: (93.00%) (8519/9088)
Epoch: 120 | Batch_idx: 80 |  Loss: (0.1838) |  Loss2: (0.0000) | Acc: (93.00%) (9711/10368)
Epoch: 120 | Batch_idx: 90 |  Loss: (0.1875) |  Loss2: (0.0000) | Acc: (93.00%) (10892/11648)
Epoch: 120 | Batch_idx: 100 |  Loss: (0.1881) |  Loss2: (0.0000) | Acc: (93.00%) (12084/12928)
Epoch: 120 | Batch_idx: 110 |  Loss: (0.1879) |  Loss2: (0.0000) | Acc: (93.00%) (13288/14208)
Epoch: 120 | Batch_idx: 120 |  Loss: (0.1905) |  Loss2: (0.0000) | Acc: (93.00%) (14473/15488)
Epoch: 120 | Batch_idx: 130 |  Loss: (0.1910) |  Loss2: (0.0000) | Acc: (93.00%) (15669/16768)
Epoch: 120 | Batch_idx: 140 |  Loss: (0.1908) |  Loss2: (0.0000) | Acc: (93.00%) (16868/18048)
Epoch: 120 | Batch_idx: 150 |  Loss: (0.1913) |  Loss2: (0.0000) | Acc: (93.00%) (18057/19328)
Epoch: 120 | Batch_idx: 160 |  Loss: (0.1910) |  Loss2: (0.0000) | Acc: (93.00%) (19256/20608)
Epoch: 120 | Batch_idx: 170 |  Loss: (0.1904) |  Loss2: (0.0000) | Acc: (93.00%) (20448/21888)
Epoch: 120 | Batch_idx: 180 |  Loss: (0.1902) |  Loss2: (0.0000) | Acc: (93.00%) (21643/23168)
Epoch: 120 | Batch_idx: 190 |  Loss: (0.1901) |  Loss2: (0.0000) | Acc: (93.00%) (22827/24448)
Epoch: 120 | Batch_idx: 200 |  Loss: (0.1900) |  Loss2: (0.0000) | Acc: (93.00%) (24009/25728)
Epoch: 120 | Batch_idx: 210 |  Loss: (0.1904) |  Loss2: (0.0000) | Acc: (93.00%) (25196/27008)
Epoch: 120 | Batch_idx: 220 |  Loss: (0.1911) |  Loss2: (0.0000) | Acc: (93.00%) (26385/28288)
Epoch: 120 | Batch_idx: 230 |  Loss: (0.1896) |  Loss2: (0.0000) | Acc: (93.00%) (27600/29568)
Epoch: 120 | Batch_idx: 240 |  Loss: (0.1899) |  Loss2: (0.0000) | Acc: (93.00%) (28790/30848)
Epoch: 120 | Batch_idx: 250 |  Loss: (0.1905) |  Loss2: (0.0000) | Acc: (93.00%) (29983/32128)
Epoch: 120 | Batch_idx: 260 |  Loss: (0.1916) |  Loss2: (0.0000) | Acc: (93.00%) (31160/33408)
Epoch: 120 | Batch_idx: 270 |  Loss: (0.1912) |  Loss2: (0.0000) | Acc: (93.00%) (32359/34688)
Epoch: 120 | Batch_idx: 280 |  Loss: (0.1907) |  Loss2: (0.0000) | Acc: (93.00%) (33557/35968)
Epoch: 120 | Batch_idx: 290 |  Loss: (0.1918) |  Loss2: (0.0000) | Acc: (93.00%) (34737/37248)
Epoch: 120 | Batch_idx: 300 |  Loss: (0.1932) |  Loss2: (0.0000) | Acc: (93.00%) (35914/38528)
Epoch: 120 | Batch_idx: 310 |  Loss: (0.1938) |  Loss2: (0.0000) | Acc: (93.00%) (37096/39808)
Epoch: 120 | Batch_idx: 320 |  Loss: (0.1950) |  Loss2: (0.0000) | Acc: (93.00%) (38272/41088)
Epoch: 120 | Batch_idx: 330 |  Loss: (0.1952) |  Loss2: (0.0000) | Acc: (93.00%) (39468/42368)
Epoch: 120 | Batch_idx: 340 |  Loss: (0.1950) |  Loss2: (0.0000) | Acc: (93.00%) (40670/43648)
Epoch: 120 | Batch_idx: 350 |  Loss: (0.1941) |  Loss2: (0.0000) | Acc: (93.00%) (41887/44928)
Epoch: 120 | Batch_idx: 360 |  Loss: (0.1934) |  Loss2: (0.0000) | Acc: (93.00%) (43092/46208)
Epoch: 120 | Batch_idx: 370 |  Loss: (0.1933) |  Loss2: (0.0000) | Acc: (93.00%) (44285/47488)
Epoch: 120 | Batch_idx: 380 |  Loss: (0.1931) |  Loss2: (0.0000) | Acc: (93.00%) (45480/48768)
Epoch: 120 | Batch_idx: 390 |  Loss: (0.1927) |  Loss2: (0.0000) | Acc: (93.00%) (46642/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_120.pth.tar'
# TEST : Loss: (0.4214) | Acc: (86.00%) (8699/10000)
percent tensor([0.5178, 0.4822], device='cuda:0')
percent tensor([0.4639, 0.5361], device='cuda:0')
percent tensor([0.5609, 0.4391], device='cuda:0')
percent tensor([0.5974, 0.4026], device='cuda:0')
percent tensor([0.5747, 0.4253], device='cuda:0')
percent tensor([0.6682, 0.3318], device='cuda:0')
percent tensor([0.6897, 0.3103], device='cuda:0')
percent tensor([0.9990, 0.0010], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(181.6576, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(815.0617, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(807.9974, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1513.1487, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(499.3472, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2244.6589, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4269.4136, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1384.5138, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6197.1504, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11808.5938, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3899.5923, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16509.7891, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 121 | Batch_idx: 0 |  Loss: (0.2340) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 121 | Batch_idx: 10 |  Loss: (0.1907) |  Loss2: (0.0000) | Acc: (93.00%) (1320/1408)
Epoch: 121 | Batch_idx: 20 |  Loss: (0.1793) |  Loss2: (0.0000) | Acc: (94.00%) (2532/2688)
Epoch: 121 | Batch_idx: 30 |  Loss: (0.1757) |  Loss2: (0.0000) | Acc: (94.00%) (3739/3968)
Epoch: 121 | Batch_idx: 40 |  Loss: (0.1751) |  Loss2: (0.0000) | Acc: (94.00%) (4944/5248)
Epoch: 121 | Batch_idx: 50 |  Loss: (0.1724) |  Loss2: (0.0000) | Acc: (94.00%) (6153/6528)
Epoch: 121 | Batch_idx: 60 |  Loss: (0.1751) |  Loss2: (0.0000) | Acc: (94.00%) (7350/7808)
Epoch: 121 | Batch_idx: 70 |  Loss: (0.1805) |  Loss2: (0.0000) | Acc: (93.00%) (8538/9088)
Epoch: 121 | Batch_idx: 80 |  Loss: (0.1781) |  Loss2: (0.0000) | Acc: (94.00%) (9748/10368)
Epoch: 121 | Batch_idx: 90 |  Loss: (0.1788) |  Loss2: (0.0000) | Acc: (94.00%) (10952/11648)
Epoch: 121 | Batch_idx: 100 |  Loss: (0.1827) |  Loss2: (0.0000) | Acc: (93.00%) (12136/12928)
Epoch: 121 | Batch_idx: 110 |  Loss: (0.1814) |  Loss2: (0.0000) | Acc: (93.00%) (13346/14208)
Epoch: 121 | Batch_idx: 120 |  Loss: (0.1814) |  Loss2: (0.0000) | Acc: (93.00%) (14540/15488)
Epoch: 121 | Batch_idx: 130 |  Loss: (0.1834) |  Loss2: (0.0000) | Acc: (93.00%) (15730/16768)
Epoch: 121 | Batch_idx: 140 |  Loss: (0.1847) |  Loss2: (0.0000) | Acc: (93.00%) (16918/18048)
Epoch: 121 | Batch_idx: 150 |  Loss: (0.1848) |  Loss2: (0.0000) | Acc: (93.00%) (18113/19328)
Epoch: 121 | Batch_idx: 160 |  Loss: (0.1858) |  Loss2: (0.0000) | Acc: (93.00%) (19300/20608)
Epoch: 121 | Batch_idx: 170 |  Loss: (0.1862) |  Loss2: (0.0000) | Acc: (93.00%) (20496/21888)
Epoch: 121 | Batch_idx: 180 |  Loss: (0.1867) |  Loss2: (0.0000) | Acc: (93.00%) (21686/23168)
Epoch: 121 | Batch_idx: 190 |  Loss: (0.1867) |  Loss2: (0.0000) | Acc: (93.00%) (22897/24448)
Epoch: 121 | Batch_idx: 200 |  Loss: (0.1864) |  Loss2: (0.0000) | Acc: (93.00%) (24097/25728)
Epoch: 121 | Batch_idx: 210 |  Loss: (0.1863) |  Loss2: (0.0000) | Acc: (93.00%) (25295/27008)
Epoch: 121 | Batch_idx: 220 |  Loss: (0.1861) |  Loss2: (0.0000) | Acc: (93.00%) (26500/28288)
Epoch: 121 | Batch_idx: 230 |  Loss: (0.1859) |  Loss2: (0.0000) | Acc: (93.00%) (27694/29568)
Epoch: 121 | Batch_idx: 240 |  Loss: (0.1868) |  Loss2: (0.0000) | Acc: (93.00%) (28878/30848)
Epoch: 121 | Batch_idx: 250 |  Loss: (0.1867) |  Loss2: (0.0000) | Acc: (93.00%) (30077/32128)
Epoch: 121 | Batch_idx: 260 |  Loss: (0.1865) |  Loss2: (0.0000) | Acc: (93.00%) (31273/33408)
Epoch: 121 | Batch_idx: 270 |  Loss: (0.1864) |  Loss2: (0.0000) | Acc: (93.00%) (32465/34688)
Epoch: 121 | Batch_idx: 280 |  Loss: (0.1866) |  Loss2: (0.0000) | Acc: (93.00%) (33661/35968)
Epoch: 121 | Batch_idx: 290 |  Loss: (0.1870) |  Loss2: (0.0000) | Acc: (93.00%) (34848/37248)
Epoch: 121 | Batch_idx: 300 |  Loss: (0.1875) |  Loss2: (0.0000) | Acc: (93.00%) (36038/38528)
Epoch: 121 | Batch_idx: 310 |  Loss: (0.1874) |  Loss2: (0.0000) | Acc: (93.00%) (37229/39808)
Epoch: 121 | Batch_idx: 320 |  Loss: (0.1867) |  Loss2: (0.0000) | Acc: (93.00%) (38429/41088)
Epoch: 121 | Batch_idx: 330 |  Loss: (0.1869) |  Loss2: (0.0000) | Acc: (93.00%) (39624/42368)
Epoch: 121 | Batch_idx: 340 |  Loss: (0.1876) |  Loss2: (0.0000) | Acc: (93.00%) (40818/43648)
Epoch: 121 | Batch_idx: 350 |  Loss: (0.1871) |  Loss2: (0.0000) | Acc: (93.00%) (42031/44928)
Epoch: 121 | Batch_idx: 360 |  Loss: (0.1865) |  Loss2: (0.0000) | Acc: (93.00%) (43241/46208)
Epoch: 121 | Batch_idx: 370 |  Loss: (0.1860) |  Loss2: (0.0000) | Acc: (93.00%) (44451/47488)
Epoch: 121 | Batch_idx: 380 |  Loss: (0.1865) |  Loss2: (0.0000) | Acc: (93.00%) (45635/48768)
Epoch: 121 | Batch_idx: 390 |  Loss: (0.1864) |  Loss2: (0.0000) | Acc: (93.00%) (46791/50000)
# TEST : Loss: (0.4171) | Acc: (87.00%) (8723/10000)
percent tensor([0.5177, 0.4823], device='cuda:0')
percent tensor([0.4645, 0.5355], device='cuda:0')
percent tensor([0.5592, 0.4408], device='cuda:0')
percent tensor([0.5978, 0.4022], device='cuda:0')
percent tensor([0.5744, 0.4256], device='cuda:0')
percent tensor([0.6687, 0.3313], device='cuda:0')
percent tensor([0.6873, 0.3127], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
Epoch: 122 | Batch_idx: 0 |  Loss: (0.2171) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 122 | Batch_idx: 10 |  Loss: (0.1904) |  Loss2: (0.0000) | Acc: (93.00%) (1319/1408)
Epoch: 122 | Batch_idx: 20 |  Loss: (0.1757) |  Loss2: (0.0000) | Acc: (94.00%) (2532/2688)
Epoch: 122 | Batch_idx: 30 |  Loss: (0.1801) |  Loss2: (0.0000) | Acc: (93.00%) (3729/3968)
Epoch: 122 | Batch_idx: 40 |  Loss: (0.1857) |  Loss2: (0.0000) | Acc: (93.00%) (4916/5248)
Epoch: 122 | Batch_idx: 50 |  Loss: (0.1868) |  Loss2: (0.0000) | Acc: (93.00%) (6102/6528)
Epoch: 122 | Batch_idx: 60 |  Loss: (0.1828) |  Loss2: (0.0000) | Acc: (93.00%) (7313/7808)
Epoch: 122 | Batch_idx: 70 |  Loss: (0.1795) |  Loss2: (0.0000) | Acc: (93.00%) (8522/9088)
Epoch: 122 | Batch_idx: 80 |  Loss: (0.1788) |  Loss2: (0.0000) | Acc: (93.00%) (9724/10368)
Epoch: 122 | Batch_idx: 90 |  Loss: (0.1784) |  Loss2: (0.0000) | Acc: (93.00%) (10922/11648)
Epoch: 122 | Batch_idx: 100 |  Loss: (0.1803) |  Loss2: (0.0000) | Acc: (93.00%) (12116/12928)
Epoch: 122 | Batch_idx: 110 |  Loss: (0.1800) |  Loss2: (0.0000) | Acc: (93.00%) (13318/14208)
Epoch: 122 | Batch_idx: 120 |  Loss: (0.1786) |  Loss2: (0.0000) | Acc: (93.00%) (14527/15488)
Epoch: 122 | Batch_idx: 130 |  Loss: (0.1773) |  Loss2: (0.0000) | Acc: (93.00%) (15746/16768)
Epoch: 122 | Batch_idx: 140 |  Loss: (0.1777) |  Loss2: (0.0000) | Acc: (93.00%) (16944/18048)
Epoch: 122 | Batch_idx: 150 |  Loss: (0.1782) |  Loss2: (0.0000) | Acc: (93.00%) (18142/19328)
Epoch: 122 | Batch_idx: 160 |  Loss: (0.1767) |  Loss2: (0.0000) | Acc: (93.00%) (19352/20608)
Epoch: 122 | Batch_idx: 170 |  Loss: (0.1770) |  Loss2: (0.0000) | Acc: (93.00%) (20558/21888)
Epoch: 122 | Batch_idx: 180 |  Loss: (0.1769) |  Loss2: (0.0000) | Acc: (93.00%) (21758/23168)
Epoch: 122 | Batch_idx: 190 |  Loss: (0.1771) |  Loss2: (0.0000) | Acc: (93.00%) (22966/24448)
Epoch: 122 | Batch_idx: 200 |  Loss: (0.1789) |  Loss2: (0.0000) | Acc: (93.00%) (24152/25728)
Epoch: 122 | Batch_idx: 210 |  Loss: (0.1792) |  Loss2: (0.0000) | Acc: (93.00%) (25353/27008)
Epoch: 122 | Batch_idx: 220 |  Loss: (0.1783) |  Loss2: (0.0000) | Acc: (93.00%) (26568/28288)
Epoch: 122 | Batch_idx: 230 |  Loss: (0.1797) |  Loss2: (0.0000) | Acc: (93.00%) (27753/29568)
Epoch: 122 | Batch_idx: 240 |  Loss: (0.1805) |  Loss2: (0.0000) | Acc: (93.00%) (28949/30848)
Epoch: 122 | Batch_idx: 250 |  Loss: (0.1805) |  Loss2: (0.0000) | Acc: (93.00%) (30155/32128)
Epoch: 122 | Batch_idx: 260 |  Loss: (0.1805) |  Loss2: (0.0000) | Acc: (93.00%) (31368/33408)
Epoch: 122 | Batch_idx: 270 |  Loss: (0.1804) |  Loss2: (0.0000) | Acc: (93.00%) (32568/34688)
Epoch: 122 | Batch_idx: 280 |  Loss: (0.1808) |  Loss2: (0.0000) | Acc: (93.00%) (33771/35968)
Epoch: 122 | Batch_idx: 290 |  Loss: (0.1810) |  Loss2: (0.0000) | Acc: (93.00%) (34967/37248)
Epoch: 122 | Batch_idx: 300 |  Loss: (0.1821) |  Loss2: (0.0000) | Acc: (93.00%) (36152/38528)
Epoch: 122 | Batch_idx: 310 |  Loss: (0.1827) |  Loss2: (0.0000) | Acc: (93.00%) (37342/39808)
Epoch: 122 | Batch_idx: 320 |  Loss: (0.1829) |  Loss2: (0.0000) | Acc: (93.00%) (38548/41088)
Epoch: 122 | Batch_idx: 330 |  Loss: (0.1838) |  Loss2: (0.0000) | Acc: (93.00%) (39734/42368)
Epoch: 122 | Batch_idx: 340 |  Loss: (0.1838) |  Loss2: (0.0000) | Acc: (93.00%) (40931/43648)
Epoch: 122 | Batch_idx: 350 |  Loss: (0.1838) |  Loss2: (0.0000) | Acc: (93.00%) (42121/44928)
Epoch: 122 | Batch_idx: 360 |  Loss: (0.1834) |  Loss2: (0.0000) | Acc: (93.00%) (43334/46208)
Epoch: 122 | Batch_idx: 370 |  Loss: (0.1835) |  Loss2: (0.0000) | Acc: (93.00%) (44529/47488)
Epoch: 122 | Batch_idx: 380 |  Loss: (0.1829) |  Loss2: (0.0000) | Acc: (93.00%) (45735/48768)
Epoch: 122 | Batch_idx: 390 |  Loss: (0.1828) |  Loss2: (0.0000) | Acc: (93.00%) (46887/50000)
# TEST : Loss: (0.5014) | Acc: (85.00%) (8515/10000)
percent tensor([0.5178, 0.4822], device='cuda:0')
percent tensor([0.4644, 0.5356], device='cuda:0')
percent tensor([0.5602, 0.4398], device='cuda:0')
percent tensor([0.5967, 0.4033], device='cuda:0')
percent tensor([0.5753, 0.4247], device='cuda:0')
percent tensor([0.6670, 0.3330], device='cuda:0')
percent tensor([0.6938, 0.3062], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 123 | Batch_idx: 0 |  Loss: (0.1835) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 123 | Batch_idx: 10 |  Loss: (0.1620) |  Loss2: (0.0000) | Acc: (94.00%) (1335/1408)
Epoch: 123 | Batch_idx: 20 |  Loss: (0.1799) |  Loss2: (0.0000) | Acc: (93.00%) (2526/2688)
Epoch: 123 | Batch_idx: 30 |  Loss: (0.1805) |  Loss2: (0.0000) | Acc: (93.00%) (3728/3968)
Epoch: 123 | Batch_idx: 40 |  Loss: (0.1762) |  Loss2: (0.0000) | Acc: (94.00%) (4941/5248)
Epoch: 123 | Batch_idx: 50 |  Loss: (0.1770) |  Loss2: (0.0000) | Acc: (94.00%) (6155/6528)
Epoch: 123 | Batch_idx: 60 |  Loss: (0.1769) |  Loss2: (0.0000) | Acc: (94.00%) (7350/7808)
Epoch: 123 | Batch_idx: 70 |  Loss: (0.1772) |  Loss2: (0.0000) | Acc: (94.00%) (8544/9088)
Epoch: 123 | Batch_idx: 80 |  Loss: (0.1782) |  Loss2: (0.0000) | Acc: (94.00%) (9746/10368)
Epoch: 123 | Batch_idx: 90 |  Loss: (0.1767) |  Loss2: (0.0000) | Acc: (94.00%) (10957/11648)
Epoch: 123 | Batch_idx: 100 |  Loss: (0.1758) |  Loss2: (0.0000) | Acc: (94.00%) (12168/12928)
Epoch: 123 | Batch_idx: 110 |  Loss: (0.1752) |  Loss2: (0.0000) | Acc: (94.00%) (13371/14208)
Epoch: 123 | Batch_idx: 120 |  Loss: (0.1755) |  Loss2: (0.0000) | Acc: (94.00%) (14568/15488)
Epoch: 123 | Batch_idx: 130 |  Loss: (0.1758) |  Loss2: (0.0000) | Acc: (93.00%) (15759/16768)
Epoch: 123 | Batch_idx: 140 |  Loss: (0.1761) |  Loss2: (0.0000) | Acc: (93.00%) (16960/18048)
Epoch: 123 | Batch_idx: 150 |  Loss: (0.1750) |  Loss2: (0.0000) | Acc: (94.00%) (18171/19328)
Epoch: 123 | Batch_idx: 160 |  Loss: (0.1751) |  Loss2: (0.0000) | Acc: (93.00%) (19367/20608)
Epoch: 123 | Batch_idx: 170 |  Loss: (0.1750) |  Loss2: (0.0000) | Acc: (93.00%) (20573/21888)
Epoch: 123 | Batch_idx: 180 |  Loss: (0.1748) |  Loss2: (0.0000) | Acc: (93.00%) (21768/23168)
Epoch: 123 | Batch_idx: 190 |  Loss: (0.1756) |  Loss2: (0.0000) | Acc: (93.00%) (22960/24448)
Epoch: 123 | Batch_idx: 200 |  Loss: (0.1753) |  Loss2: (0.0000) | Acc: (93.00%) (24165/25728)
Epoch: 123 | Batch_idx: 210 |  Loss: (0.1745) |  Loss2: (0.0000) | Acc: (93.00%) (25376/27008)
Epoch: 123 | Batch_idx: 220 |  Loss: (0.1745) |  Loss2: (0.0000) | Acc: (93.00%) (26582/28288)
Epoch: 123 | Batch_idx: 230 |  Loss: (0.1753) |  Loss2: (0.0000) | Acc: (93.00%) (27768/29568)
Epoch: 123 | Batch_idx: 240 |  Loss: (0.1759) |  Loss2: (0.0000) | Acc: (93.00%) (28962/30848)
Epoch: 123 | Batch_idx: 250 |  Loss: (0.1752) |  Loss2: (0.0000) | Acc: (93.00%) (30174/32128)
Epoch: 123 | Batch_idx: 260 |  Loss: (0.1748) |  Loss2: (0.0000) | Acc: (93.00%) (31386/33408)
Epoch: 123 | Batch_idx: 270 |  Loss: (0.1755) |  Loss2: (0.0000) | Acc: (93.00%) (32583/34688)
Epoch: 123 | Batch_idx: 280 |  Loss: (0.1753) |  Loss2: (0.0000) | Acc: (93.00%) (33783/35968)
Epoch: 123 | Batch_idx: 290 |  Loss: (0.1757) |  Loss2: (0.0000) | Acc: (93.00%) (34988/37248)
Epoch: 123 | Batch_idx: 300 |  Loss: (0.1762) |  Loss2: (0.0000) | Acc: (93.00%) (36194/38528)
Epoch: 123 | Batch_idx: 310 |  Loss: (0.1758) |  Loss2: (0.0000) | Acc: (93.00%) (37402/39808)
Epoch: 123 | Batch_idx: 320 |  Loss: (0.1757) |  Loss2: (0.0000) | Acc: (93.00%) (38606/41088)
Epoch: 123 | Batch_idx: 330 |  Loss: (0.1758) |  Loss2: (0.0000) | Acc: (93.00%) (39804/42368)
Epoch: 123 | Batch_idx: 340 |  Loss: (0.1753) |  Loss2: (0.0000) | Acc: (93.00%) (41012/43648)
Epoch: 123 | Batch_idx: 350 |  Loss: (0.1766) |  Loss2: (0.0000) | Acc: (93.00%) (42193/44928)
Epoch: 123 | Batch_idx: 360 |  Loss: (0.1765) |  Loss2: (0.0000) | Acc: (93.00%) (43392/46208)
Epoch: 123 | Batch_idx: 370 |  Loss: (0.1764) |  Loss2: (0.0000) | Acc: (93.00%) (44597/47488)
Epoch: 123 | Batch_idx: 380 |  Loss: (0.1770) |  Loss2: (0.0000) | Acc: (93.00%) (45785/48768)
Epoch: 123 | Batch_idx: 390 |  Loss: (0.1772) |  Loss2: (0.0000) | Acc: (93.00%) (46940/50000)
# TEST : Loss: (0.4462) | Acc: (86.00%) (8671/10000)
percent tensor([0.5183, 0.4817], device='cuda:0')
percent tensor([0.4640, 0.5360], device='cuda:0')
percent tensor([0.5599, 0.4401], device='cuda:0')
percent tensor([0.5963, 0.4037], device='cuda:0')
percent tensor([0.5749, 0.4251], device='cuda:0')
percent tensor([0.6697, 0.3303], device='cuda:0')
percent tensor([0.7005, 0.2995], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 124 | Batch_idx: 0 |  Loss: (0.1633) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 124 | Batch_idx: 10 |  Loss: (0.1717) |  Loss2: (0.0000) | Acc: (94.00%) (1324/1408)
Epoch: 124 | Batch_idx: 20 |  Loss: (0.1622) |  Loss2: (0.0000) | Acc: (94.00%) (2544/2688)
Epoch: 124 | Batch_idx: 30 |  Loss: (0.1616) |  Loss2: (0.0000) | Acc: (94.00%) (3756/3968)
Epoch: 124 | Batch_idx: 40 |  Loss: (0.1637) |  Loss2: (0.0000) | Acc: (94.00%) (4959/5248)
Epoch: 124 | Batch_idx: 50 |  Loss: (0.1641) |  Loss2: (0.0000) | Acc: (94.00%) (6164/6528)
Epoch: 124 | Batch_idx: 60 |  Loss: (0.1665) |  Loss2: (0.0000) | Acc: (94.00%) (7368/7808)
Epoch: 124 | Batch_idx: 70 |  Loss: (0.1655) |  Loss2: (0.0000) | Acc: (94.00%) (8584/9088)
Epoch: 124 | Batch_idx: 80 |  Loss: (0.1666) |  Loss2: (0.0000) | Acc: (94.00%) (9786/10368)
Epoch: 124 | Batch_idx: 90 |  Loss: (0.1684) |  Loss2: (0.0000) | Acc: (94.00%) (10986/11648)
Epoch: 124 | Batch_idx: 100 |  Loss: (0.1682) |  Loss2: (0.0000) | Acc: (94.00%) (12187/12928)
Epoch: 124 | Batch_idx: 110 |  Loss: (0.1677) |  Loss2: (0.0000) | Acc: (94.00%) (13392/14208)
Epoch: 124 | Batch_idx: 120 |  Loss: (0.1697) |  Loss2: (0.0000) | Acc: (94.00%) (14589/15488)
Epoch: 124 | Batch_idx: 130 |  Loss: (0.1699) |  Loss2: (0.0000) | Acc: (94.00%) (15788/16768)
Epoch: 124 | Batch_idx: 140 |  Loss: (0.1697) |  Loss2: (0.0000) | Acc: (94.00%) (16997/18048)
Epoch: 124 | Batch_idx: 150 |  Loss: (0.1710) |  Loss2: (0.0000) | Acc: (94.00%) (18194/19328)
Epoch: 124 | Batch_idx: 160 |  Loss: (0.1706) |  Loss2: (0.0000) | Acc: (94.00%) (19409/20608)
Epoch: 124 | Batch_idx: 170 |  Loss: (0.1698) |  Loss2: (0.0000) | Acc: (94.00%) (20620/21888)
Epoch: 124 | Batch_idx: 180 |  Loss: (0.1694) |  Loss2: (0.0000) | Acc: (94.00%) (21830/23168)
Epoch: 124 | Batch_idx: 190 |  Loss: (0.1706) |  Loss2: (0.0000) | Acc: (94.00%) (23018/24448)
Epoch: 124 | Batch_idx: 200 |  Loss: (0.1721) |  Loss2: (0.0000) | Acc: (94.00%) (24211/25728)
Epoch: 124 | Batch_idx: 210 |  Loss: (0.1726) |  Loss2: (0.0000) | Acc: (94.00%) (25408/27008)
Epoch: 124 | Batch_idx: 220 |  Loss: (0.1724) |  Loss2: (0.0000) | Acc: (94.00%) (26603/28288)
Epoch: 124 | Batch_idx: 230 |  Loss: (0.1719) |  Loss2: (0.0000) | Acc: (94.00%) (27811/29568)
Epoch: 124 | Batch_idx: 240 |  Loss: (0.1717) |  Loss2: (0.0000) | Acc: (94.00%) (29014/30848)
Epoch: 124 | Batch_idx: 250 |  Loss: (0.1728) |  Loss2: (0.0000) | Acc: (94.00%) (30208/32128)
Epoch: 124 | Batch_idx: 260 |  Loss: (0.1741) |  Loss2: (0.0000) | Acc: (93.00%) (31390/33408)
Epoch: 124 | Batch_idx: 270 |  Loss: (0.1746) |  Loss2: (0.0000) | Acc: (93.00%) (32577/34688)
Epoch: 124 | Batch_idx: 280 |  Loss: (0.1739) |  Loss2: (0.0000) | Acc: (93.00%) (33802/35968)
Epoch: 124 | Batch_idx: 290 |  Loss: (0.1745) |  Loss2: (0.0000) | Acc: (93.00%) (35005/37248)
Epoch: 124 | Batch_idx: 300 |  Loss: (0.1747) |  Loss2: (0.0000) | Acc: (93.00%) (36197/38528)
Epoch: 124 | Batch_idx: 310 |  Loss: (0.1743) |  Loss2: (0.0000) | Acc: (93.00%) (37406/39808)
Epoch: 124 | Batch_idx: 320 |  Loss: (0.1741) |  Loss2: (0.0000) | Acc: (93.00%) (38605/41088)
Epoch: 124 | Batch_idx: 330 |  Loss: (0.1739) |  Loss2: (0.0000) | Acc: (93.00%) (39813/42368)
Epoch: 124 | Batch_idx: 340 |  Loss: (0.1735) |  Loss2: (0.0000) | Acc: (93.00%) (41014/43648)
Epoch: 124 | Batch_idx: 350 |  Loss: (0.1733) |  Loss2: (0.0000) | Acc: (93.00%) (42221/44928)
Epoch: 124 | Batch_idx: 360 |  Loss: (0.1735) |  Loss2: (0.0000) | Acc: (93.00%) (43425/46208)
Epoch: 124 | Batch_idx: 370 |  Loss: (0.1740) |  Loss2: (0.0000) | Acc: (93.00%) (44621/47488)
Epoch: 124 | Batch_idx: 380 |  Loss: (0.1742) |  Loss2: (0.0000) | Acc: (93.00%) (45817/48768)
Epoch: 124 | Batch_idx: 390 |  Loss: (0.1745) |  Loss2: (0.0000) | Acc: (93.00%) (46975/50000)
# TEST : Loss: (0.4415) | Acc: (86.00%) (8663/10000)
percent tensor([0.5183, 0.4817], device='cuda:0')
percent tensor([0.4635, 0.5365], device='cuda:0')
percent tensor([0.5611, 0.4389], device='cuda:0')
percent tensor([0.5974, 0.4026], device='cuda:0')
percent tensor([0.5743, 0.4257], device='cuda:0')
percent tensor([0.6671, 0.3329], device='cuda:0')
percent tensor([0.6964, 0.3036], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 125 | Batch_idx: 0 |  Loss: (0.1717) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 125 | Batch_idx: 10 |  Loss: (0.1949) |  Loss2: (0.0000) | Acc: (93.00%) (1312/1408)
Epoch: 125 | Batch_idx: 20 |  Loss: (0.1903) |  Loss2: (0.0000) | Acc: (92.00%) (2499/2688)
Epoch: 125 | Batch_idx: 30 |  Loss: (0.1952) |  Loss2: (0.0000) | Acc: (92.00%) (3682/3968)
Epoch: 125 | Batch_idx: 40 |  Loss: (0.2011) |  Loss2: (0.0000) | Acc: (92.00%) (4853/5248)
Epoch: 125 | Batch_idx: 50 |  Loss: (0.2030) |  Loss2: (0.0000) | Acc: (92.00%) (6039/6528)
Epoch: 125 | Batch_idx: 60 |  Loss: (0.2035) |  Loss2: (0.0000) | Acc: (92.00%) (7224/7808)
Epoch: 125 | Batch_idx: 70 |  Loss: (0.2041) |  Loss2: (0.0000) | Acc: (92.00%) (8411/9088)
Epoch: 125 | Batch_idx: 80 |  Loss: (0.2043) |  Loss2: (0.0000) | Acc: (92.00%) (9602/10368)
Epoch: 125 | Batch_idx: 90 |  Loss: (0.2012) |  Loss2: (0.0000) | Acc: (92.00%) (10804/11648)
Epoch: 125 | Batch_idx: 100 |  Loss: (0.2026) |  Loss2: (0.0000) | Acc: (92.00%) (11987/12928)
Epoch: 125 | Batch_idx: 110 |  Loss: (0.2066) |  Loss2: (0.0000) | Acc: (92.00%) (13153/14208)
Epoch: 125 | Batch_idx: 120 |  Loss: (0.2071) |  Loss2: (0.0000) | Acc: (92.00%) (14335/15488)
Epoch: 125 | Batch_idx: 130 |  Loss: (0.2074) |  Loss2: (0.0000) | Acc: (92.00%) (15520/16768)
Epoch: 125 | Batch_idx: 140 |  Loss: (0.2081) |  Loss2: (0.0000) | Acc: (92.00%) (16704/18048)
Epoch: 125 | Batch_idx: 150 |  Loss: (0.2080) |  Loss2: (0.0000) | Acc: (92.00%) (17886/19328)
Epoch: 125 | Batch_idx: 160 |  Loss: (0.2084) |  Loss2: (0.0000) | Acc: (92.00%) (19074/20608)
Epoch: 125 | Batch_idx: 170 |  Loss: (0.2070) |  Loss2: (0.0000) | Acc: (92.00%) (20269/21888)
Epoch: 125 | Batch_idx: 180 |  Loss: (0.2057) |  Loss2: (0.0000) | Acc: (92.00%) (21470/23168)
Epoch: 125 | Batch_idx: 190 |  Loss: (0.2056) |  Loss2: (0.0000) | Acc: (92.00%) (22658/24448)
Epoch: 125 | Batch_idx: 200 |  Loss: (0.2060) |  Loss2: (0.0000) | Acc: (92.00%) (23841/25728)
Epoch: 125 | Batch_idx: 210 |  Loss: (0.2060) |  Loss2: (0.0000) | Acc: (92.00%) (25022/27008)
Epoch: 125 | Batch_idx: 220 |  Loss: (0.2056) |  Loss2: (0.0000) | Acc: (92.00%) (26215/28288)
Epoch: 125 | Batch_idx: 230 |  Loss: (0.2052) |  Loss2: (0.0000) | Acc: (92.00%) (27404/29568)
Epoch: 125 | Batch_idx: 240 |  Loss: (0.2030) |  Loss2: (0.0000) | Acc: (92.00%) (28616/30848)
Epoch: 125 | Batch_idx: 250 |  Loss: (0.2032) |  Loss2: (0.0000) | Acc: (92.00%) (29806/32128)
Epoch: 125 | Batch_idx: 260 |  Loss: (0.2025) |  Loss2: (0.0000) | Acc: (92.00%) (31014/33408)
Epoch: 125 | Batch_idx: 270 |  Loss: (0.2022) |  Loss2: (0.0000) | Acc: (92.00%) (32196/34688)
Epoch: 125 | Batch_idx: 280 |  Loss: (0.2019) |  Loss2: (0.0000) | Acc: (92.00%) (33394/35968)
Epoch: 125 | Batch_idx: 290 |  Loss: (0.2008) |  Loss2: (0.0000) | Acc: (92.00%) (34605/37248)
Epoch: 125 | Batch_idx: 300 |  Loss: (0.2006) |  Loss2: (0.0000) | Acc: (92.00%) (35798/38528)
Epoch: 125 | Batch_idx: 310 |  Loss: (0.1995) |  Loss2: (0.0000) | Acc: (92.00%) (37002/39808)
Epoch: 125 | Batch_idx: 320 |  Loss: (0.1992) |  Loss2: (0.0000) | Acc: (92.00%) (38196/41088)
Epoch: 125 | Batch_idx: 330 |  Loss: (0.1985) |  Loss2: (0.0000) | Acc: (93.00%) (39404/42368)
Epoch: 125 | Batch_idx: 340 |  Loss: (0.1982) |  Loss2: (0.0000) | Acc: (93.00%) (40599/43648)
Epoch: 125 | Batch_idx: 350 |  Loss: (0.1978) |  Loss2: (0.0000) | Acc: (93.00%) (41803/44928)
Epoch: 125 | Batch_idx: 360 |  Loss: (0.1975) |  Loss2: (0.0000) | Acc: (93.00%) (43004/46208)
Epoch: 125 | Batch_idx: 370 |  Loss: (0.1977) |  Loss2: (0.0000) | Acc: (93.00%) (44183/47488)
Epoch: 125 | Batch_idx: 380 |  Loss: (0.1971) |  Loss2: (0.0000) | Acc: (93.00%) (45372/48768)
Epoch: 125 | Batch_idx: 390 |  Loss: (0.1968) |  Loss2: (0.0000) | Acc: (93.00%) (46525/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_125.pth.tar'
# TEST : Loss: (0.4107) | Acc: (87.00%) (8741/10000)
percent tensor([0.5180, 0.4820], device='cuda:0')
percent tensor([0.4653, 0.5347], device='cuda:0')
percent tensor([0.5582, 0.4418], device='cuda:0')
percent tensor([0.5985, 0.4015], device='cuda:0')
percent tensor([0.5800, 0.4200], device='cuda:0')
percent tensor([0.6400, 0.3600], device='cuda:0')
percent tensor([0.6499, 0.3501], device='cuda:0')
percent tensor([0.9990, 0.0010], device='cuda:0')
Epoch: 126 | Batch_idx: 0 |  Loss: (0.2459) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 126 | Batch_idx: 10 |  Loss: (0.1609) |  Loss2: (0.0000) | Acc: (94.00%) (1331/1408)
Epoch: 126 | Batch_idx: 20 |  Loss: (0.1797) |  Loss2: (0.0000) | Acc: (93.00%) (2516/2688)
Epoch: 126 | Batch_idx: 30 |  Loss: (0.1901) |  Loss2: (0.0000) | Acc: (93.00%) (3710/3968)
Epoch: 126 | Batch_idx: 40 |  Loss: (0.1893) |  Loss2: (0.0000) | Acc: (93.00%) (4910/5248)
Epoch: 126 | Batch_idx: 50 |  Loss: (0.1890) |  Loss2: (0.0000) | Acc: (93.00%) (6114/6528)
Epoch: 126 | Batch_idx: 60 |  Loss: (0.1889) |  Loss2: (0.0000) | Acc: (93.00%) (7309/7808)
Epoch: 126 | Batch_idx: 70 |  Loss: (0.1853) |  Loss2: (0.0000) | Acc: (93.00%) (8518/9088)
Epoch: 126 | Batch_idx: 80 |  Loss: (0.1849) |  Loss2: (0.0000) | Acc: (93.00%) (9721/10368)
Epoch: 126 | Batch_idx: 90 |  Loss: (0.1834) |  Loss2: (0.0000) | Acc: (93.00%) (10934/11648)
Epoch: 126 | Batch_idx: 100 |  Loss: (0.1846) |  Loss2: (0.0000) | Acc: (93.00%) (12125/12928)
Epoch: 126 | Batch_idx: 110 |  Loss: (0.1818) |  Loss2: (0.0000) | Acc: (93.00%) (13338/14208)
Epoch: 126 | Batch_idx: 120 |  Loss: (0.1803) |  Loss2: (0.0000) | Acc: (93.00%) (14549/15488)
Epoch: 126 | Batch_idx: 130 |  Loss: (0.1806) |  Loss2: (0.0000) | Acc: (93.00%) (15749/16768)
Epoch: 126 | Batch_idx: 140 |  Loss: (0.1805) |  Loss2: (0.0000) | Acc: (93.00%) (16948/18048)
Epoch: 126 | Batch_idx: 150 |  Loss: (0.1803) |  Loss2: (0.0000) | Acc: (93.00%) (18149/19328)
Epoch: 126 | Batch_idx: 160 |  Loss: (0.1807) |  Loss2: (0.0000) | Acc: (93.00%) (19347/20608)
Epoch: 126 | Batch_idx: 170 |  Loss: (0.1807) |  Loss2: (0.0000) | Acc: (93.00%) (20544/21888)
Epoch: 126 | Batch_idx: 180 |  Loss: (0.1803) |  Loss2: (0.0000) | Acc: (93.00%) (21742/23168)
Epoch: 126 | Batch_idx: 190 |  Loss: (0.1822) |  Loss2: (0.0000) | Acc: (93.00%) (22926/24448)
Epoch: 126 | Batch_idx: 200 |  Loss: (0.1816) |  Loss2: (0.0000) | Acc: (93.00%) (24137/25728)
Epoch: 126 | Batch_idx: 210 |  Loss: (0.1816) |  Loss2: (0.0000) | Acc: (93.00%) (25332/27008)
Epoch: 126 | Batch_idx: 220 |  Loss: (0.1807) |  Loss2: (0.0000) | Acc: (93.00%) (26540/28288)
Epoch: 126 | Batch_idx: 230 |  Loss: (0.1792) |  Loss2: (0.0000) | Acc: (93.00%) (27759/29568)
Epoch: 126 | Batch_idx: 240 |  Loss: (0.1798) |  Loss2: (0.0000) | Acc: (93.00%) (28958/30848)
Epoch: 126 | Batch_idx: 250 |  Loss: (0.1793) |  Loss2: (0.0000) | Acc: (93.00%) (30171/32128)
Epoch: 126 | Batch_idx: 260 |  Loss: (0.1788) |  Loss2: (0.0000) | Acc: (93.00%) (31372/33408)
Epoch: 126 | Batch_idx: 270 |  Loss: (0.1795) |  Loss2: (0.0000) | Acc: (93.00%) (32571/34688)
Epoch: 126 | Batch_idx: 280 |  Loss: (0.1794) |  Loss2: (0.0000) | Acc: (93.00%) (33762/35968)
Epoch: 126 | Batch_idx: 290 |  Loss: (0.1797) |  Loss2: (0.0000) | Acc: (93.00%) (34960/37248)
Epoch: 126 | Batch_idx: 300 |  Loss: (0.1794) |  Loss2: (0.0000) | Acc: (93.00%) (36162/38528)
Epoch: 126 | Batch_idx: 310 |  Loss: (0.1789) |  Loss2: (0.0000) | Acc: (93.00%) (37368/39808)
Epoch: 126 | Batch_idx: 320 |  Loss: (0.1782) |  Loss2: (0.0000) | Acc: (93.00%) (38579/41088)
Epoch: 126 | Batch_idx: 330 |  Loss: (0.1784) |  Loss2: (0.0000) | Acc: (93.00%) (39780/42368)
Epoch: 126 | Batch_idx: 340 |  Loss: (0.1781) |  Loss2: (0.0000) | Acc: (93.00%) (40983/43648)
Epoch: 126 | Batch_idx: 350 |  Loss: (0.1787) |  Loss2: (0.0000) | Acc: (93.00%) (42167/44928)
Epoch: 126 | Batch_idx: 360 |  Loss: (0.1790) |  Loss2: (0.0000) | Acc: (93.00%) (43370/46208)
Epoch: 126 | Batch_idx: 370 |  Loss: (0.1793) |  Loss2: (0.0000) | Acc: (93.00%) (44559/47488)
Epoch: 126 | Batch_idx: 380 |  Loss: (0.1793) |  Loss2: (0.0000) | Acc: (93.00%) (45771/48768)
Epoch: 126 | Batch_idx: 390 |  Loss: (0.1790) |  Loss2: (0.0000) | Acc: (93.00%) (46935/50000)
# TEST : Loss: (0.4000) | Acc: (87.00%) (8775/10000)
percent tensor([0.5160, 0.4840], device='cuda:0')
percent tensor([0.4653, 0.5347], device='cuda:0')
percent tensor([0.5613, 0.4387], device='cuda:0')
percent tensor([0.6055, 0.3945], device='cuda:0')
percent tensor([0.5827, 0.4173], device='cuda:0')
percent tensor([0.6415, 0.3585], device='cuda:0')
percent tensor([0.6448, 0.3552], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 127 | Batch_idx: 0 |  Loss: (0.1585) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 127 | Batch_idx: 10 |  Loss: (0.1963) |  Loss2: (0.0000) | Acc: (93.00%) (1311/1408)
Epoch: 127 | Batch_idx: 20 |  Loss: (0.1908) |  Loss2: (0.0000) | Acc: (93.00%) (2504/2688)
Epoch: 127 | Batch_idx: 30 |  Loss: (0.1862) |  Loss2: (0.0000) | Acc: (93.00%) (3705/3968)
Epoch: 127 | Batch_idx: 40 |  Loss: (0.1779) |  Loss2: (0.0000) | Acc: (93.00%) (4925/5248)
Epoch: 127 | Batch_idx: 50 |  Loss: (0.1773) |  Loss2: (0.0000) | Acc: (93.00%) (6130/6528)
Epoch: 127 | Batch_idx: 60 |  Loss: (0.1738) |  Loss2: (0.0000) | Acc: (94.00%) (7341/7808)
Epoch: 127 | Batch_idx: 70 |  Loss: (0.1729) |  Loss2: (0.0000) | Acc: (94.00%) (8553/9088)
Epoch: 127 | Batch_idx: 80 |  Loss: (0.1727) |  Loss2: (0.0000) | Acc: (94.00%) (9756/10368)
Epoch: 127 | Batch_idx: 90 |  Loss: (0.1735) |  Loss2: (0.0000) | Acc: (94.00%) (10958/11648)
Epoch: 127 | Batch_idx: 100 |  Loss: (0.1711) |  Loss2: (0.0000) | Acc: (94.00%) (12173/12928)
Epoch: 127 | Batch_idx: 110 |  Loss: (0.1702) |  Loss2: (0.0000) | Acc: (94.00%) (13375/14208)
Epoch: 127 | Batch_idx: 120 |  Loss: (0.1681) |  Loss2: (0.0000) | Acc: (94.00%) (14594/15488)
Epoch: 127 | Batch_idx: 130 |  Loss: (0.1666) |  Loss2: (0.0000) | Acc: (94.00%) (15805/16768)
Epoch: 127 | Batch_idx: 140 |  Loss: (0.1659) |  Loss2: (0.0000) | Acc: (94.00%) (17012/18048)
Epoch: 127 | Batch_idx: 150 |  Loss: (0.1665) |  Loss2: (0.0000) | Acc: (94.00%) (18216/19328)
Epoch: 127 | Batch_idx: 160 |  Loss: (0.1667) |  Loss2: (0.0000) | Acc: (94.00%) (19419/20608)
Epoch: 127 | Batch_idx: 170 |  Loss: (0.1670) |  Loss2: (0.0000) | Acc: (94.00%) (20625/21888)
Epoch: 127 | Batch_idx: 180 |  Loss: (0.1683) |  Loss2: (0.0000) | Acc: (94.00%) (21817/23168)
Epoch: 127 | Batch_idx: 190 |  Loss: (0.1677) |  Loss2: (0.0000) | Acc: (94.00%) (23029/24448)
Epoch: 127 | Batch_idx: 200 |  Loss: (0.1680) |  Loss2: (0.0000) | Acc: (94.00%) (24240/25728)
Epoch: 127 | Batch_idx: 210 |  Loss: (0.1672) |  Loss2: (0.0000) | Acc: (94.00%) (25455/27008)
Epoch: 127 | Batch_idx: 220 |  Loss: (0.1679) |  Loss2: (0.0000) | Acc: (94.00%) (26652/28288)
Epoch: 127 | Batch_idx: 230 |  Loss: (0.1683) |  Loss2: (0.0000) | Acc: (94.00%) (27851/29568)
Epoch: 127 | Batch_idx: 240 |  Loss: (0.1688) |  Loss2: (0.0000) | Acc: (94.00%) (29053/30848)
Epoch: 127 | Batch_idx: 250 |  Loss: (0.1682) |  Loss2: (0.0000) | Acc: (94.00%) (30264/32128)
Epoch: 127 | Batch_idx: 260 |  Loss: (0.1685) |  Loss2: (0.0000) | Acc: (94.00%) (31467/33408)
Epoch: 127 | Batch_idx: 270 |  Loss: (0.1694) |  Loss2: (0.0000) | Acc: (94.00%) (32670/34688)
Epoch: 127 | Batch_idx: 280 |  Loss: (0.1689) |  Loss2: (0.0000) | Acc: (94.00%) (33885/35968)
Epoch: 127 | Batch_idx: 290 |  Loss: (0.1681) |  Loss2: (0.0000) | Acc: (94.00%) (35105/37248)
Epoch: 127 | Batch_idx: 300 |  Loss: (0.1680) |  Loss2: (0.0000) | Acc: (94.00%) (36308/38528)
Epoch: 127 | Batch_idx: 310 |  Loss: (0.1693) |  Loss2: (0.0000) | Acc: (94.00%) (37488/39808)
Epoch: 127 | Batch_idx: 320 |  Loss: (0.1698) |  Loss2: (0.0000) | Acc: (94.00%) (38690/41088)
Epoch: 127 | Batch_idx: 330 |  Loss: (0.1694) |  Loss2: (0.0000) | Acc: (94.00%) (39898/42368)
Epoch: 127 | Batch_idx: 340 |  Loss: (0.1696) |  Loss2: (0.0000) | Acc: (94.00%) (41097/43648)
Epoch: 127 | Batch_idx: 350 |  Loss: (0.1697) |  Loss2: (0.0000) | Acc: (94.00%) (42306/44928)
Epoch: 127 | Batch_idx: 360 |  Loss: (0.1700) |  Loss2: (0.0000) | Acc: (94.00%) (43507/46208)
Epoch: 127 | Batch_idx: 370 |  Loss: (0.1697) |  Loss2: (0.0000) | Acc: (94.00%) (44704/47488)
Epoch: 127 | Batch_idx: 380 |  Loss: (0.1697) |  Loss2: (0.0000) | Acc: (94.00%) (45908/48768)
Epoch: 127 | Batch_idx: 390 |  Loss: (0.1694) |  Loss2: (0.0000) | Acc: (94.00%) (47074/50000)
# TEST : Loss: (0.3901) | Acc: (87.00%) (8799/10000)
percent tensor([0.5143, 0.4857], device='cuda:0')
percent tensor([0.4652, 0.5348], device='cuda:0')
percent tensor([0.5592, 0.4408], device='cuda:0')
percent tensor([0.6083, 0.3917], device='cuda:0')
percent tensor([0.5813, 0.4187], device='cuda:0')
percent tensor([0.6477, 0.3523], device='cuda:0')
percent tensor([0.6593, 0.3407], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 128 | Batch_idx: 0 |  Loss: (0.2300) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 128 | Batch_idx: 10 |  Loss: (0.1620) |  Loss2: (0.0000) | Acc: (94.00%) (1331/1408)
Epoch: 128 | Batch_idx: 20 |  Loss: (0.1602) |  Loss2: (0.0000) | Acc: (94.00%) (2544/2688)
Epoch: 128 | Batch_idx: 30 |  Loss: (0.1688) |  Loss2: (0.0000) | Acc: (94.00%) (3740/3968)
Epoch: 128 | Batch_idx: 40 |  Loss: (0.1701) |  Loss2: (0.0000) | Acc: (94.00%) (4945/5248)
Epoch: 128 | Batch_idx: 50 |  Loss: (0.1691) |  Loss2: (0.0000) | Acc: (94.00%) (6155/6528)
Epoch: 128 | Batch_idx: 60 |  Loss: (0.1673) |  Loss2: (0.0000) | Acc: (94.00%) (7366/7808)
Epoch: 128 | Batch_idx: 70 |  Loss: (0.1660) |  Loss2: (0.0000) | Acc: (94.00%) (8578/9088)
Epoch: 128 | Batch_idx: 80 |  Loss: (0.1646) |  Loss2: (0.0000) | Acc: (94.00%) (9787/10368)
Epoch: 128 | Batch_idx: 90 |  Loss: (0.1640) |  Loss2: (0.0000) | Acc: (94.00%) (11000/11648)
Epoch: 128 | Batch_idx: 100 |  Loss: (0.1649) |  Loss2: (0.0000) | Acc: (94.00%) (12207/12928)
Epoch: 128 | Batch_idx: 110 |  Loss: (0.1642) |  Loss2: (0.0000) | Acc: (94.00%) (13423/14208)
Epoch: 128 | Batch_idx: 120 |  Loss: (0.1651) |  Loss2: (0.0000) | Acc: (94.00%) (14619/15488)
Epoch: 128 | Batch_idx: 130 |  Loss: (0.1649) |  Loss2: (0.0000) | Acc: (94.00%) (15823/16768)
Epoch: 128 | Batch_idx: 140 |  Loss: (0.1658) |  Loss2: (0.0000) | Acc: (94.00%) (17031/18048)
Epoch: 128 | Batch_idx: 150 |  Loss: (0.1659) |  Loss2: (0.0000) | Acc: (94.00%) (18234/19328)
Epoch: 128 | Batch_idx: 160 |  Loss: (0.1643) |  Loss2: (0.0000) | Acc: (94.00%) (19452/20608)
Epoch: 128 | Batch_idx: 170 |  Loss: (0.1644) |  Loss2: (0.0000) | Acc: (94.00%) (20668/21888)
Epoch: 128 | Batch_idx: 180 |  Loss: (0.1657) |  Loss2: (0.0000) | Acc: (94.00%) (21862/23168)
Epoch: 128 | Batch_idx: 190 |  Loss: (0.1655) |  Loss2: (0.0000) | Acc: (94.00%) (23077/24448)
Epoch: 128 | Batch_idx: 200 |  Loss: (0.1654) |  Loss2: (0.0000) | Acc: (94.00%) (24284/25728)
Epoch: 128 | Batch_idx: 210 |  Loss: (0.1643) |  Loss2: (0.0000) | Acc: (94.00%) (25503/27008)
Epoch: 128 | Batch_idx: 220 |  Loss: (0.1648) |  Loss2: (0.0000) | Acc: (94.00%) (26705/28288)
Epoch: 128 | Batch_idx: 230 |  Loss: (0.1646) |  Loss2: (0.0000) | Acc: (94.00%) (27917/29568)
Epoch: 128 | Batch_idx: 240 |  Loss: (0.1647) |  Loss2: (0.0000) | Acc: (94.00%) (29130/30848)
Epoch: 128 | Batch_idx: 250 |  Loss: (0.1649) |  Loss2: (0.0000) | Acc: (94.00%) (30337/32128)
Epoch: 128 | Batch_idx: 260 |  Loss: (0.1648) |  Loss2: (0.0000) | Acc: (94.00%) (31549/33408)
Epoch: 128 | Batch_idx: 270 |  Loss: (0.1642) |  Loss2: (0.0000) | Acc: (94.00%) (32763/34688)
Epoch: 128 | Batch_idx: 280 |  Loss: (0.1641) |  Loss2: (0.0000) | Acc: (94.00%) (33974/35968)
Epoch: 128 | Batch_idx: 290 |  Loss: (0.1640) |  Loss2: (0.0000) | Acc: (94.00%) (35184/37248)
Epoch: 128 | Batch_idx: 300 |  Loss: (0.1639) |  Loss2: (0.0000) | Acc: (94.00%) (36388/38528)
Epoch: 128 | Batch_idx: 310 |  Loss: (0.1639) |  Loss2: (0.0000) | Acc: (94.00%) (37604/39808)
Epoch: 128 | Batch_idx: 320 |  Loss: (0.1635) |  Loss2: (0.0000) | Acc: (94.00%) (38825/41088)
Epoch: 128 | Batch_idx: 330 |  Loss: (0.1627) |  Loss2: (0.0000) | Acc: (94.00%) (40046/42368)
Epoch: 128 | Batch_idx: 340 |  Loss: (0.1631) |  Loss2: (0.0000) | Acc: (94.00%) (41243/43648)
Epoch: 128 | Batch_idx: 350 |  Loss: (0.1633) |  Loss2: (0.0000) | Acc: (94.00%) (42439/44928)
Epoch: 128 | Batch_idx: 360 |  Loss: (0.1634) |  Loss2: (0.0000) | Acc: (94.00%) (43650/46208)
Epoch: 128 | Batch_idx: 370 |  Loss: (0.1638) |  Loss2: (0.0000) | Acc: (94.00%) (44864/47488)
Epoch: 128 | Batch_idx: 380 |  Loss: (0.1635) |  Loss2: (0.0000) | Acc: (94.00%) (46076/48768)
Epoch: 128 | Batch_idx: 390 |  Loss: (0.1635) |  Loss2: (0.0000) | Acc: (94.00%) (47235/50000)
# TEST : Loss: (0.3864) | Acc: (88.00%) (8812/10000)
percent tensor([0.5135, 0.4865], device='cuda:0')
percent tensor([0.4649, 0.5351], device='cuda:0')
percent tensor([0.5615, 0.4385], device='cuda:0')
percent tensor([0.6055, 0.3945], device='cuda:0')
percent tensor([0.5796, 0.4204], device='cuda:0')
percent tensor([0.6459, 0.3541], device='cuda:0')
percent tensor([0.6630, 0.3370], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 129 | Batch_idx: 0 |  Loss: (0.1406) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 129 | Batch_idx: 10 |  Loss: (0.1464) |  Loss2: (0.0000) | Acc: (95.00%) (1341/1408)
Epoch: 129 | Batch_idx: 20 |  Loss: (0.1479) |  Loss2: (0.0000) | Acc: (94.00%) (2553/2688)
Epoch: 129 | Batch_idx: 30 |  Loss: (0.1490) |  Loss2: (0.0000) | Acc: (94.00%) (3762/3968)
Epoch: 129 | Batch_idx: 40 |  Loss: (0.1523) |  Loss2: (0.0000) | Acc: (94.00%) (4968/5248)
Epoch: 129 | Batch_idx: 50 |  Loss: (0.1580) |  Loss2: (0.0000) | Acc: (94.00%) (6166/6528)
Epoch: 129 | Batch_idx: 60 |  Loss: (0.1571) |  Loss2: (0.0000) | Acc: (94.00%) (7386/7808)
Epoch: 129 | Batch_idx: 70 |  Loss: (0.1549) |  Loss2: (0.0000) | Acc: (94.00%) (8599/9088)
Epoch: 129 | Batch_idx: 80 |  Loss: (0.1529) |  Loss2: (0.0000) | Acc: (94.00%) (9824/10368)
Epoch: 129 | Batch_idx: 90 |  Loss: (0.1532) |  Loss2: (0.0000) | Acc: (94.00%) (11044/11648)
Epoch: 129 | Batch_idx: 100 |  Loss: (0.1551) |  Loss2: (0.0000) | Acc: (94.00%) (12246/12928)
Epoch: 129 | Batch_idx: 110 |  Loss: (0.1560) |  Loss2: (0.0000) | Acc: (94.00%) (13458/14208)
Epoch: 129 | Batch_idx: 120 |  Loss: (0.1560) |  Loss2: (0.0000) | Acc: (94.00%) (14675/15488)
Epoch: 129 | Batch_idx: 130 |  Loss: (0.1555) |  Loss2: (0.0000) | Acc: (94.00%) (15897/16768)
Epoch: 129 | Batch_idx: 140 |  Loss: (0.1557) |  Loss2: (0.0000) | Acc: (94.00%) (17109/18048)
Epoch: 129 | Batch_idx: 150 |  Loss: (0.1552) |  Loss2: (0.0000) | Acc: (94.00%) (18328/19328)
Epoch: 129 | Batch_idx: 160 |  Loss: (0.1556) |  Loss2: (0.0000) | Acc: (94.00%) (19539/20608)
Epoch: 129 | Batch_idx: 170 |  Loss: (0.1552) |  Loss2: (0.0000) | Acc: (94.00%) (20759/21888)
Epoch: 129 | Batch_idx: 180 |  Loss: (0.1569) |  Loss2: (0.0000) | Acc: (94.00%) (21954/23168)
Epoch: 129 | Batch_idx: 190 |  Loss: (0.1582) |  Loss2: (0.0000) | Acc: (94.00%) (23162/24448)
Epoch: 129 | Batch_idx: 200 |  Loss: (0.1597) |  Loss2: (0.0000) | Acc: (94.00%) (24370/25728)
Epoch: 129 | Batch_idx: 210 |  Loss: (0.1606) |  Loss2: (0.0000) | Acc: (94.00%) (25566/27008)
Epoch: 129 | Batch_idx: 220 |  Loss: (0.1601) |  Loss2: (0.0000) | Acc: (94.00%) (26786/28288)
Epoch: 129 | Batch_idx: 230 |  Loss: (0.1598) |  Loss2: (0.0000) | Acc: (94.00%) (28000/29568)
Epoch: 129 | Batch_idx: 240 |  Loss: (0.1603) |  Loss2: (0.0000) | Acc: (94.00%) (29208/30848)
Epoch: 129 | Batch_idx: 250 |  Loss: (0.1604) |  Loss2: (0.0000) | Acc: (94.00%) (30417/32128)
Epoch: 129 | Batch_idx: 260 |  Loss: (0.1606) |  Loss2: (0.0000) | Acc: (94.00%) (31625/33408)
Epoch: 129 | Batch_idx: 270 |  Loss: (0.1604) |  Loss2: (0.0000) | Acc: (94.00%) (32840/34688)
Epoch: 129 | Batch_idx: 280 |  Loss: (0.1610) |  Loss2: (0.0000) | Acc: (94.00%) (34040/35968)
Epoch: 129 | Batch_idx: 290 |  Loss: (0.1615) |  Loss2: (0.0000) | Acc: (94.00%) (35241/37248)
Epoch: 129 | Batch_idx: 300 |  Loss: (0.1617) |  Loss2: (0.0000) | Acc: (94.00%) (36449/38528)
Epoch: 129 | Batch_idx: 310 |  Loss: (0.1620) |  Loss2: (0.0000) | Acc: (94.00%) (37645/39808)
Epoch: 129 | Batch_idx: 320 |  Loss: (0.1614) |  Loss2: (0.0000) | Acc: (94.00%) (38868/41088)
Epoch: 129 | Batch_idx: 330 |  Loss: (0.1611) |  Loss2: (0.0000) | Acc: (94.00%) (40085/42368)
Epoch: 129 | Batch_idx: 340 |  Loss: (0.1614) |  Loss2: (0.0000) | Acc: (94.00%) (41288/43648)
Epoch: 129 | Batch_idx: 350 |  Loss: (0.1617) |  Loss2: (0.0000) | Acc: (94.00%) (42498/44928)
Epoch: 129 | Batch_idx: 360 |  Loss: (0.1619) |  Loss2: (0.0000) | Acc: (94.00%) (43709/46208)
Epoch: 129 | Batch_idx: 370 |  Loss: (0.1618) |  Loss2: (0.0000) | Acc: (94.00%) (44928/47488)
Epoch: 129 | Batch_idx: 380 |  Loss: (0.1622) |  Loss2: (0.0000) | Acc: (94.00%) (46134/48768)
Epoch: 129 | Batch_idx: 390 |  Loss: (0.1622) |  Loss2: (0.0000) | Acc: (94.00%) (47291/50000)
# TEST : Loss: (0.3804) | Acc: (88.00%) (8842/10000)
percent tensor([0.5134, 0.4866], device='cuda:0')
percent tensor([0.4634, 0.5366], device='cuda:0')
percent tensor([0.5601, 0.4399], device='cuda:0')
percent tensor([0.6059, 0.3941], device='cuda:0')
percent tensor([0.5817, 0.4183], device='cuda:0')
percent tensor([0.6539, 0.3461], device='cuda:0')
percent tensor([0.6658, 0.3342], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 130 | Batch_idx: 0 |  Loss: (0.1485) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 130 | Batch_idx: 10 |  Loss: (0.1392) |  Loss2: (0.0000) | Acc: (95.00%) (1345/1408)
Epoch: 130 | Batch_idx: 20 |  Loss: (0.1548) |  Loss2: (0.0000) | Acc: (94.00%) (2550/2688)
Epoch: 130 | Batch_idx: 30 |  Loss: (0.1551) |  Loss2: (0.0000) | Acc: (94.00%) (3760/3968)
Epoch: 130 | Batch_idx: 40 |  Loss: (0.1545) |  Loss2: (0.0000) | Acc: (94.00%) (4969/5248)
Epoch: 130 | Batch_idx: 50 |  Loss: (0.1551) |  Loss2: (0.0000) | Acc: (94.00%) (6177/6528)
Epoch: 130 | Batch_idx: 60 |  Loss: (0.1570) |  Loss2: (0.0000) | Acc: (94.00%) (7393/7808)
Epoch: 130 | Batch_idx: 70 |  Loss: (0.1586) |  Loss2: (0.0000) | Acc: (94.00%) (8591/9088)
Epoch: 130 | Batch_idx: 80 |  Loss: (0.1610) |  Loss2: (0.0000) | Acc: (94.00%) (9791/10368)
Epoch: 130 | Batch_idx: 90 |  Loss: (0.1623) |  Loss2: (0.0000) | Acc: (94.00%) (10994/11648)
Epoch: 130 | Batch_idx: 100 |  Loss: (0.1596) |  Loss2: (0.0000) | Acc: (94.00%) (12212/12928)
Epoch: 130 | Batch_idx: 110 |  Loss: (0.1641) |  Loss2: (0.0000) | Acc: (94.00%) (13399/14208)
Epoch: 130 | Batch_idx: 120 |  Loss: (0.1650) |  Loss2: (0.0000) | Acc: (94.00%) (14605/15488)
Epoch: 130 | Batch_idx: 130 |  Loss: (0.1672) |  Loss2: (0.0000) | Acc: (94.00%) (15806/16768)
Epoch: 130 | Batch_idx: 140 |  Loss: (0.1686) |  Loss2: (0.0000) | Acc: (94.00%) (17007/18048)
Epoch: 130 | Batch_idx: 150 |  Loss: (0.1697) |  Loss2: (0.0000) | Acc: (94.00%) (18210/19328)
Epoch: 130 | Batch_idx: 160 |  Loss: (0.1708) |  Loss2: (0.0000) | Acc: (94.00%) (19403/20608)
Epoch: 130 | Batch_idx: 170 |  Loss: (0.1728) |  Loss2: (0.0000) | Acc: (94.00%) (20597/21888)
Epoch: 130 | Batch_idx: 180 |  Loss: (0.1729) |  Loss2: (0.0000) | Acc: (94.00%) (21797/23168)
Epoch: 130 | Batch_idx: 190 |  Loss: (0.1720) |  Loss2: (0.0000) | Acc: (94.00%) (23011/24448)
Epoch: 130 | Batch_idx: 200 |  Loss: (0.1731) |  Loss2: (0.0000) | Acc: (94.00%) (24209/25728)
Epoch: 130 | Batch_idx: 210 |  Loss: (0.1739) |  Loss2: (0.0000) | Acc: (94.00%) (25408/27008)
Epoch: 130 | Batch_idx: 220 |  Loss: (0.1733) |  Loss2: (0.0000) | Acc: (94.00%) (26615/28288)
Epoch: 130 | Batch_idx: 230 |  Loss: (0.1729) |  Loss2: (0.0000) | Acc: (94.00%) (27824/29568)
Epoch: 130 | Batch_idx: 240 |  Loss: (0.1735) |  Loss2: (0.0000) | Acc: (94.00%) (29014/30848)
Epoch: 130 | Batch_idx: 250 |  Loss: (0.1743) |  Loss2: (0.0000) | Acc: (94.00%) (30204/32128)
Epoch: 130 | Batch_idx: 260 |  Loss: (0.1744) |  Loss2: (0.0000) | Acc: (94.00%) (31412/33408)
Epoch: 130 | Batch_idx: 270 |  Loss: (0.1735) |  Loss2: (0.0000) | Acc: (94.00%) (32623/34688)
Epoch: 130 | Batch_idx: 280 |  Loss: (0.1737) |  Loss2: (0.0000) | Acc: (94.00%) (33818/35968)
Epoch: 130 | Batch_idx: 290 |  Loss: (0.1739) |  Loss2: (0.0000) | Acc: (93.00%) (35010/37248)
Epoch: 130 | Batch_idx: 300 |  Loss: (0.1745) |  Loss2: (0.0000) | Acc: (93.00%) (36207/38528)
Epoch: 130 | Batch_idx: 310 |  Loss: (0.1743) |  Loss2: (0.0000) | Acc: (93.00%) (37419/39808)
Epoch: 130 | Batch_idx: 320 |  Loss: (0.1741) |  Loss2: (0.0000) | Acc: (94.00%) (38623/41088)
Epoch: 130 | Batch_idx: 330 |  Loss: (0.1747) |  Loss2: (0.0000) | Acc: (93.00%) (39815/42368)
Epoch: 130 | Batch_idx: 340 |  Loss: (0.1747) |  Loss2: (0.0000) | Acc: (93.00%) (41015/43648)
Epoch: 130 | Batch_idx: 350 |  Loss: (0.1745) |  Loss2: (0.0000) | Acc: (93.00%) (42220/44928)
Epoch: 130 | Batch_idx: 360 |  Loss: (0.1742) |  Loss2: (0.0000) | Acc: (93.00%) (43422/46208)
Epoch: 130 | Batch_idx: 370 |  Loss: (0.1737) |  Loss2: (0.0000) | Acc: (94.00%) (44642/47488)
Epoch: 130 | Batch_idx: 380 |  Loss: (0.1736) |  Loss2: (0.0000) | Acc: (94.00%) (45849/48768)
Epoch: 130 | Batch_idx: 390 |  Loss: (0.1738) |  Loss2: (0.0000) | Acc: (94.00%) (47009/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_130.pth.tar'
# TEST : Loss: (0.4217) | Acc: (87.00%) (8734/10000)
percent tensor([0.5130, 0.4870], device='cuda:0')
percent tensor([0.4630, 0.5370], device='cuda:0')
percent tensor([0.5604, 0.4396], device='cuda:0')
percent tensor([0.6067, 0.3933], device='cuda:0')
percent tensor([0.5821, 0.4179], device='cuda:0')
percent tensor([0.6559, 0.3441], device='cuda:0')
percent tensor([0.6640, 0.3360], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(182.2843, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(816.2087, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(809.9854, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1511.8651, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(497.6857, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2251.1531, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4267.2109, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1379.5607, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6212.1899, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11775.9512, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3884.4653, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16443.7930, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 131 | Batch_idx: 0 |  Loss: (0.1523) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 131 | Batch_idx: 10 |  Loss: (0.1614) |  Loss2: (0.0000) | Acc: (94.00%) (1336/1408)
Epoch: 131 | Batch_idx: 20 |  Loss: (0.1619) |  Loss2: (0.0000) | Acc: (94.00%) (2545/2688)
Epoch: 131 | Batch_idx: 30 |  Loss: (0.1600) |  Loss2: (0.0000) | Acc: (94.00%) (3759/3968)
Epoch: 131 | Batch_idx: 40 |  Loss: (0.1601) |  Loss2: (0.0000) | Acc: (94.00%) (4970/5248)
Epoch: 131 | Batch_idx: 50 |  Loss: (0.1559) |  Loss2: (0.0000) | Acc: (94.00%) (6193/6528)
Epoch: 131 | Batch_idx: 60 |  Loss: (0.1561) |  Loss2: (0.0000) | Acc: (94.00%) (7412/7808)
Epoch: 131 | Batch_idx: 70 |  Loss: (0.1588) |  Loss2: (0.0000) | Acc: (94.00%) (8622/9088)
Epoch: 131 | Batch_idx: 80 |  Loss: (0.1628) |  Loss2: (0.0000) | Acc: (94.00%) (9818/10368)
Epoch: 131 | Batch_idx: 90 |  Loss: (0.1627) |  Loss2: (0.0000) | Acc: (94.00%) (11029/11648)
Epoch: 131 | Batch_idx: 100 |  Loss: (0.1639) |  Loss2: (0.0000) | Acc: (94.00%) (12233/12928)
Epoch: 131 | Batch_idx: 110 |  Loss: (0.1655) |  Loss2: (0.0000) | Acc: (94.00%) (13428/14208)
Epoch: 131 | Batch_idx: 120 |  Loss: (0.1666) |  Loss2: (0.0000) | Acc: (94.00%) (14629/15488)
Epoch: 131 | Batch_idx: 130 |  Loss: (0.1654) |  Loss2: (0.0000) | Acc: (94.00%) (15844/16768)
Epoch: 131 | Batch_idx: 140 |  Loss: (0.1644) |  Loss2: (0.0000) | Acc: (94.00%) (17058/18048)
Epoch: 131 | Batch_idx: 150 |  Loss: (0.1640) |  Loss2: (0.0000) | Acc: (94.00%) (18268/19328)
Epoch: 131 | Batch_idx: 160 |  Loss: (0.1658) |  Loss2: (0.0000) | Acc: (94.00%) (19470/20608)
Epoch: 131 | Batch_idx: 170 |  Loss: (0.1652) |  Loss2: (0.0000) | Acc: (94.00%) (20685/21888)
Epoch: 131 | Batch_idx: 180 |  Loss: (0.1647) |  Loss2: (0.0000) | Acc: (94.00%) (21890/23168)
Epoch: 131 | Batch_idx: 190 |  Loss: (0.1631) |  Loss2: (0.0000) | Acc: (94.00%) (23114/24448)
Epoch: 131 | Batch_idx: 200 |  Loss: (0.1618) |  Loss2: (0.0000) | Acc: (94.00%) (24334/25728)
Epoch: 131 | Batch_idx: 210 |  Loss: (0.1629) |  Loss2: (0.0000) | Acc: (94.00%) (25531/27008)
Epoch: 131 | Batch_idx: 220 |  Loss: (0.1633) |  Loss2: (0.0000) | Acc: (94.00%) (26740/28288)
Epoch: 131 | Batch_idx: 230 |  Loss: (0.1636) |  Loss2: (0.0000) | Acc: (94.00%) (27938/29568)
Epoch: 131 | Batch_idx: 240 |  Loss: (0.1650) |  Loss2: (0.0000) | Acc: (94.00%) (29126/30848)
Epoch: 131 | Batch_idx: 250 |  Loss: (0.1659) |  Loss2: (0.0000) | Acc: (94.00%) (30324/32128)
Epoch: 131 | Batch_idx: 260 |  Loss: (0.1663) |  Loss2: (0.0000) | Acc: (94.00%) (31525/33408)
Epoch: 131 | Batch_idx: 270 |  Loss: (0.1663) |  Loss2: (0.0000) | Acc: (94.00%) (32731/34688)
Epoch: 131 | Batch_idx: 280 |  Loss: (0.1671) |  Loss2: (0.0000) | Acc: (94.00%) (33925/35968)
Epoch: 131 | Batch_idx: 290 |  Loss: (0.1675) |  Loss2: (0.0000) | Acc: (94.00%) (35130/37248)
Epoch: 131 | Batch_idx: 300 |  Loss: (0.1673) |  Loss2: (0.0000) | Acc: (94.00%) (36343/38528)
Epoch: 131 | Batch_idx: 310 |  Loss: (0.1677) |  Loss2: (0.0000) | Acc: (94.00%) (37550/39808)
Epoch: 131 | Batch_idx: 320 |  Loss: (0.1677) |  Loss2: (0.0000) | Acc: (94.00%) (38762/41088)
Epoch: 131 | Batch_idx: 330 |  Loss: (0.1676) |  Loss2: (0.0000) | Acc: (94.00%) (39965/42368)
Epoch: 131 | Batch_idx: 340 |  Loss: (0.1681) |  Loss2: (0.0000) | Acc: (94.00%) (41160/43648)
Epoch: 131 | Batch_idx: 350 |  Loss: (0.1682) |  Loss2: (0.0000) | Acc: (94.00%) (42366/44928)
Epoch: 131 | Batch_idx: 360 |  Loss: (0.1682) |  Loss2: (0.0000) | Acc: (94.00%) (43574/46208)
Epoch: 131 | Batch_idx: 370 |  Loss: (0.1682) |  Loss2: (0.0000) | Acc: (94.00%) (44785/47488)
Epoch: 131 | Batch_idx: 380 |  Loss: (0.1676) |  Loss2: (0.0000) | Acc: (94.00%) (46010/48768)
Epoch: 131 | Batch_idx: 390 |  Loss: (0.1672) |  Loss2: (0.0000) | Acc: (94.00%) (47178/50000)
# TEST : Loss: (0.4490) | Acc: (86.00%) (8604/10000)
percent tensor([0.5130, 0.4870], device='cuda:0')
percent tensor([0.4633, 0.5367], device='cuda:0')
percent tensor([0.5605, 0.4395], device='cuda:0')
percent tensor([0.6060, 0.3940], device='cuda:0')
percent tensor([0.5817, 0.4183], device='cuda:0')
percent tensor([0.6529, 0.3471], device='cuda:0')
percent tensor([0.6657, 0.3343], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 132 | Batch_idx: 0 |  Loss: (0.1229) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 132 | Batch_idx: 10 |  Loss: (0.1388) |  Loss2: (0.0000) | Acc: (95.00%) (1351/1408)
Epoch: 132 | Batch_idx: 20 |  Loss: (0.1525) |  Loss2: (0.0000) | Acc: (95.00%) (2554/2688)
Epoch: 132 | Batch_idx: 30 |  Loss: (0.1513) |  Loss2: (0.0000) | Acc: (95.00%) (3773/3968)
Epoch: 132 | Batch_idx: 40 |  Loss: (0.1515) |  Loss2: (0.0000) | Acc: (95.00%) (4987/5248)
Epoch: 132 | Batch_idx: 50 |  Loss: (0.1559) |  Loss2: (0.0000) | Acc: (94.00%) (6190/6528)
Epoch: 132 | Batch_idx: 60 |  Loss: (0.1566) |  Loss2: (0.0000) | Acc: (94.00%) (7408/7808)
Epoch: 132 | Batch_idx: 70 |  Loss: (0.1549) |  Loss2: (0.0000) | Acc: (94.00%) (8620/9088)
Epoch: 132 | Batch_idx: 80 |  Loss: (0.1560) |  Loss2: (0.0000) | Acc: (94.00%) (9831/10368)
Epoch: 132 | Batch_idx: 90 |  Loss: (0.1580) |  Loss2: (0.0000) | Acc: (94.00%) (11028/11648)
Epoch: 132 | Batch_idx: 100 |  Loss: (0.1553) |  Loss2: (0.0000) | Acc: (94.00%) (12255/12928)
Epoch: 132 | Batch_idx: 110 |  Loss: (0.1581) |  Loss2: (0.0000) | Acc: (94.00%) (13455/14208)
Epoch: 132 | Batch_idx: 120 |  Loss: (0.1584) |  Loss2: (0.0000) | Acc: (94.00%) (14668/15488)
Epoch: 132 | Batch_idx: 130 |  Loss: (0.1588) |  Loss2: (0.0000) | Acc: (94.00%) (15876/16768)
Epoch: 132 | Batch_idx: 140 |  Loss: (0.1600) |  Loss2: (0.0000) | Acc: (94.00%) (17076/18048)
Epoch: 132 | Batch_idx: 150 |  Loss: (0.1595) |  Loss2: (0.0000) | Acc: (94.00%) (18284/19328)
Epoch: 132 | Batch_idx: 160 |  Loss: (0.1576) |  Loss2: (0.0000) | Acc: (94.00%) (19502/20608)
Epoch: 132 | Batch_idx: 170 |  Loss: (0.1579) |  Loss2: (0.0000) | Acc: (94.00%) (20712/21888)
Epoch: 132 | Batch_idx: 180 |  Loss: (0.1593) |  Loss2: (0.0000) | Acc: (94.00%) (21910/23168)
Epoch: 132 | Batch_idx: 190 |  Loss: (0.1576) |  Loss2: (0.0000) | Acc: (94.00%) (23139/24448)
Epoch: 132 | Batch_idx: 200 |  Loss: (0.1579) |  Loss2: (0.0000) | Acc: (94.00%) (24342/25728)
Epoch: 132 | Batch_idx: 210 |  Loss: (0.1581) |  Loss2: (0.0000) | Acc: (94.00%) (25543/27008)
Epoch: 132 | Batch_idx: 220 |  Loss: (0.1587) |  Loss2: (0.0000) | Acc: (94.00%) (26751/28288)
Epoch: 132 | Batch_idx: 230 |  Loss: (0.1587) |  Loss2: (0.0000) | Acc: (94.00%) (27961/29568)
Epoch: 132 | Batch_idx: 240 |  Loss: (0.1586) |  Loss2: (0.0000) | Acc: (94.00%) (29167/30848)
Epoch: 132 | Batch_idx: 250 |  Loss: (0.1576) |  Loss2: (0.0000) | Acc: (94.00%) (30395/32128)
Epoch: 132 | Batch_idx: 260 |  Loss: (0.1582) |  Loss2: (0.0000) | Acc: (94.00%) (31599/33408)
Epoch: 132 | Batch_idx: 270 |  Loss: (0.1593) |  Loss2: (0.0000) | Acc: (94.00%) (32800/34688)
Epoch: 132 | Batch_idx: 280 |  Loss: (0.1593) |  Loss2: (0.0000) | Acc: (94.00%) (34006/35968)
Epoch: 132 | Batch_idx: 290 |  Loss: (0.1598) |  Loss2: (0.0000) | Acc: (94.00%) (35215/37248)
Epoch: 132 | Batch_idx: 300 |  Loss: (0.1606) |  Loss2: (0.0000) | Acc: (94.00%) (36407/38528)
Epoch: 132 | Batch_idx: 310 |  Loss: (0.1612) |  Loss2: (0.0000) | Acc: (94.00%) (37609/39808)
Epoch: 132 | Batch_idx: 320 |  Loss: (0.1620) |  Loss2: (0.0000) | Acc: (94.00%) (38799/41088)
Epoch: 132 | Batch_idx: 330 |  Loss: (0.1628) |  Loss2: (0.0000) | Acc: (94.00%) (39998/42368)
Epoch: 132 | Batch_idx: 340 |  Loss: (0.1630) |  Loss2: (0.0000) | Acc: (94.00%) (41208/43648)
Epoch: 132 | Batch_idx: 350 |  Loss: (0.1637) |  Loss2: (0.0000) | Acc: (94.00%) (42406/44928)
Epoch: 132 | Batch_idx: 360 |  Loss: (0.1635) |  Loss2: (0.0000) | Acc: (94.00%) (43619/46208)
Epoch: 132 | Batch_idx: 370 |  Loss: (0.1645) |  Loss2: (0.0000) | Acc: (94.00%) (44807/47488)
Epoch: 132 | Batch_idx: 380 |  Loss: (0.1644) |  Loss2: (0.0000) | Acc: (94.00%) (46016/48768)
Epoch: 132 | Batch_idx: 390 |  Loss: (0.1643) |  Loss2: (0.0000) | Acc: (94.00%) (47179/50000)
# TEST : Loss: (0.4461) | Acc: (86.00%) (8674/10000)
percent tensor([0.5130, 0.4870], device='cuda:0')
percent tensor([0.4634, 0.5366], device='cuda:0')
percent tensor([0.5599, 0.4401], device='cuda:0')
percent tensor([0.6061, 0.3939], device='cuda:0')
percent tensor([0.5815, 0.4185], device='cuda:0')
percent tensor([0.6538, 0.3462], device='cuda:0')
percent tensor([0.6657, 0.3343], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 133 | Batch_idx: 0 |  Loss: (0.1755) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 133 | Batch_idx: 10 |  Loss: (0.1614) |  Loss2: (0.0000) | Acc: (94.00%) (1333/1408)
Epoch: 133 | Batch_idx: 20 |  Loss: (0.1637) |  Loss2: (0.0000) | Acc: (94.00%) (2531/2688)
Epoch: 133 | Batch_idx: 30 |  Loss: (0.1630) |  Loss2: (0.0000) | Acc: (94.00%) (3738/3968)
Epoch: 133 | Batch_idx: 40 |  Loss: (0.1712) |  Loss2: (0.0000) | Acc: (93.00%) (4930/5248)
Epoch: 133 | Batch_idx: 50 |  Loss: (0.1699) |  Loss2: (0.0000) | Acc: (94.00%) (6139/6528)
Epoch: 133 | Batch_idx: 60 |  Loss: (0.1655) |  Loss2: (0.0000) | Acc: (94.00%) (7367/7808)
Epoch: 133 | Batch_idx: 70 |  Loss: (0.1648) |  Loss2: (0.0000) | Acc: (94.00%) (8586/9088)
Epoch: 133 | Batch_idx: 80 |  Loss: (0.1642) |  Loss2: (0.0000) | Acc: (94.00%) (9788/10368)
Epoch: 133 | Batch_idx: 90 |  Loss: (0.1660) |  Loss2: (0.0000) | Acc: (94.00%) (10982/11648)
Epoch: 133 | Batch_idx: 100 |  Loss: (0.1662) |  Loss2: (0.0000) | Acc: (94.00%) (12180/12928)
Epoch: 133 | Batch_idx: 110 |  Loss: (0.1653) |  Loss2: (0.0000) | Acc: (94.00%) (13387/14208)
Epoch: 133 | Batch_idx: 120 |  Loss: (0.1629) |  Loss2: (0.0000) | Acc: (94.00%) (14615/15488)
Epoch: 133 | Batch_idx: 130 |  Loss: (0.1619) |  Loss2: (0.0000) | Acc: (94.00%) (15827/16768)
Epoch: 133 | Batch_idx: 140 |  Loss: (0.1627) |  Loss2: (0.0000) | Acc: (94.00%) (17033/18048)
Epoch: 133 | Batch_idx: 150 |  Loss: (0.1630) |  Loss2: (0.0000) | Acc: (94.00%) (18235/19328)
Epoch: 133 | Batch_idx: 160 |  Loss: (0.1628) |  Loss2: (0.0000) | Acc: (94.00%) (19447/20608)
Epoch: 133 | Batch_idx: 170 |  Loss: (0.1623) |  Loss2: (0.0000) | Acc: (94.00%) (20657/21888)
Epoch: 133 | Batch_idx: 180 |  Loss: (0.1622) |  Loss2: (0.0000) | Acc: (94.00%) (21867/23168)
Epoch: 133 | Batch_idx: 190 |  Loss: (0.1627) |  Loss2: (0.0000) | Acc: (94.00%) (23073/24448)
Epoch: 133 | Batch_idx: 200 |  Loss: (0.1632) |  Loss2: (0.0000) | Acc: (94.00%) (24266/25728)
Epoch: 133 | Batch_idx: 210 |  Loss: (0.1634) |  Loss2: (0.0000) | Acc: (94.00%) (25472/27008)
Epoch: 133 | Batch_idx: 220 |  Loss: (0.1635) |  Loss2: (0.0000) | Acc: (94.00%) (26679/28288)
Epoch: 133 | Batch_idx: 230 |  Loss: (0.1631) |  Loss2: (0.0000) | Acc: (94.00%) (27886/29568)
Epoch: 133 | Batch_idx: 240 |  Loss: (0.1633) |  Loss2: (0.0000) | Acc: (94.00%) (29087/30848)
Epoch: 133 | Batch_idx: 250 |  Loss: (0.1644) |  Loss2: (0.0000) | Acc: (94.00%) (30279/32128)
Epoch: 133 | Batch_idx: 260 |  Loss: (0.1637) |  Loss2: (0.0000) | Acc: (94.00%) (31498/33408)
Epoch: 133 | Batch_idx: 270 |  Loss: (0.1636) |  Loss2: (0.0000) | Acc: (94.00%) (32712/34688)
Epoch: 133 | Batch_idx: 280 |  Loss: (0.1639) |  Loss2: (0.0000) | Acc: (94.00%) (33916/35968)
Epoch: 133 | Batch_idx: 290 |  Loss: (0.1627) |  Loss2: (0.0000) | Acc: (94.00%) (35139/37248)
Epoch: 133 | Batch_idx: 300 |  Loss: (0.1623) |  Loss2: (0.0000) | Acc: (94.00%) (36356/38528)
Epoch: 133 | Batch_idx: 310 |  Loss: (0.1620) |  Loss2: (0.0000) | Acc: (94.00%) (37571/39808)
Epoch: 133 | Batch_idx: 320 |  Loss: (0.1616) |  Loss2: (0.0000) | Acc: (94.00%) (38787/41088)
Epoch: 133 | Batch_idx: 330 |  Loss: (0.1613) |  Loss2: (0.0000) | Acc: (94.00%) (39994/42368)
Epoch: 133 | Batch_idx: 340 |  Loss: (0.1614) |  Loss2: (0.0000) | Acc: (94.00%) (41200/43648)
Epoch: 133 | Batch_idx: 350 |  Loss: (0.1617) |  Loss2: (0.0000) | Acc: (94.00%) (42404/44928)
Epoch: 133 | Batch_idx: 360 |  Loss: (0.1622) |  Loss2: (0.0000) | Acc: (94.00%) (43598/46208)
Epoch: 133 | Batch_idx: 370 |  Loss: (0.1630) |  Loss2: (0.0000) | Acc: (94.00%) (44790/47488)
Epoch: 133 | Batch_idx: 380 |  Loss: (0.1635) |  Loss2: (0.0000) | Acc: (94.00%) (45996/48768)
Epoch: 133 | Batch_idx: 390 |  Loss: (0.1633) |  Loss2: (0.0000) | Acc: (94.00%) (47170/50000)
# TEST : Loss: (0.4081) | Acc: (87.00%) (8793/10000)
percent tensor([0.5127, 0.4873], device='cuda:0')
percent tensor([0.4633, 0.5367], device='cuda:0')
percent tensor([0.5600, 0.4400], device='cuda:0')
percent tensor([0.6051, 0.3949], device='cuda:0')
percent tensor([0.5820, 0.4180], device='cuda:0')
percent tensor([0.6534, 0.3466], device='cuda:0')
percent tensor([0.6641, 0.3359], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 134 | Batch_idx: 0 |  Loss: (0.1193) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 134 | Batch_idx: 10 |  Loss: (0.1550) |  Loss2: (0.0000) | Acc: (95.00%) (1339/1408)
Epoch: 134 | Batch_idx: 20 |  Loss: (0.1563) |  Loss2: (0.0000) | Acc: (94.00%) (2550/2688)
Epoch: 134 | Batch_idx: 30 |  Loss: (0.1537) |  Loss2: (0.0000) | Acc: (94.00%) (3769/3968)
Epoch: 134 | Batch_idx: 40 |  Loss: (0.1576) |  Loss2: (0.0000) | Acc: (94.00%) (4979/5248)
Epoch: 134 | Batch_idx: 50 |  Loss: (0.1509) |  Loss2: (0.0000) | Acc: (95.00%) (6209/6528)
Epoch: 134 | Batch_idx: 60 |  Loss: (0.1522) |  Loss2: (0.0000) | Acc: (95.00%) (7431/7808)
Epoch: 134 | Batch_idx: 70 |  Loss: (0.1500) |  Loss2: (0.0000) | Acc: (95.00%) (8655/9088)
Epoch: 134 | Batch_idx: 80 |  Loss: (0.1521) |  Loss2: (0.0000) | Acc: (95.00%) (9870/10368)
Epoch: 134 | Batch_idx: 90 |  Loss: (0.1511) |  Loss2: (0.0000) | Acc: (95.00%) (11086/11648)
Epoch: 134 | Batch_idx: 100 |  Loss: (0.1529) |  Loss2: (0.0000) | Acc: (95.00%) (12294/12928)
Epoch: 134 | Batch_idx: 110 |  Loss: (0.1539) |  Loss2: (0.0000) | Acc: (94.00%) (13497/14208)
Epoch: 134 | Batch_idx: 120 |  Loss: (0.1526) |  Loss2: (0.0000) | Acc: (95.00%) (14717/15488)
Epoch: 134 | Batch_idx: 130 |  Loss: (0.1526) |  Loss2: (0.0000) | Acc: (94.00%) (15929/16768)
Epoch: 134 | Batch_idx: 140 |  Loss: (0.1515) |  Loss2: (0.0000) | Acc: (95.00%) (17155/18048)
Epoch: 134 | Batch_idx: 150 |  Loss: (0.1507) |  Loss2: (0.0000) | Acc: (95.00%) (18380/19328)
Epoch: 134 | Batch_idx: 160 |  Loss: (0.1515) |  Loss2: (0.0000) | Acc: (94.00%) (19577/20608)
Epoch: 134 | Batch_idx: 170 |  Loss: (0.1516) |  Loss2: (0.0000) | Acc: (94.00%) (20780/21888)
Epoch: 134 | Batch_idx: 180 |  Loss: (0.1519) |  Loss2: (0.0000) | Acc: (94.00%) (21988/23168)
Epoch: 134 | Batch_idx: 190 |  Loss: (0.1536) |  Loss2: (0.0000) | Acc: (94.00%) (23182/24448)
Epoch: 134 | Batch_idx: 200 |  Loss: (0.1531) |  Loss2: (0.0000) | Acc: (94.00%) (24400/25728)
Epoch: 134 | Batch_idx: 210 |  Loss: (0.1527) |  Loss2: (0.0000) | Acc: (94.00%) (25616/27008)
Epoch: 134 | Batch_idx: 220 |  Loss: (0.1535) |  Loss2: (0.0000) | Acc: (94.00%) (26819/28288)
Epoch: 134 | Batch_idx: 230 |  Loss: (0.1553) |  Loss2: (0.0000) | Acc: (94.00%) (28019/29568)
Epoch: 134 | Batch_idx: 240 |  Loss: (0.1558) |  Loss2: (0.0000) | Acc: (94.00%) (29227/30848)
Epoch: 134 | Batch_idx: 250 |  Loss: (0.1569) |  Loss2: (0.0000) | Acc: (94.00%) (30427/32128)
Epoch: 134 | Batch_idx: 260 |  Loss: (0.1568) |  Loss2: (0.0000) | Acc: (94.00%) (31632/33408)
Epoch: 134 | Batch_idx: 270 |  Loss: (0.1575) |  Loss2: (0.0000) | Acc: (94.00%) (32840/34688)
Epoch: 134 | Batch_idx: 280 |  Loss: (0.1587) |  Loss2: (0.0000) | Acc: (94.00%) (34031/35968)
Epoch: 134 | Batch_idx: 290 |  Loss: (0.1591) |  Loss2: (0.0000) | Acc: (94.00%) (35241/37248)
Epoch: 134 | Batch_idx: 300 |  Loss: (0.1592) |  Loss2: (0.0000) | Acc: (94.00%) (36448/38528)
Epoch: 134 | Batch_idx: 310 |  Loss: (0.1594) |  Loss2: (0.0000) | Acc: (94.00%) (37659/39808)
Epoch: 134 | Batch_idx: 320 |  Loss: (0.1592) |  Loss2: (0.0000) | Acc: (94.00%) (38886/41088)
Epoch: 134 | Batch_idx: 330 |  Loss: (0.1585) |  Loss2: (0.0000) | Acc: (94.00%) (40110/42368)
Epoch: 134 | Batch_idx: 340 |  Loss: (0.1591) |  Loss2: (0.0000) | Acc: (94.00%) (41311/43648)
Epoch: 134 | Batch_idx: 350 |  Loss: (0.1592) |  Loss2: (0.0000) | Acc: (94.00%) (42518/44928)
Epoch: 134 | Batch_idx: 360 |  Loss: (0.1590) |  Loss2: (0.0000) | Acc: (94.00%) (43736/46208)
Epoch: 134 | Batch_idx: 370 |  Loss: (0.1594) |  Loss2: (0.0000) | Acc: (94.00%) (44935/47488)
Epoch: 134 | Batch_idx: 380 |  Loss: (0.1593) |  Loss2: (0.0000) | Acc: (94.00%) (46145/48768)
Epoch: 134 | Batch_idx: 390 |  Loss: (0.1596) |  Loss2: (0.0000) | Acc: (94.00%) (47305/50000)
# TEST : Loss: (0.4659) | Acc: (86.00%) (8653/10000)
percent tensor([0.5130, 0.4870], device='cuda:0')
percent tensor([0.4635, 0.5365], device='cuda:0')
percent tensor([0.5602, 0.4398], device='cuda:0')
percent tensor([0.6056, 0.3944], device='cuda:0')
percent tensor([0.5820, 0.4180], device='cuda:0')
percent tensor([0.6529, 0.3471], device='cuda:0')
percent tensor([0.6728, 0.3272], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 135 | Batch_idx: 0 |  Loss: (0.1152) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 135 | Batch_idx: 10 |  Loss: (0.1577) |  Loss2: (0.0000) | Acc: (94.00%) (1328/1408)
Epoch: 135 | Batch_idx: 20 |  Loss: (0.1993) |  Loss2: (0.0000) | Acc: (93.00%) (2502/2688)
Epoch: 135 | Batch_idx: 30 |  Loss: (0.2126) |  Loss2: (0.0000) | Acc: (92.00%) (3676/3968)
Epoch: 135 | Batch_idx: 40 |  Loss: (0.2131) |  Loss2: (0.0000) | Acc: (92.00%) (4855/5248)
Epoch: 135 | Batch_idx: 50 |  Loss: (0.2206) |  Loss2: (0.0000) | Acc: (92.00%) (6021/6528)
Epoch: 135 | Batch_idx: 60 |  Loss: (0.2184) |  Loss2: (0.0000) | Acc: (92.00%) (7204/7808)
Epoch: 135 | Batch_idx: 70 |  Loss: (0.2181) |  Loss2: (0.0000) | Acc: (92.00%) (8379/9088)
Epoch: 135 | Batch_idx: 80 |  Loss: (0.2212) |  Loss2: (0.0000) | Acc: (92.00%) (9557/10368)
Epoch: 135 | Batch_idx: 90 |  Loss: (0.2193) |  Loss2: (0.0000) | Acc: (92.00%) (10749/11648)
Epoch: 135 | Batch_idx: 100 |  Loss: (0.2188) |  Loss2: (0.0000) | Acc: (92.00%) (11933/12928)
Epoch: 135 | Batch_idx: 110 |  Loss: (0.2179) |  Loss2: (0.0000) | Acc: (92.00%) (13122/14208)
Epoch: 135 | Batch_idx: 120 |  Loss: (0.2162) |  Loss2: (0.0000) | Acc: (92.00%) (14307/15488)
Epoch: 135 | Batch_idx: 130 |  Loss: (0.2170) |  Loss2: (0.0000) | Acc: (92.00%) (15483/16768)
Epoch: 135 | Batch_idx: 140 |  Loss: (0.2154) |  Loss2: (0.0000) | Acc: (92.00%) (16676/18048)
Epoch: 135 | Batch_idx: 150 |  Loss: (0.2146) |  Loss2: (0.0000) | Acc: (92.00%) (17863/19328)
Epoch: 135 | Batch_idx: 160 |  Loss: (0.2126) |  Loss2: (0.0000) | Acc: (92.00%) (19056/20608)
Epoch: 135 | Batch_idx: 170 |  Loss: (0.2132) |  Loss2: (0.0000) | Acc: (92.00%) (20234/21888)
Epoch: 135 | Batch_idx: 180 |  Loss: (0.2128) |  Loss2: (0.0000) | Acc: (92.00%) (21423/23168)
Epoch: 135 | Batch_idx: 190 |  Loss: (0.2125) |  Loss2: (0.0000) | Acc: (92.00%) (22625/24448)
Epoch: 135 | Batch_idx: 200 |  Loss: (0.2129) |  Loss2: (0.0000) | Acc: (92.00%) (23815/25728)
Epoch: 135 | Batch_idx: 210 |  Loss: (0.2121) |  Loss2: (0.0000) | Acc: (92.00%) (25005/27008)
Epoch: 135 | Batch_idx: 220 |  Loss: (0.2119) |  Loss2: (0.0000) | Acc: (92.00%) (26186/28288)
Epoch: 135 | Batch_idx: 230 |  Loss: (0.2110) |  Loss2: (0.0000) | Acc: (92.00%) (27373/29568)
Epoch: 135 | Batch_idx: 240 |  Loss: (0.2104) |  Loss2: (0.0000) | Acc: (92.00%) (28570/30848)
Epoch: 135 | Batch_idx: 250 |  Loss: (0.2105) |  Loss2: (0.0000) | Acc: (92.00%) (29757/32128)
Epoch: 135 | Batch_idx: 260 |  Loss: (0.2096) |  Loss2: (0.0000) | Acc: (92.00%) (30940/33408)
Epoch: 135 | Batch_idx: 270 |  Loss: (0.2092) |  Loss2: (0.0000) | Acc: (92.00%) (32138/34688)
Epoch: 135 | Batch_idx: 280 |  Loss: (0.2088) |  Loss2: (0.0000) | Acc: (92.00%) (33330/35968)
Epoch: 135 | Batch_idx: 290 |  Loss: (0.2076) |  Loss2: (0.0000) | Acc: (92.00%) (34519/37248)
Epoch: 135 | Batch_idx: 300 |  Loss: (0.2072) |  Loss2: (0.0000) | Acc: (92.00%) (35707/38528)
Epoch: 135 | Batch_idx: 310 |  Loss: (0.2074) |  Loss2: (0.0000) | Acc: (92.00%) (36892/39808)
Epoch: 135 | Batch_idx: 320 |  Loss: (0.2075) |  Loss2: (0.0000) | Acc: (92.00%) (38079/41088)
Epoch: 135 | Batch_idx: 330 |  Loss: (0.2062) |  Loss2: (0.0000) | Acc: (92.00%) (39285/42368)
Epoch: 135 | Batch_idx: 340 |  Loss: (0.2053) |  Loss2: (0.0000) | Acc: (92.00%) (40486/43648)
Epoch: 135 | Batch_idx: 350 |  Loss: (0.2052) |  Loss2: (0.0000) | Acc: (92.00%) (41673/44928)
Epoch: 135 | Batch_idx: 360 |  Loss: (0.2046) |  Loss2: (0.0000) | Acc: (92.00%) (42863/46208)
Epoch: 135 | Batch_idx: 370 |  Loss: (0.2042) |  Loss2: (0.0000) | Acc: (92.00%) (44057/47488)
Epoch: 135 | Batch_idx: 380 |  Loss: (0.2039) |  Loss2: (0.0000) | Acc: (92.00%) (45256/48768)
Epoch: 135 | Batch_idx: 390 |  Loss: (0.2030) |  Loss2: (0.0000) | Acc: (92.00%) (46422/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_135.pth.tar'
# TEST : Loss: (0.4322) | Acc: (86.00%) (8687/10000)
percent tensor([0.5163, 0.4837], device='cuda:0')
percent tensor([0.4586, 0.5414], device='cuda:0')
percent tensor([0.5672, 0.4328], device='cuda:0')
percent tensor([0.5965, 0.4035], device='cuda:0')
percent tensor([0.5697, 0.4303], device='cuda:0')
percent tensor([0.6443, 0.3557], device='cuda:0')
percent tensor([0.6650, 0.3350], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 136 | Batch_idx: 0 |  Loss: (0.1941) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 136 | Batch_idx: 10 |  Loss: (0.1873) |  Loss2: (0.0000) | Acc: (93.00%) (1317/1408)
Epoch: 136 | Batch_idx: 20 |  Loss: (0.1954) |  Loss2: (0.0000) | Acc: (93.00%) (2503/2688)
Epoch: 136 | Batch_idx: 30 |  Loss: (0.1949) |  Loss2: (0.0000) | Acc: (93.00%) (3696/3968)
Epoch: 136 | Batch_idx: 40 |  Loss: (0.1877) |  Loss2: (0.0000) | Acc: (93.00%) (4902/5248)
Epoch: 136 | Batch_idx: 50 |  Loss: (0.1845) |  Loss2: (0.0000) | Acc: (93.00%) (6111/6528)
Epoch: 136 | Batch_idx: 60 |  Loss: (0.1837) |  Loss2: (0.0000) | Acc: (93.00%) (7316/7808)
Epoch: 136 | Batch_idx: 70 |  Loss: (0.1832) |  Loss2: (0.0000) | Acc: (93.00%) (8526/9088)
Epoch: 136 | Batch_idx: 80 |  Loss: (0.1843) |  Loss2: (0.0000) | Acc: (93.00%) (9727/10368)
Epoch: 136 | Batch_idx: 90 |  Loss: (0.1854) |  Loss2: (0.0000) | Acc: (93.00%) (10928/11648)
Epoch: 136 | Batch_idx: 100 |  Loss: (0.1846) |  Loss2: (0.0000) | Acc: (93.00%) (12130/12928)
Epoch: 136 | Batch_idx: 110 |  Loss: (0.1861) |  Loss2: (0.0000) | Acc: (93.00%) (13323/14208)
Epoch: 136 | Batch_idx: 120 |  Loss: (0.1838) |  Loss2: (0.0000) | Acc: (93.00%) (14535/15488)
Epoch: 136 | Batch_idx: 130 |  Loss: (0.1846) |  Loss2: (0.0000) | Acc: (93.00%) (15724/16768)
Epoch: 136 | Batch_idx: 140 |  Loss: (0.1823) |  Loss2: (0.0000) | Acc: (93.00%) (16943/18048)
Epoch: 136 | Batch_idx: 150 |  Loss: (0.1803) |  Loss2: (0.0000) | Acc: (93.00%) (18153/19328)
Epoch: 136 | Batch_idx: 160 |  Loss: (0.1789) |  Loss2: (0.0000) | Acc: (93.00%) (19371/20608)
Epoch: 136 | Batch_idx: 170 |  Loss: (0.1785) |  Loss2: (0.0000) | Acc: (93.00%) (20571/21888)
Epoch: 136 | Batch_idx: 180 |  Loss: (0.1775) |  Loss2: (0.0000) | Acc: (94.00%) (21782/23168)
Epoch: 136 | Batch_idx: 190 |  Loss: (0.1768) |  Loss2: (0.0000) | Acc: (94.00%) (22994/24448)
Epoch: 136 | Batch_idx: 200 |  Loss: (0.1768) |  Loss2: (0.0000) | Acc: (94.00%) (24187/25728)
Epoch: 136 | Batch_idx: 210 |  Loss: (0.1771) |  Loss2: (0.0000) | Acc: (93.00%) (25384/27008)
Epoch: 136 | Batch_idx: 220 |  Loss: (0.1776) |  Loss2: (0.0000) | Acc: (93.00%) (26582/28288)
Epoch: 136 | Batch_idx: 230 |  Loss: (0.1776) |  Loss2: (0.0000) | Acc: (93.00%) (27784/29568)
Epoch: 136 | Batch_idx: 240 |  Loss: (0.1776) |  Loss2: (0.0000) | Acc: (93.00%) (28997/30848)
Epoch: 136 | Batch_idx: 250 |  Loss: (0.1772) |  Loss2: (0.0000) | Acc: (94.00%) (30210/32128)
Epoch: 136 | Batch_idx: 260 |  Loss: (0.1764) |  Loss2: (0.0000) | Acc: (94.00%) (31415/33408)
Epoch: 136 | Batch_idx: 270 |  Loss: (0.1764) |  Loss2: (0.0000) | Acc: (94.00%) (32620/34688)
Epoch: 136 | Batch_idx: 280 |  Loss: (0.1761) |  Loss2: (0.0000) | Acc: (94.00%) (33822/35968)
Epoch: 136 | Batch_idx: 290 |  Loss: (0.1763) |  Loss2: (0.0000) | Acc: (94.00%) (35027/37248)
Epoch: 136 | Batch_idx: 300 |  Loss: (0.1756) |  Loss2: (0.0000) | Acc: (94.00%) (36248/38528)
Epoch: 136 | Batch_idx: 310 |  Loss: (0.1760) |  Loss2: (0.0000) | Acc: (94.00%) (37450/39808)
Epoch: 136 | Batch_idx: 320 |  Loss: (0.1765) |  Loss2: (0.0000) | Acc: (94.00%) (38640/41088)
Epoch: 136 | Batch_idx: 330 |  Loss: (0.1765) |  Loss2: (0.0000) | Acc: (94.00%) (39836/42368)
Epoch: 136 | Batch_idx: 340 |  Loss: (0.1760) |  Loss2: (0.0000) | Acc: (94.00%) (41044/43648)
Epoch: 136 | Batch_idx: 350 |  Loss: (0.1757) |  Loss2: (0.0000) | Acc: (94.00%) (42255/44928)
Epoch: 136 | Batch_idx: 360 |  Loss: (0.1754) |  Loss2: (0.0000) | Acc: (94.00%) (43465/46208)
Epoch: 136 | Batch_idx: 370 |  Loss: (0.1755) |  Loss2: (0.0000) | Acc: (94.00%) (44667/47488)
Epoch: 136 | Batch_idx: 380 |  Loss: (0.1755) |  Loss2: (0.0000) | Acc: (94.00%) (45871/48768)
Epoch: 136 | Batch_idx: 390 |  Loss: (0.1753) |  Loss2: (0.0000) | Acc: (94.00%) (47037/50000)
# TEST : Loss: (0.4171) | Acc: (87.00%) (8734/10000)
percent tensor([0.5183, 0.4817], device='cuda:0')
percent tensor([0.4560, 0.5440], device='cuda:0')
percent tensor([0.5640, 0.4360], device='cuda:0')
percent tensor([0.5960, 0.4040], device='cuda:0')
percent tensor([0.5728, 0.4272], device='cuda:0')
percent tensor([0.6500, 0.3500], device='cuda:0')
percent tensor([0.6747, 0.3253], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 137 | Batch_idx: 0 |  Loss: (0.2135) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 137 | Batch_idx: 10 |  Loss: (0.1747) |  Loss2: (0.0000) | Acc: (94.00%) (1325/1408)
Epoch: 137 | Batch_idx: 20 |  Loss: (0.1651) |  Loss2: (0.0000) | Acc: (94.00%) (2537/2688)
Epoch: 137 | Batch_idx: 30 |  Loss: (0.1651) |  Loss2: (0.0000) | Acc: (94.00%) (3758/3968)
Epoch: 137 | Batch_idx: 40 |  Loss: (0.1614) |  Loss2: (0.0000) | Acc: (94.00%) (4977/5248)
Epoch: 137 | Batch_idx: 50 |  Loss: (0.1610) |  Loss2: (0.0000) | Acc: (94.00%) (6190/6528)
Epoch: 137 | Batch_idx: 60 |  Loss: (0.1606) |  Loss2: (0.0000) | Acc: (94.00%) (7404/7808)
Epoch: 137 | Batch_idx: 70 |  Loss: (0.1656) |  Loss2: (0.0000) | Acc: (94.00%) (8608/9088)
Epoch: 137 | Batch_idx: 80 |  Loss: (0.1657) |  Loss2: (0.0000) | Acc: (94.00%) (9819/10368)
Epoch: 137 | Batch_idx: 90 |  Loss: (0.1663) |  Loss2: (0.0000) | Acc: (94.00%) (11026/11648)
Epoch: 137 | Batch_idx: 100 |  Loss: (0.1664) |  Loss2: (0.0000) | Acc: (94.00%) (12235/12928)
Epoch: 137 | Batch_idx: 110 |  Loss: (0.1654) |  Loss2: (0.0000) | Acc: (94.00%) (13444/14208)
Epoch: 137 | Batch_idx: 120 |  Loss: (0.1647) |  Loss2: (0.0000) | Acc: (94.00%) (14659/15488)
Epoch: 137 | Batch_idx: 130 |  Loss: (0.1646) |  Loss2: (0.0000) | Acc: (94.00%) (15867/16768)
Epoch: 137 | Batch_idx: 140 |  Loss: (0.1651) |  Loss2: (0.0000) | Acc: (94.00%) (17076/18048)
Epoch: 137 | Batch_idx: 150 |  Loss: (0.1646) |  Loss2: (0.0000) | Acc: (94.00%) (18293/19328)
Epoch: 137 | Batch_idx: 160 |  Loss: (0.1643) |  Loss2: (0.0000) | Acc: (94.00%) (19501/20608)
Epoch: 137 | Batch_idx: 170 |  Loss: (0.1639) |  Loss2: (0.0000) | Acc: (94.00%) (20712/21888)
Epoch: 137 | Batch_idx: 180 |  Loss: (0.1638) |  Loss2: (0.0000) | Acc: (94.00%) (21917/23168)
Epoch: 137 | Batch_idx: 190 |  Loss: (0.1627) |  Loss2: (0.0000) | Acc: (94.00%) (23128/24448)
Epoch: 137 | Batch_idx: 200 |  Loss: (0.1637) |  Loss2: (0.0000) | Acc: (94.00%) (24331/25728)
Epoch: 137 | Batch_idx: 210 |  Loss: (0.1638) |  Loss2: (0.0000) | Acc: (94.00%) (25533/27008)
Epoch: 137 | Batch_idx: 220 |  Loss: (0.1630) |  Loss2: (0.0000) | Acc: (94.00%) (26746/28288)
Epoch: 137 | Batch_idx: 230 |  Loss: (0.1628) |  Loss2: (0.0000) | Acc: (94.00%) (27960/29568)
Epoch: 137 | Batch_idx: 240 |  Loss: (0.1633) |  Loss2: (0.0000) | Acc: (94.00%) (29163/30848)
Epoch: 137 | Batch_idx: 250 |  Loss: (0.1662) |  Loss2: (0.0000) | Acc: (94.00%) (30342/32128)
Epoch: 137 | Batch_idx: 260 |  Loss: (0.1660) |  Loss2: (0.0000) | Acc: (94.00%) (31541/33408)
Epoch: 137 | Batch_idx: 270 |  Loss: (0.1655) |  Loss2: (0.0000) | Acc: (94.00%) (32756/34688)
Epoch: 137 | Batch_idx: 280 |  Loss: (0.1651) |  Loss2: (0.0000) | Acc: (94.00%) (33967/35968)
Epoch: 137 | Batch_idx: 290 |  Loss: (0.1648) |  Loss2: (0.0000) | Acc: (94.00%) (35175/37248)
Epoch: 137 | Batch_idx: 300 |  Loss: (0.1640) |  Loss2: (0.0000) | Acc: (94.00%) (36386/38528)
Epoch: 137 | Batch_idx: 310 |  Loss: (0.1641) |  Loss2: (0.0000) | Acc: (94.00%) (37594/39808)
Epoch: 137 | Batch_idx: 320 |  Loss: (0.1643) |  Loss2: (0.0000) | Acc: (94.00%) (38797/41088)
Epoch: 137 | Batch_idx: 330 |  Loss: (0.1645) |  Loss2: (0.0000) | Acc: (94.00%) (39999/42368)
Epoch: 137 | Batch_idx: 340 |  Loss: (0.1646) |  Loss2: (0.0000) | Acc: (94.00%) (41201/43648)
Epoch: 137 | Batch_idx: 350 |  Loss: (0.1639) |  Loss2: (0.0000) | Acc: (94.00%) (42418/44928)
Epoch: 137 | Batch_idx: 360 |  Loss: (0.1639) |  Loss2: (0.0000) | Acc: (94.00%) (43620/46208)
Epoch: 137 | Batch_idx: 370 |  Loss: (0.1631) |  Loss2: (0.0000) | Acc: (94.00%) (44844/47488)
Epoch: 137 | Batch_idx: 380 |  Loss: (0.1625) |  Loss2: (0.0000) | Acc: (94.00%) (46064/48768)
Epoch: 137 | Batch_idx: 390 |  Loss: (0.1628) |  Loss2: (0.0000) | Acc: (94.00%) (47224/50000)
# TEST : Loss: (0.4007) | Acc: (87.00%) (8773/10000)
percent tensor([0.5191, 0.4809], device='cuda:0')
percent tensor([0.4586, 0.5414], device='cuda:0')
percent tensor([0.5632, 0.4368], device='cuda:0')
percent tensor([0.5977, 0.4023], device='cuda:0')
percent tensor([0.5775, 0.4225], device='cuda:0')
percent tensor([0.6541, 0.3459], device='cuda:0')
percent tensor([0.6829, 0.3171], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 138 | Batch_idx: 0 |  Loss: (0.1438) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 138 | Batch_idx: 10 |  Loss: (0.1444) |  Loss2: (0.0000) | Acc: (94.00%) (1336/1408)
Epoch: 138 | Batch_idx: 20 |  Loss: (0.1609) |  Loss2: (0.0000) | Acc: (94.00%) (2541/2688)
Epoch: 138 | Batch_idx: 30 |  Loss: (0.1624) |  Loss2: (0.0000) | Acc: (94.00%) (3755/3968)
Epoch: 138 | Batch_idx: 40 |  Loss: (0.1653) |  Loss2: (0.0000) | Acc: (94.00%) (4960/5248)
Epoch: 138 | Batch_idx: 50 |  Loss: (0.1586) |  Loss2: (0.0000) | Acc: (94.00%) (6189/6528)
Epoch: 138 | Batch_idx: 60 |  Loss: (0.1599) |  Loss2: (0.0000) | Acc: (94.00%) (7398/7808)
Epoch: 138 | Batch_idx: 70 |  Loss: (0.1566) |  Loss2: (0.0000) | Acc: (94.00%) (8618/9088)
Epoch: 138 | Batch_idx: 80 |  Loss: (0.1585) |  Loss2: (0.0000) | Acc: (94.00%) (9810/10368)
Epoch: 138 | Batch_idx: 90 |  Loss: (0.1581) |  Loss2: (0.0000) | Acc: (94.00%) (11021/11648)
Epoch: 138 | Batch_idx: 100 |  Loss: (0.1575) |  Loss2: (0.0000) | Acc: (94.00%) (12237/12928)
Epoch: 138 | Batch_idx: 110 |  Loss: (0.1581) |  Loss2: (0.0000) | Acc: (94.00%) (13443/14208)
Epoch: 138 | Batch_idx: 120 |  Loss: (0.1569) |  Loss2: (0.0000) | Acc: (94.00%) (14660/15488)
Epoch: 138 | Batch_idx: 130 |  Loss: (0.1578) |  Loss2: (0.0000) | Acc: (94.00%) (15865/16768)
Epoch: 138 | Batch_idx: 140 |  Loss: (0.1590) |  Loss2: (0.0000) | Acc: (94.00%) (17069/18048)
Epoch: 138 | Batch_idx: 150 |  Loss: (0.1580) |  Loss2: (0.0000) | Acc: (94.00%) (18291/19328)
Epoch: 138 | Batch_idx: 160 |  Loss: (0.1564) |  Loss2: (0.0000) | Acc: (94.00%) (19516/20608)
Epoch: 138 | Batch_idx: 170 |  Loss: (0.1565) |  Loss2: (0.0000) | Acc: (94.00%) (20722/21888)
Epoch: 138 | Batch_idx: 180 |  Loss: (0.1566) |  Loss2: (0.0000) | Acc: (94.00%) (21934/23168)
Epoch: 138 | Batch_idx: 190 |  Loss: (0.1578) |  Loss2: (0.0000) | Acc: (94.00%) (23132/24448)
Epoch: 138 | Batch_idx: 200 |  Loss: (0.1571) |  Loss2: (0.0000) | Acc: (94.00%) (24352/25728)
Epoch: 138 | Batch_idx: 210 |  Loss: (0.1567) |  Loss2: (0.0000) | Acc: (94.00%) (25573/27008)
Epoch: 138 | Batch_idx: 220 |  Loss: (0.1572) |  Loss2: (0.0000) | Acc: (94.00%) (26773/28288)
Epoch: 138 | Batch_idx: 230 |  Loss: (0.1569) |  Loss2: (0.0000) | Acc: (94.00%) (27985/29568)
Epoch: 138 | Batch_idx: 240 |  Loss: (0.1571) |  Loss2: (0.0000) | Acc: (94.00%) (29194/30848)
Epoch: 138 | Batch_idx: 250 |  Loss: (0.1577) |  Loss2: (0.0000) | Acc: (94.00%) (30395/32128)
Epoch: 138 | Batch_idx: 260 |  Loss: (0.1574) |  Loss2: (0.0000) | Acc: (94.00%) (31614/33408)
Epoch: 138 | Batch_idx: 270 |  Loss: (0.1576) |  Loss2: (0.0000) | Acc: (94.00%) (32820/34688)
Epoch: 138 | Batch_idx: 280 |  Loss: (0.1588) |  Loss2: (0.0000) | Acc: (94.00%) (34015/35968)
Epoch: 138 | Batch_idx: 290 |  Loss: (0.1581) |  Loss2: (0.0000) | Acc: (94.00%) (35234/37248)
Epoch: 138 | Batch_idx: 300 |  Loss: (0.1578) |  Loss2: (0.0000) | Acc: (94.00%) (36456/38528)
Epoch: 138 | Batch_idx: 310 |  Loss: (0.1583) |  Loss2: (0.0000) | Acc: (94.00%) (37650/39808)
Epoch: 138 | Batch_idx: 320 |  Loss: (0.1586) |  Loss2: (0.0000) | Acc: (94.00%) (38862/41088)
Epoch: 138 | Batch_idx: 330 |  Loss: (0.1581) |  Loss2: (0.0000) | Acc: (94.00%) (40078/42368)
Epoch: 138 | Batch_idx: 340 |  Loss: (0.1581) |  Loss2: (0.0000) | Acc: (94.00%) (41289/43648)
Epoch: 138 | Batch_idx: 350 |  Loss: (0.1581) |  Loss2: (0.0000) | Acc: (94.00%) (42496/44928)
Epoch: 138 | Batch_idx: 360 |  Loss: (0.1576) |  Loss2: (0.0000) | Acc: (94.00%) (43717/46208)
Epoch: 138 | Batch_idx: 370 |  Loss: (0.1571) |  Loss2: (0.0000) | Acc: (94.00%) (44929/47488)
Epoch: 138 | Batch_idx: 380 |  Loss: (0.1575) |  Loss2: (0.0000) | Acc: (94.00%) (46129/48768)
Epoch: 138 | Batch_idx: 390 |  Loss: (0.1571) |  Loss2: (0.0000) | Acc: (94.00%) (47300/50000)
# TEST : Loss: (0.3996) | Acc: (87.00%) (8799/10000)
percent tensor([0.5201, 0.4799], device='cuda:0')
percent tensor([0.4588, 0.5412], device='cuda:0')
percent tensor([0.5626, 0.4374], device='cuda:0')
percent tensor([0.5981, 0.4019], device='cuda:0')
percent tensor([0.5760, 0.4240], device='cuda:0')
percent tensor([0.6559, 0.3441], device='cuda:0')
percent tensor([0.6905, 0.3095], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 139 | Batch_idx: 0 |  Loss: (0.1884) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 139 | Batch_idx: 10 |  Loss: (0.1461) |  Loss2: (0.0000) | Acc: (94.00%) (1332/1408)
Epoch: 139 | Batch_idx: 20 |  Loss: (0.1422) |  Loss2: (0.0000) | Acc: (94.00%) (2551/2688)
Epoch: 139 | Batch_idx: 30 |  Loss: (0.1390) |  Loss2: (0.0000) | Acc: (95.00%) (3783/3968)
Epoch: 139 | Batch_idx: 40 |  Loss: (0.1459) |  Loss2: (0.0000) | Acc: (95.00%) (4994/5248)
Epoch: 139 | Batch_idx: 50 |  Loss: (0.1475) |  Loss2: (0.0000) | Acc: (95.00%) (6212/6528)
Epoch: 139 | Batch_idx: 60 |  Loss: (0.1489) |  Loss2: (0.0000) | Acc: (95.00%) (7428/7808)
Epoch: 139 | Batch_idx: 70 |  Loss: (0.1475) |  Loss2: (0.0000) | Acc: (95.00%) (8639/9088)
Epoch: 139 | Batch_idx: 80 |  Loss: (0.1450) |  Loss2: (0.0000) | Acc: (95.00%) (9865/10368)
Epoch: 139 | Batch_idx: 90 |  Loss: (0.1456) |  Loss2: (0.0000) | Acc: (95.00%) (11085/11648)
Epoch: 139 | Batch_idx: 100 |  Loss: (0.1445) |  Loss2: (0.0000) | Acc: (95.00%) (12310/12928)
Epoch: 139 | Batch_idx: 110 |  Loss: (0.1454) |  Loss2: (0.0000) | Acc: (95.00%) (13519/14208)
Epoch: 139 | Batch_idx: 120 |  Loss: (0.1469) |  Loss2: (0.0000) | Acc: (95.00%) (14724/15488)
Epoch: 139 | Batch_idx: 130 |  Loss: (0.1486) |  Loss2: (0.0000) | Acc: (94.00%) (15927/16768)
Epoch: 139 | Batch_idx: 140 |  Loss: (0.1491) |  Loss2: (0.0000) | Acc: (94.00%) (17139/18048)
Epoch: 139 | Batch_idx: 150 |  Loss: (0.1489) |  Loss2: (0.0000) | Acc: (94.00%) (18356/19328)
Epoch: 139 | Batch_idx: 160 |  Loss: (0.1495) |  Loss2: (0.0000) | Acc: (94.00%) (19572/20608)
Epoch: 139 | Batch_idx: 170 |  Loss: (0.1512) |  Loss2: (0.0000) | Acc: (94.00%) (20773/21888)
Epoch: 139 | Batch_idx: 180 |  Loss: (0.1523) |  Loss2: (0.0000) | Acc: (94.00%) (21976/23168)
Epoch: 139 | Batch_idx: 190 |  Loss: (0.1521) |  Loss2: (0.0000) | Acc: (94.00%) (23187/24448)
Epoch: 139 | Batch_idx: 200 |  Loss: (0.1522) |  Loss2: (0.0000) | Acc: (94.00%) (24399/25728)
Epoch: 139 | Batch_idx: 210 |  Loss: (0.1520) |  Loss2: (0.0000) | Acc: (94.00%) (25610/27008)
Epoch: 139 | Batch_idx: 220 |  Loss: (0.1523) |  Loss2: (0.0000) | Acc: (94.00%) (26818/28288)
Epoch: 139 | Batch_idx: 230 |  Loss: (0.1529) |  Loss2: (0.0000) | Acc: (94.00%) (28028/29568)
Epoch: 139 | Batch_idx: 240 |  Loss: (0.1524) |  Loss2: (0.0000) | Acc: (94.00%) (29247/30848)
Epoch: 139 | Batch_idx: 250 |  Loss: (0.1519) |  Loss2: (0.0000) | Acc: (94.00%) (30468/32128)
Epoch: 139 | Batch_idx: 260 |  Loss: (0.1526) |  Loss2: (0.0000) | Acc: (94.00%) (31668/33408)
Epoch: 139 | Batch_idx: 270 |  Loss: (0.1527) |  Loss2: (0.0000) | Acc: (94.00%) (32879/34688)
Epoch: 139 | Batch_idx: 280 |  Loss: (0.1528) |  Loss2: (0.0000) | Acc: (94.00%) (34086/35968)
Epoch: 139 | Batch_idx: 290 |  Loss: (0.1534) |  Loss2: (0.0000) | Acc: (94.00%) (35289/37248)
Epoch: 139 | Batch_idx: 300 |  Loss: (0.1535) |  Loss2: (0.0000) | Acc: (94.00%) (36500/38528)
Epoch: 139 | Batch_idx: 310 |  Loss: (0.1532) |  Loss2: (0.0000) | Acc: (94.00%) (37713/39808)
Epoch: 139 | Batch_idx: 320 |  Loss: (0.1531) |  Loss2: (0.0000) | Acc: (94.00%) (38929/41088)
Epoch: 139 | Batch_idx: 330 |  Loss: (0.1534) |  Loss2: (0.0000) | Acc: (94.00%) (40144/42368)
Epoch: 139 | Batch_idx: 340 |  Loss: (0.1531) |  Loss2: (0.0000) | Acc: (94.00%) (41366/43648)
Epoch: 139 | Batch_idx: 350 |  Loss: (0.1534) |  Loss2: (0.0000) | Acc: (94.00%) (42578/44928)
Epoch: 139 | Batch_idx: 360 |  Loss: (0.1535) |  Loss2: (0.0000) | Acc: (94.00%) (43782/46208)
Epoch: 139 | Batch_idx: 370 |  Loss: (0.1535) |  Loss2: (0.0000) | Acc: (94.00%) (44995/47488)
Epoch: 139 | Batch_idx: 380 |  Loss: (0.1532) |  Loss2: (0.0000) | Acc: (94.00%) (46220/48768)
Epoch: 139 | Batch_idx: 390 |  Loss: (0.1537) |  Loss2: (0.0000) | Acc: (94.00%) (47383/50000)
# TEST : Loss: (0.3938) | Acc: (88.00%) (8809/10000)
percent tensor([0.5203, 0.4797], device='cuda:0')
percent tensor([0.4574, 0.5426], device='cuda:0')
percent tensor([0.5630, 0.4370], device='cuda:0')
percent tensor([0.6015, 0.3985], device='cuda:0')
percent tensor([0.5790, 0.4210], device='cuda:0')
percent tensor([0.6537, 0.3463], device='cuda:0')
percent tensor([0.6921, 0.3079], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 140 | Batch_idx: 0 |  Loss: (0.0838) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 140 | Batch_idx: 10 |  Loss: (0.1285) |  Loss2: (0.0000) | Acc: (96.00%) (1353/1408)
Epoch: 140 | Batch_idx: 20 |  Loss: (0.1276) |  Loss2: (0.0000) | Acc: (95.00%) (2580/2688)
Epoch: 140 | Batch_idx: 30 |  Loss: (0.1292) |  Loss2: (0.0000) | Acc: (96.00%) (3814/3968)
Epoch: 140 | Batch_idx: 40 |  Loss: (0.1352) |  Loss2: (0.0000) | Acc: (95.00%) (5018/5248)
Epoch: 140 | Batch_idx: 50 |  Loss: (0.1395) |  Loss2: (0.0000) | Acc: (95.00%) (6233/6528)
Epoch: 140 | Batch_idx: 60 |  Loss: (0.1461) |  Loss2: (0.0000) | Acc: (95.00%) (7439/7808)
Epoch: 140 | Batch_idx: 70 |  Loss: (0.1485) |  Loss2: (0.0000) | Acc: (95.00%) (8646/9088)
Epoch: 140 | Batch_idx: 80 |  Loss: (0.1503) |  Loss2: (0.0000) | Acc: (95.00%) (9853/10368)
Epoch: 140 | Batch_idx: 90 |  Loss: (0.1511) |  Loss2: (0.0000) | Acc: (94.00%) (11065/11648)
Epoch: 140 | Batch_idx: 100 |  Loss: (0.1530) |  Loss2: (0.0000) | Acc: (94.00%) (12268/12928)
Epoch: 140 | Batch_idx: 110 |  Loss: (0.1499) |  Loss2: (0.0000) | Acc: (94.00%) (13497/14208)
Epoch: 140 | Batch_idx: 120 |  Loss: (0.1504) |  Loss2: (0.0000) | Acc: (94.00%) (14707/15488)
Epoch: 140 | Batch_idx: 130 |  Loss: (0.1507) |  Loss2: (0.0000) | Acc: (94.00%) (15922/16768)
Epoch: 140 | Batch_idx: 140 |  Loss: (0.1523) |  Loss2: (0.0000) | Acc: (94.00%) (17137/18048)
Epoch: 140 | Batch_idx: 150 |  Loss: (0.1525) |  Loss2: (0.0000) | Acc: (94.00%) (18344/19328)
Epoch: 140 | Batch_idx: 160 |  Loss: (0.1521) |  Loss2: (0.0000) | Acc: (94.00%) (19566/20608)
Epoch: 140 | Batch_idx: 170 |  Loss: (0.1522) |  Loss2: (0.0000) | Acc: (94.00%) (20782/21888)
Epoch: 140 | Batch_idx: 180 |  Loss: (0.1537) |  Loss2: (0.0000) | Acc: (94.00%) (21985/23168)
Epoch: 140 | Batch_idx: 190 |  Loss: (0.1545) |  Loss2: (0.0000) | Acc: (94.00%) (23194/24448)
Epoch: 140 | Batch_idx: 200 |  Loss: (0.1555) |  Loss2: (0.0000) | Acc: (94.00%) (24395/25728)
Epoch: 140 | Batch_idx: 210 |  Loss: (0.1554) |  Loss2: (0.0000) | Acc: (94.00%) (25608/27008)
Epoch: 140 | Batch_idx: 220 |  Loss: (0.1558) |  Loss2: (0.0000) | Acc: (94.00%) (26831/28288)
Epoch: 140 | Batch_idx: 230 |  Loss: (0.1565) |  Loss2: (0.0000) | Acc: (94.00%) (28037/29568)
Epoch: 140 | Batch_idx: 240 |  Loss: (0.1566) |  Loss2: (0.0000) | Acc: (94.00%) (29244/30848)
Epoch: 140 | Batch_idx: 250 |  Loss: (0.1582) |  Loss2: (0.0000) | Acc: (94.00%) (30444/32128)
Epoch: 140 | Batch_idx: 260 |  Loss: (0.1574) |  Loss2: (0.0000) | Acc: (94.00%) (31658/33408)
Epoch: 140 | Batch_idx: 270 |  Loss: (0.1579) |  Loss2: (0.0000) | Acc: (94.00%) (32864/34688)
Epoch: 140 | Batch_idx: 280 |  Loss: (0.1592) |  Loss2: (0.0000) | Acc: (94.00%) (34058/35968)
Epoch: 140 | Batch_idx: 290 |  Loss: (0.1596) |  Loss2: (0.0000) | Acc: (94.00%) (35266/37248)
Epoch: 140 | Batch_idx: 300 |  Loss: (0.1599) |  Loss2: (0.0000) | Acc: (94.00%) (36466/38528)
Epoch: 140 | Batch_idx: 310 |  Loss: (0.1596) |  Loss2: (0.0000) | Acc: (94.00%) (37679/39808)
Epoch: 140 | Batch_idx: 320 |  Loss: (0.1592) |  Loss2: (0.0000) | Acc: (94.00%) (38898/41088)
Epoch: 140 | Batch_idx: 330 |  Loss: (0.1594) |  Loss2: (0.0000) | Acc: (94.00%) (40105/42368)
Epoch: 140 | Batch_idx: 340 |  Loss: (0.1590) |  Loss2: (0.0000) | Acc: (94.00%) (41331/43648)
Epoch: 140 | Batch_idx: 350 |  Loss: (0.1591) |  Loss2: (0.0000) | Acc: (94.00%) (42539/44928)
Epoch: 140 | Batch_idx: 360 |  Loss: (0.1595) |  Loss2: (0.0000) | Acc: (94.00%) (43741/46208)
Epoch: 140 | Batch_idx: 370 |  Loss: (0.1598) |  Loss2: (0.0000) | Acc: (94.00%) (44942/47488)
Epoch: 140 | Batch_idx: 380 |  Loss: (0.1595) |  Loss2: (0.0000) | Acc: (94.00%) (46150/48768)
Epoch: 140 | Batch_idx: 390 |  Loss: (0.1590) |  Loss2: (0.0000) | Acc: (94.00%) (47316/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_140.pth.tar'
# TEST : Loss: (0.4500) | Acc: (86.00%) (8686/10000)
percent tensor([0.5200, 0.4800], device='cuda:0')
percent tensor([0.4571, 0.5429], device='cuda:0')
percent tensor([0.5625, 0.4375], device='cuda:0')
percent tensor([0.6019, 0.3981], device='cuda:0')
percent tensor([0.5786, 0.4214], device='cuda:0')
percent tensor([0.6539, 0.3461], device='cuda:0')
percent tensor([0.6971, 0.3029], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(182.8739, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(817.1645, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(811.7325, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1510.2257, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(495.9375, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2257.4609, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4265.2510, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1374.7454, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6227.5732, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11743.0430, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3869.4504, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16377.9883, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 141 | Batch_idx: 0 |  Loss: (0.2041) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 141 | Batch_idx: 10 |  Loss: (0.1292) |  Loss2: (0.0000) | Acc: (96.00%) (1352/1408)
Epoch: 141 | Batch_idx: 20 |  Loss: (0.1455) |  Loss2: (0.0000) | Acc: (95.00%) (2564/2688)
Epoch: 141 | Batch_idx: 30 |  Loss: (0.1467) |  Loss2: (0.0000) | Acc: (95.00%) (3786/3968)
Epoch: 141 | Batch_idx: 40 |  Loss: (0.1471) |  Loss2: (0.0000) | Acc: (95.00%) (5000/5248)
Epoch: 141 | Batch_idx: 50 |  Loss: (0.1507) |  Loss2: (0.0000) | Acc: (95.00%) (6211/6528)
Epoch: 141 | Batch_idx: 60 |  Loss: (0.1510) |  Loss2: (0.0000) | Acc: (95.00%) (7426/7808)
Epoch: 141 | Batch_idx: 70 |  Loss: (0.1501) |  Loss2: (0.0000) | Acc: (95.00%) (8637/9088)
Epoch: 141 | Batch_idx: 80 |  Loss: (0.1532) |  Loss2: (0.0000) | Acc: (94.00%) (9838/10368)
Epoch: 141 | Batch_idx: 90 |  Loss: (0.1542) |  Loss2: (0.0000) | Acc: (94.00%) (11044/11648)
Epoch: 141 | Batch_idx: 100 |  Loss: (0.1522) |  Loss2: (0.0000) | Acc: (94.00%) (12256/12928)
Epoch: 141 | Batch_idx: 110 |  Loss: (0.1508) |  Loss2: (0.0000) | Acc: (94.00%) (13477/14208)
Epoch: 141 | Batch_idx: 120 |  Loss: (0.1501) |  Loss2: (0.0000) | Acc: (94.00%) (14692/15488)
Epoch: 141 | Batch_idx: 130 |  Loss: (0.1496) |  Loss2: (0.0000) | Acc: (94.00%) (15911/16768)
Epoch: 141 | Batch_idx: 140 |  Loss: (0.1516) |  Loss2: (0.0000) | Acc: (94.00%) (17105/18048)
Epoch: 141 | Batch_idx: 150 |  Loss: (0.1505) |  Loss2: (0.0000) | Acc: (94.00%) (18319/19328)
Epoch: 141 | Batch_idx: 160 |  Loss: (0.1496) |  Loss2: (0.0000) | Acc: (94.00%) (19539/20608)
Epoch: 141 | Batch_idx: 170 |  Loss: (0.1502) |  Loss2: (0.0000) | Acc: (94.00%) (20746/21888)
Epoch: 141 | Batch_idx: 180 |  Loss: (0.1507) |  Loss2: (0.0000) | Acc: (94.00%) (21951/23168)
Epoch: 141 | Batch_idx: 190 |  Loss: (0.1515) |  Loss2: (0.0000) | Acc: (94.00%) (23160/24448)
Epoch: 141 | Batch_idx: 200 |  Loss: (0.1518) |  Loss2: (0.0000) | Acc: (94.00%) (24368/25728)
Epoch: 141 | Batch_idx: 210 |  Loss: (0.1523) |  Loss2: (0.0000) | Acc: (94.00%) (25570/27008)
Epoch: 141 | Batch_idx: 220 |  Loss: (0.1517) |  Loss2: (0.0000) | Acc: (94.00%) (26788/28288)
Epoch: 141 | Batch_idx: 230 |  Loss: (0.1512) |  Loss2: (0.0000) | Acc: (94.00%) (28012/29568)
Epoch: 141 | Batch_idx: 240 |  Loss: (0.1509) |  Loss2: (0.0000) | Acc: (94.00%) (29231/30848)
Epoch: 141 | Batch_idx: 250 |  Loss: (0.1504) |  Loss2: (0.0000) | Acc: (94.00%) (30453/32128)
Epoch: 141 | Batch_idx: 260 |  Loss: (0.1511) |  Loss2: (0.0000) | Acc: (94.00%) (31655/33408)
Epoch: 141 | Batch_idx: 270 |  Loss: (0.1516) |  Loss2: (0.0000) | Acc: (94.00%) (32867/34688)
Epoch: 141 | Batch_idx: 280 |  Loss: (0.1520) |  Loss2: (0.0000) | Acc: (94.00%) (34077/35968)
Epoch: 141 | Batch_idx: 290 |  Loss: (0.1520) |  Loss2: (0.0000) | Acc: (94.00%) (35282/37248)
Epoch: 141 | Batch_idx: 300 |  Loss: (0.1522) |  Loss2: (0.0000) | Acc: (94.00%) (36483/38528)
Epoch: 141 | Batch_idx: 310 |  Loss: (0.1521) |  Loss2: (0.0000) | Acc: (94.00%) (37699/39808)
Epoch: 141 | Batch_idx: 320 |  Loss: (0.1520) |  Loss2: (0.0000) | Acc: (94.00%) (38918/41088)
Epoch: 141 | Batch_idx: 330 |  Loss: (0.1525) |  Loss2: (0.0000) | Acc: (94.00%) (40119/42368)
Epoch: 141 | Batch_idx: 340 |  Loss: (0.1525) |  Loss2: (0.0000) | Acc: (94.00%) (41335/43648)
Epoch: 141 | Batch_idx: 350 |  Loss: (0.1519) |  Loss2: (0.0000) | Acc: (94.00%) (42552/44928)
Epoch: 141 | Batch_idx: 360 |  Loss: (0.1526) |  Loss2: (0.0000) | Acc: (94.00%) (43752/46208)
Epoch: 141 | Batch_idx: 370 |  Loss: (0.1527) |  Loss2: (0.0000) | Acc: (94.00%) (44961/47488)
Epoch: 141 | Batch_idx: 380 |  Loss: (0.1534) |  Loss2: (0.0000) | Acc: (94.00%) (46160/48768)
Epoch: 141 | Batch_idx: 390 |  Loss: (0.1531) |  Loss2: (0.0000) | Acc: (94.00%) (47332/50000)
# TEST : Loss: (0.4261) | Acc: (87.00%) (8706/10000)
percent tensor([0.5197, 0.4803], device='cuda:0')
percent tensor([0.4580, 0.5420], device='cuda:0')
percent tensor([0.5617, 0.4383], device='cuda:0')
percent tensor([0.6002, 0.3998], device='cuda:0')
percent tensor([0.5789, 0.4211], device='cuda:0')
percent tensor([0.6550, 0.3450], device='cuda:0')
percent tensor([0.6938, 0.3062], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Epoch: 142 | Batch_idx: 0 |  Loss: (0.1258) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 142 | Batch_idx: 10 |  Loss: (0.1458) |  Loss2: (0.0000) | Acc: (95.00%) (1345/1408)
Epoch: 142 | Batch_idx: 20 |  Loss: (0.1435) |  Loss2: (0.0000) | Acc: (95.00%) (2562/2688)
Epoch: 142 | Batch_idx: 30 |  Loss: (0.1420) |  Loss2: (0.0000) | Acc: (95.00%) (3777/3968)
Epoch: 142 | Batch_idx: 40 |  Loss: (0.1409) |  Loss2: (0.0000) | Acc: (95.00%) (5002/5248)
Epoch: 142 | Batch_idx: 50 |  Loss: (0.1381) |  Loss2: (0.0000) | Acc: (95.00%) (6225/6528)
Epoch: 142 | Batch_idx: 60 |  Loss: (0.1390) |  Loss2: (0.0000) | Acc: (95.00%) (7437/7808)
Epoch: 142 | Batch_idx: 70 |  Loss: (0.1403) |  Loss2: (0.0000) | Acc: (95.00%) (8647/9088)
Epoch: 142 | Batch_idx: 80 |  Loss: (0.1418) |  Loss2: (0.0000) | Acc: (95.00%) (9857/10368)
Epoch: 142 | Batch_idx: 90 |  Loss: (0.1418) |  Loss2: (0.0000) | Acc: (95.00%) (11078/11648)
Epoch: 142 | Batch_idx: 100 |  Loss: (0.1439) |  Loss2: (0.0000) | Acc: (95.00%) (12293/12928)
Epoch: 142 | Batch_idx: 110 |  Loss: (0.1454) |  Loss2: (0.0000) | Acc: (95.00%) (13506/14208)
Epoch: 142 | Batch_idx: 120 |  Loss: (0.1474) |  Loss2: (0.0000) | Acc: (94.00%) (14712/15488)
Epoch: 142 | Batch_idx: 130 |  Loss: (0.1456) |  Loss2: (0.0000) | Acc: (95.00%) (15932/16768)
Epoch: 142 | Batch_idx: 140 |  Loss: (0.1454) |  Loss2: (0.0000) | Acc: (95.00%) (17156/18048)
Epoch: 142 | Batch_idx: 150 |  Loss: (0.1453) |  Loss2: (0.0000) | Acc: (95.00%) (18373/19328)
Epoch: 142 | Batch_idx: 160 |  Loss: (0.1459) |  Loss2: (0.0000) | Acc: (95.00%) (19581/20608)
Epoch: 142 | Batch_idx: 170 |  Loss: (0.1447) |  Loss2: (0.0000) | Acc: (95.00%) (20807/21888)
Epoch: 142 | Batch_idx: 180 |  Loss: (0.1468) |  Loss2: (0.0000) | Acc: (94.00%) (22005/23168)
Epoch: 142 | Batch_idx: 190 |  Loss: (0.1485) |  Loss2: (0.0000) | Acc: (94.00%) (23200/24448)
Epoch: 142 | Batch_idx: 200 |  Loss: (0.1496) |  Loss2: (0.0000) | Acc: (94.00%) (24407/25728)
Epoch: 142 | Batch_idx: 210 |  Loss: (0.1486) |  Loss2: (0.0000) | Acc: (94.00%) (25627/27008)
Epoch: 142 | Batch_idx: 220 |  Loss: (0.1483) |  Loss2: (0.0000) | Acc: (94.00%) (26843/28288)
Epoch: 142 | Batch_idx: 230 |  Loss: (0.1481) |  Loss2: (0.0000) | Acc: (94.00%) (28064/29568)
Epoch: 142 | Batch_idx: 240 |  Loss: (0.1484) |  Loss2: (0.0000) | Acc: (94.00%) (29282/30848)
Epoch: 142 | Batch_idx: 250 |  Loss: (0.1500) |  Loss2: (0.0000) | Acc: (94.00%) (30476/32128)
Epoch: 142 | Batch_idx: 260 |  Loss: (0.1509) |  Loss2: (0.0000) | Acc: (94.00%) (31673/33408)
Epoch: 142 | Batch_idx: 270 |  Loss: (0.1509) |  Loss2: (0.0000) | Acc: (94.00%) (32883/34688)
Epoch: 142 | Batch_idx: 280 |  Loss: (0.1506) |  Loss2: (0.0000) | Acc: (94.00%) (34096/35968)
Epoch: 142 | Batch_idx: 290 |  Loss: (0.1511) |  Loss2: (0.0000) | Acc: (94.00%) (35301/37248)
Epoch: 142 | Batch_idx: 300 |  Loss: (0.1516) |  Loss2: (0.0000) | Acc: (94.00%) (36514/38528)
Epoch: 142 | Batch_idx: 310 |  Loss: (0.1512) |  Loss2: (0.0000) | Acc: (94.00%) (37732/39808)
Epoch: 142 | Batch_idx: 320 |  Loss: (0.1512) |  Loss2: (0.0000) | Acc: (94.00%) (38946/41088)
Epoch: 142 | Batch_idx: 330 |  Loss: (0.1516) |  Loss2: (0.0000) | Acc: (94.00%) (40147/42368)
Epoch: 142 | Batch_idx: 340 |  Loss: (0.1515) |  Loss2: (0.0000) | Acc: (94.00%) (41362/43648)
Epoch: 142 | Batch_idx: 350 |  Loss: (0.1516) |  Loss2: (0.0000) | Acc: (94.00%) (42584/44928)
Epoch: 142 | Batch_idx: 360 |  Loss: (0.1519) |  Loss2: (0.0000) | Acc: (94.00%) (43786/46208)
Epoch: 142 | Batch_idx: 370 |  Loss: (0.1522) |  Loss2: (0.0000) | Acc: (94.00%) (44993/47488)
Epoch: 142 | Batch_idx: 380 |  Loss: (0.1525) |  Loss2: (0.0000) | Acc: (94.00%) (46198/48768)
Epoch: 142 | Batch_idx: 390 |  Loss: (0.1532) |  Loss2: (0.0000) | Acc: (94.00%) (47358/50000)
# TEST : Loss: (0.3982) | Acc: (88.00%) (8816/10000)
percent tensor([0.5199, 0.4801], device='cuda:0')
percent tensor([0.4579, 0.5421], device='cuda:0')
percent tensor([0.5624, 0.4376], device='cuda:0')
percent tensor([0.6009, 0.3991], device='cuda:0')
percent tensor([0.5793, 0.4207], device='cuda:0')
percent tensor([0.6542, 0.3458], device='cuda:0')
percent tensor([0.6942, 0.3058], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 143 | Batch_idx: 0 |  Loss: (0.0743) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 143 | Batch_idx: 10 |  Loss: (0.1507) |  Loss2: (0.0000) | Acc: (95.00%) (1342/1408)
Epoch: 143 | Batch_idx: 20 |  Loss: (0.1510) |  Loss2: (0.0000) | Acc: (95.00%) (2557/2688)
Epoch: 143 | Batch_idx: 30 |  Loss: (0.1448) |  Loss2: (0.0000) | Acc: (95.00%) (3781/3968)
Epoch: 143 | Batch_idx: 40 |  Loss: (0.1426) |  Loss2: (0.0000) | Acc: (95.00%) (5000/5248)
Epoch: 143 | Batch_idx: 50 |  Loss: (0.1428) |  Loss2: (0.0000) | Acc: (95.00%) (6222/6528)
Epoch: 143 | Batch_idx: 60 |  Loss: (0.1415) |  Loss2: (0.0000) | Acc: (95.00%) (7438/7808)
Epoch: 143 | Batch_idx: 70 |  Loss: (0.1402) |  Loss2: (0.0000) | Acc: (95.00%) (8664/9088)
Epoch: 143 | Batch_idx: 80 |  Loss: (0.1399) |  Loss2: (0.0000) | Acc: (95.00%) (9884/10368)
Epoch: 143 | Batch_idx: 90 |  Loss: (0.1405) |  Loss2: (0.0000) | Acc: (95.00%) (11098/11648)
Epoch: 143 | Batch_idx: 100 |  Loss: (0.1438) |  Loss2: (0.0000) | Acc: (95.00%) (12293/12928)
Epoch: 143 | Batch_idx: 110 |  Loss: (0.1438) |  Loss2: (0.0000) | Acc: (95.00%) (13507/14208)
Epoch: 143 | Batch_idx: 120 |  Loss: (0.1428) |  Loss2: (0.0000) | Acc: (95.00%) (14733/15488)
Epoch: 143 | Batch_idx: 130 |  Loss: (0.1425) |  Loss2: (0.0000) | Acc: (95.00%) (15954/16768)
Epoch: 143 | Batch_idx: 140 |  Loss: (0.1427) |  Loss2: (0.0000) | Acc: (95.00%) (17173/18048)
Epoch: 143 | Batch_idx: 150 |  Loss: (0.1432) |  Loss2: (0.0000) | Acc: (95.00%) (18384/19328)
Epoch: 143 | Batch_idx: 160 |  Loss: (0.1427) |  Loss2: (0.0000) | Acc: (95.00%) (19613/20608)
Epoch: 143 | Batch_idx: 170 |  Loss: (0.1433) |  Loss2: (0.0000) | Acc: (95.00%) (20828/21888)
Epoch: 143 | Batch_idx: 180 |  Loss: (0.1435) |  Loss2: (0.0000) | Acc: (95.00%) (22040/23168)
Epoch: 143 | Batch_idx: 190 |  Loss: (0.1436) |  Loss2: (0.0000) | Acc: (95.00%) (23262/24448)
Epoch: 143 | Batch_idx: 200 |  Loss: (0.1441) |  Loss2: (0.0000) | Acc: (95.00%) (24482/25728)
Epoch: 143 | Batch_idx: 210 |  Loss: (0.1444) |  Loss2: (0.0000) | Acc: (95.00%) (25694/27008)
Epoch: 143 | Batch_idx: 220 |  Loss: (0.1453) |  Loss2: (0.0000) | Acc: (95.00%) (26900/28288)
Epoch: 143 | Batch_idx: 230 |  Loss: (0.1448) |  Loss2: (0.0000) | Acc: (95.00%) (28117/29568)
Epoch: 143 | Batch_idx: 240 |  Loss: (0.1446) |  Loss2: (0.0000) | Acc: (95.00%) (29336/30848)
Epoch: 143 | Batch_idx: 250 |  Loss: (0.1445) |  Loss2: (0.0000) | Acc: (95.00%) (30552/32128)
Epoch: 143 | Batch_idx: 260 |  Loss: (0.1437) |  Loss2: (0.0000) | Acc: (95.00%) (31778/33408)
Epoch: 143 | Batch_idx: 270 |  Loss: (0.1435) |  Loss2: (0.0000) | Acc: (95.00%) (32997/34688)
Epoch: 143 | Batch_idx: 280 |  Loss: (0.1436) |  Loss2: (0.0000) | Acc: (95.00%) (34212/35968)
Epoch: 143 | Batch_idx: 290 |  Loss: (0.1440) |  Loss2: (0.0000) | Acc: (95.00%) (35426/37248)
Epoch: 143 | Batch_idx: 300 |  Loss: (0.1441) |  Loss2: (0.0000) | Acc: (95.00%) (36637/38528)
Epoch: 143 | Batch_idx: 310 |  Loss: (0.1440) |  Loss2: (0.0000) | Acc: (95.00%) (37858/39808)
Epoch: 143 | Batch_idx: 320 |  Loss: (0.1450) |  Loss2: (0.0000) | Acc: (95.00%) (39066/41088)
Epoch: 143 | Batch_idx: 330 |  Loss: (0.1448) |  Loss2: (0.0000) | Acc: (95.00%) (40284/42368)
Epoch: 143 | Batch_idx: 340 |  Loss: (0.1449) |  Loss2: (0.0000) | Acc: (95.00%) (41496/43648)
Epoch: 143 | Batch_idx: 350 |  Loss: (0.1449) |  Loss2: (0.0000) | Acc: (95.00%) (42713/44928)
Epoch: 143 | Batch_idx: 360 |  Loss: (0.1449) |  Loss2: (0.0000) | Acc: (95.00%) (43930/46208)
Epoch: 143 | Batch_idx: 370 |  Loss: (0.1452) |  Loss2: (0.0000) | Acc: (95.00%) (45137/47488)
Epoch: 143 | Batch_idx: 380 |  Loss: (0.1457) |  Loss2: (0.0000) | Acc: (95.00%) (46342/48768)
Epoch: 143 | Batch_idx: 390 |  Loss: (0.1454) |  Loss2: (0.0000) | Acc: (95.00%) (47522/50000)
# TEST : Loss: (0.4097) | Acc: (87.00%) (8780/10000)
percent tensor([0.5199, 0.4801], device='cuda:0')
percent tensor([0.4578, 0.5422], device='cuda:0')
percent tensor([0.5630, 0.4370], device='cuda:0')
percent tensor([0.6009, 0.3991], device='cuda:0')
percent tensor([0.5791, 0.4209], device='cuda:0')
percent tensor([0.6558, 0.3442], device='cuda:0')
percent tensor([0.6936, 0.3064], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 144 | Batch_idx: 0 |  Loss: (0.1609) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 144 | Batch_idx: 10 |  Loss: (0.1280) |  Loss2: (0.0000) | Acc: (95.00%) (1347/1408)
Epoch: 144 | Batch_idx: 20 |  Loss: (0.1271) |  Loss2: (0.0000) | Acc: (96.00%) (2582/2688)
Epoch: 144 | Batch_idx: 30 |  Loss: (0.1251) |  Loss2: (0.0000) | Acc: (96.00%) (3814/3968)
Epoch: 144 | Batch_idx: 40 |  Loss: (0.1323) |  Loss2: (0.0000) | Acc: (95.00%) (5024/5248)
Epoch: 144 | Batch_idx: 50 |  Loss: (0.1358) |  Loss2: (0.0000) | Acc: (95.00%) (6243/6528)
Epoch: 144 | Batch_idx: 60 |  Loss: (0.1377) |  Loss2: (0.0000) | Acc: (95.00%) (7457/7808)
Epoch: 144 | Batch_idx: 70 |  Loss: (0.1378) |  Loss2: (0.0000) | Acc: (95.00%) (8676/9088)
Epoch: 144 | Batch_idx: 80 |  Loss: (0.1365) |  Loss2: (0.0000) | Acc: (95.00%) (9899/10368)
Epoch: 144 | Batch_idx: 90 |  Loss: (0.1354) |  Loss2: (0.0000) | Acc: (95.00%) (11127/11648)
Epoch: 144 | Batch_idx: 100 |  Loss: (0.1351) |  Loss2: (0.0000) | Acc: (95.00%) (12354/12928)
Epoch: 144 | Batch_idx: 110 |  Loss: (0.1352) |  Loss2: (0.0000) | Acc: (95.00%) (13580/14208)
Epoch: 144 | Batch_idx: 120 |  Loss: (0.1374) |  Loss2: (0.0000) | Acc: (95.00%) (14787/15488)
Epoch: 144 | Batch_idx: 130 |  Loss: (0.1389) |  Loss2: (0.0000) | Acc: (95.00%) (15994/16768)
Epoch: 144 | Batch_idx: 140 |  Loss: (0.1400) |  Loss2: (0.0000) | Acc: (95.00%) (17205/18048)
Epoch: 144 | Batch_idx: 150 |  Loss: (0.1391) |  Loss2: (0.0000) | Acc: (95.00%) (18435/19328)
Epoch: 144 | Batch_idx: 160 |  Loss: (0.1401) |  Loss2: (0.0000) | Acc: (95.00%) (19662/20608)
Epoch: 144 | Batch_idx: 170 |  Loss: (0.1400) |  Loss2: (0.0000) | Acc: (95.00%) (20882/21888)
Epoch: 144 | Batch_idx: 180 |  Loss: (0.1407) |  Loss2: (0.0000) | Acc: (95.00%) (22086/23168)
Epoch: 144 | Batch_idx: 190 |  Loss: (0.1407) |  Loss2: (0.0000) | Acc: (95.00%) (23300/24448)
Epoch: 144 | Batch_idx: 200 |  Loss: (0.1399) |  Loss2: (0.0000) | Acc: (95.00%) (24523/25728)
Epoch: 144 | Batch_idx: 210 |  Loss: (0.1396) |  Loss2: (0.0000) | Acc: (95.00%) (25746/27008)
Epoch: 144 | Batch_idx: 220 |  Loss: (0.1401) |  Loss2: (0.0000) | Acc: (95.00%) (26961/28288)
Epoch: 144 | Batch_idx: 230 |  Loss: (0.1407) |  Loss2: (0.0000) | Acc: (95.00%) (28183/29568)
Epoch: 144 | Batch_idx: 240 |  Loss: (0.1414) |  Loss2: (0.0000) | Acc: (95.00%) (29392/30848)
Epoch: 144 | Batch_idx: 250 |  Loss: (0.1409) |  Loss2: (0.0000) | Acc: (95.00%) (30620/32128)
Epoch: 144 | Batch_idx: 260 |  Loss: (0.1406) |  Loss2: (0.0000) | Acc: (95.00%) (31846/33408)
Epoch: 144 | Batch_idx: 270 |  Loss: (0.1419) |  Loss2: (0.0000) | Acc: (95.00%) (33049/34688)
Epoch: 144 | Batch_idx: 280 |  Loss: (0.1427) |  Loss2: (0.0000) | Acc: (95.00%) (34251/35968)
Epoch: 144 | Batch_idx: 290 |  Loss: (0.1420) |  Loss2: (0.0000) | Acc: (95.00%) (35479/37248)
Epoch: 144 | Batch_idx: 300 |  Loss: (0.1416) |  Loss2: (0.0000) | Acc: (95.00%) (36706/38528)
Epoch: 144 | Batch_idx: 310 |  Loss: (0.1418) |  Loss2: (0.0000) | Acc: (95.00%) (37923/39808)
Epoch: 144 | Batch_idx: 320 |  Loss: (0.1413) |  Loss2: (0.0000) | Acc: (95.00%) (39153/41088)
Epoch: 144 | Batch_idx: 330 |  Loss: (0.1409) |  Loss2: (0.0000) | Acc: (95.00%) (40373/42368)
Epoch: 144 | Batch_idx: 340 |  Loss: (0.1412) |  Loss2: (0.0000) | Acc: (95.00%) (41589/43648)
Epoch: 144 | Batch_idx: 350 |  Loss: (0.1409) |  Loss2: (0.0000) | Acc: (95.00%) (42813/44928)
Epoch: 144 | Batch_idx: 360 |  Loss: (0.1409) |  Loss2: (0.0000) | Acc: (95.00%) (44035/46208)
Epoch: 144 | Batch_idx: 370 |  Loss: (0.1415) |  Loss2: (0.0000) | Acc: (95.00%) (45238/47488)
Epoch: 144 | Batch_idx: 380 |  Loss: (0.1415) |  Loss2: (0.0000) | Acc: (95.00%) (46452/48768)
Epoch: 144 | Batch_idx: 390 |  Loss: (0.1413) |  Loss2: (0.0000) | Acc: (95.00%) (47625/50000)
# TEST : Loss: (0.4255) | Acc: (87.00%) (8760/10000)
percent tensor([0.5198, 0.4802], device='cuda:0')
percent tensor([0.4576, 0.5424], device='cuda:0')
percent tensor([0.5629, 0.4371], device='cuda:0')
percent tensor([0.6014, 0.3986], device='cuda:0')
percent tensor([0.5792, 0.4208], device='cuda:0')
percent tensor([0.6533, 0.3467], device='cuda:0')
percent tensor([0.6937, 0.3063], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 145 | Batch_idx: 0 |  Loss: (0.1473) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 145 | Batch_idx: 10 |  Loss: (0.1714) |  Loss2: (0.0000) | Acc: (94.00%) (1325/1408)
Epoch: 145 | Batch_idx: 20 |  Loss: (0.1757) |  Loss2: (0.0000) | Acc: (93.00%) (2519/2688)
Epoch: 145 | Batch_idx: 30 |  Loss: (0.1831) |  Loss2: (0.0000) | Acc: (93.00%) (3711/3968)
Epoch: 145 | Batch_idx: 40 |  Loss: (0.1840) |  Loss2: (0.0000) | Acc: (93.00%) (4909/5248)
Epoch: 145 | Batch_idx: 50 |  Loss: (0.1787) |  Loss2: (0.0000) | Acc: (93.00%) (6119/6528)
Epoch: 145 | Batch_idx: 60 |  Loss: (0.1805) |  Loss2: (0.0000) | Acc: (93.00%) (7307/7808)
Epoch: 145 | Batch_idx: 70 |  Loss: (0.1812) |  Loss2: (0.0000) | Acc: (93.00%) (8502/9088)
Epoch: 145 | Batch_idx: 80 |  Loss: (0.1787) |  Loss2: (0.0000) | Acc: (93.00%) (9715/10368)
Epoch: 145 | Batch_idx: 90 |  Loss: (0.1755) |  Loss2: (0.0000) | Acc: (93.00%) (10927/11648)
Epoch: 145 | Batch_idx: 100 |  Loss: (0.1769) |  Loss2: (0.0000) | Acc: (93.00%) (12124/12928)
Epoch: 145 | Batch_idx: 110 |  Loss: (0.1775) |  Loss2: (0.0000) | Acc: (93.00%) (13329/14208)
Epoch: 145 | Batch_idx: 120 |  Loss: (0.1782) |  Loss2: (0.0000) | Acc: (93.00%) (14524/15488)
Epoch: 145 | Batch_idx: 130 |  Loss: (0.1781) |  Loss2: (0.0000) | Acc: (93.00%) (15714/16768)
Epoch: 145 | Batch_idx: 140 |  Loss: (0.1780) |  Loss2: (0.0000) | Acc: (93.00%) (16912/18048)
Epoch: 145 | Batch_idx: 150 |  Loss: (0.1765) |  Loss2: (0.0000) | Acc: (93.00%) (18117/19328)
Epoch: 145 | Batch_idx: 160 |  Loss: (0.1770) |  Loss2: (0.0000) | Acc: (93.00%) (19320/20608)
Epoch: 145 | Batch_idx: 170 |  Loss: (0.1774) |  Loss2: (0.0000) | Acc: (93.00%) (20515/21888)
Epoch: 145 | Batch_idx: 180 |  Loss: (0.1769) |  Loss2: (0.0000) | Acc: (93.00%) (21723/23168)
Epoch: 145 | Batch_idx: 190 |  Loss: (0.1774) |  Loss2: (0.0000) | Acc: (93.00%) (22919/24448)
Epoch: 145 | Batch_idx: 200 |  Loss: (0.1761) |  Loss2: (0.0000) | Acc: (93.00%) (24130/25728)
Epoch: 145 | Batch_idx: 210 |  Loss: (0.1747) |  Loss2: (0.0000) | Acc: (93.00%) (25342/27008)
Epoch: 145 | Batch_idx: 220 |  Loss: (0.1733) |  Loss2: (0.0000) | Acc: (93.00%) (26566/28288)
Epoch: 145 | Batch_idx: 230 |  Loss: (0.1724) |  Loss2: (0.0000) | Acc: (93.00%) (27777/29568)
Epoch: 145 | Batch_idx: 240 |  Loss: (0.1722) |  Loss2: (0.0000) | Acc: (93.00%) (28981/30848)
Epoch: 145 | Batch_idx: 250 |  Loss: (0.1725) |  Loss2: (0.0000) | Acc: (93.00%) (30184/32128)
Epoch: 145 | Batch_idx: 260 |  Loss: (0.1727) |  Loss2: (0.0000) | Acc: (93.00%) (31380/33408)
Epoch: 145 | Batch_idx: 270 |  Loss: (0.1719) |  Loss2: (0.0000) | Acc: (93.00%) (32598/34688)
Epoch: 145 | Batch_idx: 280 |  Loss: (0.1721) |  Loss2: (0.0000) | Acc: (93.00%) (33796/35968)
Epoch: 145 | Batch_idx: 290 |  Loss: (0.1716) |  Loss2: (0.0000) | Acc: (93.00%) (35006/37248)
Epoch: 145 | Batch_idx: 300 |  Loss: (0.1705) |  Loss2: (0.0000) | Acc: (94.00%) (36219/38528)
Epoch: 145 | Batch_idx: 310 |  Loss: (0.1689) |  Loss2: (0.0000) | Acc: (94.00%) (37448/39808)
Epoch: 145 | Batch_idx: 320 |  Loss: (0.1694) |  Loss2: (0.0000) | Acc: (94.00%) (38643/41088)
Epoch: 145 | Batch_idx: 330 |  Loss: (0.1696) |  Loss2: (0.0000) | Acc: (94.00%) (39842/42368)
Epoch: 145 | Batch_idx: 340 |  Loss: (0.1693) |  Loss2: (0.0000) | Acc: (94.00%) (41045/43648)
Epoch: 145 | Batch_idx: 350 |  Loss: (0.1693) |  Loss2: (0.0000) | Acc: (94.00%) (42249/44928)
Epoch: 145 | Batch_idx: 360 |  Loss: (0.1690) |  Loss2: (0.0000) | Acc: (94.00%) (43465/46208)
Epoch: 145 | Batch_idx: 370 |  Loss: (0.1685) |  Loss2: (0.0000) | Acc: (94.00%) (44676/47488)
Epoch: 145 | Batch_idx: 380 |  Loss: (0.1680) |  Loss2: (0.0000) | Acc: (94.00%) (45888/48768)
Epoch: 145 | Batch_idx: 390 |  Loss: (0.1676) |  Loss2: (0.0000) | Acc: (94.00%) (47059/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_145.pth.tar'
# TEST : Loss: (0.4327) | Acc: (87.00%) (8722/10000)
percent tensor([0.5213, 0.4787], device='cuda:0')
percent tensor([0.4695, 0.5305], device='cuda:0')
percent tensor([0.5674, 0.4326], device='cuda:0')
percent tensor([0.5980, 0.4020], device='cuda:0')
percent tensor([0.5742, 0.4258], device='cuda:0')
percent tensor([0.6871, 0.3129], device='cuda:0')
percent tensor([0.6664, 0.3336], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
